[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Superimposed text annotations have been under-investigated, yet areubiquitous, useful and important, especially in medical images. Medical imagesalso highlight the challenges posed by low resolution, noise and superimposedtextual meta-information. Therefor we probed the impact of superimposing textonto medical images on VQA. Our results revealed that this textualmeta-information can be added without severely degrading key measures of VQAperformance. Our findings are significant because they validate the practice ofsuperimposing text on images, even for medical images subjected to the VQA taskusing AI techniques. The work helps advance understanding of VQA in generaland, in particular, in the domain of healthcare and medicine.", "output": "Visual Question Answering (VQA) on Images with Superimposed Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The exploration of few-shot tabular learning becomes imperative. Tabular datais a versatile representation that captures diverse information, yet it is notexempt from limitations, property of data and model size. Labeling extensivetabular data can be challenging, and it may not be feasible to capture everyimportant feature. Few-shot tabular learning, however, remains relativelyunexplored, primarily due to scarcity of shared information among independentdatasets and the inherent ambiguity in defining boundaries within tabular data.To the best of our knowledge, no meaningful and unrestricted few-shot tabularlearning techniques have been developed without imposing constraints on thedataset. In this paper, we propose an innovative framework called TablEye,which aims to overcome the limit of forming prior knowledge for tabular data byadopting domain transformation. It facilitates domain transformation bygenerating tabular images, which effectively conserve the intrinsic semanticsof the original tabular data. This approach harnesses rigorously testedfew-shot learning algorithms and embedding functions to acquire and apply priorknowledge. Leveraging shared data domains allows us to utilize this priorknowledge, originally learned from the image domain. Specifically, TablEyedemonstrated a superior performance by outstripping the TabLLM in a 4-shot taskwith a maximum 0.11 AUC and a STUNT in a 1- shot setting, where it led onaverage by 3.17% accuracy.", "output": "TablEye: Seeing small Tables through the Lens of Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "From a service perspective, Multi-Source Domain Adaptation (MSDA) is apromising scenario to adapt a deployed model to a client's dataset. It canprovide adaptation without a target label and support the case where a sourcedataset is constructed from multiple domains. However, it is impractical,wherein its training heavily relies on prior domain information of themulti-source dataset -- how many domains exist and the domain label of eachdata sample. Moreover, MSDA requires both source and target datasetssimultaneously (physically), causing storage limitations on the client deviceor data privacy issues by transferring client data to a server. For a morepractical scenario of model adaptation from a service provider's point of view,we relax these constraints and present a novel problem scenario of Three-FreeDomain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, andmostly 3) source domain information (domain labels + the number of domains) areunavailable. Under the problem scenario, we propose a practical adaptationframework called FREEDOM. It leverages the power of the generative model,disentangling data into class and style aspects, where the style is defined asthe class-independent information from the source data and designed with anonparametric Bayesian approach. In the adaptation stage, FREEDOM aims to matchthe source class distribution with the target's under the philosophy that classdistribution is consistent even if the style is different; after then, onlypart of the classification model is deployed as a personalized network. As aresult, FREEDOM achieves state-of-the-art or comparable performance evenwithout domain information, with reduced final model size on the target side,independent of the number of source domains.", "output": "FREEDOM: Target Label & Source Data & Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study several methods for detecting anomalies in color images, constructedon patch-based auto-encoders. Wecompare the performance of three types ofmethods based, first, on the error between the original image and itsreconstruction,second, on the support estimation of the normal imagedistribution in the latent space, and third, on the error between theoriginalimage and a restored version of the reconstructed image. These methodsare evaluated on the industrial image database MVTecADand compared to twocompetitive state-of-the-art methods.", "output": "Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Document understanding refers to automatically extract, analyze andcomprehend information from various types of digital documents, such as a webpage. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl,have demonstrated promising zero-shot capabilities in shallow OCR-free textrecognition, indicating their potential for OCR-free document understanding.Nevertheless, without in-domain training, these models tend to ignorefine-grained OCR features, such as sophisticated tables or large blocks oftext, which are essential for OCR-free document understanding. In this paper,we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding.Specifically, we first construct a instruction tuning dataset featuring a widerange of visual-text understanding tasks. Then, we strengthen the OCR-freedocument understanding ability by jointly train the model on language-only,general vision-and-language, and document instruction tuning dataset with ourunified instruction tuning strategy. We also build an OCR-free documentinstruction understanding evaluation set LLMDoc to better compare models'capabilities on instruct compliance and document understanding. Experimentalresults show that our model outperforms existing multi-modal models,demonstrating its strong ability of document understanding. Besides, withoutspecific fine-tuning, mPLUG-DocOwl generalizes well on various downstreamtasks. Our code, models, training data and evaluation set are available at", "output": "mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The advancement in generative AI could be boosted with more accessiblemathematics. Beyond human-AI chat, large language models (LLMs) are emerging inprogramming, algorithm discovery, and theorem proving, yet their genomicsapplication is limited. This project introduces Math Agents and mathematicalembedding as fresh entries to the \"Moore's Law of Mathematics\", using aGPT-based workflow to convert equations from literature into LaTeX and Pythonformats. While many digital equation representations exist, there's a lack ofautomated large-scale evaluation tools. LLMs are pivotal as linguistic userinterfaces, providing natural language access for human-AI chat and formallanguages for large-scale AI-assisted computational infrastructure. Given theinfinite formal possibility spaces, Math Agents, which interact with math,could potentially shift us from \"big data\" to \"big math\". Math, unlike the moreflexible natural language, has properties subject to proof, enabling its usebeyond traditional applications like high-validation math-certified icons forAI alignment aims. This project aims to use Math Agents and mathematicalembeddings to address the ageing issue in information systems biology byapplying multiscalar physics mathematics to disease models and genomic data.Generative AI with episodic memory could help analyse causal relations inlongitudinal health records, using SIR Precision Health models. Genomic data issuggested for addressing the unsolved Alzheimer's disease problem.", "output": "Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper provides a comprehensive review of the literature concerning theutilization of Natural Language Processing (NLP) techniques, with a particularfocus on transformer-based large language models (LLMs) trained using Big Code,within the domain of AI-assisted programming tasks. LLMs, augmented withsoftware naturalness, have played a crucial role in facilitating AI-assistedprogramming applications, including code generation, code completion, codetranslation, code refinement, code summarization, defect detection, and clonedetection. Notable examples of such applications include the GitHub Copilotpowered by OpenAI's Codex and DeepMind AlphaCode. This paper presents anoverview of the major LLMs and their applications in downstream tasks relatedto AI-assisted programming. Furthermore, it explores the challenges andopportunities associated with incorporating NLP techniques with softwarenaturalness in these applications, with a discussion on extending AI-assistedprogramming capabilities to Apple's Xcode for mobile software development. Thispaper also presents the challenges of and opportunities for incorporating NLPtechniques with software naturalness, empowering developers with advancedcoding assistance and streamlining the software development process.", "output": "Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficiently capturing the complex spatiotemporal representations fromlarge-scale unlabeled traffic data remains to be a challenging task. Inconsidering of the dilemma, this work employs the advanced contrastive learningand proposes a novel Spatial-Temporal Synchronous Contextual ContrastiveLearning (STS-CCL) model. First, we elaborate the basic and strong augmentationmethods for spatiotemporal graph data, which not only perturb the data in termsof graph structure and temporal characteristics, but also employ alearning-based dynamic graph view generator for adaptive augmentation. Second,we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) tosimultaneously capture the decent spatial-temporal dependencies and realizegraph-level contrasting. To further discriminate node individuals in negativefiltering, a Semantic Contextual Contrastive method is designed based onsemantic features and spatial heterogeneity, achieving node-level contrastivelearning along with negative filtering. Finally, we present a hard mutual-viewcontrastive training scheme and extend the classic contrastive loss to anintegrated objective function, yielding better performance. Extensiveexperiments and evaluations demonstrate that building a predictor upon STS-CCLcontrastive learning model gains superior performance than existing trafficforecasting benchmarks. The proposed STS-CCL is highly suitable for largedatasets with only a few labeled data and other spatiotemporal tasks with datascarcity issue.", "output": "STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "AI Image generators based on diffusion models are widely discussed recentlyfor their capability to create images from simple text prompts. But, forpractical use in civil engineering they need to be able to create specificconstruction plans for given constraints. Within this paper we explore thecapabilities of those diffusion-based AI generators for computational design atthe example of floor plans and identify their current limitation. We explainhow the diffusion-models work and propose new diffusion models with improvedsemantic encoding. In several experiments we show that we can improve validityof generated floor plans from 6% to 90% and query performance for differentexamples. We identify short comings and derive future research challenges ofthose models and discuss the need to combine diffusion models with buildinginformation modelling. With this we provide key insights into the current stateand future directions for diffusion models in civil engineering.", "output": "Diffusion Models for Computational Design at the Example of Floor Plans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Alzheimer's disease (AD) is a common form of dementia that severely impactspatient health. As AD impairs the patient's language understanding andexpression ability, the speech of AD patients can serve as an indicator of thisdisease. This study investigates various methods for detecting AD usingpatients' speech and transcripts data from the DementiaBank Pitt database. Theproposed approach involves pre-trained language models and Graph Neural Network(GNN) that constructs a graph from the speech transcript, and extracts featuresusing GNN for AD detection. Data augmentation techniques, including synonymreplacement, GPT-based augmenter, and so on, were used to address the smalldataset size. Audio data was also introduced, and WavLM model was used toextract audio features. These features were then fused with text features usingvarious methods. Finally, a contrastive learning approach was attempted byconverting speech transcripts back to audio and using it for contrastivelearning with the original audio. We conducted intensive experiments andanalysis on the above methods. Our findings shed light on the challenges andpotential solutions in AD detection using speech and audio data.", "output": "Exploring Multimodal Approaches for Alzheimer's Disease Detection Using Patient Speech Transcript and Audio Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Independently trained machine learning models tend to learn similar features.Given an ensemble of independently trained models, this results in correlatedpredictions and common failure modes. Previous attempts focusing ondecorrelation of output predictions or logits yielded mixed results,particularly due to their reduction in model accuracy caused by conflictingoptimization objectives. In this paper, we propose the novel idea of utilizingmethods of the representational similarity field to promote dissimilarityduring training instead of measuring similarity of trained models. To this end,we promote intermediate representations to be dissimilar at different depthsbetween architectures, with the goal of learning robust ensembles with disjointfailure modes. We show that highly dissimilar intermediate representationsresult in less correlated output predictions and slightly lower errorconsistency, resulting in higher ensemble accuracy. With this, we shine firstlight on the connection between intermediate representations and their impacton the output predictions.", "output": "Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic graph embedding has emerged as a very effective technique foraddressing diverse temporal graph analytic tasks (i.e., link prediction, nodeclassification, recommender systems, anomaly detection, and graph generation)in various applications. Such temporal graphs exhibit heterogeneous transientdynamics, varying time intervals, and highly evolving node features throughouttheir evolution. Hence, incorporating long-range dependencies from thehistorical graph context plays a crucial role in accurately learning theirtemporal dynamics. In this paper, we develop a graph embedding model withuncertainty quantification, TransformerG2G, by exploiting the advancedtransformer encoder to first learn intermediate node representations from itscurrent state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ isthe length of context). Moreover, we employ two projection layers to generatelower-dimensional multivariate Gaussian distributions as each node's latentembedding at timestamp $t$. We consider diverse benchmarks with varying levelsof ``novelty\" as measured by the TEA plots. Our experiments demonstrate thatthe proposed TransformerG2G model outperforms conventional multi-step methodsand our prior work (DynG2G) in terms of both link prediction accuracy andcomputational efficiency, especially for high degree of novelty. Furthermore,the learned time-dependent attention weights across multiple graph snapshotsreveal the development of an automatic adaptive time stepping enabled by thetransformer. Importantly, by examining the attention weights, we can uncovertemporal dependencies, identify influential elements, and gain insights intothe complex interactions within the graph structure. For example, we identifieda strong correlation between attention weights and node degree at the variousstages of the graph topology evolution.", "output": "TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Opioid related aberrant behaviors (ORAB) present novel risk factors foropioid overdose. Previously, ORAB have been mainly assessed by survey resultsand by monitoring drug administrations. Such methods however, cannot scale upand do not cover the entire spectrum of aberrant behaviors. On the other hand,ORAB are widely documented in electronic health record notes. This paperintroduces a novel biomedical natural language processing benchmark datasetnamed ODD, for ORAB Detection Dataset. ODD is an expert-annotated datasetcomprising of more than 750 publicly available EHR notes. ODD has been designedto identify ORAB from patients' EHR notes and classify them into ninecategories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3)Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7)Medication Changes, 8) Central Nervous System-related, and 9) SocialDeterminants of Health. We explored two state-of-the-art natural languageprocessing (NLP) models (finetuning pretrained language models andprompt-tuning approaches) to identify ORAB. Experimental results show that theprompt-tuning models outperformed the finetuning models in most cateogories andthe gains were especially higher among uncommon categories (Suggested aberrantbehavior, Diagnosed opioid dependency and Medication change). Although the bestmodel achieved the highest 83.92% on area under precision recall curve,uncommon classes (Suggested Aberrant Behavior, Diagnosed Opioid Dependence, andMedication Change) still have a large room for performance improvement.", "output": "ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT brings revolutionary social value but also raises concerns about themisuse of AI-generated content. Consequently, an important question is how todetect whether content is generated by ChatGPT or by human. Existing detectorsare built upon the assumption that there are distributional gaps betweenhuman-generated and AI-generated content. These gaps are typically identifiedusing statistical information or classifiers. Our research challenges thedistributional gap assumption in detectors. We find that detectors do noteffectively discriminate the semantic and stylistic gaps betweenhuman-generated and AI-generated content. Instead, the \"subtle differences\",such as an extra space, become crucial for detection. Based on this discovery,we propose the SpaceInfi strategy to evade detection. Experiments demonstratethe effectiveness of this strategy across multiple benchmarks and detectors. Wealso provide a theoretical explanation for why SpaceInfi is successful inevading perplexity-based detection. Our findings offer new insights andchallenges for understanding and constructing more applicable ChatGPTdetectors.", "output": "Evade ChatGPT Detectors via A Single Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human language acquisition is an efficient, supervised, and continualprocess. In this work, we took inspiration from how human babies acquire theirfirst language, and developed a computational process for word acquisitionthrough comparative learning. Motivated by cognitive findings, we generated asmall dataset that enables the computation models to compare the similaritiesand differences of various attributes, learn to filter out and extract thecommon information for each shared linguistic label. We frame the acquisitionof words as not only the information filtration process, but also asrepresentation-symbol mapping. This procedure does not involve a fixedvocabulary size, nor a discriminative objective, and allows the models tocontinually learn more concepts efficiently. Our results in controlledexperiments have shown the potential of this approach for efficient continuallearning of grounded words.", "output": "Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The surveillance of a pandemic is a challenging task, especially when crucialdata is distributed and stakeholders cannot or are unwilling to share. Toovercome this obstacle, federated methodologies should be developed toincorporate less sensitive evidence that entities are willing to provide. Thisstudy aims to explore the feasibility of pushing hypothesis tests behind eachcustodian's firewall and then meta-analysis to combine the results, and todetermine the optimal approach for reconstructing the hypothesis test andoptimizing the inference. We propose a hypothesis testing framework to identifya surge in the indicators and conduct power analyses and experiments on realand semi-synthetic data to showcase the properties of our proposed hypothesistest and suggest suitable methods for combining $p$-values. Our findingshighlight the potential of using $p$-value combination as a federatedmethodology for pandemic surveillance and provide valuable insights intointegrating available data sources.", "output": "Federated Epidemic Surveillance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) has been shown to learn sophisticated controlpolicies for complex tasks including games, robotics, heating and coolingsystems and text generation. The action-perception cycle in RL, however,generally assumes that a measurement of the state of the environment isavailable at each time step without a cost. In applications such as deep-seaand planetary robot exploration, materials design and medicine, however, therecan be a high cost associated with measuring, or even approximating, the stateof the environment. In this paper, we survey the recently growing literaturethat adopts the perspective that an RL agent might not need, or even want, acostly measurement at each time step. Within this context, we propose the DeepDynamic Multi-Step Observationless Agent (DMSOA), contrast it with theliterature and empirically evaluate it on OpenAI gym and Atari Pongenvironments. Our results, show that DMSOA learns a better policy with fewerdecision steps and measurements than the considered alternative from theliterature.", "output": "Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hosting database services on cloud systems has become a common practice. Thishas led to the increasing volume of database workloads, which provides theopportunity for pattern analysis. Discovering workload patterns from a businesslogic perspective is conducive to better understanding the trends andcharacteristics of the database system. However, existing workload patterndiscovery systems are not suitable for large-scale cloud databases which arecommonly employed by the industry. This is because the workload patterns oflarge-scale cloud databases are generally far more complicated than those ofordinary databases. In this paper, we propose Alibaba Workload Miner (AWM), areal-time system for discovering workload patterns in complicated large-scaleworkloads. AWM encodes and discovers the SQL query patterns logged from userrequests and optimizes the querying processing based on the discoveredpatterns. First, Data Collection &amp; Preprocessing Module collects streamingquery logs and encodes them into high-dimensional feature embeddings with richsemantic contexts and execution features. Next, Online Workload Mining Moduleseparates encoded queries by business groups and discovers the workloadpatterns for each group. Meanwhile, Offline Training Module collects labels andtrains the classification model using the labels. Finally, Pattern-basedOptimizing Module optimizes query processing in cloud databases by exploitingdiscovered patterns. Extensive experimental results on one synthetic datasetand two real-life datasets (extracted from Alibaba Cloud databases) show thatAWM enhances the accuracy of pattern discovery by 66% and reduce the latency ofonline inference by 22%, compared with the state-of-the-arts.", "output": "Real-time Workload Pattern Analysis for Large-scale Cloud Databases."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Acute Myeloid Leukemia (AML) is one of the most aggressive types ofhematological neoplasm. To support the specialists' decision about theappropriate therapy, patients with AML receive a prognostic of outcomesaccording to their cytogenetic and molecular characteristics, often dividedinto three risk categories: favorable, intermediate, and adverse. However, thecurrent risk classification has known problems, such as the heterogeneitybetween patients of the same risk group and no clear definition of theintermediate risk category. Moreover, as most patients with AML receive anintermediate-risk classification, specialists often demand other tests andanalyses, leading to delayed treatment and worsening of the patient's clinicalcondition. This paper presents the data analysis and an explainablemachine-learning model to support the decision about the most appropriatetherapy protocol according to the patient's survival prediction. In addition tothe prediction model being explainable, the results obtained are promising andindicate that it is possible to use it to support the specialists' decisionssafely. Most importantly, the findings offered in this study have the potentialto open new avenues of research toward better treatments and prognosticmarkers.", "output": "An explainable model to support the decision about the therapy protocol for AML."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large events such as conferences, concerts and sports games, often causesurges in demand for ride services that are not captured in average demandpatterns, posing unique challenges for routing algorithms. We propose alearning framework for an autonomous fleet of taxis that scrapes event datafrom the internet to predict and adapt to surges in demand and generatescooperative routing and pickup policies that service a higher number ofrequests than other routing protocols. We achieve this through a combination of(i) an event processing framework that scrapes the internet for eventinformation and generates dense vector representations that can be used asinput features for a neural network that predicts demand; (ii) a two neuralnetwork system that predicts hourly demand over the entire map, using thesedense vector representations; (iii) a probabilistic approach that leverageslocale occupancy schedules to map publicly available demand data over sectorsto discretized street intersections; and finally, (iv) a scalable model-basedreinforcement learning framework that uses the predicted demand overintersections to anticipate surges and route taxis using one-agent-at-a-timerollout with limited sampling certainty equivalence. We learn routing andpickup policies using real NYC ride share data for 2022 and information formore than 2000 events across 300 unique venues in Manhattan. We test ourapproach with a fleet of 100 taxis on a map with 38 different sectors (2235street intersections). Our experimental results demonstrate that our methodobtains routing policies that service $6$ more requests on average per minute(around $360$ more requests per hour) than other model-based RL frameworks andother classical algorithms in operations research when dealing with surgedemand conditions.", "output": "Surge Routing: Event-informed Multiagent Reinforcement Learning for Autonomous Rideshare."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world problems are often comprised of many objectives and requiresolutions that carefully trade-off between them. Current approaches tomany-objective optimization often require challenging assumptions, likeknowledge of the importance/difficulty of objectives in a weighted-sumsingle-objective paradigm, or enormous populations to overcome the curse ofdimensionality in multi-objective Pareto optimization. Combining elements fromMany-Objective Evolutionary Algorithms and Quality Diversity algorithms likeMAP-Elites, we propose Many-objective Optimization via Voting for Elites(MOVE). MOVE maintains a map of elites that perform well on different subsetsof the objective functions. On a 14-objective image-neuroevolution problem, wedemonstrate that MOVE is viable with a population of as few as 50 elites andoutperforms a naive single-objective baseline. We find that the algorithm'sperformance relies on solutions jumping across bins (for a parent to produce achild that is elite for a different subset of objectives). We suggest that thistype of goal-switching is an implicit method to automatic identification ofstepping stones or curriculum learning. We comment on the similarities anddifferences between MOVE and MAP-Elites, hoping to provide insight to aid inthe understanding of that approach $unicode{x2013}$ and suggest future workthat may inform this approach's use for many-objective problems in general.", "output": "Many-objective Optimization via Voting for Elites."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Connected and autonomous vehicles (CAVs) can reduce human errors in trafficaccidents, increase road efficiency, and execute various tasks ranging fromdelivery to smart city surveillance. Reaping these benefits requires CAVs toautonomously navigate to target destinations. To this end, each CAV'snavigation controller must leverage the information collected by sensors andwireless systems for decision-making on longitudinal and lateral movements.However, enabling autonomous navigation for CAVs requires a convergentintegration of communication, control, and learning systems. The goal of thisarticle is to explicitly expose the challenges related to this convergence andpropose solutions to address them in two major use cases: Uncoordinated andcoordinated CAVs. In particular, challenges related to the navigation ofuncoordinated CAVs include stable path tracking, robust control againstcyber-physical attacks, and adaptive navigation controller design. Meanwhile,when multiple CAVs coordinate their movements during navigation, fundamentalproblems such as stable formation, fast collaborative learning, and distributedintrusion detection are analyzed. For both cases, solutions using theconvergence of communication theory, control theory, and machine learning areproposed to enable effective and secure CAV navigation. Preliminary simulationresults are provided to show the merits of proposed solutions.", "output": "Convergence of Communications, Control, and Machine Learning for Secure and Autonomous Vehicle Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article is a short introduction to AI4OPT, the NSF AI Institute forAdvances in Optimization. AI4OPT fuses AI and Optimization, inspired by end-usecases in supply chains, energy systems, chip design and manufacturing, andsustainable food systems. AI4OPT also applies its \"teaching the teachers\"philosophy to provide longitudinal educational pathways in AI for engineering.", "output": "AI4OPT: AI Institute for Advances in Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent surge of large language models (LLMs) highlights their ability toperform in-context learning, i.e., \"learning\" to perform a task from a fewdemonstrations in the context without any parameter updates. However, theircapabilities of in-context learning are limited by the model architecture: 1)the use of demonstrations is constrained by a maximum sentence length due topositional embeddings; 2) the quadratic complexity of attention hinders usersfrom using more demonstrations efficiently; 3) LLMs are shown to be sensitiveto the order of the demonstrations. In this work, we tackle these challenges byproposing a better architectural design for in-context learning. We proposeSAICL (Structured Attention for In-Context Learning), which replaces thefull-attention by a structured attention mechanism designed for in-contextlearning, and removes unnecessary dependencies between individualdemonstrations, while making the model invariant to the permutation ofdemonstrations. We evaluate SAICL in a meta-training framework and show thatSAICL achieves comparable or better performance than full attention whileobtaining up to 3.4x inference speed-up. SAICL also consistently outperforms astrong Fusion-in-Decoder (FiD) baseline which processes each demonstrationindependently. Finally, thanks to its linear nature, we demonstrate that SAICLcan easily scale to hundreds of demonstrations with continuous performancegains with scaling.", "output": "Scaling In-Context Demonstrations with Structured Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-Agent Path Finding (MAPF) is a crucial component for many large-scalerobotic systems, where agents must plan their collision-free paths to theirgiven goal positions. Recently, multi-agent reinforcement learning has beenintroduced to solve the partially observable variant of MAPF by learning adecentralized single-agent policy in a centralized fashion based on eachagent's partial observation. However, existing learning-based methods areineffective in achieving complex multi-agent cooperation, especially incongested environments, due to the non-stationarity of this setting. To tacklethis challenge, we propose a multi-agent actor-critic method called SoftActor-Critic with Heuristic-Based Attention (SACHA), which employs novelheuristic-based attention mechanisms for both the actors and critics toencourage cooperation among agents. SACHA learns a neural network for eachagent to selectively pay attention to the shortest path heuristic guidance frommultiple agents within its field of view, thereby allowing for more scalablelearning of cooperation. SACHA also extends the existing multi-agentactor-critic framework by introducing a novel critic centered on each agent toapproximate $Q$-values. Compared to existing methods that use a fullyobservable critic, our agent-centered multi-agent actor-critic method resultsin more impartial credit assignment and better generalizability of the learnedpolicy to MAPF instances with varying numbers of agents and types ofenvironments. We also implement SACHA(C), which embeds a communication modulein the agent's policy network to enable information exchange among agents. Weevaluate both SACHA and SACHA(C) on a variety of MAPF instances and demonstratedecent improvements over several state-of-the-art learning-based MAPF methodswith respect to success rate and solution quality.", "output": "SACHA: Soft Actor-Critic with Heuristic-Based Attention for Partially Observable Multi-Agent Path Finding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the essential components of deep learning is the choice of the lossfunction and performance metrics used to train and evaluate models. This paperreviews the most prevalent loss functions and performance measurements in deeplearning. We examine the benefits and limits of each technique and illustratetheir application to various deep-learning problems. Our review aims to give acomprehensive picture of the different loss functions and performanceindicators used in the most common deep learning tasks and help practitionerschoose the best method for their specific task.", "output": "Loss Functions and Metrics in Deep Learning. A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Logical assessment formula (LAF) is a new theory proposed for evaluationswith inaccurate ground-truth labels (IAGTLs) to assess the predictive modelsfor various artificial intelligence applications. However, the practicabilityof LAF for evaluations with IAGTLs has not yet been validated in real-worldpractice. In this paper, to address this issue, we applied LAF to tumoursegmentation for breast cancer (TSfBC) in medical histopathology whole slideimage analysis (MHWSIA). Experimental results and analysis show the validity ofLAF for evaluations with IAGTLs in the case of TSfBC and reflect the potentialsof LAF applied to MHWSIA.", "output": "Validation of the Practicability of Logical Assessment Formula for Evaluations with Inaccurate Ground-Truth Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accommodating all the weights on-chip for large-scale NNs remains a greatchallenge for SRAM based computing-in-memory (SRAM-CIM) with limited on-chipcapacity. Previous non-volatile SRAM-CIM (nvSRAM-CIM) addresses this issue byintegrating high-density single-level ReRAMs on the top of high-efficiencySRAM-CIM for weight storage to eliminate the off-chip memory access. However,previous SL-nvSRAM-CIM suffers from poor scalability for an increased number ofSL-ReRAMs and limited computing efficiency. To overcome these challenges, thiswork proposes an ultra-high-density three-level ReRAMs-assistedcomputing-in-nonvolatile-SRAM (TL-nvSRAM-CIM) scheme for large NN models. Theclustered n-selector-n-ReRAM (cluster-nSnRs) is employed for reliableweight-restore with eliminated DC power. Furthermore, a ternary SRAM-CIMmechanism with differential computing scheme is proposed for energy-efficientternary MAC operations while preserving high NN accuracy. The proposedTL-nvSRAM-CIM achieves 7.8x higher storage density, compared with thestate-of-art works. Moreover, TL-nvSRAM-CIM shows up to 2.9x and 1.9x enhancedenergy-efficiency, respectively, compared to the baseline designs of SRAM-CIMand ReRAM-CIM, respectively.", "output": "TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "General purpose agents will require large repertoires of skills. Empowerment-- the maximum mutual information between skills and the states -- provides apathway for learning large collections of distinct skills, but mutualinformation is difficult to optimize. We introduce a new framework,Hierarchical Empowerment, that makes computing empowerment more tractable byintegrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning.Our framework makes two specific contributions. First, we introduce a newvariational lower bound on mutual information that can be used to computeempowerment over short horizons. Second, we introduce a hierarchicalarchitecture for computing empowerment over exponentially longer time scales.We verify the contributions of the framework in a series of simulated roboticstasks. In a popular ant navigation domain, our four level agents are able tolearn skills that cover a surface area over two orders of magnitude larger thanprior work.", "output": "Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The fine-grained action analysis of the existing action datasets ischallenged by insufficient action categories, low fine granularities, limitedmodalities, and tasks. In this paper, we propose a Multi-modality andMulti-task dataset of Figure Skating (MMFS) which was collected from the WorldFigure Skating Championships. MMFS, which possesses action recognition andaction quality assessment, captures RGB, skeleton, and is collected the scoreof actions from 11671 clips with 256 categories including spatial and temporallabels. The key contributions of our dataset fall into three aspects asfollows. (1) Independently spatial and temporal categories are first proposedto further explore fine-grained action recognition and quality assessment. (2)MMFS first introduces the skeleton modality for complex fine-grained actionquality assessment. (3) Our multi-modality and multi-task dataset encouragemore action analysis models. To benchmark our dataset, we adopt RGB-based andskeleton-based baseline methods for action recognition and action qualityassessment.", "output": "Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ideal long-term memory mechanism for Large Language Model (LLM) basedchatbots, would lay the foundation for continual learning, complex reasoningand allow sequential and temporal dependencies to be learnt. Creating this typeof memory mechanism is an extremely challenging problem. In this paper weexplore different methods of achieving the effect of long-term memory. Wepropose a new architecture focused on creating adaptable and updatablelong-term memory for AGI systems. We demonstrate through various experimentsthe benefits of the RecallM architecture, particularly the improved temporalunderstanding it provides.", "output": "RecallM: An Architecture for Temporal Context Understanding and Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The prevalent use of benchmarks in current offline reinforcement learning(RL) research has led to a neglect of the imbalance of real-world datasetdistributions in the development of models. The real-world offline RL datasetis often imbalanced over the state space due to the challenge of exploration orsafety considerations. In this paper, we specify properties of imbalanceddatasets in offline RL, where the state coverage follows a power lawdistribution characterized by skewed policies. Theoretically and empirically,we show that typically offline RL methods based on distributional constraints,such as conservative Q-learning (CQL), are ineffective in extracting policiesunder the imbalanced dataset. Inspired by natural intelligence, we propose anovel offline RL method that utilizes the augmentation of CQL with a retrievalprocess to recall past related experiences, effectively alleviating thechallenges posed by imbalanced datasets. We evaluate our method on severaltasks in the context of imbalanced datasets with varying levels of imbalance,utilizing the variant of D4RL. Empirical results demonstrate the superiority ofour method over other baselines.", "output": "Offline Reinforcement Learning with Imbalanced Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce a new self-supervised rationalization method,called KGRec, for knowledge-aware recommender systems. To effectively identifyinformative knowledge connections, we propose an attentive knowledgerationalization mechanism that generates rational scores for knowledgetriplets. With these scores, KGRec integrates generative and contrastiveself-supervised tasks for recommendation through rational masking. To highlightrationales in the knowledge graph, we design a novel generative task in theform of masking-reconstructing. By masking important knowledge with highrational scores, KGRec is trained to rebuild and highlight useful knowledgeconnections that serve as rationales. To further rationalize the effect ofcollaborative interactions on knowledge graph learning, we introduce acontrastive learning task that aligns signals from knowledge and user-iteminteraction views. To ensure noise-resistant contrasting, potential noisy edgesin both graphs judged by the rational scores are masked. Extensive experimentson three real-world datasets demonstrate that KGRec outperformsstate-of-the-art methods. We also provide the implementation codes for ourapproach at ", "output": "Knowledge Graph Self-Supervised Rationalization for Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, the quality of responses generated by different modern largelanguage models (LLMs) are hard to evaluate and compare automatically. Recentstudies suggest and predominantly use LLMs as a reference-free metric foropen-ended question answering. More specifically, they use the recognized\"strongest\" LLM as the evaluator, which conducts pairwise comparisons ofcandidate models' answers and provides a ranking score. However, this intuitivemethod has multiple problems, such as bringing in self-enhancement (favoringits own answers) and positional bias. We draw insights and lessons from theeducational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-basedevaluations. Specifically, we propose the (1) peer rank (PR) algorithm thattakes into account each peer LLM's pairwise preferences of all answer pairs,and outputs a final ranking of models; and (2) peer discussion (PD), where weprompt two LLMs to discuss and try to reach a mutual agreement on preferencesof two answers. We conduct experiments on two benchmark datasets. We find thatour approaches achieve higher accuracy and align better with human judgments,respectively. Interestingly, PR can induce a relatively accurate self-rankingof models under the anonymous setting, where each model's name is unrevealed.Our work provides space to explore evaluating models that are hard to comparefor humans.", "output": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have recently shown remarkable success in high-quality imagegeneration. Sometimes, however, a pre-trained diffusion model exhibits partialmisalignment in the sense that the model can generate good images, but itsometimes outputs undesirable images. If so, we simply need to prevent thegeneration of the bad images, and we call this task censoring. In this work, wepresent censored generation with a pre-trained diffusion model using a rewardmodel trained on minimal human feedback. We show that censoring can beaccomplished with extreme human feedback efficiency and that labels generatedwith a mere few minutes of human feedback are sufficient. Code available at:", "output": "Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate performance disparities in deep classifiers. We find that theability of classifiers to separate individuals into subgroups variessubstantially across medical imaging modalities and protected characteristics;crucially, we show that this property is predictive of algorithmic bias.Through theoretical analysis and extensive empirical evaluation, we find arelationship between subgroup separability, subgroup disparities, andperformance degradation when models are trained on data with systematic biassuch as underdiagnosis. Our findings shed new light on the question of howmodels become biased, providing important insights for the development of fairmedical imaging AI.", "output": "The Role of Subgroup Separability in Group-Fair Medical Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapid advances of large language models (LLMs), such as ChatGPT, arerevolutionizing data science and statistics. These state-of-the-art tools canstreamline complex processes. As a result, it reshapes the role of datascientists. We argue that LLMs are transforming the responsibilities of datascientists, shifting their focus from hands-on coding, data-wrangling andconducting standard analyses to assessing and managing analyses performed bythese automated AIs. This evolution of roles is reminiscent of the transitionfrom a software engineer to a product manager. We illustrate this transitionwith concrete data science case studies using LLMs in this paper. Thesedevelopments necessitate a meaningful evolution in data science education.Pedagogy must now place greater emphasis on cultivating diverse skillsets amongstudents, such as LLM-informed creativity, critical thinking, AI-guidedprogramming. LLMs can also play a significant role in the classroom asinteractive teaching and learning tools, contributing to personalizededucation. This paper discusses the opportunities, resources and openchallenges for each of these directions. As with any transformative technology,integrating LLMs into education calls for careful consideration. While LLMs canperform repetitive tasks efficiently, it's crucial to remember that their roleis to supplement human intelligence and creativity, not to replace it.Therefore, the new era of data science education should balance the benefits ofLLMs while fostering complementary human expertise and innovations. Inconclusion, the rise of LLMs heralds a transformative period for data scienceand its education. This paper seeks to shed light on the emerging trends,potential opportunities, and challenges accompanying this paradigm shift,hoping to spark further discourse and investigation into this exciting,uncharted territory.", "output": "What Should Data Science Education Do with Large Language Models?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the realm of personalized recommendation systems, the increasing concernis the amplification of belief imbalance and user biases, a phenomenonprimarily attributed to the filter bubble. Addressing this critical issue, weintroduce an innovative intermediate agency (BHEISR) between users and existingrecommendation systems to attenuate the negative repercussions of the filterbubble effect in extant recommendation systems. The main objective is to strikea belief balance for users while minimizing the detrimental influence caused byfilter bubbles. The BHEISR model amalgamates principles from nudge theory whileupholding democratic and transparent principles. It harnesses user-specificcategory information to stimulate curiosity, even in areas users mightinitially deem uninteresting. By progressively stimulating interest in novelcategories, the model encourages users to broaden their belief horizons andexplore the information they typically overlook. Our model is time-sensitiveand operates on a user feedback loop. It utilizes the existing recommendationalgorithm of the model and incorporates user feedback from the prior timeframe. This approach endeavors to transcend the constraints of the filterbubble, enrich recommendation diversity, and strike a belief balance amongusers while also catering to user preferences and system-specific businessrequirements. To validate the effectiveness and reliability of the BHEISRmodel, we conducted a series of comprehensive experiments with real-worlddatasets. These experiments compared the performance of the BHEISR modelagainst several baseline models using nearly 200 filter bubble-impacted usersas test subjects. Our experimental results conclusively illustrate the superiorperformance of the BHEISR model in mitigating filter bubbles and balancing userperspectives.", "output": "BHEISR: Nudging from Bias to Balance -- Promoting Belief Harmony by Eliminating Ideological Segregation in Knowledge-based Recommendations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although unsupervised domain adaptation (UDA) is a promising direction toalleviate domain shift, they fall short of their supervised counterparts. Inthis work, we investigate relatively less explored semi-supervised domainadaptation (SSDA) for medical image segmentation, where access to a few labeledtarget samples can improve the adaptation performance substantially.Specifically, we propose a two-stage training process. First, an encoder ispre-trained in a self-learning paradigm using a novel domain-contentdisentangled contrastive learning (CL) along with a pixel-level featureconsistency constraint. The proposed CL enforces the encoder to learndiscriminative content-specific but domain-invariant semantics on a globalscale from the source and target images, whereas consistency regularizationenforces the mining of local pixel-level information by maintaining spatialsensitivity. This pre-trained encoder, along with a decoder, is furtherfine-tuned for the downstream task, (i.e. pixel-level segmentation) using asemi-supervised setting. Furthermore, we experimentally validate that ourproposed method can easily be extended for UDA settings, adding to thesuperiority of the proposed strategy. Upon evaluation on two domain adaptiveimage segmentation tasks, our proposed method outperforms the SoTA methods,both in SSDA and UDA settings. Code is available at", "output": "Semi-supervised Domain Adaptive Medical Image Segmentation through Consistency Regularized Disentangled Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Speech emotion recognition is a challenging task in speech processing field.For this reason, feature extraction process has a crucial importance todemonstrate and process the speech signals. In this work, we represent a model,which feeds raw audio files directly into the deep neural networks without anyfeature extraction stage for the recognition of emotions utilizing sixdifferent data sets, EMO-DB, RAVDESS, TESS, CREMA, SAVEE, and TESS+RAVDESS. Todemonstrate the contribution of proposed model, the performance of traditionalfeature extraction techniques namely, mel-scale spectogram, mel-frequencycepstral coefficients, are blended with machine learning algorithms, ensemblelearning methods, deep and hybrid deep learning techniques. Support vectormachine, decision tree, naive Bayes, random forests models are evaluated asmachine learning algorithms while majority voting and stacking methods areassessed as ensemble learning techniques. Moreover, convolutional neuralnetworks, long short-term memory networks, and hybrid CNN- LSTM model areevaluated as deep learning techniques and compared with machine learning andensemble learning methods. To demonstrate the effectiveness of proposed model,the comparison with state-of-the-art studies are carried out. Based on theexperiment results, CNN model excels existent approaches with 95.86% ofaccuracy for TESS+RAVDESS data set using raw audio files, thence determiningthe new state-of-the-art. The proposed model performs 90.34% of accuracy forEMO-DB with CNN model, 90.42% of accuracy for RAVDESS with CNN model, 99.48% ofaccuracy for TESS with LSTM model, 69.72% of accuracy for CREMA with CNN model,85.76% of accuracy for SAVEE with CNN model in speaker-independent audiocategorization problems.", "output": "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "News summary generation is an important task in the field of intelligenceanalysis, which can provide accurate and comprehensive information to helppeople better understand and respond to complex real-world events. However,traditional news summary generation methods face some challenges, which arelimited by the model itself and the amount of training data, as well as theinfluence of text noise, making it difficult to generate reliable informationaccurately. In this paper, we propose a new paradigm for news summarygeneration using LLM with powerful natural language understanding andgenerative capabilities. We use LLM to extract multiple structured eventpatterns from the events contained in news paragraphs, evolve the event patternpopulation with genetic algorithm, and select the most adaptive event patternto input into the LLM to generate news summaries. A News Summary Generator(NSG) is designed to select and evolve the event pattern populations andgenerate news summaries. The experimental results show that the news summarygenerator is able to generate accurate and reliable news summaries with somegeneralization ability.", "output": "Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traditional automation technologies alone are not sufficient to enabledriverless operation of trains (called Grade of Automation (GoA) 4) onnon-restricted infrastructure. The required perception tasks are nowadaysrealized using Machine Learning (ML) and thus need to be developed and deployedreliably and efficiently. One important aspect to achieve this is to use anMLOps process for tackling improved reproducibility, traceability,collaboration, and continuous adaptation of a driverless operation to changingconditions. MLOps mixes ML application development and operation (Ops) andenables high frequency software releases and continuous innovation based on thefeedback from operations. In this paper, we outline a safe MLOps process forthe continuous development and safety assurance of ML-based systems in therailway domain. It integrates system engineering, safety assurance, and the MLlife-cycle in a comprehensive workflow. We present the individual stages of theprocess and their interactions. Moreover, we describe relevant challenges toautomate the different stages of the safe MLOps process.", "output": "Towards a safe MLOps Process for the Continuous Development and Safety Assurance of ML-based Systems in the Railway Domain."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we analyze data-scarce classification scenarios, whereavailable labeled legal data is small and imbalanced, potentially hurting thequality of the results. We focused on two finetuning objectives; SetFit(Sentence Transformer Finetuning), a contrastive learning setup, and a vanillafinetuning setup on a legal provision classification task. Additionally, wecompare the features that are extracted with LIME (Local InterpretableModel-agnostic Explanations) to see which particular features contributed tothe model's classification decisions. The results show that a contrastive setupwith SetFit performed better than vanilla finetuning while using a fraction ofthe training samples. LIME results show that the contrastive learning approachhelps boost both positive and negative features which are legally informativeand contribute to the classification results. Thus a model finetuned with acontrastive objective seems to base its decisions more confidently on legallyinformative features.", "output": "Contrast Is All You Need."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of unfair discrimination between two groups andpropose a pre-processing method to achieve fairness. Corrective methods likestatistical parity usually lead to bad accuracy and do not really achievefairness in situations where there is a correlation between the sensitiveattribute S and the legitimate attribute E (explanatory variable) that shoulddetermine the decision. To overcome these drawbacks, other notions of fairnesshave been proposed, in particular, conditional statistical parity and equalopportunity. However, E is often not directly observable in the data, i.e., itis a latent variable. We may observe some other variable Z representing E, butthe problem is that Z may also be affected by S, hence Z itself can be biased.To deal with this problem, we propose BaBE (Bayesian Bias Elimination), anapproach based on a combination of Bayes inference and theExpectation-Maximization method, to estimate the most likely value of E for agiven Z for each group. The decision can then be based directly on theestimated E. We show, by experiments on synthetic and real data sets, that ourapproach provides a good level of fairness as well as high accuracy.", "output": "BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate recognition of cocktail party speech containing overlappingspeakers, noise and reverberation remains a highly challenging task to date.Motivated by the invariance of visual modality to acoustic signal corruption,an audio-visual multi-channel speech separation, dereverberation andrecognition approach featuring a full incorporation of visual information intoall system components is proposed in this paper. The efficacy of the videoinput is consistently demonstrated in mask-based MVDR speech separation,DNN-WPE or spectral mapping (SpecM) based speech dereverberation front-end andConformer ASR back-end. Audio-visual integrated front-end architecturesperforming speech separation and dereverberation in a pipelined or jointfashion via mask-based WPD are investigated. The error cost mismatch betweenthe speech enhancement front-end and ASR back-end components is minimized byend-to-end jointly fine-tuning using either the ASR cost function alone, or itsinterpolation with the speech enhancement loss. Experiments were conducted onthe mixture overlapped and reverberant speech data constructed using simulationor replay of the Oxford LRS2 dataset. The proposed audio-visual multi-channelspeech separation, dereverberation and recognition systems consistentlyoutperformed the comparable audio-only baseline by 9.1% and 6.2% absolute(41.7% and 36.0% relative) word error rate (WER) reductions. Consistent speechenhancement improvements were also obtained on PESQ, STOI and SRMR scores.", "output": "Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation and Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Textual noise, such as typos or abbreviations, is a well-known issue thatpenalizes vanilla Transformers for most downstream tasks. We show that this isalso the case for sentence similarity, a fundamental task in multiple domains,e.g. matching, retrieval or paraphrasing. Sentence similarity can be approachedusing cross-encoders, where the two sentences are concatenated in the inputallowing the model to exploit the inter-relations between them. Previous worksaddressing the noise issue mainly rely on data augmentation strategies, showingimproved robustness when dealing with corrupted samples that are similar to theones used for training. However, all these methods still suffer from the tokendistribution shift induced by typos. In this work, we propose to tackle textualnoise by equipping cross-encoders with a novel LExical-aware Attention module(LEA) that incorporates lexical similarities between words in both sentences.By using raw text similarities, our approach avoids the tokenization shiftproblem obtaining improved robustness. We demonstrate that the attention biasintroduced by LEA helps cross-encoders to tackle complex scenarios with textualnoise, specially in domains with short-text descriptions and limited context.Experiments using three popular Transformer encoders in five e-commercedatasets for product matching show that LEA consistently boosts performanceunder the presence of noise, while remaining competitive on the original(clean) splits. We also evaluate our approach in two datasets for textualentailment and paraphrasing showing that LEA is robust to typos in domains withlonger sentences and more natural context. Additionally, we thoroughly analyzeseveral design choices in our approach, providing insights about the impact ofthe decisions made and fostering future research in cross-encoders dealing withtypos.", "output": "LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robotic solutions, in particular robotic arms, are becoming more frequentlydeployed for close collaboration with humans, for example in manufacturing ordomestic care environments. These robotic arms require the user to controlseveral Degrees-of-Freedom (DoFs) to perform tasks, primarily involvinggrasping and manipulating objects. Standard input devices predominantly havetwo DoFs, requiring time-consuming and cognitively demanding mode switches toselect individual DoFs. Contemporary Adaptive DoF Mapping Controls (ADMCs) haveshown to decrease the necessary number of mode switches but were up to now notable to significantly reduce the perceived workload. Users still bear themental workload of incorporating abstract mode switching into their workflow.We address this by providing feed-forward multimodal feedback using updatedrecommendations of ADMC, allowing users to visually compare the current and thesuggested mapping in real-time. We contrast the effectiveness of two newapproaches that a) continuously recommend updated DoF combinations or b) usediscrete thresholds between current robot movements and new recommendations.Both are compared in a Virtual Reality (VR) in-person study against a classiccontrol method. Significant results for lowered task completion time, fewermode switches, and reduced perceived workload conclusively establish that incombination with feedforward, ADMC methods can indeed outperform classic modeswitching. A lack of apparent quantitative differences between Continuous andThreshold reveals the importance of user-centered customization options.Including these implications in the development process will improve usability,which is essential for successfully implementing robotic technologies with highuser acceptance.", "output": "In Time and Space: Towards Usable Adaptive Control for Assistive Robotic Arms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement Learning (RL) provides a powerful framework for decision-makingin complex environments. However, implementing RL in hardware-efficient andbio-inspired ways remains a challenge. This paper presents a novel SpikingNeural Network (SNN) architecture for solving RL problems with real-valuedobservations. The proposed model incorporates multi-layered event-basedclustering, with the addition of Temporal Difference (TD)-error modulation andeligibility traces, building upon prior work. An ablation study confirms thesignificant impact of these components on the proposed model's performance. Atabular actor-critic algorithm with eligibility traces and a state-of-the-artProximal Policy Optimization (PPO) algorithm are used as benchmarks. Ournetwork consistently outperforms the tabular approach and successfullydiscovers stable control policies on classic RL environments: mountain car,cart-pole, and acrobot. The proposed model offers an appealing trade-off interms of computational and hardware implementation requirements. The model doesnot require an external memory buffer nor a global error gradient computation,and synaptic updates occur online, driven by local learning rules and abroadcasted TD-error signal. Thus, this work contributes to the development ofmore hardware-efficient RL solutions.", "output": "A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One challenge in text-to-image (T2I) generation is the inadvertent reflectionof culture gaps present in the training data, which signifies the disparity ingenerated image quality when the cultural elements of the input text are rarelycollected in the training set. Although various T2I models have shownimpressive but arbitrary examples, there is no benchmark to systematicallyevaluate a T2I model's ability to generate cross-cultural images. To bridge thegap, we propose a Challenging Cross-Cultural (C3) benchmark with comprehensiveevaluation criteria, which can assess how well-suited a model is to a targetculture. By analyzing the flawed images generated by the Stable Diffusion modelon the C3 benchmark, we find that the model often fails to generate certaincultural objects. Accordingly, we propose a novel multi-modal metric thatconsiders object-text alignment to filter the fine-tuning data in the targetculture, which is used to fine-tune a T2I model to improve cross-culturalgeneration. Experimental results show that our multi-modal metric providesstronger data selection performance on the C3 benchmark than existing metrics,in which the object-text alignment is crucial. We release the benchmark, data,code, and generated images to facilitate future research on culturally diverseT2I generation (", "output": "On the Cultural Gap in Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Adversarial Networks (GANs) have demonstrated their ability togenerate synthetic samples that match a target distribution. However, from aprivacy perspective, using GANs as a proxy for data sharing is not a safesolution, as they tend to embed near-duplicates of real samples in the latentspace. Recent works, inspired by k-anonymity principles, address this issuethrough sample aggregation in the latent space, with the drawback of reducingthe dataset by a factor of k. Our work aims to mitigate this problem byproposing a latent space navigation strategy able to generate diverse syntheticsamples that may support effective training of deep models, while addressingprivacy concerns in a principled way. Our approach leverages an auxiliaryidentity classifier as a guide to non-linearly walk between points in thelatent space, minimizing the risk of collision with near-duplicates of realsamples. We empirically demonstrate that, given any random pair of points inthe latent space, our walking strategy is safer than linear interpolation. Wethen test our path-finding strategy combined to k-same methods and demonstrate,on two benchmarks for tuberculosis and diabetic retinopathy classification,that training a model using samples generated by our approach mitigate drops inperformance, while keeping privacy preservation.", "output": "A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Manual assembly workers face increasing complexity in their work.Human-centered assistance systems could help, but object recognition as anenabling technology hinders sophisticated human-centered design of thesesystems. At the same time, activity recognition based on hand poses suffersfrom poor pose estimation in complex usage scenarios, such as wearing gloves.This paper presents a self-supervised pipeline for adapting hand poseestimation to specific use cases with minimal human interaction. This enablescheap and robust hand posebased activity recognition. The pipeline consists ofa general machine learning model for hand pose estimation trained on ageneralized dataset, spatial and temporal filtering to account for anatomicalconstraints of the hand, and a retraining step to improve the model. Differentparameter combinations are evaluated on a publicly available and annotateddataset. The best parameter and model combination is then applied to unlabelledvideos from a manual assembly scenario. The effectiveness of the pipeline isdemonstrated by training an activity recognition as a downstream task in themanual assembly scenario.", "output": "Self-supervised Optimization of Hand Pose Estimation using Anatomical Features and Iterative Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There are two major challenges for scaling up robot navigation around dynamicobstacles: the complex interaction dynamics of the obstacles can be hard tomodel analytically, and the complexity of planning and control growsexponentially in the number of obstacles. Data-driven and learning-basedmethods are thus particularly valuable in this context. However, data-drivenmethods are sensitive to distribution drift, making it hard to train andgeneralize learned models across different obstacle densities. We propose anovel method for compositional learning of Sequential Neural Control Barriermodels (SNCBFs) to achieve scalability. Our approach exploits an importantobservation: the spatial interaction patterns of multiple dynamic obstacles canbe decomposed and predicted through temporal sequences of states for eachobstacle. Through decomposition, we can generalize control policies trainedonly with a small number of obstacles, to environments where the obstacledensity can be 100x higher. We demonstrate the benefits of the proposed methodsin improving dynamic collision avoidance in comparison with existing methodsincluding potential fields, end-to-end reinforcement learning, andmodel-predictive control. We also perform hardware experiments and show thepractical effectiveness of the approach in the supplementary video.", "output": "Sequential Neural Barriers for Scalable Dynamic Obstacle Avoidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, Transformers, initially developed for language, have beensuccessfully applied to visual tasks. Vision Transformers have been shown topush the state-of-the-art in a wide range of tasks, including imageclassification, object detection, and semantic segmentation. While ampleresearch has shown promising results in art attribution and art authenticationtasks using Convolutional Neural Networks, this paper examines if thesuperiority of Vision Transformers extends to art authentication, improving,thus, the reliability of computer-based authentication of artworks. Using acarefully compiled dataset of authentic paintings by Vincent van Gogh and twocontrast datasets, we compare the art authentication performances of SwinTransformers with those of EfficientNet. Using a standard contrast setcontaining imitations and proxies (works by painters with styles closelyrelated to van Gogh), we find that EfficientNet achieves the best performanceoverall. With a contrast set that only consists of imitations, we find the SwinTransformer to be superior to EfficientNet by achieving an authenticationaccuracy of over 85%. These results lead us to conclude that VisionTransformers represent a strong and promising contender in art authentication,particularly in enhancing the computer-based ability to detect artisticimitations.", "output": "Art Authentication with Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many popular feature-attribution methods for interpreting deep neuralnetworks rely on computing the gradients of a model's output with respect toits inputs. While these methods can indicate which input features may beimportant for the model's prediction, they reveal little about the innerworkings of the model itself. In this paper, we observe that the gradientcomputation of a model is a special case of a more general formulation usingsemirings. This observation allows us to generalize the backpropagationalgorithm to efficiently compute other interpretable statistics about thegradient graph of a neural network, such as the highest-weighted path andentropy. We implement this generalized algorithm, evaluate it on syntheticdatasets to better understand the statistics it computes, and apply it to studyBERT's behavior on the subject-verb number agreement task (SVA). With thismethod, we (a) validate that the amount of gradient flow through a component ofa model reflects its importance to a prediction and (b) for SVA, identify whichpathways of the self-attention mechanism are most important.", "output": "Generalizing Backpropagation for Gradient-Based Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Applying deep learning techniques, particularly language models (LMs), inontology engineering has raised widespread attention. However, deep learningframeworks like PyTorch and Tensorflow are predominantly developed for Pythonprogramming, while widely-used ontology APIs, such as the OWL API and Jena, areprimarily Java-based. To facilitate seamless integration of these frameworksand APIs, we present Deeponto, a Python package designed for ontologyengineering. The package encompasses a core ontology processing module foundedon the widely-recognised and reliable OWL API, encapsulating its fundamentalfeatures in a more \"Pythonic\" manner and extending its capabilities to includeother essential components including reasoning, verbalisation, normalisation,projection, and more. Building on this module, Deeponto offers a suite oftools, resources, and algorithms that support various ontology engineeringtasks, such as ontology alignment and completion, by harnessing deep learningmethodologies, primarily pre-trained LMs. In this paper, we also demonstratethe practical utility of Deeponto through two use-cases: the Digital HealthCoaching in Samsung Research UK and the Bio-ML track of the Ontology AlignmentEvaluation Initiative (OAEI).", "output": "DeepOnto: A Python Package for Ontology Engineering with Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The scale of large pre-trained models (PTMs) poses significant challenges inadapting to downstream tasks due to the high optimization overhead and storagecosts associated with full-parameter fine-tuning. To address this, many studiesexplore parameter-efficient tuning methods, also framed as \"delta tuning\",which updates only a small subset of parameters, known as \"delta modules\",while keeping the backbone model's parameters fixed. However, the practicalityand flexibility of delta tuning have been limited due to existingimplementations that directly modify the code of the backbone PTMs andhard-code specific delta tuning methods for each PTM. In this paper, we presentOpenDelta, an open-source library that overcomes these limitations by providinga plug-and-play implementation of various delta tuning methods. Our noveltechniques eliminate the need to modify the backbone PTMs' code, makingOpenDelta compatible with different, even novel PTMs. OpenDelta is designed tobe simple, modular, and extensible, providing a comprehensive platform forresearchers and practitioners to adapt large PTMs efficiently.", "output": "OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sentence embeddings enable us to capture the semantic similarity of shorttexts. Most sentence embedding models are trained for general semantic textualsimilarity (STS) tasks. Therefore, to use sentence embeddings in a particulardomain, the model must be adapted to it in order to achieve good results.Usually, this is done by fine-tuning the entire sentence embedding model forthe domain of interest. While this approach yields state-of-the-art results,all of the model's weights are updated during fine-tuning, making this methodresource-intensive. Therefore, instead of fine-tuning entire sentence embeddingmodels for each target domain individually, we propose to train lightweightadapters. These domain-specific adapters do not require fine-tuning allunderlying sentence embedding model parameters. Instead, we only train a smallnumber of additional parameters while keeping the weights of the underlyingsentence embedding model fixed. Training domain-specific adapters allows alwaysusing the same base model and only exchanging the domain-specific adapters toadapt sentence embeddings to a specific domain. We show that using adapters forparameter-efficient domain adaptation of sentence embeddings yields competitiveperformance within 1% of a domain-adapted, entirely fine-tuned sentenceembedding model while only training approximately 3.6% of the parameters.", "output": "Efficient Domain Adaptation of Sentence Embeddings using Adapters."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) are gaining increasing popularity in bothacademia and industry, owing to their unprecedented performance in variousapplications. As LLMs continue to play a vital role in both research and dailyuse, their evaluation becomes increasingly critical, not only at the tasklevel, but also at the society level for better understanding of theirpotential risks. Over the past years, significant efforts have been made toexamine LLMs from various perspectives. This paper presents a comprehensivereview of these evaluation methods for LLMs, focusing on three key dimensions:what to evaluate, where to evaluate, and how to evaluate. Firstly, we providean overview from the perspective of evaluation tasks, encompassing generalnatural language processing tasks, reasoning, medical usage, ethics,educations, natural and social sciences, agent applications, and other areas.Secondly, we answer the `where' and `how' questions by diving into theevaluation methods and benchmarks, which serve as crucial components inassessing performance of LLMs. Then, we summarize the success and failure casesof LLMs in different tasks. Finally, we shed light on several future challengesthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights toresearchers in the realm of LLMs evaluation, thereby aiding the development ofmore proficient LLMs. Our key point is that evaluation should be treated as anessential discipline to better assist the development of LLMs. We consistentlymaintain the related open-source materials at:", "output": "A Survey on Evaluation of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Order execution is a fundamental task in quantitative finance, aiming atfinishing acquisition or liquidation for a number of trading orders of thespecific assets. Recent advance in model-free reinforcement learning (RL)provides a data-driven solution to the order execution problem. However, theexisting works always optimize execution for an individual order, overlookingthe practice that multiple orders are specified to execute simultaneously,resulting in suboptimality and bias. In this paper, we first present amulti-agent RL (MARL) method for multi-order execution considering practicalconstraints. Specifically, we treat every agent as an individual operator totrade one specific order, while keeping communicating with each other andcollaborating for maximizing the overall profits. Nevertheless, the existingMARL algorithms often incorporate communication among agents by exchanging onlythe information of their partial observations, which is inefficient incomplicated financial market. To improve collaboration, we then propose alearnable multi-round communication protocol, for the agents communicating theintended actions with each other and refining accordingly. It is optimizedthrough a novel action value attribution method which is provably consistentwith the original learning objective yet more efficient. The experiments on thedata from two real-world markets have illustrated superior performance withsignificantly better collaboration effectiveness achieved by our method.", "output": "Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large vision-language models have achieved outstanding performance, but theirsize and computational requirements make their deployment onresource-constrained devices and time-sensitive tasks impractical. Modeldistillation, the process of creating smaller, faster models that maintain theperformance of larger models, is a promising direction towards the solution.This paper investigates the distillation of visual representations in largeteacher vision-language models into lightweight student models using a small-or mid-scale dataset. Notably, this study focuses on open-vocabularyout-of-distribution (OOD) generalization, a challenging problem that has beenoverlooked in previous model distillation literature. We propose two principlesfrom vision and language modality perspectives to enhance student's OODgeneralization: (1) by better imitating teacher's visual representation space,and carefully promoting better coherence in vision-language alignment with theteacher; (2) by enriching the teacher's language representations withinformative and finegrained semantic attributes to effectively distinguishbetween different labels. We propose several metrics and conduct extensiveexperiments to investigate their techniques. The results demonstratesignificant improvements in zero-shot and few-shot student performance onopen-vocabulary out-of-distribution classification, highlighting theeffectiveness of our proposed approaches. Our code will be released at", "output": "Distilling Large Vision-Language Model with Out-of-Distribution Generalizability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The assembly instruction is a mandatory component of Lego-like brick sets.Theconventional production of assembly instructions requires a considerable amountof manual fine-tuning, which is intractable for casual users and customizedbrick sets.Moreover, the traditional paper-based instructions lackexpressiveness and interactivity.To tackle the two problems above, we presentBrickPal, an augmented reality-based system, which visualizes assemblyinstructions in an augmented reality head-mounted display. It utilizes NaturalLanguage Processing (NLP) techniques to generate plausible assembly sequences,and provide real-time guidance in the AR headset.Our user study demonstratesBrickPal's effectiveness at assisting users in brick assembly compared totraditional assembly methods. Additionally, the NLP algorithm-generatedassembly sequences achieve the same usability with manually adapted sequences.", "output": "BrickPal: Augmented Reality-based Assembly Instructions for Brick Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models have an exceptional capability to incorporate newinformation in a contextual manner. However, the full potential of such anapproach is often restrained due to a limitation in the effective contextlength. One solution to this issue is to endow an attention layer with accessto an external memory, which comprises of (key, value) pairs. Yet, as thenumber of documents increases, the proportion of relevant keys to irrelevantones decreases, leading the model to focus more on the irrelevant keys. Weidentify a significant challenge, dubbed the distraction issue, where keyslinked to different semantic values might overlap, making them hard todistinguish. To tackle this problem, we introduce the Focused Transformer(FoT), a technique that employs a training process inspired by contrastivelearning. This novel approach enhances the structure of the (key, value) space,enabling an extension of the context length. Our method allows for fine-tuningpre-existing, large-scale models to lengthen their effective context. This isdemonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. Theresulting models, which we name LongLLaMA, exhibit advancements in tasksrequiring a long context. We further illustrate that our LongLLaMA modelsadeptly manage a $256 k$ context length for passkey retrieval.", "output": "Focused Transformer: Contrastive Training for Context Scaling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Approaches based on Binary decision diagrams (BDDs) have recently achievedstate-of-the-art results for multiobjective integer programming problems. Thevariable ordering used in constructing BDDs can have a significant impact ontheir size and on the quality of bounds derived from relaxed or restricted BDDsfor single-objective optimization problems. We first showcase a similar impactof variable ordering on the Pareto frontier (PF) enumeration time for themultiobjective knapsack problem, suggesting the need for deriving variableordering methods that improve the scalability of the multiobjective BDDapproach. To that end, we derive a novel parameter configuration space based onvariable scoring functions which are linear in a small set of interpretable andeasy-to-compute variable features. We show how the configuration space can beefficiently explored using black-box optimization, circumventing the curse ofdimensionality (in the number of variables and objectives), and finding goodorderings that reduce the PF enumeration time. However, black-box optimizationapproaches incur a computational overhead that outweighs the reduction in timedue to good variable ordering. To alleviate this issue, we propose LEO, asupervised learning approach for finding efficient variable orderings thatreduce the enumeration time. Experiments on benchmark sets from the knapsackproblem with 3-7 objectives and up to 80 variables show that LEO is ~30-300%and ~10-200% faster at PF enumeration than common ordering strategies andalgorithm configuration. Our code and instances are available at", "output": "LEO: Learning Efficient Orderings for Multiobjective Binary Decision Diagrams."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous agriculture applications (e.g., inspection, phenotyping, pluckingfruits) require manipulating the plant foliage to look behind the leaves andthe branches. Partial visibility, extreme clutter, thin structures, and unknowngeometry and dynamics for plants make such manipulation challenging. We tacklethese challenges through data-driven methods. We use self-supervision to trainSRPNet, a neural network that predicts what space is revealed on execution of acandidate action on a given plant. We use SRPNet with the cross-entropy methodto predict actions that are effective at revealing space beneath plant foliage.Furthermore, as SRPNet does not just predict how much space is revealed butalso where it is revealed, we can execute a sequence of actions thatincrementally reveal more and more space beneath the plant foliage. Weexperiment with a synthetic (vines) and a real plant (Dracaena) on a physicaltest-bed across 5 settings including 2 settings that test generalization tonovel plant configurations. Our experiments reveal the effectiveness of ouroverall method, PPG, over a competitive hand-crafted exploration method, andthe effectiveness of SRPNet over a hand-crafted dynamics model and relevantablations.", "output": "Push Past Green: Learning to Look Behind Plant Foliage by Moving It."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We compare the impact of hardware advancement and algorithm advancement forSAT solving over the last two decades. In particular, we compare 20-year-oldSAT-solvers on new computer hardware with modern SAT-solvers on 20-year-oldhardware. Our findings show that the progress on the algorithmic side has atleast as much impact as the progress on the hardware side.", "output": "A Time Leap Challenge for SAT Solving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The integration of reasoning, learning, and decision-making is key to buildmore general artificial intelligence systems. As a step in this direction, wepropose a novel neural-logic architecture, called differentiable logic machine(DLM), that can solve both inductive logic programming (ILP) and reinforcementlearning (RL) problems, where the solution can be interpreted as a first-orderlogic program. Our proposition includes several innovations. Firstly, ourarchitecture defines a restricted but expressive continuous relaxation of thespace of first-order logic programs by assigning weights to predicates insteadof rules, in contrast to most previous neural-logic approaches. Secondly, withthis differentiable architecture, we propose several (supervised and RL)training procedures, based on gradient descent, which can recover afully-interpretable solution (i.e., logic formula). Thirdly, to accelerate RLtraining, we also design a novel critic architecture that enables actor-criticalgorithms. Fourthly, to solve hard problems, we propose an incrementaltraining procedure that can learn a logic program progressively. Compared tostate-of-the-art (SOTA) differentiable ILP methods, DLM successfully solves allthe considered ILP problems with a higher percentage of successful seeds (up to3.5$times$). On RL problems, without requiring an interpretable solution, DLMoutperforms other non-interpretable neural-logic RL approaches in terms ofrewards (up to 3.9%). When enforcing interpretability, DLM can solve harder RLproblems (e.g., Sorting, Path) Moreover, we show that deep logic programs canbe learned via incremental supervised training. In addition to this excellentperformance, DLM can scale well in terms of memory and computational time,especially during the testing phase where it can deal with much more constants($&gt;$2$times$) than SOTA.", "output": "Differentiable Logic Machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There are demographic biases present in current facial recognition (FR)models. To measure these biases across different ethnic and gender subgroups,we introduce our Balanced Faces in the Wild (BFW) dataset. This dataset allowsfor the characterization of FR performance per subgroup. We found that relyingon a single score threshold to differentiate between genuine and imposterssample pairs leads to suboptimal results. Additionally, performance withinsubgroups often varies significantly from the global average. Therefore,specific error rates only hold for populations that match the validation data.To mitigate imbalanced performances, we propose a novel domain adaptationlearning scheme that uses facial features extracted from state-of-the-artneural networks. This scheme boosts the average performance and preservesidentity information while removing demographic knowledge. Removing demographicknowledge prevents potential biases from affecting decision-making and protectsprivacy by eliminating demographic information. We explore the proposed methodand demonstrate that subgroup classifiers can no longer learn from featuresprojected using our domain adaptation scheme. For access to the source code anddata, please visit ", "output": "Balancing Biases and Preserving Privacy on Balanced Faces in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We establish a generic, model-theoretic characterization of belief revisionoperators implementing the paradigm of minimal change according to the seminalwork by Alchourr'{o}n, G\"{a}rdenfors, and Makinson (AGM). Ourcharacterization applies to all Tarskian logics, that is, all logics with aclassical model-theoretic semantics, and hence a wide variety of formalismsused in knowledge representation and beyond, including many for which amodel-theoretic characterization has hitherto been lacking. Our starting pointis the approach by Katsuno and Mendelzon (K&amp;M), who provided such acharacterization for propositional logic over finite signatures. We generalizeK&amp;M's approach to the setting of AGM-style revision over bases in arbitraryTarskian logics, where base may refer to one of the various ways ofrepresenting an agent's beliefs (such as belief sets, arbitrary or finite setsof sentences, or single sentences). Our first core result is a representationtheorem providing a two-way correspondence between AGM-style revision operatorsand specific assignments: functions associating every base to a \"preference\"relation over interpretations, which must be total but is - in contrast toprior approaches - not always transitive. As our second core contribution, weprovide a characterization of all logics for which our result can bestrengthened to assignments producing transitive preference relations (as inK&amp;M's original work). Alongside these main contributions, we discuss diversevariants of our findings as well as ramifications for other areas of beliefrevision theory.", "output": "AGM Belief Revision, Semantically."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An increasing number of reports raise concerns about the risk that machinelearning algorithms could amplify health disparities due to biases embedded inthe training data. Seyyed-Kalantari et al. find that models trained on threechest X-ray datasets yield disparities in false-positive rates (FPR) acrosssubgroups on the 'no-finding' label (indicating the absence of disease). Themodels consistently yield higher FPR on subgroups known to be historicallyunderserved, and the study concludes that the models exhibit and potentiallyeven amplify systematic underdiagnosis. We argue that the experimental setup inthe study is insufficient to study algorithmic underdiagnosis. In the absenceof specific knowledge (or assumptions) about the extent and nature of thedataset bias, it is difficult to investigate model bias. Importantly, their useof test data exhibiting the same bias as the training data (due to randomsplitting) severely complicates the interpretation of the reported disparities.", "output": "Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ontology Matching (OM) plays an important role in many domains such asbioinformatics and the Semantic Web, and its research is becoming increasinglypopular, especially with the application of machine learning (ML) techniques.Although the Ontology Alignment Evaluation Initiative (OAEI) represents animpressive effort for the systematic evaluation of OM systems, it still suffersfrom several limitations including limited evaluation of subsumption mappings,suboptimal reference mappings, and limited support for the evaluation ofML-based systems. To tackle these limitations, we introduce five new biomedicalOM tasks involving ontologies extracted from Mondo and UMLS. Each task includesboth equivalence and subsumption matching; the quality of reference mappings isensured by human curation, ontology pruning, etc.; and a comprehensiveevaluation framework is proposed to measure OM performance from variousperspectives for both ML-based and non-ML-based OM systems. We reportevaluation results for OM systems of different types to demonstrate the usageof these resources, all of which are publicly available as part of the newBioML track at OAEI 2022.", "output": "Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline reinforcement learning has shown great promise in leveraging largepre-collected datasets for policy learning, allowing agents to forgooften-expensive online data collection. However, offline reinforcement learningfrom visual observations with continuous action spaces remains under-explored,with a limited understanding of the key challenges in this complex domain. Inthis paper, we establish simple baselines for continuous control in the visualdomain and introduce a suite of benchmarking tasks for offline reinforcementlearning from visual observations designed to better represent the datadistributions present in real-world offline RL problems and guided by a set ofdesiderata for offline RL from visual observations, including robustness tovisual distractions and visually identifiable changes in dynamics. Using thissuite of benchmarking tasks, we show that simple modifications to two popularvision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2,suffice to outperform existing offline RL methods and establish competitivebaselines for continuous control in the visual domain. We rigorously evaluatethese algorithms and perform an empirical evaluation of the differences betweenstate-of-the-art model-based and model-free offline RL methods for continuouscontrol from visual observations. All code and data used in this evaluation areopen-sourced to facilitate progress in this domain.", "output": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The occurrence of West Nile Virus (WNV) represents one of the most commonmosquito-borne zoonosis viral infections. Its circulation is usually associatedwith climatic and environmental conditions suitable for vector proliferationand virus replication. On top of that, several statistical models have beendeveloped to shape and forecast WNV circulation: in particular, the recentmassive availability of Earth Observation (EO) data, coupled with thecontinuous advances in the field of Artificial Intelligence, offer valuableopportunities.In this paper, we seek to predict WNV circulation by feeding Deep NeuralNetworks (DNNs) with satellite images, which have been extensively shown tohold environmental and climatic features. Notably, while previous approachesanalyze each geographical site independently, we propose a spatial-awareapproach that considers also the characteristics of close sites. Specifically,we build upon Graph Neural Networks (GNN) to aggregate features fromneighbouring places, and further extend these modules to consider multiplerelations, such as the difference in temperature and soil moisture between twosites, as well as the geographical distance. Moreover, we inject time-relatedinformation directly into the model to take into account the seasonality ofvirus spread.We design an experimental setting that combines satellite images - fromLandsat and Sentinel missions - with ground truth observations of WNVcirculation in Italy. We show that our proposed Multi-Adjacency Graph AttentionNetwork (MAGAT) consistently leads to higher performance when paired with anappropriate pre-training stage. Finally, we assess the importance of eachcomponent of MAGAT in our ablation studies.", "output": "Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With recent advancements in diffusion models, users can generate high-qualityimages by writing text prompts in natural language. However, generating imageswith desired details requires proper prompts, and it is often unclear how amodel reacts to different prompts or what the best prompts are. To helpresearchers tackle these critical challenges, we introduce DiffusionDB, thefirst large-scale text-to-image prompt dataset totaling 6.5TB, containing 14million images generated by Stable Diffusion, 1.8 million unique prompts, andhyperparameters specified by real users. We analyze the syntactic and semanticcharacteristics of prompts. We pinpoint specific hyperparameter values andprompt styles that can lead to model errors and present evidence of potentiallyharmful model usage, such as the generation of misinformation. Theunprecedented scale and diversity of this human-actuated dataset provideexciting research opportunities in understanding the interplay between promptsand generative models, detecting deepfakes, and designing human-AI interactiontools to help users more easily use these models. DiffusionDB is publiclyavailable at: ", "output": "DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce our work of building a Streaming MultilingualSpeech Model (SM2), which can transcribe or translate multiple spoken languagesinto texts of the target language. The backbone of SM2 is TransformerTransducer, which has high streaming capability. Instead of human labeledspeech translation (ST) data, SM2 models are trained using weakly superviseddata generated by converting the transcriptions in speech recognition corporawith a machine translation service. With 351 thousand hours of anonymizedspeech training data from 25 languages, SM2 models achieve comparable or evenbetter ST quality than some recent popular large-scale non-streaming speechmodels. More importantly, we show that SM2 has the truly zero-shot capabilitywhen expanding to new target languages, yielding high quality ST results for{source-speech, target-text} pairs that are not seen during training.", "output": "A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent scene graph generation (SGG) frameworks have focused on learningcomplex relationships among multiple objects in an image. Thanks to the natureof the message passing neural network (MPNN) that models high-orderinteractions between objects and their neighboring objects, they are dominantrepresentation learning modules for SGG. However, existing MPNN-basedframeworks assume the scene graph as a homogeneous graph, which restricts thecontext-awareness of visual relations between objects. That is, they overlookthe fact that the relations tend to be highly dependent on the objects withwhich the relations are associated. In this paper, we propose an unbiasedheterogeneous scene graph generation (HetSGG) framework that capturesrelation-aware context using message passing neural networks. We devise a novelmessage passing layer, called relation-aware message passing neural network(RMP), that aggregates the contextual information of an image considering thepredicate type between objects. Our extensive evaluations demonstrate thatHetSGG outperforms state-of-the-art methods, especially outperforming on tailpredicate classes.", "output": "Unbiased Heterogeneous Scene Graph Generation with Relation-aware Message Passing Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Compared to conventional bilingual translation systems, massivelymultilingual machine translation is appealing because a single model cantranslate into multiple languages and benefit from knowledge transfer for lowresource languages. On the other hand, massively multilingual models sufferfrom the curse of multilinguality, unless scaling their size massively, whichincreases their training and inference costs. Sparse Mixture-of-Experts modelsare a way to drastically increase model capacity without the need for aproportional amount of computing. The recently released NLLB-200 is an exampleof such a model. It covers 202 languages but requires at least four 32GB GPUsjust for inference. In this work, we propose a pruning method that allows theremoval of up to 80% of experts with a negligible loss in translation quality,which makes it feasible to run the model on a single 32GB GPU. Further analysissuggests that our pruning metrics allow to identify language-specific expertsand prune non-relevant experts for a given language pair.", "output": "Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Answering complex queries on incomplete knowledge graphs is a challengingtask where a model needs to answer complex logical queries in the presence ofmissing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022)showed that neural link predictors could also be used for answering complexqueries: their Continuous Query Decomposition (CQD) method works by decomposingcomplex queries into atomic sub-queries, answers them using neural linkpredictors and aggregates their scores via t-norms for ranking the answers toeach complex query. However, CQD does not handle negations and only uses thetraining signal from atomic training queries: neural link prediction scores arenot calibrated to interact together via fuzzy logic t-norms during complexquery answering. In this work, we propose to address this problem by training aparameter-efficient score adaptation model to re-calibrate neural linkprediction scores: this new component is trained on complex queries byback-propagating through the complex query-answering process. Our method,CQD$^{A}$, produces significantly more accurate results than currentstate-of-the-art methods, improving from $34.4$ to $35.1$ Mean Reciprocal Rankvalues averaged across all datasets and query types while using $leq 35%$ ofthe available training query types. We further show that CQD$^{A}$ isdata-efficient, achieving competitive results with only $1%$ of the trainingdata, and robust in out-of-domain evaluations.", "output": "Adapting Neural Link Predictors for Complex Query Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to accurately locate and navigate to a specific object is acrucial capability for embodied agents that operate in the real world andinteract with objects to complete tasks. Such object navigation tasks usuallyrequire large-scale training in visual environments with labeled objects, whichgeneralizes poorly to novel objects in unknown environments. In this work, wepresent a novel zero-shot object navigation method, Exploration with SoftCommonsense constraints (ESC), that transfers commonsense knowledge inpre-trained models to open-world object navigation without any navigationexperience nor any other training on the visual environments. First, ESCleverages a pre-trained vision and language model for open-world prompt-basedgrounding and a pre-trained commonsense language model for room and objectreasoning. Then ESC converts commonsense knowledge into navigation actions bymodeling it as soft logic predicates for efficient exploration. Extensiveexperiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC methodimproves significantly over baselines, and achieves new state-of-the-artresults for zero-shot object navigation (e.g., 288% relative Success Rateimprovement than CoW on MP3D).", "output": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This manuscript is technical memoir about my work at Stanford and MicrosoftResearch. Included are fundamental concepts central to machine learning andartificial intelligence, applications of these concepts, and stories behindtheir creation.", "output": "Heckerthoughts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays many research articles are prefaced with research highlights tosummarize the main findings of the paper. Highlights not only help researchersprecisely and quickly identify the contributions of a paper, they also enhancethe discoverability of the article via search engines. We aim to automaticallyconstruct research highlights given certain segments of a research paper. Weuse a pointer-generator network with coverage mechanism and a contextualembedding layer at the input that encodes the input tokens into SciBERTembeddings. We test our model on a benchmark dataset, CSPubSum, and alsopresent MixSub, a new multi-disciplinary corpus of papers for automaticresearch highlight generation. For both CSPubSum and MixSub, we have observedthat the proposed model achieves the best performance compared to relatedvariants and other models proposed in the literature. On the CSPubSum dataset,our model achieves the best performance when the input is only the abstract ofa paper as opposed to other segments of the paper. It produces ROUGE-1, ROUGE-2and ROUGE-L F1-scores of 38.26, 14.26 and 35.51, respectively, METEOR score of32.62, and BERTScore F1 of 86.65 which outperform all other baselines. On thenew MixSub dataset, where only the abstract is the input, our proposed model(when trained on the whole training corpus without distinguishing between thesubject categories) achieves ROUGE-1, ROUGE-2 and ROUGE-L F1-scores of 31.78,9.76 and 29.3, respectively, METEOR score of 24.00, and BERTScore F1 of 85.25.", "output": "Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapid expansion of the Internet of Things (IoT) and Edge Computing haspresented challenges for centralized Machine and Deep Learning (ML/DL) methodsdue to the presence of distributed data silos that hold sensitive information.To address concerns regarding data privacy, collaborative andprivacy-preserving ML/DL techniques like Federated Learning (FL) have emerged.However, ensuring data privacy and performance alone is insufficient sincethere is a growing need to establish trust in model predictions. Existingliterature has proposed various approaches on trustworthy ML/DL (excluding dataprivacy), identifying robustness, fairness, explainability, and accountabilityas important pillars. Nevertheless, further research is required to identifytrustworthiness pillars and evaluation metrics specifically relevant to FLmodels, as well as to develop solutions that can compute the trustworthinesslevel of FL models. This work examines the existing requirements for evaluatingtrustworthiness in FL and introduces a comprehensive taxonomy consisting of sixpillars (privacy, robustness, fairness, explainability, accountability, andfederation), along with over 30 metrics for computing the trustworthiness of FLmodels. Subsequently, an algorithm named FederatedTrust is designed based onthe pillars and metrics identified in the taxonomy to compute thetrustworthiness score of FL models. A prototype of FederatedTrust isimplemented and integrated into the learning process of FederatedScope, awell-established FL framework. Finally, five experiments are conducted usingdifferent configurations of FederatedScope to demonstrate the utility ofFederatedTrust in computing the trustworthiness of FL models. Three experimentsemploy the FEMNIST dataset, and two utilize the N-BaIoT dataset considering areal-world IoT security use case.", "output": "FederatedTrust: A Solution for Trustworthy Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in equivariant graph neural networks (GNNs) have made deeplearning amenable to developing fast surrogate models to expensive ab initioquantum mechanics (QM) approaches for molecular potential predictions. However,building accurate and transferable potential models using GNNs remainschallenging, as the data is greatly limited by the expensive computationalcosts and level of theory of QM methods, especially for large and complexmolecular systems. In this work, we propose denoise pretraining onnonequilibrium molecular conformations to achieve more accurate andtransferable GNN potential predictions. Specifically, atomic coordinates ofsampled nonequilibrium conformations are perturbed by random noises and GNNsare pretrained to denoise the perturbed molecular conformations which recoversthe original coordinates. Rigorous experiments on multiple benchmarks revealthat pretraining significantly improves the accuracy of neural potentials.Furthermore, we show that the proposed pretraining approach is model-agnostic,as it improves the performance of different invariant and equivariant GNNs.Notably, our models pretrained on small molecules demonstrate remarkabletransferability, improving performance when fine-tuned on diverse molecularsystems, including different elements, charged molecules, biomolecules, andlarger systems. These results highlight the potential for leveraging denoisepretraining approaches to build more generalizable neural potentials forcomplex molecular systems.", "output": "Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a huge, online stream of time-evolving events with multiple attributes,such as online shopping logs: (item, price, brand, time), and local mobilityactivities: (pick-up and drop-off locations, time), how can we summarize large,dynamic high-order tensor streams? How can we see any hidden patterns, rules,and anomalies? Our answer is to focus on two types of patterns, i.e.,''regimes'' and ''components'', for which we present CubeScope, an efficientand effective method over high-order tensor streams. Specifically, itidentifies any sudden discontinuity and recognizes distinct dynamical patterns,''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it alsoperforms multi-way summarization for all attributes (e.g., item, price, brand,and time) and discovers hidden ''components'' representing latent groups (e.g.,item/brand groups) and their relationship. Thanks to its concise but effectivesummarization, CubeScope can also detect the sudden appearance of anomalies andidentify the types of anomalies that occur in practice. Our proposed method hasthe following properties: (a) Effective: it captures dynamical multi-aspectpatterns, i.e., regimes and components, and statistically summarizes all theevents; (b) General: it is practical for successful application to datacompression, pattern discovery, and anomaly detection on various types oftensor streams; (c) Scalable: our algorithm does not depend on the length ofthe data stream and its dimensionality. Extensive experiments on real datasetsdemonstrate that CubeScope finds meaningful patterns and anomalies correctly,and consistently outperforms the state-of-the-art methods as regards accuracyand execution speed.", "output": "Fast and Multi-aspect Mining of Complex Time-stamped Event Streams."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This research project investigates Lenia, an artificial life platform thatsimulates ecosystems of digital creatures. Lenia's ecosystem consists ofsimple, artificial organisms that can move, consume, grow, and reproduce. Theplatform is important as a tool for studying artificial life and evolution, asit provides a scalable and flexible environment for creating a diverse range oforganisms with varying abilities and behaviors. Measuring complexity in Leniais a key aspect of the study, which identifies the metrics for measuringlong-term complex emerging behavior of rules, with the aim of evolving betterLenia behaviors which are yet not discovered. The Genetic Algorithm usesneighborhoods or kernels as genotype while keeping the rest of the parametersof Lenia as fixed, for example growth function, to produce different behaviorsrespective to the population and then measures fitness value to decide thecomplexity of the resulting behavior. First, we use Variation over Time as afitness function where higher variance between the frames are rewarded. Second,we use Auto-encoder based fitness where variation of the list of reconstructionloss for the frames is rewarded. Third, we perform combined fitness wherehigher variation of the pixel density of reconstructed frames is rewarded. Allthree experiments are tweaked with pixel alive threshold and frames used.Finally, after performing nine experiments of each fitness for 500 generations,we pick configurations from all experiments such that there is a scope offurther evolution, and run it for 2500 generations. Results show that thekernel's center of mass increases with a specific set of pixels and togetherwith borders the kernel try to achieve a Gaussian distribution.", "output": "Capturing Emerging Complexity in Lenia."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of large language models (LLMs) such as ChatGPT has brought alot of attention recently. However, their evaluation in the benchmark academicdatasets remains under-explored due to the difficulty of evaluating thegenerative outputs produced by this model against the ground truth. In thispaper, we aim to present a thorough evaluation of ChatGPT's performance ondiverse academic datasets, covering tasks like question-answering, textsummarization, code generation, commonsense reasoning, mathematicalproblem-solving, machine translation, bias detection, and ethicalconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze255K responses it generates in these datasets. This makes our work the largestevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validatethe strengths and weaknesses of ChatGPT in various tasks and provide insightsfor future research using LLMs. We also report a new emergent ability to followmulti-query instructions that we mostly found in ChatGPT and otherinstruction-tuned models. Our extensive evaluation shows that even thoughChatGPT is capable of performing a wide variety of tasks, and may obtainimpressive performance in several benchmark datasets, it is still far fromachieving the ability to reliably solve many challenging tasks. By providing athorough assessment of ChatGPT's performance across diverse NLP tasks, thispaper sets the stage for a targeted deployment of ChatGPT-like LLMs inreal-world applications.", "output": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robots are notoriously difficult to design because of complexinterdependencies between their physical structure, sensory and motor layouts,and behavior. Despite this, almost every detail of every robot built to datehas been manually determined by a human designer after several months or yearsof iterative ideation, prototyping, and testing. Inspired by evolutionarydesign in nature, the automated design of robots using evolutionary algorithmshas been attempted for two decades, but it too remains inefficient: days ofsupercomputing are required to design robots in simulation that, whenmanufactured, exhibit desired behavior. Here we show for the first time de-novooptimization of a robot's structure to exhibit a desired behavior, withinseconds on a single consumer-grade computer, and the manufactured robot'sretention of that behavior. Unlike other gradient-based robot design methods,this algorithm does not presuppose any particular anatomical form; startinginstead from a randomly-generated apodous body plan, it consistently discoverslegged locomotion, the most efficient known form of terrestrial movement. Ifcombined with automated fabrication and scaled up to more challenging tasks,this advance promises near instantaneous design, manufacture, and deployment ofunique and useful machines for medical, environmental, vehicular, andspace-based tasks.", "output": "Efficient automatic design of robots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline reinforcement learning (RL) offers an appealing approach toreal-world tasks by learning policies from pre-collected datasets withoutinteracting with the environment. However, the performance of existing offlineRL algorithms heavily depends on the scale and state-action space coverage ofdatasets. Real-world data collection is often expensive and uncontrollable,leading to small and narrowly covered datasets and posing significantchallenges for practical deployments of offline RL. In this paper, we provide anew insight that leveraging the fundamental symmetry of system dynamics cansubstantially enhance offline RL performance under small datasets.Specifically, we propose a Time-reversal symmetry (T-symmetry) enforcedDynamics Model (TDM), which establishes consistency between a pair of forwardand reverse latent dynamics. TDM provides both well-behaved representations forsmall datasets and a new reliability measure for OOD samples based oncompliance with the T-symmetry. These can be readily used to construct a newoffline RL algorithm (TSRL) with less conservative policy constraints and areliable latent space data augmentation procedure. Based on extensiveexperiments, we find TSRL achieves great performance on small benchmarkdatasets with as few as 1% of the original samples, which significantlyoutperforms the recent offline RL algorithms in terms of data efficiency andgeneralizability.", "output": "Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural sequence models based on the transformer architecture havedemonstrated remarkable emph{in-context learning} (ICL) abilities, where theycan perform new tasks when prompted with training and test examples, withoutany parameter update to the model. This work first provides a comprehensivestatistical theory for transformers to perform ICL. Concretely, we show thattransformers can implement a broad class of standard machine learningalgorithms in context, such as least squares, ridge regression, Lasso, learninggeneralized linear models, and gradient descent on two-layer neural networks,with near-optimal predictive power on various in-context data distributions.Using an efficient implementation of in-context gradient descent as theunderlying mechanism, our transformer constructions admit mild size bounds, andcan be learned with polynomially many pretraining sequences.Building on these ``base'' ICL algorithms, intriguingly, we show thattransformers can implement more complex ICL procedures involvingemph{in-context algorithm selection}, akin to what a statistician can do inreal life -- A emph{single} transformer can adaptively select different baseICL algorithms -- or even perform qualitatively different tasks -- on differentinput sequences, without any explicit prompting of the right algorithm or task.We both establish this in theory by explicit constructions, and also observethis phenomenon experimentally. In theory, we construct two general mechanismsfor algorithm selection with concrete examples: pre-ICL testing, and post-ICLvalidation. As an example, we use the post-ICL validation mechanism toconstruct a transformer that can perform nearly Bayes-optimal ICL on achallenging task -- noisy linear models with mixed noise levels.Experimentally, we demonstrate the strong in-context algorithm selectioncapabilities of standard transformer architectures.", "output": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Coding problems are problems that require a solution in the form of acomputer program. Coding problems are popular among students and professionalsas it enhances their skills and career opportunities. An AI system that wouldhelp those who practice coding problems would be highly useful and there is ahuge potential for such a system. In this work, we propose a model which usesstacking of hyperparameter tuned boosting models to achieve impressive metricscores of 77.8% accuracy and 0.815 PR-AUC on the dataset that was scraped fromCodeforces and Leetcode. We open source the dataset and the models developedfor this work.", "output": "Stacking of Hyperparameter Tuned Models for Tagging Coding Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) has made significant strides in various complexdomains. However, identifying an effective policy via RL often necessitatesextensive exploration. Imitation learning aims to mitigate this issue by usingexpert demonstrations to guide exploration. In real-world scenarios, one oftenhas access to multiple suboptimal black-box experts, rather than a singleoptimal oracle. These experts do not universally outperform each other acrossall states, presenting a challenge in actively deciding which oracle to use andin which state. We introduce MAPS and MAPS-SE, a class of policy improvementalgorithms that perform imitation learning from multiple suboptimal oracles. Inparticular, MAPS actively selects which of the oracles to imitate and improvetheir value function estimates, and MAPS-SE additionally leverages an activestate exploration criterion to determine which states one should explore. Weprovide a comprehensive theoretical analysis and demonstrate that MAPS andMAPS-SE enjoy sample efficiency advantage over the state-of-the-art policyimprovement algorithms. Empirical results show that MAPS-SE significantlyaccelerates policy optimization via state-wise imitation learning from multipleoracles across a broad spectrum of control tasks in the DeepMind Control Suite.Our code is publicly available at: ", "output": "Active Policy Improvement from Multiple Black-box Oracles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Entity-level fine-grained sentiment analysis in the financial domain is acrucial subtask of sentiment analysis and currently faces numerous challenges.The primary challenge stems from the lack of high-quality and large-scaleannotated corpora specifically designed for financial text sentiment analysis,which in turn limits the availability of data necessary for developingeffective text processing techniques. Recent advancements in large languagemodels (LLMs) have yielded remarkable performance in natural languageprocessing tasks, primarily centered around language pattern matching. In thispaper, we propose a novel and extensive Chinese fine-grained financialsentiment analysis dataset, FinChina SA, for enterprise early warning. Wethoroughly evaluate and experiment with well-known existing open-source LLMsusing our dataset. We firmly believe that our dataset will serve as a valuableresource to advance the exploration of real-world financial sentiment analysistasks, which should be the focus of future research. Our dataset and all codeto replicate the experimental results will be released.", "output": "Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a novel approach to address the challenges ofprinted Urdu text recognition using high-resolution, multi-scale semanticfeature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model,demonstrates state-of-the-art performance on benchmark datasets. To address thelimitations of previous works, which struggle to generalize to the intricaciesof the Urdu script and the lack of sufficient annotated real-world data, wehave introduced the UTRSet-Real, a large-scale annotated real-world datasetcomprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000lines closely resembling real-world and made corrections to the ground truth ofthe existing IIITH dataset, making it a more reliable resource for futureresearch. We also provide UrduDoc, a benchmark dataset for Urdu text linedetection in scanned documents. Additionally, we have developed an online toolfor end-to-end Urdu OCR from printed documents by integrating UTRNet with atext detection model. Our work not only addresses the current limitations ofUrdu OCR but also paves the way for future research in this area andfacilitates the continued advancement of Urdu OCR technology. The project pagewith source code, datasets, annotations, trained models, and online tool isavailable at abdur75648.github.io/UTRNet.", "output": "UTRNet: High-Resolution Urdu Text Recognition In Printed Documents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of visual scene understanding, deep neural networks have madeimpressive advancements in various core tasks like segmentation, tracking, anddetection. However, most approaches operate on the close-set assumption,meaning that the model can only identify pre-defined categories that arepresent in the training set. Recently, open vocabulary settings were proposeddue to the rapid progress of vision language pre-training. These new approachesseek to locate and recognize categories beyond the annotated label space. Theopen vocabulary approach is more general, practical, and effective compared toweakly supervised and zero-shot settings. This paper provides a thorough reviewof open vocabulary learning, summarizing and analyzing recent developments inthe field. In particular, we begin by comparing it to related concepts such aszero-shot learning, open-set recognition, and out-of-distribution detection.Then, we review several closely related tasks in the case of segmentation anddetection, including long-tail problems, few-shot, and zero-shot settings. Forthe method survey, we first present the basic knowledge of detection andsegmentation in close-set as the preliminary knowledge. Next, we examinevarious scenarios in which open vocabulary learning is used, identifying commondesign elements and core ideas. Then, we compare the recent detection andsegmentation approaches in commonly used datasets and benchmarks. Finally, weconclude with insights, issues, and discussions regarding future researchdirections. To our knowledge, this is the first comprehensive literature reviewof open vocabulary learning. We keep tracing related works at", "output": "Towards Open Vocabulary Learning: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperbole, or exaggeration, is a common linguistic phenomenon. The detectionof hyperbole is an important part of understanding human expression. There havebeen several studies on hyperbole detection, but most of which focus on textmodality only. However, with the development of social media, people can createhyperbolic expressions with various modalities, including text, images, videos,etc. In this paper, we focus on multimodal hyperbole detection. We create amultimodal detection datasetfootnote{The dataset will be released to thecommunity.} from Weibo (a Chinese social media) and carry out some studies onit. We treat the text and image from a piece of weibo as two modalities andexplore the role of text and image for hyperbole detection. Differentpre-trained multimodal encoders are also evaluated on this downstream task toshow their performance. Besides, since this dataset is constructed from fivedifferent topics, we also evaluate the cross-domain performance of differentmodels. These studies can serve as a benchmark and point out the direction offurther study on multimodal hyperbole detection.", "output": "Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a limited labeling budget, active learning (AL) aims to sample the mostinformative instances from an unlabeled pool to acquire labels for subsequentmodel training. To achieve this, AL typically measures the informativeness ofunlabeled instances based on uncertainty and diversity. However, it does notconsider erroneous instances with their neighborhood error density, which havegreat potential to improve the model performance. To address this limitation,we propose $REAL$, a novel approach to select data instances with$underline{R}$epresentative $underline{E}$rrors for $underline{A}$ctive$underline{L}$earning. It identifies minority predictions as emph{pseudoerrors} within a cluster and allocates an adaptive sampling budget for thecluster based on estimated error density. Extensive experiments on five textclassification datasets demonstrate that $REAL$ consistently outperforms allbest-performing baselines regarding accuracy and F1-macro scores across a widerange of hyperparameter settings. Our analysis also shows that $REAL$ selectsthe most representative pseudo errors that match the distribution ofground-truth errors along the decision boundary. Our code is publicly availableat ", "output": "REAL: A Representative Error-Driven Approach for Active Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this survey, we aim to explore the fundamental question of whether thenext generation of artificial intelligence requires quantum computing.Artificial intelligence is increasingly playing a crucial role in many aspectsof our daily lives and is central to the fourth industrial revolution. It istherefore imperative that artificial intelligence is reliable and trustworthy.However, there are still many issues with reliability of artificialintelligence, such as privacy, responsibility, safety, and security, in areassuch as autonomous driving, healthcare, robotics, and others. These problemscan have various causes, including insufficient data, biases, and robustnessproblems, as well as fundamental issues such as computability problems ondigital hardware. The cause of these computability problems is rooted in thefact that digital hardware is based on the computing model of the Turingmachine, which is inherently discrete. Notably, our findings demonstrate thatdigital hardware is inherently constrained in solving problems aboutoptimization, deep learning, or differential equations. Therefore, theselimitations carry substantial implications for the field of artificialintelligence, in particular for machine learning. Furthermore, although it iswell known that the quantum computer shows a quantum advantage for certainclasses of problems, our findings establish that some of these limitationspersist when employing quantum computing models based on the quantum circuit orthe quantum Turing machine paradigm. In contrast, analog computing models, suchas the Blum-Shub-Smale machine, exhibit the potential to surmount theselimitations.", "output": "Reliable AI: Does the Next Generation Require Quantum Computing?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current natural language systems designed for multi-step claim validationtypically operate in two phases: retrieve a set of relevant premise statementsusing heuristics (planning), then generate novel conclusions from thosestatements using a large language model (deduction). The planning step oftenrequires expensive Transformer operations and does not scale to arbitrarynumbers of premise statements. In this paper, we investigate whether anefficient planning heuristic is possible via embedding spaces compatible withdeductive reasoning. Specifically, we evaluate whether embedding spaces exhibita property we call deductive additivity: the sum of premise statementembeddings should be close to embeddings of conclusions based on thosepremises. We explore multiple sources of off-the-shelf dense embeddings inaddition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. Westudy embedding models both intrinsically, evaluating whether the property ofdeductive additivity holds, and extrinsically, using them to assist planning innatural language proof generation. Lastly, we create a dataset, Single-StepReasoning Contrast (SSRC), to further probe performance on various reasoningtypes. Our findings suggest that while standard embedding methods frequentlyembed conclusions near the sums of their premises, they fall short of beingeffective heuristics and lack the ability to model certain categories ofreasoning.", "output": "Deductive Additivity for Planning of Natural Language Proofs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Equipping embodied agents with commonsense is important for robots tosuccessfully complete complex human instructions in general environments.Recent large language models (LLM) can embed rich semantic knowledge for agentsin plan generation of complex tasks, while they lack the information about therealistic world and usually yield infeasible action sequences. In this paper,we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planningwith physical scene constraint, where the agent generates executable plansaccording to the existed objects in the scene by aligning LLMs with the visualperception models. Specifically, we first construct a multimodal datasetcontaining triplets of indoor scenes, instructions and action plans, where weprovide the designed prompts and the list of existing objects in the scene forGPT-3.5 to generate a large number of instructions and corresponding plannedactions. The generated data is leveraged for grounded plan tuning ofpre-trained LLMs. During inference, we discover the objects in the scene byextending open-vocabulary object detectors to multi-view RGB images collectedin different achievable locations. Experimental results show that the generatedplan from our TaPA framework can achieve higher success rate than LLaVA andGPT-3.5 by a sizable margin, which indicates the practicality of embodied taskplanning in general and complex environments.", "output": "Embodied Task Planning with Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Superimposed text annotations have been under-investigated, yet areubiquitous, useful and important, especially in medical images. Medical imagesalso highlight the challenges posed by low resolution, noise and superimposedtextual meta-information. Therefor we probed the impact of superimposing textonto medical images on VQA. Our results revealed that this textualmeta-information can be added without severely degrading key measures of VQAperformance. Our findings are significant because they validate the practice ofsuperimposing text on images, even for medical images subjected to the VQA taskusing AI techniques. The work helps advance understanding of VQA in generaland, in particular, in the domain of healthcare and medicine.", "output": "Visual Question Answering (VQA) on Images with Superimposed Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study several methods for detecting anomalies in color images, constructedon patch-based auto-encoders. Wecompare the performance of three types ofmethods based, first, on the error between the original image and itsreconstruction,second, on the support estimation of the normal imagedistribution in the latent space, and third, on the error between theoriginalimage and a restored version of the reconstructed image. These methodsare evaluated on the industrial image database MVTecADand compared to twocompetitive state-of-the-art methods.", "output": "Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electrolysis is crucial for eco-friendly hydrogen production, but gas bubblesgenerated during the process hinder reactions, reduce cell efficiency, andincrease energy consumption. Additionally, these gas bubbles cause changes inthe conductivity inside the cell, resulting in corresponding variations in theinduced magnetic field around the cell. Therefore, measuring these gasbubble-induced magnetic field fluctuations using external magnetic sensors andsolving the inverse problem of Biot-Savart Law allows for estimating theconductivity in the cell and, thus, bubble size and location. However,determining high-resolution conductivity maps from only a few induced magneticfield measurements is an ill-posed inverse problem. To overcome this, weexploit Invertible Neural Networks (INNs) to reconstruct the conductivityfield. Our qualitative results and quantitative evaluation using random errordiffusion show that INN achieves far superior performance compared to Tikhonovregularization.", "output": "Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the perpetual increase of complexity of the state-of-the-art deep neuralnetworks, it becomes a more and more challenging task to maintain theirinterpretability. Our work aims to evaluate the effects of adversarial trainingutilized to produce robust models - less vulnerable to adversarial attacks. Ithas been shown to make computer vision models more interpretable.Interpretability is as essential as robustness when we deploy the models to thereal world. To prove the correlation between these two problems, we extensivelyexamine the models using local feature-importance methods (SHAP, IntegratedGradients) and feature visualization techniques (Representation Inversion,Class Specific Image Generation). Standard models, compared to robust are moresusceptible to adversarial attacks, and their learned representations are lessmeaningful to humans. Conversely, these models focus on distinctive regions ofthe images which support their predictions. Moreover, the features learned bythe robust model are closer to the real ones.", "output": "Interpretable Computer Vision Models through Adversarial Training: Unveiling the Robustness-Interpretability Connection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objecttracking and segmentation. In this study, we convert the bounding boxes tomasks in reference frames with the help of the Segment Anything Model (SAM) andAlpha-Refine, and then propagate the masks to the current frame, transformingthe task from Video Object Tracking (VOT) to video object segmentation (VOS).Furthermore, we introduce MSDeAOT, a variant of the AOT series thatincorporates transformers at multiple feature scales. MSDeAOT efficientlypropagates object masks from previous frames to the current frame using twofeature scales of 16 and 8. As a testament to the effectiveness of our design,we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object TrackingChallenge.", "output": "ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a computational framework for the Wassersteinauto-encoding of merge trees (MT-WAE), a novel extension of the classicalauto-encoder neural network architecture to the Wasserstein metric space ofmerge trees. In contrast to traditional auto-encoders which operate onvectorized data, our formulation explicitly manipulates merge trees on theirassociated metric space at each layer of the network, resulting in superioraccuracy and interpretability. Our novel neural network approach can beinterpreted as a non-linear generalization of previous linear attempts [65] atmerge tree encoding. It also trivially extends to persistence diagrams.Extensive experiments on public ensembles demonstrate the efficiency of ouralgorithms, with MT-WAE computations in the orders of minutes on average. Weshow the utility of our contributions in two applications adapted from previouswork on merge tree encoding [65]. First, we apply MT-WAE to data reduction andreliably compress merge trees by concisely representing them with theircoordinates in the final layer of our auto-encoder. Second, we document anapplication to dimensionality reduction, by exploiting the latent space of ourauto-encoder, for the visual analysis of ensemble data. We illustrate theversatility of our framework by introducing two penalty terms, to help preservein the latent space both the Wasserstein distances between merge trees, as wellas their clusters. In both applications, quantitative experiments assess therelevance of our framework. Finally, we provide a C++ implementation that canbe used for reproducibility.", "output": "Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Independently trained machine learning models tend to learn similar features.Given an ensemble of independently trained models, this results in correlatedpredictions and common failure modes. Previous attempts focusing ondecorrelation of output predictions or logits yielded mixed results,particularly due to their reduction in model accuracy caused by conflictingoptimization objectives. In this paper, we propose the novel idea of utilizingmethods of the representational similarity field to promote dissimilarityduring training instead of measuring similarity of trained models. To this end,we promote intermediate representations to be dissimilar at different depthsbetween architectures, with the goal of learning robust ensembles with disjointfailure modes. We show that highly dissimilar intermediate representationsresult in less correlated output predictions and slightly lower errorconsistency, resulting in higher ensemble accuracy. With this, we shine firstlight on the connection between intermediate representations and their impacton the output predictions.", "output": "Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate building height estimation is key to the automatic derivation of 3Dcity models from emerging big geospatial data, including VolunteeredGeographical Information (VGI). However, an automatic solution for large-scalebuilding height estimation based on low-cost VGI data is currently missing. Thefast development of VGI data platforms, especially OpenStreetMap (OSM) andcrowdsourced street-view images (SVI), offers a stimulating opportunity to fillthis research gap. In this work, we propose a semi-supervised learning (SSL)method of automatically estimating building height from Mapillary SVI and OSMdata to generate low-cost and open-source 3D city modeling in LoD1. Theproposed method consists of three parts: first, we propose an SSL schema withthe option of setting a different ratio of \"pseudo label\" during the supervisedregression; second, we extract multi-level morphometric features from OSM data(i.e., buildings and streets) for the purposed of inferring building height;last, we design a building floor estimation workflow with a pre-trained facadeobject detection network to generate \"pseudo label\" from SVI and assign it tothe corresponding OSM building footprint. In a case study, we validate theproposed SSL method in the city of Heidelberg, Germany and evaluate the modelperformance against the reference data of building heights. Based on threedifferent regression models, namely Random Forest (RF), Support Vector Machine(SVM), and Convolutional Neural Network (CNN), the SSL method leads to a clearperformance boosting in estimating building heights with a Mean Absolute Error(MAE) around 2.1 meters, which is competitive to state-of-the-art approaches.The preliminary result is promising and motivates our future work in scaling upthe proposed method based on low-cost VGI data, with possibilities in evenregions and areas with diverse data quality and availability.", "output": "Semi-supervised Learning from Street-View Images and OpenStreetMap for Automatic Building Height Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper argues for the introduction of a mainline rail-orientedperformance metric for driver-replacing on-board perception systems. Perceptionat the head of a train is divided into several subfunctions. This articlepresents a preliminary submetric for the obstacle detection subfunction. To thebest of the author's knowledge, no other such proposal for obstacle detectionexists. A set of submetrics for the subfunctions should facilitate thecomparison of perception systems among each other and guide the measurement ofhuman driver performance. It should also be useful for a standardizedprediction of the number of accidents for a given perception system in a givenoperational design domain. In particular, for the proposal of the obstacledetection submetric, the professional readership is invited to provide theirfeedback and quantitative information to the author. The analysis results ofthe feedback will be published separately later.", "output": "Mainline Automatic Train Horn and Brake Performance Metric."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores the application of event-based cameras in the domains ofimage segmentation and motion estimation. These cameras offer a groundbreakingtechnology by capturing visual information as a continuous stream ofasynchronous events, departing from the conventional frame-based imageacquisition. We introduce a Generalized Nash Equilibrium based framework thatleverages the temporal and spatial information derived from the event stream tocarry out segmentation and velocity estimation. To establish the theoreticalfoundations, we derive an existence criteria and propose a multi-leveloptimization method for calculating equilibrium. The efficacy of this approachis shown through a series of experiments.", "output": "GNEP Based Dynamic Segmentation and Motion Estimation for Neuromorphic Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Verbal and non-verbal human reaction generation is a challenging task, asdifferent reactions could be appropriate for responding to the same behaviour.This paper proposes the first multiple and multimodal (verbal and nonverbal)appropriate human reaction generation framework that can generate appropriateand realistic human-style reactions (displayed in the form of synchronisedtext, audio and video streams) in response to an input user behaviour. Thisnovel technique can be applied to various human-computer interaction scenariosby generating appropriate virtual agent/robot behaviours. Our demo is availableat url{", "output": "MRecGen: Multimodal Appropriate Reaction Generator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Images captured in poorly lit conditions are often corrupted by acquisitionnoise. Leveraging recent advances in graph-based regularization, we propose afast Retinex-based restoration scheme that denoises and contrast-enhances animage. Specifically, by Retinex theory we first assume that each image pixel isa multiplication of its reflectance and illumination components. We next assumethat the reflectance and illumination components are piecewise constant (PWC)and continuous piecewise planar (PWP) signals, which can be recovered via graphLaplacian regularizer (GLR) and gradient graph Laplacian regularizer (GGLR)respectively. We formulate quadratic objectives regularized by GLR and GGLR,which are minimized alternately until convergence by solving linear systems --with improved condition numbers via proposed preconditioners -- via conjugategradient (CG) efficiently. Experimental results show that our algorithmachieves competitive visual image quality while reducing computation complexitynoticeably.", "output": "Retinex-based Image Denoising / Contrast Enhancement using Gradient Graph Laplacian Regularizer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For real-world applications, robots will need to continually learn in theirenvironments through limited interactions with their users. Toward this,previous works in few-shot class incremental learning (FSCIL) and active classselection (ACS) have achieved promising results but were tested in constrainedsetups. Therefore, in this paper, we combine ideas from FSCIL and ACS todevelop a novel framework that can allow an autonomous agent to continuallylearn new objects by asking its users to label only a few of the mostinformative objects in the environment. To this end, we build on astate-of-the-art (SOTA) FSCIL model and extend it with techniques from ACSliterature. We term this model Few-shot Incremental Active class SeleCtiOn(FIASco). We further integrate a potential field-based navigation techniquewith our model to develop a complete framework that can allow an agent toprocess and reason on its sensory data through the FIASco model, navigatetowards the most informative object in the environment, gather data about theobject through its sensors and incrementally update the FIASco model.Experimental results on a simulated agent and a real robot show thesignificance of our approach for long-term real-world robotics applications.", "output": "Active Class Selection for Few-Shot Class-Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation for spherical data is a challenging problem in machinelearning since conventional planar approaches require projecting the sphericalimage to the Euclidean plane. Representing the signal on a fundamentallydifferent topology introduces edges and distortions which impact networkperformance. Recently, graph-based approaches have bypassed these challenges toattain significant improvements by representing the signal on a spherical mesh.Current approaches to spherical segmentation exclusively use variants of theUNet architecture, meaning more successful planar architectures remainunexplored. Inspired by the success of feature pyramid networks (FPNs) inplanar image segmentation, we leverage the pyramidal hierarchy of graph-basedspherical CNNs to design spherical FPNs. Our spherical FPN models showconsistent improvements over spherical UNets, whilst using fewer parameters. Onthe Stanford 2D-3D-S dataset, our models achieve state-of-the-art performancewith an mIOU of 48.75, an improvement of 3.75 IoU points over the previous bestspherical CNN.", "output": "Spherical Feature Pyramid Networks For Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks tend to make overconfident predictions and often requireadditional detectors for misclassifications, particularly for safety-criticalapplications. Existing detection methods usually only focus on adversarialattacks or out-of-distribution samples as reasons for false predictions.However, generalization errors occur due to diverse reasons often related topoorly learning relevant invariances. We therefore propose GIT, a holisticapproach for the detection of generalization errors that combines the usage ofgradient information and invariance transformations. The invariancetransformations are designed to shift misclassified samples back into thegeneralization area of the neural network, while the gradient informationmeasures the contradiction between the initial prediction and the correspondinginherent computations of the neural network using the transformed sample. Ourexperiments demonstrate the superior performance of GIT compared to thestate-of-the-art on a variety of network architectures, problem setups andperturbation types.", "output": "GIT: Detecting Uncertainty, Out-Of-Distribution and Adversarial Samples using Gradients and Invariance Transformations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has received increasing interests in face recognition recently.Large quantities of deep learning methods have been proposed to handle variousproblems appeared in face recognition. Quite a lot deep methods claimed thatthey have gained or even surpassed human-level face verification performance incertain databases. As we know, face image quality poses a great challenge totraditional face recognition methods, e.g. model-driven methods withhand-crafted features. However, a little research focus on the impact of faceimage quality on deep learning methods, and even human performance. Therefore,we raise a question: Is face image quality still one of the challenges for deeplearning based face recognition, especially in unconstrained condition. Basedon this, we further investigate this problem on human level. In this paper, wepartition face images into three different quality sets to evaluate theperformance of deep learning methods on cross-quality face images in the wild,and then design a human face verification experiment on these cross-qualitydata. The result indicates that quality issue still needs to be studiedthoroughly in deep learning, human own better capability in building therelations between different face images with large quality gaps, and sayingdeep learning method surpasses human-level is too optimistic.", "output": "A Study on the Impact of Face Image Quality on Face Recognition in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dense video captioning, a task of localizing meaningful moments andgenerating relevant captions for videos, often requires a large, expensivecorpus of annotated video segments paired with text. In an effort to minimizethe annotation cost, we propose ZeroTA, a novel method for dense videocaptioning in a zero-shot manner. Our method does not require any videos orannotations for training; instead, it localizes and describes events withineach input video at test time by optimizing solely on the input. This isaccomplished by introducing a soft moment mask that represents a temporalsegment in the video and jointly optimizing it with the prefix parameters of alanguage model. This joint optimization aligns a frozen language generationmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,CLIP) by maximizing the matching score between the generated text and a momentwithin the video. We also introduce a pairwise temporal IoU loss to let a setof soft moment masks capture multiple distinct events within the video. Ourmethod effectively discovers diverse significant events within the video, withthe resulting captions appropriately describing these events. The empiricalresults demonstrate that ZeroTA surpasses zero-shot baselines and evenoutperforms the state-of-the-art few-shot method on the widely-used benchmarkActivityNet Captions. Moreover, our method shows greater robustness compared tosupervised methods when evaluated in out-of-domain scenarios. This researchprovides insight into the potential of aligning widely-used models, such aslanguage generation models and vision-language models, to unlock a newcapability: understanding temporal aspects of videos.", "output": "Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the essential components of deep learning is the choice of the lossfunction and performance metrics used to train and evaluate models. This paperreviews the most prevalent loss functions and performance measurements in deeplearning. We examine the benefits and limits of each technique and illustratetheir application to various deep-learning problems. Our review aims to give acomprehensive picture of the different loss functions and performanceindicators used in the most common deep learning tasks and help practitionerschoose the best method for their specific task.", "output": "Loss Functions and Metrics in Deep Learning. A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We demonstrate two novel editing procedures in the context of fantasy cardart. Palette transfer applies a specified reference palette to a given card.For fantasy art, the desired change in palette can be very large, leading tohuge changes in the \"look\" of the art. We demonstrate that a pipeline of vectorquantization; matching; and \"vector dequantization\" (using a diffusion model)produces successful extreme palette transfers. Segment control allows an artistto move one or more image segments, and to optionally specify the desired colorof the result. The combination of these two types of edit yields valuableworkflows, including: move a segment, then recolor; recolor, then force somesegments to take a prescribed color. We demonstrate our methods on thechallenging Yu-Gi-Oh card art dataset.", "output": "Applying a Color Palette with Local Control using Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal summarization usually suffers from the problem that thecontribution of the visual modality is unclear. Existing multimodalsummarization approaches focus on designing the fusion methods of differentmodalities, while ignoring the adaptive conditions under which visualmodalities are useful. Therefore, we propose a novel Coarse-to-Finecontribution network for multimodal Summarization (CFSum) to consider differentcontributions of images for summarization. First, to eliminate the interferenceof useless images, we propose a pre-filter module to abandon useless images.Second, to make accurate use of useful images, we propose two levels of visualcomplement modules, word level and phrase level. Specifically, imagecontributions are calculated and are adopted to guide the attention of bothtextual and visual modalities. Experimental results have shown that CFSumsignificantly outperforms multiple strong baselines on the standard benchmark.Furthermore, the analysis verifies that useful images can even help generatenon-visual words which are implicitly represented in the image.", "output": "CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The fine-grained action analysis of the existing action datasets ischallenged by insufficient action categories, low fine granularities, limitedmodalities, and tasks. In this paper, we propose a Multi-modality andMulti-task dataset of Figure Skating (MMFS) which was collected from the WorldFigure Skating Championships. MMFS, which possesses action recognition andaction quality assessment, captures RGB, skeleton, and is collected the scoreof actions from 11671 clips with 256 categories including spatial and temporallabels. The key contributions of our dataset fall into three aspects asfollows. (1) Independently spatial and temporal categories are first proposedto further explore fine-grained action recognition and quality assessment. (2)MMFS first introduces the skeleton modality for complex fine-grained actionquality assessment. (3) Our multi-modality and multi-task dataset encouragemore action analysis models. To benchmark our dataset, we adopt RGB-based andskeleton-based baseline methods for action recognition and action qualityassessment.", "output": "Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advanced manipulation techniques have provided criminals with opportunitiesto make social panic or gain illicit profits through the generation ofdeceptive media, such as forged face images. In response, various deepfakedetection methods have been proposed to assess image authenticity. Sequentialdeepfake detection, which is an extension of deepfake detection, aims toidentify forged facial regions with the correct sequence for recovery.Nonetheless, due to the different combinations of spatial and sequentialmanipulations, forged face images exhibit substantial discrepancies thatseverely impact detection performance. Additionally, the recovery of forgedimages requires knowledge of the manipulation model to implement inversetransformations, which is difficult to ascertain as relevant techniques areoften concealed by attackers. To address these issues, we proposeMulti-Collaboration and Multi-Supervision Network (MMNet) that handles variousspatial scales and sequential permutations in forged face images and achieverecovery without requiring knowledge of the corresponding manipulation method.Furthermore, existing evaluation metrics only consider detection accuracy at asingle inferring step, without accounting for the matching degree withground-truth under continuous multiple steps. To overcome this limitation, wepropose a novel evaluation metric called Complete Sequence Matching (CSM),which considers the detection accuracy at multiple inferring steps, reflectingthe ability to detect integrally forged sequences. Extensive experiments onseveral typical datasets demonstrate that MMNet achieves state-of-the-artdetection performance and independent recovery performance.", "output": "MMNet: Multi-Collaboration and Multi-Supervision Network for Sequential Deepfake Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Objective: Quantitative $T_1rho$ imaging has potential for assessment ofbiochemical alterations of liver pathologies. Deep learning methods have beenemployed to accelerate quantitative $T_1rho$ imaging. To employ artificialintelligence-based quantitative imaging methods in complicated clinicalenvironment, it is valuable to estimate the uncertainty of the predicated$T_1rho$ values to provide the confidence level of the quantification results.The uncertainty should also be utilized to aid the post-hoc quantitativeanalysis and model learning tasks. Approach: To address this need, we propose aparametric map refinement approach for learning-based $T_1rho$ mapping andtrain the model in a probabilistic way to model the uncertainty. We alsopropose to utilize the uncertainty map to spatially weight the training of animproved $T_1rho$ mapping network to further improve the mapping performanceand to remove pixels with unreliable $T_1rho$ values in the region ofinterest. The framework was tested on a dataset of 51 patients with differentliver fibrosis stages. Main results: Our results indicate that thelearning-based map refinement method leads to a relative mapping error of lessthan 3% and provides uncertainty estimation simultaneously. The estimateduncertainty reflects the actual error level, and it can be used to furtherreduce relative $T_1rho$ mapping error to 2.60% as well as removing unreliablepixels in the region of interest effectively. Significance: Our studiesdemonstrate the proposed approach has potential to provide a learning-basedquantitative MRI system for trustworthy $T_1rho$ mapping of the liver.", "output": "An Uncertainty Aided Framework for Learning based Liver $T_1\\rho$ Mapping and Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has played a significant role in the success of facialexpression recognition (FER), thanks to large models and vast amounts oflabelled data. However, obtaining labelled data requires a tremendous amount ofhuman effort, time, and financial resources. Even though some prior works havefocused on reducing the need for large amounts of labelled data using differentunsupervised methods, another promising approach called active learning isbarely explored in the context of FER. This approach involves selecting andlabelling the most representative samples from an unlabelled set to make thebest use of a limited 'labelling budget'. In this paper, we implement and study8 recent active learning methods on three public FER datasets, FER13, RAF-DB,and KDEF. Our findings show that existing active learning methods do notperform well in the context of FER, likely suffering from a phenomenon called'Cold Start', which occurs when the initial set of labelled samples is not wellrepresentative of the entire dataset. To address this issue, we proposecontrastive self-supervised pre-training, which first learns the underlyingrepresentations based on the entire unlabelled dataset. We then follow thiswith the active learning methods and observe that our 2-step approach shows upto 9.2% improvement over random sampling and up to 6.7% improvement over thebest existing active learning baseline without the pre-training. We will makethe code for this study public upon publication at:github.com/ShuvenduRoy/ActiveFER.", "output": "Active Learning with Contrastive Pre-training for Facial Expression Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-Camera Multi-Target Tracking (MCMT) is a computer vision technique thatinvolves tracking multiple targets simultaneously across multiple cameras. MCMTin urban traffic visual analysis faces great challenges due to the complex anddynamic nature of urban traffic scenes, where multiple cameras with differentviews and perspectives are often used to cover a large city-scale area. Targetsin urban traffic scenes often undergo occlusion, illumination changes, andperspective changes, making it difficult to associate targets across differentcameras accurately. To overcome these challenges, we propose a novel systematicMCMT framework, called CityTrack. Specifically, we present a Location-AwareSCMT tracker which integrates various advanced techniques to improve itseffectiveness in the MCMT task and propose a novel Box-Grained Matching (BGM)method for the ICA module to solve the aforementioned problems. We evaluatedour approach on the public test set of the CityFlowV2 dataset and achieved anIDF1 of 84.91%, ranking 1st in the 2022 AI CITY CHALLENGE. Our experimentalresults demonstrate the effectiveness of our approach in overcoming thechallenges posed by urban traffic scenes.", "output": "CityTrack: Improving City-Scale Multi-Camera Multi-Target Tracking by Location-Aware Tracking and Box-Grained Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have recently shown remarkable success in high-quality imagegeneration. Sometimes, however, a pre-trained diffusion model exhibits partialmisalignment in the sense that the model can generate good images, but itsometimes outputs undesirable images. If so, we simply need to prevent thegeneration of the bad images, and we call this task censoring. In this work, wepresent censored generation with a pre-trained diffusion model using a rewardmodel trained on minimal human feedback. We show that censoring can beaccomplished with extreme human feedback efficiency and that labels generatedwith a mere few minutes of human feedback are sufficient. Code available at:", "output": "Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a sentiment-enriched lightweight network SeLiNetand an end-to-end on-device pipeline for contextual emotion recognition inimages. SeLiNet model consists of body feature extractor, image aestheticsfeature extractor, and learning-based fusion network which jointly estimatesdiscrete emotion and human sentiments tasks. On the EMOTIC dataset, theproposed approach achieves an Average Precision (AP) score of 27.17 incomparison to the baseline AP score of 27.38 while reducing the model size by&gt;85%. In addition, we report an on-device AP score of 26.42 with reduction inmodel size by &gt;93% when compared to the baseline.", "output": "SeLiNet: Sentiment enriched Lightweight Network for Emotion Recognition in Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, artificial intelligence has played an important role inmedicine and disease diagnosis, with many applications to be mentioned, one ofwhich is Medical Visual Question Answering (MedVQA). By combining computervision and natural language processing, MedVQA systems can assist experts inextracting relevant information from medical image based on a given questionand providing precise diagnostic answers. The ImageCLEFmed-MEDVQA-GI-2023challenge carried out visual question answering task in the gastrointestinaldomain, which includes gastroscopy and colonoscopy images. Our team approachedTask 1 of the challenge by proposing a multimodal learning method with imageenhancement to improve the VQA performance on gastrointestinal images. Themultimodal architecture is set up with BERT encoder and different pre-trainedvision models based on convolutional neural network (CNN) and Transformerarchitecture for features extraction from question and endoscopy image. Theresult of this study highlights the dominance of Transformer-based visionmodels over the CNNs and demonstrates the effectiveness of the imageenhancement process, with six out of the eight vision models achieving betterF1-Score. Our best method, which takes advantages of BERT+BEiT fusion and imageenhancement, achieves up to 87.25% accuracy and 91.85% F1-Score on thedevelopment test set, while also producing good result on the private test setwith accuracy of 82.01%.", "output": "UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate performance disparities in deep classifiers. We find that theability of classifiers to separate individuals into subgroups variessubstantially across medical imaging modalities and protected characteristics;crucially, we show that this property is predictive of algorithmic bias.Through theoretical analysis and extensive empirical evaluation, we find arelationship between subgroup separability, subgroup disparities, andperformance degradation when models are trained on data with systematic biassuch as underdiagnosis. Our findings shed new light on the question of howmodels become biased, providing important insights for the development of fairmedical imaging AI.", "output": "The Role of Subgroup Separability in Group-Fair Medical Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although unsupervised domain adaptation (UDA) is a promising direction toalleviate domain shift, they fall short of their supervised counterparts. Inthis work, we investigate relatively less explored semi-supervised domainadaptation (SSDA) for medical image segmentation, where access to a few labeledtarget samples can improve the adaptation performance substantially.Specifically, we propose a two-stage training process. First, an encoder ispre-trained in a self-learning paradigm using a novel domain-contentdisentangled contrastive learning (CL) along with a pixel-level featureconsistency constraint. The proposed CL enforces the encoder to learndiscriminative content-specific but domain-invariant semantics on a globalscale from the source and target images, whereas consistency regularizationenforces the mining of local pixel-level information by maintaining spatialsensitivity. This pre-trained encoder, along with a decoder, is furtherfine-tuned for the downstream task, (i.e. pixel-level segmentation) using asemi-supervised setting. Furthermore, we experimentally validate that ourproposed method can easily be extended for UDA settings, adding to thesuperiority of the proposed strategy. Upon evaluation on two domain adaptiveimage segmentation tasks, our proposed method outperforms the SoTA methods,both in SSDA and UDA settings. Code is available at", "output": "Semi-supervised Domain Adaptive Medical Image Segmentation through Consistency Regularized Disentangled Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Digital humans have witnessed extensive applications in various domains,necessitating related quality assessment studies. However, there is a lack ofcomprehensive digital human quality assessment (DHQA) databases. To addressthis gap, we propose SJTU-H3D, a subjective quality assessment databasespecifically designed for full-body digital humans. It comprises 40high-quality reference digital humans and 1,120 labeled distorted counterpartsgenerated with seven types of distortions. The SJTU-H3D database can serve as abenchmark for DHQA research, allowing evaluation and refinement of processingalgorithms. Further, we propose a zero-shot DHQA approach that focuses onno-reference (NR) scenarios to ensure generalization capabilities whilemitigating database bias. Our method leverages semantic and distortion featuresextracted from projections, as well as geometry features derived from the meshstructure of digital humans. Specifically, we employ the ContrastiveLanguage-Image Pre-training (CLIP) model to measure semantic affinity andincorporate the Naturalness Image Quality Evaluator (NIQE) model to capturelow-level distortion information. Additionally, we utilize dihedral angles asgeometry descriptors to extract mesh features. By aggregating these measures,we introduce the Digital Human Quality Index (DHQI), which demonstratessignificant improvements in zero-shot performance. The DHQI can also serve as arobust baseline for DHQA tasks, facilitating advancements in the field. Thedatabase and the code are available at ", "output": "Advancing Zero-Shot Digital Human Quality Assessment through Text-Prompted Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Digital imaging aims to replicate realistic scenes, but Low Dynamic Range(LDR) cameras cannot represent the wide dynamic range of real scenes, resultingin under-/overexposed images. This paper presents a deep learning-basedapproach for recovering intricate details from shadows and highlights whilereconstructing High Dynamic Range (HDR) images. We formulate the problem as animage-to-image (I2I) translation task and propose a conditional DenoisingDiffusion Probabilistic Model (DDPM) based framework using classifier-freeguidance. We incorporate a deep CNN-based autoencoder in our proposed frameworkto enhance the quality of the latent representation of the input LDR image usedfor conditioning. Moreover, we introduce a new loss function for LDR-HDRtranslation tasks, termed Exposure Loss. This loss helps direct gradients inthe opposite direction of the saturation, further improving the results'quality. By conducting comprehensive quantitative and qualitative experiments,we have effectively demonstrated the proficiency of our proposed method. Theresults indicate that a simple conditional diffusion-based method can replacethe complex camera pipeline-based architectures.", "output": "Single Image LDR to HDR Conversion using Conditional Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tractography traces the peak directions extracted from fiber orientationdistribution (FOD) suffering from ambiguous spatial correspondences betweendiffusion directions and fiber geometry, which is prone to producing erroneoustracks while missing true positive connections. The peaks-based tractographymethods 'locally' reconstructed streamlines in 'single to single' manner, thuslacking of global information about the trend of the whole fiber bundle. Inthis work, we propose a novel tractography method based on a bundle-specifictractogram distribution function by using a higher-order streamlinedifferential equation, which reconstructs the streamline bundles in 'cluster tocluster' manner. A unified framework for any higher-order streamlinedifferential equation is presented to describe the fiber bundles with disjointstreamlines defined based on the diffusion tensor vector field. At the globallevel, the tractography process is simplified as the estimation ofbundle-specific tractogram distribution (BTD) coefficients by minimizing theenergy optimization model, and is used to characterize the relations betweenBTD and diffusion tensor vector under the prior guidance by introducing thetractogram bundle information to provide anatomic priors. Experiments areperformed on simulated Hough, Sine, Circle data, ISMRM 2015 TractographyChallenge data, FiberCup data, and in vivo data from the Human ConnectomeProject (HCP) data for qualitative and quantitative evaluation. The resultsdemonstrate that our approach can reconstruct the complex global fiber bundlesdirectly. BTD reduces the error deviation and accumulation at the local leveland shows better results in reconstructing long-range, twisting, and largefanning tracts.", "output": "Bundle-specific Tractogram Distribution Estimation Using Higher-order Streamline Differential Equation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are known to be vulnerable to adversarial examplescrafted by adding human-imperceptible perturbations to the benign input. Afterachieving nearly 100% attack success rates in white-box setting, more focus isshifted to black-box attacks, of which the transferability of adversarialexamples has gained significant attention. In either case, the commongradient-based methods generally use the sign function to generateperturbations on the gradient update, that offers a roughly correct directionand has gained great success. But little work pays attention to its possiblelimitation. In this work, we observe that the deviation between the originalgradient and the generated noise may lead to inaccurate gradient updateestimation and suboptimal solutions for adversarial transferability. To thisend, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM).Specifically, we use data rescaling to substitute the sign function withoutextra computational cost. We further propose a Depth First Sampling method toeliminate the fluctuation of rescaling and stabilize the gradient update. Ourmethod could be used in any gradient-based attacks and is extensible to beintegrated with various input transformation or ensemble methods to furtherimprove the adversarial transferability. Extensive experiments on the standardImageNet dataset show that our method could significantly boost thetransferability of gradient-based attacks and outperform the state-of-the-artbaselines.", "output": "Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomaly detection has a wide range of applications and is especiallyimportant in industrial quality inspection. Currently, many top-performinganomaly-detection models rely on feature-embedding methods. However, thesemethods do not perform well on datasets with large variations in objectlocations. Reconstruction-based methods use reconstruction errors to detectanomalies without considering positional differences between samples. In thisstudy, a reconstruction-based method using the noise-to-norm paradigm isproposed, which avoids the invariant reconstruction of anomalous regions. Ourreconstruction network is based on M-net and incorporates multiscale fusion andresidual attention modules to enable end-to-end anomaly detection andlocalization. Experiments demonstrate that the method is effective inreconstructing anomalous regions into normal patterns and achieving accurateanomaly detection and localization. On the MPDD and VisA datasets, our proposedmethod achieved more competitive results than the latest methods, and it set anew state-of-the-art standard on the MPDD dataset.", "output": "Noise-to-Norm Reconstruction for Industrial Anomaly Detection and Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tuberculosis (TB) is a major global health threat, causing millions of deathsannually. Although early diagnosis and treatment can greatly improve thechances of survival, it remains a major challenge, especially in developingcountries. Recently, computer-aided tuberculosis diagnosis (CTD) using deeplearning has shown promise, but progress is hindered by limited training data.To address this, we establish a large-scale dataset, namely the TuberculosisX-ray (TBX11K) dataset, which contains 11,200 chest X-ray (CXR) images withcorresponding bounding box annotations for TB areas. This dataset enables thetraining of sophisticated detectors for high-quality CTD. Furthermore, wepropose a strong baseline, SymFormer, for simultaneous CXR image classificationand TB infection area detection. SymFormer incorporates Symmetric SearchAttention (SymAttention) to tackle the bilateral symmetry property of CXRimages for learning discriminative features. Since CXR images may not strictlyadhere to the bilateral symmetry property, we also propose Symmetric PositionalEncoding (SPE) to facilitate SymAttention through feature recalibration. Topromote future research on CTD, we build a benchmark by introducing evaluationmetrics, evaluating baseline models reformed from existing detectors, andrunning an online challenge. Experiments show that SymFormer achievesstate-of-the-art performance on the TBX11K dataset. The data, code, and modelswill be released.", "output": "Revisiting Computer-Aided Tuberculosis Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face presentation attacks, also known as spoofing attacks, pose a significantthreat to biometric systems that rely on facial recognition systems, such asaccess control systems, mobile payments, and identity verification systems. Toprevent spoofing, several video-based methods have been presented in theliterature that analyze facial motion in successive video frames. However,estimating the motion between adjacent frames is a challenging task andrequires high computational cost. In this paper, we reformulate the faceanti-spoofing task as a motion prediction problem and introduce a deep ensemblelearning model with a frame skipping mechanism. The proposed frame skipping isbased on a uniform sampling approach where the original video is divided intofixed size video clips. In this way, every nth frame of the clip is selected toensure that the temporal patterns can easily be perceived during the trainingof three different recurrent neural networks (RNNs). Motivated by theperformance of each RNNs, a meta-model is developed to improve the overallrecognition performance by combining the predictions of the individual RNNs.Extensive experiments were conducted on four datasets, and state-of-the-artperformance is reported for MSU-MFSD (3.12%), Replay-Attack (11.19%), andOULU-NPU (12.23%) using half total error rate (HTER) in the most challengingcross-dataset test scenario.", "output": "Deep Ensemble Learning with Frame Skipping for Face Anti-Spoofing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years large model trained on huge amount of cross-modality data,which is usually be termed as foundation model, achieves conspicuousaccomplishment in many fields, such as image recognition and generation. Thoughachieving great success in their original application case, it is still unclearwhether those foundation models can be applied to other different downstreamtasks. In this paper, we conduct a short survey on the current methods fordiscriminative dense recognition tasks, which are built on the pretrainedfoundation model. And we also provide some preliminary experimental analysis ofan existing open-vocabulary segmentation method based on Stable Diffusion,which indicates the current way of deploying diffusion model for segmentationis not optimal. This aims to provide insights for future research on adoptingfoundation model for downstream task.", "output": "A Critical Look at the Current Usage of Foundation Model for Dense Recognition Task."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video moment retrieval pursues an efficient and generalized solution toidentify the specific temporal segments within an untrimmed video thatcorrespond to a given language description. To achieve this goal, we provide agenerative diffusion-based framework called MomentDiff, which simulates atypical human retrieval process from random browsing to gradual localization.Specifically, we first diffuse the real span to random noise, and learn todenoise the random noise to the original span with the guidance of similaritybetween text and video. This allows the model to learn a mapping from arbitraryrandom locations to real moments, enabling the ability to locate segments fromrandom initialization. Once trained, MomentDiff could sample random temporalsegments as initial guesses and iteratively refine them to generate an accuratetemporal boundary. Different from discriminative works (e.g., based onlearnable proposals or queries), MomentDiff with random initialized spans couldresist the temporal location biases from datasets. To evaluate the influence ofthe temporal location biases, we propose two anti-bias datasets with locationdistribution shifts, named Charades-STA-Len and Charades-STA-Mom. Theexperimental results demonstrate that our efficient framework consistentlyoutperforms state-of-the-art methods on three public benchmarks, and exhibitsbetter generalization and robustness on the proposed anti-bias datasets. Thecode, model, and anti-bias evaluation datasets are available at", "output": "MomentDiff: Generative Video Moment Retrieval from Random to Real."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the recent advancement in the study of removing motion blur in animage, it is still hard to deal with strong blurs. While there are limits inremoving blurs from a single image, it has more potential to use multipleimages, e.g., using an additional image as a reference to deblur a blurryimage. A typical setting is deburring an image using a nearby sharp image(s) ina video sequence, as in the studies of video deblurring. This paper proposes abetter method to use the information present in a reference image. The methoddoes not need a strong assumption on the reference image. We can utilize analternative shot of the identical scene, just like in video deblurring, or wecan even employ a distinct image from another scene. Our method first matcheslocal patches of the target and reference images and then fuses their featuresto estimate a sharp image. We employ a patch-based feature matching strategy tosolve the difficult problem of matching the blurry image with the sharpreference. Our method can be integrated into pre-existing networks designed forsingle image deblurring. The experimental results show the effectiveness of theproposed method.", "output": "Reference-based Motion Blur Removal: Learning to Utilize Sharpness in the Reference Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Panoptic segmentation is the combination of semantic and instancesegmentation: assign the points in a 3D point cloud to semantic categories andpartition them into distinct object instances. It has many obvious applicationsfor outdoor scene understanding, from city mapping to forest management.Existing methods struggle to segment nearby instances of the same semanticcategory, like adjacent pieces of street furniture or neighbouring trees, whichlimits their usability for inventory- or management-type applications that relyon object instances. This study explores the steps of the panoptic segmentationpipeline concerned with clustering points into object instances, with the goalto alleviate that bottleneck. We find that a carefully designed clusteringstrategy, which leverages multiple types of learned point embeddings,significantly improves instance segmentation. Experiments on the NPM3D urbanmobile mapping dataset and the FOR-instance forest dataset demonstrate theeffectiveness and versatility of the proposed strategy.", "output": "Towards accurate instance segmentation in large-scale LiDAR point clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper begins with a description of methods for estimating probabilitydensity functions for images that reflects the observation that such data isusually constrained to lie in restricted regions of the high-dimensional imagespace - not every pattern of pixels is an image. It is common to say thatimages lie on a lower-dimensional manifold in the high-dimensional space.However, although images may lie on such lower-dimensional manifolds, it is notthe case that all points on the manifold have an equal probability of beingimages. Images are unevenly distributed on the manifold, and our task is todevise ways to model this distribution as a probability distribution. Inpursuing this goal, we consider generative models that are popular in AI andcomputer vision community. For our purposes, generative/probabilistic modelsshould have the properties of 1) sample generation: it should be possible tosample from this distribution according to the modelled density function, and2) probability computation: given a previously unseen sample from the datasetof interest, one should be able to compute the probability of the sample, atleast up to a normalising constant. To this end, we investigate the use ofmethods such as normalising flow and diffusion models. We then show that suchprobabilistic descriptions can be used to construct defences againstadversarial attacks. In addition to describing the manifold in terms ofdensity, we also consider how semantic interpretations can be used to describepoints on the manifold. To this end, we consider an emergent language frameworkwhich makes use of variational encoders to produce a disentangledrepresentation of points that reside on a given manifold. Trajectories betweenpoints on a manifold can then be described in terms of evolving semanticdescriptions.", "output": "Probabilistic and Semantic Descriptions of Image Manifolds and Their Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Smartphones equipped with a multi-camera system comprising multiple cameraswith different field-of-view (FoVs) are becoming more prevalent. These cameraconfigurations are compatible with reference-based SR and video SR, which canbe executed simultaneously while recording video on the device. Thus, combiningthese two SR methods can improve image quality. Recently, Lee et al. havepresented such a method, RefVSR. In this paper, we consider how to optimallyutilize the observations obtained, including input low-resolution (LR) videoand reference (Ref) video. RefVSR extends conventional video SR quite simply,aggregating the LR and Ref inputs over time in a single bidirectional stream.However, considering the content difference between LR and Ref images due totheir FoVs, we can derive the maximum information from the two image sequencesby aggregating them independently in the temporal direction. Then, we proposean improved method, RefVSR++, which can aggregate two features in parallel inthe temporal direction, one for aggregating the fused LR and Ref inputs and theother for Ref inputs over time. Furthermore, we equip RefVSR++ with enhancedmechanisms to align image features over time, which is the key to the successof video SR. We experimentally show that RefVSR++ outperforms RefVSR by over1dB in PSNR, achieving the new state-of-the-art.", "output": "RefVSR++: Exploiting Reference Inputs for Reference-based Video Super-resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sensor-based Human Activity Recognition facilitates unobtrusive monitoring ofhuman movements. However, determining the most effective sensor placement foroptimal classification performance remains challenging. This paper introduces anovel methodology to resolve this issue, using real-time 2D pose estimationsderived from video recordings of target activities. The derived skeleton dataprovides a unique strategy for identifying the optimal sensor location. Wevalidate our approach through a feasibility study, applying inertial sensors tomonitor 13 different activities across ten subjects. Our findings indicate thatthe vision-based method for sensor placement offers comparable results to theconventional deep learning approach, demonstrating its efficacy. This researchsignificantly advances the field of Human Activity Recognition by providing alightweight, on-device solution for determining the optimal sensor placement,thereby enhancing data anonymization and supporting a multimodal classificationapproach.", "output": "A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Asymmetry is a crucial characteristic of bilateral mammograms (Bi-MG) whenabnormalities are developing. It is widely utilized by radiologists fordiagnosis. The question of 'what the symmetrical Bi-MG would look like when theasymmetrical abnormalities have been removed ?' has not yet received strongattention in the development of algorithms on mammograms. Addressing thisquestion could provide valuable insights into mammographic anatomy and aid indiagnostic interpretation. Hence, we propose a novel framework, DisAsymNet,which utilizes asymmetrical abnormality transformer guided self-adversariallearning for disentangling abnormalities and symmetric Bi-MG. At the same time,our proposed method is partially guided by randomly synthesized abnormalities.We conduct experiments on three public and one in-house dataset, anddemonstrate that our method outperforms existing methods in abnormalityclassification, segmentation, and localization tasks. Additionally,reconstructed normal mammograms can provide insights toward betterinterpretable visual cues for clinical diagnosis. The code will be accessibleto the public.", "output": "DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms using Self-adversarial Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, U-shaped networks have dominated the field of medical imagesegmentation due to their simple and easily tuned structure. However, existingU-shaped segmentation networks: 1) mostly focus on designing complexself-attention modules to compensate for the lack of long-term dependence basedon convolution operation, which increases the overall number of parameters andcomputational complexity of the network; 2) simply fuse the features of encoderand decoder, ignoring the connection between their spatial locations. In thispaper, we rethink the above problem and build a lightweight medical imagesegmentation network, called SegNetr. Specifically, we introduce a novelSegNetr block that can perform local-global interactions dynamically at anystage and with only linear complexity. At the same time, we design a generalinformation retention skip connection (IRSC) to preserve the spatial locationinformation of encoder features and achieve accurate fusion with the decoderfeatures. We validate the effectiveness of SegNetr on four mainstream medicalimage segmentation datasets, with 59% and 76% fewer parameters and GFLOPsthan vanilla U-Net, while achieving segmentation performance comparable tostate-of-the-art methods. Notably, the components proposed in this paper can beapplied to other U-shaped networks to improve their segmentation performance.", "output": "SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One challenge in text-to-image (T2I) generation is the inadvertent reflectionof culture gaps present in the training data, which signifies the disparity ingenerated image quality when the cultural elements of the input text are rarelycollected in the training set. Although various T2I models have shownimpressive but arbitrary examples, there is no benchmark to systematicallyevaluate a T2I model's ability to generate cross-cultural images. To bridge thegap, we propose a Challenging Cross-Cultural (C3) benchmark with comprehensiveevaluation criteria, which can assess how well-suited a model is to a targetculture. By analyzing the flawed images generated by the Stable Diffusion modelon the C3 benchmark, we find that the model often fails to generate certaincultural objects. Accordingly, we propose a novel multi-modal metric thatconsiders object-text alignment to filter the fine-tuning data in the targetculture, which is used to fine-tune a T2I model to improve cross-culturalgeneration. Experimental results show that our multi-modal metric providesstronger data selection performance on the C3 benchmark than existing metrics,in which the object-text alignment is crucial. We release the benchmark, data,code, and generated images to facilitate future research on culturally diverseT2I generation (", "output": "On the Cultural Gap in Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Remote sensing image super-resolution (RSISR) plays a vital role in enhancingspatial detials and improving the quality of satellite imagery. Recently,Transformer-based models have shown competitive performance in RSISR. Tomitigate the quadratic computational complexity resulting from globalself-attention, various methods constrain attention to a local window,enhancing its efficiency. Consequently, the receptive fields in a singleattention layer are inadequate, leading to insufficient context modeling.Furthermore, while most transform-based approaches reuse shallow featuresthrough skip connections, relying solely on these connections treats shallowand deep features equally, impeding the model's ability to characterize them.To address these issues, we propose a novel transformer architecture calledCross-Spatial Pixel Integration and Cross-Stage Feature Fusion BasedTransformer Network (SPIFFNet) for RSISR. Our proposed model effectivelyenhances global cognition and understanding of the entire image, facilitatingefficient integration of features cross-stages. The model incorporatescross-spatial pixel integration attention (CSPIA) to introduce contextualinformation into a local window, while cross-stage feature fusion attention(CSFFA) adaptively fuses features from the previous stage to improve featureexpression in line with the requirements of the current stage. We conductedcomprehensive experiments on multiple benchmark datasets, demonstrating thesuperior performance of our proposed SPIFFNet in terms of both quantitativemetrics and visual quality when compared to state-of-the-art methods.", "output": "Cross-Spatial Pixel Integration and Cross-Stage Feature Fusion Based Transformer Network for Remote Sensing Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Parkinson disease is the second most common neurodegenerative disorder, asreported by the World Health Organization. In this paper, we propose a directthree-Class PD classification using two different modalities, namely, MRI andDTI. The three classes used for classification are PD, Scans Without Evidenceof Dopamine Deficit and Healthy Control. We use white matter and gray matterfrom the MRI and fractional anisotropy and mean diffusivity from the DTI toachieve our goal. We train four separate CNNs on the above four types of data.At the decision level, the outputs of the four CNN models are fused with anoptimal weighted average fusion technique. We achieve an accuracy of 95.53percentage for the direct three class classification of PD, HC and SWEDD on thepublicly available PPMI database. Extensive comparisons including a series ofablation studies clearly demonstrate the effectiveness of our proposedsolution.", "output": "Multi-modal multi-class Parkinson disease classification using CNN and decision level fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Adversarial Networks (GANs) have demonstrated their ability togenerate synthetic samples that match a target distribution. However, from aprivacy perspective, using GANs as a proxy for data sharing is not a safesolution, as they tend to embed near-duplicates of real samples in the latentspace. Recent works, inspired by k-anonymity principles, address this issuethrough sample aggregation in the latent space, with the drawback of reducingthe dataset by a factor of k. Our work aims to mitigate this problem byproposing a latent space navigation strategy able to generate diverse syntheticsamples that may support effective training of deep models, while addressingprivacy concerns in a principled way. Our approach leverages an auxiliaryidentity classifier as a guide to non-linearly walk between points in thelatent space, minimizing the risk of collision with near-duplicates of realsamples. We empirically demonstrate that, given any random pair of points inthe latent space, our walking strategy is safer than linear interpolation. Wethen test our path-finding strategy combined to k-same methods and demonstrate,on two benchmarks for tuberculosis and diabetic retinopathy classification,that training a model using samples generated by our approach mitigate drops inperformance, while keeping privacy preservation.", "output": "A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "U-Net style networks are commonly utilized in unsupervised image registrationto predict dense displacement fields, which for high-resolution volumetricimage data is a resource-intensive and time-consuming task. To tackle thischallenge, we first propose Fourier-Net, which replaces the costly U-Net styleexpansive path with a parameter-free model-driven decoder. Instead of directlypredicting a full-resolution displacement field, our Fourier-Net learns alow-dimensional representation of the displacement field in the band-limitedFourier domain which our model-driven decoder converts to a full-resolutiondisplacement field in the spatial domain. Expanding upon Fourier-Net, we thenintroduce Fourier-Net+, which additionally takes the band-limited spatialrepresentation of the images as input and further reduces the number ofconvolutional layers in the U-Net style network's contracting path. Finally, toenhance the registration performance, we propose a cascaded version ofFourier-Net+. We evaluate our proposed methods on three datasets, on which ourproposed Fourier-Net and its variants achieve comparable results with currentstate-of-the art methods, while exhibiting faster inference speeds, lowermemory footprint, and fewer multiply-add operations. With such smallcomputational cost, our Fourier-Net+ enables the efficient training oflarge-scale 3D registration on low-VRAM GPUs. Our code is publicly available aturl{", "output": "Fourier-Net+: Leveraging Band-Limited Representation for Efficient 3D Medical Image Registration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Manual assembly workers face increasing complexity in their work.Human-centered assistance systems could help, but object recognition as anenabling technology hinders sophisticated human-centered design of thesesystems. At the same time, activity recognition based on hand poses suffersfrom poor pose estimation in complex usage scenarios, such as wearing gloves.This paper presents a self-supervised pipeline for adapting hand poseestimation to specific use cases with minimal human interaction. This enablescheap and robust hand posebased activity recognition. The pipeline consists ofa general machine learning model for hand pose estimation trained on ageneralized dataset, spatial and temporal filtering to account for anatomicalconstraints of the hand, and a retraining step to improve the model. Differentparameter combinations are evaluated on a publicly available and annotateddataset. The best parameter and model combination is then applied to unlabelledvideos from a manual assembly scenario. The effectiveness of the pipeline isdemonstrated by training an activity recognition as a downstream task in themanual assembly scenario.", "output": "Self-supervised Optimization of Hand Pose Estimation using Anatomical Features and Iterative Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has become a valuable tool for the automation of certainmedical image segmentation tasks, significantly relieving the workload ofmedical specialists. Some of these tasks require segmentation to be performedon a subset of the input dimensions, the most common case being 3D-to-2D.However, the performance of existing methods is strongly conditioned by theamount of labeled data available, as there is currently no data efficientmethod, e.g. transfer learning, that has been validated on these tasks. In thiswork, we propose a novel convolutional neural network (CNN) and self-supervisedlearning (SSL) method for label-efficient 3D-to-2D segmentation. The CNN iscomposed of a 3D encoder and a 2D decoder connected by novel 3D-to-2D blocks.The SSL method consists of reconstructing image pairs of modalities withdifferent dimensionality. The approach has been validated in two tasks withclinical relevance: the en-face segmentation of geographic atrophy andreticular pseudodrusen in optical coherence tomography. Results on differentdatasets demonstrate that the proposed CNN significantly improves the state ofthe art in scenarios with limited labeled data by up to 8% in Dice score.Moreover, the proposed SSL method allows further improvement of thisperformance by up to 23%, and we show that the SSL is beneficial regardless ofthe network architecture.", "output": "Self-supervised learning via inter-modal reconstruction and feature projection networks for label-efficient 3D-to-2D segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rise of Extended Reality (XR) technology, there is a growing needfor real-time light field generation from sparse view inputs. Existing methodscan be classified into offline techniques, which can generate high-qualitynovel views but at the cost of long inference/training time, and onlinemethods, which either lack generalizability or produce unsatisfactory results.However, we have observed that the intrinsic sparse manifold of Multi-planeImages (MPI) enables a significant acceleration of light field generation whilemaintaining rendering quality. Based on this insight, we introduce EffLiFe, anovel light field optimization method, which leverages the proposedHierarchical Sparse Gradient Descent (HSGD) to produce high-quality lightfields from sparse view images in real time. Technically, the coarse MPI of ascene is first generated using a 3D CNN, and it is further sparsely optimizedby focusing only on important MPI gradients in a few iterations. Nevertheless,relying solely on optimization can lead to artifacts at occlusion boundaries.Therefore, we propose an occlusion-aware iterative refinement module thatremoves visual artifacts in occluded regions by iteratively filtering theinput. Extensive experiments demonstrate that our method achieves comparablevisual quality while being 100x faster on average than state-of-the-art offlinemethods and delivering better performance (about 2 dB higher in PSNR) comparedto other online approaches.", "output": "EffLiFe: Efficient Light Field Generation via Hierarchical Sparse Gradient Descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, Transformers, initially developed for language, have beensuccessfully applied to visual tasks. Vision Transformers have been shown topush the state-of-the-art in a wide range of tasks, including imageclassification, object detection, and semantic segmentation. While ampleresearch has shown promising results in art attribution and art authenticationtasks using Convolutional Neural Networks, this paper examines if thesuperiority of Vision Transformers extends to art authentication, improving,thus, the reliability of computer-based authentication of artworks. Using acarefully compiled dataset of authentic paintings by Vincent van Gogh and twocontrast datasets, we compare the art authentication performances of SwinTransformers with those of EfficientNet. Using a standard contrast setcontaining imitations and proxies (works by painters with styles closelyrelated to van Gogh), we find that EfficientNet achieves the best performanceoverall. With a contrast set that only consists of imitations, we find the SwinTransformer to be superior to EfficientNet by achieving an authenticationaccuracy of over 85%. These results lead us to conclude that VisionTransformers represent a strong and promising contender in art authentication,particularly in enhancing the computer-based ability to detect artisticimitations.", "output": "Art Authentication with Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel framework for few-shot learning by leveraging large-scalevision-language models such as CLIP. Motivated by the unimodal prototypicalnetworks for few-shot learning, we introduce PROTO-CLIP that utilizes imageprototypes and text prototypes for few-shot learning. Specifically, PROTO-CLIPadapts the image encoder and text encoder in CLIP in a joint fashion usingfew-shot examples. The two encoders are used to compute prototypes of imageclasses for classification. During adaptation, we propose aligning the imageand text prototypes of corresponding classes. Such a proposed alignment isbeneficial for few-shot classification due to the contributions from both typesof prototypes. We demonstrate the effectiveness of our method by conductingexperiments on benchmark datasets for few-shot learning as well as in the realworld for robot perception.", "output": "Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Previous works on unsupervised industrial anomaly detection mainly focus onlocal structural anomalies such as cracks and color contamination. Whileachieving significantly high detection performance on this kind of anomaly,they are faced with logical anomalies that violate the long-range dependenciessuch as a normal object placed in the wrong position. In this paper, based onprevious knowledge distillation works, we propose to use two students (localand global) to better mimic the teacher's behavior. The local student, which isused in previous studies mainly focuses on structural anomaly detection whilethe global student pays attention to logical anomalies. To further encouragethe global student's learning to capture long-range dependencies, we design theglobal context condensing block (GCCB) and propose a contextual affinity lossfor the student training and anomaly scoring. Experimental results show theproposed method doesn't need cumbersome training techniques and achieves a newstate-of-the-art performance on the MVTec LOCO AD dataset.", "output": "Contextual Affinity Distillation for Image Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent text-to-image diffusion models have shown surprising performance ingenerating high-quality images. However, concerns have arisen regarding theunauthorized usage of data during the training process. One example is when amodel trainer collects a set of images created by a particular artist andattempts to train a model capable of generating similar images withoutobtaining permission from the artist. To address this issue, it becomes crucialto detect unauthorized data usage. In this paper, we propose a method fordetecting such unauthorized data usage by planting injected memorization intothe text-to-image diffusion models trained on the protected dataset.Specifically, we modify the protected image dataset by adding unique contentson the images such as stealthy image wrapping functions that are imperceptibleto human vision but can be captured and memorized by diffusion models. Byanalyzing whether the model has memorization for the injected content (i.e.,whether the generated images are processed by the chosen post-processingfunction), we can detect models that had illegally utilized the unauthorizeddata. Our experiments conducted on Stable Diffusion and LoRA model demonstratethe effectiveness of the proposed method in detecting unauthorized data usages.", "output": "How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Search spaces hallmark the advancement of Neural Architecture Search (NAS).Large and complex search spaces with versatile building operators andstructures provide more opportunities to brew promising architectures, yet posesevere challenges on efficient exploration and exploitation. Subsequently,several search space shrinkage methods optimize by selecting a singlesub-region that contains some well-performing networks. Small performance andefficiency gains are observed with these methods but such techniques leave roomfor significantly improved search performance and are ineffective at retainingarchitectural diversity. We propose LISSNAS, an automated algorithm thatshrinks a large space into a diverse, small search space with SOTA searchperformance. Our approach leverages locality, the relationship betweenstructural and performance similarity, to efficiently extract many pockets ofwell-performing networks. We showcase our method on an array of search spacesspanning various sizes and datasets. We accentuate the effectiveness of ourshrunk spaces when used in one-shot search by achieving the best Top-1 accuracyin two different search spaces. Our method achieves a SOTA Top-1 accuracy of77.6% in ImageNet under mobile constraints, best-in-class Kendal-Tau,architectural diversity, and search space size.", "output": "LISSNAS: Locality-based Iterative Search Space Shrinkage for Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we demonstrate how sub-Riemannian geometry can be used formanifold learning and surface reconstruction by combining local linearapproximations of a point cloud to obtain lower dimensional bundles. Localapproximations obtained by local PCAs are collected into a rank $k$ tangentsubbundle on $mathbb{R}^d$, $k&lt;d$, which we call a principal subbundle. Thisdetermines a sub-Riemannian metric on $mathbb{R}^d$. We show thatsub-Riemannian geodesics with respect to this metric can successfully beapplied to a number of important problems, such as: explicit construction of anapproximating submanifold $M$, construction of a representation of thepoint-cloud in $mathbb{R}^k$, and computation of distances betweenobservations, taking the learned geometry into account. The reconstruction isguaranteed to equal the true submanifold in the limit case where tangent spacesare estimated exactly. Via simulations, we show that the framework is robustwhen applied to noisy data. Furthermore, the framework generalizes toobservations on an a priori known Riemannian manifold.", "output": "Principal subbundles for dimension reduction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large web-sourced multimodal datasets have powered a slew of new methods forlearning general-purpose visual representations, advancing the state of the artin computer vision and revolutionizing zero- and few-shot recognition. Onecrucial decision facing practitioners is how, if at all, to curate theseever-larger datasets. For example, the creators of the LAION-5B dataset choseto retain only image-caption pairs whose CLIP similarity score exceeded adesignated threshold. In this paper, we propose a new state-of-the-art datafiltering approach motivated by our observation that nearly 40% of LAION'simages contain text that overlaps significantly with the caption. Intuitively,such data could be wasteful as it incentivizes models to perform opticalcharacter recognition rather than learning visual features. However, naivelyremoving all such data could also be wasteful, as it throws away images thatcontain visual features (in addition to overlapping text). Our simple andscalable approach, T-MARS (Text Masking and Re-Scoring), filters out only thosepairs where the text dominates the remaining visual features -- by firstmasking out the text and then filtering out those with a low CLIP similarityscore of the masked image. Experimentally, T-MARS outperforms the top-rankedmethod on the \"medium scale\" of DataComp (a data filtering benchmark) by amargin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematicevaluation on various data pool sizes from 2M to 64M shows that the accuracygains enjoyed by T-MARS linearly increase as data and compute are scaledexponentially. Code is available at ", "output": "T-MARS: Improving Visual Representations by Circumventing Text Feature Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Test-time adaptation (TTA) is a technique aimed at enhancing thegeneralization performance of models by leveraging unlabeled samples solelyduring prediction. Given the need for robustness in neural network systems whenfaced with distribution shifts, numerous TTA methods have recently beenproposed. However, evaluating these methods is often done under differentsettings, such as varying distribution shifts, backbones, and designingscenarios, leading to a lack of consistent and fair benchmarks to validatetheir effectiveness. To address this issue, we present a benchmark thatsystematically evaluates 13 prominent TTA methods and their variants on fivewidely used image classification datasets: CIFAR-10-C, CIFAR-100-C, ImageNet-C,DomainNet, and Office-Home. These methods encompass a wide range of adaptationscenarios (e.g. online adaptation v.s. offline adaptation, instance adaptationv.s. batch adaptation v.s. domain adaptation). Furthermore, we explore thecompatibility of different TTA methods with diverse network backbones. Toimplement this benchmark, we have developed a unified framework in PyTorch,which allows for consistent evaluation and comparison of the TTA methods acrossthe different datasets and network architectures. By establishing thisbenchmark, we aim to provide researchers and practitioners with a reliablemeans of assessing and comparing the effectiveness of TTA methods in improvingmodel robustness and generalization performance. Our code is available at", "output": "Benchmarking Test-Time Adaptation against Distribution Shifts in Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large vision-language models have achieved outstanding performance, but theirsize and computational requirements make their deployment onresource-constrained devices and time-sensitive tasks impractical. Modeldistillation, the process of creating smaller, faster models that maintain theperformance of larger models, is a promising direction towards the solution.This paper investigates the distillation of visual representations in largeteacher vision-language models into lightweight student models using a small-or mid-scale dataset. Notably, this study focuses on open-vocabularyout-of-distribution (OOD) generalization, a challenging problem that has beenoverlooked in previous model distillation literature. We propose two principlesfrom vision and language modality perspectives to enhance student's OODgeneralization: (1) by better imitating teacher's visual representation space,and carefully promoting better coherence in vision-language alignment with theteacher; (2) by enriching the teacher's language representations withinformative and finegrained semantic attributes to effectively distinguishbetween different labels. We propose several metrics and conduct extensiveexperiments to investigate their techniques. The results demonstratesignificant improvements in zero-shot and few-shot student performance onopen-vocabulary out-of-distribution classification, highlighting theeffectiveness of our proposed approaches. Our code will be released at", "output": "Distilling Large Vision-Language Model with Out-of-Distribution Generalizability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Segmentation networks are not explicitly imposed to learn global invariantsof an image, such as the shape of an object and the geometry between multipleobjects, when they are trained with a standard loss function. On the otherhand, incorporating such invariants into network training may help improveperformance for various segmentation tasks when they are the intrinsiccharacteristics of the objects to be segmented. One example is segmentation ofaorta and great vessels in computed tomography (CT) images where vessels arefound in a particular geometry in the body due to the human anatomy and theymostly seem as round objects on a 2D CT image. This paper addresses this issueby introducing a new topology-aware loss function that penalizes topologydissimilarities between the ground truth and prediction through persistenthomology. Different from the previously suggested segmentation network designs,which apply the threshold filtration on a likelihood function of the predictionmap and the Betti numbers of the ground truth, this paper proposes to apply theVietoris-Rips filtration to obtain persistence diagrams of both ground truthand prediction maps and calculate the dissimilarity with the Wassersteindistance between the corresponding persistence diagrams. The use of thisfiltration has advantage of modeling shape and geometry at the same time, whichmay not happen when the threshold filtration is applied. Our experiments on4327 CT images of 24 subjects reveal that the proposed topology-aware lossfunction leads to better results than its counterparts, indicating theeffectiveness of this use.", "output": "Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Everyday news coverage has shifted from traditional broadcasts towards a widerange of presentation formats such as first-hand, unedited video footage.Datasets that reflect the diverse array of multimodal, multilingual newssources available online could be used to teach models to benefit from thisshift, but existing news video datasets focus on traditional news broadcastsproduced for English-speaking audiences. We address this limitation byconstructing MultiVENT, a dataset of multilingual, event-centric videosgrounded in text documents across five target languages. MultiVENT includesboth news broadcast videos and non-professional event footage, which we use toanalyze the state of online news videos and how they can be leveraged to buildrobust, factually accurate models. Finally, we provide a model for complex,multilingual video retrieval to serve as a baseline for information retrievalusing MultiVENT.", "output": "MultiVENT: Multilingual Videos of Events with Aligned Natural Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning-based diagnostic system has demonstrated potential inclassifying skin cancer conditions when labeled training example are abundant.However, skin lesion analysis often suffers from a scarcity of labeled data,hindering the development of an accurate and reliable diagnostic system. Inthis work, we leverage multiple skin lesion datasets and investigate thefeasibility of various unsupervised domain adaptation (UDA) methods in binaryand multi-class skin lesion classification. In particular, we assess three UDAtraining schemes: single-, combined-, and multi-source. Our experiment resultsshow that UDA is effective in binary classification, with further improvementbeing observed when imbalance is mitigated. In multi-class task, itsperformance is less prominent, and imbalance problem again needs to beaddressed to achieve above-baseline accuracy. Through our quantitativeanalysis, we find that the test error of multi-class tasks is stronglycorrelated with label shift, and feature-level UDA methods have limitationswhen handling imbalanced datasets. Finally, our study reveals that UDA caneffectively reduce bias against minority groups and promote fairness, evenwithout the explicit use of fairness-focused techniques.", "output": "Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We evaluate existing foundation models video understanding capabilities usinga carefully designed experiment protocol consisting of three hallmark tasks(action recognition, temporal localization, and spatiotemporal localization),eight datasets well received by the community, and four adaptation methodstailoring a foundation model (FM) for a downstream task. Moreover, we propose ascalar VideoGLUE score (VGS) to measure an FMs efficacy and efficiency whenadapting to general video understanding tasks. Our main findings are asfollows. First, task-specialized models significantly outperform the six FMsstudied in this work, in sharp contrast to what FMs have achieved in naturallanguage and image understanding. Second,video-native FMs, whose pretrainingdata contains the video modality, are generally better than image-native FMs inclassifying motion-rich videos, localizing actions in time, and understanding avideo of more than one action. Third, the video-native FMs can perform well onvideo tasks under light adaptations to downstream tasks(e.g., freezing the FMbackbones), while image-native FMs win in full end-to-end finetuning. The firsttwo observations reveal the need and tremendous opportunities to conductresearch on video-focused FMs, and the last confirms that both tasks andadaptation methods matter when it comes to the evaluation of FMs.", "output": "VideoGLUE: Video General Understanding Evaluation of Foundation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous agriculture applications (e.g., inspection, phenotyping, pluckingfruits) require manipulating the plant foliage to look behind the leaves andthe branches. Partial visibility, extreme clutter, thin structures, and unknowngeometry and dynamics for plants make such manipulation challenging. We tacklethese challenges through data-driven methods. We use self-supervision to trainSRPNet, a neural network that predicts what space is revealed on execution of acandidate action on a given plant. We use SRPNet with the cross-entropy methodto predict actions that are effective at revealing space beneath plant foliage.Furthermore, as SRPNet does not just predict how much space is revealed butalso where it is revealed, we can execute a sequence of actions thatincrementally reveal more and more space beneath the plant foliage. Weexperiment with a synthetic (vines) and a real plant (Dracaena) on a physicaltest-bed across 5 settings including 2 settings that test generalization tonovel plant configurations. Our experiments reveal the effectiveness of ouroverall method, PPG, over a competitive hand-crafted exploration method, andthe effectiveness of SRPNet over a hand-crafted dynamics model and relevantablations.", "output": "Push Past Green: Learning to Look Behind Plant Foliage by Moving It."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating complete 360-degree panoramas from narrow field of view images isongoing research as omnidirectional RGB data is not readily available. ExistingGAN-based approaches face some barriers to achieving higher quality output, andhave poor generalization performance over different mask types. In this paper,we present our 360-degree indoor RGB panorama outpainting model using latentdiffusion models (LDM), called IPO-LDM. We introduce a new bi-modal latentdiffusion structure that utilizes both RGB and depth panoramic data duringtraining, but works surprisingly well to outpaint normal depth-free RGB imagesduring inference. We further propose a novel technique of introducingprogressive camera rotations during each diffusion denoising step, which leadsto substantial improvement in achieving panorama wraparound consistency.Results show that our IPO-LDM not only significantly outperformsstate-of-the-art methods on RGB panorama outpainting, but can also producemultiple and diverse well-structured results for different types of masks.", "output": "IPO-LDM: Depth-aided 360-degree Indoor RGB Panorama Outpainting via Latent Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Artistic Cinemagraph, a fully automated method for creatingcinemagraphs from text descriptions - an especially challenging task whenprompts feature imaginary elements and artistic styles, given the complexity ofinterpreting the semantics and motions of these images. Existing single-imageanimation methods fall short on artistic inputs, and recent text-based videomethods frequently introduce temporal inconsistencies, struggling to keepcertain regions static. To address these challenges, we propose an idea ofsynthesizing image twins from a single text prompt - a pair of an artisticimage and its pixel-aligned corresponding natural-looking twin. While theartistic image depicts the style and appearance detailed in our text prompt,the realistic counterpart greatly simplifies layout and motion analysis.Leveraging existing natural image and video datasets, we can accurately segmentthe realistic image and predict plausible motion given the semanticinformation. The predicted motion can then be transferred to the artistic imageto create the final cinemagraph. Our method outperforms existing approaches increating cinemagraphs for natural landscapes as well as artistic andother-worldly scenes, as validated by automated metrics and user studies.Finally, we demonstrate two extensions: animating existing paintings andcontrolling motion directions using text.", "output": "Synthesizing Artistic Cinemagraphs from Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There are demographic biases present in current facial recognition (FR)models. To measure these biases across different ethnic and gender subgroups,we introduce our Balanced Faces in the Wild (BFW) dataset. This dataset allowsfor the characterization of FR performance per subgroup. We found that relyingon a single score threshold to differentiate between genuine and imposterssample pairs leads to suboptimal results. Additionally, performance withinsubgroups often varies significantly from the global average. Therefore,specific error rates only hold for populations that match the validation data.To mitigate imbalanced performances, we propose a novel domain adaptationlearning scheme that uses facial features extracted from state-of-the-artneural networks. This scheme boosts the average performance and preservesidentity information while removing demographic knowledge. Removing demographicknowledge prevents potential biases from affecting decision-making and protectsprivacy by eliminating demographic information. We explore the proposed methodand demonstrate that subgroup classifiers can no longer learn from featuresprojected using our domain adaptation scheme. For access to the source code anddata, please visit ", "output": "Balancing Biases and Preserving Privacy on Balanced Faces in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpreting objects with basic geometric primitives has long been studied incomputer vision. Among geometric primitives, superquadrics are well known fortheir ability to represent a wide range of shapes with few parameters. However,as the first and foremost step, recovering superquadrics accurately androbustly from 3D data still remains challenging. The existing methods aresubject to local optima and sensitive to noise and outliers in real-worldscenarios, resulting in frequent failure in capturing geometric shapes. In thispaper, we propose the first probabilistic method to recover superquadrics frompoint clouds. Our method builds a Gaussian-uniform mixture model (GUM) on theparametric surface of a superquadric, which explicitly models the generation ofoutliers and noise. The superquadric recovery is formulated as a MaximumLikelihood Estimation (MLE) problem. We propose an algorithm, Expectation,Maximization, and Switching (EMS), to solve this problem, where: (1) outliersare predicted from the posterior perspective; (2) the superquadric parameter isoptimized by the trust-region reflective algorithm; and (3) local optima areavoided by globally searching and switching among parameters encoding similarsuperquadrics. We show that our method can be extended to themulti-superquadrics recovery for complex objects. The proposed methodoutperforms the state-of-the-art in terms of accuracy, efficiency, androbustness on both synthetic and real-world datasets. The code is at<a href=\" http URL</a>", "output": "Robust and Accurate Superquadric Recovery: a Probabilistic Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The aim of this paper is to describe a novel non-parametric noise reductiontechnique from the point of view of Bayesian inference that may automaticallyimprove the signal-to-noise ratio of one- and two-dimensional data, such ase.g. astronomical images and spectra. The algorithm iteratively evaluatespossible smoothed versions of the data, the smooth models, obtaining anestimation of the underlying signal that is statistically compatible with thenoisy measurements. Iterations stop based on the evidence and the $chi^2$statistic of the last smooth model, and we compute the expected value of thesignal as a weighted average of the whole set of smooth models. In this paper,we explain the mathematical formalism and numerical implementation of thealgorithm, and we evaluate its performance in terms of the peak signal to noiseratio, the structural similarity index, and the time payload, using a batteryof real astronomical observations. Our Fully Adaptive Bayesian Algorithm forData Analysis (FABADA) yields results that, without any parameter tuning, arecomparable to standard image processing algorithms whose parameters have beenoptimized based on the true signal to be recovered, something that isimpossible in a real application. State-of-the-art non-parametric methods, suchas BM3D, offer slightly better performance at high signal-to-noise ratio, whileour algorithm is significantly more accurate for extremely noisy data (higherthan $20-40%$ relative errors, a situation of particular interest in the fieldof astronomy). In this range, the standard deviation of the residuals obtainedby our reconstruction may become more than an order of magnitude lower thanthat of the original measurements. The source code needed to reproduce all theresults presented in this report, including the implementation of the method,is publicly available at ", "output": "Fully Adaptive Bayesian Algorithm for Data Analysis, FABADA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline reinforcement learning has shown great promise in leveraging largepre-collected datasets for policy learning, allowing agents to forgooften-expensive online data collection. However, offline reinforcement learningfrom visual observations with continuous action spaces remains under-explored,with a limited understanding of the key challenges in this complex domain. Inthis paper, we establish simple baselines for continuous control in the visualdomain and introduce a suite of benchmarking tasks for offline reinforcementlearning from visual observations designed to better represent the datadistributions present in real-world offline RL problems and guided by a set ofdesiderata for offline RL from visual observations, including robustness tovisual distractions and visually identifiable changes in dynamics. Using thissuite of benchmarking tasks, we show that simple modifications to two popularvision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2,suffice to outperform existing offline RL methods and establish competitivebaselines for continuous control in the visual domain. We rigorously evaluatethese algorithms and perform an empirical evaluation of the differences betweenstate-of-the-art model-based and model-free offline RL methods for continuouscontrol from visual observations. All code and data used in this evaluation areopen-sourced to facilitate progress in this domain.", "output": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The occurrence of West Nile Virus (WNV) represents one of the most commonmosquito-borne zoonosis viral infections. Its circulation is usually associatedwith climatic and environmental conditions suitable for vector proliferationand virus replication. On top of that, several statistical models have beendeveloped to shape and forecast WNV circulation: in particular, the recentmassive availability of Earth Observation (EO) data, coupled with thecontinuous advances in the field of Artificial Intelligence, offer valuableopportunities.In this paper, we seek to predict WNV circulation by feeding Deep NeuralNetworks (DNNs) with satellite images, which have been extensively shown tohold environmental and climatic features. Notably, while previous approachesanalyze each geographical site independently, we propose a spatial-awareapproach that considers also the characteristics of close sites. Specifically,we build upon Graph Neural Networks (GNN) to aggregate features fromneighbouring places, and further extend these modules to consider multiplerelations, such as the difference in temperature and soil moisture between twosites, as well as the geographical distance. Moreover, we inject time-relatedinformation directly into the model to take into account the seasonality ofvirus spread.We design an experimental setting that combines satellite images - fromLandsat and Sentinel missions - with ground truth observations of WNVcirculation in Italy. We show that our proposed Multi-Adjacency Graph AttentionNetwork (MAGAT) consistently leads to higher performance when paired with anappropriate pre-training stage. Finally, we assess the importance of eachcomponent of MAGAT in our ablation studies.", "output": "Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large neural networks can now generate jokes, but do they really \"understand\"humor? We challenge AI models with three tasks derived from the New YorkerCartoon Caption Contest: matching a joke to a cartoon, identifying a winningcaption, and explaining why a winning caption is funny. These tasks encapsulateprogressively more sophisticated aspects of \"understanding\" a cartoon; keyelements are the complex, often surprising relationships between images andcaptions and the frequent inclusion of indirect and playful allusions to humanexperience and culture. We investigate both multimodal and language-onlymodels: the former are challenged with the cartoon images directly, while thelatter are given multifaceted descriptions of the visual scene to simulatehuman-level visual understanding. We find that both types of models struggle atall three tasks. For example, our best multimodal models fall 30 accuracypoints behind human performance on the matching task, and, even when providedground-truth visual scene descriptors, human-authored explanations arepreferred head-to-head over the best machine-authored ones (few-shot GPT-4) inmore than 2/3 of cases. We release models, code, leaderboard, and corpus, whichincludes newly-gathered annotations describing the image's locations/entities,what's unusual in the scene, and an explanation of the joke.", "output": "Do Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision transformer has emerged as a new paradigm in computer vision, showingexcellent performance while accompanied by expensive computational cost. Imagetoken pruning is one of the main approaches for ViT compression, due to thefacts that the complexity is quadratic with respect to the token number, andmany tokens containing only background regions do not truly contribute to thefinal prediction. Existing works either rely on additional modules to score theimportance of individual tokens, or implement a fixed ratio pruning strategyfor different input instances. In this work, we propose an adaptive sparsetoken pruning framework with a minimal cost. Specifically, we firstly proposean inexpensive attention head importance weighted class attention scoringmechanism. Then, learnable parameters are inserted as thresholds to distinguishinformative tokens from unimportant ones. By comparing token attention scoresand thresholds, we can discard useless tokens hierarchically and thusaccelerate inference. The learnable thresholds are optimized in budget-awaretraining to balance accuracy and complexity, performing the correspondingpruning configurations for different input instances. Extensive experimentsdemonstrate the effectiveness of our approach. Our method improves thethroughput of DeiT-S by 50% and brings only 0.2% drop in top-1 accuracy, whichachieves a better trade-off between accuracy and latency than the previousmethods.", "output": "Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With recent advancements in diffusion models, users can generate high-qualityimages by writing text prompts in natural language. However, generating imageswith desired details requires proper prompts, and it is often unclear how amodel reacts to different prompts or what the best prompts are. To helpresearchers tackle these critical challenges, we introduce DiffusionDB, thefirst large-scale text-to-image prompt dataset totaling 6.5TB, containing 14million images generated by Stable Diffusion, 1.8 million unique prompts, andhyperparameters specified by real users. We analyze the syntactic and semanticcharacteristics of prompts. We pinpoint specific hyperparameter values andprompt styles that can lead to model errors and present evidence of potentiallyharmful model usage, such as the generation of misinformation. Theunprecedented scale and diversity of this human-actuated dataset provideexciting research opportunities in understanding the interplay between promptsand generative models, detecting deepfakes, and designing human-AI interactiontools to help users more easily use these models. DiffusionDB is publiclyavailable at: ", "output": "DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-quality estimation of surface normal can help reduce ambiguity in manygeometry understanding problems, such as collision avoidance and occlusioninference. This paper presents a technique for estimating the normal from 3Dpoint clouds and 2D colour images. We have developed a transformer neuralnetwork that learns to utilise the hybrid information of visual semantic and 3Dgeometric data, as well as effective learning strategies. Compared to existingmethods, the information fusion of the proposed method is more effective, whichis supported by experiments. We have also built a simulation environment ofoutdoor traffic scenes in a 3D rendering engine to obtain annotated data totrain the normal estimator. The model trained on synthetic data is tested onthe real scenes in the KITTI dataset. And subsequent tasks built upon theestimated normal directions in the KITTI dataset show that the proposedestimator has advantage over existing methods.", "output": "Normal Transformer: Extracting Surface Geometry from LiDAR Points Enhanced by Visual Semantics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised image registration commonly adopts U-Net style networks topredict dense displacement fields in the full-resolution spatial domain. Forhigh-resolution volumetric image data, this process is howeverresource-intensive and time-consuming. To tackle this problem, we propose theFourier-Net, replacing the expansive path in a U-Net style network with aparameter-free model-driven decoder. Specifically, instead of our Fourier-Netlearning to output a full-resolution displacement field in the spatial domain,we learn its low-dimensional representation in a band-limited Fourier domain.This representation is then decoded by our devised model-driven decoder(consisting of a zero padding layer and an inverse discrete Fourier transformlayer) to the dense, full-resolution displacement field in the spatial domain.These changes allow our unsupervised Fourier-Net to contain fewer parametersand computational operations, resulting in faster inference speeds. Fourier-Netis then evaluated on two public 3D brain datasets against variousstate-of-the-art approaches. For example, when compared to a recenttransformer-based method, named TransMorph, our Fourier-Net, which only uses2.2% of its parameters and 6.66% of the multiply-add operations, achieves a0.5% higher Dice score and an 11.48 times faster inference speed. Code isavailable at url{", "output": "Fourier-Net: Fast Image Registration with Band-limited Deformation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation for extracting buildings and roads from uncrewed aerialvehicle (UAV) remote sensing images by deep learning becomes a more efficientand convenient method than traditional manual segmentation in surveying andmapping fields. In order to make the model lightweight and improve the modelaccuracy, a Lightweight Network Using Object Attention (LOANet) for Buildingsand Roads from UAV Aerial Remote Sensing Images is proposed. The proposednetwork adopts an encoder-decoder architecture in which a Lightweight DenselyConnected Network (LDCNet) is developed as the encoder. In the decoder part,the dual multi-scale context modules which consist of the Atrous SpatialPyramid Pooling module (ASPP) and the Object Attention Module (OAM) aredesigned to capture more context information from feature maps of UAV remotesensing images. Between ASPP and OAM, a Feature Pyramid Network (FPN) module isused to fuse multi-scale features extracted from ASPP. A private dataset ofremote sensing images taken by UAV which contains 2431 training sets, 945validation sets, and 475 test sets is constructed. The proposed basic modelperforms well on this dataset, with only 1.4M parameters and 5.48G floatingpoint operations (FLOPs), achieving excellent mean Intersection-over-Union(mIoU). Further experiments on the publicly available LoveDA and CITY-OSMdatasets have been conducted to further validate the effectiveness of theproposed basic and large model, and outstanding mIoU results have beenachieved. All codes are available on ", "output": "LOANet: A Lightweight Network Using Object Attention for Extracting Buildings and Roads from UAV Aerial Remote Sensing Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the past years, deep learning has seen an increase in usage in the domainof histopathological applications. However, while these approaches have showngreat potential, in high-risk environments deep learning models need to be ableto judge their uncertainty and be able to reject inputs when there is asignificant chance of misclassification. In this work, we conduct a rigorousevaluation of the most commonly used uncertainty and robustness methods for theclassification of Whole Slide Images, with a focus on the task of selectiveclassification, where the model should reject the classification in situationsin which it is uncertain. We conduct our experiments on tile-level under theaspects of domain shift and label noise, as well as on slide-level. In ourexperiments, we compare Deep Ensembles, Monte-Carlo Dropout, StochasticVariational Inference, Test-Time Data Augmentation as well as ensembles of thelatter approaches. We observe that ensembles of methods generally lead tobetter uncertainty estimates as well as an increased robustness towards domainshifts and label noise, while contrary to results from classical computervision benchmarks no systematic gain of the other methods can be shown. Acrossmethods, a rejection of the most uncertain samples reliably leads to asignificant increase in classification accuracy on both in-distribution as wellas out-of-distribution data. Furthermore, we conduct experiments comparingthese methods under varying conditions of label noise. Lastly, we publish ourcode framework to facilitate further research on uncertainty estimation onhistopathological data.", "output": "Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to accurately locate and navigate to a specific object is acrucial capability for embodied agents that operate in the real world andinteract with objects to complete tasks. Such object navigation tasks usuallyrequire large-scale training in visual environments with labeled objects, whichgeneralizes poorly to novel objects in unknown environments. In this work, wepresent a novel zero-shot object navigation method, Exploration with SoftCommonsense constraints (ESC), that transfers commonsense knowledge inpre-trained models to open-world object navigation without any navigationexperience nor any other training on the visual environments. First, ESCleverages a pre-trained vision and language model for open-world prompt-basedgrounding and a pre-trained commonsense language model for room and objectreasoning. Then ESC converts commonsense knowledge into navigation actions bymodeling it as soft logic predicates for efficient exploration. Extensiveexperiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC methodimproves significantly over baselines, and achieves new state-of-the-artresults for zero-shot object navigation (e.g., 288% relative Success Rateimprovement than CoW on MP3D).", "output": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To operate safely and efficiently alongside human workers, collaborativerobots (cobots) require the ability to quickly understand the dynamics ofmanipulated objects. However, traditional methods for estimating the full setof inertial parameters rely on motions that are necessarily fast and unsafe (toachieve a sufficient signal-to-noise ratio). In this work, we take analternative approach: by combining visual and force-torque measurements, wedevelop an inertial parameter identification algorithm that requires slow or'stop-and-go' motions only, and hence is ideally tailored for use aroundhumans. Our technique, called Homogeneous Part Segmentation (HPS), leveragesthe observation that man-made objects are often composed of distinct,homogeneous parts. We combine a surface-based point clustering method with avolumetric shape segmentation algorithm to quickly produce a part-levelsegmentation of a manipulated object; the segmented representation is then usedby HPS to accurately estimate the object's inertial parameters. To benchmarkour algorithm, we create and utilize a novel dataset consisting of realisticmeshes, segmented point clouds, and inertial parameters for 20 common workshoptools. Finally, we demonstrate the real-world performance and accuracy of HPSby performing an intricate 'hammer balancing act' autonomously and online witha low-cost collaborative robotic arm. Our code and dataset are open source andfreely available.", "output": "The Sum of Its Parts: Visual Part Segmentation for Inertial Parameter Identification of Manipulated Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Oriented object detection is one of the most fundamental and challengingtasks in remote sensing, aiming at locating the oriented objects of numerouspredefined object categories. Recently, deep learning based methods haveachieved remarkable performance in detecting oriented objects in optical remotesensing imagery. However, a thorough review of the literature in remote sensinghas not yet emerged. Therefore, we give a comprehensive survey of recentadvances and cover many aspects of oriented object detection, including problemdefinition, commonly used datasets, evaluation protocols, detection frameworks,oriented object representations, and feature representations. Besides, thestate-of-the-art methods are analyzed and discussed. We finally discuss futureresearch directions to put forward some useful research guidance. We believethat this survey shall be valuable to researchers across academia and industry", "output": "Oriented Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, flat minima are proven to be effective for improving generalizationand sharpness-aware minimization (SAM) achieves state-of-the-art performance.Yet the current definition of flatness discussed in SAM and its follow-ups arelimited to the zeroth-order flatness (i.e., the worst-case loss within aperturbation radius). We show that the zeroth-order flatness can beinsufficient to discriminate minima with low generalization error from thosewith high generalization error both when there is a single minimum or multipleminima within the given perturbation radius. Thus we present first-orderflatness, a stronger measure of flatness focusing on the maximal gradient normwithin a perturbation radius which bounds both the maximal eigenvalue ofHessian at local minima and the regularization function of SAM. We also presenta novel training procedure named Gradient norm Aware Minimization (GAM) to seekminima with uniformly small curvature across all directions. Experimentalresults show that GAM improves the generalization of models trained withcurrent optimizers such as SGD and AdamW on various datasets and networks.Furthermore, we show that GAM can help SAM find flatter minima and achievebetter generalization.", "output": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Novel view synthesis using neural radiance fields (NeRF) is thestate-of-the-art technique for generating high-quality images from novelviewpoints. Existing methods require a priori knowledge about extrinsic andintrinsic camera parameters. This limits their applicability to syntheticscenes, or real-world scenarios with the necessity of a preprocessing step.Current research on the joint optimization of camera parameters and NeRFfocuses on refining noisy extrinsic camera parameters and often relies on thepreprocessing of intrinsic camera parameters. Further approaches are limited tocover only one single camera intrinsic. To address these limitations, wepropose a novel end-to-end trainable approach called NeRFtrinsic Four. Weutilize Gaussian Fourier features to estimate extrinsic camera parameters anddynamically predict varying intrinsic camera parameters through the supervisionof the projection error. Our approach outperforms existing joint optimizationmethods on LLFF and BLEFF. In addition to these existing datasets, we introducea new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsicFour is a step forward in joint optimization NeRF-based view synthesis andenables more realistic and flexible rendering in real-world scenarios withvarying camera parameters.", "output": "NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing Diverse Intrinsic and Extrinsic Camera Parameters."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, end-to-end transformer-based detectors~(DETRs) have achievedremarkable performance. However, the issue of the high computational cost ofDETRs has not been effectively addressed, limiting their practical applicationand preventing them from fully exploiting the benefits of no post-processing,such as non-maximum suppression (NMS). In this paper, we first analyze theinfluence of NMS in modern real-time object detectors on inference speed, andestablish an end-to-end speed benchmark. To avoid the inference delay caused byNMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the firstreal-time end-to-end object detector to our best knowledge. Specifically, wedesign an efficient hybrid encoder to efficiently process multi-scale featuresby decoupling the intra-scale interaction and cross-scale fusion, and proposeIoU-aware query selection to improve the initialization of object queries. Inaddition, our proposed detector supports flexibly adjustment of the inferencespeed by using different decoder layers without the need for retraining, whichfacilitates the practical application of real-time object detectors. OurRT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, whileRT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of thesame scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP inaccuracy and by about 21 times in FPS. ource code and pre-trained models areavailable at ", "output": "DETRs Beat YOLOs on Real-time Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human beings cooperatively navigate rule-constrained environments by adheringto mutually known navigational patterns, which may be represented asdirectional pathways or road lanes. Inferring these navigational patterns fromincompletely observed environments is required for intelligent mobile robotsoperating in unmapped locations. However, algorithmically defining thesenavigational patterns is nontrivial. This paper presents the firstself-supervised learning (SSL) method for learning to infer navigationalpatterns in real-world environments from partial observations only. We explainhow geometric data augmentation, predictive world modeling, and aninformation-theoretic regularizer enables our model to predict an unbiasedlocal directional soft lane probability (DSLP) field in the limit of infinitedata. We demonstrate how to infer global navigational patterns by fitting amaximum likelihood graph to the DSLP field. Experiments show that our SSL modeloutperforms two SOTA supervised lane graph prediction models on the nuScenesdataset. We propose our SSL method as a scalable and interpretable continuallearning paradigm for navigation by perception. Code is available at", "output": "Learning to Predict Navigational Patterns from Partial Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Controllable image captioning is an emerging multimodal topic that aims todescribe the image with natural language following human purpose,$textit{e.g.}$, looking at the specified regions or telling in a particulartext style. State-of-the-art methods are trained on annotated pairs of inputcontrols and output captions. However, the scarcity of such well-annotatedmultimodal data largely limits their usability and scalability for interactiveAI systems. Leveraging unimodal instruction-following foundation models is apromising alternative that benefits from broader sources of data. In thispaper, we present Caption AnyThing (CAT), a foundation model augmented imagecaptioning framework supporting a wide range of multimodel controls: 1) visualcontrols, including points, boxes, and trajectories; 2) language controls, suchas sentiment, length, language, and factuality. Powered by Segment AnythingModel (SAM) and ChatGPT, we unify the visual and language prompts into amodularized framework, enabling the flexible combination between differentcontrols. Extensive case studies demonstrate the user intention alignmentcapabilities of our framework, shedding light on effective user interactionmodeling in vision-language applications. Our code is publicly available at", "output": "Caption Anything: Interactive Image Description with Diverse Multimodal Controls."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing knowledge distillation works for semantic segmentation mainly focuson transferring high-level contextual knowledge from teacher to student.However, low-level texture knowledge is also of vital importance forcharacterizing the local structural pattern and global statistical property,such as boundary, smoothness, regularity and color contrast, which may not bewell addressed by high-level deep features. In this paper, we are intended totake full advantage of both structural and statistical texture knowledge andpropose a novel Structural and Statistical Texture Knowledge Distillation(SSTKD) framework for semantic segmentation. Specifically, for structuraltexture knowledge, we introduce a Contourlet Decomposition Module (CDM) thatdecomposes low-level features with iterative Laplacian pyramid and directionalfilter bank to mine the structural texture knowledge. For statisticalknowledge, we propose a Denoised Texture Intensity Equalization Module (DTIEM)to adaptively extract and enhance statistical texture knowledge throughheuristics iterative quantization and denoised operation. Finally, eachknowledge learning is supervised by an individual loss function, forcing thestudent network to mimic the teacher better from a broader perspective.Experiments show that the proposed method achieves state-of-the-art performanceon Cityscapes, Pascal VOC 2012 and ADE20K datasets.", "output": "Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene flow estimation is the task of describing the 3D motion field betweentemporally successive point clouds. State-of-the-art methods use strong priorsand test-time optimization techniques, but require on the order of tens ofseconds for large-scale point clouds, making them unusable as computer visionprimitives for real-time applications such as open world object detection. Feedforward methods are considerably faster, running on the order of tens tohundreds of milliseconds for large-scale point clouds, but require expensivehuman supervision. To address both limitations, we propose Scene Flow viaDistillation, a simple distillation framework that uses a label-freeoptimization method to produce pseudo-labels to supervise a feed forward model.Our instantiation of this framework, ZeroFlow, produces scene flow estimates inreal-time on large-scale point clouds at quality competitive withstate-of-the-art methods while using zero human labels. Notably, at test-timeZeroFlow is over 1000$times$ faster than label-free state-of-the-artoptimization-based methods on large-scale point clouds and over 1000$times$cheaper to train on unlabeled data compared to the cost of human annotation ofthat data. To facilitate research reuse, we release our code, trained modelweights, and high quality pseudo-labels for the Argoverse 2 and Waymo Opendatasets.", "output": "ZeroFlow: Fast Zero Label Scene Flow via Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Photometric calibration is essential to many computer vision applications.One of its key benefits is enhancing the performance of Visual SLAM, especiallywhen it depends on a direct method for tracking, such as the standard KLTalgorithm. Another advantage could be in retrieving the sensor irradiancevalues from measured intensities, as a pre-processing step for some visionalgorithms, such as shape-from-shading. Current photometric calibration systemsrely on a joint optimization problem and encounter an ambiguity in theestimates, which can only be resolved using ground truth information. Wepropose a novel method that solves for photometric parameters using asequential estimation approach. Our proposed method achieves high accuracy inestimating all parameters; furthermore, the formulations are linear and convex,which makes the solution fast and suitable for online applications. Experimentson a Visual Odometry system validate the proposed method and demonstrate itsadvantages.", "output": "OSPC: Online Sequential Photometric Calibration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "DDIM inversion has revealed the remarkable potential of real image editingwithin diffusion-based methods. However, the accuracy of DDIM reconstructiondegrades as larger classifier-free guidance (CFG) scales being used forenhanced editing. Null-text inversion (NTI) optimizes null embeddings to alignthe reconstruction and inversion trajectories with larger CFG scales, enablingreal image editing with cross-attention control. Negative-prompt inversion(NPI) further offers a training-free closed-form solution of NTI. However, itmay introduce artifacts and is still constrained by DDIM reconstructionquality. To overcome these limitations, we propose proximal guidance andincorporate it to NPI with cross-attention control. We enhance NPI with aregularization term and reconstruction guidance, which reduces artifacts whilecapitalizing on its training-free nature. Additionally, we extend the conceptsto incorporate mutual self-attention control, enabling geometry and layoutalterations in the editing process. Our method provides an efficient andstraightforward approach, effectively addressing real image editing tasks withminimal computational overhead.", "output": "Improving Tuning-Free Real Image Editing with Proximal Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study investigates the transformative potential of Large Language Models(LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of publicdata, these models, which possess remarkable language understanding andgeneration capabilities, are augmenting the interpretive skills ofradiologists, enhancing patient-physician communication, and streamliningclinical workflows. The paper introduces an analytic framework for presentingthe complex interactions between LLMs and the broader ecosystem of medicalimaging stakeholders, including businesses, insurance entities, governments,research institutions, and hospitals (nicknamed BIGR-H). Through detailedanalyses, illustrative use cases, and discussions on the broader implicationsand future directions, this perspective seeks to raise discussion in strategicplanning and decision-making in the era of AI-enabled healthcare.", "output": "The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video Anomaly Detection (VAD) is an essential yet challenging task in signalprocessing. Since certain anomalies cannot be detected by isolated analysis ofeither temporal or spatial information, the interaction between these two typesof data is considered crucial for VAD. However, current dual-streamarchitectures either confine this integral interaction to the bottleneck of theautoencoder or introduce anomaly-irrelevant background pixels into theinteractive process, hindering the accuracy of VAD. To address thesedeficiencies, we propose a Multi-scale Spatial-Temporal Interaction Network(MSTI-Net) for VAD. First, to prioritize the detection of moving objects in thescene and harmonize the substantial semantic discrepancies between the twotypes of data, we propose an Attention-based Spatial-Temporal Fusion Module(ASTFM) as a substitute for the conventional direct fusion. Furthermore, weinject multi-ASTFM-based connections that bridge the appearance and motionstreams of the dual-stream network, thus fostering multi-scale spatial-temporalinteraction. Finally, to bolster the delineation between normal and abnormalactivities, our system records the regular information in a memory module.Experimental results on three benchmark datasets validate the effectiveness ofour approach, which achieves AUCs of 96.8%, 87.6%, and 73.9% on the UCSD Ped2,CUHK Avenue, and ShanghaiTech datasets, respectively.", "output": "Multi-scale Spatial-temporal Interaction Network for Video Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Synthesising a text-to-image model of high-quality images by guiding thegenerative model through the Text description is an innovative and challengingtask. In recent years, AttnGAN based on the Attention mechanism to guide GANtraining has been proposed, SD-GAN, which adopts a self-distillation techniqueto improve the performance of the generator and the quality of imagegeneration, and Stack-GAN++, which gradually improves the details and qualityof the image by stacking multiple generators and discriminators. However, thisseries of improvements to GAN all have redundancy to a certain extent, whichaffects the generation performance and complexity to a certain extent. We usethe popular simple and effective idea (1) to remove redundancy structure andimprove the backbone network of AttnGAN. (2) to integrate and reconstructmultiple losses of DAMSM. Our improvements have significantly improved themodel size and training efficiency while ensuring that the model's performanceis unchanged and finally proposed our SEAttnGAN. Code is avalilable at", "output": "A Simple and Effective Baseline for Attentional Generative Adversarial Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a novel approach to address the challenges ofprinted Urdu text recognition using high-resolution, multi-scale semanticfeature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model,demonstrates state-of-the-art performance on benchmark datasets. To address thelimitations of previous works, which struggle to generalize to the intricaciesof the Urdu script and the lack of sufficient annotated real-world data, wehave introduced the UTRSet-Real, a large-scale annotated real-world datasetcomprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000lines closely resembling real-world and made corrections to the ground truth ofthe existing IIITH dataset, making it a more reliable resource for futureresearch. We also provide UrduDoc, a benchmark dataset for Urdu text linedetection in scanned documents. Additionally, we have developed an online toolfor end-to-end Urdu OCR from printed documents by integrating UTRNet with atext detection model. Our work not only addresses the current limitations ofUrdu OCR but also paves the way for future research in this area andfacilitates the continued advancement of Urdu OCR technology. The project pagewith source code, datasets, annotations, trained models, and online tool isavailable at abdur75648.github.io/UTRNet.", "output": "UTRNet: High-Resolution Urdu Text Recognition In Printed Documents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of visual scene understanding, deep neural networks have madeimpressive advancements in various core tasks like segmentation, tracking, anddetection. However, most approaches operate on the close-set assumption,meaning that the model can only identify pre-defined categories that arepresent in the training set. Recently, open vocabulary settings were proposeddue to the rapid progress of vision language pre-training. These new approachesseek to locate and recognize categories beyond the annotated label space. Theopen vocabulary approach is more general, practical, and effective compared toweakly supervised and zero-shot settings. This paper provides a thorough reviewof open vocabulary learning, summarizing and analyzing recent developments inthe field. In particular, we begin by comparing it to related concepts such aszero-shot learning, open-set recognition, and out-of-distribution detection.Then, we review several closely related tasks in the case of segmentation anddetection, including long-tail problems, few-shot, and zero-shot settings. Forthe method survey, we first present the basic knowledge of detection andsegmentation in close-set as the preliminary knowledge. Next, we examinevarious scenarios in which open vocabulary learning is used, identifying commondesign elements and core ideas. Then, we compare the recent detection andsegmentation approaches in commonly used datasets and benchmarks. Finally, weconclude with insights, issues, and discussions regarding future researchdirections. To our knowledge, this is the first comprehensive literature reviewof open vocabulary learning. We keep tracing related works at", "output": "Towards Open Vocabulary Learning: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While natural language offers a convenient shared interface for humans androbots, enabling robots to interpret and follow language commands remains alongstanding challenge in manipulation. A crucial step to realizing aperformant instruction-following robot is achieving semantic manipulation,where a robot interprets language at different specificities, from high-levelinstructions like \"Pick up the stuffed animal\" to more detailed inputs like\"Grab the left ear of the elephant.\" To tackle this, we propose Keypoints +Instructions to Execution (KITE), a two-step framework for semanticmanipulation which attends to both scene semantics (distinguishing betweendifferent objects in a visual scene) and object semantics (precisely localizingdifferent parts within an object instance). KITE first grounds an inputinstruction in a visual scene through 2D image keypoints, providing a highlyaccurate object-centric bias for downstream action inference. Provided an RGB-Dscene observation, KITE then executes a learned keypoint-conditioned skill tocarry out the instruction. The combined precision of keypoints andparameterized skills enables fine-grained manipulation with generalization toscene and object variations. Empirically, we demonstrate KITE in 3 real-worldenvironments: long-horizon 6-DoF tabletop manipulation, semantic grasping, anda high-precision coffee-making task. In these settings, KITE achieves a 75%,70%, and 71% overall success rate for instruction-following, respectively. KITEoutperforms frameworks that opt for pre-trained visual language models overkeypoint-based grounding, or omit skills in favor of end-to-end visuomotorcontrol, all while being trained from fewer or comparable amounts ofdemonstrations. Supplementary material, datasets, code, and videos can be foundon our website: <a href=\" http URL</a>", "output": "KITE: Keypoint-Conditioned Policies for Semantic Manipulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperbole, or exaggeration, is a common linguistic phenomenon. The detectionof hyperbole is an important part of understanding human expression. There havebeen several studies on hyperbole detection, but most of which focus on textmodality only. However, with the development of social media, people can createhyperbolic expressions with various modalities, including text, images, videos,etc. In this paper, we focus on multimodal hyperbole detection. We create amultimodal detection datasetfootnote{The dataset will be released to thecommunity.} from Weibo (a Chinese social media) and carry out some studies onit. We treat the text and image from a piece of weibo as two modalities andexplore the role of text and image for hyperbole detection. Differentpre-trained multimodal encoders are also evaluated on this downstream task toshow their performance. Besides, since this dataset is constructed from fivedifferent topics, we also evaluate the cross-domain performance of differentmodels. These studies can serve as a benchmark and point out the direction offurther study on multimodal hyperbole detection.", "output": "Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Massive captured face images are stored in the database for theidentification of individuals. However, the stored images can be observedintentionally or unintentionally by data managers, which is not at the will ofindividuals and may cause privacy violations. Existing protection works onlyslightly change the visual content of the face while maintaining the utility ofidentification, making it susceptible to the inference of the true identity byhuman vision. In this paper, we propose an identity hider that enablessignificant visual content change for human vision while preserving highidentifiability for face recognizers. Firstly, the identity hider generates avirtual face with new visual content by manipulating the latent space inStyleGAN2. In particular, the virtual face has the same irrelevant attributesas the original face, e.g., pose and expression. Secondly, the visual contentof the virtual face is transferred into the original face and then thebackground is replaced with the original one. In addition, the identity hiderhas strong transferability, which ensures an arbitrary face recognizer canachieve satisfactory accuracy. Adequate experiments show that the proposedidentity hider achieves excellent performance on privacy protection andidentifiability preservation.", "output": "Seeing is not Believing: An Identity Hider for Human Vision Privacy Protection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most existing ultra-high resolution (UHR) segmentation methods alwaysstruggle in the dilemma of balancing memory cost and local characterizationaccuracy, which are both taken into account in our proposed GuidedPatch-Grouping Wavelet Transformer (GPWFormer) that achieves impressiveperformances. In this work, GPWFormer is a Transformer ($mathcal{T}$)-CNN($mathcal{C}$) mutual leaning framework, where $mathcal{T}$ takes the wholeUHR image as input and harvests both local details and fine-grained long-rangecontextual dependencies, while $mathcal{C}$ takes downsampled image as inputfor learning the category-wise deep context. For the sake of high inferencespeed and low computation complexity, $mathcal{T}$ partitions the original UHRimage into patches and groups them dynamically, then learns the low-level localdetails with the lightweight multi-head Wavelet Transformer (WFormer) network.Meanwhile, the fine-grained long-range contextual dependencies are alsocaptured during this process, since patches that are far away in the spatialdomain can also be assigned to the same group. In addition, masks produced by$mathcal{C}$ are utilized to guide the patch grouping process, providing aheuristics decision. Moreover, the congruence constraints between the twobranches are also exploited to maintain the spatial consistency among thepatches. Overall, we stack the multi-stage process in a pyramid way.Experiments show that GPWFormer outperforms the existing methods withsignificant improvements on five benchmark datasets.", "output": "Guided Patch-Grouping Wavelet Transformer with Spatial Congruence for Ultra-High Resolution Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative latent diffusion models have been established as state-of-the-artin data generation. One promising application is generation of realisticsynthetic medical imaging data for open data sharing without compromisingpatient privacy. Despite the promise, the capacity of such models to memorizesensitive patient training data and synthesize samples showing high resemblanceto training data samples is relatively unexplored. Here, we assess thememorization capacity of 3D latent diffusion models on photon-counting coronarycomputed tomography angiography and knee magnetic resonance imaging datasets.To detect potential memorization of training samples, we utilizeself-supervised models based on contrastive learning. Our results suggest thatsuch latent diffusion models indeed memorize training data, and there is a direneed for devising strategies to mitigate memorization.", "output": "Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The application of mobile biometrics as a user-friendly authentication methodhas increased in the last years. Recent studies have proposed novel behavioralbiometric recognition systems based on Transformers, which currently outperformthe state of the art in several application scenarios. On-line handwrittensignature verification aims to verify the identity of subjects, based on theirbiometric signatures acquired using electronic devices such as tablets orsmartphones. This paper investigates the suitability of architectures based onrecent Transformers for on-line signature verification. In particular, fourdifferent configurations are studied, two of them rely on the VanillaTransformer encoder, and the two others have been successfully applied to thetasks of gait and activity recognition. We evaluate the four proposedconfigurations according to the experimental protocol proposed in theSVC-onGoing competition. The results obtained in our experiments are promising,and promote the use of Transformers for on-line signature verification.", "output": "Exploring Transformers for On-Line Handwritten Signature Verification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging(MRI) images from partial measurement is essential to medical imaging research.Benefiting from the diverse and complementary information of multi-contrast MRimages in different imaging modalities, multi-contrast Super-Resolution (SR)reconstruction is promising to yield SR images with higher quality. In themedical scenario, to fully visualize the lesion, radiologists are accustomed tozooming the MR images at arbitrary scales rather than using a fixed scale, asused by most MRI SR methods. In addition, existing multi-contrast MRI SRmethods often require a fixed resolution for the reference image, which makesacquiring reference images difficult and imposes limitations on arbitrary scaleSR tasks. To address these issues, we proposed an implicit neuralrepresentations based dual-arbitrary multi-contrast MRI super-resolutionmethod, called Dual-ArbNet. First, we decouple the resolution of the target andreference images by a feature encoder, enabling the network to input target andreference images at arbitrary scales. Then, an implicit fusion decoder fusesthe multi-contrast features and uses an Implicit Decoding Function~(IDF) toobtain the final MRI SR results. Furthermore, we introduce a curriculumlearning strategy to train our network, which improves the generalization andperformance of our Dual-ArbNet. Extensive experiments in two public MRIdatasets demonstrate that our method outperforms state-of-the-art approachesunder different scale factors and has great potential in clinical practice.", "output": "Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Equipping embodied agents with commonsense is important for robots tosuccessfully complete complex human instructions in general environments.Recent large language models (LLM) can embed rich semantic knowledge for agentsin plan generation of complex tasks, while they lack the information about therealistic world and usually yield infeasible action sequences. In this paper,we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planningwith physical scene constraint, where the agent generates executable plansaccording to the existed objects in the scene by aligning LLMs with the visualperception models. Specifically, we first construct a multimodal datasetcontaining triplets of indoor scenes, instructions and action plans, where weprovide the designed prompts and the list of existing objects in the scene forGPT-3.5 to generate a large number of instructions and corresponding plannedactions. The generated data is leveraged for grounded plan tuning ofpre-trained LLMs. During inference, we discover the objects in the scene byextending open-vocabulary object detectors to multi-view RGB images collectedin different achievable locations. Experimental results show that the generatedplan from our TaPA framework can achieve higher success rate than LLaVA andGPT-3.5 by a sizable margin, which indicates the practicality of embodied taskplanning in general and complex environments.", "output": "Embodied Task Planning with Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The exploration of few-shot tabular learning becomes imperative. Tabular datais a versatile representation that captures diverse information, yet it is notexempt from limitations, property of data and model size. Labeling extensivetabular data can be challenging, and it may not be feasible to capture everyimportant feature. Few-shot tabular learning, however, remains relativelyunexplored, primarily due to scarcity of shared information among independentdatasets and the inherent ambiguity in defining boundaries within tabular data.To the best of our knowledge, no meaningful and unrestricted few-shot tabularlearning techniques have been developed without imposing constraints on thedataset. In this paper, we propose an innovative framework called TablEye,which aims to overcome the limit of forming prior knowledge for tabular data byadopting domain transformation. It facilitates domain transformation bygenerating tabular images, which effectively conserve the intrinsic semanticsof the original tabular data. This approach harnesses rigorously testedfew-shot learning algorithms and embedding functions to acquire and apply priorknowledge. Leveraging shared data domains allows us to utilize this priorknowledge, originally learned from the image domain. Specifically, TablEyedemonstrated a superior performance by outstripping the TabLLM in a 4-shot taskwith a maximum 0.11 AUC and a STUNT in a 1- shot setting, where it led onaverage by 3.17% accuracy.", "output": "TablEye: Seeing small Tables through the Lens of Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "From a service perspective, Multi-Source Domain Adaptation (MSDA) is apromising scenario to adapt a deployed model to a client's dataset. It canprovide adaptation without a target label and support the case where a sourcedataset is constructed from multiple domains. However, it is impractical,wherein its training heavily relies on prior domain information of themulti-source dataset -- how many domains exist and the domain label of eachdata sample. Moreover, MSDA requires both source and target datasetssimultaneously (physically), causing storage limitations on the client deviceor data privacy issues by transferring client data to a server. For a morepractical scenario of model adaptation from a service provider's point of view,we relax these constraints and present a novel problem scenario of Three-FreeDomain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, andmostly 3) source domain information (domain labels + the number of domains) areunavailable. Under the problem scenario, we propose a practical adaptationframework called FREEDOM. It leverages the power of the generative model,disentangling data into class and style aspects, where the style is defined asthe class-independent information from the source data and designed with anonparametric Bayesian approach. In the adaptation stage, FREEDOM aims to matchthe source class distribution with the target's under the philosophy that classdistribution is consistent even if the style is different; after then, onlypart of the classification model is deployed as a personalized network. As aresult, FREEDOM achieves state-of-the-art or comparable performance evenwithout domain information, with reduced final model size on the target side,independent of the number of source domains.", "output": "FREEDOM: Target Label & Source Data & Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electrolysis is crucial for eco-friendly hydrogen production, but gas bubblesgenerated during the process hinder reactions, reduce cell efficiency, andincrease energy consumption. Additionally, these gas bubbles cause changes inthe conductivity inside the cell, resulting in corresponding variations in theinduced magnetic field around the cell. Therefore, measuring these gasbubble-induced magnetic field fluctuations using external magnetic sensors andsolving the inverse problem of Biot-Savart Law allows for estimating theconductivity in the cell and, thus, bubble size and location. However,determining high-resolution conductivity maps from only a few induced magneticfield measurements is an ill-posed inverse problem. To overcome this, weexploit Invertible Neural Networks (INNs) to reconstruct the conductivityfield. Our qualitative results and quantitative evaluation using random errordiffusion show that INN achieves far superior performance compared to Tikhonovregularization.", "output": "Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tackling the difficult problem of estimating spatially distributedhydrological parameters, especially for floods on ungauged watercourses, thiscontribution presents a novel seamless regionalization technique for learningcomplex regional transfer functions designed for high-resolution hydrologicalmodels. The transfer functions rely on: (i) a multilayer perceptron enabling aseamless flow of gradient computation to employ machine learning optimizationalgorithms, or (ii) a multivariate regression mapping optimized by variationaldata assimilation algorithms and guided by Bayesian estimation, addressing theequifinality issue of feasible solutions. The approach involves incorporatingthe inferable regionalization mappings into a differentiable hydrological modeland optimizing a cost function computed on multi-gauge data with accurateadjoint-based spatially distributed gradients.", "output": "Multi-gauge Hydrological Variational Data Assimilation: Regionalization Learning with Spatial Gradients using Multilayer Perceptron and Bayesian-Guided Multivariate Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Algorithm- and data-dependent generalization bounds are required to explainthe generalization behavior of modern machine learning algorithms. In thiscontext, there exists information theoretic generalization bounds that involve(various forms of) mutual information, as well as bounds based on hypothesisset stability. We propose a conceptually related, but technically distinctcomplexity measure to control generalization error, which is the empiricalRademacher complexity of an algorithm- and data-dependent hypothesis class.Combining standard properties of Rademacher complexity with the convenientstructure of this class, we are able to (i) obtain novel bounds based on thefinite fractal dimension, which (a) extend previous fractal dimension-typebounds from continuous to finite hypothesis classes, and (b) avoid a mutualinformation term that was required in prior work; (ii) we greatly simplify theproof of a recent dimension-independent generalization bound for stochasticgradient descent; and (iii) we easily recover results for VC classes andcompression schemes, similar to approaches based on conditional mutualinformation.", "output": "Generalization Guarantees via Algorithm-dependent Rademacher Complexity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficiently capturing the complex spatiotemporal representations fromlarge-scale unlabeled traffic data remains to be a challenging task. Inconsidering of the dilemma, this work employs the advanced contrastive learningand proposes a novel Spatial-Temporal Synchronous Contextual ContrastiveLearning (STS-CCL) model. First, we elaborate the basic and strong augmentationmethods for spatiotemporal graph data, which not only perturb the data in termsof graph structure and temporal characteristics, but also employ alearning-based dynamic graph view generator for adaptive augmentation. Second,we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) tosimultaneously capture the decent spatial-temporal dependencies and realizegraph-level contrasting. To further discriminate node individuals in negativefiltering, a Semantic Contextual Contrastive method is designed based onsemantic features and spatial heterogeneity, achieving node-level contrastivelearning along with negative filtering. Finally, we present a hard mutual-viewcontrastive training scheme and extend the classic contrastive loss to anintegrated objective function, yielding better performance. Extensiveexperiments and evaluations demonstrate that building a predictor upon STS-CCLcontrastive learning model gains superior performance than existing trafficforecasting benchmarks. The proposed STS-CCL is highly suitable for largedatasets with only a few labeled data and other spatiotemporal tasks with datascarcity issue.", "output": "STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a computational framework for the Wassersteinauto-encoding of merge trees (MT-WAE), a novel extension of the classicalauto-encoder neural network architecture to the Wasserstein metric space ofmerge trees. In contrast to traditional auto-encoders which operate onvectorized data, our formulation explicitly manipulates merge trees on theirassociated metric space at each layer of the network, resulting in superioraccuracy and interpretability. Our novel neural network approach can beinterpreted as a non-linear generalization of previous linear attempts [65] atmerge tree encoding. It also trivially extends to persistence diagrams.Extensive experiments on public ensembles demonstrate the efficiency of ouralgorithms, with MT-WAE computations in the orders of minutes on average. Weshow the utility of our contributions in two applications adapted from previouswork on merge tree encoding [65]. First, we apply MT-WAE to data reduction andreliably compress merge trees by concisely representing them with theircoordinates in the final layer of our auto-encoder. Second, we document anapplication to dimensionality reduction, by exploiting the latent space of ourauto-encoder, for the visual analysis of ensemble data. We illustrate theversatility of our framework by introducing two penalty terms, to help preservein the latent space both the Wasserstein distances between merge trees, as wellas their clusters. In both applications, quantitative experiments assess therelevance of our framework. Finally, we provide a C++ implementation that canbe used for reproducibility.", "output": "Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "AI Image generators based on diffusion models are widely discussed recentlyfor their capability to create images from simple text prompts. But, forpractical use in civil engineering they need to be able to create specificconstruction plans for given constraints. Within this paper we explore thecapabilities of those diffusion-based AI generators for computational design atthe example of floor plans and identify their current limitation. We explainhow the diffusion-models work and propose new diffusion models with improvedsemantic encoding. In several experiments we show that we can improve validityof generated floor plans from 6% to 90% and query performance for differentexamples. We identify short comings and derive future research challenges ofthose models and discuss the need to combine diffusion models with buildinginformation modelling. With this we provide key insights into the current stateand future directions for diffusion models in civil engineering.", "output": "Diffusion Models for Computational Design at the Example of Floor Plans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Independently trained machine learning models tend to learn similar features.Given an ensemble of independently trained models, this results in correlatedpredictions and common failure modes. Previous attempts focusing ondecorrelation of output predictions or logits yielded mixed results,particularly due to their reduction in model accuracy caused by conflictingoptimization objectives. In this paper, we propose the novel idea of utilizingmethods of the representational similarity field to promote dissimilarityduring training instead of measuring similarity of trained models. To this end,we promote intermediate representations to be dissimilar at different depthsbetween architectures, with the goal of learning robust ensembles with disjointfailure modes. We show that highly dissimilar intermediate representationsresult in less correlated output predictions and slightly lower errorconsistency, resulting in higher ensemble accuracy. With this, we shine firstlight on the connection between intermediate representations and their impacton the output predictions.", "output": "Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conditional independence (CI) testing is fundamental and challenging inmodern statistics and machine learning. Many modern methods for CI testing relyon powerful supervised learning methods to learn regression functions or Bayespredictors as an intermediate step. Although the methods are guaranteed tocontrol Type-I error when the supervised learning methods accurately estimatethe regression functions or Bayes predictors, their behavior is less understoodwhen they fail due to model misspecification. In a broader sense, modelmisspecification can arise even when universal approximators (e.g., deep neuralnets) are employed. Then, we study the performance of regression-based CI testsunder model misspecification. Namely, we propose new approximations or upperbounds for the testing errors of three regression-based tests that depend onmisspecification errors. Moreover, we introduce the Rao-Blackwellized PredictorTest (RBPT), a novel regression-based CI test robust against modelmisspecification. Finally, we conduct experiments with artificial and realdata, showcasing the usefulness of our theory and methods.", "output": "Conditional independence testing under model misspecification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a methodology for improving the accuracy of surrogate models ofthe observable response of physical systems as a function of the systems'spatially heterogeneous parameter fields with applications to uncertaintyquantification and parameter estimation in high-dimensional problems.Practitioners often formulate finite-dimensional representations of spatiallyheterogeneous parameter fields using truncated unconditional Karhunen-Lo'{e}veexpansions (KLEs) for a certain choice of unconditional covariance kernel andconstruct surrogate models of the observable response with respect to therandom variables in the KLE. When direct measurements of the parameter fieldsare available, we propose improving the accuracy of these surrogate models byrepresenting the parameter fields via conditional Karhunen-Lo'{e}ve expansions(CKLEs). CKLEs are constructed by conditioning the covariance kernel of theunconditional expansion on the direct measurements via Gaussian processregression and then truncating the corresponding KLE. We apply the proposedmethodology to constructing surrogate models via the Basis Adaptation (BA)method of the stationary hydraulic head response, measured at spatiallydiscrete observation locations, of a groundwater flow model of the HanfordSite, as a function of the 1,000-dimensional representation of the model'slog-transmissivity field. We find that BA surrogate models of the hydraulichead based on CKLEs are more accurate than BA surrogate models based onunconditional expansions for forward uncertainty quantification tasks.Furthermore, we find that inverse estimates of the hydraulic transmissivityfield computed using CKLE-based BA surrogate models are more accurate thanthose computed using unconditional BA surrogate models.", "output": "Conditional Korhunen-Lo\\'{e}ve regression model with Basis Adaptation for high-dimensional problems: uncertainty quantification and inverse modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate building height estimation is key to the automatic derivation of 3Dcity models from emerging big geospatial data, including VolunteeredGeographical Information (VGI). However, an automatic solution for large-scalebuilding height estimation based on low-cost VGI data is currently missing. Thefast development of VGI data platforms, especially OpenStreetMap (OSM) andcrowdsourced street-view images (SVI), offers a stimulating opportunity to fillthis research gap. In this work, we propose a semi-supervised learning (SSL)method of automatically estimating building height from Mapillary SVI and OSMdata to generate low-cost and open-source 3D city modeling in LoD1. Theproposed method consists of three parts: first, we propose an SSL schema withthe option of setting a different ratio of \"pseudo label\" during the supervisedregression; second, we extract multi-level morphometric features from OSM data(i.e., buildings and streets) for the purposed of inferring building height;last, we design a building floor estimation workflow with a pre-trained facadeobject detection network to generate \"pseudo label\" from SVI and assign it tothe corresponding OSM building footprint. In a case study, we validate theproposed SSL method in the city of Heidelberg, Germany and evaluate the modelperformance against the reference data of building heights. Based on threedifferent regression models, namely Random Forest (RF), Support Vector Machine(SVM), and Convolutional Neural Network (CNN), the SSL method leads to a clearperformance boosting in estimating building heights with a Mean Absolute Error(MAE) around 2.1 meters, which is competitive to state-of-the-art approaches.The preliminary result is promising and motivates our future work in scaling upthe proposed method based on low-cost VGI data, with possibilities in evenregions and areas with diverse data quality and availability.", "output": "Semi-supervised Learning from Street-View Images and OpenStreetMap for Automatic Building Height Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Satellite Earth observations (EO) can provide affordable and timelyinformation for assessing crop conditions and food production. Such monitoringsystems are essential in Africa, where there is high food insecurity and sparseagricultural statistics. EO-based monitoring systems require accurate croplandmaps to provide information about croplands, but there is a lack of data todetermine which of the many available land cover maps most accurately identifycropland in African countries. This study provides a quantitative evaluationand intercomparison of 11 publicly available land cover maps to assess theirsuitability for cropland classification and EO-based agriculture monitoring inAfrica using statistically rigorous reference datasets from 8 countries. Wehope the results of this study will help users determine the most suitable mapfor their needs and encourage future work to focus on resolving inconsistenciesbetween maps and improving accuracy in low-accuracy regions.", "output": "How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal demand forecasting aims at predicting product demand utilizingvisual, textual, and contextual information. This paper proposes a method formultimodal product demand forecasting using convolutional, graph-based, andtransformer-based architectures. Traditional approaches to demand forecastingrely on historical demand, product categories, and additional contextualinformation such as seasonality and events. However, these approaches haveseveral shortcomings, such as the cold start problem making it difficult topredict product demand until sufficient historical data is available for aparticular product, and their inability to properly deal with categorydynamics. By incorporating multimodal information, such as product images andtextual descriptions, our architecture aims to address the shortcomings oftraditional approaches and outperform them. The experiments conducted on alarge real-world dataset show that the proposed approach effectively predictsdemand for a wide range of products. The multimodal pipeline presented in thiswork enhances the accuracy and reliability of the predictions, demonstratingthe potential of leveraging multimodal information in product demandforecasting.", "output": "Multimodal Temporal Fusion Transformers Are Good Product Demand Forecasters."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic graph embedding has emerged as a very effective technique foraddressing diverse temporal graph analytic tasks (i.e., link prediction, nodeclassification, recommender systems, anomaly detection, and graph generation)in various applications. Such temporal graphs exhibit heterogeneous transientdynamics, varying time intervals, and highly evolving node features throughouttheir evolution. Hence, incorporating long-range dependencies from thehistorical graph context plays a crucial role in accurately learning theirtemporal dynamics. In this paper, we develop a graph embedding model withuncertainty quantification, TransformerG2G, by exploiting the advancedtransformer encoder to first learn intermediate node representations from itscurrent state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ isthe length of context). Moreover, we employ two projection layers to generatelower-dimensional multivariate Gaussian distributions as each node's latentembedding at timestamp $t$. We consider diverse benchmarks with varying levelsof ``novelty\" as measured by the TEA plots. Our experiments demonstrate thatthe proposed TransformerG2G model outperforms conventional multi-step methodsand our prior work (DynG2G) in terms of both link prediction accuracy andcomputational efficiency, especially for high degree of novelty. Furthermore,the learned time-dependent attention weights across multiple graph snapshotsreveal the development of an automatic adaptive time stepping enabled by thetransformer. Importantly, by examining the attention weights, we can uncovertemporal dependencies, identify influential elements, and gain insights intothe complex interactions within the graph structure. For example, we identifieda strong correlation between attention weights and node degree at the variousstages of the graph topology evolution.", "output": "TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We tackle the problems of latent variables identification and\"out-of-support\" image generation in representation learning. We show that bothare possible for a class of decoders that we call additive, which arereminiscent of decoders used for object-centric representation learning (OCRL)and well suited for images that can be decomposed as a sum of object-specificimages. We provide conditions under which exactly solving the reconstructionproblem using an additive decoder is guaranteed to identify the blocks oflatent variables up to permutation and block-wise invertible transformations.This guarantee relies only on very weak assumptions about the distribution ofthe latent factors, which might present statistical dependencies and have analmost arbitrarily shaped support. Our result provides a new setting wherenonlinear independent component analysis (ICA) is possible and adds to ourtheoretical understanding of OCRL methods. We also show theoretically thatadditive decoders can generate novel images by recombining observed factors ofvariations in novel ways, an ability we refer to as Cartesian-productextrapolation. We show empirically that additivity is crucial for bothidentifiability and extrapolation on simulated data.", "output": "Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human language acquisition is an efficient, supervised, and continualprocess. In this work, we took inspiration from how human babies acquire theirfirst language, and developed a computational process for word acquisitionthrough comparative learning. Motivated by cognitive findings, we generated asmall dataset that enables the computation models to compare the similaritiesand differences of various attributes, learn to filter out and extract thecommon information for each shared linguistic label. We frame the acquisitionof words as not only the information filtration process, but also asrepresentation-symbol mapping. This procedure does not involve a fixedvocabulary size, nor a discriminative objective, and allows the models tocontinually learn more concepts efficiently. Our results in controlledexperiments have shown the potential of this approach for efficient continuallearning of grounded words.", "output": "Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) has been shown to learn sophisticated controlpolicies for complex tasks including games, robotics, heating and coolingsystems and text generation. The action-perception cycle in RL, however,generally assumes that a measurement of the state of the environment isavailable at each time step without a cost. In applications such as deep-seaand planetary robot exploration, materials design and medicine, however, therecan be a high cost associated with measuring, or even approximating, the stateof the environment. In this paper, we survey the recently growing literaturethat adopts the perspective that an RL agent might not need, or even want, acostly measurement at each time step. Within this context, we propose the DeepDynamic Multi-Step Observationless Agent (DMSOA), contrast it with theliterature and empirically evaluate it on OpenAI gym and Atari Pongenvironments. Our results, show that DMSOA learns a better policy with fewerdecision steps and measurements than the considered alternative from theliterature.", "output": "Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) allows machine learning models to train locally onindividual mobile devices, synchronizing model updates via a shared server.This approach safeguards user privacy; however, it also generates aheterogeneous training environment due to the varying performance capabilitiesacross devices. As a result, straggler devices with lower performance oftendictate the overall training time in FL. In this work, we aim to alleviate thisperformance bottleneck due to stragglers by dynamically balancing the trainingload across the system. We introduce Invariant Dropout, a method that extractsa sub-model based on the weight update threshold, thereby minimizing potentialimpacts on accuracy. Building on this dropout technique, we develop an adaptivetraining framework, Federated Learning using Invariant Dropout (FLuID). FLuIDoffers a lightweight sub-model extraction to regulate computational intensity,thereby reducing the load on straggler devices without affecting model quality.Our method leverages neuron updates from non-straggler devices to construct atailored sub-model for each straggler based on client performance profiling.Furthermore, FLuID can dynamically adapt to changes in stragglers as runtimeconditions shift. We evaluate FLuID using five real-world mobile clients. Theevaluations show that Invariant Dropout maintains baseline model efficiencywhile alleviating the performance bottleneck of stragglers through a dynamic,runtime approach.", "output": "FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Acute Myeloid Leukemia (AML) is one of the most aggressive types ofhematological neoplasm. To support the specialists' decision about theappropriate therapy, patients with AML receive a prognostic of outcomesaccording to their cytogenetic and molecular characteristics, often dividedinto three risk categories: favorable, intermediate, and adverse. However, thecurrent risk classification has known problems, such as the heterogeneitybetween patients of the same risk group and no clear definition of theintermediate risk category. Moreover, as most patients with AML receive anintermediate-risk classification, specialists often demand other tests andanalyses, leading to delayed treatment and worsening of the patient's clinicalcondition. This paper presents the data analysis and an explainablemachine-learning model to support the decision about the most appropriatetherapy protocol according to the patient's survival prediction. In addition tothe prediction model being explainable, the results obtained are promising andindicate that it is possible to use it to support the specialists' decisionssafely. Most importantly, the findings offered in this study have the potentialto open new avenues of research toward better treatments and prognosticmarkers.", "output": "An explainable model to support the decision about the therapy protocol for AML."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Q-learning has become an important part of the reinforcement learning toolkitsince its introduction in the dissertation of Chris Watkins in the 1980s. Thepurpose of this paper is in part a tutorial on stochastic approximation andQ-learning, providing details regarding the INFORMS APS inaugural AppliedProbability Trust Plenary Lecture, presented in Nancy France, June 2023.The paper also presents new approaches to ensure stability and potentiallyaccelerated convergence for these algorithms, and stochastic approximation inother settings. Two contributions are entirely new:1. Stability of Q-learning with linear function approximation has been anopen topic for research for over three decades. It is shown that withappropriate optimistic training in the form of a modified Gibbs policy, thereexists a solution to the projected Bellman equation, and the algorithm isstable (in terms of bounded parameter estimates). Convergence remains one ofmany open topics for research.2. The new Zap Zero algorithm is designed to approximate the Newton-Raphsonflow without matrix inversion. It is stable and convergent under mildassumptions on the mean flow vector field for the algorithm, and compatiblestatistical assumption on an underlying Markov chain. The algorithm is ageneral approach to stochastic approximation which in particular applies toQ-learning with \"oblivious\" training even with non-linear functionapproximation.", "output": "Stability of Q-Learning Through Design and Optimism."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Entanglement forging based variational algorithms leverage the bi-partitionof quantum systems for addressing ground state problems. The primary limitationof these approaches lies in the exponential summation required over thenumerous potential basis states, or bitstrings, when performing the Schmidtdecomposition of the whole system. To overcome this challenge, we propose a newmethod for entanglement forging employing generative neural networks toidentify the most pertinent bitstrings, eliminating the need for theexponential sum. Through empirical demonstrations on systems of increasingcomplexity, we show that the proposed algorithm achieves comparable or superiorperformance compared to the existing standard implementation of entanglementforging. Moreover, by controlling the amount of required resources, this schemecan be applied to larger, as well as non permutation invariant systems, wherethe latter constraint is associated with the Heisenberg forging procedure. Wesubstantiate our findings through numerical simulations conducted on spinsmodels exhibiting one-dimensional ring, two-dimensional triangular latticetopologies, and nuclear shell model configurations.", "output": "Hybrid Ground-State Quantum Algorithms based on Neural Schr\\\"odinger Forging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For real-world applications, robots will need to continually learn in theirenvironments through limited interactions with their users. Toward this,previous works in few-shot class incremental learning (FSCIL) and active classselection (ACS) have achieved promising results but were tested in constrainedsetups. Therefore, in this paper, we combine ideas from FSCIL and ACS todevelop a novel framework that can allow an autonomous agent to continuallylearn new objects by asking its users to label only a few of the mostinformative objects in the environment. To this end, we build on astate-of-the-art (SOTA) FSCIL model and extend it with techniques from ACSliterature. We term this model Few-shot Incremental Active class SeleCtiOn(FIASco). We further integrate a potential field-based navigation techniquewith our model to develop a complete framework that can allow an agent toprocess and reason on its sensory data through the FIASco model, navigatetowards the most informative object in the environment, gather data about theobject through its sensors and incrementally update the FIASco model.Experimental results on a simulated agent and a real robot show thesignificance of our approach for long-term real-world robotics applications.", "output": "Active Class Selection for Few-Shot Class-Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks tend to make overconfident predictions and often requireadditional detectors for misclassifications, particularly for safety-criticalapplications. Existing detection methods usually only focus on adversarialattacks or out-of-distribution samples as reasons for false predictions.However, generalization errors occur due to diverse reasons often related topoorly learning relevant invariances. We therefore propose GIT, a holisticapproach for the detection of generalization errors that combines the usage ofgradient information and invariance transformations. The invariancetransformations are designed to shift misclassified samples back into thegeneralization area of the neural network, while the gradient informationmeasures the contradiction between the initial prediction and the correspondinginherent computations of the neural network using the transformed sample. Ourexperiments demonstrate the superior performance of GIT compared to thestate-of-the-art on a variety of network architectures, problem setups andperturbation types.", "output": "GIT: Detecting Uncertainty, Out-Of-Distribution and Adversarial Samples using Gradients and Invariance Transformations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent surge of large language models (LLMs) highlights their ability toperform in-context learning, i.e., \"learning\" to perform a task from a fewdemonstrations in the context without any parameter updates. However, theircapabilities of in-context learning are limited by the model architecture: 1)the use of demonstrations is constrained by a maximum sentence length due topositional embeddings; 2) the quadratic complexity of attention hinders usersfrom using more demonstrations efficiently; 3) LLMs are shown to be sensitiveto the order of the demonstrations. In this work, we tackle these challenges byproposing a better architectural design for in-context learning. We proposeSAICL (Structured Attention for In-Context Learning), which replaces thefull-attention by a structured attention mechanism designed for in-contextlearning, and removes unnecessary dependencies between individualdemonstrations, while making the model invariant to the permutation ofdemonstrations. We evaluate SAICL in a meta-training framework and show thatSAICL achieves comparable or better performance than full attention whileobtaining up to 3.4x inference speed-up. SAICL also consistently outperforms astrong Fusion-in-Decoder (FiD) baseline which processes each demonstrationindependently. Finally, thanks to its linear nature, we demonstrate that SAICLcan easily scale to hundreds of demonstrations with continuous performancegains with scaling.", "output": "Scaling In-Context Demonstrations with Structured Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lecture notes from the course given by Professor Julia Kempe at the summerschool \"Statistical physics of Machine Learning\" in Les Houches. The notesdiscuss the so-called NTK approach to problems in machine learning, whichconsists of gaining an understanding of generally unsolvable problems byfinding a tractable kernel formulation. The notes are mainly focused onpractical applications such as data distillation and adversarial robustness,examples of inductive bias are also discussed.", "output": "Kernels, Data & Physics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the essential components of deep learning is the choice of the lossfunction and performance metrics used to train and evaluate models. This paperreviews the most prevalent loss functions and performance measurements in deeplearning. We examine the benefits and limits of each technique and illustratetheir application to various deep-learning problems. Our review aims to give acomprehensive picture of the different loss functions and performanceindicators used in the most common deep learning tasks and help practitionerschoose the best method for their specific task.", "output": "Loss Functions and Metrics in Deep Learning. A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of generating periodic materials with deep models.While symmetry-aware molecule generation has been studied extensively, periodicmaterials possess different symmetries, which have not been completely capturedby existing methods. In this work, we propose SyMat, a novel materialgeneration approach that can capture physical symmetries of periodic materialstructures. SyMat generates atom types and lattices of materials throughgenerating atom type sets, lattice lengths and lattice angles with avariational auto-encoder model. In addition, SyMat employs a score-baseddiffusion model to generate atom coordinates of materials, in which a novelsymmetry-aware probabilistic model is used in the coordinate diffusion process.We show that SyMat is theoretically invariant to all symmetry transformationson materials and demonstrate that SyMat achieves promising performance onrandom generation and property optimization tasks.", "output": "Towards Symmetry-Aware Generation of Periodic Materials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a similarity metric, contrastive methods learn a representation inwhich examples that are similar are pushed together and examples that aredissimilar are pulled apart. Contrastive learning techniques have been utilizedextensively to learn representations for tasks ranging from imageclassification to caption generation. However, existing contrastive learningapproaches can fail to generalize because they do not take into account thepossibility of different similarity relations. In this paper, we propose anovel multi-similarity contrastive loss (MSCon), that learns generalizableembeddings by jointly utilizing supervision from multiple metrics ofsimilarity. Our method automatically learns contrastive similarity weightingsbased on the uncertainty in the corresponding similarity, down-weightinguncertain tasks and leading to better out-of-domain generalization to newtasks. We show empirically that networks trained with MSCon outperformstate-of-the-art baselines on in-domain and out-of-domain settings.", "output": "Multi-Similarity Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncertainty sampling is a prevalent active learning algorithm that queriessequentially the annotations of data samples which the current prediction modelis uncertain about. However, the usage of uncertainty sampling has been largelyheuristic: (i) There is no consensus on the proper definition of \"uncertainty\"for a specific task under a specific loss; (ii) There is no theoreticalguarantee that prescribes a standard protocol to implement the algorithm, forexample, how to handle the sequentially arrived annotated data under theframework of optimization algorithms such as stochastic gradient descent. Inthis work, we systematically examine uncertainty sampling algorithms under bothstream-based and pool-based active learning. We propose a notion of equivalentloss which depends on the used uncertainty measure and the original lossfunction and establish that an uncertainty sampling algorithm essentiallyoptimizes against such an equivalent loss. The perspective verifies theproperness of existing uncertainty measures from two aspects: surrogateproperty and loss convexity. Furthermore, we propose a new notion for designinguncertainty measures called textit{loss as uncertainty}. The idea is to usethe conditional expected loss given the features as the uncertainty measure.Such an uncertainty measure has nice analytical properties and generality tocover both classification and regression problems, which enable us to providethe first generalization bound for uncertainty sampling algorithms under bothstream-based and pool-based settings, in the full generality of the underlyingmodel and problem. Lastly, we establish connections between certain variants ofthe uncertainty sampling algorithms with risk-sensitive objectives anddistributional robustness, which can partly explain the advantage ofuncertainty sampling algorithms when the sample size is small.", "output": "Understanding Uncertainty Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Entity matching (EM) is a challenging problem studied by differentcommunities for over half a century. Algorithmic fairness has also become atimely topic to address machine bias and its societal impacts. Despiteextensive research on these two topics, little attention has been paid to thefairness of entity matching.Towards addressing this gap, we perform an extensive experimental evaluationof a variety of EM techniques in this paper. We generated two social datasetsfrom publicly available datasets for the purpose of auditing EM through thelens of fairness. Our findings underscore potential unfairness under two commonconditions in real-world societies: (i) when some demographic groups areoverrepresented, and (ii) when names are more similar in some groups comparedto others. Among our many findings, it is noteworthy to mention that whilevarious fairness definitions are valuable for different settings, due to EM'sclass imbalance nature, measures such as positive predictive value parity andtrue positive rate parity are, in general, more capable of revealing EMunfairness.", "output": "Through the Fairness Lens: Experimental Analysis and Evaluation of Entity Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "General purpose agents will require large repertoires of skills. Empowerment-- the maximum mutual information between skills and the states -- provides apathway for learning large collections of distinct skills, but mutualinformation is difficult to optimize. We introduce a new framework,Hierarchical Empowerment, that makes computing empowerment more tractable byintegrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning.Our framework makes two specific contributions. First, we introduce a newvariational lower bound on mutual information that can be used to computeempowerment over short horizons. Second, we introduce a hierarchicalarchitecture for computing empowerment over exponentially longer time scales.We verify the contributions of the framework in a series of simulated roboticstasks. In a popular ant navigation domain, our four level agents are able tolearn skills that cover a surface area over two orders of magnitude larger thanprior work.", "output": "Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Numerous benchmarks for Few-Shot Learning have been proposed in the lastdecade. However all of these benchmarks focus on performance averaged over manytasks, and the question of how to reliably evaluate and tune models trained forindividual tasks in this regime has not been addressed. This paper presents thefirst investigation into task-level evaluation -- a fundamental step whendeploying a model. We measure the accuracy of performance estimators in thefew-shot setting, consider strategies for model selection, and examine thereasons for the failure of evaluators usually thought of as being robust. Weconclude that cross-validation with a low number of folds is the best choicefor directly estimating the performance of a model, whereas using bootstrappingor cross validation with a large number of folds is better for model selectionpurposes. Overall, we find that existing benchmarks for few-shot learning arenot designed in such a way that one can get a reliable picture of howeffectively methods can be used on individual tasks.", "output": "Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The prevalent use of benchmarks in current offline reinforcement learning(RL) research has led to a neglect of the imbalance of real-world datasetdistributions in the development of models. The real-world offline RL datasetis often imbalanced over the state space due to the challenge of exploration orsafety considerations. In this paper, we specify properties of imbalanceddatasets in offline RL, where the state coverage follows a power lawdistribution characterized by skewed policies. Theoretically and empirically,we show that typically offline RL methods based on distributional constraints,such as conservative Q-learning (CQL), are ineffective in extracting policiesunder the imbalanced dataset. Inspired by natural intelligence, we propose anovel offline RL method that utilizes the augmentation of CQL with a retrievalprocess to recall past related experiences, effectively alleviating thechallenges posed by imbalanced datasets. We evaluate our method on severaltasks in the context of imbalanced datasets with varying levels of imbalance,utilizing the variant of D4RL. Empirical results demonstrate the superiority ofour method over other baselines.", "output": "Offline Reinforcement Learning with Imbalanced Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cascades are a classical strategy to enable inference cost to vary adaptivelyacross samples, wherein a sequence of classifiers are invoked in turn. Adeferral rule determines whether to invoke the next classifier in the sequence,or to terminate prediction. One simple deferral rule employs the confidence ofthe current classifier, e.g., based on the maximum predicted softmaxprobability. Despite being oblivious to the structure of the cascade -- e.g.,not modelling the errors of downstream models -- such confidence-based deferraloften works remarkably well in practice. In this paper, we seek to betterunderstand the conditions under which confidence-based deferral may fail, andwhen alternate deferral strategies can perform better. We first present atheoretical characterisation of the optimal deferral rule, which preciselycharacterises settings under which confidence-based deferral may suffer. Wethen study post-hoc deferral mechanisms, and demonstrate they can significantlyimprove upon confidence-based deferral in settings where (i) downstream modelsare specialists that only work well on a subset of inputs, (ii) samples aresubject to label noise, and (iii) there is distribution shift between the trainand test set.", "output": "When Does Confidence-Based Cascade Deferral Suffice?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a deep learning framework for solvinghigh-dimensional partial integro-differential equations (PIDEs) based on thetemporal difference learning. We introduce a set of Levy processes andconstruct a corresponding reinforcement learning model. To simulate the entireprocess, we use deep neural networks to represent the solutions and non-localterms of the equations. Subsequently, we train the networks using the temporaldifference error, termination condition, and properties of the non-local termsas the loss function. The relative error of the method reaches O(10^{-3}) in100-dimensional experiments and O(10^{-4}) in one-dimensional pure jumpproblems. Additionally, our method demonstrates the advantages of lowcomputational cost and robustness, making it well-suited for addressingproblems with different forms and intensities of jumps.", "output": "Temporal Difference Learning for High-Dimensional PIDEs with Jumps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evolution of wireless networks gravitates towards connected intelligence,a concept that envisions seamless interconnectivity among humans, objects, andintelligence in a hyper-connected cyber-physical world. Edge AI emerges as apromising solution to achieve connected intelligence by deliveringhigh-quality, low-latency, and privacy-preserving AI services at the networkedge. In this article, we introduce an autonomous edge AI system thatautomatically organizes, adapts, and optimizes itself to meet users' diverserequirements. The system employs a cloud-edge-client hierarchical architecture,where the large language model, i.e., Generative Pretrained Transformer (GPT),resides in the cloud, and other AI models are co-deployed on devices and edgeservers. By leveraging the powerful abilities of GPT in language understanding,planning, and code generation, we present a versatile framework thatefficiently coordinates edge AI models to cater to users' personal demandswhile automatically generating code to train new models via edge federatedlearning. Experimental results demonstrate the system's remarkable ability toaccurately comprehend user demands, efficiently execute AI models with minimalcost, and effectively create high-performance AI models through federatedlearning.", "output": "Large Language Models Empowered Autonomous Edge AI for Connected Intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate performance disparities in deep classifiers. We find that theability of classifiers to separate individuals into subgroups variessubstantially across medical imaging modalities and protected characteristics;crucially, we show that this property is predictive of algorithmic bias.Through theoretical analysis and extensive empirical evaluation, we find arelationship between subgroup separability, subgroup disparities, andperformance degradation when models are trained on data with systematic biassuch as underdiagnosis. Our findings shed new light on the question of howmodels become biased, providing important insights for the development of fairmedical imaging AI.", "output": "The Role of Subgroup Separability in Group-Fair Medical Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative AI has made significant strides, yet concerns about the accuracyand reliability of its outputs continue to grow. Such inaccuracies can haveserious consequences such as inaccurate decision-making, the spread of falseinformation, privacy violations, legal liabilities, and more. Although effortsto address these risks are underway, including explainable AI and responsibleAI practices such as transparency, privacy protection, bias mitigation, andsocial and environmental responsibility, misinformation caused by generative AIwill remain a significant challenge. We propose that verifying the outputs ofgenerative AI from a data management perspective is an emerging issue forgenerative AI. This involves analyzing the underlying data from multi-modaldata lakes, including text files, tables, and knowledge graphs, and assessingits quality and consistency. By doing so, we can establish a strongerfoundation for evaluating the outputs of generative AI models. Such an approachcan ensure the correctness of generative AI, promote transparency, and enabledecision-making with greater confidence. Our vision is to promote thedevelopment of verifiable generative AI and contribute to a more trustworthyand responsible use of AI.", "output": "VerifAI: Verified Generative AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a few-shot personalized saliency prediction usingtensor-to-matrix regression for preserving the structural global information ofpersonalized saliency maps (PSMs). In contrast to a general saliency map, a PSMhas been great potential since its map indicates the person-specific visualattention that is useful for obtaining individual visual preferences fromheterogeneity of gazed areas. The PSM prediction is needed for acquiring thePSM for the unseen image, but its prediction is still a challenging task due tothe complexity of individual gaze patterns. For recognizing individual gazepatterns from the limited amount of eye-tracking data, the previous methodsadopt the similarity of gaze tendency between persons. However, in the previousmethods, the PSMs are vectorized for the prediction model. In this way, thestructural global information of the PSMs corresponding to the image isignored. For automatically revealing the relationship between PSMs, we focus onthe tensor-based regression model that can preserve the structural informationof PSMs, and realize the improvement of the prediction accuracy. In theexperimental results, we confirm the proposed method including the tensor-basedregression outperforms the comparative methods.", "output": "Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine Learning requires a large amount of training data in order to buildaccurate models. Sometimes the data arrives over time, requiring significantstorage space and recalculating the model to account for the new data. On-linelearning addresses these issues by incrementally modifying the model as data isencountered, and then discarding the data. In this study we introduce a newonline linear regression approach. Our approach combines newly arriving datawith a previously existing model to create a new model. The introduced model,named OLR-WA (OnLine Regression with Weighted Average) uses user-definedweights to provide flexibility in the face of changing data to bias the resultsin favor of old or new data. We have conducted 2-D and 3-D experimentscomparing OLR-WA to a static model using the entire data set. The results showthat for consistent data, OLR-WA and the static batch model perform similarlyand for varying data, the user can set the OLR-WA to adapt more quickly or toresist change.", "output": "OLR-WA Online Regression with Weighted Average."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic graph data mining has gained popularity in recent years due to therich information contained in dynamic graphs and their widespread use in thereal world. Despite the advances in dynamic graph neural networks (DGNNs), therich information and diverse downstream tasks have posed significantdifficulties for the practical application of DGNNs in industrial scenarios. Tothis end, in this paper, we propose to address them by pre-training and presentthe Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG).CPDG tackles the challenges of pre-training for DGNNs, including generalizationand long-short term modeling capability, through a flexible structural-temporalsubgraph sampler along with structural-temporal contrastive pre-trainingschemes. Extensive experiments conducted on both large-scale research andindustrial dynamic graph datasets show that CPDG outperforms existing methodsin dynamic graph pre-training for various downstream tasks under three transfersettings.", "output": "CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a systematic literature review on Brain-ComputerInterfaces (BCIs) in the context of Machine Learning. Our focus is onElectroencephalography (EEG) research, highlighting the latest trends as of2023. The objective is to provide undergraduate researchers with an accessibleoverview of the BCI field, covering tasks, algorithms, and datasets. Bysynthesizing recent findings, our aim is to offer a fundamental understandingof BCI research, identifying promising avenues for future investigations.", "output": "Trends in Machine Learning and Electroencephalogram (EEG): A Review for Undergraduate Researchers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are known to be vulnerable to adversarial examplescrafted by adding human-imperceptible perturbations to the benign input. Afterachieving nearly 100% attack success rates in white-box setting, more focus isshifted to black-box attacks, of which the transferability of adversarialexamples has gained significant attention. In either case, the commongradient-based methods generally use the sign function to generateperturbations on the gradient update, that offers a roughly correct directionand has gained great success. But little work pays attention to its possiblelimitation. In this work, we observe that the deviation between the originalgradient and the generated noise may lead to inaccurate gradient updateestimation and suboptimal solutions for adversarial transferability. To thisend, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM).Specifically, we use data rescaling to substitute the sign function withoutextra computational cost. We further propose a Depth First Sampling method toeliminate the fluctuation of rescaling and stabilize the gradient update. Ourmethod could be used in any gradient-based attacks and is extensible to beintegrated with various input transformation or ensemble methods to furtherimprove the adversarial transferability. Extensive experiments on the standardImageNet dataset show that our method could significantly boost thetransferability of gradient-based attacks and outperform the state-of-the-artbaselines.", "output": "Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial imitation learning (AIL) is a popular method that has recentlyachieved much success. However, the performance of AIL is still unsatisfactoryon the more challenging tasks. We find that one of the major reasons is due tothe low quality of AIL discriminator representation. Since the AILdiscriminator is trained via binary classification that does not necessarilydiscriminate the policy from the expert in a meaningful way, the resultingreward might not be meaningful either. We propose a new method called PolicyContrastive Imitation Learning (PCIL) to resolve this issue. PCIL learns acontrastive representation space by anchoring on different policies andgenerates a smooth cosine-similarity-based reward. Our proposed representationlearning objective can be viewed as a stronger version of the AIL objective andprovide a more meaningful comparison between the agent and the policy. From atheoretical perspective, we show the validity of our method using theapprenticeship learning framework. Furthermore, our empirical evaluation on theDeepMind Control suite demonstrates that PCIL can achieve state-of-the-artperformance. Finally, qualitative results suggest that PCIL builds a smootherand more meaningful representation space for imitation learning.", "output": "Policy Contrastive Imitation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Risk-sensitive reinforcement learning (RL) aims to optimize policies thatbalance the expected reward and risk. In this paper, we investigate a novelrisk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR)objective under linear and general function approximations. This newformulation, named ICVaR-RL with function approximation, provides a principledway to guarantee safety at each decision step. For ICVaR-RL with linearfunction approximation, we propose a computationally efficient algorithmICVaR-L, which achieves an$widetilde{O}(sqrt{alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $alpha$ isthe risk level, $d$ is the dimension of state-action features, $H$ is thelength of each episode, and $K$ is the number of episodes. We also establish amatching lower bound $Omega(sqrt{alpha^{-(H-1)}d^2K})$ to validate theoptimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with generalfunction approximation, we propose algorithm ICVaR-G, which achieves an$widetilde{O}(sqrt{alpha^{-(H+1)}DH^4K})$ regret, where $D$ is a dimensionalparameter that depends on the eluder dimension and covering number.Furthermore, our analysis provides several novel techniques for risk-sensitiveRL, including an efficient approximation of the CVaR operator, a new ridgeregression with CVaR-adapted features, and a refined elliptical potentiallemma.", "output": "Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a novel tag-based recommender system called PLIERS,which relies on the assumption that users are mainly interested in items andtags with similar popularity to those they already own. PLIERS is aimed atreaching a good tradeoff between algorithmic complexity and the level ofpersonalization of recommended items. To evaluate PLIERS, we performed a set ofexperiments on real OSN datasets, demonstrating that it outperformsstate-of-the-art solutions in terms of personalization, relevance, and noveltyof recommendations.", "output": "PLIERS: a Popularity-Based Recommender System for Content Dissemination in Online Social Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traditional automation technologies alone are not sufficient to enabledriverless operation of trains (called Grade of Automation (GoA) 4) onnon-restricted infrastructure. The required perception tasks are nowadaysrealized using Machine Learning (ML) and thus need to be developed and deployedreliably and efficiently. One important aspect to achieve this is to use anMLOps process for tackling improved reproducibility, traceability,collaboration, and continuous adaptation of a driverless operation to changingconditions. MLOps mixes ML application development and operation (Ops) andenables high frequency software releases and continuous innovation based on thefeedback from operations. In this paper, we outline a safe MLOps process forthe continuous development and safety assurance of ML-based systems in therailway domain. It integrates system engineering, safety assurance, and the MLlife-cycle in a comprehensive workflow. We present the individual stages of theprocess and their interactions. Moreover, we describe relevant challenges toautomate the different stages of the safe MLOps process.", "output": "Towards a safe MLOps Process for the Continuous Development and Safety Assurance of ML-based Systems in the Railway Domain."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies the sample-efficiency of learning in Partially ObservableMarkov Decision Processes (POMDPs), a challenging problem in reinforcementlearning that is known to be exponentially hard in the worst-case. Motivated byreal-world settings such as loading in game playing, we propose an enhancedfeedback model called ``multiple observations in hindsight'', where after eachepisode of interaction with the POMDP, the learner may collect multipleadditional observations emitted from the encountered latent states, but may notobserve the latent states themselves. We show that sample-efficient learningunder this feedback model is possible for two new subclasses of POMDPs:emph{multi-observation revealing POMDPs} and emph{distinguishable POMDPs}.Both subclasses generalize and substantially relax emph{revealing POMDPs} -- awidely studied subclass for which sample-efficient learning is possible understandard trajectory feedback. Notably, distinguishable POMDPs only require theemission distributions from different latent states to be emph{different}instead of emph{linearly independent} as required in revealing POMDPs.", "output": "Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Demonstrations are widely used in Deep Reinforcement Learning (DRL) forfacilitating solving tasks with sparse rewards. However, the tasks inreal-world scenarios can often have varied initial conditions from thedemonstration, which would require additional prior behaviours. For example,consider we are given the demonstration for the task of emph{picking up anobject from an open drawer}, but the drawer is closed in the training. Withoutacquiring the prior behaviours of opening the drawer, the robot is unlikely tosolve the task. To address this, in this paper we propose an Intrinsic RewardsDriven Example-based Control textbf{(IRDEC)}. Our method can endow agents withthe ability to explore and acquire the required prior behaviours and thenconnect to the task-specific behaviours in the demonstration to solvesparse-reward tasks without requiring additional demonstration of the priorbehaviours. The performance of our method outperforms other baselines on threenavigation tasks and one robotic manipulation task with sparse rewards. Codesare available at ", "output": "Learning to Solve Tasks with Exploring Prior Behaviours."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of unfair discrimination between two groups andpropose a pre-processing method to achieve fairness. Corrective methods likestatistical parity usually lead to bad accuracy and do not really achievefairness in situations where there is a correlation between the sensitiveattribute S and the legitimate attribute E (explanatory variable) that shoulddetermine the decision. To overcome these drawbacks, other notions of fairnesshave been proposed, in particular, conditional statistical parity and equalopportunity. However, E is often not directly observable in the data, i.e., itis a latent variable. We may observe some other variable Z representing E, butthe problem is that Z may also be affected by S, hence Z itself can be biased.To deal with this problem, we propose BaBE (Bayesian Bias Elimination), anapproach based on a combination of Bayes inference and theExpectation-Maximization method, to estimate the most likely value of E for agiven Z for each group. The decision can then be based directly on theestimated E. We show, by experiments on synthetic and real data sets, that ourapproach provides a good level of fairness as well as high accuracy.", "output": "BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixed-precision quantization, where a deep neural network's layers arequantized to different precisions, offers the opportunity to optimize thetrade-offs between model size, latency, and statistical accuracy beyond whatcan be achieved with homogeneous-bit-width quantization. To navigate theintractable search space of mixed-precision configurations for a given network,this paper proposes a hybrid search methodology. It consists of ahardware-agnostic differentiable search algorithm followed by a hardware-awareheuristic optimization to find mixed-precision configurations latency-optimizedfor a specific hardware target. We evaluate our algorithm on MobileNetV1 andMobileNetV2 and deploy the resulting networks on a family of multi-core RISC-Vmicrocontroller platforms with different hardware characteristics. We achieveup to 28.6% reduction of end-to-end latency compared to an 8-bit model at anegligible accuracy drop from a full-precision baseline on the 1000-classImageNet dataset. We demonstrate speedups relative to an 8-bit baseline, evenon systems with no hardware support for sub-byte arithmetic at negligibleaccuracy drop. Furthermore, we show the superiority of our approach withrespect to differentiable search targeting reduced binary operation counts as aproxy for latency.", "output": "Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks on the Edge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurately predicting vapor pressure is vital for various industrial andenvironmental applications. However, obtaining accurate measurements for allcompounds of interest is not possible due to the resource and labor intensityof experiments. The demand for resources and labor further multiplies when atemperature-dependent relationship for predicting vapor pressure is desired. Inthis paper, we propose PUFFIN (Path-Unifying Feed-Forward Interfaced Network),a machine learning framework that combines transfer learning with a newinductive bias node inspired by domain knowledge (the Antoine equation) toimprove vapor pressure prediction. By leveraging inductive bias and transferlearning using graph embeddings, PUFFIN outperforms alternative strategies thatdo not use inductive bias or that use generic descriptors of compounds. Theframework's incorporation of domain-specific knowledge to overcome thelimitation of poor data availability shows its potential for broaderapplications in chemical compound analysis, including the prediction of otherphysicochemical properties. Importantly, our proposed machine learningframework is partially interpretable, because the inductive Antoine node yieldsnetwork-derived Antoine equation coefficients. It would then be possible todirectly incorporate the obtained analytical expression in process designsoftware for better prediction and control of processes occurring in industryand the environment.", "output": "PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sensor-based Human Activity Recognition facilitates unobtrusive monitoring ofhuman movements. However, determining the most effective sensor placement foroptimal classification performance remains challenging. This paper introduces anovel methodology to resolve this issue, using real-time 2D pose estimationsderived from video recordings of target activities. The derived skeleton dataprovides a unique strategy for identifying the optimal sensor location. Wevalidate our approach through a feasibility study, applying inertial sensors tomonitor 13 different activities across ten subjects. Our findings indicate thatthe vision-based method for sensor placement offers comparable results to theconventional deep learning approach, demonstrating its efficacy. This researchsignificantly advances the field of Human Activity Recognition by providing alightweight, on-device solution for determining the optimal sensor placement,thereby enhancing data anonymization and supporting a multimodal classificationapproach.", "output": "A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning with rejection is a prototypical model for studying the interactionbetween humans and AI on prediction tasks. The model has two components, apredictor and a rejector. Upon the arrival of a sample, the rejector firstdecides whether to accept it; if accepted, the predictor fulfills theprediction task, and if rejected, the prediction will be deferred to humans.The learning problem requires learning a predictor and a rejectorsimultaneously. This changes the structure of the conventional loss functionand often results in non-convexity and inconsistency issues. For theclassification with rejection problem, several works develop surrogate lossesfor the jointly learning with provable consistency guarantees; in parallel,there has been less work for the regression counterpart. We study theregression with rejection (RwR) problem and investigate the no-rejectionlearning strategy which treats the RwR problem as a standard regression task tolearn the predictor. We establish that the suboptimality of the no-rejectionlearning strategy observed in the literature can be mitigated by enlarging thefunction class of the predictor. Then we introduce the truncated loss to singleout the learning for the predictor and we show that a consistent surrogateproperty can be established for the predictor individually in an easier waythan for the predictor and the rejector jointly. Our findings advocate for atwo-step learning procedure that first uses all the data to learn the predictorand then calibrates the prediction loss for the rejector. It is better alignedwith the common intuition that more data samples will lead to a betterpredictor and it calls for more efforts on a better design of calibrationalgorithms for learning the rejector. While our discussions mainly focus on theregression problem, the theoretical results and insights generalize to theclassification problem as well.", "output": "When No-Rejection Learning is Optimal for Regression with Rejection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, U-shaped networks have dominated the field of medical imagesegmentation due to their simple and easily tuned structure. However, existingU-shaped segmentation networks: 1) mostly focus on designing complexself-attention modules to compensate for the lack of long-term dependence basedon convolution operation, which increases the overall number of parameters andcomputational complexity of the network; 2) simply fuse the features of encoderand decoder, ignoring the connection between their spatial locations. In thispaper, we rethink the above problem and build a lightweight medical imagesegmentation network, called SegNetr. Specifically, we introduce a novelSegNetr block that can perform local-global interactions dynamically at anystage and with only linear complexity. At the same time, we design a generalinformation retention skip connection (IRSC) to preserve the spatial locationinformation of encoder features and achieve accurate fusion with the decoderfeatures. We validate the effectiveness of SegNetr on four mainstream medicalimage segmentation datasets, with 59% and 76% fewer parameters and GFLOPsthan vanilla U-Net, while achieving segmentation performance comparable tostate-of-the-art methods. Notably, the components proposed in this paper can beapplied to other U-shaped networks to improve their segmentation performance.", "output": "SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Privacy-preserving clustering groups data points in an unsupervised mannerwhilst ensuring that sensitive information remains protected. Previousprivacy-preserving clustering focused on identifying concentration of pointclouds. In this paper, we take another path and focus on identifyingappropriate separators that split a data set. We introduce the noveldifferentially private clustering algorithm DPM that searches for accurate datapoint separators in a differentially private manner. DPM addresses two keychallenges for finding accurate separators: identifying separators that arelarge gaps between clusters instead of small gaps within a cluster and, toefficiently spend the privacy budget, prioritising separators that split thedata into large subparts. Using the differentially private ExponentialMechanism, DPM randomly chooses cluster separators with provably high utility:For a data set $D$, if there is a wide low-density separator in the central$60%$ quantile, DPM finds that separator with probability $1 -exp(-sqrt{|D|})$. Our experimental evaluation demonstrates that DPM achievessignificant improvements in terms of the clustering metric inertia. With theinertia results of the non-private KMeans++ as a baseline, for $varepsilon =1$ and $delta=10^{-5}$ DPM improves upon the difference to the baseline by upto $50%$ for a synthetic data set and by up to $62%$ for a real-world dataset compared to a state-of-the-art clustering algorithm by Chang and Kamath.", "output": "DPM: Clustering Sensitive Data through Separation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural network pruning and quantization techniques are almost as old asneural networks themselves. However, to date only ad-hoc comparisons betweenthe two have been published. In this paper, we set out to answer the questionon which is better: neural network quantization or pruning? By answering thisquestion, we hope to inform design decisions made on neural network hardwaregoing forward. We provide an extensive comparison between the two techniquesfor compressing deep neural networks. First, we give an analytical comparisonof expected quantization and pruning error for general data distributions.Then, we provide lower bounds for the per-layer pruning and quantization errorin trained networks, and compare these to empirical error after optimization.Finally, we provide an extensive experimental comparison for training 8large-scale models on 3 tasks. Our results show that in most cases quantizationoutperforms pruning. Only in some scenarios with very high compression ratio,pruning might be beneficial from an accuracy standpoint.", "output": "Pruning vs Quantization: Which is Better?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Disease detection from smartphone data represents an open research challengein mobile health (m-health) systems. COVID-19 and its respiratory symptoms arean important case study in this area and their early detection is a potentialreal instrument to counteract the pandemic situation. The efficacy of thissolution mainly depends on the performances of AI algorithms applied to thecollected data and their possible implementation directly on the users' mobiledevices. Considering these issues, and the limited amount of available data, inthis paper we present the experimental evaluation of 3 different deep learningmodels, compared also with hand-crafted features, and of two main approaches oftransfer learning in the considered scenario: both feature extraction andfine-tuning. Specifically, we considered VGGish, YAMNET, andLtextsuperscript{3}-Net (including 12 different configurations) evaluatedthrough user-independent experiments on 4 different datasets (13,447 samples intotal). Results clearly show the advantages of Ltextsuperscript{3}-Net in allthe experimental settings as it overcomes the other solutions by 12.3% interms of Precision-Recall AUC as features extractor, and by 10% when the modelis fine-tuned. Moreover, we note that to fine-tune only the fully-connectedlayers of the pre-trained models generally leads to worse performances, with anaverage drop of 6.6% with respect to feature extraction. %highlighting theneed for further investigations. Finally, we evaluate the memory footprints ofthe different models for their possible applications on commercial mobiledevices.", "output": "Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Adversarial Networks (GANs) have demonstrated their ability togenerate synthetic samples that match a target distribution. However, from aprivacy perspective, using GANs as a proxy for data sharing is not a safesolution, as they tend to embed near-duplicates of real samples in the latentspace. Recent works, inspired by k-anonymity principles, address this issuethrough sample aggregation in the latent space, with the drawback of reducingthe dataset by a factor of k. Our work aims to mitigate this problem byproposing a latent space navigation strategy able to generate diverse syntheticsamples that may support effective training of deep models, while addressingprivacy concerns in a principled way. Our approach leverages an auxiliaryidentity classifier as a guide to non-linearly walk between points in thelatent space, minimizing the risk of collision with near-duplicates of realsamples. We empirically demonstrate that, given any random pair of points inthe latent space, our walking strategy is safer than linear interpolation. Wethen test our path-finding strategy combined to k-same methods and demonstrate,on two benchmarks for tuberculosis and diabetic retinopathy classification,that training a model using samples generated by our approach mitigate drops inperformance, while keeping privacy preservation.", "output": "A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present ContainerGym, a benchmark for reinforcement learning inspired by areal-world industrial resource allocation task. The proposed benchmark encodesa range of challenges commonly encountered in real-world sequential decisionmaking problems, such as uncertainty. It can be configured to instantiateproblems of varying degrees of difficulty, e.g., in terms of variabledimensionality. Our benchmark differs from other reinforcement learningbenchmarks, including the ones aiming to encode real-world difficulties, inthat it is directly derived from a real-world industrial problem, whichunderwent minimal simplification and streamlining. It is sufficiently versatileto evaluate reinforcement learning algorithms on any real-world problem thatfits our resource allocation framework. We provide results of standard baselinemethods. Going beyond the usual training reward curves, our results and thestatistical tools used to interpret them allow to highlight interestinglimitations of well-known deep reinforcement learning algorithms, namely PPO,TRPO and DQN.", "output": "ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Information systems increasingly leverage artificial intelligence (AI) andmachine learning (ML) to generate value from vast amounts of data. However, MLmodels are imperfect and can generate incorrect classifications. Hence,human-in-the-loop (HITL) extensions to ML models add a human review forinstances that are difficult to classify. This study argues that continuouslyrelying on human experts to handle difficult model classifications leads to astrong increase in human effort, which strains limited resources. To addressthis issue, we propose a hybrid system that creates artificial experts thatlearn to classify data instances from unknown classes previously reviewed byhuman experts. Our hybrid system assesses which artificial expert is suitablefor classifying an instance from an unknown class and automatically assigns it.Over time, this reduces human effort and increases the efficiency of thesystem. Our experiments demonstrate that our approach outperforms traditionalHITL systems for several benchmarks on image classification.", "output": "Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Retrieval augmentation enables large language models to take advantage ofexternal knowledge, for example on tasks like question answering and dataimputation. However, the performance of such retrieval-augmented models islimited by the data quality of their underlying retrieval corpus. In thispaper, we propose an algorithm based on multilinear extension for evaluatingthe data importance of retrieved data points. There are exponentially manyterms in the multilinear extension, and one key contribution of this paper is apolynomial time algorithm that computes exactly, given a retrieval-augmentedmodel with an additive utility function and a validation set, the dataimportance of data points in the retrieval corpus using the multilinearextension of the model's utility function. We further proposed an even moreefficient ({epsilon}, {delta})-approximation algorithm. Our experimentalresults illustrate that we can enhance the performance of large language modelsby only pruning or reweighting the retrieval corpus, without requiring furthertraining. For some tasks, this even allows a small model (e.g., GPT-JT),augmented with a search engine API, to outperform GPT-3.5 (without retrievalaugmentation). Moreover, we show that weights based on multilinear extensioncan be computed efficiently in practice (e.g., in less than ten minutes for acorpus with 100 million elements).", "output": "Improving Retrieval-Augmented Large Language Models via Data Importance Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we consider a general observation model for restlessmulti-armed bandit problems. The operation of the player needs to be based oncertain feedback mechanism that is error-prone due to resource constraints orenvironmental or intrinsic noises. By establishing a general probabilisticmodel for dynamics of feedback/observation, we formulate the problem as arestless bandit with a countable belief state space starting from an arbitraryinitial belief (a priori information). We apply the achievable region methodwith partial conservation law (PCL) to the infinite-state problem and analyzeits indexability and priority index (Whittle index). Finally, we propose anapproximation process to transform the problem into which the AG algorithm ofNi~no-Mora and Bertsimas for finite-state problems can be applied to.Numerical experiments show that our algorithm has an excellent performance.", "output": "PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adapting pretrained language models to novel domains, such as clinicalapplications, traditionally involves retraining their entire set of parameters.However, this approach is increasingly proven to be impractical owing to thesubstantial computational requirements associated with training such largelanguage models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)techniques offer a viable solution by selectively fine-tuning a small subset ofadditional parameters, significantly reducing the computational requirementsfor domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFTadapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA istrained using clinical notes obtained from the MIMIC-IV database, therebycreating a specialised adapter designed for the clinical domain. Additionally,we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA withDownstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.We evaluate this framework on multiple clinical outcome prediction datasets,comparing it to clinically trained language models. Our proposed frameworkachieves a state-of-the-art AUROC score averaged across all clinical downstreamtasks. We observe substantial improvements of 6-9% AUROC score in thelarge-scale multilabel classification tasks, such as diagnoses and proceduresclassification.", "output": "Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For any two point sets $A,B subset mathbb{R}^d$ of size up to $n$, theChamfer distance from $A$ to $B$ is defined as $text{CH}(A,B)=sum_{a in A}min_{b in B} d_X(a,b)$, where $d_X$ is the underlying distance measure (e.g.,the Euclidean or Manhattan distance). The Chamfer distance is a popular measureof dissimilarity between point clouds, used in many machine learning, computervision, and graphics applications, and admits a straightforward $O(d n^2)$-timebrute force algorithm. Further, the Chamfer distance is often used as a proxyfor the more computationally demanding Earth-Mover (Optimal Transport)Distance. However, the emph{quadratic} dependence on $n$ in the running timemakes the naive approach intractable for large datasets.We overcome this bottleneck and present the first $(1+epsilon)$-approximatealgorithm for estimating the Chamfer distance with a near-linear running time.Specifically, our algorithm runs in time $O(nd log (n)/varepsilon^2)$ and isimplementable. Our experiments demonstrate that it is both accurate and fast onlarge high-dimensional datasets. We believe that our algorithm will open newavenues for analyzing large high-dimensional point clouds. We also giveevidence that if the goal is to emph{report} a $(1+varepsilon)$-approximatemapping from $A$ to $B$ (as opposed to just its value), then any sub-quadratictime algorithm is unlikely to exist.", "output": "A Near-Linear Time Algorithm for the Chamfer Distance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces Track Mix, a personalized playlist generation systemreleased in 2022 on the music streaming service Deezer. Track Mix automaticallygenerates \"mix\" playlists inspired by initial music tracks, allowing users todiscover music similar to their favorite content. To generate these mixes, weconsider a Transformer model trained on millions of track sequences from userplaylists. In light of the growing popularity of Transformers in recent years,we analyze the advantages, drawbacks, and technical challenges of using such amodel for mix generation on the service, compared to a more traditionalcollaborative filtering approach. Since its release, Track Mix has beengenerating playlists for millions of users daily, enhancing their musicdiscovery experience on Deezer.", "output": "Track Mix Generation on Music Streaming Services using Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given an origin (O), a destination (D), and a departure time (T), anOrigin-Destination (OD) travel time oracle~(ODT-Oracle) returns an estimate ofthe time it takes to travel from O to D when departing at T. ODT-Oracles serveimportant purposes in map-based services. To enable the construction of suchoracles, we provide a travel-time estimation (TTE) solution that leverageshistorical trajectories to estimate time-varying travel times for OD pairs.The problem is complicated by the fact that multiple historical trajectorieswith different travel times may connect an OD pair, while trajectories may varyfrom one another. To solve the problem, it is crucial to remove outliertrajectories when doing travel time estimation for future queries.We propose a novel, two-stage framework called Diffusion-basedOrigin-destination Travel Time Estimation (DOT), that solves the problem.First, DOT employs a conditioned Pixelated Trajectories (PiT) denoiser thatenables building a diffusion-based PiT inference process by learningcorrelations between OD pairs and historical trajectories. Specifically, givenan OD pair and a departure time, we aim to infer a PiT. Next, DOT encompasses aMasked Vision Transformer~(MViT) that effectively and efficiently estimates atravel time based on the inferred PiT. We report on extensive experiments ontwo real-world datasets that offer evidence that DOT is capable ofoutperforming baseline methods in terms of accuracy, scalability, andexplainability.", "output": "Origin-Destination Travel Time Oracle for Map-based Services."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many popular feature-attribution methods for interpreting deep neuralnetworks rely on computing the gradients of a model's output with respect toits inputs. While these methods can indicate which input features may beimportant for the model's prediction, they reveal little about the innerworkings of the model itself. In this paper, we observe that the gradientcomputation of a model is a special case of a more general formulation usingsemirings. This observation allows us to generalize the backpropagationalgorithm to efficiently compute other interpretable statistics about thegradient graph of a neural network, such as the highest-weighted path andentropy. We implement this generalized algorithm, evaluate it on syntheticdatasets to better understand the statistics it computes, and apply it to studyBERT's behavior on the subject-verb number agreement task (SVA). With thismethod, we (a) validate that the amount of gradient flow through a component ofa model reflects its importance to a prediction and (b) for SVA, identify whichpathways of the self-attention mechanism are most important.", "output": "Generalizing Backpropagation for Gradient-Based Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Applying deep learning techniques, particularly language models (LMs), inontology engineering has raised widespread attention. However, deep learningframeworks like PyTorch and Tensorflow are predominantly developed for Pythonprogramming, while widely-used ontology APIs, such as the OWL API and Jena, areprimarily Java-based. To facilitate seamless integration of these frameworksand APIs, we present Deeponto, a Python package designed for ontologyengineering. The package encompasses a core ontology processing module foundedon the widely-recognised and reliable OWL API, encapsulating its fundamentalfeatures in a more \"Pythonic\" manner and extending its capabilities to includeother essential components including reasoning, verbalisation, normalisation,projection, and more. Building on this module, Deeponto offers a suite oftools, resources, and algorithms that support various ontology engineeringtasks, such as ontology alignment and completion, by harnessing deep learningmethodologies, primarily pre-trained LMs. In this paper, we also demonstratethe practical utility of Deeponto through two use-cases: the Digital HealthCoaching in Samsung Research UK and the Bio-ML track of the Ontology AlignmentEvaluation Initiative (OAEI).", "output": "DeepOnto: A Python Package for Ontology Engineering with Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, physiological data such as electroencephalography (EEG) signalshave attracted significant attention in affective computing. In this context,the main goal is to design an automated model that can assess emotional states.Lately, deep neural networks have shown promising performance in emotionrecognition tasks. However, designing a deep architecture that can extractpractical information from raw data is still a challenge. Here, we introduce adeep neural network that acquires interpretable physiological representationsby a hybrid structure of spatio-temporal encoding and recurrent attentionnetwork blocks. Furthermore, a preprocessing step is applied to the raw datausing graph signal processing tools to perform graph smoothing in the spatialdomain. We demonstrate that our proposed architecture exceeds state-of-the-artresults for emotion classification on the publicly available DEAP dataset. Toexplore the generality of the learned model, we also evaluate the performanceof our architecture towards transfer learning (TL) by transferring the modelparameters from a specific source to other target domains. Using DEAP as thesource dataset, we demonstrate the effectiveness of our model in performingcross-modality TL and improving emotion classification accuracy on DREAMER andthe Emotional English Word (EEWD) datasets, which involve EEG-based emotionclassification tasks with different stimuli.", "output": "A Hybrid End-to-End Spatio-Temporal Attention Neural Network with Graph-Smooth Signals for EEG Emotion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Signed graphs are complex systems that represent trust relationships orpreferences in various domains. Learning node representations in such graphs iscrucial for many mining tasks. Although real-world signed relationships can beinfluenced by multiple latent factors, most existing methods often oversimplifythe modeling of signed relationships by relying on social theories and treatingthem as simplistic factors. This limits their expressiveness and their abilityto capture the diverse factors that shape these relationships. In this paper,we propose DINES, a novel method for learning disentangled node representationsin signed directed graphs without social assumptions. We adopt a disentangledframework that separates each embedding into distinct factors, allowing forcapturing multiple latent factors. We also explore lightweight graphconvolutions that focus solely on sign and direction, without depending onsocial theories. Additionally, we propose a decoder that effectively classifiesan edge's sign by considering correlations between the factors. To furtherenhance disentanglement, we jointly train a self-supervised factordiscriminator with our encoder and decoder. Throughout extensive experiments onreal-world signed directed graphs, we show that DINES effectively learnsdisentangled node representations, and significantly outperforms itscompetitors in the sign prediction task.", "output": "Learning Disentangled Representations in Signed Directed Graphs without Social Assumptions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The scale of large pre-trained models (PTMs) poses significant challenges inadapting to downstream tasks due to the high optimization overhead and storagecosts associated with full-parameter fine-tuning. To address this, many studiesexplore parameter-efficient tuning methods, also framed as \"delta tuning\",which updates only a small subset of parameters, known as \"delta modules\",while keeping the backbone model's parameters fixed. However, the practicalityand flexibility of delta tuning have been limited due to existingimplementations that directly modify the code of the backbone PTMs andhard-code specific delta tuning methods for each PTM. In this paper, we presentOpenDelta, an open-source library that overcomes these limitations by providinga plug-and-play implementation of various delta tuning methods. Our noveltechniques eliminate the need to modify the backbone PTMs' code, makingOpenDelta compatible with different, even novel PTMs. OpenDelta is designed tobe simple, modular, and extensible, providing a comprehensive platform forresearchers and practitioners to adapt large PTMs efficiently.", "output": "OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gaussian Processes (GPs) offer an attractive method for regression oversmall, structured and correlated datasets. However, their deployment ishindered by computational costs and limited guidelines on how to apply GPsbeyond simple low-dimensional datasets. We propose a framework to identify thesuitability of GPs to a given problem and how to set up a robust andwell-specified GP model. The guidelines formalise the decisions of experiencedGP practitioners, with an emphasis on kernel design and options forcomputational scalability. The framework is then applied to a case study ofglacier elevation change yielding more accurate results at test time.", "output": "Beyond Intuition, a Framework for Applying GPs to Real-World Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent text-to-image diffusion models have shown surprising performance ingenerating high-quality images. However, concerns have arisen regarding theunauthorized usage of data during the training process. One example is when amodel trainer collects a set of images created by a particular artist andattempts to train a model capable of generating similar images withoutobtaining permission from the artist. To address this issue, it becomes crucialto detect unauthorized data usage. In this paper, we propose a method fordetecting such unauthorized data usage by planting injected memorization intothe text-to-image diffusion models trained on the protected dataset.Specifically, we modify the protected image dataset by adding unique contentson the images such as stealthy image wrapping functions that are imperceptibleto human vision but can be captured and memorized by diffusion models. Byanalyzing whether the model has memorization for the injected content (i.e.,whether the generated images are processed by the chosen post-processingfunction), we can detect models that had illegally utilized the unauthorizeddata. Our experiments conducted on Stable Diffusion and LoRA model demonstratethe effectiveness of the proposed method in detecting unauthorized data usages.", "output": "How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose a novel architecture (and several variants thereof)based on quantum cryptographic primitives with provable privacy and securityguarantees regarding membership inference attacks on generative models. Ourarchitecture can be used on top of any existing classical or quantum generativemodels. We argue that the use of quantum gates associated with unitaryoperators provides inherent advantages compared to standard DifferentialPrivacy based techniques for establishing guaranteed security from allpolynomial-time adversaries.", "output": "Quantum Solutions to the Privacy vs. Utility Tradeoff."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Order execution is a fundamental task in quantitative finance, aiming atfinishing acquisition or liquidation for a number of trading orders of thespecific assets. Recent advance in model-free reinforcement learning (RL)provides a data-driven solution to the order execution problem. However, theexisting works always optimize execution for an individual order, overlookingthe practice that multiple orders are specified to execute simultaneously,resulting in suboptimality and bias. In this paper, we first present amulti-agent RL (MARL) method for multi-order execution considering practicalconstraints. Specifically, we treat every agent as an individual operator totrade one specific order, while keeping communicating with each other andcollaborating for maximizing the overall profits. Nevertheless, the existingMARL algorithms often incorporate communication among agents by exchanging onlythe information of their partial observations, which is inefficient incomplicated financial market. To improve collaboration, we then propose alearnable multi-round communication protocol, for the agents communicating theintended actions with each other and refining accordingly. It is optimizedthrough a novel action value attribution method which is provably consistentwith the original learning objective yet more efficient. The experiments on thedata from two real-world markets have illustrated superior performance withsignificantly better collaboration effectiveness achieved by our method.", "output": "Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wi-Fi Direct is a promising technology for the support of device-to-devicecommunications (D2D) on commercial mobile devices. However, the standardas-it-is is not sufficient to support the real deployment of networkingsolutions entirely based on D2D such as opportunistic networks. In fact, WiFiDirect presents some characteristics that could limit the autonomous creationof D2D connections among users' personal devices. Specifically, the standardexplicitly requires the user's authorization to establish a connection betweentwo or more devices, and it provides a limited support for inter-groupcommunication. In some cases, this might lead to the creation of isolatedgroups of nodes which cannot communicate among each other. In this paper, wepropose a novel middleware-layer protocol for the efficient configuration andmanagement of WiFi Direct groups (WiFi Direct Group Manager, WFD-GM) to enableautonomous connections and inter-group communication. This enablesopportunistic networks in real conditions (e.g., variable mobility and networksize). WFD-GM defines a context function that takes into account heterogeneousparameters for the creation of the best group configuration in a specific timewindow, including an index of nodes' stability and power levels. We evaluatethe protocol performances by simulating three reference scenarios includingdifferent mobility models, geographical areas and number of nodes. Simulationsare also supported by experimental results related to the evaluation in a realtestbed of the involved context parameters. We compare WFD-GM with thestate-of-the-art solutions and we show that it performs significantly betterthan a Baseline approach in scenarios with medium/low mobility, and it iscomparable with it in case of high mobility, without introducing additionaloverhead.", "output": "Context-Aware Configuration and Management of WiFi Direct Groups for Real Opportunistic Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we demonstrate how sub-Riemannian geometry can be used formanifold learning and surface reconstruction by combining local linearapproximations of a point cloud to obtain lower dimensional bundles. Localapproximations obtained by local PCAs are collected into a rank $k$ tangentsubbundle on $mathbb{R}^d$, $k&lt;d$, which we call a principal subbundle. Thisdetermines a sub-Riemannian metric on $mathbb{R}^d$. We show thatsub-Riemannian geodesics with respect to this metric can successfully beapplied to a number of important problems, such as: explicit construction of anapproximating submanifold $M$, construction of a representation of thepoint-cloud in $mathbb{R}^k$, and computation of distances betweenobservations, taking the learned geometry into account. The reconstruction isguaranteed to equal the true submanifold in the limit case where tangent spacesare estimated exactly. Via simulations, we show that the framework is robustwhen applied to noisy data. Furthermore, the framework generalizes toobservations on an a priori known Riemannian manifold.", "output": "Principal subbundles for dimension reduction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large web-sourced multimodal datasets have powered a slew of new methods forlearning general-purpose visual representations, advancing the state of the artin computer vision and revolutionizing zero- and few-shot recognition. Onecrucial decision facing practitioners is how, if at all, to curate theseever-larger datasets. For example, the creators of the LAION-5B dataset choseto retain only image-caption pairs whose CLIP similarity score exceeded adesignated threshold. In this paper, we propose a new state-of-the-art datafiltering approach motivated by our observation that nearly 40% of LAION'simages contain text that overlaps significantly with the caption. Intuitively,such data could be wasteful as it incentivizes models to perform opticalcharacter recognition rather than learning visual features. However, naivelyremoving all such data could also be wasteful, as it throws away images thatcontain visual features (in addition to overlapping text). Our simple andscalable approach, T-MARS (Text Masking and Re-Scoring), filters out only thosepairs where the text dominates the remaining visual features -- by firstmasking out the text and then filtering out those with a low CLIP similarityscore of the masked image. Experimentally, T-MARS outperforms the top-rankedmethod on the \"medium scale\" of DataComp (a data filtering benchmark) by amargin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematicevaluation on various data pool sizes from 2M to 64M shows that the accuracygains enjoyed by T-MARS linearly increase as data and compute are scaledexponentially. Code is available at ", "output": "T-MARS: Improving Visual Representations by Circumventing Text Feature Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Test-time adaptation (TTA) is a technique aimed at enhancing thegeneralization performance of models by leveraging unlabeled samples solelyduring prediction. Given the need for robustness in neural network systems whenfaced with distribution shifts, numerous TTA methods have recently beenproposed. However, evaluating these methods is often done under differentsettings, such as varying distribution shifts, backbones, and designingscenarios, leading to a lack of consistent and fair benchmarks to validatetheir effectiveness. To address this issue, we present a benchmark thatsystematically evaluates 13 prominent TTA methods and their variants on fivewidely used image classification datasets: CIFAR-10-C, CIFAR-100-C, ImageNet-C,DomainNet, and Office-Home. These methods encompass a wide range of adaptationscenarios (e.g. online adaptation v.s. offline adaptation, instance adaptationv.s. batch adaptation v.s. domain adaptation). Furthermore, we explore thecompatibility of different TTA methods with diverse network backbones. Toimplement this benchmark, we have developed a unified framework in PyTorch,which allows for consistent evaluation and comparison of the TTA methods acrossthe different datasets and network architectures. By establishing thisbenchmark, we aim to provide researchers and practitioners with a reliablemeans of assessing and comparing the effectiveness of TTA methods in improvingmodel robustness and generalization performance. Our code is available at", "output": "Benchmarking Test-Time Adaptation against Distribution Shifts in Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large vision-language models have achieved outstanding performance, but theirsize and computational requirements make their deployment onresource-constrained devices and time-sensitive tasks impractical. Modeldistillation, the process of creating smaller, faster models that maintain theperformance of larger models, is a promising direction towards the solution.This paper investigates the distillation of visual representations in largeteacher vision-language models into lightweight student models using a small-or mid-scale dataset. Notably, this study focuses on open-vocabularyout-of-distribution (OOD) generalization, a challenging problem that has beenoverlooked in previous model distillation literature. We propose two principlesfrom vision and language modality perspectives to enhance student's OODgeneralization: (1) by better imitating teacher's visual representation space,and carefully promoting better coherence in vision-language alignment with theteacher; (2) by enriching the teacher's language representations withinformative and finegrained semantic attributes to effectively distinguishbetween different labels. We propose several metrics and conduct extensiveexperiments to investigate their techniques. The results demonstratesignificant improvements in zero-shot and few-shot student performance onopen-vocabulary out-of-distribution classification, highlighting theeffectiveness of our proposed approaches. Our code will be released at", "output": "Distilling Large Vision-Language Model with Out-of-Distribution Generalizability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study online convex optimization where the possible actions are trace-oneelements in a symmetric cone, generalizing the extensively-studied expertssetup and its quantum counterpart. Symmetric cones provide a unifying frameworkfor some of the most important optimization models, including linear,second-order cone, and semidefinite optimization. Using tools from the field ofEuclidean Jordan Algebras, we introduce the Symmetric-Cone MultiplicativeWeights Update (SCMWU), a projection-free algorithm for online optimizationover the trace-one slice of an arbitrary symmetric cone. We show that SCMWU isequivalent to Follow-the-Regularized-Leader and Online Mirror Descent withsymmetric-cone negative entropy as regularizer. Using this structural result weshow that SCMWU is a no-regret algorithm, and verify our theoretical resultswith extensive experiments. Our results unify and generalize the analysis forthe Multiplicative Weights Update method over the probability simplex and theMatrix Multiplicative Weights Update method over the set of density matrices.", "output": "Multiplicative Updates for Online Convex Optimization over Symmetric Cones."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Segmentation networks are not explicitly imposed to learn global invariantsof an image, such as the shape of an object and the geometry between multipleobjects, when they are trained with a standard loss function. On the otherhand, incorporating such invariants into network training may help improveperformance for various segmentation tasks when they are the intrinsiccharacteristics of the objects to be segmented. One example is segmentation ofaorta and great vessels in computed tomography (CT) images where vessels arefound in a particular geometry in the body due to the human anatomy and theymostly seem as round objects on a 2D CT image. This paper addresses this issueby introducing a new topology-aware loss function that penalizes topologydissimilarities between the ground truth and prediction through persistenthomology. Different from the previously suggested segmentation network designs,which apply the threshold filtration on a likelihood function of the predictionmap and the Betti numbers of the ground truth, this paper proposes to apply theVietoris-Rips filtration to obtain persistence diagrams of both ground truthand prediction maps and calculate the dissimilarity with the Wassersteindistance between the corresponding persistence diagrams. The use of thisfiltration has advantage of modeling shape and geometry at the same time, whichmay not happen when the threshold filtration is applied. Our experiments on4327 CT images of 24 subjects reveal that the proposed topology-aware lossfunction leads to better results than its counterparts, indicating theeffectiveness of this use.", "output": "Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning-based diagnostic system has demonstrated potential inclassifying skin cancer conditions when labeled training example are abundant.However, skin lesion analysis often suffers from a scarcity of labeled data,hindering the development of an accurate and reliable diagnostic system. Inthis work, we leverage multiple skin lesion datasets and investigate thefeasibility of various unsupervised domain adaptation (UDA) methods in binaryand multi-class skin lesion classification. In particular, we assess three UDAtraining schemes: single-, combined-, and multi-source. Our experiment resultsshow that UDA is effective in binary classification, with further improvementbeing observed when imbalance is mitigated. In multi-class task, itsperformance is less prominent, and imbalance problem again needs to beaddressed to achieve above-baseline accuracy. Through our quantitativeanalysis, we find that the test error of multi-class tasks is stronglycorrelated with label shift, and feature-level UDA methods have limitationswhen handling imbalanced datasets. Finally, our study reveals that UDA caneffectively reduce bias against minority groups and promote fairness, evenwithout the explicit use of fairness-focused techniques.", "output": "Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models have an exceptional capability to incorporate newinformation in a contextual manner. However, the full potential of such anapproach is often restrained due to a limitation in the effective contextlength. One solution to this issue is to endow an attention layer with accessto an external memory, which comprises of (key, value) pairs. Yet, as thenumber of documents increases, the proportion of relevant keys to irrelevantones decreases, leading the model to focus more on the irrelevant keys. Weidentify a significant challenge, dubbed the distraction issue, where keyslinked to different semantic values might overlap, making them hard todistinguish. To tackle this problem, we introduce the Focused Transformer(FoT), a technique that employs a training process inspired by contrastivelearning. This novel approach enhances the structure of the (key, value) space,enabling an extension of the context length. Our method allows for fine-tuningpre-existing, large-scale models to lengthen their effective context. This isdemonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. Theresulting models, which we name LongLLaMA, exhibit advancements in tasksrequiring a long context. We further illustrate that our LongLLaMA modelsadeptly manage a $256 k$ context length for passkey retrieval.", "output": "Focused Transformer: Contrastive Training for Context Scaling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous agriculture applications (e.g., inspection, phenotyping, pluckingfruits) require manipulating the plant foliage to look behind the leaves andthe branches. Partial visibility, extreme clutter, thin structures, and unknowngeometry and dynamics for plants make such manipulation challenging. We tacklethese challenges through data-driven methods. We use self-supervision to trainSRPNet, a neural network that predicts what space is revealed on execution of acandidate action on a given plant. We use SRPNet with the cross-entropy methodto predict actions that are effective at revealing space beneath plant foliage.Furthermore, as SRPNet does not just predict how much space is revealed butalso where it is revealed, we can execute a sequence of actions thatincrementally reveal more and more space beneath the plant foliage. Weexperiment with a synthetic (vines) and a real plant (Dracaena) on a physicaltest-bed across 5 settings including 2 settings that test generalization tonovel plant configurations. Our experiments reveal the effectiveness of ouroverall method, PPG, over a competitive hand-crafted exploration method, andthe effectiveness of SRPNet over a hand-crafted dynamics model and relevantablations.", "output": "Push Past Green: Learning to Look Behind Plant Foliage by Moving It."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Feature bagging is a well-established ensembling method which aims to reduceprediction variance by training estimators in an ensemble on random subsamplesor projections of features. Typically, ensembles are chosen to be homogeneous,in the sense the the number of feature dimensions available to an estimator isuniform across the ensemble. Here, we introduce heterogeneous featureensembling, with estimators built on varying number of feature dimensions, andconsider its performance in a linear regression setting. We study an ensembleof linear predictors, each fit using ridge regression on a subset of theavailable features. We allow the number of features included in these subsetsto vary. Using the replica trick from statistical physics, we derive learningcurves for ridge ensembles with deterministic linear masks. We obtain explicitexpressions for the learning curves in the case of equicorrelated data with anisotropic feature noise. Using the derived expressions, we investigate theeffect of subsampling and ensembling, finding sharp transitions in the optimalensembling strategy in the parameter space of noise level, data correlations,and data-task alignment. Finally, we suggest variable-dimension feature baggingas a strategy to mitigate double descent for robust machine learning inpractice.", "output": "Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning from rewards (i.e., reinforcement learning or RL) and learning toimitate a teacher (i.e., teacher-student learning) are two establishedapproaches for solving sequential decision-making problems. To combine thebenefits of these different forms of learning, it is common to train a policyto maximize a combination of reinforcement and teacher-student learningobjectives. However, without a principled method to balance these objectives,prior work used heuristics and problem-specific hyperparameter searches tobalance the two objectives. We present a $textit{principled}$ approach, alongwith an approximate implementation for $textit{dynamically}$ and$textit{automatically}$ balancing when to follow the teacher and when to userewards. The main idea is to adjust the importance of teacher supervision bycomparing the agent's performance to the counterfactual scenario of the agentlearning without teacher supervision and only from rewards. If using teachersupervision improves performance, the importance of teacher supervision isincreased and otherwise it is decreased. Our method, $textit{Teacher GuidedReinforcement Learning}$ (TGRL), outperforms strong baselines across diversedomains without hyper-parameter tuning.", "output": "TGRL: An Algorithm for Teacher Guided Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Artistic Cinemagraph, a fully automated method for creatingcinemagraphs from text descriptions - an especially challenging task whenprompts feature imaginary elements and artistic styles, given the complexity ofinterpreting the semantics and motions of these images. Existing single-imageanimation methods fall short on artistic inputs, and recent text-based videomethods frequently introduce temporal inconsistencies, struggling to keepcertain regions static. To address these challenges, we propose an idea ofsynthesizing image twins from a single text prompt - a pair of an artisticimage and its pixel-aligned corresponding natural-looking twin. While theartistic image depicts the style and appearance detailed in our text prompt,the realistic counterpart greatly simplifies layout and motion analysis.Leveraging existing natural image and video datasets, we can accurately segmentthe realistic image and predict plausible motion given the semanticinformation. The predicted motion can then be transferred to the artistic imageto create the final cinemagraph. Our method outperforms existing approaches increating cinemagraphs for natural landscapes as well as artistic andother-worldly scenes, as validated by automated metrics and user studies.Finally, we demonstrate two extensions: animating existing paintings andcontrolling motion directions using text.", "output": "Synthesizing Artistic Cinemagraphs from Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the increased usage of AI accelerators on mobile and edge devices,on-device machine learning (ML) is gaining popularity. Thousands of proprietaryML models are being deployed today on billions of untrusted devices. Thisraises serious security concerns about model privacy. However, protecting modelprivacy without losing access to the untrusted AI accelerators is a challengingproblem. In this paper, we present a novel on-device model inference system,ShadowNet. ShadowNet protects the model privacy with Trusted ExecutionEnvironment (TEE) while securely outsourcing the heavy linear layers of themodel to the untrusted hardware accelerators. ShadowNet achieves this bytransforming the weights of the linear layers before outsourcing them andrestoring the results inside the TEE. The non-linear layers are also keptsecure inside the TEE. ShadowNet's design ensures efficient transformation ofthe weights and the subsequent restoration of the results. We build a ShadowNetprototype based on TensorFlow Lite and evaluate it on five popular CNNs,namely, MobileNet, ResNet-44, MiniVGG, ResNet-404, and YOLOv4-tiny. Ourevaluation shows that ShadowNet achieves strong security guarantees withreasonable performance, offering a practical solution for secure on-devicemodel inference.", "output": "ShadowNet: A Secure and Efficient On-device Model Inference System for Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The current design of aerodynamic shapes, like airfoils, involvescomputationally intensive simulations to explore the possible design space.Usually, such design relies on the prior definition of design parameters andplaces restrictions on synthesizing novel shapes. In this work, we propose adata-driven shape encoding and generating method, which automatically learnsrepresentations from existing airfoils and uses the learned representations togenerate new airfoils. The representations are then used in the optimization ofsynthesized airfoil shapes based on their aerodynamic performance. Our model isbuilt upon VAEGAN, a neural network that combines Variational Autoencoder withGenerative Adversarial Network and is trained by the gradient-based technique.Our model can (1) encode the existing airfoil into a latent vector andreconstruct the airfoil from that, (2) generate novel airfoils by randomlysampling the latent vectors and mapping the vectors to the airfoil coordinatedomain, and (3) synthesize airfoils with desired aerodynamic properties byoptimizing learned features via a genetic algorithm. Our experiments show thatthe learned features encode shape information thoroughly and comprehensivelywithout predefined design parameters. By interpolating/extrapolating featurevectors or sampling from Gaussian noises, the model can automaticallysynthesize novel airfoil shapes, some of which possess competitive or evenbetter aerodynamic properties comparing to airfoils used for model trainingpurposes. By optimizing shapes on the learned latent domain via a geneticalgorithm, synthesized airfoils can evolve to target aerodynamic properties.This demonstrates an efficient learning-based airfoil design framework, whichencodes and optimizes the airfoil on the latent domain and synthesizespromising airfoil candidates for required aerodynamic performance.", "output": "Airfoil GAN: Encoding and Synthesizing Airfoils for Aerodynamic Shape Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we introduce and study the algebraic generalization of noncommutative convolutional neural networks. We leverage the theory of algebraicsignal processing to model convolutional non commutative architectures, and wederive concrete stability bounds that extend those obtained in the literaturefor commutative convolutional neural networks. We show that non commutativeconvolutional architectures can be stable to deformations on the space ofoperators. We develop the spectral representation of non commutative signalmodels to show that non commutative filters process Fourier componentsindependently of each other. In particular we prove that although the spectraldecompositions of signals in non commutative models are associated toeigenspaces of dimension larger than one, there exists a trade-off betweenstability and selectivity, which is controlled by matrix polynomial functionsin spaces of matrices of low dimension. This tradeoff shows how when thefilters in the algebra are restricted to be stable, there is a loss indiscriminability that is compensated in the network by the pointwisenonlinearities. The results derived in this paper have direct applications andimplications in non commutative convolutional architectures such as groupneural networks, multigraph neural networks, and quaternion neural networks,for which we provide a set of numerical experiments showing their behavior whenperturbations are present.", "output": "Convolutional Filtering and Neural Networks with Non Commutative Algebras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Min-max optimization problems (i.e., min-max games) have been attracting agreat deal of attention because of their applicability to a wide range ofmachine learning problems. Although significant progress has been maderecently, the literature to date has focused on games with independent strategysets; little is known about solving games with dependent strategy sets, whichcan be characterized as min-max Stackelberg games. We introduce two first-ordermethods that solve a large class of convex-concave min-max Stackelberg games,and show that our methods converge in polynomial time. Min-max Stackelberggames were first studied by Wald, under the posthumous name of Wald's maximinmodel, a variant of which is the main paradigm used in robust optimization,which means that our methods can likewise solve many convex robust optimizationproblems. We observe that the computation of competitive equilibria in Fishermarkets also comprises a min-max Stackelberg game. Further, we demonstrate theefficacy and efficiency of our algorithms in practice by computing competitiveequilibria in Fisher markets with varying utility structures. Our experimentssuggest potential ways to extend our theoretical results, by demonstrating howdifferent smoothness properties can affect the convergence rate of ouralgorithms.", "output": "Convex-Concave Min-Max Stackelberg Games."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning algorithms dedicated to financial time series forecastinghave gained a lot of interest. But choosing between several algorithms can bechallenging, as their estimation accuracy may be unstable over time. Onlineaggregation of experts combine the forecasts of a finite set of models in asingle approach without making any assumption about the models. In this paper,a Bernstein Online Aggregation (BOA) procedure is applied to the constructionof long-short strategies built from individual stock return forecasts comingfrom different machine learning models. The online mixture of experts leads toattractive portfolio performances even in environments characterised bynon-stationarity. The aggregation outperforms individual algorithms, offering ahigher portfolio Sharpe Ratio, lower shortfall, with a similar turnover.Extensions to expert and aggregation specialisations are also proposed toimprove the overall mixture on a family of portfolio evaluation metrics.", "output": "Expert Aggregation for Financial Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An increasing number of reports raise concerns about the risk that machinelearning algorithms could amplify health disparities due to biases embedded inthe training data. Seyyed-Kalantari et al. find that models trained on threechest X-ray datasets yield disparities in false-positive rates (FPR) acrosssubgroups on the 'no-finding' label (indicating the absence of disease). Themodels consistently yield higher FPR on subgroups known to be historicallyunderserved, and the study concludes that the models exhibit and potentiallyeven amplify systematic underdiagnosis. We argue that the experimental setup inthe study is insufficient to study algorithmic underdiagnosis. In the absenceof specific knowledge (or assumptions) about the extent and nature of thedataset bias, it is difficult to investigate model bias. Importantly, their useof test data exhibiting the same bias as the training data (due to randomsplitting) severely complicates the interpretation of the reported disparities.", "output": "Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a kernel-spectral embedding algorithm for learning low-dimensionalnonlinear structures from high-dimensional and noisy observations, where thedatasets are assumed to be sampled from an intrinsically low-dimensionalmanifold and corrupted by high-dimensional noise. The algorithm employs anadaptive bandwidth selection procedure which does not rely on prior knowledgeof the underlying manifold. The obtained low-dimensional embeddings can befurther utilized for downstream purposes such as data visualization, clusteringand prediction. Our method is theoretically justified and practicallyinterpretable. Specifically, we establish the convergence of the finalembeddings to their noiseless counterparts when the dimension and size of thesamples are comparably large, and characterize the effect of thesignal-to-noise ratio on the rate of convergence and phase transition. We alsoprove convergence of the embeddings to the eigenfunctions of an integraloperator defined by the kernel map of some reproducing kernel Hilbert spacecapturing the underlying nonlinear structures. Numerical simulations andanalysis of three real datasets show the superior empirical performance of theproposed method, compared to many existing methods, on learning variousmanifolds in diverse applications.", "output": "Learning Low-Dimensional Nonlinear Structures from High-Dimensional Noisy Data: An Integral Operator Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-autoregressive (NAR) generation, which is first proposed in neuralmachine translation (NMT) to speed up inference, has attracted much attentionin both machine learning and natural language processing communities. While NARgeneration can significantly accelerate inference speed for machinetranslation, the speedup comes at the cost of sacrificed translation accuracycompared to its counterpart, autoregressive (AR) generation. In recent years,many new models and algorithms have been designed/proposed to bridge theaccuracy gap between NAR generation and AR generation. In this paper, weconduct a systematic survey with comparisons and discussions of variousnon-autoregressive translation (NAT) models from different aspects.Specifically, we categorize the efforts of NAT into several groups, includingdata manipulation, modeling methods, training criterion, decoding algorithms,and the benefit from pre-trained models. Furthermore, we briefly review otherapplications of NAR models beyond machine translation, such as grammaticalerror correction, text summarization, text style transfer, dialogue, semanticparsing, automatic speech recognition, and so on. In addition, we also discusspotential directions for future exploration, including releasing the dependencyof KD, reasonable training objectives, pre-training for NAR, and widerapplications, etc. We hope this survey can help researchers capture the latestprogress in NAR generation, inspire the design of advanced NAR models andalgorithms, and enable industry practitioners to choose appropriate solutionsfor their applications. The web page of this survey is aturl{", "output": "A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ontology Matching (OM) plays an important role in many domains such asbioinformatics and the Semantic Web, and its research is becoming increasinglypopular, especially with the application of machine learning (ML) techniques.Although the Ontology Alignment Evaluation Initiative (OAEI) represents animpressive effort for the systematic evaluation of OM systems, it still suffersfrom several limitations including limited evaluation of subsumption mappings,suboptimal reference mappings, and limited support for the evaluation ofML-based systems. To tackle these limitations, we introduce five new biomedicalOM tasks involving ontologies extracted from Mondo and UMLS. Each task includesboth equivalence and subsumption matching; the quality of reference mappings isensured by human curation, ontology pruning, etc.; and a comprehensiveevaluation framework is proposed to measure OM performance from variousperspectives for both ML-based and non-ML-based OM systems. We reportevaluation results for OM systems of different types to demonstrate the usageof these resources, all of which are publicly available as part of the newBioML track at OAEI 2022.", "output": "Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training deep neural networks (DNNs) in low-dimensional subspaces is apromising direction for achieving efficient training and better generalizationperformance. Previous works extract the subspaces by using random projection orperforming dimensionality reduction method on the training trajectory, butthese methods can be inefficient or unstable in terms of dimensionality andnumerical operations. In this paper, we connect subspace training to weightaveraging and propose Trainable Weight Averaging (TWA), a general approach forsubspace training that generalizes the previous efforts. TWA is efficient interms of dimensionality and also easy to use, making it a promising new methodfor subspace training. We further design an efficient scheme for subspacetraining to cope with large-scale problems, which allows parallel trainingacross multiple nodes and evenly distributing the memory and computation burdento each node. We apply TWA to efficient neural network training and improvingfine-tuning performance tasks to demonstrate the great efficiency andeffectiveness of our approach. We conduct extensive experiments that covervarious benchmark computer vision and neural language processing tasks withvarious architectures. The code of implementation is available at", "output": "Trainable Weight Averaging: A General Approach for Subspace Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline reinforcement learning has shown great promise in leveraging largepre-collected datasets for policy learning, allowing agents to forgooften-expensive online data collection. However, offline reinforcement learningfrom visual observations with continuous action spaces remains under-explored,with a limited understanding of the key challenges in this complex domain. Inthis paper, we establish simple baselines for continuous control in the visualdomain and introduce a suite of benchmarking tasks for offline reinforcementlearning from visual observations designed to better represent the datadistributions present in real-world offline RL problems and guided by a set ofdesiderata for offline RL from visual observations, including robustness tovisual distractions and visually identifiable changes in dynamics. Using thissuite of benchmarking tasks, we show that simple modifications to two popularvision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2,suffice to outperform existing offline RL methods and establish competitivebaselines for continuous control in the visual domain. We rigorously evaluatethese algorithms and perform an empirical evaluation of the differences betweenstate-of-the-art model-based and model-free offline RL methods for continuouscontrol from visual observations. All code and data used in this evaluation areopen-sourced to facilitate progress in this domain.", "output": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial intelligence (AI) techniques have been proposed for automatinganalysis of short axis (SAX) cine cardiac magnetic resonance (CMR), but no CMRanalysis tool exists to automatically analyse large (unstructured) clinical CMRdatasets. We develop and validate a robust AI tool for start-to-end automaticquantification of cardiac function from SAX cine CMR in large clinicaldatabases. Our pipeline for processing and analysing CMR databases includesautomated steps to identify the correct data, robust image pre-processing, anAI algorithm for biventricular segmentation of SAX CMR and estimation offunctional biomarkers, and automated post-analysis quality control to detectand correct errors. The segmentation algorithm was trained on 2793 CMR scansfrom two NHS hospitals and validated on additional cases from this dataset(n=414) and five external datasets (n=6888), including scans of patients with arange of diseases acquired at 12 different centres using CMR scanners from allmajor vendors. Median absolute errors in cardiac biomarkers were within therange of inter-observer variability: &lt;8.4mL (left ventricle volume), &lt;9.2mL(right ventricle volume), &lt;13.3g (left ventricular mass), and &lt;5.9% (ejectionfraction) across all datasets. Stratification of cases according to phenotypesof cardiac disease and scanner vendors showed good performance across allgroups. We show that our proposed tool, which combines image pre-processingsteps, a domain-generalisable AI algorithm trained on a large-scalemulti-domain CMR dataset and quality control steps, allows robust analysis of(clinical or research) databases from multiple centres, vendors, and cardiacdiseases. This enables translation of our tool for use in fully-automatedprocessing of large multi-centre databases.", "output": "An AI tool for automated analysis of large-scale unstructured clinical cine CMR databases."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Content creators compete for user attention. Their reach crucially depends onalgorithmic choices made by developers on online platforms. To maximizeexposure, many creators adapt strategically, as evidenced by examples like thesprawling search engine optimization industry. This begets competition for thefinite user attention pool. We formalize these dynamics in what we call anexposure game, a model of incentives induced by algorithms, including modernfactorization and (deep) two-tower architectures. We prove that seeminglyinnocuous algorithmic choices, e.g., non-negative vs. unconstrainedfactorization, significantly affect the existence and character of (Nash)equilibria in exposure games. We proffer use of creator behavior models, likeexposure games, for an (ex-ante) pre-deployment audit. Such an audit canidentify misalignment between desirable and incentivized content, and thuscomplement post-hoc measures like content filtering and moderation. To thisend, we propose tools for numerically finding equilibria in exposure games, andillustrate results of an audit on the MovieLens and LastFM datasets. Amongelse, we find that the strategically produced content exhibits strongdependence between algorithmic exploration and content diversity, and betweenmodel expressivity and bias towards gender-based user and creator groups.", "output": "Modeling Content Creator Incentives on Algorithm-Curated Platforms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning research has long focused on models rather than datasets,and prominent datasets are used for common ML tasks without regard to thebreadth, difficulty, and faithfulness of the underlying problems. Neglectingthe fundamental importance of data has given rise to inaccuracy, bias, andfragility in real-world applications, and research is hindered by saturationacross existing dataset benchmarks. In response, we present DataPerf, acommunity-led benchmark suite for evaluating ML datasets and data-centricalgorithms. We aim to foster innovation in data-centric AI through competition,comparability, and reproducibility. We enable the ML community to iterate ondatasets, instead of just architectures, and we provide an open, onlineplatform with multiple rounds of challenges to support this iterativedevelopment. The first iteration of DataPerf contains five benchmarks coveringa wide spectrum of data-centric techniques, tasks, and modalities in vision,speech, acquisition, debugging, and diffusion prompting, and we support hostingnew contributed benchmarks from the community. The benchmarks, onlineevaluation platform, and baseline implementations are open source, and theMLCommons Association will maintain DataPerf to ensure long-term benefits toacademia and industry.", "output": "DataPerf: Benchmarks for Data-Centric AI Development."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electrical Distribution Systems are extensively penetrated with DistributedEnergy Resources (DERs) to cater the energy demands with the general perceptionthat it enhances the system's resilience. However, integration of DERs mayadversely affect the grid operation and affect the system resilience due tovarious factors like their intermittent availability, dynamics of weatherconditions, non-linearity, complexity, number of malicious threats, andimproved reliability requirements of consumers. This paper proposes amethodology to evaluate the planning and operational resilience of powerdistribution systems under extreme events and determines the withstandcapability of the electrical network. The proposed framework is developed byeffectively employing the complex network theory. Correlated networks forundesirable configurations are developed from the time series data of activepower monitored at nodes of the electrical network. For these correlatednetworks, computed the network parameters such as clustering coefficient,assortative coefficient, average degree and power law exponent for theanticipation; and percolation threshold for the determination of the networkwithstand capability under extreme conditions. The proposed methodology is alsosuitable for identifying the hosting capacity of solar panels in the systemwhile maintaining resilience under different unfavourable conditions andidentifying the most critical nodes of the system that could drive the systeminto non-resilience. This framework is demonstrated on IEEE 123 node testfeeder by generating active power time-series data for a variety of electricalconditions using simulation software, GridLAB-D. The percolation thresholdresulted as an effective metric for the determination of the planning andoperational resilience of the power distribution system.", "output": "Evaluating the Planning and Operational Resilience of Electrical Distribution Systems with Distributed Energy Resources using Complex Network Theory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The occurrence of West Nile Virus (WNV) represents one of the most commonmosquito-borne zoonosis viral infections. Its circulation is usually associatedwith climatic and environmental conditions suitable for vector proliferationand virus replication. On top of that, several statistical models have beendeveloped to shape and forecast WNV circulation: in particular, the recentmassive availability of Earth Observation (EO) data, coupled with thecontinuous advances in the field of Artificial Intelligence, offer valuableopportunities.In this paper, we seek to predict WNV circulation by feeding Deep NeuralNetworks (DNNs) with satellite images, which have been extensively shown tohold environmental and climatic features. Notably, while previous approachesanalyze each geographical site independently, we propose a spatial-awareapproach that considers also the characteristics of close sites. Specifically,we build upon Graph Neural Networks (GNN) to aggregate features fromneighbouring places, and further extend these modules to consider multiplerelations, such as the difference in temperature and soil moisture between twosites, as well as the geographical distance. Moreover, we inject time-relatedinformation directly into the model to take into account the seasonality ofvirus spread.We design an experimental setting that combines satellite images - fromLandsat and Sentinel missions - with ground truth observations of WNVcirculation in Italy. We show that our proposed Multi-Adjacency Graph AttentionNetwork (MAGAT) consistently leads to higher performance when paired with anappropriate pre-training stage. Finally, we assess the importance of eachcomponent of MAGAT in our ablation studies.", "output": "Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weight decay is one of the most widely used forms of regularization in deeplearning, and has been shown to improve generalization and robustness. Theoptimization objective driving weight decay is a sum of losses plus a termproportional to the sum of squared weights. This paper argues that stochasticgradient descent (SGD) may be an inefficient algorithm for this objective. Forneural networks with ReLU activations, solutions to the weight decay objectiveare equivalent to those of a different objective in which the regularizationterm is instead a sum of products of $ell_2$ (not squared) norms of the inputand output weights associated with each ReLU neuron. This alternative (andeffectively equivalent) regularization suggests a novel proximal gradientalgorithm for network training. Theory and experiments support the new trainingapproach, showing that it can converge much faster to the sparse solutions itshares with standard weight decay training.", "output": "PathProx: A Proximal Gradient Algorithm for Weight Decay Regularized Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent developments in the methods of explainable AI (XAI) allow researchersto explore the inner workings of deep neural networks (DNNs), revealing crucialinformation about input-output relationships and realizing how data connectswith machine learning models. In this paper we explore interpretability of DNNmodels designed to identify jets coming from top quark decay in high energyproton-proton collisions at the Large Hadron Collider (LHC). We review a subsetof existing top tagger models and explore different quantitative methods toidentify which features play the most important roles in identifying the topjets. We also investigate how and why feature importance varies acrossdifferent XAI metrics, how correlations among features impact theirexplainability, and how latent space representations encode information as wellas correlate with physically meaningful quantities. Our studies uncover somemajor pitfalls of existing XAI methods and illustrate how they can be overcometo obtain consistent and meaningful interpretation of these models. Weadditionally illustrate the activity of hidden layers as Neural ActivationPattern (NAP) diagrams and demonstrate how they can be used to understand howDNNs relay information across the layers and how this understanding can help tomake such models significantly simpler by allowing effective modelreoptimization and hyperparameter tuning. These studies not only facilitate amethodological approach to interpreting models but also unveil new insightsabout what these models learn. Incorporating these observations into augmentedmodel design, we propose the Particle Flow Interaction Network (PFIN) model anddemonstrate how interpretability-inspired model augmentation can improve toptagging performance.", "output": "A Detailed Study of Interpretability of Deep Neural Network based Top Taggers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Physics-informed neural networks (PINNs) are emerging as popular mesh-freesolvers for partial differential equations (PDEs). Recent extensions decomposethe domain, apply different PINNs to solve the problem in each subdomain, andstitch the subdomains at the interface. Thereby, they can further alleviate theproblem complexity, reduce the computational cost, and allow parallelization.However, the performance of multi-domain PINNs is sensitive to the choice ofthe interface conditions. While quite a few conditions have been proposed,there is no suggestion about how to select the conditions according to specificproblems. To address this gap, we propose META Learning of Interface Conditions(METALIC), a simple, efficient yet powerful approach to dynamically determineappropriate interface conditions for solving a family of parametric PDEs.Specifically, we develop two contextual multi-arm bandit (MAB) models. Thefirst one applies to the entire training course, and online updates a Gaussianprocess (GP) reward that given the PDE parameters and interface conditionspredicts the performance. We prove a sub-linear regret bound for both UCB andThompson sampling, which in theory guarantees the effectiveness of our MAB. Thesecond one partitions the training into two stages, one is the stochastic phaseand the other deterministic phase; we update a GP reward for each phase toenable different condition selections at the two stages to further bolster theflexibility and performance. We have shown the advantage of METALIC on fourbench-mark PDE families.", "output": "Meta Learning of Interface Conditions for Multi-Domain Physics-Informed Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks, being susceptible to adversarial attacks, should face astrict level of scrutiny before being deployed in critical or adversarialapplications. This paper uses ideas from Chaos Theory to explain, analyze, andquantify the degree to which neural networks are susceptible to or robustagainst adversarial attacks. To this end, we present a new metric, the\"susceptibility ratio,\" given by $hat Psi(h, theta)$, which captures howgreatly a model's output will be changed by perturbations to a given input.Our results show that susceptibility to attack grows significantly with thedepth of the model, which has safety implications for the design of neuralnetworks for production environments. We provide experimental evidence of therelationship between $hat Psi$ and the post-attack accuracy of classificationmodels, as well as a discussion of its application to tasks lacking harddecision boundaries. We also demonstrate how to quickly and easily approximatethe certified robustness radii for extremely large models, which until now hasbeen computationally infeasible to calculate directly.", "output": "Chaos Theory and Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With recent advancements in diffusion models, users can generate high-qualityimages by writing text prompts in natural language. However, generating imageswith desired details requires proper prompts, and it is often unclear how amodel reacts to different prompts or what the best prompts are. To helpresearchers tackle these critical challenges, we introduce DiffusionDB, thefirst large-scale text-to-image prompt dataset totaling 6.5TB, containing 14million images generated by Stable Diffusion, 1.8 million unique prompts, andhyperparameters specified by real users. We analyze the syntactic and semanticcharacteristics of prompts. We pinpoint specific hyperparameter values andprompt styles that can lead to model errors and present evidence of potentiallyharmful model usage, such as the generation of misinformation. Theunprecedented scale and diversity of this human-actuated dataset provideexciting research opportunities in understanding the interplay between promptsand generative models, detecting deepfakes, and designing human-AI interactiontools to help users more easily use these models. DiffusionDB is publiclyavailable at: ", "output": "DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We provide the first finite-particle convergence rate for Stein variationalgradient descent (SVGD), a popular algorithm for approximating a probabilitydistribution with a collection of particles. Specifically, whenever the targetdistribution is sub-Gaussian with a Lipschitz score, SVGD with n particles andan appropriate step size sequence drives the kernel Stein discrepancy to zeroat an order 1/sqrt(log log n) rate. We suspect that the dependence on n can beimproved, and we hope that our explicit, non-asymptotic proof strategy willserve as a template for future refinements.", "output": "A Finite-Particle Convergence Rate for Stein Variational Gradient Descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Meta-learning of numerical algorithms for a given task consists of thedata-driven identification and adaptation of an algorithmic structure and theassociated hyperparameters. To limit the complexity of the meta-learningproblem, neural architectures with a certain inductive bias towards favorablealgorithmic structures can, and should, be used. We generalize our previouslyintroduced Runge-Kutta neural network to a recursively recurrent neural network(R2N2) superstructure for the design of customized iterative algorithms. Incontrast to off-the-shelf deep learning approaches, it features a distinctdivision into modules for generation of information and for the subsequentassembly of this information towards a solution. Local information in the formof a subspace is generated by subordinate, inner, iterations of recurrentfunction evaluations starting at the current outer iterate. The update to thenext outer iterate is computed as a linear combination of these evaluations,reducing the residual in this space, and constitutes the output of the network.We demonstrate that regular training of the weight parameters inside theproposed superstructure on input/output data of various computational problemclasses yields iterations similar to Krylov solvers for linear equationsystems, Newton-Krylov solvers for nonlinear equation systems, and Runge-Kuttaintegrators for ordinary differential equations. Due to its modularity, thesuperstructure can be readily extended with functionalities needed to representmore general classes of iterative algorithms traditionally based on Taylorseries expansions.", "output": "A Recursively Recurrent Neural Network (R2N2) Architecture for Learning Iterative Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Compared to conventional bilingual translation systems, massivelymultilingual machine translation is appealing because a single model cantranslate into multiple languages and benefit from knowledge transfer for lowresource languages. On the other hand, massively multilingual models sufferfrom the curse of multilinguality, unless scaling their size massively, whichincreases their training and inference costs. Sparse Mixture-of-Experts modelsare a way to drastically increase model capacity without the need for aproportional amount of computing. The recently released NLLB-200 is an exampleof such a model. It covers 202 languages but requires at least four 32GB GPUsjust for inference. In this work, we propose a pruning method that allows theremoval of up to 80% of experts with a negligible loss in translation quality,which makes it feasible to run the model on a single 32GB GPU. Further analysissuggests that our pruning metrics allow to identify language-specific expertsand prune non-relevant experts for a given language pair.", "output": "Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the past years, deep learning has seen an increase in usage in the domainof histopathological applications. However, while these approaches have showngreat potential, in high-risk environments deep learning models need to be ableto judge their uncertainty and be able to reject inputs when there is asignificant chance of misclassification. In this work, we conduct a rigorousevaluation of the most commonly used uncertainty and robustness methods for theclassification of Whole Slide Images, with a focus on the task of selectiveclassification, where the model should reject the classification in situationsin which it is uncertain. We conduct our experiments on tile-level under theaspects of domain shift and label noise, as well as on slide-level. In ourexperiments, we compare Deep Ensembles, Monte-Carlo Dropout, StochasticVariational Inference, Test-Time Data Augmentation as well as ensembles of thelatter approaches. We observe that ensembles of methods generally lead tobetter uncertainty estimates as well as an increased robustness towards domainshifts and label noise, while contrary to results from classical computervision benchmarks no systematic gain of the other methods can be shown. Acrossmethods, a rejection of the most uncertain samples reliably leads to asignificant increase in classification accuracy on both in-distribution as wellas out-of-distribution data. Furthermore, we conduct experiments comparingthese methods under varying conditions of label noise. Lastly, we publish ourcode framework to facilitate further research on uncertainty estimation onhistopathological data.", "output": "Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Answering complex queries on incomplete knowledge graphs is a challengingtask where a model needs to answer complex logical queries in the presence ofmissing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022)showed that neural link predictors could also be used for answering complexqueries: their Continuous Query Decomposition (CQD) method works by decomposingcomplex queries into atomic sub-queries, answers them using neural linkpredictors and aggregates their scores via t-norms for ranking the answers toeach complex query. However, CQD does not handle negations and only uses thetraining signal from atomic training queries: neural link prediction scores arenot calibrated to interact together via fuzzy logic t-norms during complexquery answering. In this work, we propose to address this problem by training aparameter-efficient score adaptation model to re-calibrate neural linkprediction scores: this new component is trained on complex queries byback-propagating through the complex query-answering process. Our method,CQD$^{A}$, produces significantly more accurate results than currentstate-of-the-art methods, improving from $34.4$ to $35.1$ Mean Reciprocal Rankvalues averaged across all datasets and query types while using $leq 35%$ ofthe available training query types. We further show that CQD$^{A}$ isdata-efficient, achieving competitive results with only $1%$ of the trainingdata, and robust in out-of-domain evaluations.", "output": "Adapting Neural Link Predictors for Complex Query Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to accurately locate and navigate to a specific object is acrucial capability for embodied agents that operate in the real world andinteract with objects to complete tasks. Such object navigation tasks usuallyrequire large-scale training in visual environments with labeled objects, whichgeneralizes poorly to novel objects in unknown environments. In this work, wepresent a novel zero-shot object navigation method, Exploration with SoftCommonsense constraints (ESC), that transfers commonsense knowledge inpre-trained models to open-world object navigation without any navigationexperience nor any other training on the visual environments. First, ESCleverages a pre-trained vision and language model for open-world prompt-basedgrounding and a pre-trained commonsense language model for room and objectreasoning. Then ESC converts commonsense knowledge into navigation actions bymodeling it as soft logic predicates for efficient exploration. Extensiveexperiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC methodimproves significantly over baselines, and achieves new state-of-the-artresults for zero-shot object navigation (e.g., 288% relative Success Rateimprovement than CoW on MP3D).", "output": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Shapley value is arguably the most popular approach for assigning ameaningful contribution value to players in a cooperative game, which hasrecently been used intensively in explainable artificial intelligence. Themeaningfulness is due to axiomatic properties that only the Shapley valuesatisfies, which, however, comes at the expense of an exact computation growingexponentially with the number of agents. Accordingly, a number of works aredevoted to the efficient approximation of the Shapley values, most of themrevolve around the notion of an agent's marginal contribution. In this paper,we propose with SVARM and Stratified SVARM two parameter-free anddomain-independent approximation algorithms based on a representation of theShapley value detached from the notion of marginal contributions. We proveunmatched theoretical guarantees regarding their approximation quality andprovide empirical results including synthetic games as well as commonexplainability use cases comparing ourselves with state-of-the-art methods.", "output": "Approximating the Shapley Value without Marginal Contributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the course of the past two decades, a substantial body of research hassubstantiated the viability of utilising cardiac signals as a biometricmodality. This paper presents a novel approach for patient identification inhealthcare systems using electrocardiogram signals. A convolutional neuralnetwork is used to classify users based on images extracted from ECG signals.The proposed identification system is evaluated in multiple databases,providing a comprehensive understanding of its potential in real-worldscenarios. The impact of Cardiovascular Diseases on generic user identificationhas been largely overlooked in previous studies. The presented method takesinto account the cardiovascular condition of the patients, ensuring that theresults obtained are not biased or limited. Furthermore, the results obtainedare consistent and reliable, with lower error rates and higher accuracymetrics, as demonstrated through extensive experimentation. All these featuresmake the proposed method a valuable contribution to the field of patientidentification in healthcare systems, and make it a strong contender forpractical applications.", "output": "Unleashing the Power of Electrocardiograms: A novel approach for Patient Identification in Healthcare Systems with ECG Signals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays many research articles are prefaced with research highlights tosummarize the main findings of the paper. Highlights not only help researchersprecisely and quickly identify the contributions of a paper, they also enhancethe discoverability of the article via search engines. We aim to automaticallyconstruct research highlights given certain segments of a research paper. Weuse a pointer-generator network with coverage mechanism and a contextualembedding layer at the input that encodes the input tokens into SciBERTembeddings. We test our model on a benchmark dataset, CSPubSum, and alsopresent MixSub, a new multi-disciplinary corpus of papers for automaticresearch highlight generation. For both CSPubSum and MixSub, we have observedthat the proposed model achieves the best performance compared to relatedvariants and other models proposed in the literature. On the CSPubSum dataset,our model achieves the best performance when the input is only the abstract ofa paper as opposed to other segments of the paper. It produces ROUGE-1, ROUGE-2and ROUGE-L F1-scores of 38.26, 14.26 and 35.51, respectively, METEOR score of32.62, and BERTScore F1 of 86.65 which outperform all other baselines. On thenew MixSub dataset, where only the abstract is the input, our proposed model(when trained on the whole training corpus without distinguishing between thesubject categories) achieves ROUGE-1, ROUGE-2 and ROUGE-L F1-scores of 31.78,9.76 and 29.3, respectively, METEOR score of 24.00, and BERTScore F1 of 85.25.", "output": "Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of mixed sparse linear regression with twocomponents, where two real $k$-sparse signals $beta_1, beta_2$ are to berecovered from $n$ unlabelled noisy linear measurements. The sparsity isallowed to be sublinear in the dimension, and additive noise is assumed to beindependent Gaussian with variance $sigma^2$. Prior work has shown that theproblem suffers from a $frac{k}{SNR^2}$-to-$frac{k^2}{SNR^2}$statistical-to-computational gap, resembling other computationally challenginghigh-dimensional inference problems such as Sparse PCA and Robust Sparse MeanEstimation; here $SNR$ is the signal-to-noise ratio. We establish the existenceof a more extensive computational barrier for this problem through the methodof low-degree polynomials, but show that the problem is computationally hardonly in a very narrow symmetric parameter regime. We identify a smoothinformation-computation tradeoff between the sample complexity $n$ and runtimefor any randomized algorithm in this hard regime. Via a simple reduction, thisprovides novel rigorous evidence for the existence of a computational barrierto solving exact support recovery in sparse phase retrieval with samplecomplexity $n = tilde{o}(k^2)$. Our second contribution is to analyze a simplethresholding algorithm which, outside of the narrow regime where the problem ishard, solves the associated mixed regression detection problem in $O(np)$ timewith square-root the number of samples and matches the sample complexityrequired for (non-mixed) sparse linear regression; this allows the recoveryproblem to be subsequently solved by state-of-the-art techniques from the densecase. As a special case of our results, we show that this simple algorithm isorder-optimal among a large family of algorithms in solving exact signedsupport recovery in sparse linear regression.", "output": "Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in equivariant graph neural networks (GNNs) have made deeplearning amenable to developing fast surrogate models to expensive ab initioquantum mechanics (QM) approaches for molecular potential predictions. However,building accurate and transferable potential models using GNNs remainschallenging, as the data is greatly limited by the expensive computationalcosts and level of theory of QM methods, especially for large and complexmolecular systems. In this work, we propose denoise pretraining onnonequilibrium molecular conformations to achieve more accurate andtransferable GNN potential predictions. Specifically, atomic coordinates ofsampled nonequilibrium conformations are perturbed by random noises and GNNsare pretrained to denoise the perturbed molecular conformations which recoversthe original coordinates. Rigorous experiments on multiple benchmarks revealthat pretraining significantly improves the accuracy of neural potentials.Furthermore, we show that the proposed pretraining approach is model-agnostic,as it improves the performance of different invariant and equivariant GNNs.Notably, our models pretrained on small molecules demonstrate remarkabletransferability, improving performance when fine-tuned on diverse molecularsystems, including different elements, charged molecules, biomolecules, andlarger systems. These results highlight the potential for leveraging denoisepretraining approaches to build more generalizable neural potentials forcomplex molecular systems.", "output": "Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, flat minima are proven to be effective for improving generalizationand sharpness-aware minimization (SAM) achieves state-of-the-art performance.Yet the current definition of flatness discussed in SAM and its follow-ups arelimited to the zeroth-order flatness (i.e., the worst-case loss within aperturbation radius). We show that the zeroth-order flatness can beinsufficient to discriminate minima with low generalization error from thosewith high generalization error both when there is a single minimum or multipleminima within the given perturbation radius. Thus we present first-orderflatness, a stronger measure of flatness focusing on the maximal gradient normwithin a perturbation radius which bounds both the maximal eigenvalue ofHessian at local minima and the regularization function of SAM. We also presenta novel training procedure named Gradient norm Aware Minimization (GAM) to seekminima with uniformly small curvature across all directions. Experimentalresults show that GAM improves the generalization of models trained withcurrent optimizers such as SGD and AdamW on various datasets and networks.Furthermore, we show that GAM can help SAM find flatter minima and achievebetter generalization.", "output": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a huge, online stream of time-evolving events with multiple attributes,such as online shopping logs: (item, price, brand, time), and local mobilityactivities: (pick-up and drop-off locations, time), how can we summarize large,dynamic high-order tensor streams? How can we see any hidden patterns, rules,and anomalies? Our answer is to focus on two types of patterns, i.e.,''regimes'' and ''components'', for which we present CubeScope, an efficientand effective method over high-order tensor streams. Specifically, itidentifies any sudden discontinuity and recognizes distinct dynamical patterns,''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it alsoperforms multi-way summarization for all attributes (e.g., item, price, brand,and time) and discovers hidden ''components'' representing latent groups (e.g.,item/brand groups) and their relationship. Thanks to its concise but effectivesummarization, CubeScope can also detect the sudden appearance of anomalies andidentify the types of anomalies that occur in practice. Our proposed method hasthe following properties: (a) Effective: it captures dynamical multi-aspectpatterns, i.e., regimes and components, and statistically summarizes all theevents; (b) General: it is practical for successful application to datacompression, pattern discovery, and anomaly detection on various types oftensor streams; (c) Scalable: our algorithm does not depend on the length ofthe data stream and its dimensionality. Extensive experiments on real datasetsdemonstrate that CubeScope finds meaningful patterns and anomalies correctly,and consistently outperforms the state-of-the-art methods as regards accuracyand execution speed.", "output": "Fast and Multi-aspect Mining of Complex Time-stamped Event Streams."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic Robotic Assembly Sequence Planning (RASP) can significantly improveproductivity and resilience in modern manufacturing along with the growing needfor greater product customization. One of the main challenges in realizing suchautomation resides in efficiently finding solutions from a growing number ofpotential sequences for increasingly complex assemblies. Besides, costlyfeasibility checks are always required for the robotic system. To address this,we propose a holistic graphical approach including a graph representationcalled Assembly Graph for product assemblies and a policy architecture, GraphAssembly Processing Network, dubbed GRACE for assembly sequence generation.Secondly, we use GRACE to extract meaningful information from the graph inputand predict assembly sequences in a step-by-step manner. In experiments, weshow that our approach can predict feasible assembly sequences across productvariants of aluminum profiles based on data collected in simulation of adual-armed robotic system. We further demonstrate that our method is capable ofdetecting infeasible assemblies, substantially alleviating the undesirableimpacts from false predictions, and hence facilitating real-world deploymentsoon. Code and training data are available at ", "output": "Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose sandwiched video compression -- a video compression system thatwraps neural networks around a standard video codec. The sandwich frameworkconsists of a neural pre- and post-processor with a standard video codecbetween them. The networks are trained jointly to optimize a rate-distortionloss function with the goal of significantly improving over the standard codecin various compression scenarios. End-to-end training in this setting requiresa differentiable proxy for the standard video codec, which incorporatestemporal processing with motion compensation, inter/intra mode decisions, andin-loop filtering. We propose differentiable approximations to key video codeccomponents and demonstrate that, in addition to providing meaningfulcompression improvements over the standard codec, the neural codes of thesandwich lead to significantly better rate-distortion performance in twoimportant scenarios.When transporting high-resolution video via low-resolutionHEVC, the sandwich system obtains 6.5 dB improvements over standard HEVC. Moreimportantly, using the well-known perceptual similarity metric, LPIPS, weobserve 30% improvements in rate at the same quality over HEVC. Last but notleast, we show that pre- and post-processors formed by verymodestly-parameterized, light-weight networks can closely approximate theseresults.", "output": "Sandwiched Video Compression: Efficiently Extending the Reach of Standard Codecs with Neural Wrappers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data privacy concerns has made centralized training of data, which isscattered across silos, infeasible, leading to the need for collaborativelearning frameworks. To address that, two prominent frameworks emerged, i.e.,federated learning (FL) and split learning (SL). While FL has establishedvarious benchmark frameworks and research libraries,SL currently lacks aunified library despite its diversity in terms of label sharing, modelaggregation, and cut layer choice. This lack of standardization makes comparingSL paradigms difficult. To address this, we propose SLPerf, a unified researchframework and open research library for SL, and conduct extensive experimentson four widely-used datasets under both IID and Non-IID data settings. Ourcontributions include a comprehensive survey of recently proposed SL paradigms,a detailed benchmark comparison of different SL paradigms in differentsituations, and rich engineering take-away messages and research insights forimproving SL paradigms. SLPerf can facilitate SL algorithm development and fairperformance comparisons. The code is available at .", "output": "SLPerf: a Unified Framework for Benchmarking Split Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Examination of the umbilical artery with Doppler ultrasonography is performedto investigate blood supply to the fetus through the umbilical cord, which isvital for the monitoring of fetal health. Such examination involves severalsteps that must be performed correctly: identifying suitable sites on theumbilical artery for the measurement, acquiring the blood flow curve in theform of a Doppler spectrum, and ensuring compliance to a set of qualitystandards. These steps rely heavily on the operator's skill, and the shortageof experienced sonographers has thus created a demand for machine assistance.In this work, we propose an automatic system to fill the gap. By using amodified Faster R-CNN network, we obtain an algorithm that can suggestlocations suitable for Doppler measurement. Meanwhile, we have also developed amethod for assessment of the Doppler spectrum's quality. The proposed system isvalidated on 657 images from a national ultrasound screening database, withresults demonstrating its potential as a guidance system.", "output": "An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human beings cooperatively navigate rule-constrained environments by adheringto mutually known navigational patterns, which may be represented asdirectional pathways or road lanes. Inferring these navigational patterns fromincompletely observed environments is required for intelligent mobile robotsoperating in unmapped locations. However, algorithmically defining thesenavigational patterns is nontrivial. This paper presents the firstself-supervised learning (SSL) method for learning to infer navigationalpatterns in real-world environments from partial observations only. We explainhow geometric data augmentation, predictive world modeling, and aninformation-theoretic regularizer enables our model to predict an unbiasedlocal directional soft lane probability (DSLP) field in the limit of infinitedata. We demonstrate how to infer global navigational patterns by fitting amaximum likelihood graph to the DSLP field. Experiments show that our SSL modeloutperforms two SOTA supervised lane graph prediction models on the nuScenesdataset. We propose our SSL method as a scalable and interpretable continuallearning paradigm for navigation by perception. Code is available at", "output": "Learning to Predict Navigational Patterns from Partial Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In a vertical federated learning (VFL) system consisting of a central serverand many distributed clients, the training data are vertically partitioned suchthat different features are privately stored on different clients. The problemof split VFL is to train a model split between the server and the clients. Thispaper aims to address two major challenges in split VFL: 1) performancedegradation due to straggling clients during training; and 2) data and modelprivacy leakage from clients' uploaded data embeddings. We propose FedVS tosimultaneously address these two challenges. The key idea of FedVS is to designsecret sharing schemes for the local data and models, such thatinformation-theoretical privacy against colluding clients and curious server isguaranteed, and the aggregation of all clients' embeddings is reconstructedlosslessly, via decrypting computation shares from the non-straggling clients.Extensive experiments on various types of VFL datasets (including tabular, CV,and multi-view) demonstrate the universal advantages of FedVS in stragglermitigation and privacy protection over baseline protocols.", "output": "FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spain is the third-largest producer of pork meat in the world, and many farmsin several regions depend on the evolution of this market. However, the currentpricing system is unfair, as some actors have better market information thanothers. In this context, historical pricing is an easy-to-find and affordabledata source that can help all agents to be better informed. However, the timelag in data acquisition can affect their pricing decisions. In this paper, westudy the effect that data acquisition delay has on a price prediction systemusing multiple prediction algorithms. We describe the integration of the bestproposal into a decision support system prototype and test it in a real-casescenario. Specifically, we use public data from the most important regionalpork meat markets in Spain published by the Ministry of Agriculture with atwo-week delay and subscription-based data of the same markets obtained on thesame day. The results show that the error difference between the best publicand data subscription models is 0.6 Euro cents in favor of the data withoutdelay. The market dimension makes these differences significant in the supplychain, giving pricing agents a better tool to negotiate market prices.", "output": "Effects of data time lag in a decision-making system using machine learning for pork price prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding differences between sub-stellar spectral data and models hasproven to be a major challenge, especially for self-consistent model grids thatare necessary for a thorough investigation of brown dwarf atmospheres. Usingthe supervised machine learning method of the random forest, we study theinformation content of 14 previously published model grids of brown dwarfs(from 1997 to 2021). The random forest method allows us to analyze thepredictive power of these model grids, as well as interpret data within theframework of Approximate Bayesian Computation (ABC). Our curated datasetincludes 3 benchmark brown dwarfs (Gl 570D, {epsilon} Indi Ba and Bb) as wellas a sample of 19 L and T dwarfs; this sample was previously analyzed in Lueberet al. (2022) using traditional Bayesian methods (nested sampling). We findthat the effective temperature of a brown dwarf can be robustly predictedindependent of the model grid chosen for the interpretation. However, inferenceof the surface gravity is model-dependent. Specifically, the BT-Settl, SonoraBobcat and Sonora Cholla model grids tend to predict logg ~3-4 (cgs units) evenafter data blueward of 1.2 {mu}m have been disregarded to mitigate for ourincomplete knowledge of the shapes of alkali lines. Two major, longstandingchallenges associated with understanding the influence of clouds in brown dwarfatmospheres remain: our inability to model them from first principles and alsoto robustly validate these models.", "output": "Intercomparison of Brown Dwarf Model Grids and Atmospheric Retrieval Using Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene flow estimation is the task of describing the 3D motion field betweentemporally successive point clouds. State-of-the-art methods use strong priorsand test-time optimization techniques, but require on the order of tens ofseconds for large-scale point clouds, making them unusable as computer visionprimitives for real-time applications such as open world object detection. Feedforward methods are considerably faster, running on the order of tens tohundreds of milliseconds for large-scale point clouds, but require expensivehuman supervision. To address both limitations, we propose Scene Flow viaDistillation, a simple distillation framework that uses a label-freeoptimization method to produce pseudo-labels to supervise a feed forward model.Our instantiation of this framework, ZeroFlow, produces scene flow estimates inreal-time on large-scale point clouds at quality competitive withstate-of-the-art methods while using zero human labels. Notably, at test-timeZeroFlow is over 1000$times$ faster than label-free state-of-the-artoptimization-based methods on large-scale point clouds and over 1000$times$cheaper to train on unlabeled data compared to the cost of human annotation ofthat data. To facilitate research reuse, we release our code, trained modelweights, and high quality pseudo-labels for the Argoverse 2 and Waymo Opendatasets.", "output": "ZeroFlow: Fast Zero Label Scene Flow via Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series data, spanning applications ranging from climatology to financeto healthcare, presents significant challenges in data mining due to its sizeand complexity. One open issue lies in time series clustering, which is crucialfor processing large volumes of unlabeled time series data and unlockingvaluable insights. Traditional and modern analysis methods, however, oftenstruggle with these complexities. To address these limitations, we introduceR-Clustering, a novel method that utilizes convolutional architectures withrandomly selected parameters. Through extensive evaluations, R-Clusteringdemonstrates superior performance over existing methods in terms of clusteringaccuracy, computational efficiency and scalability. Empirical results obtainedusing the UCR archive demonstrate the effectiveness of our approach acrossdiverse time series datasets. The findings highlight the significance ofR-Clustering in various domains and applications, contributing to theadvancement of time series data mining.", "output": "Time Series Clustering With Random Convolutional Kernels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a new per-layer adaptive step-size procedure for stochasticfirst-order optimization methods for minimizing empirical loss functions indeep learning, eliminating the need for the user to tune the learning rate(LR). The proposed approach exploits the layer-wise stochastic curvatureinformation contained in the diagonal blocks of the Hessian in deep neuralnetworks (DNNs) to compute adaptive step-sizes (i.e., LRs) for each layer. Themethod has memory requirements that are comparable to those of first-ordermethods, while its per-iteration time complexity is only increased by an amountthat is roughly equivalent to an additional gradient computation. Numericalexperiments show that SGD with momentum and AdamW combined with the proposedper-layer step-sizes are able to choose effective LR schedules and outperformfine-tuned LR versions of these methods as well as popular first-order andsecond-order algorithms for training DNNs on Autoencoder, Convolutional NeuralNetwork (CNN) and Graph Convolutional Network (GCN) models. Finally, it isproved that an idealized version of SGD with the layer-wise step sizesconverges linearly when using full-batch gradients.", "output": "Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a unifying setting that combines existing restricted kernelmachine methods into a single primal-dual multi-view framework for kernelprincipal component analysis in both supervised and unsupervised settings. Wederive the primal and dual representations of the framework and relatedifferent training and inference algorithms from a theoretical perspective. Weshow how to achieve full equivalence in primal and dual formulations byrescaling primal variables. Finally, we experimentally validate the equivalenceand provide insight into the relationships between different methods on anumber of time series data sets by recursively forecasting unseen test data andvisualizing the learned features.", "output": "Duality in Multi-View Restricted Kernel Machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of large language models (LLMs) such as ChatGPT has brought alot of attention recently. However, their evaluation in the benchmark academicdatasets remains under-explored due to the difficulty of evaluating thegenerative outputs produced by this model against the ground truth. In thispaper, we aim to present a thorough evaluation of ChatGPT's performance ondiverse academic datasets, covering tasks like question-answering, textsummarization, code generation, commonsense reasoning, mathematicalproblem-solving, machine translation, bias detection, and ethicalconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze255K responses it generates in these datasets. This makes our work the largestevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validatethe strengths and weaknesses of ChatGPT in various tasks and provide insightsfor future research using LLMs. We also report a new emergent ability to followmulti-query instructions that we mostly found in ChatGPT and otherinstruction-tuned models. Our extensive evaluation shows that even thoughChatGPT is capable of performing a wide variety of tasks, and may obtainimpressive performance in several benchmark datasets, it is still far fromachieving the ability to reliably solve many challenging tasks. By providing athorough assessment of ChatGPT's performance across diverse NLP tasks, thispaper sets the stage for a targeted deployment of ChatGPT-like LLMs inreal-world applications.", "output": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline reinforcement learning (RL) offers an appealing approach toreal-world tasks by learning policies from pre-collected datasets withoutinteracting with the environment. However, the performance of existing offlineRL algorithms heavily depends on the scale and state-action space coverage ofdatasets. Real-world data collection is often expensive and uncontrollable,leading to small and narrowly covered datasets and posing significantchallenges for practical deployments of offline RL. In this paper, we provide anew insight that leveraging the fundamental symmetry of system dynamics cansubstantially enhance offline RL performance under small datasets.Specifically, we propose a Time-reversal symmetry (T-symmetry) enforcedDynamics Model (TDM), which establishes consistency between a pair of forwardand reverse latent dynamics. TDM provides both well-behaved representations forsmall datasets and a new reliability measure for OOD samples based oncompliance with the T-symmetry. These can be readily used to construct a newoffline RL algorithm (TSRL) with less conservative policy constraints and areliable latent space data augmentation procedure. Based on extensiveexperiments, we find TSRL achieves great performance on small benchmarkdatasets with as few as 1% of the original samples, which significantlyoutperforms the recent offline RL algorithms in terms of data efficiency andgeneralizability.", "output": "Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT is a large language model developed by OpenAI. Despite its impressiveperformance across various tasks, no prior work has investigated its capabilityin the biomedical domain yet. To this end, this paper aims to evaluate theperformance of ChatGPT on various benchmark biomedical tasks, such as relationextraction, document classification, question answering, and summarization. Tothe best of our knowledge, this is the first work that conducts an extensiveevaluation of ChatGPT in the biomedical domain. Interestingly, we find based onour evaluation that in biomedical datasets that have smaller training sets,zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generativetransformer models, such as BioGPT and BioBART. This suggests that ChatGPT'spre-training on large text corpora makes it quite specialized even in thebiomedical domain. Our findings demonstrate that ChatGPT has the potential tobe a valuable tool for various tasks in the biomedical domain that lack largeannotated data.", "output": "Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural sequence models based on the transformer architecture havedemonstrated remarkable emph{in-context learning} (ICL) abilities, where theycan perform new tasks when prompted with training and test examples, withoutany parameter update to the model. This work first provides a comprehensivestatistical theory for transformers to perform ICL. Concretely, we show thattransformers can implement a broad class of standard machine learningalgorithms in context, such as least squares, ridge regression, Lasso, learninggeneralized linear models, and gradient descent on two-layer neural networks,with near-optimal predictive power on various in-context data distributions.Using an efficient implementation of in-context gradient descent as theunderlying mechanism, our transformer constructions admit mild size bounds, andcan be learned with polynomially many pretraining sequences.Building on these ``base'' ICL algorithms, intriguingly, we show thattransformers can implement more complex ICL procedures involvingemph{in-context algorithm selection}, akin to what a statistician can do inreal life -- A emph{single} transformer can adaptively select different baseICL algorithms -- or even perform qualitatively different tasks -- on differentinput sequences, without any explicit prompting of the right algorithm or task.We both establish this in theory by explicit constructions, and also observethis phenomenon experimentally. In theory, we construct two general mechanismsfor algorithm selection with concrete examples: pre-ICL testing, and post-ICLvalidation. As an example, we use the post-ICL validation mechanism toconstruct a transformer that can perform nearly Bayes-optimal ICL on achallenging task -- noisy linear models with mixed noise levels.Experimentally, we demonstrate the strong in-context algorithm selectioncapabilities of standard transformer architectures.", "output": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks offer an alternative paradigm for modeling weatherconditions. The ability of neural models to make a prediction in less than asecond once the data is available and to do so with very high temporal andspatial resolution, and the ability to learn directly from atmosphericobservations, are just some of these models' unique advantages. Neural modelstrained using atmospheric observations, the highest fidelity and lowest latencydata, have to date achieved good performance only up to twelve hours of leadtime when compared with state-of-the-art probabilistic Numerical WeatherPrediction models and only for the sole variable of precipitation. In thispaper, we present MetNet-3 that extends significantly both the lead time rangeand the variables that an observation based neural model can predict well.MetNet-3 learns from both dense and sparse data sensors and makes predictionsup to 24 hours ahead for precipitation, wind, temperature and dew point.MetNet-3 introduces a key densification technique that implicitly captures dataassimilation and produces spatially dense forecasts in spite of the networktraining on extremely sparse targets. MetNet-3 has a high temporal and spatialresolution of, respectively, up to 2 minutes and 1 km as well as a lowoperational latency. We find that MetNet-3 is able to outperform the bestsingle- and multi-member NWPs such as HRRR and ENS over the CONUS region for upto 24 hours ahead setting a new performance milestone for observation basedneural models. MetNet-3 is operational and its forecasts are served in GoogleSearch in conjunction with other models.", "output": "Deep Learning for Day Forecasts from Sparse Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study investigates the transformative potential of Large Language Models(LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of publicdata, these models, which possess remarkable language understanding andgeneration capabilities, are augmenting the interpretive skills ofradiologists, enhancing patient-physician communication, and streamliningclinical workflows. The paper introduces an analytic framework for presentingthe complex interactions between LLMs and the broader ecosystem of medicalimaging stakeholders, including businesses, insurance entities, governments,research institutions, and hospitals (nicknamed BIGR-H). Through detailedanalyses, illustrative use cases, and discussions on the broader implicationsand future directions, this perspective seeks to raise discussion in strategicplanning and decision-making in the era of AI-enabled healthcare.", "output": "The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Coding problems are problems that require a solution in the form of acomputer program. Coding problems are popular among students and professionalsas it enhances their skills and career opportunities. An AI system that wouldhelp those who practice coding problems would be highly useful and there is ahuge potential for such a system. In this work, we propose a model which usesstacking of hyperparameter tuned boosting models to achieve impressive metricscores of 77.8% accuracy and 0.815 PR-AUC on the dataset that was scraped fromCodeforces and Leetcode. We open source the dataset and the models developedfor this work.", "output": "Stacking of Hyperparameter Tuned Models for Tagging Coding Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Algorithmic Fairness is an established field in machine learning that aims toreduce biases in data. Recent advances have proposed various methods to ensurefairness in a univariate environment, where the goal is to de-bias a singletask. However, extending fairness to a multi-task setting, where more than oneobjective is optimised using a shared representation, remains underexplored. Tobridge this gap, we develop a method that extends the definition of StrongDemographic Parity to multi-task learning using multi-marginal Wassersteinbarycenters. Our approach provides a closed form solution for the optimal fairmulti-task predictor including both regression and binary classification tasks.We develop a data-driven estimation procedure for the solution and runnumerical experiments on both synthetic and real datasets. The empiricalresults highlight the practical value of our post-processing methodology inpromoting fair decision-making.", "output": "Fairness in Multi-Task Learning via Wasserstein Barycenters."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) has made significant strides in various complexdomains. However, identifying an effective policy via RL often necessitatesextensive exploration. Imitation learning aims to mitigate this issue by usingexpert demonstrations to guide exploration. In real-world scenarios, one oftenhas access to multiple suboptimal black-box experts, rather than a singleoptimal oracle. These experts do not universally outperform each other acrossall states, presenting a challenge in actively deciding which oracle to use andin which state. We introduce MAPS and MAPS-SE, a class of policy improvementalgorithms that perform imitation learning from multiple suboptimal oracles. Inparticular, MAPS actively selects which of the oracles to imitate and improvetheir value function estimates, and MAPS-SE additionally leverages an activestate exploration criterion to determine which states one should explore. Weprovide a comprehensive theoretical analysis and demonstrate that MAPS andMAPS-SE enjoy sample efficiency advantage over the state-of-the-art policyimprovement algorithms. Empirical results show that MAPS-SE significantlyaccelerates policy optimization via state-wise imitation learning from multipleoracles across a broad spectrum of control tasks in the DeepMind Control Suite.Our code is publicly available at: ", "output": "Active Policy Improvement from Multiple Black-box Oracles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vector search has emerged as the foundation for large-scale informationretrieval and machine learning systems, with search engines like Google andBing processing tens of thousands of queries per second on petabyte-scaledocument datasets by evaluating vector similarities between encoded query textsand web documents. As performance demands for vector search systems surge,accelerated hardware offers a promising solution in the post-Moore's Law era.We introduce textit{FANNS}, an end-to-end and scalable vector search frameworkon FPGAs. Given a user-provided recall requirement on a dataset and a hardwareresource budget, textit{FANNS} automatically co-designs hardware andalgorithm, subsequently generating the corresponding accelerator. The frameworkalso supports scale-out by incorporating a hardware TCP/IP stack in theaccelerator. textit{FANNS} attains up to 23.0$times$ and 37.2$times$ speedupcompared to FPGA and CPU baselines, respectively, and demonstrates superiorscalability to GPUs, achieving 5.5$times$ and 7.6$times$ speedup in medianand 95textsuperscript{th} percentile (P95) latency within an eight-acceleratorconfiguration. The remarkable performance of textit{FANNS} lays a robustgroundwork for future FPGA integration in data centers and AI supercomputers.", "output": "Co-design Hardware and Algorithm for Vector Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of non-disclosively collecting a sample of data that isbalanced with respect to sensitive groups when group membership is unavailableor prohibited from use at collection time. Specifically, our collectionmechanism does not reveal significantly more about group membership of anyindividual sample than can be ascertained from base rates alone. To do this, weadopt a fairness pipeline perspective, in which a learner can use a small setof labeled data to train a proxy function that can later be used for thisfiltering task. We then associate the range of the proxy function with samplingprobabilities; given a new candidate, we classify it using our proxy function,and then select it for our sample with probability proportional to the samplingprobability corresponding to its proxy classification. Importantly, we requirethat the proxy classification itself not reveal significant information aboutthe sensitive group membership of any individual sample (i.e., it should besufficiently non-disclosive). We show that under modest algorithmicassumptions, we find such a proxy in a sample- and oracle-efficient manner.Finally, we experimentally evaluate our algorithm and analyze generalizationproperties.", "output": "Balanced Filtering via Non-Disclosive Proxies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a novel approach to address the challenges ofprinted Urdu text recognition using high-resolution, multi-scale semanticfeature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model,demonstrates state-of-the-art performance on benchmark datasets. To address thelimitations of previous works, which struggle to generalize to the intricaciesof the Urdu script and the lack of sufficient annotated real-world data, wehave introduced the UTRSet-Real, a large-scale annotated real-world datasetcomprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000lines closely resembling real-world and made corrections to the ground truth ofthe existing IIITH dataset, making it a more reliable resource for futureresearch. We also provide UrduDoc, a benchmark dataset for Urdu text linedetection in scanned documents. Additionally, we have developed an online toolfor end-to-end Urdu OCR from printed documents by integrating UTRNet with atext detection model. Our work not only addresses the current limitations ofUrdu OCR but also paves the way for future research in this area andfacilitates the continued advancement of Urdu OCR technology. The project pagewith source code, datasets, annotations, trained models, and online tool isavailable at abdur75648.github.io/UTRNet.", "output": "UTRNet: High-Resolution Urdu Text Recognition In Printed Documents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a limited labeling budget, active learning (AL) aims to sample the mostinformative instances from an unlabeled pool to acquire labels for subsequentmodel training. To achieve this, AL typically measures the informativeness ofunlabeled instances based on uncertainty and diversity. However, it does notconsider erroneous instances with their neighborhood error density, which havegreat potential to improve the model performance. To address this limitation,we propose $REAL$, a novel approach to select data instances with$underline{R}$epresentative $underline{E}$rrors for $underline{A}$ctive$underline{L}$earning. It identifies minority predictions as emph{pseudoerrors} within a cluster and allocates an adaptive sampling budget for thecluster based on estimated error density. Extensive experiments on five textclassification datasets demonstrate that $REAL$ consistently outperforms allbest-performing baselines regarding accuracy and F1-macro scores across a widerange of hyperparameter settings. Our analysis also shows that $REAL$ selectsthe most representative pseudo errors that match the distribution ofground-truth errors along the decision boundary. Our code is publicly availableat ", "output": "REAL: A Representative Error-Driven Approach for Active Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine Learning (ML) models in Robotic Assembly Sequence Planning (RASP)need to be introspective on the predicted solutions, i.e. whether they arefeasible or not, to circumvent potential efficiency degradation. Previous worksneed both feasible and infeasible examples during training. However, theinfeasible ones are hard to collect sufficiently when re-training is requiredfor swift adaptation to new product variants. In this work, we propose adensity-based feasibility learning method that requires only feasible examples.Concretely, we formulate the feasibility learning problem asOut-of-Distribution (OOD) detection with Normalizing Flows (NF), which arepowerful generative models for estimating complex probability distributions.Empirically, the proposed method is demonstrated on robotic assembly use casesand outperforms other single-class baselines in detecting infeasibleassemblies. We further investigate the internal working mechanism of our methodand show that a large memory saving can be obtained based on an advancedvariant of NF.", "output": "Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Smart home energy management systems help the distribution grid operate moreefficiently and reliably, and enable effective penetration of distributedrenewable energy sources. These systems rely on robust forecasting,optimization, and control/scheduling algorithms that can handle the uncertainnature of demand and renewable generation. This paper proposes an advanced MLalgorithm, called Recurrent Trend Predictive Neural Network based ForecastEmbedded Scheduling (rTPNN-FES), to provide efficient residential demandcontrol. rTPNN-FES is a novel neural network architecture that simultaneouslyforecasts renewable energy generation and schedules household appliances. Byits embedded structure, rTPNN-FES eliminates the utilization of separatealgorithms for forecasting and scheduling and generates a schedule that isrobust against forecasting errors. This paper also evaluates the performance ofthe proposed algorithm for an IoT-enabled smart home. The evaluation resultsreveal that rTPNN-FES provides near-optimal scheduling $37.5$ times faster thanthe optimization while outperforming state-of-the-art forecasting techniques.", "output": "Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Connectionist Temporal Classification (CTC) is a widely used criterion fortraining supervised sequence-to-sequence (seq2seq) models. It enables learningthe relations between input and output sequences, termed alignments, bymarginalizing over perfect alignments (that yield the ground truth), at theexpense of imperfect alignments. This binary differentiation of perfect andimperfect alignments falls short of capturing other essential alignmentproperties that hold significance in other real-world applications. Here wepropose $textit{Align With Purpose}$, a $textbf{general Plug-and-Playframework}$ for enhancing a desired property in models trained with the CTCcriterion. We do that by complementing the CTC with an additional loss termthat prioritizes alignments according to a desired property. Our method doesnot require any intervention in the CTC loss function, enables easyoptimization of a variety of properties, and allows differentiation betweenboth perfect and imperfect alignments. We apply our framework in the domain ofAutomatic Speech Recognition (ASR) and show its generality in terms of propertyselection, architectural choice, and scale of training dataset (up to 280,000hours). To demonstrate the effectiveness of our framework, we apply it to twounrelated properties: emission time and word error rate (WER). For the former,we report an improvement of up to 570ms in latency optimization with a minorreduction in WER, and for the latter, we report a relative improvement of 4.5%WER over the baseline models. To the best of our knowledge, these applicationshave never been demonstrated to work on a scale of data as large as ours.Notably, our method can be implemented using only a few lines of code, and canbe extended to other alignment-free loss functions and to domains other thanASR.", "output": "Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, the weight-sharing technique has significantly speeded up thetraining and evaluation procedure of neural architecture search. However, mostexisting weight-sharing strategies are solely based on experience orobservation, which makes the searching results lack interpretability andrationality. In addition, due to the negligence of fairness, current methodsare prone to make misjudgments in module evaluation. To address these problems,we propose a novel neural architecture search algorithm based on dynamicalisometry. We use the fix point analysis method in the mean field theory toanalyze the dynamics behavior in the steady state random neural network, andhow dynamic isometry guarantees the fairness of weight-sharing based NAS.Meanwhile, we prove that our module selection strategy is rigorous fair byestimating the generalization error of all modules with well-conditionedJacobian. Extensive experiments show that, with the same size, the architecturesearched by the proposed method can achieve state-of-the-art top-1 validationaccuracy on ImageNet classification. In addition, we demonstrate that ourmethod is able to achieve better and more stable training performance withoutloss of generality.", "output": "Dynamical Isometry based Rigorous Fair Neural Architecture Search."}]