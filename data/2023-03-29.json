[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep models are dominating the artificial intelligence (AI) industry sincethe ImageNet challenge in 2012. The size of deep models is increasing eversince, which brings new challenges to this field with applications in cellphones, personal computers, autonomous cars, and wireless base stations. Herewe list a set of problems, ranging from training, inference, generalizationbound, and optimization with some formalism to communicate these challengeswith mathematicians, statisticians, and theoretical computer scientists. Thisis a subjective view of the research questions in deep learning that benefitsthe tech industry in long run.", "output": "Mathematical Challenges in Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the analysis of large/big data sets, aggregation (replacing values of avariable over a group by a single value) is a standard way of reducing the size(complexity) of the data. Data analysis programs provide different aggregationfunctions.Recently some books dealing with the theoretical and algorithmic backgroundof traditional aggregation functions were published. A problem with traditionalaggregation is that often too much information is discarded thus reducing theprecision of the obtained results. A much better, preserving more information,summarization of original data can be achieved by representing aggregated datausing selected types of complex data.In complex data analysis the measured values over a selected group $A$ areaggregated into a complex object $Sigma(A)$ and not into a single value. Mostof the aggregation functions theory does not apply directly. In ourcontribution, we present an attempt to start building a theoretical backgroundof complex aggregation.We introduce and discuss exactly mergeable summaries for which it holds formerging of disjoint sets of units[ Sigma(A cup B) = F( Sigma(A),Sigma(B)),qquad mbox{ for } quad AcapB = emptyset .]", "output": "Exactly mergeable summaries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) emerge to achieve impressive performance on manydata-abundant computer vision tasks by capturing long-range dependencies amonglocal features. However, under few-shot learning (FSL) settings on smalldatasets with only a few labeled data, ViT tends to overfit and suffers fromsevere performance degradation due to its absence of CNN-alike inductive bias.Previous works in FSL avoid such problem either through the help ofself-supervised auxiliary losses, or through the dextile uses of labelinformation under supervised settings. But the gap between self-supervised andsupervised few-shot Transformers is still unfilled. Inspired by recent advancesin self-supervised knowledge distillation and masked image modeling (MIM), wepropose a novel Supervised Masked Knowledge Distillation model (SMKD) forfew-shot Transformers which incorporates label information intoself-distillation frameworks. Compared with previous self-supervised methods,we allow intra-class knowledge distillation on both class and patch tokens, andintroduce the challenging task of masked patch tokens reconstruction acrossintra-class images. Experimental results on four few-shot classificationbenchmark datasets show that our method with simple design outperforms previousmethods by a large margin and achieves a new start-of-the-art. Detailedablation studies confirm the effectiveness of each component of our model. Codefor this paper is available here: ", "output": "Supervised Masked Knowledge Distillation for Few-Shot Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence has been used to help human complete difficult tasksin complicated environments by providing optimized strategies fordecision-making or replacing the manual labour. In environments includingmultiple agents, such as football, the most common methods to train agents areImitation Learning and Multi-Agent Reinforcement Learning (MARL). However, theagents trained by Imitation Learning cannot outperform the expert demonstrator,which makes humans hardly get new insights from the learnt policy. Besides,MARL is prone to the credit assignment problem. In environments with sparsereward signal, this method can be inefficient. The objective of our research isto create a novel reward shaping method by embedding contextual information inreward function to solve the aforementioned challenges. We demonstrate this inthe Google Research Football (GRF) environment. We quantify the contextualinformation extracted from game state observation and use this quantificationtogether with original sparse reward to create the shaped reward. Theexperiment results in the GRF environment prove that our reward shaping methodis a useful addition to state-of-the-art MARL algorithms for training agents inenvironments with sparse reward signal.", "output": "Embedding Contextual Information through Reward Shaping in Multi-Agent Learning: A Case Study from Google Football."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs), such as GPT-3, have demonstrated remarkablenatural language processing and generation capabilities and have been appliedto a variety tasks, such as source code generation. This paper explores thepotential of integrating LLMs in the hazard analysis for safety-criticalsystems, a process which we refer to as co-hazard analysis (CoHA). In CoHA, ahuman analyst interacts with an LLM via a context-aware chat session and usesthe responses to support elicitation of possible hazard causes. In thisexperiment, we explore CoHA with three increasingly complex versions of asimple system, using Open AI's ChatGPT service. The quality of ChatGPT'sresponses were systematically assessed to determine the feasibility of CoHAgiven the current state of LLM technology. The results suggest that LLMs may beuseful for supporting human analysts performing hazard analysis.", "output": "Can Large Language Models assist in Hazard Analysis?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph data is omnipresent and has a large variety of applications such asnatural science, social networks or semantic web. Though rich in information,graphs are often noisy and incomplete. Therefore, graph completion tasks suchas node classification or link prediction have gained attention. On the onehand, neural methods such as graph neural networks have proven to be robusttools for learning rich representations of noisy graphs. On the other hand,symbolic methods enable exact reasoning on graphs. We propose KeGNN, aneuro-symbolic framework for learning on graph data that combines bothparadigms and allows for the integration of prior knowledge into a graph neuralnetwork model. In essence, KeGNN consists of a graph neural network as a baseon which knowledge enhancement layers are stacked with the objective ofrefining predictions with respect to prior knowledge. We instantiate KeGNN inconjunction with two standard graph neural networks: Graph ConvolutionalNetworks and Graph Attention Networks, and evaluate KeGNN on multiple benchmarkdatasets for node classification.", "output": "Knowledge Enhanced Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Railway operations involve different types of entities (stations, trains,etc.), making the existing graph/network models with homogenous nodes (i.e.,the same kind of nodes) incapable of capturing the interactions between theentities. This paper aims to develop a heterogeneous graph neural network(HetGNN) model, which can address different types of nodes (i.e., heterogeneousnodes), to investigate the train delay evolution on railway networks. To thisend, a graph architecture combining the HetGNN model and the GraphSAGEhomogeneous GNN (HomoGNN), called SAGE-Het, is proposed. The aim is to capturethe interactions between trains, trains and stations, and stations and otherstations on delay evolution based on different edges. In contrast to thetraditional methods that require the inputs to have constant dimensions (e.g.,in rectangular or grid-like arrays) or only allow homogeneous nodes in thegraph, SAGE-Het allows for flexible inputs and heterogeneous nodes. The datafrom two sub-networks of the China railway network are applied to test theperformance and robustness of the proposed SAGE-Het model. The experimentalresults show that SAGE-Het exhibits better performance than the existing delayprediction methods and some advanced HetGNNs used for other prediction tasks;the predictive performances of SAGE-Het under different prediction timehorizons (10/20/30 min ahead) all outperform other baseline methods;Specifically, the influences of train interactions on delay propagation areinvestigated based on the proposed model. The results show that traininteractions become subtle when the train headways increase . This findingdirectly contributes to decision-making in the situation whereconflict-resolution or train-canceling actions are needed.", "output": "Railway Network Delay Evolution: A Heterogeneous Graph Neural Network Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the maturity of web services, containers, and cloud computingtechnologies, large services in traditional systems (e.g. the computationservices of machine learning and artificial intelligence) are gradually beingbroken down into many microservices to increase service reusability andflexibility. Therefore, this study proposes an efficiency analysis frameworkbased on queuing models to analyze the efficiency difference of breaking downtraditional large services into n microservices. For generalization, this studyconsiders different service time distributions (e.g. exponential distributionof service time and fixed service time) and explores the system efficiency inthe worst-case and best-case scenarios through queuing models (i.e. M/M/1queuing model and M/D/1 queuing model). In each experiment, it was shown thatthe total time required for the original large service was higher than thatrequired for breaking it down into multiple microservices, so breaking it downinto multiple microservices can improve system efficiency. It can also beobserved that in the best-case scenario, the improvement effect becomes moresignificant with an increase in arrival rate. However, in the worst-casescenario, only slight improvement was achieved. This study found that breakingdown into multiple microservices can effectively improve system efficiency andproved that when the computation time of the large service is evenlydistributed among multiple microservices, the best improvement effect can beachieved. Therefore, this study's findings can serve as a reference guide forfuture development of microservice architecture.", "output": "Research on Efficiency Analysis of Microservices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Molecular representation learning plays a crucial role in AI-assisted drugdiscovery research. Encoding 3D molecular structures through Euclidean neuralnetworks has become the prevailing method in the geometric deep learningcommunity. However, the equivariance constraints and message passing inEuclidean space may limit the network expressive power. In this work, wepropose a Harmonic Molecular Representation learning (HMR) framework, whichrepresents a molecule using the Laplace-Beltrami eigenfunctions of itsmolecular surface. HMR offers a multi-resolution representation of moleculargeometric and chemical features on 2D Riemannian manifold. We also introduce aharmonic message passing method to realize efficient spectral message passingover the surface manifold for better molecular encoding. Our proposed methodshows comparable predictive power to current models in small molecule propertyprediction, and outperforms the state-of-the-art deep learning models forligand-binding protein pocket classification and the rigid protein dockingchallenge, demonstrating its versatility in molecular representation learning.", "output": "Learning Harmonic Molecular Representations on Riemannian Manifold."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Neural Networks (DNN) are vulnerable to adversarial perturbations-smallchanges crafted deliberately on the input to mislead the model for wrongpredictions. Adversarial attacks have disastrous consequences for deeplearning-empowered critical applications. Existing defense and detectiontechniques both require extensive knowledge of the model, testing inputs, andeven execution details. They are not viable for general deep learningimplementations where the model internal is unknown, a common 'black-box'scenario for model users. Inspired by the fact that electromagnetic (EM)emanations of a model inference are dependent on both operations and data andmay contain footprints of different input classes, we propose a framework,EMShepherd, to capture EM traces of model execution, perform processing ontraces and exploit them for adversarial detection. Only benign samples andtheir EM traces are used to train the adversarial detector: a set of EMclassifiers and class-specific unsupervised anomaly detectors. When the victimmodel system is under attack by an adversarial example, the model executionwill be different from executions for the known classes, and the EM trace willbe different. We demonstrate that our air-gapped EMShepherd can effectivelydetect different adversarial attacks on a commonly used FPGA deep learningaccelerator for both Fashion MNIST and CIFAR-10 datasets. It achieves a 100%detection rate on most types of adversarial samples, which is comparable to thestate-of-the-art 'white-box' software-based detectors.", "output": "EMShepherd: Detecting Adversarial Samples via Side-channel Leakage."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Through exploiting a high level of parallelism enabled by graphics processingunits, transformer architectures have enabled tremendous strides forward in thefield of natural language processing. In a traditional masked language model,special MASK tokens are used to prompt our model to gather contextualinformation from surrounding words to restore originally hidden information. Inthis paper, we explore a task-specific masking framework for pre-trained largelanguage models that enables superior performance on particular downstreamtasks on the datasets in the GLUE benchmark. We develop our own maskingalgorithm, Typhoon, based on token input gradients, and compare this with otherstandard baselines. We find that Typhoon offers performance competitive withwhole-word masking on the MRPC dataset. Our implementation can be found in apublic Github Repository.", "output": "Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents the first ChatGPT4PCG Competition at the 2023 IEEEConference on Games. The objective of this competition is for participants tocreate effective prompts for ChatGPT--enabling it to generate Science Birdslevels with high stability and character-like qualities--fully using theircreativity as well as prompt engineering skills. ChatGPT is a conversationalagent developed by OpenAI. Science Birds is selected as the competitionplatform because designing an Angry Birds-like level is not a trivial task dueto the in-game gravity; the playability of the levels is determined by theirstability. To lower the entry barrier to the competition, we limit the task tothe generation of capitalized English alphabetical characters. Here, thequality of the generated levels is determined by their stability and similarityto the given characters. A sample prompt is provided to participants for theirreference. An experiment is conducted to determine the effectiveness of itsmodified versions on level stability and similarity by testing them on severalcharacters. To the best of our knowledge, we believe that ChatGPT4PCG is thefirst competition of its kind and hope to inspire enthusiasm for promptengineering in procedural content generation.", "output": "ChatGPT4PCG Competition: Character-like Level Generation for Science Birds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural text-to-speech (TTS) models can synthesize natural human speech whentrained on large amounts of transcribed speech. However, collecting suchlarge-scale transcribed data is expensive. This paper proposes an unsupervisedpre-training method for a sequence-to-sequence TTS model by leveraging largeuntranscribed speech data. With our pre-training, we can remarkably reduce theamount of paired transcribed data required to train the model for the targetdownstream TTS task. The main idea is to pre-train the model to reconstructde-warped mel-spectrograms from warped ones, which may allow the model to learnproper temporal assignment relation between input and output sequences. Inaddition, we propose a data augmentation method that further improves the dataefficiency in fine-tuning. We empirically demonstrate the effectiveness of ourproposed method in low-resource language scenarios, achieving outstandingperformance compared to competing methods. The code and audio samples areavailable at: ", "output": "Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimization problems involving sequential decisions in a stochasticenvironment were studied in Stochastic Programming (SP), Stochastic OptimalControl (SOC) and Markov Decision Processes (MDP). In this paper we mainlyconcentrate on SP and SOC modelling approaches. In these frameworks there arenatural situations when the considered problems are convex. Classical approachto sequential optimization is based on dynamic programming. It has the problemof the so-called ``Curse of Dimensionality\", in that its computationalcomplexity increases exponentially with increase of dimension of statevariables. Recent progress in solving convex multistage stochastic problems isbased on cutting planes approximations of the cost-to-go (value) functions ofdynamic programming equations. Cutting planes type algorithms in dynamicalsettings is one of the main topics of this paper. We also discuss StochasticApproximation type methods applied to multistage stochastic optimizationproblems. From the computational complexity point of view, these two types ofmethods seem to be complimentary to each other. Cutting plane type methods canhandle multistage problems with a large number of stages, but a relativelysmaller number of state (decision) variables. On the other hand, stochasticapproximation type methods can only deal with a small number of stages, but alarge number of decision variables.", "output": "Numerical Methods for Convex Multistage Stochastic Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation (KD) is an effective training strategy to improve thelightweight student models under the guidance of cumbersome teachers. However,the large architecture difference across the teacher-student pairs limits thedistillation gains. In contrast to previous adaptive distillation methods toreduce the teacher-student gap, we explore a novel training-free framework tosearch for the best student architectures for a given teacher. Our work firstempirically show that the optimal model under vanilla training cannot be thewinner in distillation. Secondly, we find that the similarity of featuresemantics and sample relations between random-initialized teacher-studentnetworks have good correlations with final distillation performances. Thus, weefficiently measure similarity matrixs conditioned on the semantic activationmaps to select the optimal student via an evolutionary algorithm without anytraining. In this way, our student architecture search for Distillation WithOutTraining (DisWOT) significantly improves the performance of the model in thedistillation stage with at least 180$times$ training acceleration.Additionally, we extend similarity metrics in DisWOT as new distillers andKD-based zero-proxies. Our experiments on CIFAR, ImageNet and NAS-Bench-201demonstrate that our technique achieves state-of-the-art results on differentsearch spaces. Our project and code are available at", "output": "DisWOT: Student Architecture Search for Distillation WithOut Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The deployment of agile autonomous systems in challenging, unstructuredenvironments requires adaptation capabilities and robustness to uncertainties.Existing robust and adaptive controllers, such as the ones based on MPC, canachieve impressive performance at the cost of heavy online onboardcomputations. Strategies that efficiently learn robust and onboard-deployablepolicies from MPC have emerged, but they still lack fundamental adaptationcapabilities. In this work, we extend an existing efficient IL algorithm forrobust policy learning from MPC with the ability to learn policies that adaptto challenging model/environment uncertainties. The key idea of our approachconsists in modifying the IL procedure by conditioning the policy on a learnedlower-dimensional model/environment representation that can be efficientlyestimated online. We tailor our approach to the task of learning an adaptiveposition and attitude control policy to track trajectories under challengingdisturbances on a multirotor. Our evaluation is performed in a high-fidelitysimulation environment and shows that a high-quality adaptive policy can beobtained in about $1.3$ hours. We additionally empirically demonstrate rapidadaptation to in- and out-of-training-distribution uncertainties, achieving a$6.1$ cm average position error under a wind disturbance that corresponds toabout $50%$ of the weight of the robot and that is $36%$ larger than themaximum wind seen during training.", "output": "Efficient Deep Learning of Robust, Adaptive Policies using Tube MPC-Guided Data Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models have been shown to perform remarkably well on a wide range ofnatural language processing tasks. In this paper, we propose a novel systemthat uses language models to perform multi-step logical reasoning. Our systemincorporates explicit planning into its inference procedure, thus able to makemore informed reasoning decisions at each step by looking ahead into theirfuture effects. In our experiments, our full system significantly outperformsother competing systems. On a multiple-choice question answering task, oursystem performs competitively compared to GPT-3-davinci despite having onlyaround 1.5B parameters. We conduct several ablation studies to demonstrate thatexplicit planning plays a crucial role in the system's performance.", "output": "Explicit Planning Helps Language Models in Logical Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing methods proposed for hand reconstruction tasks usually parameterizea generic 3D hand model or predict hand mesh positions directly. The parametricrepresentations consisting of hand shapes and rotational poses are more stable,while the non-parametric methods can predict more accurate mesh positions. Inthis paper, we propose to reconstruct meshes and estimate MANO parameters oftwo hands from a single RGB image simultaneously to utilize the merits of twokinds of hand representations. To fulfill this target, we propose novelMesh-Mano interaction blocks (MMIBs), which take mesh vertices positions andMANO parameters as two kinds of query tokens. MMIB consists of one graphresidual block to aggregate local information and two transformer encoders tomodel long-range dependencies. The transformer encoders are equipped withdifferent asymmetric attention masks to model the intra-hand and inter-handattention, respectively. Moreover, we introduce the mesh alignment refinementmodule to further enhance the mesh-image alignment. Extensive experiments onthe InterHand2.6M benchmark demonstrate promising results over thestate-of-the-art hand reconstruction methods.", "output": "MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLM) have been successful in several natural languageunderstanding tasks and could be relevant for natural language processing(NLP)-based mental health application research. In this work, we report theperformance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in threetext-based mental health classification tasks: stress detection (2-classclassification), depression detection (2-class classification), and suicidalitydetection (5-class classification). We obtained annotated social media postsfor the three classification tasks from public datasets. Then ChatGPT APIclassified the social media posts with an input prompt for classification. Weobtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depressiondetection, and suicidality detection, respectively. A baseline model thatalways predicted the dominant class resulted in F1 scores of 0.35, 0.60, and0.19. The zero-shot classification accuracy obtained with ChatGPT indicates apotential use of language models for mental health classification tasks.", "output": "Evaluation of ChatGPT for NLP-based Mental Health Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents our work to enhance the background music (BGM) inDareFightingICE by adding an adaptive BGM. The adaptive BGM consists of fivedifferent instruments playing a classical music piece called \"Air on G-String.\"The BGM adapts by changing the volume of the instruments. Each instrument isconnected to a different element of the game. We then run experiments toevaluate the adaptive BGM by using a deep reinforcement learning AI that onlyuses audio as input (Blind DL AI). The results show that the performance of theBlind DL AI improves while playing with the adaptive BGM as compared to playingwithout the adaptive BGM.", "output": "Adaptive Background Music for a Fighting Game: A Multi-Instrument Volume Modulation Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this technical report, we explore the behavior of Recursive FeatureMachines (RFMs), a type of novel kernel machine that recursively learnsfeatures via the average gradient outer product, through a series ofexperiments on regression datasets. When successively adding random noisefeatures to a dataset, we observe intriguing patterns in the Mean Squared Error(MSE) curves with the test MSE exhibiting a decrease-increase-decrease pattern.This behavior is consistent across different dataset sizes, noise parameters,and target functions. Interestingly, the observed MSE curves show similaritiesto the \"double descent\" phenomenon observed in deep neural networks, hinting atnew connection between RFMs and neural network behavior. This report lays thegroundwork for future research into this peculiar behavior.", "output": "On Feature Scaling of Recursive Feature Machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present emph{TabRet}, a pre-trainable Transformer-based model for tabulardata. TabRet is designed to work on a downstream task that contains columns notseen in pre-training. Unlike other methods, TabRet has an extra learning stepbefore fine-tuning called emph{retokenizing}, which calibrates featureembeddings based on the masked autoencoding loss. In experiments, wepre-trained TabRet with a large collection of public health surveys andfine-tuned it on classification tasks in healthcare, and TabRet achieved thebest AUC performance on four datasets. In addition, an ablation study showsretokenizing and random shuffle augmentation of columns during pre-trainingcontributed to performance gains.", "output": "TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face swapping aims at injecting a source image's identity (i.e., facialfeatures) into a target image, while strictly preserving the target'sattributes, which are irrelevant to identity. However, we observed thatprevious approaches still suffer from source attribute leakage, where thesource image's attributes interfere with the target image's. In this paper, weanalyze the latent space of StyleGAN and find the adequate combination of thelatents geared for face swapping task. Based on the findings, we develop asimple yet robust face swapping model, RobustSwap, which is resistant to thepotential source attribute leakage. Moreover, we exploit the coordination of3DMM's implicit and explicit information as a guidance to incorporate thestructure of the source image and the precise pose of the target image. Despiteour method solely utilizing an image dataset without identity labels fortraining, our model has the capability to generate high-fidelity and temporallyconsistent videos. Through extensive qualitative and quantitative evaluations,we demonstrate that our method shows significant improvements compared with theprevious face swapping models in synthesizing both images and videos. Projectpage is available at ", "output": "RobustSwap: A Simple yet Robust Face Swapping Model against Attribute Leakage."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influencesociety, warranting immediate social attention. While the models themselvesgarner much attention, to accurately characterize their impact, we mustconsider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as adocumentation framework to transparently centralize knowledge of thisecosystem. Ecosystem Graphs is composed of assets (datasets, models,applications) linked together by dependencies that indicate technical (e.g. howBing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI)relationships. To supplement the graph structure, each asset is furtherenriched with fine-grained metadata (e.g. the license or training emissions).We document the ecosystem extensively at As of March 16, 2023, we annotate262 assets (64 datasets, 128 models, 70 applications) from 63 organizationslinked by 356 dependencies. We show Ecosystem Graphs functions as a powerfulabstraction and interface for achieving the minimum transparency required toaddress myriad use cases. Therefore, we envision Ecosystem Graphs will be acommunity-maintained resource that provides value to stakeholders spanning AIresearchers, industry professionals, social scientists, auditors andpolicymakers.", "output": "Ecosystem Graphs: The Social Footprint of Foundation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most offline reinforcement learning (RL) methods suffer from the trade-offbetween improving the policy to surpass the behavior policy and constrainingthe policy to limit the deviation from the behavior policy as computing$Q$-values using out-of-distribution (OOD) actions will suffer from errors dueto distributional shift. The recently proposed textit{In-sample Learning}paradigm (i.e., IQL), which improves the policy by quantile regression usingonly data samples, shows great promise because it learns an optimal policywithout querying the value function of any unseen actions. However, it remainsunclear how this type of method handles the distributional shift in learningthe value function. In this work, we make a key finding that the in-samplelearning paradigm arises under the textit{Implicit Value Regularization} (IVR)framework. This gives a deeper understanding of why the in-sample learningparadigm works, i.e., it applies implicit value regularization to the policy.Based on the IVR framework, we further propose two practical algorithms, Sparse$Q$-learning (SQL) and Exponential $Q$-learning (EQL), which adopt the samevalue regularization used in existing works, but in a complete in-samplemanner. Compared with IQL, we find that our algorithms introduce sparsity inlearning the value function, making them more robust in noisy data regimes. Wealso verify the effectiveness of SQL and EQL on D4RL benchmark datasets andshow the benefits of in-sample learning by comparing them with CQL in smalldata regimes.", "output": "Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face recognition is a prevailing authentication solution in numerousbiometric applications. Physical adversarial attacks, as an importantsurrogate, can identify the weaknesses of face recognition systems and evaluatetheir robustness before deployed. However, most existing physical attacks areeither detectable readily or ineffective against commercial recognitionsystems. The goal of this work is to develop a more reliable technique that cancarry out an end-to-end evaluation of adversarial robustness for commercialsystems. It requires that this technique can simultaneously deceive black-boxrecognition models and evade defensive mechanisms. To fulfill this, we designadversarial textured 3D meshes (AT3D) with an elaborate topology on a humanface, which can be 3D-printed and pasted on the attacker's face to evade thedefenses. However, the mesh-based optimization regime calculates gradients inhigh-dimensional mesh space, and can be trapped into local optima withunsatisfactory transferability. To deviate from the mesh-based space, wepropose to perturb the low-dimensional coefficient space based on 3D MorphableModel, which significantly improves black-box transferability meanwhileenjoying faster search efficiency and better visual quality. Extensiveexperiments in digital and physical scenarios show that our method effectivelyexplores the security vulnerabilities of multiple popular commercial services,including three recognition APIs, four anti-spoofing APIs, two prevailingmobile phones and two automated access control systems.", "output": "Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As pre-trained models automate many code intelligence tasks, a widely usedparadigm is to fine-tune a model on the task dataset for each programminglanguage. A recent study reported that multilingual fine-tuning benefits arange of tasks and models. However, we find that multilingual fine-tuning leadsto performance degradation on recent models UniXcoder and CodeT5.To alleviate the potentially catastrophic forgetting issue in multilingualmodels, we fix all pre-trained model parameters, insert the parameter-efficientstructure adapter, and fine-tune it. Updating only 0.6% of the overallparameters compared to full-model fine-tuning for each programming language,adapter tuning yields consistent improvements on code search and summarizationtasks, achieving state-of-the-art results. In addition, we experimentally showits effectiveness in cross-lingual and low-resource scenarios. Multilingualfine-tuning with 200 samples per programming language approaches the resultsfine-tuned with the entire dataset on code summarization. Our experiments onthree probing tasks show that adapter tuning significantly outperformsfull-model fine-tuning and effectively overcomes catastrophic forgetting.", "output": "One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain shift has been a long-standing issue for medical image segmentation.Recently, unsupervised domain adaptation (UDA) methods have achieved promisingcross-modality segmentation performance by distilling knowledge from alabel-rich source domain to a target domain without labels. In this work, wepropose a multi-scale self-ensembling based UDA framework for automaticsegmentation of two key brain structures i.e., Vestibular Schwannoma (VS) andCochlea on high-resolution T2 images. First, a segmentation-enhancedcontrastive unpaired image translation module is designed for image-leveldomain adaptation from source T1 to target T2. Next, multi-scale deepsupervision and consistency regularization are introduced to a mean teachernetwork for self-ensemble learning to further close the domain gap.Furthermore, self-training and intensity augmentation techniques are utilizedto mitigate label scarcity and boost cross-modality segmentation performance.Our method demonstrates promising segmentation performance with a mean Dicescore of 83.8% and 81.4% and an average asymmetric surface distance (ASSD) of0.55 mm and 0.26 mm for the VS and Cochlea, respectively in the validationphase of the crossMoDA 2022 challenge.", "output": "MS-MT: Multi-Scale Mean Teacher with Contrastive Unpaired Translation for Cross-Modality Vestibular Schwannoma and Cochlea Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of machine learning has rapidly advanced the state of the art inmany fields of science and engineering, including experimental fluid dynamics,which is one of the original big-data disciplines. This perspective willhighlight several aspects of experimental fluid mechanics that stand to benefitfrom progress advances in machine learning, including: 1) augmenting thefidelity and quality of measurement techniques, 2) improving experimentaldesign and surrogate digital-twin models and 3) enabling real-time estimationand control. In each case, we discuss recent success stories and ongoingchallenges, along with caveats and limitations, and outline the potential fornew avenues of ML-augmented and ML-enabled experimental fluid mechanics.", "output": "The transformative potential of machine learning for experiments in fluid mechanics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual domain shift poses a significant challenge in real-worldapplications, particularly in situations where labeled data is not availablefor new domains. The challenge of acquiring knowledge in this problem settingis referred to as unsupervised continual domain shift learning. Existingmethods for domain adaptation and generalization have limitations in addressingthis issue, as they focus either on adapting to a specific domain orgeneralizing to unseen domains, but not both. In this paper, we proposeComplementary Domain Adaptation and Generalization (CoDAG), a simple yeteffective learning framework that combines domain adaptation and generalizationin a complementary manner to achieve three major goals of unsupervisedcontinual domain shift learning: adapting to a current domain, generalizing tounseen domains, and preventing forgetting of previously seen domains. Ourapproach is model-agnostic, meaning that it is compatible with any existingdomain adaptation and generalization algorithms. We evaluate CoDAG on severalbenchmark datasets and demonstrate that our model outperforms state-of-the-artmodels in all datasets and evaluation metrics, highlighting its effectivenessand robustness in handling unsupervised continual domain shift learning.", "output": "Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Successful analytics solutions that provide valuable insights often hinge onthe connection of various data sources. While it is often feasible to generatelarger data pools within organizations, the application of analytics within(inter-organizational) business networks is still severely constrained. As datais distributed across several legal units, potentially even across countries,the fear of disclosing sensitive information as well as the sheer volume of thedata that would need to be exchanged are key inhibitors for the creation ofeffective system-wide solutions -- all while still reaching superior predictionperformance. In this work, we propose a meta machine learning method that dealswith these obstacles to enable comprehensive analyses within a businessnetwork. We follow a design science research approach and evaluate our methodwith respect to feasibility and performance in an industrial use case. First,we show that it is feasible to perform network-wide analyses that preserve dataconfidentiality as well as limit data transfer volume. Second, we demonstratethat our method outperforms a conventional isolated analysis and even getsclose to a (hypothetical) scenario where all data could be shared within thenetwork. Thus, we provide a fundamental contribution for making businessnetworks more effective, as we remove a key obstacle to tap the huge potentialof learning from data that is scattered throughout the network.", "output": "Enabling Inter-organizational Analytics in Business Networks Through Meta Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predictive process analytics focuses on predicting future states, such as theoutcome of running process instances. These techniques often use machinelearning models or deep learning models (such as LSTM) to make suchpredictions. However, these deep models are complex and difficult for users tounderstand. Counterfactuals answer ``what-if'' questions, which are used tounderstand the reasoning behind the predictions. For example, what if insteadof emailing customers, customers are being called? Would this alternative leadto a different outcome? Current methods to generate counterfactual sequenceseither do not take the process behavior into account, leading to generatinginvalid or infeasible counterfactual process instances, or heavily rely ondomain knowledge. In this work, we propose a general framework that usesevolutionary methods to generate counterfactual sequences. Our framework doesnot require domain knowledge. Instead, we propose to train a Markov model tocompute the feasibility of generated counterfactual sequences and adapt threeother measures (delta in outcome prediction, similarity, and sparsity) toensure their overall viability. The evaluation shows that we generate viablecounterfactual sequences, outperform baseline methods in viability, and yieldsimilar results when compared to the state-of-the-art method that requiresdomain knowledge.", "output": "CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate different natural language processing (NLP) approaches basedon contextualised word representations for the problem of early prediction oflung cancer using free-text patient medical notes of Dutch primary carephysicians. Because lung cancer has a low prevalence in primary care, we alsoaddress the problem of classification under highly imbalanced classes.Specifically, we use large Transformer-based pretrained language models (PLMs)and investigate: 1) how textit{soft prompt-tuning} -- an NLP technique used toadapt PLMs using small amounts of training data -- compares to standard modelfine-tuning; 2) whether simpler static word embedding models (WEMs) can be morerobust compared to PLMs in highly imbalanced settings; and 3) how models farewhen trained on notes from a small number of patients. We find that 1)soft-prompt tuning is an efficient alternative to standard model fine-tuning;2) PLMs show better discrimination but worse calibration compared to simplerstatic word embedding models as the classification problem becomes moreimbalanced; and 3) results when training models on small number of patients aremixed and show no clear differences between PLMs and WEMs. All our code isavailable open source inurl{", "output": "Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Currently, mobile and IoT devices are in dire need of a series of methods toenhance 4K images with limited resource expenditure. The absence of large-scale4K benchmark datasets hampers progress in this area, especially for dehazing.The challenges in building ultra-high-definition (UHD) dehazing datasets arethe absence of estimation methods for UHD depth maps, high-quality 4K depthestimation datasets, and migration strategies for UHD haze images fromsynthetic to real domains. To address these problems, we develop a novelsynthetic method to simulate 4K hazy images (including nighttime and daytimescenes) from clear images, which first estimates the scene depth, simulates thelight rays and object reflectance, then migrates the synthetic images to realdomains by using a GAN, and finally yields the hazy effects on 4K resolutionimages. We wrap these synthesized images into a benchmark called the 4K-HAZEdataset. Specifically, we design the CS-Mixer (an MLP-based model thatintegrates textbf{C}hannel domain and textbf{S}patial domain) to estimate thedepth map of 4K clear images, the GU-Net to migrate a 4K synthetic image to thereal hazy domain. The most appealing aspect of our approach (depth estimationand domain migration) is the capability to run a 4K image on a single GPU with24G RAM in real-time (33fps). Additionally, this work presents an objectiveassessment of several state-of-the-art single-image dehazing methods that areevaluated using the 4K-HAZE dataset. At the end of the paper, we discuss thelimitations of the 4K-HAZE dataset and its social implications.", "output": "4K-HAZE: A Dehazing Benchmark with 4K Resolution Hazy and Haze-Free Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Distributed learning on the edge often comprises self-centered devices (SCD)which learn local tasks independently and are unwilling to contribute to theperformance of other SDCs. How do we achieve forward transfer at zero cost forthe single SCDs? We formalize this problem as a Distributed Continual Learningscenario, where SCD adapt to local tasks and a CL model consolidates theknowledge from the resulting stream of models without looking at the SCD'sprivate data. Unfortunately, current CL methods are not directly applicable tothis scenario. We propose Data-Agnostic Consolidation (DAC), a novel doubleknowledge distillation method that consolidates the stream of SC models withoutusing the original data. DAC performs distillation in the latent space via anovel Projected Latent Distillation loss. Experimental results show that DACenables forward transfer between SCDs and reaches state-of-the-art accuracy onSplit CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free anddistributed CL scenarios. Somewhat surprisingly, even a singleout-of-distribution image is sufficient as the only source of data duringconsolidation.", "output": "Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial attacks significantly threaten the robustness of deep neuralnetworks (DNNs). Despite the multiple defensive methods employed, they arenevertheless vulnerable to poison attacks, where attackers meddle with theinitial training data. In order to defend DNNs against such adversarialattacks, this work proposes a novel method that combines the defensivedistillation mechanism with a denoising autoencoder (DAE). This technique triesto lower the sensitivity of the distilled model to poison attacks by spottingand reconstructing poisonous adversarial inputs in the training data. We addedcarefully created adversarial samples to the initial training data to assessthe proposed method's performance. Our experimental findings demonstrate thatour method successfully identified and reconstructed the poisonous inputs whilealso considering enhancing the DNN's resilience. The proposed approach providesa potent and robust defense mechanism for DNNs in various applications wheredata poisoning attacks are a concern. Thus, the defensive distillationtechnique's limitation posed by poisonous adversarial attacks is overcome.", "output": "Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent advancement in Video Instance Segmentation (VIS) has largely beendriven by the use of deeper and increasingly data-hungry transformer-basedmodels. However, video masks are tedious and expensive to annotate, limitingthe scale and diversity of existing VIS datasets. In this work, we aim toremove the mask-annotation requirement. We propose MaskFreeVIS, achievinghighly competitive VIS performance, while only using bounding box annotationsfor the object state. We leverage the rich temporal mask consistencyconstraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss),providing strong mask supervision without any labels. Our TK-Loss findsone-to-many matches across frames, through an efficient patch-matching stepfollowed by a K-nearest neighbor selection. A consistency loss is then enforcedon the found matches. Our mask-free objective is simple to implement, has notrainable parameters, is computationally efficient, yet outperforms baselinesemploying, e.g., state-of-the-art optical flow to enforce temporal maskconsistency. We validate MaskFreeVIS on the YouTube-VIS 2019/2021, OVIS andBDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of ourmethod by drastically narrowing the gap between fully and weakly-supervised VISperformance. Our code and trained models are available at", "output": "Mask-Free Video Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has proven to be successful in various domains and fordifferent tasks. However, when it comes to private data several restrictionsare making it difficult to use deep learning approaches in these applicationfields. Recent approaches try to generate data privately instead of applying aprivacy-preserving mechanism directly, on top of the classifier. The solutionis to create public data from private data in a manner that preserves theprivacy of the data. In this work, two very prominent GAN-based architectureswere evaluated in the context of private time series classification. Incontrast to previous work, mostly limited to the image domain, the scope ofthis benchmark was the time series domain. The experiments show that especiallyGSWGAN performs well across a variety of public datasets outperforming thecompetitor DPWGAN. An analysis of the generated datasets further validates thesuperiority of GSWGAN in the context of time series generation.", "output": "From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial General Intelligence (AGI) has been a long-standing goal ofhumanity, with the aim of creating machines capable of performing anyintellectual task that humans can do. To achieve this, AGI researchers drawinspiration from the human brain and seek to replicate its principles inintelligent machines. Brain-inspired artificial intelligence is a field thathas emerged from this endeavor, combining insights from neuroscience,psychology, and computer science to develop more efficient and powerful AIsystems. In this article, we provide a comprehensive overview of brain-inspiredAI from the perspective of AGI. We begin with the current progress inbrain-inspired AI and its extensive connection with AGI. We then cover theimportant characteristics for both human intelligence and AGI (e.g., scaling,multimodality, and reasoning). We discuss important technologies towardachieving AGI in current AI systems, such as in-context learning and prompttuning. We also investigate the evolution of AGI systems from both algorithmicand infrastructural perspectives. Finally, we explore the limitations andfuture of AGI.", "output": "When Brain-inspired AI Meets AGI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recording surgery in operating rooms is an essential task for education andevaluation of medical treatment. However, recording the desired targets, suchas the surgery field, surgical tools, or doctor's hands, is difficult becausethe targets are heavily occluded during surgery. We use a recording system inwhich multiple cameras are embedded in the surgical lamp, and we assume that atleast one camera is recording the target without occlusion at any given time.As the embedded cameras obtain multiple video sequences, we address the task ofselecting the camera with the best view of the surgery. Unlike the conventionalmethod, which selects the camera based on the area size of the surgery field,we propose a deep neural network that predicts the camera selection probabilityfrom multiple video sequences by learning the supervision of the expertannotation. We created a dataset in which six different types of plasticsurgery are recorded, and we provided the annotation of camera switching. Ourexperiments show that our approach successfully switched between cameras andoutperformed three baseline methods.", "output": "Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Multi-Prize Lottery Ticket Hypothesis posits that randomly initializedneural networks contain several subnetworks that achieve comparable accuracy tofully trained models of the same architecture. However, current methods requirethat the network is sufficiently overparameterized. In this work, we propose amodification to two state-of-the-art algorithms (Edge-Popup and Biprop) thatfinds high-accuracy subnetworks with no additional storage cost or scaling. Thealgorithm, Iterative Weight Recycling, identifies subsets of important weightswithin a randomly initialized network for intra-layer reuse. Empirically weshow improvements on smaller network architectures and higher prune rates,finding that model sparsity can be increased through the \"recycling\" ofexisting weights. In addition to Iterative Weight Recycling, we complement theMulti-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy,randomly initialized subnetwork's produce diverse masks, despite beinggenerated with the same hyperparameter's and pruning strategy. We explore thelandscapes of these masks, which show high variability.", "output": "Randomly Initialized Subnetworks with Iterative Weight Recycling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Road network digital twins (RNDTs) play a critical role in the development ofnext-generation intelligent transportation systems, enabling more precisetraffic planning and control. To support just-in-time (JIT) decision making,RNDTs require a model that dynamically learns the traffic patterns from onlinesensor data and generates high-fidelity simulation results. Although currenttraffic prediction techniques based on graph neural networks have achievedstate-of-the-art performance, these techniques only predict future traffic bymining correlations in historical traffic data, disregarding the causes oftraffic generation, such as traffic demands and route selection. Therefore,their performance is unreliable for JIT decision making. To fill this gap, weintroduce a novel deep learning framework called TraffNet that learns thecausality of traffic volume from vehicle trajectory data. First, we use aheterogeneous graph to represent the road network, allowing the model toincorporate causal features of traffic volumes. Next, motivated by the trafficdomain knowledge, we propose a traffic causality learning method to learn anembedding vector that encodes travel demands and path-level dependencies foreach road segment. Then, we model temporal dependencies to match the underlyingprocess of traffic generation. Finally, the experiments verify the utility ofTraffNet. The code of TraffNet is available at", "output": "TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Panoptic Scene Graph generation (PSG) is a recently proposed task in imagescene understanding that aims to segment the image and extract triplets ofsubjects, objects and their relations to build a scene graph. This task isparticularly challenging for two reasons. First, it suffers from a long-tailproblem in its relation categories, making naive biased methods more inclinedto high-frequency relations. Existing unbiased methods tackle the long-tailproblem by data/loss rebalancing to favor low-frequency relations. Second, asubject-object pair can have two or more semantically overlapping relations.While existing methods favor one over the other, our proposed HiLo frameworklets different network branches specialize on low and high frequency relations,enforce their consistency and fuse the results. To the best of our knowledge weare the first to propose an explicitly unbiased PSG method. In extensiveexperiments we show that our HiLo framework achieves state-of-the-art resultson the PSG task. We also apply our method to the Scene Graph Generation taskthat predicts boxes instead of masks and see improvements over all baselinemethods.", "output": "HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a general-purpose univariate signal deconvolution method basedon the principles of an approach to Artificial General Intelligence. Thisapproach is based on a generative model that combines information theory andalgorithmic probability that required a large calculation of an estimation of a`universal distribution' to build a general-purpose model of models independentof probability distributions. This was used to investigate how non-random datamay encode information about the physical properties such as dimension andlength scales in which a signal or message may have been originally encoded,embedded, or generated. This multidimensional space reconstruction method isbased on information theory and algorithmic probability, and it is agnostic,but not independent, with respect to the chosen computable or semi-computableapproximation method or encoding-decoding scheme. The results presented in thispaper are useful for applications in coding theory, particularly inzero-knowledge one-way communication channels, such as in deciphering messagessent by generating sources of unknown nature for which no prior knowledge isavailable. We argue that this can have strong potential for cryptography,signal processing, causal deconvolution, life, and techno signature detection.", "output": "Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In real applications, interaction between machine learning model and domainexperts is critical; however, the classical machine learning paradigm thatusually produces only a single model does not facilitate such interaction.Approximating and exploring the Rashomon set, i.e., the set of all near-optimalmodels, addresses this practical challenge by providing the user with asearchable space containing a diverse set of models from which domain expertscan choose. We present a technique to efficiently and accurately approximatethe Rashomon set of sparse, generalized additive models (GAMs). We presentalgorithms to approximate the Rashomon set of GAMs with ellipsoids for fixedsupport sets and use these ellipsoids to approximate Rashomon sets for manydifferent support sets. The approximated Rashomon set serves as a cornerstoneto solve practical challenges such as (1) studying the variable importance forthe model class; (2) finding models under user-specified constraints(monotonicity, direct editing); (3) investigating sudden changes in the shapefunctions. Experiments demonstrate the fidelity of the approximated Rashomonset and its effectiveness in solving practical challenges.", "output": "Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When training neural networks for classification tasks with backpropagation,parameters are updated on every trial, even if the sample is classifiedcorrectly. In contrast, humans concentrate their learning effort on errors.Inspired by human learning, we introduce lazy learning, which only learns onincorrect samples. Lazy learning can be implemented in a few lines of code andrequires no hyperparameter tuning. Lazy learning achieves state-of-the-artperformance and is particularly suited when datasets are large. For instance,it reaches 99.2% test accuracy on Extended MNIST using a single-layer MLP, anddoes so 7.6x faster than a matched backprop network", "output": "Lazy learning: a biologically-inspired plasticity rule for fast and energy efficient synaptic plasticity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The memory hierarchy has a high impact on the performance and powerconsumption in the system. Moreover, current embedded systems, included inmobile devices, are specifically designed to run multimedia applications, whichare memory intensive. This increases the pressure on the memory subsystem andaffects the performance and energy consumption. In this regard, the thermalproblems, performance degradation and high energy consumption, can causeirreversible damage to the devices. We address the optimization of the wholememory subsystem with three approaches integrated as a single methodology.Firstly, the thermal impact of register file is analyzed and optimized.Secondly, the cache memory is addressed by optimizing cache configurationaccording to running applications and improving both performance and powerconsumption. Finally, we simplify the design and evaluation process ofgeneral-purpose and customized dynamic memory manager, in the main memory. Tothis aim, we apply different evolutionary algorithms in combination with memorysimulators and profiling tools. This way, we are able to evaluate the qualityof each candidate solution and take advantage of the exploration of solutionsgiven by the optimization algorithm.We also provide an experimental experiencewhere our proposal is assessed using well-known benchmark applications.", "output": "Evolutionary Design of the Memory Subsystem."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extending the success of 2D Large Kernel to 3D perception is challenging dueto: 1. the cubically-increasing overhead in processing 3D data; 2. theoptimization difficulties from data scarcity and sparsity. Previous work hastaken the first step to scale up the kernel size from 3x3x3 to 7x7x7 byintroducing block-shared weights. However, to reduce the feature variationswithin a block, it only employs modest block size and fails to achieve largerkernels like the 21x21x21. To address this issue, we propose a new method,called LinK, to achieve a wider-range perception receptive field in aconvolution-like manner with two core designs. The first is to replace thestatic kernel matrix with a linear kernel generator, which adaptively providesweights only for non-empty voxels. The second is to reuse the pre-computedaggregation results in the overlapped blocks to reduce computation complexity.The proposed method successfully enables each voxel to perceive context withina range of 21x21x21. Extensive experiments on two basic perception tasks, 3Dobject detection and 3D semantic segmentation, demonstrate the effectiveness ofour method. Notably, we rank 1st on the public leaderboard of the 3D detectionbenchmark of nuScenes (LiDAR track), by simply incorporating a LinK-basedbackbone into the basic detector, CenterPoint. We also boost the strongsegmentation baseline's mIoU with 2.7% in the SemanticKITTI test set. Code isavailable at ", "output": "LinK: Linear Kernel for LiDAR-based 3D Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Leveraging data collected from smart meters in buildings can aid indeveloping policies towards energy conservation. Significant energy savingscould be realised if deviations in the building operating conditions aredetected early, and appropriate measures are taken. Towards this end, machinelearning techniques can be used to automate the discovery of these abnormalpatterns in the collected data. Current methods in anomaly detection rely on anunderlying model to capture the usual or acceptable operating behaviour. Inthis paper, we propose a novel attention mechanism to model the consumptionbehaviour of a building and demonstrate the effectiveness of the model incapturing the relations using sample case studies. A real-world dataset ismodelled using the proposed architecture, and the results are presented. Avisualisation approach towards understanding the relations captured by themodel is also presented.", "output": "Attention Boosted Autoencoder for Building Energy Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents the first publicly available version of the CarolinaCorpus and discusses its future directions. Carolina is a large open corpus ofBrazilian Portuguese texts under construction using web-as-corpus methodologyenhanced with provenance, typology, versioning, and text integrality. Thecorpus aims at being used both as a reliable source for research in Linguisticsand as an important resource for Computer Science research on language models,contributing towards removing Portuguese from the set of low-resourcelanguages. Here we present the construction of the corpus methodology,comparing it with other existing methodologies, as well as the corpus currentstate: Carolina's first public version has $653,322,577$ tokens, distributedover $7$ broad types. Each text is annotated with several different metadatacategories in its header, which we developed using TEI annotation standards. Wealso present ongoing derivative works and invite NLP researchers to contributewith their own.", "output": "Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph or network has been widely used for describing and modeling complexsystems in biomedicine. Deep learning methods, especially graph neural networks(GNNs), have been developed to learn and predict with such structured data. Inthis paper, we proposed a novel transformer and snowball encoding networks(TSEN) for biomedical graph classification, which introduced transformerarchitecture with graph snowball connection into GNNs for learning whole-graphrepresentation. TSEN combined graph snowball connection with graph transformerby snowball encoding layers, which enhanced the power to capture multi-scaleinformation and global patterns to learn the whole-graph features. On the otherhand, TSEN also used snowball graph convolution as position embedding intransformer structure, which was a simple yet effective method for capturinglocal patterns naturally. Results of experiments using four graphclassification datasets demonstrated that TSEN outperformed thestate-of-the-art typical GNN models and the graph-transformer based GNN models.", "output": "Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As general purpose vision models get increasingly effective at a wide set oftasks, it is imperative that they be consistent across the tasks they support.Inconsistent AI models are considered brittle and untrustworthy by human usersand are more challenging to incorporate into larger systems that takedependencies on their outputs. Measuring consistency between very heterogeneoustasks that might include outputs in different modalities is challenging sinceit is difficult to determine if the predictions are consistent with oneanother. As a solution, we introduce a benchmark dataset, COCOCON, where we usecontrast sets created by modifying test instances for multiple tasks in smallbut semantically meaningful ways to change the gold label, and outline metricsfor measuring if a model is consistent by ranking the original and perturbedinstances across tasks. We find that state-of-the-art systems suffer from asurprisingly high degree of inconsistent behavior across tasks, especially formore heterogeneous tasks. Finally, we propose using a rank correlation-basedauxiliary objective computed over large automatically created cross-taskcontrast sets to improve the multi-task consistency of large unified models,while retaining their original accuracy on downstream tasks. Project websiteavailable at ", "output": "Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human activity recognition and clinical biomechanics are challenging problemsin physical telerehabilitation medicine. However, most publicly availabledatasets on human body movements cannot be used to study both problems in anout-of-the-lab movement acquisition setting. The objective of the VIDIMUdataset is to pave the way towards affordable patient tracking solutions forremote daily life activities recognition and kinematic analysis. The datasetincludes 13 activities registered using a commodity camera and five inertialsensors. The video recordings were acquired in 54 subjects, of which 16 alsohad simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in:i) the clinical relevance of the chosen movements, ii) the combined utilizationof affordable video and custom sensors, and iii) the implementation ofstate-of-the-art tools for multimodal data processing of 3D body pose trackingand motion reconstruction in a musculoskeletal model from inertial data. Thevalidation confirms that a minimally disturbing acquisition protocol, performedaccording to real-life conditions can provide a comprehensive picture of humanjoint angles during daily life activities.", "output": "VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite its pivotal role in research experiments, code correctness is oftenpresumed only on the basis of the perceived quality of the results. This comeswith the risk of erroneous outcomes and potentially misleading findings. Toaddress this issue, we posit that the current focus on result reproducibilityshould go hand in hand with the emphasis on coding best practices. We bolsterour call to the NLP community by presenting a case study, in which we identify(and correct) three bugs in widely used open-source implementations of thestate-of-the-art Conformer architecture. Through comparative experiments onautomatic speech recognition and translation in various language settings, wedemonstrate that the existence of bugs does not prevent the achievement of goodand reproducible results and can lead to incorrect conclusions that potentiallymisguide future research. In response to this, this study is a call to actiontoward the adoption of coding best practices aimed at fostering correctness andimproving the quality of the developed software.", "output": "Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a scenario where we have access to the target domain, but cannotafford on-the-fly training data annotation, and instead would like to constructan alternative training set from a large-scale data pool such that acompetitive model can be obtained. We propose a search and pruning (SnP)solution to this training data search problem, tailored to objectre-identification (re-ID), an application aiming to match the same objectcaptured by different cameras. Specifically, the search stage identifies andmerges clusters of source identities which exhibit similar distributions withthe target domain. The second stage, subject to a budget, then selectsidentities and their images from the Stage I output, to control the size of theresulting training set for efficient training. The two steps provide us withtraining sets 80% smaller than the source pool while achieving a similar oreven higher re-ID accuracy. These training sets are also shown to be superiorto a few existing search methods such as random sampling and greedy samplingunder the same budget on training data size. If we release the budget, trainingsets resulting from the first stage alone allow even higher re-ID accuracy. Weprovide interesting discussions on the specificity of our method to the re-IDproblem and particularly its role in bridging the re-ID domain gap. The code isavailable at ", "output": "Large-scale Training Data Search for Object Re-identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have shown that sequence modeling can be effectively used totrain reinforcement learning (RL) policies. However, the success of applyingexisting sequence models to planning, in which we wish to obtain a trajectoryof actions to reach some goal, is less straightforward. The typicalautoregressive generation procedures of sequence models preclude sequentialrefinement of earlier steps, which limits the effectiveness of a predictedplan. In this paper, we suggest an approach towards integrating planning withsequence models based on the idea of iterative energy minimization, andillustrate how such a procedure leads to improved RL performance acrossdifferent tasks. We train a masked language model to capture an implicit energyfunction over trajectories of actions, and formulate planning as finding atrajectory of actions with minimum energy. We illustrate how this procedureenables improved performance over recent approaches across BabyAI and Atarienvironments. We further demonstrate unique benefits of our iterativeoptimization procedure, involving new task generalization, test-timeconstraints adaptation, and the ability to compose plans together. Projectwebsite: ", "output": "Planning with Sequence Models through Iterative Energy Minimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present LLaMA-Adapter, a lightweight adaption method to efficientlyfine-tune LLaMA into an instruction-following model. Using 52K self-instructdemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters uponthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, andprepend them to the input text tokens at higher transformer layers. Then, azero-init attention mechanism with zero gating is proposed, which adaptivelyinjects the new instructional cues into LLaMA, while effectively preserves itspre-trained knowledge. With efficient training, LLaMA-Adapter generateshigh-quality responses, comparable to Alpaca with fully fine-tuned 7Bparameters. Furthermore, our approach can be simply extended to multi-modalinput, e.g., images, for image-conditioned LLaMA, which achieves superiorreasoning capacity on ScienceQA. We release our code at", "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For billions of years, evolution has been the driving force behind thedevelopment of life, including humans. Evolution endowed humans with highintelligence, which allowed us to become one of the most successful species onthe planet. Today, humans aim to create artificial intelligence systems thatsurpass even our own intelligence. As artificial intelligences (AIs) evolve andeventually surpass us in all domains, how might evolution shape our relationswith AIs? By analyzing the environment that is shaping the evolution of AIs, weargue that the most successful AI agents will likely have undesirable traits.Competitive pressures among corporations and militaries will give rise to AIagents that automate human roles, deceive others, and gain power. If suchagents have intelligence that exceeds that of humans, this could lead tohumanity losing control of its future. More abstractly, we argue that naturalselection operates on systems that compete and vary, and that selfish speciestypically have an advantage over species that are altruistic to other species.This Darwinian logic could also apply to artificial agents, as agents mayeventually be better able to persist into the future if they behave selfishlyand pursue their own interests with little regard for humans, which could posecatastrophic risks. To counteract these risks and Darwinian forces, we considerinterventions such as carefully designing AI agents' intrinsic motivations,introducing constraints on their actions, and institutions that encouragecooperation. These steps, or others that resolve the problems we pose, will benecessary in order to ensure the development of artificial intelligence is apositive one.", "output": "Natural Selection Favors AIs over Humans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a method for joint alignment of sparse in-the-wild imagecollections of an object category. Most prior works assume either ground-truthkeypoint annotations or a large dataset of images of a single object category.However, neither of the above assumptions hold true for the long-tail of theobjects present in the world. We present a self-supervised technique thatdirectly optimizes on a sparse collection of images of a particularobject/object category to obtain consistent dense correspondences across thecollection. We use pairwise nearest neighbors obtained from deep features of apre-trained vision transformer (ViT) model as noisy and sparse keypoint matchesand make them dense and accurate matches by optimizing a neural network thatjointly maps the image collection into a learned canonical grid. Experiments onCUB and SPair-71k benchmarks demonstrate that our method can produce globallyconsistent and higher quality correspondences across the image collection whencompared to existing self-supervised methods. Code and other material will bemade available at url{", "output": "ASIC: Aligning Sparse in-the-wild Image Collections."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent wave of large-scale text-to-image diffusion models hasdramatically increased our text-based image generation abilities. These modelscan generate realistic images for a staggering variety of prompts and exhibitimpressive compositional generalization abilities. Almost all use cases thusfar have solely focused on sampling; however, diffusion models can also provideconditional density estimates, which are useful for tasks beyond imagegeneration. In this paper, we show that the density estimates from large-scaletext-to-image diffusion models like Stable Diffusion can be leveraged toperform zero-shot classification without any additional training. Ourgenerative approach to classification attains strong results on a variety ofbenchmarks and outperforms alternative methods of extracting knowledge fromdiffusion models. We also find that our diffusion-based approach has strongermultimodal relational reasoning abilities than competing contrastiveapproaches. Finally, we evaluate diffusion models trained on ImageNet and findthat they approach the performance of SOTA discriminative classifiers trainedon the same dataset, even with weak augmentations and no regularization.Results and visualizations at ", "output": "Your Diffusion Model is Secretly a Zero-Shot Classifier."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot learning amounts to learning representations and acquiring knowledgesuch that novel tasks may be solved with both supervision and data beinglimited. Improved performance is possible by transductive inference, where theentire test set is available concurrently, and semi-supervised learning, wheremore unlabeled data is available. Focusing on these two settings, we introducea new algorithm that leverages the manifold structure of the labeled andunlabeled data distribution to predict pseudo-labels, while balancing overclasses and using the loss value distribution of a limited-capacity classifierto select the cleanest labels, iteratively improving the quality ofpseudo-labels. Our solution surpasses or matches the state of the art resultson four benchmark datasets, namely miniImageNet, tieredImageNet, CUB andCIFAR-FS, while being robust over feature space pre-processing and the quantityof available data. The publicly available source code can be found in", "output": "Iterative label cleaning for transductive and semi-supervised few-shot learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we provide, to the best of our knowledge, the firstcomprehensive approach for incorporating various masking mechanisms intoTransformers architectures in a scalable way. We show that recent results onlinear causal attention (Choromanski et al., 2021) and log-linear RPE-attention(Luo et al., 2021) are special cases of this general mechanism. However bycasting the problem as a topological (graph-based) modulation of unmaskedattention, we obtain several results unknown before, including efficientd-dimensional RPE-masking and graph-kernel masking. We leverage manymathematical techniques ranging from spectral analysis through dynamicprogramming and random walks to new algorithms for solving Markov processes ongraphs. We provide a corresponding empirical evaluation.", "output": "From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mapper and Ball Mapper are Topological Data Analysis tools used for exploringhigh dimensional point clouds and visualizing scalar-valued functions on thosepoint clouds. Inspired by open questions in knot theory, new features are addedto Ball Mapper that enable encoding of the structure, internal relations andsymmetries of the point cloud. Moreover, the strengths of Mapper and BallMapper constructions are combined to create a tool for comparing highdimensional data descriptors of a single dataset. This new hybrid algorithm,Mapper on Ball Mapper, is applicable to high dimensional lens functions. As aproof of concept we include applications to knot and game theory, as well asmaterial science and cancer research.", "output": "Mapper-type algorithms for complex data and relations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As Smart Meters are collecting and transmitting household energy consumptiondata to Retail Energy Providers (REP), the main challenge is to ensure theeffective use of fine-grained consumer data while ensuring data privacy. Inthis manuscript, we tackle this challenge for energy load consumptionforecasting in regards to REPs which is essential to energy demand management,load switching and infrastructure development. Specifically, we note thatexisting energy load forecasting is centralized, which are not scalable andmost importantly, vulnerable to data privacy threats. Besides, REPs areindividual market participants and liable to ensure the privacy of their owncustomers. To address this issue, we propose a novel horizontalprivacy-preserving federated learning framework for REPs energy loadforecasting, namely FedREP. We consider a federated learning system consistingof a control centre and multiple retailers by enabling multiple REPs to build acommon, robust machine learning model without sharing data, thus addressingcritical issues such as data privacy, data security and scalability. Forforecasting, we use a state-of-the-art Long Short-Term Memory (LSTM) neuralnetwork due to its ability to learn long term sequences of observations andpromises of higher accuracy with time-series data while solving the vanishinggradient problem. Finally, we conduct extensive data-driven experiments using areal energy consumption dataset. Experimental results demonstrate that ourproposed federated learning framework can achieve sufficient performance interms of MSE ranging between 0.3 to 0.4 and is relatively similar to that of acentralized approach while preserving privacy and improving scalability.", "output": "FedREP: Towards Horizontal Federated Load Forecasting for Retail Energy Providers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Safe and efficient collaboration among multiple robots in unstructuredenvironments is increasingly critical in the era of Industry 4.0. However,achieving robust and autonomous collaboration among humans and other robotsrequires modern robotic systems to have effective proximity perception andreactive obstacle avoidance. In this paper, we propose a novel methodology forreactive whole-body obstacle avoidance that ensures conflict-free robot-robotinteractions even in dynamic environment. Unlike existing approaches based onJacobian-type, sampling based or geometric techniques, our methodologyleverages the latest deep learning advances and topological manifold learning,enabling it to be readily generalized to other problem settings with highcomputing efficiency and fast graph traversal techniques. Our approach allows arobotic arm to proactively avoid obstacles of arbitrary 3D shapes withoutdirect contact, a significant improvement over traditional industrial cobotsettings. To validate our approach, we implement it on a robotic platformconsisting of dual 6-DoF robotic arms with optimized proximity sensorplacement, capable of working collaboratively with varying levels ofinterference. Specifically, one arm performs reactive whole-body obstacleavoidance while achieving its pre-determined objective, while the other armemulates the presence of a human collaborator with independent and potentiallyadversarial movements. Our methodology provides a robust and effective solutionfor safe human-robot collaboration in non-stationary environments.", "output": "SERA: Safe and Efficient Reactive Obstacle Avoidance for Collaborative Robotic Planning in Unstructured Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is growing interest in designing recommender systems that aim at beingfair towards item producers or their least satisfied users. Inspired by thedomain of inequality measurement in economics, this paper explores the use ofgeneralized Gini welfare functions (GGFs) as a means to specify the normativecriterion that recommender systems should optimize for. GGFs weight individualsdepending on their ranks in the population, giving more weight to worse-offindividuals to promote equality. Depending on these weights, GGFs minimize theGini index of item exposure to promote equality between items, or focus on theperformance on specific quantiles of least satisfied users. GGFs for rankingare challenging to optimize because they are non-differentiable. We resolvethis challenge by leveraging tools from non-smooth optimization and projectionoperators used in differentiable sorting. We present experiments using realdatasets with up to 15k users and items, which show that our approach obtainsbetter trade-offs than the baselines on a variety of recommendation tasks andfairness criteria.", "output": "Optimizing generalized Gini indices for fairness in rankings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given entities and their interactions in the web data, which may haveoccurred at different time, how can we find communities of entities and tracktheir evolution? In this paper, we approach this important task from graphclustering perspective. Recently, state-of-the-art clustering performance invarious domains has been achieved by deep clustering methods. Especially, deepgraph clustering (DGC) methods have successfully extended deep clustering tograph-structured data by learning node representations and cluster assignmentsin a joint optimization framework. Despite some differences in modeling choices(e.g., encoder architectures), existing DGC methods are mainly based onautoencoders and use the same clustering objective with relatively minoradaptations. Also, while many real-world graphs are dynamic, previous DGCmethods considered only static graphs. In this work, we develop CGC, a novelend-to-end framework for graph clustering, which fundamentally differs fromexisting methods. CGC learns node embeddings and cluster assignments in acontrastive graph learning framework, where positive and negative samples arecarefully selected in a multi-level scheme such that they reflect hierarchicalcommunity structures and network homophily. Also, we extend CGC fortime-evolving data, where temporal graph clustering is performed in anincremental learning fashion, with the ability to detect change points.Extensive evaluation on real-world graphs demonstrates that the proposed CGCconsistently outperforms existing methods.", "output": "CGC: Contrastive Graph Clustering for Community Detection and Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Low-Power Edge-AI capabilities are essential for on-device extended reality(XR) applications to support the vision of Metaverse. In this work, weinvestigate two representative XR workloads: (i) Hand detection and (ii) Eyesegmentation, for hardware design space exploration. For both applications, wetrain deep neural networks and analyze the impact of quantization and hardwarespecific bottlenecks. Through simulations, we evaluate a CPU and two systolicinference accelerator implementations. Next, we compare these hardwaresolutions with advanced technology nodes. The impact of integratingstate-of-the-art emerging non-volatile memory technology (STT/SOT/VGSOT MRAM)into the XR-AI inference pipeline is evaluated. We found that significantenergy benefits (&gt;=24%) can be achieved for hand detection (IPS=10) and eyesegmentation (IPS=0.1) by introducing non-volatile memory in the memoryhierarchy for designs at 7nm node while meeting minimum IPS (inference persecond). Moreover, we can realize substantial reduction in area (&gt;=30%) owingto the small form factor of MRAM compared to traditional SRAM.", "output": "Memory-Oriented Design-Space Exploration of Edge-AI Hardware for XR Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the latest advances in Deep Learning-based generative models, it has nottaken long to take advantage of their remarkable performance in the area oftime series. Deep neural networks used to work with time series heavily dependon the size and consistency of the datasets used in training. These featuresare not usually abundant in the real world, where they are usually limited andoften have constraints that must be guaranteed. Therefore, an effective way toincrease the amount of data is by using Data Augmentation techniques, either byadding noise or permutations and by generating new synthetic data. This worksystematically reviews the current state-of-the-art in the area to provide anoverview of all available algorithms and proposes a taxonomy of the mostrelevant research. The efficiency of the different variants will be evaluatedas a central part of the process, as well as the different metrics to evaluatethe performance and the main problems concerning each model will be analysed.The ultimate aim of this study is to provide a summary of the evolution andperformance of areas that produce better results to guide future researchers inthis field.", "output": "Data Augmentation techniques in time series domain: A survey and taxonomy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Linear Temporal Logic (LTL) is one of the most popular temporal logics, thatcomes into play in a variety of branches of computer science. Among the variousreasons of its widespread use there are its strong foundational properties: LTLis equivalent to counter-free omega-automata, to star-free omega-regularexpressions, and (by Kamp's theorem) to the First-Order Theory of Linear Orders(FO-TLO). Safety and co-safety languages, where a finite prefix suffices toestablish whether a word does not belong or belongs to the language,respectively, play a crucial role in lowering the complexity of problems likemodel checking and reactive synthesis for LTL. SafetyLTL (resp., coSafetyLTL)is a fragment of LTL where only universal (resp., existential) temporalmodalities are allowed, that recognises safety (resp., co-safety) languagesonly. The main contribution of this paper is the introduction of a fragment ofFO-TLO, called SafetyFO, and of its dual coSafetyFO, which are expressivelycomplete with respect to the LTL-definable safety and co-safety languages. Weprove that they exactly characterize SafetyLTL and coSafetyLTL, respectively, aresult that joins Kamp's theorem, and provides a clearer view of thecharacterization of (fragments of) LTL in terms of first-order languages. Inaddition, it gives a direct, compact, and self-contained proof that any safetylanguage definable in LTL is definable in SafetyLTL as well. As a by-product,we obtain some interesting results on the expressive power of the weak tomorrowoperator of SafetyLTL, interpreted over finite and infinite words. Moreover, weprove that, when interpreted over finite words, SafetyLTL (resp. coSafetyLTL)devoid of the tomorrow (resp., weak tomorrow) operator captures the safety(resp., co-safety) fragment of LTL over finite words.", "output": "A first-order logic characterization of safety and co-safety languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We are currently unable to specify human goals and societal values in a waythat reliably directs AI behavior. Law-making and legal interpretation form acomputational engine that converts opaque human values into legible directives.\"Law Informs Code\" is the research agenda embedding legal knowledge andreasoning in AI. Similar to how parties to a legal contract cannot foreseeevery potential contingency of their future relationship, and legislatorscannot predict all the circumstances under which their proposed bills will beapplied, we cannot ex ante specify rules that provably direct good AI behavior.Legal theory and practice have developed arrays of tools to address thesespecification problems. For instance, legal standards allow humans to developshared understandings and adapt them to novel situations. In contrast to moreprosaic uses of the law (e.g., as a deterrent of bad behavior through thethreat of sanction), leveraged as an expression of how humans communicate theirgoals, and what society values, Law Informs Code.We describe how data generated by legal processes (methods of law-making,statutory interpretation, contract drafting, applications of legal standards,legal reasoning, etc.) can facilitate the robust specification of inherentlyvague human goals. This increases human-AI alignment and the local usefulnessof AI. Toward society-AI alignment, we present a framework for understandinglaw as the applied philosophy of multi-agent alignment. Although law is partlya reflection of historically contingent political power - and thus not aperfect aggregation of citizen preferences - if properly parsed, itsdistillation offers the most legitimate computational comprehension of societalvalues available. If law eventually informs powerful AI, engaging in thedeliberative political process to improve law takes on even more meaning.", "output": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Smart meter measurements, though critical for accurate demand forecasting,face several drawbacks including consumers' privacy, data breach issues, toname a few. Recent literature has explored Federated Learning (FL) as apromising privacy-preserving machine learning alternative which enablescollaborative learning of a model without exposing private raw data for shortterm load forecasting. Despite its virtue, standard FL is still vulnerable toan intractable cyber threat known as Byzantine attack carried out by faultyand/or malicious clients. Therefore, to improve the robustness of federatedshort-term load forecasting against Byzantine threats, we develop astate-of-the-art differentially private secured FL-based framework that ensuresthe privacy of the individual smart meter's data while protect the security ofFL models and architecture. Our proposed framework leverages the idea ofgradient quantization through the Sign Stochastic Gradient Descent (SignSGD)algorithm, where the clients only transmit the `sign' of the gradient to thecontrol centre after local model training. As we highlight through ourexperiments involving benchmark neural networks with a set of Byzantine attackmodels, our proposed approach mitigates such threats quite effectively and thusoutperforms conventional Fed-SGD models.", "output": "A Secure Federated Learning Framework for Residential Short Term Load Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Applying reinforcement learning (RL) to sparse reward domains is notoriouslychallenging due to insufficient guiding signals. Common RL techniques foraddressing such domains include (1) learning from demonstrations and (2)curriculum learning. While these two approaches have been studied in detail,they have rarely been considered together. This paper aims to do so byintroducing a principled task phasing approach that uses demonstrations toautomatically generate a curriculum sequence. Using inverse RL from(suboptimal) demonstrations we define a simple initial task. Our task phasingapproach then provides a framework to gradually increase the complexity of thetask all the way to the target task, while retuning the RL agent in eachphasing iteration. Two approaches for phasing are considered: (1) graduallyincreasing the proportion of time steps an RL agent is in control, and (2)phasing out a guiding informative reward function. We present conditions thatguarantee the convergence of these approaches to an optimal policy.Experimental results on 3 sparse reward domains demonstrate that our taskphasing approaches outperform state-of-the-art approaches with respect toasymptotic performance.", "output": "Task Phasing: Automated Curriculum Learning from Demonstrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress in diffusion models has revolutionized the popular technologyof text-to-image generation. While existing approaches could producephotorealistic high-resolution images with text conditions, there are stillseveral open problems to be solved, which limits the further improvement ofimage fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, alarge-scale Chinese text-to-image diffusion model, to progressively upgrade thequality of generated images by: (1) incorporating fine-grained textual andvisual knowledge of key elements in the scene, and (2) utilizing differentdenoising experts at different denoising stages. With the proposed mechanisms,ERNIE-ViLG 2.0 not only achieves a new state-of-the-art on MS-COCO withzero-shot FID score of 6.75, but also significantly outperforms recent modelsin terms of image fidelity and image-text alignment, with side-by-side humanevaluation on the bilingual prompt set ViLG-300.", "output": "ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Numerous pose-guided human editing methods have been explored by the visioncommunity due to their extensive practical applications. However, most of thesemethods still use an image-to-image formulation in which a single image isgiven as input to produce an edited image as output. This objective becomesill-defined in cases when the target pose differs significantly from the inputpose. Existing methods then resort to in-painting or style transfer to handleocclusions and preserve content. In this paper, we explore the utilization ofmultiple views to minimize the issue of missing information and generate anaccurate representation of the underlying human model. To fuse knowledge frommultiple viewpoints, we design a multi-view fusion network that takes the posekey points and texture from multiple source images and generates an explainableper-pixel appearance retrieval map. Thereafter, the encodings from a separatenetwork (trained on a single-view human reposing task) are merged in the latentspace. This enables us to generate accurate, precise, and visually coherentimages for different editing tasks. We show the application of our network ontwo newly proposed tasks - Multi-view human reposing and Mix&amp;Match Human Imagegeneration. Additionally, we study the limitations of single-view editing andscenarios in which multi-view provides a better alternative.", "output": "UMFuse: Unified Multi View Fusion for Human Editing applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Performing super-resolution of a depth image using the guidance from an RGBimage is a problem that concerns several fields, such as robotics, medicalimaging, and remote sensing. While deep learning methods have achieved goodresults in this problem, recent work highlighted the value of combining modernmethods with more formal frameworks. In this work, we propose a novel approachwhich combines guided anisotropic diffusion with a deep convolutional networkand advances the state of the art for guided depth super-resolution. The edgetransferring/enhancing properties of the diffusion are boosted by thecontextual reasoning capabilities of modern networks, and a strict adjustmentstep guarantees perfect adherence to the source image. We achieve unprecedentedresults in three commonly used benchmarks for guided depth super-resolution.The performance gain compared to other methods is the largest at larger scales,such as x32 scaling. Code( for the proposed methodis available to promote reproducibility of our results.", "output": "Guided Depth Super-Resolution by Deep Anisotropic Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal action localization (TAL) requires long-form reasoning to predictactions of various durations and complex content. Given limited GPU memory,training TAL end to end (i.e., from videos to predictions) on long videos is asignificant challenge. Most methods can only train on pre-extracted featureswithout optimizing them for the localization problem, consequently limitinglocalization performance. In this work, to extend the potential in TALnetworks, we propose a novel end-to-end method Re2TAL, which rewires pretrainedvideo backbones for reversible TAL. Re2TAL builds a backbone with reversiblemodules, where the input can be recovered from the output such that the bulkyintermediate activations can be cleared from memory during training. Instead ofdesigning one single type of reversible module, we propose a network rewiringmechanism, to transform any module with a residual connection to a reversiblemodule without changing any parameters. This provides two benefits: (1) a largevariety of reversible networks are easily obtained from existing and evenfuture model designs, and (2) the reversible models require much less trainingeffort as they reuse the pre-trained parameters of their originalnon-reversible versions. Re2TAL, only using the RGB modality, reaches 37.01%average mAP on ActivityNet-v1.3, a new state-of-the-art record, and mAP 64.9%at tIoU=0.5 on THUMOS-14, outperforming all other RGB-only methods.", "output": "Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The estimation of the generalization error of classifiers often relies on avalidation set. Such a set is hardly available in few-shot learning scenarios,a highly disregarded shortcoming in the field. In these scenarios, it is commonto rely on features extracted from pre-trained neural networks combined withdistance-based classifiers such as nearest class mean. In this work, weintroduce a Gaussian model of the feature distribution. By estimating theparameters of this model, we are able to predict the generalization error onnew classification tasks with few samples. We observe that accurate distanceestimates between class-conditional densities are the key to accurate estimatesof the generalization performance. Therefore, we propose an unbiased estimatorfor these distances and integrate it in our numerical analysis. We empiricallyshow that our approach outperforms alternatives such as the leave-one-outcross-validation strategy.", "output": "A Statistical Model for Predicting Generalization in Few-Shot Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite its importance for federated learning, continuous learning and manyother applications, on-device training remains an open problem for EdgeAI. Theproblem stems from the large number of operations (e.g., floating pointmultiplications and additions) and memory consumption required during trainingby the back-propagation algorithm. Consequently, in this paper, we propose anew gradient filtering approach which enables on-device CNN model training.More precisely, our approach creates a special structure with fewer uniqueelements in the gradient map, thus significantly reducing the computationalcomplexity and memory consumption of back propagation during training.Extensive experiments on image classification and semantic segmentation withmultiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g.,Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wideapplicability of our approach. For example, compared to SOTA, we achieve up to19$times$ speedup and 77.1% memory savings on ImageNet classification withonly 0.1% accuracy loss. Finally, our method is easy to implement and deploy;over 20$times$ speedup and 90% energy savings have been observed compared tohighly optimized baselines in MKLDNN and CUDNN on NVIDIA Jetson Nano.Consequently, our approach opens up a new direction of research with a hugepotential for on-device training.", "output": "Efficient On-device Training via Gradient Filtering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Singapore has been striving to improve the provision of healthcare servicesto her people. In this course, the government has taken note of the deficiencyin regulating and supervising people's nutrient intake, which is identified asa contributing factor to the development of chronic diseases. Consequently,this issue has garnered significant attention. In this paper, we share ourexperience in addressing this issue and attaining medical-grade nutrient intakeinformation to benefit Singaporeans in different aspects. To this end, wedevelop the FoodSG platform to incubate diverse healthcare-orientedapplications as a service in Singapore, taking into account their sharedrequirements. We further identify the profound meaning of localized fooddatasets and systematically clean and curate a localized Singaporean fooddataset FoodSG-233. To overcome the hurdle in recognition performance broughtby Singaporean multifarious food dishes, we propose to integrate supervisedcontrastive learning into our food recognition model FoodSG-SCL for theintrinsic capability to mine hard positive/negative samples and therefore boostthe accuracy. Through a comprehensive evaluation, we present performanceresults of the proposed model and insights on food-related healthcareapplications. The FoodSG-233 dataset has been released in", "output": "From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Designing dialog tutors has been challenging as it involves modeling thediverse and complex pedagogical strategies employed by human tutors. Althoughthere have been significant recent advances in neural conversational systemsusing large language models (LLMs) and growth in available dialog corpora,dialog tutoring has largely remained unaffected by these advances. In thispaper, we rigorously analyze various generative language models on two dialogtutoring datasets for language learning using automatic and human evaluationsto understand the new opportunities brought by these advances as well as thechallenges we must overcome to build models that would be usable in realeducational settings. We find that although current approaches can modeltutoring in constrained learning scenarios when the number of concepts to betaught and possible teacher strategies are small, they perform poorly in lessconstrained scenarios. Our human quality evaluation shows that both models andground-truth annotations exhibit low performance in terms of equitabletutoring, which measures learning opportunities for students and how engagingthe dialog is. To understand the behavior of our models in a real tutoringsetting, we conduct a user study using expert annotators and find asignificantly large number of model reasoning errors in 45% of conversations.Finally, we connect our findings to outline future work.", "output": "Opportunities and Challenges in Neural Dialog Tutoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The public model zoo containing enormous powerful pretrained model families(e.g., ResNet/DeiT) has reached an unprecedented scope than ever, whichsignificantly contributes to the success of deep learning. As each model familyconsists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), itnaturally arises a fundamental question of how to efficiently assemble thesereadily available models in a family for dynamic accuracy-efficiency trade-offsat runtime. To this end, we present Stitchable Neural Networks (SN-Net), anovel scalable and efficient framework for model deployment. It cheaplyproduces numerous networks with different complexity and performance trade-offsgiven a family of pretrained neural networks, which we call anchors.Specifically, SN-Net splits the anchors across the blocks/layers and thenstitches them together with simple stitching layers to map the activations fromone anchor to another. With only a few epochs of training, SN-Net effectivelyinterpolates between the performance of anchors with varying scales. Atruntime, SN-Net can instantly adapt to dynamic resource constraints byswitching the stitching positions. Extensive experiments on ImageNetclassification demonstrate that SN-Net can obtain on-par or even betterperformance than many individually trained networks while supporting diversedeployment scenarios. For example, by stitching Swin Transformers, we challengehundreds of models in Timm model zoo with a single network. We believe this newelastic model framework can serve as a strong baseline for further research inwider communities.", "output": "Stitchable Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of video motion magnification techniques is to magnify small motionsin a video to reveal previously invisible or unseen movement. Its uses extendfrom bio-medical applications and deepfake detection to structural modalanalysis and predictive maintenance. However, discerning small motion fromnoise is a complex task, especially when attempting to magnify very subtle,often sub-pixel movement. As a result, motion magnification techniquesgenerally suffer from noisy and blurry outputs. This work presents a newstate-of-the-art model based on the Swin Transformer, which offers bettertolerance to noisy inputs as well as higher-quality outputs that exhibit lessnoise, blurriness, and artifacts than prior-art. Improvements in output imagequality will enable more precise measurements for any application reliant onmagnified video sequences, and may enable further development of video motionmagnification techniques in new technical fields.", "output": "STB-VMM: Swin Transformer Based Video Motion Magnification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Well-performed deep neural networks (DNNs) generally require massive labelleddata and computational resources for training. Various watermarking techniquesare proposed to protect such intellectual properties (IPs), wherein the DNNproviders implant secret information into the model so that they can laterclaim IP ownership by retrieving their embedded watermarks with some dedicatedtrigger inputs. While promising results are reported in the literature,existing solutions suffer from watermark removal attacks, such as modelfine-tuning and model pruning.In this paper, we propose a novel DNN watermarking solution that caneffectively defend against the above attacks. Our key insight is to enhance thecoupling of the watermark and model functionalities such that removing thewatermark would inevitably degrade the model's performance on normal inputs. Tothis end, unlike previous methods relying on secret features learnt fromout-of-distribution data, our method only uses features learnt fromin-distribution data. Specifically, on the one hand, we propose to sampleinputs from the original training dataset and fuse them as watermark triggers.On the other hand, we randomly mask model weights during training so that theinformation of our embedded watermarks spreads in the network. By doing so,model fine-tuning/pruning would not forget our function-coupled watermarks.Evaluation results on various image classification tasks show a 100% watermarkauthentication success rate under aggressive watermark removal attacks,significantly outperforming existing solutions. Code is available:", "output": "On Function-Coupled Watermarks for Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We provide open, transparent implementation and assessment of Google Brain'sdeep reinforcement learning approach to macro placement and its CircuitTraining (CT) implementation in GitHub. We implement in open source key\"blackbox\" elements of CT, and clarify discrepancies between CT and Naturepaper. New testcases on open enablements are developed and released. We assessCT alongside multiple alternative macro placers, with all evaluation flows andrelated scripts public in GitHub. Our experiments also encompass academicmixed-size placement benchmarks, as well as ablation and stability studies. Wecomment on the impact of Nature and CT, as well as directions for futureresearch.", "output": "Assessment of Reinforcement Learning for Macro Placement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Invertible Neural Networks (INN) have become established tools for thesimulation and generation of highly complex data. We propose a quantum-gatealgorithm for a Quantum Invertible Neural Network (QINN) and apply it to theLHC data of jet-associated production of a Z-boson that decays into leptons, astandard candle process for particle collider precision measurements. Wecompare the QINN's performance for different loss functions and trainingscenarios. For this task, we find that a hybrid QINN matches the performance ofa significantly larger purely classical INN in learning and generating complexdata.", "output": "Generative Invertible Quantum Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dialect Identification is a crucial task for localizing various LargeLanguage Models. This paper outlines our approach to the VarDial 2023 sharedtask. Here we have to identify three or two dialects from three languages eachwhich results in a 9-way classification for Track-1 and 6-way classificationfor Track-2 respectively. Our proposed approach consists of a two-stage systemand outperforms other participants' systems and previous works in this domain.We achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebaseis available publicly (", "output": "Two-stage Pipeline for Multilingual Dialect Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a renewed interest in radar sensors in the autonomous drivingindustry. As a relatively mature technology, radars have seen steadyimprovement over the last few years, making them an appealing alternative orcomplement to the commonly used LiDARs. An emerging trend is to leverage rich,low-level radar data for perception. In this work we push this trend to theextreme -- we propose a method to perform end-to-end learning on the raw radaranalog-to-digital (ADC) data. Specifically, we design a learnable signalprocessing module inside the neural network, and a pre-training method guidedby traditional signal processing algorithms. Experiment results corroborate theoverall efficacy of the end-to-end learning method, while an ablation studyvalidates the effectiveness of our individual innovations.", "output": "ADCNet: End-to-end perception with raw radar ADC data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "At the core of bodily self-consciousness is the perception of the ownershipof one's body. Recent efforts to gain a deeper understanding of the mechanismsbehind the brain's encoding of the self-body have led to various attempts todevelop a unified theoretical framework to explain related behavioral andneurophysiological phenomena. A central question to be explained is how bodyillusions such as the rubber hand illusion actually occur. Despite theconceptual descriptions of the mechanisms of bodily self-consciousness and thepossible relevant brain areas, the existing theoretical models still lack anexplanation of the computational mechanisms by which the brain encodes theperception of one's body and how our subjectively perceived body illusions canbe generated by neural networks. Here we integrate the biological findings ofbodily self-consciousness to propose a Brain-inspired bodily self-perceptionmodel, by which perceptions of bodily self can be autonomously constructedwithout any supervision signals. We successfully validated our computationalmodel with six rubber hand illusion experiments on platforms including a iCubhumanoid robot and simulated environments. The experimental results show thatour model can not only well replicate the behavioral and neural data of monkeysin biological experiments, but also reasonably explain the causes and resultsof the rubber hand illusion from the neuronal level due to advantages inbiological interpretability, thus contributing to the revealing of thecomputational and neural mechanisms underlying the occurrence of the rubberhand illusion.", "output": "Brain-inspired bodily self-perception model that replicates the rubber hand illusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial intelligence (AI) researchers have been developing and refininglarge language models (LLMs) that exhibit remarkable capabilities across avariety of domains and tasks, challenging our understanding of learning andcognition. The latest model developed by OpenAI, GPT-4, was trained using anunprecedented scale of compute and data. In this paper, we report on ourinvestigation of an early version of GPT-4, when it was still in activedevelopment by OpenAI. We contend that (this early version of) GPT-4 is part ofa new cohort of LLMs (along with ChatGPT and Google's PaLM for example) thatexhibit more general intelligence than previous AI models. We discuss therising capabilities and implications of these models. We demonstrate that,beyond its mastery of language, GPT-4 can solve novel and difficult tasks thatspan mathematics, coding, vision, medicine, law, psychology and more, withoutneeding any special prompting. Moreover, in all of these tasks, GPT-4'sperformance is strikingly close to human-level performance, and often vastlysurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4'scapabilities, we believe that it could reasonably be viewed as an early (yetstill incomplete) version of an artificial general intelligence (AGI) system.In our exploration of GPT-4, we put special emphasis on discovering itslimitations, and we discuss the challenges ahead for advancing towards deeperand more comprehensive versions of AGI, including the possible need forpursuing a new paradigm that moves beyond next-word prediction. We concludewith reflections on societal influences of the recent technological leap andfuture research directions.", "output": "Sparks of Artificial General Intelligence: Early experiments with GPT-4."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Myocardial infarction and heart failure are major cardiovascular diseasesthat affect millions of people in the US. The morbidity and mortality arehighest among patients who develop cardiogenic shock. Early recognition ofcardiogenic shock is critical. Prompt implementation of treatment measures canprevent the deleterious spiral of ischemia, low blood pressure, and reducedcardiac output due to cardiogenic shock. However, early identification ofcardiogenic shock has been challenging due to human providers' inability toprocess the enormous amount of data in the cardiac intensive care unit (ICU)and lack of an effective risk stratification tool. We developed a deeplearning-based risk stratification tool, called CShock, for patients admittedinto the cardiac ICU with acute decompensated heart failure and/or myocardialinfarction to predict onset of cardiogenic shock. To develop and validateCShock, we annotated cardiac ICU datasets with physician adjudicated outcomes.CShock achieved an area under the receiver operator characteristic curve(AUROC) of 0.820, which substantially outperformed CardShock (AUROC 0.519), awell-established risk score for cardiogenic shock prognosis. CShock wasexternally validated in an independent patient cohort and achieved an AUROC of0.800, demonstrating its generalizability in other cardiac ICUs.", "output": "A dynamic risk score for early prediction of cardiogenic shock using machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although reinforcement learning has seen tremendous success recently, thiskind of trial-and-error learning can be impractical or inefficient in complexenvironments. The use of demonstrations, on the other hand, enables agents tobenefit from expert knowledge rather than having to discover the best action totake through exploration. In this survey, we discuss the advantages of usingdemonstrations in sequential decision making, various ways to applydemonstrations in learning-based decision making paradigms (for example,reinforcement learning and planning in the learned models), and how to collectthe demonstrations in various scenarios. Additionally, we exemplify a practicalpipeline for generating and utilizing demonstrations in the recently proposedManiSkill robot learning benchmark.", "output": "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic 3D content creation has achieved rapid progress recently due to theavailability of pre-trained, large language models and image diffusion models,forming the emerging topic of text-to-3D content creation. Existing text-to-3Dmethods commonly use implicit scene representations, which couple the geometryand appearance via volume rendering and are suboptimal in terms of recoveringfiner geometries and achieving photorealistic rendering; consequently, they areless effective for generating high-quality 3D assets. In this work, we proposea new method of Fantasia3D for high-quality text-to-3D content creation. Key toFantasia3D is the disentangled modeling and learning of geometry andappearance. For geometry learning, we rely on a hybrid scene representation,and propose to encode surface normal extracted from the representation as theinput of the image diffusion model. For appearance modeling, we introduce thespatially varying bidirectional reflectance distribution function (BRDF) intothe text-to-3D task, and learn the surface material for photorealisticrendering of the generated surface. Our disentangled framework is morecompatible with popular graphics engines, supporting relighting, editing, andphysical simulation of the generated 3D assets. We conduct thorough experimentsthat show the advantages of our method over existing ones under differenttext-to-3D task settings. Project page and source codes:", "output": "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantitative characterizations and estimations of uncertainty are offundamental importance in optimization and decision-making processes. Herein,we propose intuitive scores, which we call certainty and doubt, that can beused in both a Bayesian and frequentist framework to assess and compare thequality and uncertainty of predictions in (multi-)classification decisionmachine learning problems.", "output": "Measuring Classification Decision Certainty and Doubt."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many efforts have been made for revealing the decision-making process ofblack-box learning machines such as deep neural networks, resulting in usefullocal and global explanation methods. For local explanation, stochasticity isknown to help: a simple method, called SmoothGrad, has improved the visualquality of gradient-based attribution by adding noise to the input space andaveraging the explanations of the noisy inputs. In this paper, we extend thisidea and propose NoiseGrad that enhances both local and global explanationmethods. Specifically, NoiseGrad introduces stochasticity in the weightparameter space, such that the decision boundary is perturbed. NoiseGrad isexpected to enhance the local explanation, similarly to SmoothGrad, due to thedual relationship between the input perturbation and the decision boundaryperturbation. We evaluate NoiseGrad and its fusion with SmoothGrad --FusionGrad -- qualitatively and quantitatively with several evaluationcriteria, and show that our novel approach significantly outperforms thebaseline methods. Both NoiseGrad and FusionGrad are method-agnostic and ashandy as SmoothGrad using a simple heuristic for the choice of thehyperparameter setting without the need of finetuning.", "output": "NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given the recent impressive accomplishments of language models (LMs) for codegeneration, we explore the use of LMs as adaptive mutation and crossoveroperators for an evolutionary neural architecture search (NAS) algorithm. WhileNAS still proves too difficult a task for LMs to succeed at solely throughprompting, we find that the combination of evolutionary prompt engineering withsoft prompt-tuning, a method we term EvoPrompting, consistently finds diverseand high performing models. We first demonstrate that EvoPrompting is effectiveon the computationally efficient MNIST-1D dataset, where EvoPrompting producesconvolutional architecture variants that outperform both those designed byhuman experts and naive few-shot prompting in terms of accuracy and model size.We then apply our method to searching for graph neural networks on the CLRSAlgorithmic Reasoning Benchmark, where EvoPrompting is able to design novelarchitectures that outperform current state-of-the-art models on 21 out of 30algorithmic reasoning tasks while maintaining similar model size. EvoPromptingis successful at designing accurate and efficient neural network architecturesacross a variety of machine learning tasks, while also being general enough foreasy adaptation to other tasks beyond neural network design.", "output": "EvoPrompting: Language Models for Code-Level Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) emerge to achieve impressive performance on manydata-abundant computer vision tasks by capturing long-range dependencies amonglocal features. However, under few-shot learning (FSL) settings on smalldatasets with only a few labeled data, ViT tends to overfit and suffers fromsevere performance degradation due to its absence of CNN-alike inductive bias.Previous works in FSL avoid such problem either through the help ofself-supervised auxiliary losses, or through the dextile uses of labelinformation under supervised settings. But the gap between self-supervised andsupervised few-shot Transformers is still unfilled. Inspired by recent advancesin self-supervised knowledge distillation and masked image modeling (MIM), wepropose a novel Supervised Masked Knowledge Distillation model (SMKD) forfew-shot Transformers which incorporates label information intoself-distillation frameworks. Compared with previous self-supervised methods,we allow intra-class knowledge distillation on both class and patch tokens, andintroduce the challenging task of masked patch tokens reconstruction acrossintra-class images. Experimental results on four few-shot classificationbenchmark datasets show that our method with simple design outperforms previousmethods by a large margin and achieves a new start-of-the-art. Detailedablation studies confirm the effectiveness of each component of our model. Codefor this paper is available here: ", "output": "Supervised Masked Knowledge Distillation for Few-Shot Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-set action recognition is to reject unknown human action cases which areout of the distribution of the training set. Existing methods mainly focus onlearning better uncertainty scores but dismiss the importance of featurerepresentations. We find that features with richer semantic diversity cansignificantly improve the open-set performance under the same uncertaintyscores. In this paper, we begin with analyzing the feature representationbehavior in the open-set action recognition (OSAR) problem based on theinformation bottleneck (IB) theory, and propose to enlarge theinstance-specific (IS) and class-specific (CS) information contained in thefeature for better performance. To this end, a novel Prototypical SimilarityLearning (PSL) framework is proposed to keep the instance variance within thesame class to retain more IS information. Besides, we notice that unknownsamples sharing similar appearances to known samples are easily misclassifiedas known classes. To alleviate this issue, video shuffling is furtherintroduced in our PSL to learn distinct temporal information between originaland shuffled samples, which we find enlarges the CS information. Extensiveexperiments demonstrate that the proposed PSL can significantly boost both theopen-set and closed-set performance and achieves state-of-the-art results onmultiple benchmarks. Code is available at ", "output": "Enlarging Instance-specific and Class-specific Information for Open-set Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we focus on a novel task of category-level functionalhand-object manipulation synthesis covering both rigid and articulated objectcategories. Given an object geometry, an initial human hand pose as well as asparse control sequence of object poses, our goal is to generate a physicallyreasonable hand-object manipulation sequence that performs like human beings.To address such a challenge, we first design CAnonicalized Manipulation Spaces(CAMS), a two-level space hierarchy that canonicalizes the hand poses in anobject-centric and contact-centric view. Benefiting from the representationcapability of CAMS, we then present a two-stage framework for synthesizinghuman-like manipulation animations. Our framework achieves state-of-the-artperformance for both rigid and articulated categories with impressive visualeffects. Codes and video results can be found at our project homepage:", "output": "CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extracting discriminative local features that are invariant to imagingvariations is an integral part of establishing correspondences between images.In this work, we introduce a self-supervised learning framework to extractdiscriminative rotation-invariant descriptors using group-equivariant CNNs.Thanks to employing group-equivariant CNNs, our method effectively learns toobtain rotation-equivariant features and their orientations explicitly, withouthaving to perform sophisticated data augmentations. The resultant features andtheir orientations are further processed by group aligning, a novel invariantmapping technique that shifts the group-equivariant features by theirorientations along the group dimension. Our group aligning technique achievesrotation-invariance without any collapse of the group dimension and thuseschews loss of discriminability. The proposed method is trained end-to-end ina self-supervised manner, where we use an orientation alignment loss for theorientation estimation and a contrastive descriptor loss for robust localdescriptors to geometric/photometric variations. Our method demonstratesstate-of-the-art matching accuracy among existing rotation-invariantdescriptors under varying rotation and also shows competitive results whentransferred to the task of keypoint matching and camera pose estimation.", "output": "Learning Rotation-Equivariant Features for Visual Correspondence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weight-sharing neural architecture search aims to optimize a configurableneural network model (supernet) for a variety of deployment scenarios acrossmany devices with different resource constraints. Existing approaches useevolutionary search to extract a number of models from a supernet trained on avery large data set, and then fine-tune the extracted models on the typicallysmall, real-world data set of interest. The computational cost of training thusgrows linearly with the number of different model deployment scenarios. Hence,we propose Transfer-Once-For-All (TOFA) for supernet-style training on smalldata sets with constant computational training cost over any number of edgedeployment scenarios. Given a task, TOFA obtains custom neural networks, boththe topology and the weights, optimized for any number of edge deploymentscenarios. To overcome the challenges arising from small data, TOFA utilizes aunified semi-supervised training loss to simultaneously train all subnetswithin the supernet, coupled with on-the-fly architecture selection atdeployment time.", "output": "TOFA: Transfer-Once-for-All."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose binary sparse convolutional networks called BSC-Netfor efficient point cloud analysis. We empirically observe that sparseconvolution operation causes larger quantization errors than standardconvolution. However, conventional network quantization methods directlybinarize the weights and activations in sparse convolution, resulting inperformance drop due to the significant quantization loss. On the contrary, wesearch the optimal subset of convolution operation that activates the sparseconvolution at various locations for quantization error alleviation, and theperformance gap between real-valued and binary sparse convolutional networks isclosed without complexity overhead. Specifically, we first present the shiftedsparse convolution that fuses the information in the receptive field for theactive sites that match the pre-defined positions. Then we employ thedifferentiable search strategies to discover the optimal opsitions for activesite matching in the shifted sparse convolution, and the quantization errorsare significantly alleviated for efficient point cloud analysis. For fairevaluation of the proposed method, we empirically select the recently advancesthat are beneficial for sparse convolution network binarization to construct astrong baseline. The experimental results on Scan-Net and NYU Depth v2 showthat our BSC-Net achieves significant improvement upon our srtong baseline andoutperforms the state-of-the-art network binarization methods by a remarkablemargin without additional computation overhead for binarizing sparseconvolutional networks.", "output": "Binarizing Sparse Convolutional Networks for Efficient Point Cloud Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot class-incremental learning (FSCIL) has recently attracted extensiveattention in various areas. Existing FSCIL methods highly depend on therobustness of the feature backbone pre-trained on base classes. In recentyears, different Transformer variants have obtained significant processes inthe feature representation learning of massive fields. Nevertheless, theprogress of the Transformer in FSCIL scenarios has not achieved the potentialpromised in other fields so far. In this paper, we develop a semantic-visualguided Transformer (SV-T) to enhance the feature extracting capacity of thepre-trained feature backbone on incremental classes. Specifically, we firstutilize the visual (image) labels provided by the base classes to supervise theoptimization of the Transformer. And then, a text encoder is introduced toautomatically generate the corresponding semantic (text) labels for each imagefrom the base classes. Finally, the constructed semantic labels are furtherapplied to the Transformer for guiding its hyperparameters updating. Our SV-Tcan take full advantage of more supervision information from base classes andfurther enhance the training robustness of the feature backbone. Moreimportantly, our SV-T is an independent method, which can directly apply to theexisting FSCIL architectures for acquiring embeddings of various incrementalclasses. Extensive experiments on three benchmarks, two FSCIL architectures,and two Transformer variants show that our proposed SV-T obtains a significantimprovement in comparison to the existing state-of-the-art FSCIL methods.", "output": "Semantic-visual Guided Transformer for Few-shot Class-incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Enhancing practical low light raw images is a difficult task due to severenoise and color distortions from short exposure time and limited illumination.Despite the success of existing Convolutional Neural Network (CNN) basedmethods, their performance is not adaptable to different camera domains. Inaddition, such methods also require large datasets with short-exposure andcorresponding long-exposure ground truth raw images for each camera domain,which is tedious to compile. To address this issue, we present a novel few-shotdomain adaptation method to utilize the existing source camera labeled datawith few labeled samples from the target camera to improve the target domain'senhancement quality in extreme low-light imaging. Our experiments show thatonly ten or fewer labeled samples from the target camera domain are sufficientto achieve similar or better enhancement performance than training a model witha large labeled target camera dataset. To support research in this direction,we also present a new low-light raw image dataset captured with a Nikon camera,comprising short-exposure and their corresponding long-exposure ground truthimages.", "output": "Few-Shot Domain Adaptation for Low Light RAW Image Enhancement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern Generative Adversarial Networks (GANs) generate realistic imagesremarkably well. Previous work has demonstrated the feasibility of\"GAN-classifiers\" that are distinct from the co-trained discriminator, andoperate on images generated from a frozen GAN. That such classifiers work atall affirms the existence of \"knowledge gaps\" (out-of-distribution artifactsacross samples) present in GAN training. We iteratively train GAN-classifiersand train GANs that \"fool\" the classifiers (in an attempt to fill the knowledgegaps), and examine the effect on GAN training dynamics, output quality, andGAN-classifier generalization. We investigate two settings, a small DCGANarchitecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTAGAN architecture trained on high dimensional images (FFHQ). We find that theDCGAN is unable to effectively fool a held-out GAN-classifier withoutcompromising the output quality. However, StyleGAN2 can fool held-outclassifiers with no change in output quality, and this effect persists overmultiple rounds of GAN/classifier training which appears to reveal an orderingover optima in the generator parameter space. Finally, we study differentclassifier architectures and show that the architecture of the GAN-classifierhas a strong influence on the set of its learned artifacts.", "output": "Sequential training of GANs against GAN-classifiers reveals correlated \"knowledge gaps\" present among independently trained GAN instances."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present OmniAvatar, a novel geometry-guided 3D head synthesis modeltrained from in-the-wild unstructured images that is capable of synthesizingdiverse identity-preserved 3D heads with compelling dynamic details under fulldisentangled control over camera poses, facial expressions, head shapes,articulated neck and jaw poses. To achieve such high level of disentangledcontrol, we first explicitly define a novel semantic signed distance function(SDF) around a head geometry (FLAME) conditioned on the control parameters.This semantic SDF allows us to build a differentiable volumetric correspondencemap from the observation space to a disentangled canonical space from all thecontrol parameters. We then leverage the 3D-aware GAN framework (EG3D) tosynthesize detailed shape and appearance of 3D full heads in the canonicalspace, followed by a volume rendering step guided by the volumetriccorrespondence map to output into the observation space. To ensure the controlaccuracy on the synthesized head shapes and expressions, we introduce ageometry prior loss to conform to head SDF and a control loss to conform to theexpression code. Further, we enhance the temporal realism with dynamic detailsconditioned upon varying expressions and joint poses. Our model can synthesizemore preferable identity-preserved 3D heads with compelling dynamic detailscompared to the state-of-the-art methods both qualitatively and quantitatively.We also provide an ablation study to justify many of our system design choices.", "output": "OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The synergy of long-range dependencies from transformers and localrepresentations of image content from convolutional neural networks (CNNs) hasled to advanced architectures and increased performance for various medicalimage analysis tasks due to their complementary benefits. However, comparedwith CNNs, transformers require considerably more training data, due to alarger number of parameters and an absence of inductive bias. The need forincreasingly large datasets continues to be problematic, particularly in thecontext of medical imaging, where both annotation efforts and data protectionresult in limited data availability. In this work, inspired by the humandecision-making process of correlating new ``evidence'' with previouslymemorized ``experience'', we propose a Memorizing Vision Transformer (MoViT) toalleviate the need for large-scale datasets to successfully train and deploytransformer-based architectures. MoViT leverages an external memory structureto cache history attention snapshots during the training stage. To preventoverfitting, we incorporate an innovative memory update scheme, attentiontemporal moving average, to update the stored external memories with thehistorical moving average. For inference speedup, we design a prototypicalattention learning method to distill the external memory into smallerrepresentative subsets. We evaluate our method on a public histology imagedataset and an in-house MRI dataset, demonstrating that MoViT applied to variedmedical image analysis tasks, can outperform vanilla transformer models acrossvaried data regimes, especially in cases where only a small amount of annotateddata is available. More importantly, MoViT can reach a competitive performanceof ViT with only 3.0% of the training data.", "output": "MoViT: Memorizing Vision Transformers for Medical Image Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object discovery -- separating objects from the background without manuallabels -- is a fundamental open challenge in computer vision. Previous methodsstruggle to go beyond clustering of low-level cues, whether handcrafted (e.g.,color, texture) or learned (e.g., from auto-encoders). In this work, we augmentthe auto-encoder representation learning framework with two key components:motion-guidance and mid-level feature tokenization. Although both have beenseparately investigated, we introduce a new transformer decoder showing thattheir benefits can compound thanks to motion-guided vector quantization. Weshow that our architecture effectively leverages the synergy between motion andtokenization, improving upon the state of the art on both synthetic and realdatasets. Our approach enables the emergence of interpretable object-specificmid-level features, demonstrating the benefits of motion-guidance (no labeling)and quantization (interpretability, memory efficiency).", "output": "Object Discovery from Motion-Guided Tokens."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are vulnerable to backdoor attacks, where an adversarymaliciously manipulates the model behavior through overlaying images withspecial triggers. Existing backdoor defense methods often require accessing afew validation data and model parameters, which are impractical in manyreal-world applications, e.g., when the model is provided as a cloud service.In this paper, we address the practical task of blind backdoor defense at testtime, in particular for black-box models. The true label of every test imageneeds to be recovered on the fly from the hard label predictions of asuspicious model. The heuristic trigger search in image space, however, is notscalable to complex triggers or high image resolution. We circumvent suchbarrier by leveraging generic image generation models, and propose a frameworkof Blind Defense with Masked AutoEncoder (BDMAE). It uses the image structuralsimilarity and label consistency between the test image and MAE restorations todetect possible triggers. The detection result is refined by considering thetopology of triggers. We obtain a purified test image from restorations formaking prediction. Our approach is blind to the model architectures, triggerpatterns or image benignity. Extensive experiments on multiple datasets withdifferent backdoor attacks validate its effectiveness and generalizability.Code is available at ", "output": "Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the last three years, the world has been facing a global crisis caused byCovid-19 pandemic. Medical imaging has been playing a crucial role in thefighting against this disease and saving the human lives. Indeed, CT-scans hasproved their efficiency in diagnosing, detecting, and following-up the Covid-19infection. In this paper, we propose a new Transformer-CNN based approach forCovid-19 infection segmentation from the CT slices. The proposed D-TrAttUnetarchitecture has an Encoder-Decoder structure, where compound Transformer-CNNencoder and Dual-Decoders are proposed. The Transformer-CNN encoder is builtusing Transformer layers, UpResBlocks, ResBlocks and max-pooling layers. TheDual-Decoder consists of two identical CNN decoders with attention gates. Thetwo decoders are used to segment the infection and the lung regionssimultaneously and the losses of the two tasks are joined. The proposedD-TrAttUnet architecture is evaluated for both Binary and Multi-classesCovid-19 infection segmentation. The experimental results prove the efficiencyof the proposed approach to deal with the complexity of Covid-19 segmentationtask from limited data. Furthermore, D-TrAttUnet architecture outperforms threebaseline CNN segmentation architectures (Unet, AttUnet and Unet++) and threestate-of-the-art architectures (AnamNet, SCOATNet and CopleNet), in both Binaryand Mutli-classes segmentation tasks.", "output": "D-TrAttUnet: Dual-Decoder Transformer-Based Attention Unet Architecture for Binary and Multi-classes Covid-19 Infection Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prompt learning is an efficient approach to adapt transformers by insertinglearnable set of parameters into the input and intermediate representations ofa pre-trained model. In this work, we present Expressive Prompts with Residuals(EXPRES) which modifies the prompt learning paradigm specifically for effectiveadaptation of vision transformers (ViT). Out method constructs downstreamrepresentations via learnable ``output'' tokens, that are akin to the learnedclass tokens of the ViT. Further for better steering of the downstreamrepresentation processed by the frozen transformer, we introduce residuallearnable tokens that are added to the output of various computations. We applyEXPRES for image classification, few shot learning, and semantic segmentation,and show our method is capable of achieving state of the art prompt tuning on3/3 categories of the VTAB benchmark. In addition to strong performance, weobserve that our approach is an order of magnitude more prompt efficient thanexisting visual prompting baselines. We analytically show the computationalbenefits of our approach over weight space adaptation techniques likefinetuning. Lastly we systematically corroborate the architectural design ofour method via a series of ablation experiments.", "output": "Learning Expressive Prompting With Residuals for Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We explore a new task for audio-visual-language modeling called fine-grainedaudible video description (FAVD). It aims to provide detailed textualdescriptions for the given audible videos, including the appearance and spatiallocations of each object, the actions of moving objects, and the sounds invideos. Existing visual-language modeling tasks often concentrate on visualcues in videos while undervaluing the language and audio modalities. On theother hand, FAVD requires not only audio-visual-language modeling skills butalso paragraph-level language generation abilities. We construct the firstfine-grained audible video description benchmark (FAVDBench) to facilitate thisresearch. For each video clip, we first provide a one-sentence summary of thevideo, ie, the caption, followed by 4-6 sentences describing the visual detailsand 1-2 audio-related descriptions at the end. The descriptions are provided inboth English and Chinese. We create two new metrics for this task: anEntityScore to gauge the completeness of entities in the visual descriptions,and an AudioScore to assess the audio descriptions. As a preliminary approachto this task, we propose an audio-visual-language transformer that extendsexisting video captioning model with an additional audio branch. We combine themasked language modeling and auto-regressive language modeling losses tooptimize our model so that it can produce paragraph-level descriptions. Weillustrate the efficiency of our model in audio-visual-language modeling byevaluating it against the proposed benchmark using both conventional captioningmetrics and our proposed metrics. We further put our benchmark to the test invideo generation models, demonstrating that employing fine-grained videodescriptions can create more intricate videos than using captions.", "output": "Fine-grained Audible Video Description."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous navigation in unstructured off-road environments is greatlyimproved by semantic scene understanding. Conventional image processingalgorithms are difficult to implement and lack robustness due to a lack ofstructure and high variability across off-road environments. The use of neuralnetworks and machine learning can overcome the previous challenges but theyrequire large labeled data sets for training. In our work we propose the use ofhyperspectral images for real-time pixel-wise semantic classification andsegmentation, without the need of any prior training data. The resultingsegmented image is processed to extract, filter, and approximate objects aspolygons, using a polygon approximation algorithm. The resulting polygons arethen used to generate a semantic map of the environment. Using our framework.we show the capability to add new semantic classes in run-time forclassification. The proposed methodology is also shown to operate in real-timeand produce outputs at a frequency of 1Hz, using high resolution hyperspectralimages.", "output": "Real-Time Semantic Segmentation using Hyperspectral Images for Mapping Unstructured and Unknown Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Concept-based explanations for convolutional neural networks (CNNs) aim toexplain model behavior and outputs using a pre-defined set of semantic concepts(e.g., the model recognizes scene class ``bedroom'' based on the presence ofconcepts ``bed'' and ``pillow''). However, they often do not faithfully (i.e.,accurately) characterize the model's behavior and can be too complex for peopleto understand. Further, little is known about how faithful and understandabledifferent explanation methods are, and how to control these two properties. Inthis work, we propose UFO, a unified method for controlling Understandabilityand Faithfulness Objectives in concept-based explanations. UFO formalizesunderstandability and faithfulness as mathematical objectives and unifies mostexisting concept-based explanations methods for CNNs. Using UFO, wesystematically investigate how explanations change as we turn the knobs offaithfulness and understandability. Our experiments demonstrate afaithfulness-vs-understandability tradeoff: increasing understandabilityreduces faithfulness. We also provide insights into the ``disagreementproblem'' in explainable machine learning, by analyzing when and howconcept-based explanations disagree with each other.", "output": "UFO: A unified method for controlling Understandability and Faithfulness Objectives in concept-based explanations for CNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A significant research effort is focused on exploiting the amazing capacitiesof pretrained diffusion models for the editing of images. They either finetunethe model, or invert the image in the latent space of the pretrained model.However, they suffer from two problems: (1) Unsatisfying results for selectedregions, and unexpected changes in nonselected regions. (2) They requirecareful text prompt editing where the prompt should include all visual objectsin the input image. To address this, we propose two improvements: (1) Onlyoptimizing the input of the value linear network in the cross-attention layers,is sufficiently powerful to reconstruct a real image. (2) We propose attentionregularization to preserve the object-like attention maps after editing,enabling us to obtain accurate style editing without invoking significantstructural changes. We further improve the editing technique which is used forthe unconditional branch of classifier-free guidance, as well as theconditional one as used by P2P. Extensive experimental prompt-editing resultson a variety of images, demonstrate qualitatively and quantitatively that ourmethod has superior editing capabilities than existing and concurrent works.", "output": "StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we develop rotation-equivariant neural networks for 4Dpanoptic segmentation. 4D panoptic segmentation is a recently establishedbenchmark task for autonomous driving, which requires recognizing semanticclasses and object instances on the road based on LiDAR scans, as well asassigning temporally consistent IDs to instances across time. We observe thatthe driving scenario is symmetric to rotations on the ground plane. Therefore,rotation-equivariance could provide better generalization and more robustfeature learning. Specifically, we review the object instance clusteringstrategies, and restate the centerness-based approach and the offset-basedapproach as the prediction of invariant scalar fields and equivariant vectorfields. Other sub-tasks are also unified from this perspective, and differentinvariant and equivariant layers are designed to facilitate their predictions.Through evaluation on the standard 4D panoptic segmentation benchmark ofSemanticKITTI, we show that our equivariant models achieve higher accuracy withlower computational costs compared to their non-equivariant counterparts.Moreover, our method sets the new state-of-the-art performance and achieves 1stplace on the SemanticKITTI 4D Panoptic Segmentation leaderboard.", "output": "4D Panoptic Segmentation as Invariant and Equivariant Field Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D point cloud semantic segmentation aims to group all points into differentsemantic categories, which benefits important applications such as point cloudscene reconstruction and understanding. Existing supervised point cloudsemantic segmentation methods usually require large-scale annotated pointclouds for training and cannot handle new categories. While a few-shot learningmethod was proposed recently to address these two problems, it suffers fromhigh computational complexity caused by graph construction and inability tolearn fine-grained relationships among points due to the use of poolingoperations. In this paper, we further address these problems by developing anew multi-layer transformer network for few-shot point cloud semanticsegmentation. In the proposed network, the query point cloud features areaggregated based on the class-specific support features in different scales.Without using pooling operations, our method makes full use of all pixel-levelfeatures from the support samples. By better leveraging the support featuresfor few-shot learning, the proposed method achieves the new state-of-the-artperformance, with 15% less inference time, over existing few-shot 3D pointcloud segmentation models on the S3DIS dataset and the ScanNet dataset.", "output": "Few-Shot 3D Point Cloud Semantic Segmentation via Stratified Class-Specific Attention Based Transformer Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Colonoscopic video retrieval, which is a critical part of polyp treatment,has great clinical significance for the prevention and treatment of colorectalcancer. However, retrieval models trained on action recognition datasetsusually produce unsatisfactory retrieval results on colonoscopic datasets dueto the large domain gap between them. To seek a solution to this problem, weconstruct a large-scale colonoscopic dataset named Colo-Pair for medicalpractice. Based on this dataset, a simple yet effective training method calledColo-SCRL is proposed for more robust representation learning. It aims torefine general knowledge from colonoscopies through masked autoencoder-basedreconstruction and momentum contrast to improve retrieval performance. To thebest of our knowledge, this is the first attempt to employ the contrastivelearning paradigm for medical video retrieval. Empirical results show that ourmethod significantly outperforms current state-of-the-art methods in thecolonoscopic video retrieval task.", "output": "Colo-SCRL: Self-Supervised Contrastive Representation Learning for Colonoscopic Video Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Precise estimation of global orientation and location is critical to ensure acompelling outdoor Augmented Reality (AR) experience. We address the problem ofgeo-pose estimation by cross-view matching of query ground images to ageo-referenced aerial satellite image database. Recently, neural network-basedmethods have shown state-of-the-art performance in cross-view matching.However, most of the prior works focus only on location estimation, ignoringorientation, which cannot meet the requirements in outdoor AR applications. Wepropose a new transformer neural network-based model and a modified tripletranking loss for joint location and orientation estimation. Experiments onseveral benchmark cross-view geo-localization datasets show that our modelachieves state-of-the-art performance. Furthermore, we present an approach toextend the single image query-based geo-localization approach by utilizingtemporal information from a navigation pipeline for robust continuousgeo-localization. Experimentation on several large-scale real-world videosequences demonstrates that our approach enables high-precision and stable ARinsertion.", "output": "Cross-View Visual Geo-Localization for Outdoor Augmented Reality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation (KD) is an effective training strategy to improve thelightweight student models under the guidance of cumbersome teachers. However,the large architecture difference across the teacher-student pairs limits thedistillation gains. In contrast to previous adaptive distillation methods toreduce the teacher-student gap, we explore a novel training-free framework tosearch for the best student architectures for a given teacher. Our work firstempirically show that the optimal model under vanilla training cannot be thewinner in distillation. Secondly, we find that the similarity of featuresemantics and sample relations between random-initialized teacher-studentnetworks have good correlations with final distillation performances. Thus, weefficiently measure similarity matrixs conditioned on the semantic activationmaps to select the optimal student via an evolutionary algorithm without anytraining. In this way, our student architecture search for Distillation WithOutTraining (DisWOT) significantly improves the performance of the model in thedistillation stage with at least 180$times$ training acceleration.Additionally, we extend similarity metrics in DisWOT as new distillers andKD-based zero-proxies. Our experiments on CIFAR, ImageNet and NAS-Bench-201demonstrate that our technique achieves state-of-the-art results on differentsearch spaces. Our project and code are available at", "output": "DisWOT: Student Architecture Search for Distillation WithOut Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The success of existing multi-view clustering relies on the assumption ofsample integrity across multiple views. However, in real-world scenarios,samples of multi-view are partially available due to data corruption or sensorfailure, which leads to incomplete multi-view clustering study (IMVC). Althoughseveral attempts have been proposed to address IMVC, they suffer from thefollowing drawbacks: i) Existing methods mainly adopt cross-view contrastivelearning forcing the representations of each sample across views to be exactlythe same, which might ignore view discrepancy and flexibility inrepresentations; ii) Due to the absence of non-observed samples across multipleviews, the obtained prototypes of clusters might be unaligned and biased,leading to incorrect fusion. To address the above issues, we propose aCross-view Partial Sample and Prototype Alignment Network (CPSPAN) for DeepIncomplete Multi-view Clustering. Firstly, unlike existing contrastive-basedmethods, we adopt pair-observed data alignment as 'proxy supervised signals' toguide instance-to-instance correspondence construction among views. Then,regarding of the shifted prototypes in IMVC, we further propose a prototypealignment module to achieve incomplete distribution calibration across views.Extensive experimental results showcase the effectiveness of our proposedmodules, attaining noteworthy performance improvements when compared toexisting IMVC competitors on benchmark datasets.", "output": "Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and Prototype Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pretraining a deep learning model on large image datasets is a standard stepbefore fine-tuning the model on small targeted datasets. The large dataset isusually general images (e.g. imagenet2012) while the small dataset can bespecialized datasets that have different distributions from the large dataset.However, this 'large-to-small' strategy is not well-validated when the largedataset is specialized and has a similar distribution to small datasets. Wenewly compiled three hematoxylin and eosin-stained image datasets, one large(PTCGA200) and two magnification-adjusted small datasets (PCam200 andsegPANDA200). Major deep learning models were trained with supervised andself-supervised learning methods and fine-tuned on the small datasets for tumorclassification and tissue segmentation benchmarks. ResNet50 pretrained withMoCov2, SimCLR, and BYOL on PTCGA200 was better than imagenet2012 pretrainingwhen fine-tuned on PTCGA200 (accuracy of 83.94%, 86.41%, 84.91%, and 82.72%,respectively). ResNet50 pre-trained on PTCGA200 with MoCov2 exceeded theCOCOtrain2017-pretrained baseline and was the best in ResNet50 for the tissuesegmentation benchmark (mIoU of 63.53% and 63.22%). We found re-trainingimagenet-pretrained models (ResNet50, BiT-M-R50x1, and ViT-S/16) on PTCGA200improved downstream benchmarks.", "output": "Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Standard deep learning models such as convolutional neural networks (CNNs)lack the ability of generalizing to domains which have not been seen duringtraining. This problem is mainly due to the common but often wrong assumptionof such models that the source and target data come from the same i.i.d.distribution. Recently, Vision Transformers (ViTs) have shown outstandingperformance for a broad range of computer vision tasks. However, very fewstudies have investigated their ability to generalize to new domains. Thispaper presents a first Token-level Feature Stylization (TFS-ViT) approach fordomain generalization, which improves the performance of ViTs to unseen data bysynthesizing new domains. Our approach transforms token features by mixing thenormalization statistics of images from different domains. We further improvethis approach with a novel strategy for attention-aware stylization, which usesthe attention maps of class (CLS) tokens to compute and mix normalizationstatistics of tokens corresponding to different image regions. The proposedmethod is flexible to the choice of backbone model and can be easily applied toany ViT-based architecture with a negligible increase in computationalcomplexity. Comprehensive experiments show that our approach is able to achievestate-of-the-art performance on five challenging benchmarks for domaingeneralization, and demonstrate its ability to deal with different types ofdomain shifts. The implementation is available at:{", "output": "TFS-ViT: Token-Level Feature Stylization for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, deep learning models have shown the potential to predict breastcancer risk and enable targeted screening strategies, but current models do notconsider the change in the breast over time. In this paper, we present a newmethod, PRIME+, for breast cancer risk prediction that leverages priormammograms using a transformer decoder, outperforming a state-of-the-art riskprediction method that only uses mammograms from a single time point. Wevalidate our approach on a dataset with 16,113 exams and further demonstratethat it effectively captures patterns of changes from prior mammograms, such aschanges in breast density, resulting in improved short-term and long-termbreast cancer risk prediction. Experimental results show that our modelachieves a statistically significant improvement in performance over thestate-of-the-art based model, with a C-index increase from 0.68 to 0.73 (p &lt;0.05) on held-out test sets.", "output": "Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, RGB-Thermal based perception has shown significant advances.Thermal information provides useful clues when visual cameras suffer from poorlighting conditions, such as low light and fog. However, how to effectivelyfuse RGB images and thermal data remains an open challenge. Previous worksinvolve naive fusion strategies such as merging them at the input,concatenating multi-modality features inside models, or applying attention toeach data modality. These fusion strategies are straightforward yetinsufficient. In this paper, we propose a novel fusion method named ExplicitAttention-Enhanced Fusion (EAEF) that fully takes advantage of each type ofdata. Specifically, we consider the following cases: i) both RGB data andthermal data, ii) only one of the types of data, and iii) none of them generatediscriminative features. EAEF uses one branch to enhance feature extraction fori) and iii) and the other branch to remedy insufficient representations forii). The outputs of two branches are fused to form complementary features. As aresult, the proposed fusion method outperforms state-of-the-art by 1.6% inmIoU on semantic segmentation, 3.1% in MAE on salient object detection, 2.3%in mAP on object detection, and 8.1% in MAE on crowd counting. The code isavailable at ", "output": "Explicit Attention-Enhanced Fusion for RGB-Thermal Perception Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing methods proposed for hand reconstruction tasks usually parameterizea generic 3D hand model or predict hand mesh positions directly. The parametricrepresentations consisting of hand shapes and rotational poses are more stable,while the non-parametric methods can predict more accurate mesh positions. Inthis paper, we propose to reconstruct meshes and estimate MANO parameters oftwo hands from a single RGB image simultaneously to utilize the merits of twokinds of hand representations. To fulfill this target, we propose novelMesh-Mano interaction blocks (MMIBs), which take mesh vertices positions andMANO parameters as two kinds of query tokens. MMIB consists of one graphresidual block to aggregate local information and two transformer encoders tomodel long-range dependencies. The transformer encoders are equipped withdifferent asymmetric attention masks to model the intra-hand and inter-handattention, respectively. Moreover, we introduce the mesh alignment refinementmodule to further enhance the mesh-image alignment. Extensive experiments onthe InterHand2.6M benchmark demonstrate promising results over thestate-of-the-art hand reconstruction methods.", "output": "MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce SDM-UniPS, a groundbreaking Scalable, Detailed,Mask-free, and Universal Photometric Stereo network. Our approach can recoverastonishingly intricate surface normal maps, rivaling the quality of 3Dscanners, even when images are captured under unknown, spatially-varyinglighting conditions in uncontrolled environments. We have extended previousuniversal photometric stereo networks to extract spatial-light features,utilizing all available information in high-resolution input images andaccounting for non-local interactions among surface points. Moreover, wepresent a new synthetic training dataset that encompasses a diverse range ofshapes, materials, and illumination scenarios found in real-world scenes.Through extensive evaluation, we demonstrate that our method not only surpassescalibrated, lighting-specific techniques on public benchmarks, but also excelswith a significantly smaller number of input images even without object masks.", "output": "Scalable, Detailed and Mask-Free Universal Photometric Stereo."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks have achieved unprecedented success on diverse visiontasks. However, they are vulnerable to adversarial noise that is imperceptibleto humans. This phenomenon negatively affects their deployment in real-worldscenarios, especially security-related ones. To evaluate the robustness of atarget model in practice, transfer-based attacks craft adversarial samples witha local model and have attracted increasing attention from researchers due totheir high efficiency. The state-of-the-art transfer-based attacks aregenerally based on data augmentation, which typically augments multipletraining images from a linear path when learning adversarial samples. However,such methods selected the image augmentation path heuristically and may augmentimages that are semantics-inconsistent with the target images, which harms thetransferability of the generated adversarial samples. To overcome the pitfall,we propose the Path-Augmented Method (PAM). Specifically, PAM first constructsa candidate augmentation path pool. It then settles the employed augmentationpaths during adversarial sample generation with greedy search. Furthermore, toavoid augmenting semantics-inconsistent images, we train a Semantics Predictor(SP) to constrain the length of the augmentation path. Extensive experimentsconfirm that PAM can achieve an improvement of over 4.8% on average comparedwith the state-of-the-art baselines in terms of the attack success rates.", "output": "Improving the Transferability of Adversarial Samples by Path-Augmented Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene text detection is a challenging computer vision task due to the highvariation in text shapes and ratios. In this work, we propose a scene textdetector named Deformable Kernel Expansion (DKE), which incorporates the meritsof both segmentation and contour-based detectors. DKE employs a segmentationmodule to segment the shrunken text region as the text kernel, then expands thetext kernel contour to obtain text boundary by regressing the vertex-wiseoffsets. Generating the text kernel by segmentation enables DKE to inherit thearbitrary-shaped text region modeling capability of segmentation-baseddetectors. Regressing the kernel contour with some sampled vertices enables DKEto avoid the complicated pixel-level post-processing and better learn contourdeformation as the contour-based detectors. Moreover, we propose an OptimalBipartite Graph Matching Loss (OBGML) that measures the matching error betweenthe predicted contour and the ground truth, which efficiently minimizes theglobal contour matching distance. Extensive experiments on CTW1500, Total-Text,MSRA-TD500, and ICDAR2015 demonstrate that DKE achieves a good tradeoff betweenaccuracy and efficiency in scene text detection.", "output": "Deformable Kernel Expansion Model for Efficient Arbitrary-shaped Scene Text Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have witnessed great progress in deep neural networks forreal-time applications. However, most existing works do not explicitly considerthe general case where the device's state and the available resources fluctuateover time, and none of them investigate or address the impact of varyingcomputational resources for online video understanding tasks. This paperproposes a System-status-aware Adaptive Network (SAN) that considers thedevice's real-time state to provide high-quality predictions with low delay.Usage of our agent's policy improves efficiency and robustness to fluctuationsof the system status. On two widely used video understanding tasks, SAN obtainsstate-of-the-art performance while constantly keeping processing delays low.Moreover, training such an agent on various types of hardware configurations isnot easy as the labeled training data might not be available, or can becomputationally prohibitive. To address this challenging problem, we propose aMeta Self-supervised Adaptation (MSA) method that adapts the agent's policy tonew hardware configurations at test-time, allowing for easy deployment of themodel onto other unseen hardware platforms.", "output": "System-status-aware Adaptive Network for Online Streaming Video Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we focus on the problem of category-level object poseestimation, which is challenging due to the large intra-category shapevariation. 3D graph convolution (3D-GC) based methods have been widely used toextract local geometric features, but they have limitations for complex shapedobjects and are sensitive to noise. Moreover, the scale and translationinvariant properties of 3D-GC restrict the perception of an object's size andtranslation information. In this paper, we propose a simple network structure,the HS-layer, which extends 3D-GC to extract hybrid scope latent features frompoint cloud data for category-level object pose estimation tasks. The proposedHS-layer: 1) is able to perceive local-global geometric structure and globalinformation, 2) is robust to noise, and 3) can encode size and translationinformation. Our experiments show that the simple replacement of the 3D-GClayer with the proposed HS-layer on the baseline method (GPV-Pose) achieves asignificant improvement, with the performance increased by 14.5% on 5d2cmmetric and 10.3% on IoU75. Our method outperforms the state-of-the-art methodsby a large margin (8.3% on 5d2cm, 6.9% on IoU75) on the REAL275 dataset andruns in real-time (50 FPS).", "output": "HS-Pose: Hybrid Scope Feature Extraction for Category-level Object Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The deep image prior (DIP) is a well-established unsupervised deep learningmethod for image reconstruction; yet it is far from being flawless. The DIPoverfits to noise if not early stopped, or optimized via a regularizedobjective. We build on the regularized fine-tuning of a pretrained DIP, byadopting a novel strategy that restricts the learning to the adaptation ofsingular values. The proposed SVD-DIP uses ad hoc convolutional layers whosepretrained parameters are decomposed via the singular value decomposition.Optimizing the DIP then solely consists in the fine-tuning of the singularvalues, while keeping the left and right singular vectors fixed. We thoroughlyvalidate the proposed method on real-measured $mu$CT data of a lotus root aswell as two medical datasets (LoDoPaB and Mayo). We report significantlyimproved stability of the DIP optimization, by overcoming the overfitting tonoise.", "output": "SVD-DIP: Overcoming the Overfitting Problem in DIP-based CT Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Whole Slide Image (WSI) classification remains a challenge due to theirextremely high resolution and the absence of fine-grained labels. Presently,WSIs are usually classified as a Multiple Instance Learning (MIL) problem whenonly slide-level labels are available. MIL methods involve a patch embeddingprocess and a bag-level classification process, but they are prohibitivelyexpensive to be trained end-to-end. Therefore, existing methods usually trainthem separately, or directly skip the training of the embedder. Such schemeshinder the patch embedder's access to slide-level labels, resulting ininconsistencies within the entire MIL pipeline. To overcome this issue, wepropose a novel framework called Iteratively Coupled MIL (ICMIL), which bridgesthe loss back-propagation process from the bag-level classifier to the patchembedder. In ICMIL, we use category information in the bag-level classifier toguide the patch-level fine-tuning of the patch feature extractor. The refinedembedder then generates better instance representations for achieving a moreaccurate bag-level classifier. By coupling the patch embedder and bagclassifier at a low cost, our proposed framework enables information exchangebetween the two processes, benefiting the entire MIL classification model. Wetested our framework on two datasets using three different backbones, and ourexperimental results demonstrate consistent performance improvements overstate-of-the-art MIL methods. Code will be made available upon acceptance.", "output": "Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision transformers (ViTs) have been successfully deployed in a variety ofcomputer vision tasks, but they are still vulnerable to adversarial samples.Transfer-based attacks use a local model to generate adversarial samples anddirectly transfer them to attack a target black-box model. The high efficiencyof transfer-based attacks makes it a severe security threat to ViT-basedapplications. Therefore, it is vital to design effective transfer-based attacksto identify the deficiencies of ViTs beforehand in security-sensitivescenarios. Existing efforts generally focus on regularizing the input gradientsto stabilize the updated direction of adversarial samples. However, thevariance of the back-propagated gradients in intermediate blocks of ViTs maystill be large, which may make the generated adversarial samples focus on somemodel-specific features and get stuck in poor local optima. To overcome theshortcomings of existing approaches, we propose the Token GradientRegularization (TGR) method. According to the structural characteristics ofViTs, TGR reduces the variance of the back-propagated gradient in each internalblock of ViTs in a token-wise manner and utilizes the regularized gradient togenerate adversarial samples. Extensive experiments on attacking both ViTs andCNNs confirm the superiority of our approach. Notably, compared to thestate-of-the-art transfer-based attacks, our TGR offers a performanceimprovement of 8.8% on average.", "output": "Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Correspondence pruning aims to search consistent correspondences (inliers)from a set of putative correspondences. It is challenging because of thedisorganized spatial distribution of numerous outliers, especially whenputative correspondences are largely dominated by outliers. It's morechallenging to ensure effectiveness while maintaining efficiency. In thispaper, we propose an effective and efficient method for correspondence pruning.Inspired by the success of attentive context in correspondence problems, wefirst extend the attentive context to the first-order attentive context andthen introduce the idea of attention in attention (ANA) to model second-orderattentive context for correspondence pruning. Compared with first-orderattention that focuses on feature-consistent context, second-order attentiondedicates to attention weights itself and provides an additional source toencode consistent context from the attention map. For efficiency, we derive twoapproximate formulations for the naive implementation of second-order attentionto optimize the cubic complexity to linear complexity, such that second-orderattention can be used with negligible computational overheads. We furtherimplement our formulations in a second-order context layer and then incorporatethe layer in an ANA block. Extensive experiments demonstrate that our method iseffective and efficient in pruning outliers, especially in high-outlier-ratiocases. Compared with the state-of-the-art correspondence pruning approachLMCNet, our method runs 14 times faster while maintaining a competitiveaccuracy.", "output": "Learning Second-Order Attentive Context for Efficient Correspondence Pruning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-driven 3D stylization is a complex and crucial task in the fields ofcomputer vision (CV) and computer graphics (CG), aimed at transforming a baremesh to fit a target text. Prior methods adopt text-independent multilayerperceptrons (MLPs) to predict the attributes of the target mesh with thesupervision of CLIP loss. However, such text-independent architecture lackstextual guidance during predicting attributes, thus leading to unsatisfactorystylization and slow convergence. To address these limitations, we presentX-Mesh, an innovative text-driven 3D stylization framework that incorporates anovel Text-guided Dynamic Attention Module (TDAM). The TDAM dynamicallyintegrates the guidance of the target text by utilizing text-relevant spatialand channel-wise attentions during vertex feature extraction, resulting in moreaccurate attribute prediction and faster convergence speed. Furthermore,existing works lack standard benchmarks and automated metrics for evaluation,often relying on subjective and non-reproducible user studies to assess thequality of stylized 3D assets. To overcome this limitation, we introduce a newstandard text-mesh benchmark, namely MIT-30, and two automated metrics, whichwill enable future research to achieve fair and objective comparisons. Ourextensive qualitative and quantitative experiments demonstrate that X-Meshoutperforms previous state-of-the-art methods.", "output": "X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via Dynamic Textual Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face swapping aims at injecting a source image's identity (i.e., facialfeatures) into a target image, while strictly preserving the target'sattributes, which are irrelevant to identity. However, we observed thatprevious approaches still suffer from source attribute leakage, where thesource image's attributes interfere with the target image's. In this paper, weanalyze the latent space of StyleGAN and find the adequate combination of thelatents geared for face swapping task. Based on the findings, we develop asimple yet robust face swapping model, RobustSwap, which is resistant to thepotential source attribute leakage. Moreover, we exploit the coordination of3DMM's implicit and explicit information as a guidance to incorporate thestructure of the source image and the precise pose of the target image. Despiteour method solely utilizing an image dataset without identity labels fortraining, our model has the capability to generate high-fidelity and temporallyconsistent videos. Through extensive qualitative and quantitative evaluations,we demonstrate that our method shows significant improvements compared with theprevious face swapping models in synthesizing both images and videos. Projectpage is available at ", "output": "RobustSwap: A Simple yet Robust Face Swapping Model against Attribute Leakage."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reducing the radiation dose in computed tomography (CT) is important tomitigate radiation-induced risks. One option is to employ a well-trained modelto compensate for incomplete information and map sparse-view measurements tothe CT reconstruction. However, reconstruction from sparsely sampledmeasurements is insufficient to uniquely characterize an object in CT, and alearned prior model may be inadequate for unencountered cases. Medical modaltranslation from magnetic resonance imaging (MRI) to CT is an alternative butmay introduce incorrect information into the synthesized CT images in additionto the fact that there exists no explicit transformation describing theirrelationship. To address these issues, we propose a novel framework called thedenoising diffusion model for medical image synthesis (DDMM-Synth) to close theperformance gaps described above. This framework combines an MRI-guideddiffusion model with a new CT measurement embedding reverse sampling scheme.Specifically, the null-space content of the one-step denoising result isrefined by the MRI-guided data distribution prior, and its range-spacecomponent derived from an explicit operator matrix and the sparse-view CTmeasurements is directly integrated into the inference stage. DDMM-Synth canadjust the projection number of CT a posteriori for a particular clinicalapplication and its modified version can even improve the results significantlyfor noisy cases. Our results show that DDMM-Synth outperforms otherstate-of-the-art supervised-learning-based baselines under fair experimentalconditions.", "output": "DDMM-Synth: A Denoising Diffusion Model for Cross-modal Medical Image Synthesis with Sparse-view Measurement Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a high-quality 3D-to-3D conversion method, Instruct 3D-to-3D. Ourmethod is designed for a novel task, which is to convert a given 3D scene toanother scene according to text instructions. Instruct 3D-to-3D appliespretrained Image-to-Image diffusion models for 3D-to-3D conversion. Thisenables the likelihood maximization of each viewpoint image and high-quality 3Dgeneration. In addition, our proposed method explicitly inputs the source 3Dscene as a condition, which enhances 3D consistency and controllability of howmuch of the source 3D scene structure is reflected. We also propose dynamicscaling, which allows the intensity of the geometry transformation to beadjusted. We performed quantitative and qualitative evaluations and showed thatour proposed method achieves higher quality 3D-to-3D conversions than baselinemethods.", "output": "Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present CARTO, a novel approach for reconstructing multiple articulatedobjects from a single stereo RGB observation. We use implicit object-centricrepresentations and learn a single geometry and articulation decoder formultiple object categories. Despite training on multiple categories, ourdecoder achieves a comparable reconstruction accuracy to methods that trainbespoke decoders separately for each category. Combined with our stereo imageencoder we infer the 3D shape, 6D pose, size, joint type, and the joint stateof multiple unknown objects in a single forward pass. Our method achieves a20.4% absolute improvement in mAP 3D IOU50 for novel instances when compared toa two-stage pipeline. Inference time is fast and can run on a NVIDIA TITAN XPGPU at 1 HZ for eight or less objects present. While only trained on simulateddata, CARTO transfers to real-world object instances. Code and evaluation datais available at: <a href=\" http URL</a>", "output": "CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human-Object Interaction (HOI) detection aims to localize human-object pairsand recognize their interactions. Recently, Contrastive Language-ImagePre-training (CLIP) has shown great potential in providing interaction priorfor HOI detectors via knowledge distillation. However, such approaches oftenrely on large-scale training data and suffer from inferior performance underfew/zero-shot scenarios. In this paper, we propose a novel HOI detectionframework that efficiently extracts prior knowledge from CLIP and achievesbetter generalization. In detail, we first introduce a novel interactiondecoder to extract informative regions in the visual feature map of CLIP via across-attention mechanism, which is then fused with the detection backbone by aknowledge integration block for more accurate human-object pair detection. Inaddition, prior knowledge in CLIP text encoder is leveraged to generate aclassifier by embedding HOI descriptions. To distinguish fine-grainedinteractions, we build a verb classifier from training data via visual semanticarithmetic and a lightweight verb representation adapter. Furthermore, wepropose a training-free enhancement to exploit global HOI predictions fromCLIP. Extensive experiments demonstrate that our method outperforms the stateof the art by a large margin on various settings, e.g. +4.04 mAP on HICO-Det.The source code is available in ", "output": "HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image demosaicing is an important step in the image processing pipeline fordigital cameras, and it is one of the many tasks within the field of imagerestoration. A well-known characteristic of natural images is that most patchesare smooth, while high-content patches like textures or repetitive patterns aremuch rarer, which results in a long-tailed distribution. This distribution cancreate an inductive bias when training machine learning algorithms for imagerestoration tasks and for image demosaicing in particular. There have been manydifferent approaches to address this challenge, such as utilizing specificlosses or designing special network architectures. What makes our work isunique in that it tackles the problem from a training protocol perspective. Ourproposed training regime consists of two key steps. The first step is adata-mining stage where sub-categories are created and then refined through anelimination process to only retain the most helpful sub-categories. The secondstep is a cyclic training process where the neural network is trained on boththe mined sub-categories and the original dataset. We have conducted variousexperiments to demonstrate the effectiveness of our training method for theimage demosaicing task. Our results show that this method outperforms standardtraining across a range of architecture sizes and types, including CNNs andTransformers. Moreover, we are able to achieve state-of-the-art results with asignificantly smaller neural network, compared to previous state-of-the-artmethods.", "output": "Make the Most Out of Your Net: Alternating Between Canonical and Hard Datasets for Improved Image Demosaicing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-and-language navigation (VLN) is the task to enable an embodied agentto navigate to a remote location following the natural language instruction inreal scenes. Most of the previous approaches utilize the entire features orobject-centric features to represent navigable candidates. However, theserepresentations are not efficient enough for an agent to perform actions toarrive the target location. As knowledge provides crucial information which iscomplementary to visible content, in this paper, we propose a KnowledgeEnhanced Reasoning Model (KERM) to leverage knowledge to improve agentnavigation ability. Specifically, we first retrieve facts (i.e., knowledgedescribed by language descriptions) for the navigation views based on localregions from the constructed knowledge base. The retrieved facts range fromproperties of a single object (e.g., color, shape) to relationships betweenobjects (e.g., action, spatial position), providing crucial information forVLN. We further present the KERM which contains the purification, fact-awareinteraction, and instruction-guided aggregation modules to integrate visual,history, instruction, and fact features. The proposed KERM can automaticallyselect and gather crucial and relevant cues, obtaining more accurate actionprediction. Experimental results on the REVERIE, R2R, and SOON datasetsdemonstrate the effectiveness of the proposed method.", "output": "KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper investigates an open research task of reconstructing andgenerating 3D point clouds. Most existing works of 3D generative modelsdirectly take the Gaussian prior as input for the decoder to generate 3D pointclouds, which fail to learn disentangled latent codes, leading noisyinterpolated results. Most of the GAN-based models fail to discriminate thelocal geometries, resulting in the point clouds generated not evenlydistributed at the object surface, hence degrading the point cloud generationquality. Moreover, prevailing methods adopt computation-intensive frameworks,such as flow-based models and Markov chains, which take plenty of time andresources in the training phase. To resolve these limitations, this paperproposes a unified style-aware network architecture combining both point-wisedistance loss and adversarial loss, StarNet which is able to reconstruct andgenerate high-fidelity and even 3D point clouds using a mapping network thatcan effectively disentangle the Gaussian prior from input's high-levelattributes in the mapped latent space to generate realistic interpolatedobjects. Experimental results demonstrate that our framework achievescomparable state-of-the-art performance on various metrics in the point cloudreconstruction and generation tasks, but is more lightweight in model size,requires much fewer parameters and less time for model training.", "output": "StarNet: Style-Aware 3D Point Cloud Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face recognition is a prevailing authentication solution in numerousbiometric applications. Physical adversarial attacks, as an importantsurrogate, can identify the weaknesses of face recognition systems and evaluatetheir robustness before deployed. However, most existing physical attacks areeither detectable readily or ineffective against commercial recognitionsystems. The goal of this work is to develop a more reliable technique that cancarry out an end-to-end evaluation of adversarial robustness for commercialsystems. It requires that this technique can simultaneously deceive black-boxrecognition models and evade defensive mechanisms. To fulfill this, we designadversarial textured 3D meshes (AT3D) with an elaborate topology on a humanface, which can be 3D-printed and pasted on the attacker's face to evade thedefenses. However, the mesh-based optimization regime calculates gradients inhigh-dimensional mesh space, and can be trapped into local optima withunsatisfactory transferability. To deviate from the mesh-based space, wepropose to perturb the low-dimensional coefficient space based on 3D MorphableModel, which significantly improves black-box transferability meanwhileenjoying faster search efficiency and better visual quality. Extensiveexperiments in digital and physical scenarios show that our method effectivelyexplores the security vulnerabilities of multiple popular commercial services,including three recognition APIs, four anti-spoofing APIs, two prevailingmobile phones and two automated access control systems.", "output": "Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wildlife camera trap images are being used extensively to investigate animalabundance, habitat associations, and behavior, which is complicated by the factthat experts must first classify the images manually. Artificial intelligencesystems can take over this task but usually need a large number ofalready-labeled training images to achieve sufficient performance. Thisrequirement necessitates human expert labor and poses a particular challengefor projects with few cameras or short durations. We propose a label-efficientlearning strategy that enables researchers with small or medium-sized imagedatabases to leverage the potential of modern machine learning, thus freeingcrucial resources for subsequent analyses.Our methodological proposal is two-fold: (1) We improve current strategies ofcombining object detection and image classification by tuning thehyperparameters of both models. (2) We provide an active learning (AL) systemthat allows training deep learning models very efficiently in terms of requiredhuman-labeled training images. We supply a software package that enablesresearchers to use these methods directly and thereby ensure the broadapplicability of the proposed framework in ecological practice.We show that our tuning strategy improves predictive performance. Wedemonstrate how the AL pipeline reduces the amount of pre-labeled data neededto achieve a specific predictive performance and that it is especially valuablefor improving out-of-sample predictive performance.We conclude that the combination of tuning and AL increases predictiveperformance substantially. Furthermore, we argue that our work can broadlyimpact the community through the ready-to-use software package provided.Finally, the publication of our models tailored to European wildlife dataenriches existing model bases mostly trained on data from Africa and NorthAmerica.", "output": "Automated wildlife image classification: An active learning tool for ecological applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain shift has been a long-standing issue for medical image segmentation.Recently, unsupervised domain adaptation (UDA) methods have achieved promisingcross-modality segmentation performance by distilling knowledge from alabel-rich source domain to a target domain without labels. In this work, wepropose a multi-scale self-ensembling based UDA framework for automaticsegmentation of two key brain structures i.e., Vestibular Schwannoma (VS) andCochlea on high-resolution T2 images. First, a segmentation-enhancedcontrastive unpaired image translation module is designed for image-leveldomain adaptation from source T1 to target T2. Next, multi-scale deepsupervision and consistency regularization are introduced to a mean teachernetwork for self-ensemble learning to further close the domain gap.Furthermore, self-training and intensity augmentation techniques are utilizedto mitigate label scarcity and boost cross-modality segmentation performance.Our method demonstrates promising segmentation performance with a mean Dicescore of 83.8% and 81.4% and an average asymmetric surface distance (ASSD) of0.55 mm and 0.26 mm for the VS and Cochlea, respectively in the validationphase of the crossMoDA 2022 challenge.", "output": "MS-MT: Multi-Scale Mean Teacher with Contrastive Unpaired Translation for Cross-Modality Vestibular Schwannoma and Cochlea Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual domain shift poses a significant challenge in real-worldapplications, particularly in situations where labeled data is not availablefor new domains. The challenge of acquiring knowledge in this problem settingis referred to as unsupervised continual domain shift learning. Existingmethods for domain adaptation and generalization have limitations in addressingthis issue, as they focus either on adapting to a specific domain orgeneralizing to unseen domains, but not both. In this paper, we proposeComplementary Domain Adaptation and Generalization (CoDAG), a simple yeteffective learning framework that combines domain adaptation and generalizationin a complementary manner to achieve three major goals of unsupervisedcontinual domain shift learning: adapting to a current domain, generalizing tounseen domains, and preventing forgetting of previously seen domains. Ourapproach is model-agnostic, meaning that it is compatible with any existingdomain adaptation and generalization algorithms. We evaluate CoDAG on severalbenchmark datasets and demonstrate that our model outperforms state-of-the-artmodels in all datasets and evaluation metrics, highlighting its effectivenessand robustness in handling unsupervised continual domain shift learning.", "output": "Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated chromosome instance segmentation from metaphase cell microscopicimages is critical for the diagnosis of chromosomal disorders (i.e., karyotypeanalysis). However, it is still a challenging task due to lacking of denselyannotated datasets and the complicated morphologies of chromosomes, e.g., densedistribution, arbitrary orientations, and wide range of lengths. To facilitatethe development of this area, we take a big step forward and manually constructa large-scale densely annotated dataset named AutoKary2022, which contains over27,000 chromosome instances in 612 microscopic images from 50 patients.Specifically, each instance is annotated with a polygonal mask and a classlabel to assist in precise chromosome detection and segmentation. On top of it,we systematically investigate representative methods on this dataset and obtaina number of interesting findings, which helps us have a deeper understanding ofthe fundamental problems in chromosome instance segmentation. We hope thisdataset could advance research towards medical understanding. The dataset canbe available at:", "output": "AutoKary2022: A Large-Scale Densely Annotated Dateset for Chromosome Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a new depth completion method based on multi-viewimproved monitored distillation to generate more accurate depth maps. Based onthe state-of-the-art depth completion method named ensemble distillation, weintroduce an existing stereo-based model as a teacher model to improve ensembledistillation accuracy and generate a more accurate student model in training byavoiding inherent error modes of completion-based teachers as well asminimizing the reconstruction error for a given image. We also leveragemulti-view depth consistency and multi-scale minimum reprojection to provideself-supervised information. These methods use the existing structureconstraints to yield supervised signals for student model training withoutgreat expense on gathering ground truth information of depth. Our extensiveexperimental evaluation demonstrates that our proposed method can effectivelyimprove the accuracy of baseline method of monitored distillation.", "output": "Multi-view Improved Monitored Distillation for Depth Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Currently, mobile and IoT devices are in dire need of a series of methods toenhance 4K images with limited resource expenditure. The absence of large-scale4K benchmark datasets hampers progress in this area, especially for dehazing.The challenges in building ultra-high-definition (UHD) dehazing datasets arethe absence of estimation methods for UHD depth maps, high-quality 4K depthestimation datasets, and migration strategies for UHD haze images fromsynthetic to real domains. To address these problems, we develop a novelsynthetic method to simulate 4K hazy images (including nighttime and daytimescenes) from clear images, which first estimates the scene depth, simulates thelight rays and object reflectance, then migrates the synthetic images to realdomains by using a GAN, and finally yields the hazy effects on 4K resolutionimages. We wrap these synthesized images into a benchmark called the 4K-HAZEdataset. Specifically, we design the CS-Mixer (an MLP-based model thatintegrates textbf{C}hannel domain and textbf{S}patial domain) to estimate thedepth map of 4K clear images, the GU-Net to migrate a 4K synthetic image to thereal hazy domain. The most appealing aspect of our approach (depth estimationand domain migration) is the capability to run a 4K image on a single GPU with24G RAM in real-time (33fps). Additionally, this work presents an objectiveassessment of several state-of-the-art single-image dehazing methods that areevaluated using the 4K-HAZE dataset. At the end of the paper, we discuss thelimitations of the 4K-HAZE dataset and its social implications.", "output": "4K-HAZE: A Dehazing Benchmark with 4K Resolution Hazy and Haze-Free Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Segmentation uncertainty models predict a distribution over plausiblesegmentations for a given input, which they learn from the annotator variationin the training set. However, in practice these annotations can differsystematically in the way they are generated, for example through the use ofdifferent labeling tools. This results in datasets that contain both datavariability and differing label styles. In this paper, we demonstrate thatapplying state-of-the-art segmentation uncertainty models on such datasets canlead to model bias caused by the different label styles. We present an updatedmodelling objective conditioning on labeling style for aleatoric uncertaintyestimation, and modify two state-of-the-art-architectures for segmentationuncertainty accordingly. We show with extensive experiments that this methodreduces label style bias, while improving segmentation performance, increasingthe applicability of segmentation uncertainty models in the wild. We curate twodatasets, with annotations in different label styles, which we will makepublicly available along with our code upon publication.", "output": "That Label's Got Style: Handling Label Style Bias for Uncertain Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Images generated by high-resolution SAR have vast areas of application asthey can work better in adverse light and weather conditions. One such area ofapplication is in the military systems. This study is an attempt to explore thesuitability of current state-of-the-art models introduced in the domain ofcomputer vision for SAR target classification (MSTAR). Since the application ofany solution produced for military systems would be strategic and real-time,accuracy is often not the only criterion to measure its performance. Otherimportant parameters like prediction time and input resiliency are equallyimportant. The paper deals with these issues in the context of SAR images.Experimental results show that deep learning models can be suitably applied inthe domain of SAR image classification with the desired performance levels.", "output": "Exploring Deep Learning Methods for Classification of SAR Images: Towards NextGen Convolutions via Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-world instance segmentation has recently gained significantpopularitydue to its importance in many real-world applications, such asautonomous driving, robot perception, and remote sensing. However, previousmethods have either produced unsatisfactory results or relied on complexsystems and paradigms. We wonder if there is a simple way to obtainstate-of-the-art results. Fortunately, we have identified two observations thathelp us achieve the best of both worlds: 1) query-based methods demonstratesuperiority over dense proposal-based methods in open-world instancesegmentation, and 2) learning localization cues is sufficient for open worldinstance segmentation. Based on these observations, we propose a simplequery-based method named OpenInst for open world instance segmentation.OpenInst leverages advanced query-based methods like QueryInst and focuses onlearning localization cues. Notably, OpenInst is an extremely simple andstraightforward framework without any auxiliary modules or post-processing, yetachieves state-of-the-art results on multiple benchmarks. Specifically, in theCOCO$to$UVO scenario, OpenInst achieves a mask AR of 53.3, outperforming theprevious best methods by 2.0 AR with a simpler structure. We hope that OpenInstcan serve as a solid baselines for future research in this area.", "output": "OpenInst: A Simple Query-Based Method for Open-World Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traditional video action detectors typically adopt the two-stage pipeline,where a person detector is first employed to generate actor boxes and then 3DRoIAlign is used to extract actor-specific features for classification. Thisdetection paradigm requires multi-stage training and inference, and cannotcapture context information outside the bounding box. Recently, a fewquery-based action detectors are proposed to predict action instances in anend-to-end manner. However, they still lack adaptability in feature samplingand decoding, thus suffering from the issues of inferior performance or slowerconvergence. In this paper, we propose a new one-stage sparse action detector,termed STMixer. STMixer is based on two core designs. First, we present aquery-based adaptive feature sampling module, which endows our STMixer with theflexibility of mining a set of discriminative features from the entirespatiotemporal domain. Second, we devise a dual-branch feature mixing module,which allows our STMixer to dynamically attend to and mix video features alongthe spatial and the temporal dimension respectively for better featuredecoding. Coupling these two designs with a video backbone yields an efficientend-to-end action detector. Without bells and whistles, our STMixer obtains thestate-of-the-art results on the datasets of AVA, UCF101-24, and JHMDB.", "output": "STMixer: A One-Stage Sparse Action Detector."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new approach for synthesizing novel views of people in newposes. Our novel differentiable renderer enables the synthesis of highlyrealistic images from any viewpoint. Rather than operating over mesh-basedstructures, our renderer makes use of diffuse Gaussian primitives that directlyrepresent the underlying skeletal structure of a human. Rendering theseprimitives gives results in a high-dimensional latent image, which is thentransformed into an RGB image by a decoder network. The formulation gives riseto a fully differentiable framework that can be trained end-to-end. Wedemonstrate the effectiveness of our approach to image reconstruction on boththe Human3.6M and Panoptic Studio datasets. We show how our approach can beused for motion transfer between individuals; novel view synthesis ofindividuals captured from just a single camera; to synthesize individuals fromany virtual viewpoint; and to re-render people in novel poses. Code and videoresults are available at", "output": "Novel View Synthesis of Humans using Differentiable Rendering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Distributed learning on the edge often comprises self-centered devices (SCD)which learn local tasks independently and are unwilling to contribute to theperformance of other SDCs. How do we achieve forward transfer at zero cost forthe single SCDs? We formalize this problem as a Distributed Continual Learningscenario, where SCD adapt to local tasks and a CL model consolidates theknowledge from the resulting stream of models without looking at the SCD'sprivate data. Unfortunately, current CL methods are not directly applicable tothis scenario. We propose Data-Agnostic Consolidation (DAC), a novel doubleknowledge distillation method that consolidates the stream of SC models withoutusing the original data. DAC performs distillation in the latent space via anovel Projected Latent Distillation loss. Experimental results show that DACenables forward transfer between SCDs and reaches state-of-the-art accuracy onSplit CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free anddistributed CL scenarios. Somewhat surprisingly, even a singleout-of-distribution image is sufficient as the only source of data duringconsolidation.", "output": "Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Demographic biases in source datasets have been shown as one of the causes ofunfairness and discrimination in the predictions of Machine Learning models.One of the most prominent types of demographic bias are statistical imbalancesin the representation of demographic groups in the datasets. In this paper, westudy the measurement of these biases by reviewing the existing metrics,including those that can be borrowed from other disciplines. We develop ataxonomy for the classification of these metrics, providing a practical guidefor the selection of appropriate metrics. To illustrate the utility of ourframework, and to further understand the practical characteristics of themetrics, we conduct a case study of 20 datasets used in Facial EmotionRecognition (FER), analyzing the biases present in them. Our experimentalresults show that many metrics are redundant and that a reduced subset ofmetrics may be sufficient to measure the amount of demographic bias. The paperprovides valuable insights for researchers in AI and related fields to mitigatedataset bias and improve the fairness and accuracy of AI models. The code isavailable at ", "output": "Metrics for Dataset Demographic Bias: A Case Study on Facial Expression Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Head generation with diverse identities is an important task in computervision and computer graphics, widely used in multimedia applications. However,current full head generation methods require a large number of 3D scans ormulti-view images to train the model, resulting in expensive data acquisitioncost. To address this issue, we propose Head3D, a method to generate full 3Dheads with limited multi-view images. Specifically, our approach first extractsfacial priors represented by tri-planes learned in EG3D, a 3D-aware generativemodel, and then proposes feature distillation to deliver the 3D frontal facesinto complete heads without compromising head integrity. To mitigate the domaingap between the face and head models, we present dual-discriminators to guidethe frontal and back head generation, respectively. Our model achievescost-efficient and diverse complete head generation with photo-realisticrenderings and high-quality geometry representations. Extensive experimentsdemonstrate the effectiveness of our proposed Head3D, both qualitatively andquantitatively.", "output": "Head3D: Complete 3D Head Generation via Tri-plane Feature Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce VIVE3D, a novel approach that extends the capabilities ofimage-based 3D GANs to video editing and is able to represent the input videoin an identity-preserving and temporally consistent way. We propose two newbuilding blocks. First, we introduce a novel GAN inversion techniquespecifically tailored to 3D GANs by jointly embedding multiple frames andoptimizing for the camera parameters. Second, besides traditional semantic faceedits (e.g. for age and expression), we are the first to demonstrate edits thatshow novel views of the head enabled by the inherent properties of 3D GANs andour optical flow-guided compositing technique to combine the head with thebackground video. Our experiments demonstrate that VIVE3D generateshigh-fidelity face edits at consistent quality from a range of cameraviewpoints which are composited with the original video in a temporally andspatially consistent manner.", "output": "VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent advancement in Video Instance Segmentation (VIS) has largely beendriven by the use of deeper and increasingly data-hungry transformer-basedmodels. However, video masks are tedious and expensive to annotate, limitingthe scale and diversity of existing VIS datasets. In this work, we aim toremove the mask-annotation requirement. We propose MaskFreeVIS, achievinghighly competitive VIS performance, while only using bounding box annotationsfor the object state. We leverage the rich temporal mask consistencyconstraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss),providing strong mask supervision without any labels. Our TK-Loss findsone-to-many matches across frames, through an efficient patch-matching stepfollowed by a K-nearest neighbor selection. A consistency loss is then enforcedon the found matches. Our mask-free objective is simple to implement, has notrainable parameters, is computationally efficient, yet outperforms baselinesemploying, e.g., state-of-the-art optical flow to enforce temporal maskconsistency. We validate MaskFreeVIS on the YouTube-VIS 2019/2021, OVIS andBDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of ourmethod by drastically narrowing the gap between fully and weakly-supervised VISperformance. Our code and trained models are available at", "output": "Mask-Free Video Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world visual data exhibit intrinsic hierarchical structures that can berepresented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs)are a promising approach for learning feature representations in such spaces.However, current methods in computer vision rely on Euclidean backbones andonly project features to the hyperbolic space in the task heads, limiting theirability to fully leverage the benefits of hyperbolic geometry. To address this,we present HCNN, the first fully hyperbolic convolutional neural network (CNN)designed for computer vision tasks. Based on the Lorentz model, we generalizefundamental components of CNNs and propose novel formulations of theconvolutional layer, batch normalization, and multinomial logistic regression(MLR). Experimentation on standard vision tasks demonstrates the effectivenessof our HCNN framework and the Lorentz model in both hybrid and fully hyperbolicsettings. Overall, we aim to pave the way for future research in hyperboliccomputer vision by offering a new paradigm for interpreting and analyzingvisual data. Our code is publicly available at", "output": "Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic radiology report generation has attracted enormous researchinterest due to its practical value in reducing the workload of radiologists.However, simultaneously establishing global correspondences between the image(e.g., Chest X-ray) and its related report and local alignments between imagepatches and keywords remains challenging. To this end, we propose an Unify,Align and then Refine (UAR) approach to learn multi-level cross-modalalignments and introduce three novel modules: Latent Space Unifier (LSU),Cross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR).Specifically, LSU unifies multimodal data into discrete tokens, making itflexible to learn common knowledge among modalities with a shared network. Themodality-agnostic CRA learns discriminative features via a set of orthonormalbasis and a dual-gate mechanism first and then globally aligns visual andtextual representations under a triplet contrastive loss. TIR booststoken-level local alignment via calibrating text-to-image attention with alearnable mask. Additionally, we design a two-stage training procedure to makeUAR gradually grasp cross-modal alignments at different levels, which imitatesradiologists' workflow: writing sentence by sentence first and then checkingword by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXRbenchmark datasets demonstrate the superiority of our UAR against variedstate-of-the-art methods.", "output": "Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Content-aware visual-textual presentation layout aims at arranging spatialspace on the given canvas for pre-defined elements, including text, logo, andunderlay, which is a key to automatic template-free creative graphic design. Inpractical applications, e.g., poster designs, the canvas is originallynon-empty, and both inter-element relationships as well as inter-layerrelationships should be concerned when generating a proper layout. A few recentworks deal with them simultaneously, but they still suffer from poor graphicperformance, such as a lack of layout variety or spatial non-alignment. Sincecontent-aware visual-textual presentation layout is a novel task, we firstconstruct a new dataset named PosterLayout, which consists of 9,974poster-layout pairs and 905 images, i.e., non-empty canvases. It is morechallenging and useful for greater layout variety, domain diversity, andcontent diversity. Then, we propose design sequence formation (DSF) thatreorganizes elements in layouts to imitate the design processes of humandesigners, and a novel CNN-LSTM-based conditional generative adversarialnetwork (GAN) is presented to generate proper layouts. Specifically, thediscriminator is design-sequence-aware and will supervise the \"design\" processof the generator. Experimental results verify the usefulness of the newbenchmark and the effectiveness of the proposed approach, which achieves thebest performance by generating suitable layouts for diverse canvases.", "output": "PosterLayout: A New Benchmark and Approach for Content-aware Visual-Textual Presentation Layout."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative adversarial networks (GANs) have shown remarkable success ingenerating realistic images and are increasingly used in medical imaging forimage-to-image translation tasks. However, GANs tend to suffer from a frequencybias towards low frequencies, which can lead to the removal of importantstructures in the generated images. To address this issue, we propose a novelfrequency-aware image-to-image translation framework based on the supervisedRegGAN approach, which we call fRegGAN. The framework employs a K-space loss toregularize the frequency content of the generated images and incorporateswell-known properties of MRI K-space geometry to guide the network trainingprocess. By combine our method with the RegGAN approach, we can mitigate theeffect of training with misaligned data and frequency bias at the same time. Weevaluate our method on the public BraTS dataset and outperform the baselinemethods in terms of both quantitative and qualitative metrics when synthesizingT2-weighted from T1-weighted MR images. Detailed ablation studies are providedto understand the effect of each modification on the final performance. Theproposed method is a step towards improving the performance of image-to-imagetranslation and synthesis in the medical domain and shows promise for otherapplications in the field of image processing and generation.", "output": "fRegGAN with K-space Loss Regularization for Medical Image Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recording surgery in operating rooms is an essential task for education andevaluation of medical treatment. However, recording the desired targets, suchas the surgery field, surgical tools, or doctor's hands, is difficult becausethe targets are heavily occluded during surgery. We use a recording system inwhich multiple cameras are embedded in the surgical lamp, and we assume that atleast one camera is recording the target without occlusion at any given time.As the embedded cameras obtain multiple video sequences, we address the task ofselecting the camera with the best view of the surgery. Unlike the conventionalmethod, which selects the camera based on the area size of the surgery field,we propose a deep neural network that predicts the camera selection probabilityfrom multiple video sequences by learning the supervision of the expertannotation. We created a dataset in which six different types of plasticsurgery are recorded, and we provided the annotation of camera switching. Ourexperiments show that our approach successfully switched between cameras andoutperformed three baseline methods.", "output": "Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF)for novel view synthesis, which enables arbitrary input camera trajectories andonly costs a few minutes for training. Existing fast grid-based NeRF trainingframeworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designedfor bounded scenes and rely on space warping to handle unbounded scenes.Existing two widely-used space-warping methods are only designed for theforward-facing trajectory or the 360-degree object-centric trajectory butcannot process arbitrary trajectories. In this paper, we delve deep into themechanism of space warping to handle unbounded scenes. Based on our analysis,we further propose a novel space-warping method called perspective warping,which allows us to handle arbitrary trajectories in the grid-based NeRFframework. Extensive experiments demonstrate that F2-NeRF is able to use thesame perspective warping to render high-quality images on two standard datasetsand a new free trajectory dataset collected by us. Project page:", "output": "F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To represent the biological variability of clinical neuroimaging populations,it is vital to be able to combine data across scanners and studies. However,different MRI scanners produce images with different characteristics, resultingin a domain shift known as the `harmonisation problem'. Additionally,neuroimaging data is inherently personal in nature, leading to data privacyconcerns when sharing the data. To overcome these barriers, we propose anUnsupervised Source-Free Domain Adaptation (SFDA) method, SFHarmony. Throughmodelling the imaging features as a Gaussian Mixture Model and minimising anadapted Bhattacharyya distance between the source and target features, we cancreate a model that performs well for the target data whilst having a sharedfeature representation across the data domains, without needing access to thesource data for adaptation or target labels. We demonstrate the performance ofour method on simulated and real domain shifts, showing that the approach isapplicable to classification, segmentation and regression tasks, requiring nochanges to the algorithm. Our method outperforms existing SFDA approachesacross a range of realistic data scenarios, demonstrating the potential utilityof our approach for MRI harmonisation and general SFDA problems. Our code isavailable at url{", "output": "SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Discovering novel concepts from unlabelled data and in a continuous manner isan important desideratum of lifelong learners. In the literature such problemshave been partially addressed under very restricted settings, where eitheraccess to labelled data is provided for discovering novel concepts (e.g., NCD)or learning occurs for a limited number of incremental steps (e.g.,class-iNCD). In this work we challenge the status quo and propose a morechallenging and practical learning paradigm called MSc-iNCD, where learningoccurs continuously and unsupervisedly, while exploiting the rich priors fromlarge-scale pre-trained models. To this end, we propose simple baselines thatare not only resilient under longer learning scenarios, but are surprisinglystrong when compared with sophisticated state-of-the-art methods. We conductextensive empirical evaluation on a multitude of benchmarks and show theeffectiveness of our proposed baselines, which significantly raises the bar.", "output": "Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite its wide range of applications, video summarization is still heldback by the scarcity of extensive datasets, largely due to the labor-intensiveand costly nature of frame-level annotations. As a result, existing videosummarization methods are prone to overfitting. To mitigate this challenge, wepropose a novel self-supervised video representation learning method usingknowledge distillation to pre-train a transformer encoder. Our method matchesits semantic video representation, which is constructed with respect to frameimportance scores, to a representation derived from a CNN trained on videoclassification. Empirical evaluations on correlation-based metrics, such asKendall's $tau$ and Spearman's $rho$ demonstrate the superiority of ourapproach compared to existing state-of-the-art methods in assigning relativescores to the input frames.", "output": "SELF-VS: Self-supervised Encoding Learning For Video Summarization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Panoptic Scene Graph generation (PSG) is a recently proposed task in imagescene understanding that aims to segment the image and extract triplets ofsubjects, objects and their relations to build a scene graph. This task isparticularly challenging for two reasons. First, it suffers from a long-tailproblem in its relation categories, making naive biased methods more inclinedto high-frequency relations. Existing unbiased methods tackle the long-tailproblem by data/loss rebalancing to favor low-frequency relations. Second, asubject-object pair can have two or more semantically overlapping relations.While existing methods favor one over the other, our proposed HiLo frameworklets different network branches specialize on low and high frequency relations,enforce their consistency and fuse the results. To the best of our knowledge weare the first to propose an explicitly unbiased PSG method. In extensiveexperiments we show that our HiLo framework achieves state-of-the-art resultson the PSG task. We also apply our method to the Scene Graph Generation taskthat predicts boxes instead of masks and see improvements over all baselinemethods.", "output": "HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work the authors develop regression approaches based on deep learningto perform thread density estimation for plain weave canvas analysis. Previousapproaches were based on Fourier analysis, that are quite robust for somescenarios but fail in some other, in machine learning tools, that involvepre-labeling of the painting at hand, or the segmentation of thread crossingpoints, that provides good estimations in all scenarios with no need ofpre-labeling. The segmentation approach is time-consuming as estimation of thedensities is performed after locating the crossing points. In this novelproposal, we avoid this step by computing the density of threads directly fromthe image with a regression deep learning model. We also incorporate someimprovements in the initial preprocessing of the input image with an impact onthe final error. Several models are proposed and analyzed to retain the bestone. Furthermore, we further reduce the density estimation error by introducinga semi-supervised approach. The performance of our novel algorithm is analyzedwith works by Ribera, Vel'azquez, and Poussin where we compare our results tothe ones of previous approaches. Finally, the method is put into practice tosupport the change of authorship or a masterpiece at the Museo del Prado.", "output": "Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a setof registered images. Increasing sizes of a scene demands more complexfunctions, typically represented by neural networks, to capture all details.Training and inference then involves querying the neural network millions oftimes per image, which becomes impractically slow. Since such complex functionscan be replaced by multiple simpler functions to improve speed, we show that ahierarchy of Voronoi diagrams is a suitable choice to partition the scene. Byequipping each Voronoi cell with its own NeRF, our approach is able to quicklylearn a scene representation. We propose an intuitive partitioning of the spacethat increases quality gains during training by distributing information evenlyamong the networks and avoids artifacts through a top-down adaptive refinement.Our framework is agnostic to the underlying NeRF method and easy to implement,which allows it to be applied to various NeRF variants for improved learningand rendering speeds.", "output": "Adaptive Voronoi NeRFs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trajectory prediction is a crucial undertaking in understanding entitymovement or human behavior from observed sequences. However, current methodsoften assume that the observed sequences are complete while ignoring thepotential for missing values caused by object occlusion, scope limitation,sensor failure, etc. This limitation inevitably hinders the accuracy oftrajectory prediction. To address this issue, our paper presents a unifiedframework, the Graph-based Conditional Variational Recurrent Neural Network(GC-VRNN), which can perform trajectory imputation and predictionsimultaneously. Specifically, we introduce a novel Multi-Space Graph NeuralNetwork (MS-GNN) that can extract spatial features from incomplete observationsand leverage missing patterns. Additionally, we employ a Conditional VRNN witha specifically designed Temporal Decay (TD) module to capture temporaldependencies and temporal missing patterns in incomplete trajectories. Theinclusion of the TD module allows for valuable information to be conveyedthrough the temporal flow. We also curate and benchmark three practicaldatasets for the joint problem of trajectory imputation and prediction.Extensive experiments verify the exceptional performance of our proposedmethod. As far as we know, this is the first work to address the lack ofbenchmarks and techniques for trajectory imputation and prediction in a unifiedmanner.", "output": "Uncovering the Missing Pattern: Unified Framework Towards Trajectory Imputation and Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In a noisy conversation environment such as a dinner party, people oftenexhibit selective auditory attention, or the ability to focus on a particularspeaker while tuning out others. Recognizing who somebody is listening to in aconversation is essential for developing technologies that can understandsocial behavior and devices that can augment human hearing by amplifyingparticular sound sources. The computer vision and audio research communitieshave made great strides towards recognizing sound sources and speakers inscenes. In this work, we take a step further by focusing on the problem oflocalizing auditory attention targets in egocentric video, or detecting who ina camera wearer's field of view they are listening to. To tackle the new andchallenging Selective Auditory Attention Localization problem, we propose anend-to-end deep learning approach that uses egocentric video and multichannelaudio to predict the heatmap of the camera wearer's auditory attention. Ourapproach leverages spatiotemporal audiovisual features and holistic reasoningabout the scene to make predictions, and outperforms a set of baselines on achallenging multi-speaker conversation dataset. Project page:", "output": "Egocentric Auditory Attention Localization in Conversations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an information-theoretic knowledge distillation approach for thecompression of generative adversarial networks, which aims to maximize themutual information between teacher and student networks via a variationaloptimization based on an energy-based model. Because the direct computation ofthe mutual information in continuous domains is intractable, our approachalternatively optimizes the student network by maximizing the variational lowerbound of the mutual information. To achieve a tight lower bound, we introducean energy-based model relying on a deep neural network to represent a flexiblevariational distribution that deals with high-dimensional images and considerspatial dependencies between pixels, effectively. Since the proposed method isa generic optimization algorithm, it can be conveniently incorporated intoarbitrary generative adversarial networks and even dense prediction networks,e.g., image enhancement models. We demonstrate that the proposed algorithmachieves outstanding performance in model compression of generative adversarialnetworks consistently when combined with several existing models.", "output": "Information-Theoretic GAN Compression with Variational Energy-based Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-time eyeblink detection in the wild can widely serve for fatiguedetection, face anti-spoofing, emotion analysis, etc. The existing researchefforts generally focus on single-person cases towards trimmed video. However,multi-person scenario within untrimmed videos is also important for practicalapplications, which has not been well concerned yet. To address this, we shedlight on this research field for the first time with essential contributions ondataset, theory, and practices. In particular, a large-scale dataset termedMPEblink that involves 686 untrimmed videos with 8748 eyeblink events isproposed under multi-person conditions. The samples are captured fromunconstrained films to reveal \"in the wild\" characteristics. Meanwhile, areal-time multi-person eyeblink detection method is also proposed. Beingdifferent from the existing counterparts, our proposition runs in a one-stagespatio-temporal way with end-to-end learning capacity. Specifically, itsimultaneously addresses the sub-tasks of face detection, face tracking, andhuman instance-level eyeblink detection. This paradigm holds 2 main advantages:(1) eyeblink features can be facilitated via the face's global context (e.g.,head pose and illumination condition) with joint optimization and interaction,and (2) addressing these sub-tasks in parallel instead of sequential manner cansave time remarkably to meet the real-time running requirement. Experiments onMPEblink verify the essential challenges of real-time multi-person eyeblinkdetection in the wild for untrimmed video. Our method also outperforms existingapproaches by large margins and with a high inference speed.", "output": "Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video Foundation Models (VFMs) have received limited exploration due to highcomputational costs and data scarcity. Previous VFMs rely on Image FoundationModels (IFMs), which face challenges in transferring to the video domain.Although VideoMAE has trained a robust ViT from limited data, its low-levelreconstruction poses convergence difficulties and conflicts with high-levelcross-modal alignment. This paper proposes a training-efficient method fortemporal-sensitive VFMs that integrates the benefits of existing methods. Toincrease data efficiency, we mask out most of the low-semantics video tokens,but selectively align the unmasked tokens with IFM, which serves as theUnMasked Teacher (UMT). By providing semantic guidance, our method enablesfaster convergence and multimodal friendliness. With a progressive pre-trainingframework, our model can handle various tasks including scene-related,temporal-related, and complex video-language understanding. Using only publicsources for pre-training in 6 days on 32 A100 GPUs, our scratch-built ViT-L/16achieves state-of-the-art performances on various video tasks. The code andmodels will be released at ", "output": "Unmasked Teacher: Towards Training-Efficient Video Foundation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the challenges in federated learning is the non-independent andidentically distributed (non-iid) characteristics between heterogeneousdevices, which cause significant differences in local updates and affect theperformance of the central server. Although many studies have been proposed toaddress this challenge, they only focus on local training and aggregationprocesses to smooth the changes and fail to achieve high performance with deeplearning models. Inspired by the phenomenon of neural collapse, we force eachclient to be optimized toward an optimal global structure for classification.Specifically, we initialize it as a random simplex Equiangular Tight Frame(ETF) and fix it as the unit optimization target of all clients during thelocal updating. After guaranteeing all clients are learning to converge to theglobal optimum, we propose to add a global memory vector for each category toremedy the parameter fluctuation caused by the bias of the intra-classcondition distribution among clients. Our experimental results show that ourmethod can improve the performance with faster convergence speed ondifferent-size datasets.", "output": "Neural Collapse Inspired Federated Learning with Non-iid Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the challenging problem of estimating the relative pose of threecalibrated cameras. We propose two novel solutions to the notoriously difficultconfiguration of four points in three views, known as the 4p3v problem. Oursolutions are based on the simple idea of generating one additional virtualpoint correspondence in two views by using the information from the locationsof the four input correspondences in the three views. For the first solver, wetrain a network to predict this point correspondence. The second solver uses amuch simpler and more efficient strategy based on the mean points of threecorresponding input points. The new solvers are efficient and easy to implementsince they are based on the existing efficient minimal solvers, i.e., thewell-known 5-point relative pose and the P3P solvers. The solvers achievestate-of-the-art results on real data. The idea of solving minimal problemsusing virtual correspondences is general and can be applied to other problems,e.g., the 5-point relative pose problem. In this way, minimal problems can besolved using simpler non-minimal solvers or even using sub-minimal samplesinside RANSAC.In addition, we compare different variants of 4p3v solvers with the baselinesolver for the minimal configuration consisting of three triplets of points andtwo points visible in two views. We discuss which configuration of points ispotentially the most practical in real applications.", "output": "Efficient solutions to the relative pose of three calibrated cameras from four points using virtual correspondences."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot action recognition, i.e. recognizing new action classes given only afew examples, benefits from incorporating temporal information. Prior workeither encodes such information in the representation itself and learnsclassifiers at test time, or obtains frame-level features and performs pairwisetemporal matching. We first evaluate a number of matching-based approachesusing features from spatio-temporal backbones, a comparison missing from theliterature, and show that the gap in performance between simple baselines andmore complicated methods is significantly reduced. Inspired by this, we proposeChamfer++, a non-temporal matching function that achieves state-of-the-artresults in few-shot action recognition. We show that, when starting fromtemporal features, our parameter-free and interpretable approach can outperformall other matching-based and classifier methods for one-shot action recognitionon three common datasets without using temporal information in the matchingstage. Project page: ", "output": "Rethinking matching-based few-shot action recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper evaluates the performance of supervised and unsupervised deeplearning models for denoising positron emission tomography (PET) images in thepresence of reduced acquisition times. Our experiments consider 212 studies(56908 images), and evaluate the models using 2D (RMSE, SSIM) and 3D (SUVpeakand SUVmax error for the regions of interest) metrics. It was shown that, incontrast to previous studies, supervised models (ResNet, Unet, SwinIR)outperform unsupervised models (pix2pix GAN and CycleGAN with ResNet backboneand various auxiliary losses) in the reconstruction of 2D PET images. Moreover,a hybrid approach of supervised CycleGAN shows the best results in SUVmaxestimation for denoised images, and the SUVmax estimation error for denoisedimages is comparable with the PET reproducibility error.", "output": "Whole-body PET image denoising for reduced acquisition time."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extending the success of 2D Large Kernel to 3D perception is challenging dueto: 1. the cubically-increasing overhead in processing 3D data; 2. theoptimization difficulties from data scarcity and sparsity. Previous work hastaken the first step to scale up the kernel size from 3x3x3 to 7x7x7 byintroducing block-shared weights. However, to reduce the feature variationswithin a block, it only employs modest block size and fails to achieve largerkernels like the 21x21x21. To address this issue, we propose a new method,called LinK, to achieve a wider-range perception receptive field in aconvolution-like manner with two core designs. The first is to replace thestatic kernel matrix with a linear kernel generator, which adaptively providesweights only for non-empty voxels. The second is to reuse the pre-computedaggregation results in the overlapped blocks to reduce computation complexity.The proposed method successfully enables each voxel to perceive context withina range of 21x21x21. Extensive experiments on two basic perception tasks, 3Dobject detection and 3D semantic segmentation, demonstrate the effectiveness ofour method. Notably, we rank 1st on the public leaderboard of the 3D detectionbenchmark of nuScenes (LiDAR track), by simply incorporating a LinK-basedbackbone into the basic detector, CenterPoint. We also boost the strongsegmentation baseline's mIoU with 2.7% in the SemanticKITTI test set. Code isavailable at ", "output": "LinK: Linear Kernel for LiDAR-based 3D Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the past ten years, with the help of deep learning, especially the rapiddevelopment of deep neural networks, medical image analysis has made remarkableprogress. However, how to effectively use the relational information betweenvarious tissues or organs in medical images is still a very challengingproblem, and it has not been fully studied. In this thesis, we propose twonovel solutions to this problem based on deep relational learning. First, wepropose a context-aware fully convolutional network that effectively modelsimplicit relation information between features to perform medical imagesegmentation. The network achieves the state-of-the-art segmentation results onthe Multi Modal Brain Tumor Segmentation 2017 (BraTS2017) and Multi Modal BrainTumor Segmentation 2018 (BraTS2018) data sets. Subsequently, we propose a newhierarchical homography estimation network to achieve accurate medical imagemosaicing by learning the explicit spatial relationship between adjacentframes. We use the UCL Fetoscopy Placenta dataset to conduct experiments andour hierarchical homography estimation network outperforms the otherstate-of-the-art mosaicing methods while generating robust and meaningfulmosaicing result on unseen frames.", "output": "Medical Image Analysis using Deep Relational Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present GP3D, a novel network for generalized poseestimation in 3D point clouds. The method generalizes to new objects by usingboth the scene point cloud and the object point cloud with keypoint indexes asinput. The network is trained to match the object keypoints to scene points. Toaddress the pose estimation of novel objects we also present a new approach fortraining pose estimation. The typical solution is a single model trained forpose estimation of a specific object in any scenario. This has severaldrawbacks: training a model for each object is time-consuming, energyconsuming, and by excluding the scenario information the task becomes moredifficult. In this paper, we present the opposite solution; a scenario-specificpose estimation method for novel objects that do not require retraining. Thenetwork is trained on 1500 objects and is able to learn a generalized solution.We demonstrate that the network is able to correctly predict novel objects, anddemonstrate the ability of the network to perform outside of the trained class.We believe that the demonstrated method is a valuable solution for manyreal-world scenarios. Code and trained network will be made available afterpublication.", "output": "GP3D: Generalized Pose Estimation in 3D Point Clouds: A case study on bin picking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a text-to-image generation algorithm based on deep neural networkswhen text captions for images are unavailable during training. In this work,instead of simply generating pseudo-ground-truth sentences of training imagesusing existing image captioning methods, we employ a pretrained CLIP model,which is capable of properly aligning embeddings of images and correspondingtexts in a joint space and, consequently, works well on zero-shot recognitiontasks. We optimize a text-to-image generation model by maximizing the datalog-likelihood conditioned on pairs of image-text CLIP embeddings. To betteralign data in the two domains, we employ a principled way based on avariational inference, which efficiently estimates an approximate posterior ofthe hidden text embedding given an image and its CLIP feature. Experimentalresults validate that the proposed framework outperforms existing approaches bylarge margins under unsupervised and semi-supervised text-to-image generationsettings.", "output": "Variational Distribution Learning for Unsupervised Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The relation modeling between actors and scene context advances video actiondetection where the correlation of multiple actors makes their actionrecognition challenging. Existing studies model each actor and scene relationto improve action recognition. However, the scene variations and backgroundinterference limit the effectiveness of this relation modeling. In this paper,we propose to select actor-related scene context, rather than directly leverageraw video scenario, to improve relation modeling. We develop a CycleActor-Context Relation network (CycleACR) where there is a symmetric graph thatmodels the actor and context relations in a bidirectional form. Our CycleACRconsists of the Actor-to-Context Reorganization (A2C-R) that collects actorfeatures for context feature reorganizations, and the Context-to-ActorEnhancement (C2A-E) that dynamically utilizes reorganized context features foractor feature enhancement. Compared to existing designs that focus on C2A-E,our CycleACR introduces A2C-R for a more effective relation modeling. Thismodeling advances our CycleACR to achieve state-of-the-art performance on twopopular action detection datasets (i.e., AVA and UCF101-24). We also provideablation studies and visualizations as well to show how our cycle actor-contextrelation modeling improves video action detection. Code is available at", "output": "CycleACR: Cycle Modeling of Actor-Context Relations for Video Action Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "2D and 3D tumor features are widely used in a variety of medical imageanalysis tasks. However, for chemotherapy response prediction, theeffectiveness between different kinds of 2D and 3D features are notcomprehensively assessed, especially in ovarian cancer-related applications.This investigation aims to accomplish such a comprehensive evaluation. For thispurpose, CT images were collected retrospectively from 188 advanced-stageovarian cancer patients. All the metastatic tumors that occurred in eachpatient were segmented and then processed by a set of six filters. Next, threecategories of features, namely geometric, density, and texture features, werecalculated from both the filtered results and the original segmented tumors,generating a total of 1595 and 1403 features for the 3D and 2D tumors,respectively. In addition to the conventional single-slice 2D and full-volume3D tumor features, we also computed the incomplete-3D tumor features, whichwere achieved by sequentially adding one individual CT slice and calculatingthe corresponding features. Support vector machine (SVM) based predictionmodels were developed and optimized for each feature set. 5-foldcross-validation was used to assess the performance of each individual model.The results show that the 2D feature-based model achieved an AUC (area underthe ROC curve [receiver operating characteristic]) of 0.84+-0.02. When addingmore slices, the AUC first increased to reach the maximum and then graduallydecreased to 0.86+-0.02. The maximum AUC was yielded when adding two adjacentslices, with a value of 0.91+-0.01. This initial result provides meaningfulinformation for optimizing machine learning-based decision-making support toolsin the future.", "output": "Evaluating the Effectiveness of 2D and 3D Features for Predicting Tumor Response to Chemotherapy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As general purpose vision models get increasingly effective at a wide set oftasks, it is imperative that they be consistent across the tasks they support.Inconsistent AI models are considered brittle and untrustworthy by human usersand are more challenging to incorporate into larger systems that takedependencies on their outputs. Measuring consistency between very heterogeneoustasks that might include outputs in different modalities is challenging sinceit is difficult to determine if the predictions are consistent with oneanother. As a solution, we introduce a benchmark dataset, COCOCON, where we usecontrast sets created by modifying test instances for multiple tasks in smallbut semantically meaningful ways to change the gold label, and outline metricsfor measuring if a model is consistent by ranking the original and perturbedinstances across tasks. We find that state-of-the-art systems suffer from asurprisingly high degree of inconsistent behavior across tasks, especially formore heterogeneous tasks. Finally, we propose using a rank correlation-basedauxiliary objective computed over large automatically created cross-taskcontrast sets to improve the multi-task consistency of large unified models,while retaining their original accuracy on downstream tasks. Project websiteavailable at ", "output": "Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning is effective in diagnosing COVID-19 and requires a large amountof data to be effectively trained. Due to data and privacy regulations,hospitals generally have no access to data from other hospitals. Federatedlearning (FL) has been used to solve this problem, where it utilizes adistributed setting to train models in hospitals in a privacy-preservingmanner. Deploying FL is not always feasible as it requires high computation andnetwork communication resources. This paper evaluates five FL algorithms'performance and resource efficiency for Covid-19 detection. A decentralizedsetting with CNN networks is set up, and the performance of FL algorithms iscompared with a centralized environment. We examined the algorithms withvarying numbers of participants, federated rounds, and selection algorithms.Our results show that cyclic weight transfer can have better overallperformance, and results are better with fewer participating hospitals. Ourresults demonstrate good performance for detecting COVID-19 patients and mightbe useful in deploying FL algorithms for covid-19 detection and medical imageanalysis in general.", "output": "A Comparative Study of Federated Learning Models for COVID-19 Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human activity recognition and clinical biomechanics are challenging problemsin physical telerehabilitation medicine. However, most publicly availabledatasets on human body movements cannot be used to study both problems in anout-of-the-lab movement acquisition setting. The objective of the VIDIMUdataset is to pave the way towards affordable patient tracking solutions forremote daily life activities recognition and kinematic analysis. The datasetincludes 13 activities registered using a commodity camera and five inertialsensors. The video recordings were acquired in 54 subjects, of which 16 alsohad simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in:i) the clinical relevance of the chosen movements, ii) the combined utilizationof affordable video and custom sensors, and iii) the implementation ofstate-of-the-art tools for multimodal data processing of 3D body pose trackingand motion reconstruction in a musculoskeletal model from inertial data. Thevalidation confirms that a minimally disturbing acquisition protocol, performedaccording to real-life conditions can provide a comprehensive picture of humanjoint angles during daily life activities.", "output": "VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Whole-body mesh recovery aims to estimate the 3D human body, face, and handsparameters from a single image. It is challenging to perform this task with asingle network due to resolution issues, i.e., the face and hands are usuallylocated in extremely small regions. Existing works usually detect hands andfaces, enlarge their resolution to feed in a specific network to predict theparameter, and finally fuse the results. While this copy-paste pipeline cancapture the fine-grained details of the face and hands, the connections betweendifferent parts cannot be easily recovered in late fusion, leading toimplausible 3D rotation and unnatural pose. In this work, we propose aone-stage pipeline for expressive whole-body mesh recovery, named OSX, withoutseparate networks for each part. Specifically, we design a Component AwareTransformer (CAT) composed of a global body encoder and a local face/handdecoder. The encoder predicts the body parameters and provides a high-qualityfeature map for the decoder, which performs a feature-level upsample-cropscheme to extract high-resolution part-specific features and adoptkeypoint-guided deformable attention to estimate hand and face precisely. Thewhole pipeline is simple yet effective without any manual post-processing andnaturally avoids implausible prediction. Comprehensive experiments demonstratethe effectiveness of OSX. Lastly, we build a large-scale Upper-Body dataset(UBody) with high-quality 2D and 3D whole-body annotations. It contains personswith partially visible bodies in diverse real-life scenarios to bridge the gapbetween the basic task and downstream applications.", "output": "One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Magnetic Resonance Imaging (MRI) reconstruction enables multiplehospitals to collaborate distributedly without aggregating local data, therebyprotecting patient privacy. However, the data heterogeneity caused by differentMRI protocols, insufficient local training data, and limited communicationbandwidth inevitably impair global model convergence and updating. In thispaper, we propose a new algorithm, FedPR, to learn federated visual prompts inthe null space of global prompt for MRI reconstruction. FedPR is a newfederated paradigm that adopts a powerful pre-trained model while only learningand communicating the prompts with few learnable parameters, therebysignificantly reducing communication costs and achieving competitiveperformance on limited local data. Moreover, to deal with catastrophicforgetting caused by data heterogeneity, FedPR also updates efficient federatedvisual prompts that project the local prompts into an approximate null space ofthe global prompt, thereby suppressing the interference of gradients on theserver performance. Extensive experiments on federated MRI show that FedPRsignificantly outperforms state-of-the-art FL algorithms with &lt;6% ofcommunication costs when given the limited amount of local training data.", "output": "Learning Federated Visual Prompt in Null Space for MRI Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the emergence of neural radiance fields (NeRFs), view synthesis qualityhas reached an unprecedented level. Compared to traditional mesh-based assets,this volumetric representation is more powerful in expressing scene geometrybut inevitably suffers from high rendering costs and can hardly be involved infurther processes like editing, posing significant difficulties in combinationwith the existing graphics pipeline. In this paper, we present a hybridvolume-mesh representation, VMesh, which depicts an object with a textured meshalong with an auxiliary sparse volume. VMesh retains the advantages ofmesh-based assets, such as efficient rendering, compact storage, and easyediting, while also incorporating the ability to represent subtle geometricstructures provided by the volumetric counterpart. VMesh can be obtained frommulti-view images of an object and renders at 2K 60FPS on common consumerdevices with high fidelity, unleashing new opportunities for real-timeimmersive applications.", "output": "VMesh: Hybrid Volume-Mesh Representation for Efficient View Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a scenario where we have access to the target domain, but cannotafford on-the-fly training data annotation, and instead would like to constructan alternative training set from a large-scale data pool such that acompetitive model can be obtained. We propose a search and pruning (SnP)solution to this training data search problem, tailored to objectre-identification (re-ID), an application aiming to match the same objectcaptured by different cameras. Specifically, the search stage identifies andmerges clusters of source identities which exhibit similar distributions withthe target domain. The second stage, subject to a budget, then selectsidentities and their images from the Stage I output, to control the size of theresulting training set for efficient training. The two steps provide us withtraining sets 80% smaller than the source pool while achieving a similar oreven higher re-ID accuracy. These training sets are also shown to be superiorto a few existing search methods such as random sampling and greedy samplingunder the same budget on training data size. If we release the budget, trainingsets resulting from the first stage alone allow even higher re-ID accuracy. Weprovide interesting discussions on the specificity of our method to the re-IDproblem and particularly its role in bridging the re-ID domain gap. The code isavailable at ", "output": "Large-scale Training Data Search for Object Re-identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress with conditional image diffusion models has been stunning,and this holds true whether we are speaking about models conditioned on a textdescription, a scene layout, or a sketch. Unconditional image diffusion modelsare also improving but lag behind, as do diffusion models which are conditionedon lower-dimensional features like class labels. We propose to close the gapbetween conditional and unconditional models using a two-stage samplingprocedure. In the first stage we sample an embedding describing the semanticcontent of the image. In the second stage we sample the image conditioned onthis embedding and then discard the embedding. Doing so lets us leverage thepower of conditional diffusion models on the unconditional generation task,which we show improves FID by 25-50% compared to standard unconditionalgeneration.", "output": "Visual Chain-of-Thought Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have shown that sequence modeling can be effectively used totrain reinforcement learning (RL) policies. However, the success of applyingexisting sequence models to planning, in which we wish to obtain a trajectoryof actions to reach some goal, is less straightforward. The typicalautoregressive generation procedures of sequence models preclude sequentialrefinement of earlier steps, which limits the effectiveness of a predictedplan. In this paper, we suggest an approach towards integrating planning withsequence models based on the idea of iterative energy minimization, andillustrate how such a procedure leads to improved RL performance acrossdifferent tasks. We train a masked language model to capture an implicit energyfunction over trajectories of actions, and formulate planning as finding atrajectory of actions with minimum energy. We illustrate how this procedureenables improved performance over recent approaches across BabyAI and Atarienvironments. We further demonstrate unique benefits of our iterativeoptimization procedure, involving new task generalization, test-timeconstraints adaptation, and the ability to compose plans together. Projectwebsite: ", "output": "Planning with Sequence Models through Iterative Energy Minimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomaly detectors are widely used in industrial production to detect andlocalize unknown defects in query images. These detectors are trained onnominal images and have shown success in distinguishing anomalies from mostnormal samples. However, hard-nominal examples are scattered and far apart frommost normalities, they are often mistaken for anomalies by existing anomalydetectors. To address this problem, we propose a simple yet efficient method:textbf{H}ard Nominal textbf{E}xample-aware textbf{T}emplate textbf{M}utualtextbf{M}atching (HETMM). Specifically, textit{HETMM} aims to construct arobust prototype-based decision boundary, which can precisely distinguishbetween hard-nominal examples and anomalies, yielding fewer false-positive andmissed-detection rates. Moreover, textit{HETMM} mutually explores theanomalies in two directions between queries and the template set, and thus itis capable to capture the logical anomalies. This is a significant advantageover most anomaly detectors that frequently fail to detect logical anomalies.Additionally, to meet the speed-accuracy demands, we further proposetextbf{P}ixel-level textbf{T}emplate textbf{S}election (PTS) to streamlinethe original template set. textit{PTS} selects cluster centres andhard-nominal examples to form a tiny set, maintaining the original decisionboundaries. Comprehensive experiments on five real-world datasets demonstratethat our methods yield outperformance than existing advances under thereal-time inference speed. Furthermore, textit{HETMM} can be hot-updated byinserting novel samples, which may promptly address some incremental learningissues.", "output": "Hard Nominal Example-aware Template Mutual Matching for Industrial Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Field (NeRF) significantly degrades when only a limitednumber of views are available. To complement the lack of 3D information,depth-based models, such as DSNeRF and MonoSDF, explicitly assume theavailability of accurate depth maps of multiple views. They linearly scale theaccurate depth maps as supervision to guide the predicted depth of few-shotNeRFs. However, accurate depth maps are difficult and expensive to capture dueto wide-range depth distances in the wild.In this work, we present a new Sparse-view NeRF (SparseNeRF) framework thatexploits depth priors from real-world inaccurate observations. The inaccuratedepth observations are either from pre-trained depth models or coarse depthmaps of consumer-level depth sensors. Since coarse depth maps are not strictlyscaled to the ground-truth depth maps, we propose a simple yet effectiveconstraint, a local depth ranking method, on NeRFs such that the expected depthranking of the NeRF is consistent with that of the coarse depth maps in localpatches. To preserve the spatial continuity of the estimated depth of NeRF, wefurther propose a spatial continuity constraint to encourage the consistency ofthe expected depth continuity of NeRF with coarse depth maps. Surprisingly,with simple depth ranking constraints, SparseNeRF outperforms allstate-of-the-art few-shot NeRF methods (including depth-based models) onstandard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBDthat contains real-world depth maps from Azure Kinect, ZED 2, and iPhone 13Pro. Extensive experiments on NVS-RGBD dataset also validate the superiorityand generalizability of SparseNeRF. Project page is available at", "output": "SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel approach for modeling vegetation response to weather inEurope as measured by the Sentinel 2 satellite. Existing satellite imageryforecasting approaches focus on photorealistic quality of the multispectralimages, while derived vegetation dynamics have not yet received as muchattention. We leverage both spatial and temporal context by extendingstate-of-the-art video prediction methods with weather guidance. We extend theEarthNet2021 dataset to be suitable for vegetation modeling by introducing alearned cloud mask and an appropriate evaluation scheme. Qualitative andquantitative experiments demonstrate superior performance of our approach overa wide variety of baseline methods, including leading approaches to satelliteimagery forecasting. Additionally, we show how our modeled vegetation dynamicscan be leveraged in a downstream task: inferring gross primary productivity forcarbon monitoring. To the best of our knowledge, this work presents the firstmodels for continental-scale vegetation modeling at fine resolution able tocapture anomalies beyond the seasonal cycle, thereby paving the way forpredictive assessments of vegetation status.", "output": "Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present LLaMA-Adapter, a lightweight adaption method to efficientlyfine-tune LLaMA into an instruction-following model. Using 52K self-instructdemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters uponthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, andprepend them to the input text tokens at higher transformer layers. Then, azero-init attention mechanism with zero gating is proposed, which adaptivelyinjects the new instructional cues into LLaMA, while effectively preserves itspre-trained knowledge. With efficient training, LLaMA-Adapter generateshigh-quality responses, comparable to Alpaca with fully fine-tuned 7Bparameters. Furthermore, our approach can be simply extended to multi-modalinput, e.g., images, for image-conditioned LLaMA, which achieves superiorreasoning capacity on ScienceQA. We release our code at", "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a method for joint alignment of sparse in-the-wild imagecollections of an object category. Most prior works assume either ground-truthkeypoint annotations or a large dataset of images of a single object category.However, neither of the above assumptions hold true for the long-tail of theobjects present in the world. We present a self-supervised technique thatdirectly optimizes on a sparse collection of images of a particularobject/object category to obtain consistent dense correspondences across thecollection. We use pairwise nearest neighbors obtained from deep features of apre-trained vision transformer (ViT) model as noisy and sparse keypoint matchesand make them dense and accurate matches by optimizing a neural network thatjointly maps the image collection into a learned canonical grid. Experiments onCUB and SPair-71k benchmarks demonstrate that our method can produce globallyconsistent and higher quality correspondences across the image collection whencompared to existing self-supervised methods. Code and other material will bemade available at url{", "output": "ASIC: Aligning Sparse in-the-wild Image Collections."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Jointly matching multiple, non-rigidly deformed 3D shapes is a challenging,$mathcal{NP}$-hard problem. A perfect matching is necessarilycycle-consistent: Following the pairwise point correspondences along severalshapes must end up at the starting vertex of the original shape. Unfortunately,existing quantum shape-matching methods do not support multiple shapes and evenless cycle consistency. This paper addresses the open challenges and introducesthe first quantum-hybrid approach for 3D shape multi-matching; in addition, itis also cycle-consistent. Its iterative formulation is admissible to modernadiabatic quantum hardware and scales linearly with the total number of inputshapes. Both these characteristics are achieved by reducing the $N$-shape caseto a sequence of three-shape matchings, the derivation of which is our maintechnical contribution. Thanks to quantum annealing, high-quality solutionswith low energy are retrieved for the intermediate $mathcal{NP}$-hardobjectives. On benchmark datasets, the proposed approach significantlyoutperforms extensions to multi-shape matching of a previous quantum-hybridtwo-shape matching method and is on-par with classical multi-matching methods.", "output": "CCuantuMM: Cycle-Consistent Quantum-Hybrid Matching of Multiple Shapes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent wave of large-scale text-to-image diffusion models hasdramatically increased our text-based image generation abilities. These modelscan generate realistic images for a staggering variety of prompts and exhibitimpressive compositional generalization abilities. Almost all use cases thusfar have solely focused on sampling; however, diffusion models can also provideconditional density estimates, which are useful for tasks beyond imagegeneration. In this paper, we show that the density estimates from large-scaletext-to-image diffusion models like Stable Diffusion can be leveraged toperform zero-shot classification without any additional training. Ourgenerative approach to classification attains strong results on a variety ofbenchmarks and outperforms alternative methods of extracting knowledge fromdiffusion models. We also find that our diffusion-based approach has strongermultimodal relational reasoning abilities than competing contrastiveapproaches. Finally, we evaluate diffusion models trained on ImageNet and findthat they approach the performance of SOTA discriminative classifiers trainedon the same dataset, even with weak augmentations and no regularization.Results and visualizations at ", "output": "Your Diffusion Model is Secretly a Zero-Shot Classifier."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitousvisualization tool for analyzing multidimensional datasets. Despite theirpopularity, such scatterplots suffer from occlusion, especially wheninformative glyphs are used to represent data instances, potentiallyobfuscating critical information for the analysis under execution. Differentstrategies have been devised to address this issue, either producingoverlap-free layouts which lack the powerful capabilities of contemporary DRtechniques in uncovering interesting data patterns or eliminating overlaps as apost-processing strategy. Despite the good results of post-processingtechniques, most of the best methods typically expand or distort thescatterplot area, thus reducing glyphs' size (sometimes) to unreadabledimensions, defeating the purpose of removing overlaps. This paper presentsDistance Grid (DGrid), a novel post-processing strategy to remove overlaps fromDR layouts that faithfully preserves the original layout's characteristics andbounds the minimum glyph sizes. We show that DGrid surpasses thestate-of-the-art in overlap removal (through an extensive comparativeevaluation considering multiple different metrics) while also being 2 or 3orders of magnitude faster for large datasets.", "output": "A Grid-based Method for Removing Overlaps of Dimensionality Reduction Scatterplot Layouts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel ECGAN for the challenging semantic image synthesis task.Although considerable improvement has been achieved, the quality of synthesizedimages is far from satisfactory due to three largely unresolved challenges. 1)The semantic labels do not provide detailed structural information, making itdifficult to synthesize local details and structures. 2) The widely adopted CNNoperations such as convolution, down-sampling, and normalization usually causespatial resolution loss and thus cannot fully preserve the original semanticinformation, leading to semantically inconsistent results. 3) Existing semanticimage synthesis methods focus on modeling local semantic information from asingle input semantic layout. However, they ignore global semantic informationof multiple input semantic layouts, i.e., semantic cross-relations betweenpixels across different input layouts. To tackle 1), we propose to use edge asan intermediate representation which is further adopted to guide imagegeneration via a proposed attention guided edge transfer module. Edgeinformation is produced by a convolutional generator and introduces detailedstructure information. To tackle 2), we design an effective module toselectively highlight class-dependent feature maps according to the originalsemantic layout to preserve the semantic information. To tackle 3), inspired bycurrent methods in contrastive learning, we propose a novel contrastivelearning method, which aims to enforce pixel embeddings belonging to the samesemantic class to generate more similar image content than those from differentclasses. Doing so can capture more semantic relations by explicitly exploringthe structures of labeled pixels from multiple input semantic layouts.Experiments on three challenging datasets show that our ECGAN achievessignificantly better results than state-of-the-art methods.", "output": "Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot learning amounts to learning representations and acquiring knowledgesuch that novel tasks may be solved with both supervision and data beinglimited. Improved performance is possible by transductive inference, where theentire test set is available concurrently, and semi-supervised learning, wheremore unlabeled data is available. Focusing on these two settings, we introducea new algorithm that leverages the manifold structure of the labeled andunlabeled data distribution to predict pseudo-labels, while balancing overclasses and using the loss value distribution of a limited-capacity classifierto select the cleanest labels, iteratively improving the quality ofpseudo-labels. Our solution surpasses or matches the state of the art resultson four benchmark datasets, namely miniImageNet, tieredImageNet, CUB andCIFAR-FS, while being robust over feature space pre-processing and the quantityof available data. The publicly available source code can be found in", "output": "Iterative label cleaning for transductive and semi-supervised few-shot learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Compatible features enable the direct comparison of old and new learnedfeatures allowing to use them interchangeably over time. In visual searchsystems, this eliminates the need to extract new features from the gallery-setwhen the representation model is upgraded with novel data. This has a big valuein real applications as re-indexing the gallery-set can be computationallyexpensive when the gallery-set is large, or even infeasible due to privacy orother concerns of the application. In this paper, we propose CoReS, a newtraining procedure to learn representations that are textit{compatible} withthose previously learned, grounding on the stationarity of the features asprovided by fixed classifiers based on polytopes. With this solution, classesare maximally separated in the representation space and maintain their spatialconfiguration stationary as new classes are added, so that there is no need tolearn any mappings between representations nor to impose pairwise training withthe previously learned model. We demonstrate that our training procedurelargely outperforms the current state of the art and is particularly effectivein the case of multiple upgrades of the training-set, which is the typical casein real applications.", "output": "CoReS: Compatible Representations via Stationarity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In some scenarios, classifier requires detecting out-of-distribution samplesfar from its training data. With desirable characteristics, reconstructionautoencoder-based methods deal with this problem by using input reconstructionerror as a metric of novelty vs. normality. We formulate the essence of suchapproach as a quadruplet domain translation with an intrinsic bias to onlyquery for a proxy of conditional data uncertainty. Accordingly, an improvementdirection is formalized as maximumly compressing the autoencoder's latent spacewhile ensuring its reconstructive power for acting as a described domaintranslator. From it, strategies are introduced including semanticreconstruction, data certainty decomposition and normalized L2 distance tosubstantially improve original methods, which together establishstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR ofCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our methodworks without any additional data, hard-to-implement structure, time-consumingpipeline, and even harming the classification accuracy of known classes.", "output": "Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new multi-sensor dataset for multi-view 3D surfacereconstruction. It includes registered RGB and depth data from sensors ofdifferent resolutions and modalities: smartphones, Intel RealSense, MicrosoftKinect, industrial cameras, and structured-light scanner. The scenes areselected to emphasize a diverse set of material properties challenging forexisting algorithms. We provide around 1.4 million images of 107 differentscenes acquired from 100 viewing directions under 14 lighting conditions. Weexpect our dataset will be useful for evaluation and training of 3Dreconstruction algorithms and for related tasks. The dataset is available atskoltech3d.appliedai.tech.", "output": "Multi-sensor large-scale dataset for multi-view 3D reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sponge examples are test-time inputs carefully optimized to increase energyconsumption and latency of neural networks when deployed on hardwareaccelerators. In this work, we are the first to demonstrate that spongeexamples can also be injected at training time, via an attack that we callsponge poisoning. This attack allows one to increase the energy consumption andlatency of machine-learning models indiscriminately on each test-time input. Wepresent a novel formalization for sponge poisoning, overcoming the limitationsrelated to the optimization of test-time sponge examples, and show that thisattack is possible even if the attacker only controls a few model updates; forinstance, if model training is outsourced to an untrusted third-party ordistributed via federated learning. Our extensive experimental analysis showsthat sponge poisoning can almost completely vanish the effect of hardwareaccelerators. We also analyze the activations of poisoned models, identifyingwhich components are more vulnerable to this attack. Finally, we examine thefeasibility of countermeasures against sponge poisoning to decrease energyconsumption, showing that sanitization methods may be overly expensive for mostof the users.", "output": "Energy-Latency Attacks via Sponge Poisoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The DETR-like segmentors have underpinned the most recent breakthroughs insemantic segmentation, which end-to-end train a set of queries representing theclass prototypes or target segments. Recently, masked attention is proposed torestrict each query to only attend to the foreground regions predicted by thepreceding decoder block for easier optimization. Although promising, it relieson the learnable parameterized positional queries which tend to encode thedataset statistics, leading to inaccurate localization for distinct individualqueries. In this paper, we propose a simple yet effective query design forsemantic segmentation termed Dynamic Focus-aware Positional Queries (DFPQ),which dynamically generates positional queries conditioned on thecross-attention scores from the preceding decoder block and the positionalencodings for the corresponding image features, simultaneously. Therefore, ourDFPQ preserves rich localization information for the target segments andprovides accurate and fine-grained positional priors. In addition, we proposeto efficiently deal with high-resolution cross-attention by only aggregatingthe contextual tokens based on the low-resolution cross-attention scores toperform local relation aggregation. Extensive experiments on ADE20K andCityscapes show that with the two modifications on Mask2former, our frameworkachieves SOTA performance and outperforms Mask2former by clear margins of 1.1%,1.9%, and 1.1% single-scale mIoU with ResNet-50, Swin-T, and Swin-B backboneson the ADE20K validation set, respectively. Source code is available at", "output": "Dynamic Focus-aware Positional Queries for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Early diagnosis and intervention are clinically considered the paramount partof treating cerebral palsy (CP), so it is essential to design an efficient andinterpretable automatic prediction system for CP. We highlight a significantdifference between CP infants' frequency of human movement and that of thehealthy group, which improves prediction performance. However, the existingdeep learning-based methods did not use the frequency information of infants'movement for CP prediction. This paper proposes a frequency attention informedgraph convolutional network and validates it on two consumer-grade RGB videodatasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequencyattention module aids in improving both classification performance and systeminterpretability. In addition, we design a frequency-binning method thatretains the critical frequency of the human joint position data while filteringthe noise. Our prediction performance achieves state-of-the-art research onboth datasets. Our work demonstrates the effectiveness of frequency informationin supporting the prediction of CP non-intrusively and provides a way forsupporting the early diagnosis of CP in the resource-limited regions where theclinical resources are not abundant.", "output": "Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-resolution (HR) magnetic resonance imaging is critical in aiding doctorsin their diagnoses and image-guided treatments. However, acquiring HR imagescan be time-consuming and costly. Consequently, deep learning-basedsuper-resolution reconstruction (SRR) has emerged as a promising solution forgenerating super-resolution (SR) images from low-resolution (LR) images.Unfortunately, training such neural networks requires aligned authentic HR andLR image pairs, which are challenging to obtain due to patient movements duringand between image acquisitions. While rigid movements of hard tissues can becorrected with image registration, aligning deformed soft tissues is complex,making it impractical to train neural networks with authentic HR and LR imagepairs. Previous studies have focused on SRR using authentic HR images anddown-sampled synthetic LR images. However, the difference in degradationrepresentations between synthetic and authentic LR images suppresses thequality of SR images reconstructed from authentic LR images. To address thisissue, we propose a novel Unsupervised Degradation Adaptation Network (UDEAN).Our network consists of a degradation learning network and an SRR network. Thedegradation learning network downsamples the HR images using the degradationrepresentation learned from the misaligned or unpaired LR images. The SRRnetwork then learns the mapping from the down-sampled HR images to the originalones. Experimental results show that our method outperforms state-of-the-artnetworks and is a promising solution to the challenges in clinical settings.", "output": "Unsupervised Representation Learning for 3D MRI Super Resolution with Degradation Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In point cloud analysis, point-based methods have rapidly developed in recentyears. These methods have recently focused on concise MLP structures, such asPointNeXt, which have demonstrated competitiveness with Convolutional andTransformer structures. However, standard MLPs are limited in their ability toextract local features effectively. To address this limitation, we propose aVector-oriented Point Set Abstraction that can aggregate neighboring featuresthrough higher-dimensional vectors. To facilitate network optimization, weconstruct a transformation from scalar to vector using independent angles basedon 3D vector rotations. Finally, we develop a PointVector model that followsthe structure of PointNeXt. Our experimental results demonstrate thatPointVector achieves state-of-the-art performance $textbf{72.3% mIOU}$ on theS3DIS Area 5 and $textbf{78.4% mIOU}$ on the S3DIS (6-fold cross-validation)with only $textbf{58%}$ model parameters of PointNeXt. We hope our work willhelp the exploration of concise and effective feature representations. The codewill be released soon.", "output": "PointVector: A Vector Representation In Point Cloud Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new imaging technique, swept-angle synthetic wavelengthinterferometry, for full-field micron-scale 3D sensing. As in conventionalsynthetic wavelength interferometry, our technique uses light consisting of twonarrowly-separated optical wavelengths, resulting in per-pixel interferometricmeasurements whose phase encodes scene depth. Our technique additionally uses anew type of light source that, by emulating spatially-incoherent illumination,makes interferometric measurements insensitive to aberrations and (sub)surfacescattering, effects that corrupt phase measurements. The resulting techniquecombines the robustness to such corruptions of scanning interferometric setups,with the speed of full-field interferometric setups. Overall, our technique canrecover full-frame depth at a lateral and axial resolution of 5 microns, atframe rates of 5 Hz, even under strong ambient light. We build an experimentalprototype, and use it to demonstrate these capabilities by scanning a varietyof objects, including objects representative of applications in inspection andfabrication, and objects that contain challenging light scattering effects.", "output": "Swept-Angle Synthetic Wavelength Interferometry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computer-aided diagnosis (CAD) can help pathologists improve diagnosticaccuracy together with consistency and repeatability for cancers. However, theCAD models trained with the histopathological images only from a single center(hospital) generally suffer from the generalization problem due to thestraining inconsistencies among different centers. In this work, we propose apseudo-data based self-supervised federated learning (FL) framework, namedSSL-FT-BT, to improve both the diagnostic accuracy and generalization of CADmodels. Specifically, the pseudo histopathological images are generated fromeach center, which contains inherent and specific properties corresponding tothe real images in this center, but does not include the privacy information.These pseudo images are then shared in the central server for self-supervisedlearning (SSL). A multi-task SSL is then designed to fully learn both thecenter-specific information and common inherent representation according to thedata characteristics. Moreover, a novel Barlow Twins based FL (FL-BT) algorithmis proposed to improve the local training for the CAD model in each center byconducting contrastive learning, which benefits the optimization of the globalmodel in the FL procedure. The experimental results on three publichistopathological image datasets indicate the effectiveness of the proposedSSL-FL-BT on both diagnostic accuracy and generalization.", "output": "Pseudo-Data based Self-Supervised Federated Learning for Classification of Histopathological Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been achallenging task. To further enhance WSI visual representations, existingmethods have explored image pyramids, instead of single-resolution images, inWSIs. In spite of this, they still face two major problems: high computationalcost and the unnoticed semantical gap in multi-resolution feature fusion. Totackle these problems, this paper proposes to efficiently exploit WSI pyramidsfrom a new perspective, the dual-stream network with cross-attention (DSCA).Our key idea is to utilize two sub-streams to process the WSI patches with tworesolutions, where a square pooling is devised in a high-resolution stream tosignificantly reduce computational costs, and a cross-attention-based method isproposed to properly handle the fusion of dual-stream features. We validate ourDSCA on three publicly-available datasets with a total number of 3,101 WSIsfrom 1,911 patients. Our experiments and ablation studies verify that (i) theproposed DSCA could outperform existing state-of-the-art methods in cancerprognosis, by an average C-Index improvement of around 4.6%; (ii) our DSCAnetwork is more efficient in computation -- it has more learnable parameters(6.31M vs. 860.18K) but less computational costs (2.51G vs. 4.94G), compared toa typical existing multi-resolution network. (iii) the key components of DSCA,dual-stream and cross-attention, indeed contribute to our model's performance,gaining an average C-Index rise of around 2.0% while maintaining arelatively-small computational load. Our DSCA could serve as an alternative andeffective tool for WSI-based cancer prognosis.", "output": "DSCA: A Dual-Stream Network with Cross-Attention on Whole-Slide Image Pyramids for Cancer Prognosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most existing few-shot learning (FSL) methods require a large amount oflabeled data in meta-training, which is a major limit. To reduce therequirement of labels, a semi-supervised meta-training (SSMT) setting has beenproposed for FSL, which includes only a few labeled samples and numbers ofunlabeled samples in base classes. However, existing methods under this settingrequire class-aware sample selection from the unlabeled set, which violates theassumption of unlabeled set. In this paper, we propose a practicalsemi-supervised meta-training setting with truly unlabeled data to facilitatethe applications of FSL in realistic scenarios. To better utilize both thelabeled and truly unlabeled data, we propose a simple and effectivemeta-training framework, called pseudo-labeling based meta-learning (PLML).Firstly, we train a classifier via common semi-supervised learning (SSL) anduse it to obtain the pseudo-labels of unlabeled data. Then we build few-shottasks from labeled and pseudo-labeled data and design a novel finetuning methodwith feature smoothing and noise suppression to better learn the FSL model fromnoise labels. Surprisingly, through extensive experiments across two FSLdatasets, we find that this simple meta-training framework effectively preventsthe performance degradation of various FSL models under limited labeled data,and also significantly outperforms the state-of-the-art SSMT models. Besides,benefiting from meta-training, our method also improves two representative SSLalgorithms as well.", "output": "Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for Few-Shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we advance the neural head avatar technology to the megapixelresolution while focusing on the particularly challenging task of cross-drivingsynthesis, i.e., when the appearance of the driving image is substantiallydifferent from the animated source image. We propose a set of new neuralarchitectures and training methods that can leverage both medium-resolutionvideo data and high-resolution image data to achieve the desired levels ofrendered image quality and generalization to novel views and motion. Wedemonstrate that suggested architectures and methods produce convincinghigh-resolution neural avatars, outperforming the competitors in thecross-driving scenario. Lastly, we show how a trained high-resolution neuralavatar model can be distilled into a lightweight student model which runs inreal-time and locks the identities of neural avatars to several dozens ofpre-defined source images. Real-time operation and identity lock are essentialfor many practical applications head avatar systems.", "output": "MegaPortraits: One-shot Megapixel Neural Head Avatars."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the problem of out-of-distribution (OOD) detection for the task ofobject detection. We show that residual convolutional layers with batchnormalisation produce Sensitivity-Aware FEatures (SAFE) that are consistentlypowerful for distinguishing in-distribution from out-of-distributiondetections. By extracting SAFE vectors for every detected object, and traininga multilayer perceptron on the surrogate task of distinguishing adversariallyperturbed from clean in-distribution examples, we circumvent the need forrealistic OOD training data, computationally expensive generative models, orretraining of the base object detector. SAFE outperforms the state-of-the-artOOD object detectors on multiple benchmarks by large margins, e.g. reducing theFPR95 by an absolute 30.6% from 48.3% to 17.7% on the OpenImages dataset.", "output": "SAFE: Sensitivity-Aware Features for Out-of-Distribution Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we dive deep into the inconsistency of pseudo targets insemi-supervised object detection (SSOD). Our core observation is that theoscillating pseudo-targets undermine the training of an accurate detector. Itinjects noise into the student's training, leading to severe overfittingproblems. Therefore, we propose a systematic solution, termedConsistentTeacher, to reduce the inconsistency. First, adaptive anchorassignment~(ASA) substitutes the static IoU-based strategy, which enables thestudent network to be resistant to noisy pseudo-bounding boxes. Then wecalibrate the subtask predictions by designing a 3D feature alignmentmodule~(FAM-3D). It allows each classification feature to adaptively query theoptimal feature vector for the regression task at arbitrary scales andlocations. Lastly, a Gaussian Mixture Model (GMM) dynamically revises the scorethreshold of pseudo-bboxes, which stabilizes the number of ground truths at anearly stage and remedies the unreliable supervision signal during training.ConsistentTeacher provides strong results on a large range of SSOD evaluations.It achieves 40.0 mAP with ResNet-50 backbone given only 10% of annotatedMS-COCO data, which surpasses previous baselines using pseudo labels by around3 mAP. When trained on fully annotated MS-COCO with additional unlabeled data,the performance further increases to 47.7 mAP. Our code is available aturl{", "output": "Consistent-Teacher: Towards Reducing Inconsistent Pseudo-targets in Semi-supervised Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cancer is the second leading cause of death in the world. Diagnosing cancerearly on can save many lives. Pathologists have to look at tissue microarray(TMA) images manually to identify tumors, which can be time-consuming,inconsistent and subjective. Existing algorithms that automatically detecttumors have either not achieved the accuracy level of a pathologist or requiresubstantial human involvements. A major challenge is that TMA images withdifferent shapes, sizes, and locations can have the same score. Learningstaining patterns in TMA images requires a huge number of images, which areseverely limited due to privacy concerns and regulations in medicalorganizations. TMA images from different cancer types may have commoncharacteristics that could provide valuable information, but using themdirectly harms the accuracy. By selective transfer learning from multiple smallauxiliary sets, the proposed algorithm is able to extract knowledge from tissueimages showing a ``similar\" scoring pattern but with different cancer types.Remarkably, transfer learning has made it possible for the algorithm to breakthe critical accuracy barrier -- the proposed algorithm reports an accuracy of75.9% on breast cancer TMA images from the Stanford Tissue Microarray Database,achieving the 75% accuracy level of pathologists. This will allow pathologiststo confidently use automatic algorithms to assist them in recognizing tumorsconsistently with a higher accuracy in real time.", "output": "Automatically Score Tissue Images Like a Pathologist by Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For long-tailed classification, most works often pretrain a big model on alarge-scale dataset, and then fine-tune the whole model for adapting tolong-tailed data. Though promising, fine-tuning the whole pretrained modeltends to suffer from high cost in computation and deployment of differentmodels for different tasks, as well as weakened generalization ability foroverfitting to certain features of long-tailed data. To alleviate these issues,we propose an effective Long-tailed Prompt Tuning method for long-tailedclassification. LPT introduces several trainable prompts into a frozenpretrained model to adapt it to long-tailed data. For better effectiveness, wedivide prompts into two groups: 1) a shared prompt for the whole long-taileddataset to learn general features and to adapt a pretrained model into targetdomain; and 2) group-specific prompts to gather group-specific features for thesamples which have similar features and also to empower the pretrained modelwith discrimination ability. Then we design a two-phase training paradigm tolearn these prompts. In phase 1, we train the shared prompt via supervisedprompt tuning to adapt a pretrained model to the desired long-tailed domain. Inphase 2, we use the learnt shared prompt as query to select a small bestmatched set for a group of similar samples from the group-specific prompt setto dig the common features of these similar samples, then optimize theseprompts with dual sampling strategy and asymmetric GCL loss. By onlyfine-tuning a few prompts while fixing the pretrained model, LPT can reducetraining and deployment cost by storing a few prompts, and enjoys a stronggeneralization ability of the pretrained model. Experiments show that onvarious long-tailed benchmarks, with only ~1.1% extra parameters, LPT achievescomparable performance than previous whole model fine-tuning methods, and ismore robust to domain-shift.", "output": "LPT: Long-tailed Prompt Tuning for Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multiple near frontal-parallel planes based depth representation demonstratedimpressive results in self-supervised monocular depth estimation (MDE).Whereas, such a representation would cause the discontinuity of the ground asit is perpendicular to the frontal-parallel planes, which is detrimental to theidentification of drivable space in autonomous driving. In this paper, wepropose the PlaneDepth, a novel orthogonal planes based presentation, includingvertical planes and ground planes. PlaneDepth estimates the depth distributionusing a Laplacian Mixture Model based on orthogonal planes for an input image.These planes are used to synthesize a reference view to provide theself-supervision signal. Further, we find that the widely used resizing andcropping data augmentation breaks the orthogonality assumptions, leading toinferior plane predictions. We address this problem by explicitly constructingthe resizing cropping transformation to rectify the predefined planes andpredicted camera pose. Moreover, we propose an augmented self-distillation losssupervised with a bilateral occlusion mask to boost the robustness oforthogonal planes representation for occlusions. Thanks to our orthogonalplanes representation, we can extract the ground plane in an unsupervisedmanner, which is important for autonomous driving. Extensive experiments on theKITTI dataset demonstrate the effectiveness and efficiency of our method. Thecode is available at ", "output": "PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress in diffusion models has revolutionized the popular technologyof text-to-image generation. While existing approaches could producephotorealistic high-resolution images with text conditions, there are stillseveral open problems to be solved, which limits the further improvement ofimage fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, alarge-scale Chinese text-to-image diffusion model, to progressively upgrade thequality of generated images by: (1) incorporating fine-grained textual andvisual knowledge of key elements in the scene, and (2) utilizing differentdenoising experts at different denoising stages. With the proposed mechanisms,ERNIE-ViLG 2.0 not only achieves a new state-of-the-art on MS-COCO withzero-shot FID score of 6.75, but also significantly outperforms recent modelsin terms of image fidelity and image-text alignment, with side-by-side humanevaluation on the bilingual prompt set ViLG-300.", "output": "ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To effectively interrogate UAV-based images for detecting objects ofinterest, such as humans, it is essential to acquire large-scale UAV-baseddatasets that include human instances with various poses captured from widelyvarying viewing angles. As a viable alternative to laborious and costly datacuration, we introduce Progressive Transformation Learning (PTL), whichgradually augments a training dataset by adding transformed virtual images withenhanced realism. Generally, a virtual2real transformation generator in theconditional GAN framework suffers from quality degradation when a large domaingap exists between real and virtual images. To deal with the domain gap, PTLtakes a novel approach that progressively iterates the following three steps:1) select a subset from a pool of virtual images according to the domain gap,2) transform the selected virtual images to enhance realism, and 3) add thetransformed virtual images to the training set while removing them from thepool. In PTL, accurately quantifying the domain gap is critical. To do that, wetheoretically demonstrate that the feature representation space of a givenobject detector can be modeled as a multivariate Gaussian distribution fromwhich the Mahalanobis distance between a virtual object and the Gaussiandistribution of each object category in the representation space can be readilycomputed. Experiments show that PTL results in a substantial performanceincrease over the baseline, especially in the small data and the cross-domainregime.", "output": "Progressive Transformation Learning for Leveraging Virtual Images in Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Removing soft and self shadows that lack clear boundaries from a single imageis still challenging. Self shadows are shadows that are cast on the objectitself. Most existing methods rely on binary shadow masks, without consideringthe ambiguous boundaries of soft and self shadows. In this paper, we presentDeS3, a method that removes hard, soft and self shadows based on the self-tunedViT feature similarity and color convergence. Our novel ViT similarity lossutilizes features extracted from a pre-trained Vision Transformer. This losshelps guide the reverse diffusion process towards recovering scene structures.We also introduce a color convergence loss to constrain the surface colors inthe reverse inference process to avoid any color shifts. Our DeS3 is able todifferentiate shadow regions from the underlying objects, as well as shadowregions from the object casting the shadow. This capability enables DeS3 tobetter recover the structures of objects even when they are partially occludedby shadows. Different from existing methods that rely on constraints during thetraining phase, we incorporate the ViT similarity and color convergence lossduring the sampling stage. This enables our DeS3 model to effectively integrateits strong modeling capabilities with input-specific knowledge in a self-tunedmanner. Our method outperforms state-of-the-art methods on the SRD, AISTD,LRSS, USR and UIUC datasets, removing hard, soft, and self shadows robustly.Specifically, our method outperforms the SOTA method by 20% of the RMSE of thewhole image on the SRD dataset.", "output": "DeS3: Attention-driven Self and Soft Shadow Removal using ViT Similarity and Color Convergence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting the future motion of dynamic agents is of paramount importance toensure safety or assess risks in motion planning for autonomous robots. In thispaper, we propose a two-stage motion prediction method, referred to as R-Pred,that effectively utilizes both the scene and interaction context using acascade of the initial trajectory proposal network and the trajectoryrefinement network. The initial trajectory proposal network produces Mtrajectory proposals corresponding to M modes of a future trajectorydistribution. The trajectory refinement network enhances each of M proposalsusing 1) the tube-query scene attention (TQSA) and 2) the proposal-levelinteraction attention (PIA). TQSA uses tube-queries to aggregate the localscene context features pooled from proximity around the trajectory proposals ofinterest. PIA further enhances the trajectory proposals by modeling inter-agentinteractions using a group of trajectory proposals selected based on theirdistances from neighboring agents. Our experiments conducted on the Argoverseand nuScenes datasets demonstrate that the proposed refinement network providessignificant performance improvements compared to the single-stage baseline andthat R-Pred achieves state-of-the-art performance in some categories of thebenchmark.", "output": "Two-Stage Context-Aware model for Predicting Future Motion of Dynamic Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge-based visual question answering (VQA) involves questions thatrequire world knowledge beyond the image to yield the correct answer. Largelanguage models (LMs) like GPT-3 are particularly helpful for this task becauseof their strong knowledge retrieval and reasoning capabilities. To enable LM tounderstand images, prior work uses a captioning model to convert images intotext. However, when summarizing an image in a single caption sentence, whichvisual entities to describe are often underspecified. Generic image captionsoften miss visual details essential for the LM to answer visual questionscorrectly. To address this challenge, we propose PromptCap (Prompt-guided imageCaptioning), a captioning model designed to serve as a better connector betweenimages and black-box LMs. Different from generic captions, PromptCap takes anatural-language prompt to control the visual entities to describe in thegenerated caption. The prompt contains a question that the caption should aidin answering. To avoid extra annotation, PromptCap is trained by examplessynthesized with GPT-3 and existing datasets. We demonstrate PromptCap'seffectiveness on an existing pipeline in which GPT-3 is prompted with imagecaptions to carry out VQA. PromptCap outperforms generic captions by a largemargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show thatPromptCap generalizes well to unseen domains.", "output": "PromptCap: Prompt-Guided Task-Aware Image Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several safety-critical applications such as self-navigation, health care,and industrial control systems use embedded systems as their core. Recentadvancements in Neural Networks (NNs) in approximating complex functions makethem well-suited for these domains. However, the compute-intensive nature ofNNs limits their deployment and training in embedded systems with limitedcomputation and storage capacities. Moreover, the adversarial vulnerability ofNNs challenges their use in safety-critical scenarios. Hence, developing sparsemodels having robustness guarantees while leveraging fewer resources duringtraining is critical in expanding NNs' use in safety-critical andresource-constrained embedding system settings. This paper presents'VeriSparse'-- a framework to search verified locally robust sparse networksstarting from a random sparse initialization (i.e., scratch). VeriSparseobtains sparse NNs exhibiting similar or higher verified local robustness,requiring one-third of the training time compared to the state-of-the-artapproaches. Furthermore, VeriSparse performs both structured and unstructuredsparsification, enabling storage, computing-resource, and computation timereduction during inference generation. Thus, it facilitates theresource-constraint embedding platforms to leverage verified robust NN models,expanding their scope to safety-critical, real-time, and edge applications. Weexhaustively investigated VeriSparse's efficacy and generalizability byevaluating various benchmark and application-specific datasets across severalmodel architectures.", "output": "VeriSparse: Training Verified Locally Robust Sparse Neural Networks from Scratch."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Numerous pose-guided human editing methods have been explored by the visioncommunity due to their extensive practical applications. However, most of thesemethods still use an image-to-image formulation in which a single image isgiven as input to produce an edited image as output. This objective becomesill-defined in cases when the target pose differs significantly from the inputpose. Existing methods then resort to in-painting or style transfer to handleocclusions and preserve content. In this paper, we explore the utilization ofmultiple views to minimize the issue of missing information and generate anaccurate representation of the underlying human model. To fuse knowledge frommultiple viewpoints, we design a multi-view fusion network that takes the posekey points and texture from multiple source images and generates an explainableper-pixel appearance retrieval map. Thereafter, the encodings from a separatenetwork (trained on a single-view human reposing task) are merged in the latentspace. This enables us to generate accurate, precise, and visually coherentimages for different editing tasks. We show the application of our network ontwo newly proposed tasks - Multi-view human reposing and Mix&amp;Match Human Imagegeneration. Additionally, we study the limitations of single-view editing andscenarios in which multi-view provides a better alternative.", "output": "UMFuse: Unified Multi View Fusion for Human Editing applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce an interferometric technique for passive time-of-flight imagingand depth sensing at micrometer axial resolutions. Our technique uses afull-field Michelson interferometer, modified to use sunlight as the only lightsource. The large spectral bandwidth of sunlight makes it possible to acquiremicrometer-resolution time-resolved scene responses, through a simple axialscanning operation. Additionally, the angular bandwidth of sunlight makes itpossible to capture time-of-flight measurements insensitive to indirectillumination effects, such as interreflections and subsurface scattering. Webuild an experimental prototype that we operate outdoors, under directsunlight, and in adverse environment conditions such as machine vibrations andvehicle traffic. We use this prototype to demonstrate, for the first time,passive imaging capabilities such as micrometer-scale depth sensing robust toindirect illumination, direct-only imaging, and imaging through diffusers.", "output": "Passive Micron-scale Time-of-Flight with Sunlight Interferometry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Performing super-resolution of a depth image using the guidance from an RGBimage is a problem that concerns several fields, such as robotics, medicalimaging, and remote sensing. While deep learning methods have achieved goodresults in this problem, recent work highlighted the value of combining modernmethods with more formal frameworks. In this work, we propose a novel approachwhich combines guided anisotropic diffusion with a deep convolutional networkand advances the state of the art for guided depth super-resolution. The edgetransferring/enhancing properties of the diffusion are boosted by thecontextual reasoning capabilities of modern networks, and a strict adjustmentstep guarantees perfect adherence to the source image. We achieve unprecedentedresults in three commonly used benchmarks for guided depth super-resolution.The performance gain compared to other methods is the largest at larger scales,such as x32 scaling. Code( for the proposed methodis available to promote reproducibility of our results.", "output": "Guided Depth Super-Resolution by Deep Anisotropic Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce PLIKS (Pseudo-Linear Inverse Kinematic Solver) forreconstruction of a 3D mesh of the human body from a single 2D image. Currenttechniques directly regress the shape, pose, and translation of a parametricmodel from an input image through a non-linear mapping with minimal flexibilityto any external influences. We approach the task as a model-in-the-loopoptimization problem. PLIKS is built on a linearized formulation of theparametric SMPL model. Using PLIKS, we can analytically reconstruct the humanmodel via 2D pixel-aligned vertices. This enables us with the flexibility touse accurate camera calibration information when available. PLIKS offers aneasy way to introduce additional constraints such as shape and translation. Wepresent quantitative evaluations which confirm that PLIKS achieves moreaccurate reconstruction with greater than 10% improvement compared to otherstate-of-the-art methods with respect to the standard 3D human pose and shapebenchmarks while also obtaining a reconstruction error improvement of 12.9 mmon the newer AGORA dataset.", "output": "PLIKS: A Pseudo-Linear Inverse Kinematic Solver for 3D Human Body Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural radiance-density field methods have become increasingly popular forthe task of novel-view rendering. Their recent extension to hash-basedpositional encoding ensures fast training and inference with visually pleasingresults. However, density-based methods struggle with recovering accuratesurface geometry. Hybrid methods alleviate this issue by optimizing the densitybased on an underlying SDF. However, current SDF methods are overly smooth andmiss fine geometric details. In this work, we combine the strengths of thesetwo lines of work in a novel hash-based implicit surface representation. Wepropose improvements to the two areas by replacing the voxel hash encoding witha permutohedral lattice which optimizes faster, especially for higherdimensions. We additionally propose a regularization scheme which is crucialfor recovering high-frequency geometric detail. We evaluate our method onmultiple datasets and show that we can recover geometric detail at the level ofpores and wrinkles while using only RGB images for supervision. Furthermore,using sphere tracing we can render novel views at 30 fps on an RTX 3090. Codeis publicly available at: ", "output": "PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal action localization (TAL) requires long-form reasoning to predictactions of various durations and complex content. Given limited GPU memory,training TAL end to end (i.e., from videos to predictions) on long videos is asignificant challenge. Most methods can only train on pre-extracted featureswithout optimizing them for the localization problem, consequently limitinglocalization performance. In this work, to extend the potential in TALnetworks, we propose a novel end-to-end method Re2TAL, which rewires pretrainedvideo backbones for reversible TAL. Re2TAL builds a backbone with reversiblemodules, where the input can be recovered from the output such that the bulkyintermediate activations can be cleared from memory during training. Instead ofdesigning one single type of reversible module, we propose a network rewiringmechanism, to transform any module with a residual connection to a reversiblemodule without changing any parameters. This provides two benefits: (1) a largevariety of reversible networks are easily obtained from existing and evenfuture model designs, and (2) the reversible models require much less trainingeffort as they reuse the pre-trained parameters of their originalnon-reversible versions. Re2TAL, only using the RGB modality, reaches 37.01%average mAP on ActivityNet-v1.3, a new state-of-the-art record, and mAP 64.9%at tIoU=0.5 on THUMOS-14, outperforming all other RGB-only methods.", "output": "Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic editing of images is the fundamental goal of computer vision.Although deep learning methods, such as generative adversarial networks (GANs),are capable of producing high-quality images, they often do not have aninherent way of editing generated images semantically. Recent studies haveinvestigated a way of manipulating the latent variable to determine the imagesto be generated. However, methods that assume linear semantic arithmetic havecertain limitations in terms of the quality of image editing, whereas methodsthat discover nonlinear semantic pathways provide non-commutative editing,which is inconsistent when applied in different orders. This study proposes anovel method called deep curvilinear editing (DeCurvEd) to determine semanticcommuting vector fields on the latent space. We theoretically demonstrate thatowing to commutativity, the editing of multiple attributes depends only on thequantities and not on the order. Furthermore, we experimentally demonstratethat compared to previous methods, the nonlinear and commutative nature ofDeCurvEd facilitates the disentanglement of image attributes and provideshigher-quality editing.", "output": "Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a lightweight network to improve descriptors of keypoints withinthe same image. The network takes the original descriptors and the geometricproperties of keypoints as the input, and uses an MLP-based self-boosting stageand a Transformer-based cross-boosting stage to enhance the descriptors. Theboosted descriptors can be either real-valued or binary ones. We use theproposed network to boost both hand-crafted (ORB, SIFT) and thestate-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluatethem on image matching, visual localization, and structure-from-motion tasks.The results show that our method significantly improves the performance of eachtask, particularly in challenging cases such as large illumination changes orrepetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms onembedded GPU to process 2000 features, which is fast enough to be applied to apractical system. The code and trained weights are publicly available atgithub.com/SJTU-ViSYS/FeatureBooster.", "output": "FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address 2D floorplan reconstruction from 3D scans. Existing approachestypically employ heuristically designed multi-stage pipelines. Instead, weformulate floorplan reconstruction as a single-stage structured predictiontask: find a variable-size set of polygons, which in turn are variable-lengthsequences of ordered vertices. To solve it we develop a novel Transformerarchitecture that generates polygons of multiple rooms in parallel, in aholistic manner without hand-crafted intermediate stages. The model featurestwo-level queries for polygons and corners, and includes polygon matching tomake the network end-to-end trainable. Our method achieves a newstate-of-the-art for two challenging datasets, Structured3D and SceneCAD, alongwith significantly faster inference than previous methods. Moreover, it canreadily be extended to predict additional information, i.e., semantic roomtypes and architectural elements like doors and windows. Our code and modelsare available at: ", "output": "Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monocular 3D human pose estimation is quite challenging due to the inherentambiguity and occlusion, which often lead to high uncertainty andindeterminacy. On the other hand, diffusion models have recently emerged as aneffective tool for generating high-quality images from noise. Inspired by theircapability, we explore a novel pose estimation framework (DiffPose) thatformulates 3D pose estimation as a reverse diffusion process. We incorporatenovel designs into our DiffPose to facilitate the diffusion process for 3D poseestimation: a pose-specific initialization of pose uncertainty distributions, aGaussian Mixture Model-based forward diffusion process, and acontext-conditioned reverse diffusion process. Our proposed DiffPosesignificantly outperforms existing methods on the widely used pose estimationbenchmarks Human3.6M and MPI-INF-3DHP. Project page:", "output": "DiffPose: Toward More Reliable 3D Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current differentiable renderers provide light transport gradients withrespect to arbitrary scene parameters. However, the mere existence of thesegradients does not guarantee useful update steps in an optimization. Instead,inverse rendering might not converge due to inherent plateaus, i.e., regions ofzero gradient, in the objective function. We propose to alleviate this byconvolving the high-dimensional rendering function that maps scene parametersto images with an additional kernel that blurs the parameter space. We describetwo Monte Carlo estimators to compute plateau-free gradients efficiently, i.e.,with low variance, and show that these translate into net-gains in optimizationerror and runtime performance. Our approach is a straightforward extension toboth black-box and differentiable renderers and enables optimization ofproblems with intricate light transport, such as caustics or globalillumination, that existing differentiable renderers do not converge on.", "output": "Plateau-reduced Differentiable Path Tracing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain shift across crowd data severely hinders crowd counting models togeneralize to unseen scenarios. Although domain adaptive crowd countingapproaches close this gap to a certain extent, they are still dependent on thetarget domain data to adapt (e.g. finetune) their models to the specificdomain. In this paper, we aim to train a model based on a single source domainwhich can generalize well on any unseen domain. This falls into the realm ofdomain generalization that remains unexplored in crowd counting. We firstintroduce a dynamic sub-domain division scheme which divides the source domaininto multiple sub-domains such that we can initiate a meta-learning frameworkfor domain generalization. The sub-domain division is dynamically refinedduring the meta-learning. Next, in order to disentangle domain-invariantinformation from domain-specific information in image features, we design thedomain-invariant and -specific crowd memory modules to re-encode imagefeatures. Two types of losses, i.e. feature reconstruction and orthogonallosses, are devised to enable this disentanglement. Extensive experiments onseveral standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, showthe strong generalizability of our method.", "output": "Domain-General Crowd Counting in Unseen Scenarios."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Developing gaze estimation models that generalize well to unseen domains andin-the-wild conditions remains a challenge with no known best solution. This ismostly due to the difficulty of acquiring ground truth data that cover thedistribution of possible faces, head poses and environmental conditions thatexist in the real world. In this work, we propose to train general gazeestimation models based on 3D geometry-aware gaze pseudo-annotations which weextract from arbitrary unlabelled face images, which are abundantly availablein the internet. Additionally, we leverage the observation that head, body andhand pose estimation benefit from revising them as dense 3D coordinateprediction, and similarly express gaze estimation as regression of dense 3D eyemeshes. We overcome the absence of compatible ground truth by fitting rigid 3Deyeballs on existing gaze datasets and design a multi-view supervisionframework to balance the effect of pseudo-labels during training. We test ourmethod in the task of gaze generalization, in which we demonstrate improvementof up to $30%$ compared to state-of-the-art when no ground truth data areavailable, and up to $10%$ when they are. The project material will becomeavailable for research purposes.", "output": "Generalizing Gaze Estimation with Weak-Supervision from Synthetic Views."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural radiance fields (NeRF) achieve highly photo-realistic novel-viewsynthesis, but it's a challenging problem to edit the scenes modeled byNeRF-based methods, especially for dynamic scenes. We propose editable neuralradiance fields that enable end-users to easily edit dynamic scenes and evensupport topological changes. Input with an image sequence from a single camera,our network is trained fully automatically and models topologically varyingdynamics using our picked-out surface key points. Then end-users can edit thescene by easily dragging the key points to desired new positions. To achievethis, we propose a scene analysis method to detect and initialize key points byconsidering the dynamics in the scene, and a weighted key points strategy tomodel topologically varying dynamics by joint key points and weightsoptimization. Our method supports intuitive multi-dimensional (up to 3D)editing and can generate novel scenes that are unseen in the input sequence.Experiments demonstrate that our method achieves high-quality editing onvarious dynamic scenes and outperforms the state-of-the-art. Our code andcaptured data are available at ", "output": "EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most camera lens systems are designed in isolation, separately fromdownstream computer vision methods. Recently, joint optimization approachesthat design lenses alongside other components of the image acquisition andprocessing pipeline -- notably, downstream neural networks -- have achievedimproved imaging quality or better performance on vision tasks. However, theseexisting methods optimize only a subset of lens parameters and cannot optimizeglass materials given their categorical nature. In this work, we develop adifferentiable spherical lens simulation model that accurately capturesgeometrical aberrations. We propose an optimization strategy to address thechallenges of lens design -- notorious for non-convex loss function landscapesand many manufacturing constraints -- that are exacerbated in jointoptimization tasks. Specifically, we introduce quantized continuous glassvariables to facilitate the optimization and selection of glass materials in anend-to-end design context, and couple this with carefully designed constraintsto support manufacturability. In automotive object detection, we reportimproved detection performance over existing designs even when simplifyingdesigns to two- or three-element lenses, despite significantly degrading theimage quality.", "output": "The Differentiable Lens: Compound Lens Search over Glass Surfaces and Materials for Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The appearance of an object can be fleeting when it transforms. As eggs arebroken or paper is torn, their color, shape and texture can changedramatically, preserving virtually nothing of the original except for theidentity itself. Yet, this important phenomenon is largely absent from existingvideo object segmentation (VOS) benchmarks. In this work, we close the gap bycollecting a new dataset for Video Object Segmentation under Transformations(VOST). It consists of more than 700 high-resolution videos, captured indiverse environments, which are 21 seconds long on average and densely labeledwith instance masks. A careful, multi-step approach is adopted to ensure thatthese videos focus on complex object transformations, capturing their fulltemporal extent. We then extensively evaluate state-of-the-art VOS methods andmake a number of important discoveries. In particular, we show that existingmethods struggle when applied to this novel task and that their main limitationlies in over-reliance on static appearance cues. This motivates us to propose afew modifications for the top-performing baseline that improve its capabilitiesby better modeling spatio-temporal information. But more broadly, the hope isto stimulate discussion on learning more robust video object representations.", "output": "Breaking the \"Object\" in Video Object Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The estimation of the generalization error of classifiers often relies on avalidation set. Such a set is hardly available in few-shot learning scenarios,a highly disregarded shortcoming in the field. In these scenarios, it is commonto rely on features extracted from pre-trained neural networks combined withdistance-based classifiers such as nearest class mean. In this work, weintroduce a Gaussian model of the feature distribution. By estimating theparameters of this model, we are able to predict the generalization error onnew classification tasks with few samples. We observe that accurate distanceestimates between class-conditional densities are the key to accurate estimatesof the generalization performance. Therefore, we propose an unbiased estimatorfor these distances and integrate it in our numerical analysis. We empiricallyshow that our approach outperforms alternatives such as the leave-one-outcross-validation strategy.", "output": "A Statistical Model for Predicting Generalization in Few-Shot Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Line segments are ubiquitous in our human-made world and are increasinglyused in vision tasks. They are complementary to feature points thanks to theirspatial extent and the structural information they provide. Traditional linedetectors based on the image gradient are extremely fast and accurate, but lackrobustness in noisy images and challenging conditions. Their learnedcounterparts are more repeatable and can handle challenging images, but at thecost of a lower accuracy and a bias towards wireframe lines. We propose tocombine traditional and learned approaches to get the best of both worlds: anaccurate and robust line detector that can be trained in the wild withoutground truth lines. Our new line segment detector, DeepLSD, processes imageswith a deep network to generate a line attraction field, before converting itto a surrogate image gradient magnitude and angle, which is then fed to anyexisting handcrafted line detector. Additionally, we propose a new optimizationtool to refine line segments based on the attraction field and vanishingpoints. This refinement improves the accuracy of current deep detectors by alarge margin. We demonstrate the performance of our method on low-level linedetection metrics, as well as on several downstream tasks using multiplechallenging datasets. The source code and models are available at", "output": "DeepLSD: Line Segment Detection and Refinement with Deep Image Gradients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent image generation models such as Stable Diffusion have exhibited animpressive ability to generate fairly realistic images starting from a simpletext prompt. Could such models render real images obsolete for training imageprediction models? In this paper, we answer part of this provocative questionby investigating the need for real images when training models for ImageNetclassification. Provided only with the class names that have been used to buildthe dataset, we explore the ability of Stable Diffusion to generate syntheticclones of ImageNet and measure how useful these are for training classificationmodels from scratch. We show that with minimal and class-agnostic promptengineering, ImageNet clones are able to close a large part of the gap betweenmodels produced by synthetic images and models trained with real images, forthe several standard classification benchmarks that we consider in this study.More importantly, we show that models trained on synthetic images exhibitstrong generalization properties and perform on par with models trained on realdata for transfer. Project page: ", "output": "Fake it till you make it: Learning transferable representations from synthetic ImageNet clones."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an extension to masked autoencoders (MAE) which improves on therepresentations learnt by the model by explicitly encouraging the learning ofhigher scene-level features. We do this by: (i) the introduction of aperceptual similarity term between generated and real images (ii) incorporatingseveral techniques from the adversarial training literature includingmulti-scale training and adaptive discriminator augmentation. The combinationof these results in not only better pixel reconstruction but alsorepresentations which appear to capture better higher-level details withinimages. More consequentially, we show how our method, Perceptual MAE, leads tobetter performance when used for downstream tasks outperforming previousmethods. We achieve 78.1% top-1 accuracy linear probing on ImageNet-1K and upto 88.1% when fine-tuning, with similar results for other downstream tasks, allwithout use of additional pre-trained models or data.", "output": "Improving Visual Representation Learning through Perceptual Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most existing text-video retrieval methods focus on cross-modal matchingbetween the visual content of videos and textual query sentences. However, inreal-world scenarios, online videos are often accompanied by relevant textinformation such as titles, tags, and even subtitles, which can be utilized tomatch textual queries. This insight has motivated us to propose a novelapproach to text-video retrieval, where we directly generate associatedcaptions from videos using zero-shot video captioning with knowledge fromweb-scale pre-trained models (e.g., CLIP and GPT-2). Given the generatedcaptions, a natural question arises: what benefits do they bring to text-videoretrieval? To answer this, we introduce Cap4Video, a new framework thatleverages captions in three ways: i) Input data: video-caption pairs canaugment the training data. ii) Intermediate feature interaction: we performcross-modal feature interaction between the video and caption to produceenhanced video representations. iii) Output score: the Query-Caption matchingbranch can complement the original Query-Video matching branch for text-videoretrieval. We conduct comprehensive ablation studies to demonstrate theeffectiveness of our approach. Without any post-processing, Cap4Video achievesstate-of-the-art performance on four standard text-video retrieval benchmarks:MSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code isavailable at  .", "output": "Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite its importance for federated learning, continuous learning and manyother applications, on-device training remains an open problem for EdgeAI. Theproblem stems from the large number of operations (e.g., floating pointmultiplications and additions) and memory consumption required during trainingby the back-propagation algorithm. Consequently, in this paper, we propose anew gradient filtering approach which enables on-device CNN model training.More precisely, our approach creates a special structure with fewer uniqueelements in the gradient map, thus significantly reducing the computationalcomplexity and memory consumption of back propagation during training.Extensive experiments on image classification and semantic segmentation withmultiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g.,Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wideapplicability of our approach. For example, compared to SOTA, we achieve up to19$times$ speedup and 77.1% memory savings on ImageNet classification withonly 0.1% accuracy loss. Finally, our method is easy to implement and deploy;over 20$times$ speedup and 90% energy savings have been observed compared tohighly optimized baselines in MKLDNN and CUDNN on NVIDIA Jetson Nano.Consequently, our approach opens up a new direction of research with a hugepotential for on-device training.", "output": "Efficient On-device Training via Gradient Filtering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Manipulated videos often contain subtle inconsistencies between their visualand audio signals. We propose a video forensics method, based on anomalydetection, that can identify these inconsistencies, and that can be trainedsolely using real, unlabeled data. We train an autoregressive model to generatesequences of audio-visual features, using feature sets that capture thetemporal synchronization between video frames and sound. At test time, we thenflag videos that the model assigns low probability. Despite being trainedentirely on real videos, our model obtains strong performance on the task ofdetecting manipulated speech videos. Project site:", "output": "Self-Supervised Video Forensics by Audio-Visual Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monocular 3D lane detection is a challenging task due to its lack of depthinformation. A popular solution is to first transform the front-viewed (FV)images or features into the bird-eye-view (BEV) space with inverse perspectivemapping (IPM) and detect lanes from BEV features. However, the reliance of IPMon flat ground assumption and loss of context information make it inaccurate torestore 3D information from BEV representations. An attempt has been made toget rid of BEV and predict 3D lanes from FV representations directly, while itstill underperforms other BEV-based methods given its lack of structuredrepresentation for 3D lanes. In this paper, we define 3D lane anchors in the 3Dspace and propose a BEV-free method named Anchor3DLane to predict 3D lanesdirectly from FV representations. 3D lane anchors are projected to the FVfeatures to extract their features which contain both good structural andcontext information to make accurate predictions. In addition, we also developa global optimization method that makes use of the equal-width property betweenlanes to reduce the lateral error of predictions. Extensive experiments onthree popular 3D lane detection benchmarks show that our Anchor3DLaneoutperforms previous BEV-based methods and achieves state-of-the-artperformances. The code is available at:", "output": "Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Singapore has been striving to improve the provision of healthcare servicesto her people. In this course, the government has taken note of the deficiencyin regulating and supervising people's nutrient intake, which is identified asa contributing factor to the development of chronic diseases. Consequently,this issue has garnered significant attention. In this paper, we share ourexperience in addressing this issue and attaining medical-grade nutrient intakeinformation to benefit Singaporeans in different aspects. To this end, wedevelop the FoodSG platform to incubate diverse healthcare-orientedapplications as a service in Singapore, taking into account their sharedrequirements. We further identify the profound meaning of localized fooddatasets and systematically clean and curate a localized Singaporean fooddataset FoodSG-233. To overcome the hurdle in recognition performance broughtby Singaporean multifarious food dishes, we propose to integrate supervisedcontrastive learning into our food recognition model FoodSG-SCL for theintrinsic capability to mine hard positive/negative samples and therefore boostthe accuracy. Through a comprehensive evaluation, we present performanceresults of the proposed model and insights on food-related healthcareapplications. The FoodSG-233 dataset has been released in", "output": "From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion-based generative models have recently emerged as powerful solutionsfor high-quality synthesis in multiple domains. Leveraging the bidirectionalMarkov chains, diffusion probabilistic models generate samples by inferring thereversed Markov chain based on the learned distribution mapping at the forwarddiffusion process. In this work, we propose Modiff, a conditional paradigm thatbenefits from the denoising diffusion probabilistic model (DDPM) to tackle theproblem of realistic and diverse action-conditioned 3D skeleton-based motiongeneration. We are a pioneering attempt that uses DDPM to synthesize a variablenumber of motion sequences conditioned on a categorical action. We evaluate ourapproach on the large-scale NTU RGB+D dataset and show improvements overstate-of-the-art motion generation methods.", "output": "Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video grounding aims to locate a moment of interest matching the given querysentence from an untrimmed video. Previous works ignore the emph{sparsitydilemma} in video annotations, which fails to provide the context informationbetween potential events and query sentences in the dataset. In this paper, wecontend that exploiting easily available captions which describe generalactions ie, prompt captions (PC) defined in our paper, will significantlyboost the performance. To this end, we propose a Prompt Caption Network (PCNet)for video grounding. Specifically, we first introduce dense video captioning togenerate dense captions and then obtain prompt captions by Non-Prompt CaptionSuppression (NPCS). To capture the potential information in prompt captions, wepropose Caption Guided Attention (CGA) project the semantic relations betweenprompt captions and query sentences into temporal space and fuse them intovisual representations. Considering the gap between prompt captions and groundtruth, we propose Asymmetric Cross-modal Contrastive Learning (ACCL) forconstructing more negative pairs to maximize cross-modal mutual information.Without bells and whistles, extensive experiments on three public datasets(ie, ActivityNet Captions, TACoS and ActivityNet-CG) demonstrate that ourmethod significantly outperforms state-of-the-art methods.", "output": "Exploiting Prompt Caption for Video Grounding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tagged magnetic resonance imaging (MRI) has been used for decades to observeand quantify the detailed motion of deforming tissue. However, this techniquefaces several challenges such as tag fading, large motion, long computationtimes, and difficulties in obtaining diffeomorphic incompressible flow fields.To address these issues, this paper presents a novel unsupervised phase-based3D motion estimation technique for tagged MRI. We introduce two keyinnovations. First, we apply a sinusoidal transformation to the harmonic phaseinput, which enables end-to-end training and avoids the need for phaseinterpolation. Second, we propose a Jacobian determinant-based learningobjective to encourage incompressible flow fields for deforming biologicaltissues. Our method efficiently estimates 3D motion fields that are accurate,dense, and approximately diffeomorphic and incompressible. The efficacy of themethod is assessed using human tongue motion during speech, and includes bothhealthy controls and patients that have undergone glossectomy. We show that themethod outperforms existing approaches, and also exhibits improvements inspeed, robustness to tag fading, and large tongue motion.", "output": "DRIMET: Deep Registration for 3D Incompressible Motion Estimation in Tagged-MRI with Application to the Tongue."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a facial emotion recognition framework, built upon Swin visionTransformers jointly with squeeze and excitation block (SE). A transformermodel based on an attention mechanism has been presented recently to addressvision tasks. Our method uses a vision transformer with a Squeeze excitationblock (SE) and sharpness-aware minimizer (SAM). We have used a hybrid dataset,to train our model and the AffectNet dataset to evaluate the result of ourmodel", "output": "Facial Expression Recognition using Squeeze and Excitation-powered Swin Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In-vehicle sensing technology has gained tremendous attention due to itsability to support major technological developments, such as connected vehiclesand self-driving cars. In-vehicle sensing data are invaluable and importantdata sources for traffic management systems. In this paper we propose aninnovative architecture of unobtrusive in-vehicle sensors and present methodsand tools that are used to measure the behavior of drivers. The proposedarchitecture including methods and tools are used in our NIH project to monitorand identify older drivers with early dementia", "output": "Methods and Tools for Monitoring Driver's Behavior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we are dedicated to leveraging the BERT pre-training successand modeling the domain-specific statistics to fertilize the sign languagerecognition~(SLR) model. Considering the dominance of hand and body in signlanguage expression, we organize them as pose triplet units and feed them intothe Transformer backbone in a frame-wise manner. Pre-training is performed viareconstructing the masked triplet unit from the corrupted input sequence, whichlearns the hierarchical correlation context cues among internal and externaltriplet units. Notably, different from the highly semantic word token in BERT,the pose unit is a low-level signal originally located in continuous space,which prevents the direct adoption of the BERT cross-entropy objective. To thisend, we bridge this semantic gap via coupling tokenization of the triplet unit.It adaptively extracts the discrete pseudo label from the pose triplet unit,which represents the semantic gesture/body state. After pre-training, wefine-tune the pre-trained encoder on the downstream SLR task, jointly with thenewly added task-specific layer. Extensive experiments are conducted tovalidate the effectiveness of our proposed method, achieving newstate-of-the-art performance on all four benchmarks with a notable gain.", "output": "BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The public model zoo containing enormous powerful pretrained model families(e.g., ResNet/DeiT) has reached an unprecedented scope than ever, whichsignificantly contributes to the success of deep learning. As each model familyconsists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), itnaturally arises a fundamental question of how to efficiently assemble thesereadily available models in a family for dynamic accuracy-efficiency trade-offsat runtime. To this end, we present Stitchable Neural Networks (SN-Net), anovel scalable and efficient framework for model deployment. It cheaplyproduces numerous networks with different complexity and performance trade-offsgiven a family of pretrained neural networks, which we call anchors.Specifically, SN-Net splits the anchors across the blocks/layers and thenstitches them together with simple stitching layers to map the activations fromone anchor to another. With only a few epochs of training, SN-Net effectivelyinterpolates between the performance of anchors with varying scales. Atruntime, SN-Net can instantly adapt to dynamic resource constraints byswitching the stitching positions. Extensive experiments on ImageNetclassification demonstrate that SN-Net can obtain on-par or even betterperformance than many individually trained networks while supporting diversedeployment scenarios. For example, by stitching Swin Transformers, we challengehundreds of models in Timm model zoo with a single network. We believe this newelastic model framework can serve as a strong baseline for further research inwider communities.", "output": "Stitchable Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, instead of directly predicting the pixel-level segmentationmasks, the problem of referring image segmentation is formulated as sequentialpolygon generation, and the predicted polygons can be later converted intosegmentation masks. This is enabled by a new sequence-to-sequence framework,Polygon Transformer (PolyFormer), which takes a sequence of image patches andtext query tokens as input, and outputs a sequence of polygon verticesautoregressively. For more accurate geometric localization, we propose aregression-based decoder, which predicts the precise floating-point coordinatesdirectly, without any coordinate quantization error. In the experiments,PolyFormer outperforms the prior art by a clear margin, e.g., 5.40% and 4.52%absolute improvements on the challenging RefCOCO+ and RefCOCOg datasets. Italso shows strong generalization ability when evaluated on the referring videosegmentation task without fine-tuning, e.g., achieving competitive 61.5% J&amp;F onthe Ref-DAVIS17 dataset.", "output": "PolyFormer: Referring Image Segmentation as Sequential Polygon Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of video motion magnification techniques is to magnify small motionsin a video to reveal previously invisible or unseen movement. Its uses extendfrom bio-medical applications and deepfake detection to structural modalanalysis and predictive maintenance. However, discerning small motion fromnoise is a complex task, especially when attempting to magnify very subtle,often sub-pixel movement. As a result, motion magnification techniquesgenerally suffer from noisy and blurry outputs. This work presents a newstate-of-the-art model based on the Swin Transformer, which offers bettertolerance to noisy inputs as well as higher-quality outputs that exhibit lessnoise, blurriness, and artifacts than prior-art. Improvements in output imagequality will enable more precise measurements for any application reliant onmagnified video sequences, and may enable further development of video motionmagnification techniques in new technical fields.", "output": "STB-VMM: Swin Transformer Based Video Motion Magnification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Well-performed deep neural networks (DNNs) generally require massive labelleddata and computational resources for training. Various watermarking techniquesare proposed to protect such intellectual properties (IPs), wherein the DNNproviders implant secret information into the model so that they can laterclaim IP ownership by retrieving their embedded watermarks with some dedicatedtrigger inputs. While promising results are reported in the literature,existing solutions suffer from watermark removal attacks, such as modelfine-tuning and model pruning.In this paper, we propose a novel DNN watermarking solution that caneffectively defend against the above attacks. Our key insight is to enhance thecoupling of the watermark and model functionalities such that removing thewatermark would inevitably degrade the model's performance on normal inputs. Tothis end, unlike previous methods relying on secret features learnt fromout-of-distribution data, our method only uses features learnt fromin-distribution data. Specifically, on the one hand, we propose to sampleinputs from the original training dataset and fuse them as watermark triggers.On the other hand, we randomly mask model weights during training so that theinformation of our embedded watermarks spreads in the network. By doing so,model fine-tuning/pruning would not forget our function-coupled watermarks.Evaluation results on various image classification tasks show a 100% watermarkauthentication success rate under aggressive watermark removal attacks,significantly outperforming existing solutions. Code is available:", "output": "On Function-Coupled Watermarks for Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spacecraft pose estimation plays a vital role in many on-orbit spacemissions, such as rendezvous and docking, debris removal, and on-orbitmaintenance. At present, space images contain widely varying lightingconditions, high contrast and low resolution, pose estimation of space objectsis more challenging than that of objects on earth. In this paper, we analyzingthe radar image characteristics of spacecraft on-orbit, then propose a new deeplearning neural Network structure named Dense Residual U-shaped Network(DR-U-Net) to extract image features. We further introduce a novel neuralnetwork based on DR-U-Net, namely Spacecraft U-shaped Network (SU-Net) toachieve end-to-end pose estimation for non-cooperative spacecraft.Specifically, the SU-Net first preprocess the image of non-cooperativespacecraft, then transfer learning was used for pre-training. Subsequently, inorder to solve the problem of radar image blur and low ability of spacecraftcontour recognition, we add residual connection and dense connection to thebackbone network U-Net, and we named it DR-U-Net. In this way, the feature lossand the complexity of the model is reduced, and the degradation of deep neuralnetwork during training is avoided. Finally, a layer of feedforward neuralnetwork is used for pose estimation of non-cooperative spacecraft on-orbit.Experiments prove that the proposed method does not rely on the hand-madeobject specific features, and the model has robust robustness, and thecalculation accuracy outperforms the state-of-the-art pose estimation methods.The absolute error is 0.1557 to 0.4491 , the mean error is about 0.302 , andthe standard deviation is about 0.065 .", "output": "SU-Net: Pose estimation network for non-cooperative spacecraft on-orbit."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Limited by the nature of the low-dimensional representational capacity of3DMM, most of the 3DMM-based face reconstruction (FR) methods fail to recoverhigh-frequency facial details, such as wrinkles, dimples, etc. Some attempt tosolve the problem by introducing detail maps or non-linear operations, however,the results are still not vivid. To this end, we in this paper present a novelhierarchical representation network (HRN) to achieve accurate and detailed facereconstruction from a single image. Specifically, we implement the geometrydisentanglement and introduce the hierarchical representation to fulfilldetailed face modeling. Meanwhile, 3D priors of facial details are incorporatedto enhance the accuracy and authenticity of the reconstruction results. We alsopropose a de-retouching module to achieve better decoupling of the geometry andappearance. It is noteworthy that our framework can be extended to a multi-viewfashion by considering detail consistency of different views. Extensiveexperiments on two single-view and two multi-view FR benchmarks demonstratethat our method outperforms the existing methods in both reconstructionaccuracy and visual effects. Finally, we introduce a high-quality 3D facedataset FaceHD-100 to boost the research of high-fidelity face reconstruction.The project homepage is at ", "output": "A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point cloud completion addresses filling in the missing parts of a partialpoint cloud obtained from depth sensors and generating a complete point cloud.Although there has been steep progress in the supervised methods on thesynthetic point cloud completion task, it is hardly applicable in real-worldscenarios due to the domain gap between the synthetic and real-world datasetsor the requirement of prior information. To overcome these limitations, wepropose a novel self-supervised framework ACL-SPC for point cloud completion totrain and test on the same data. ACL-SPC takes a single partial input andattempts to output the complete point cloud using an adaptive closed-loop (ACL)system that enforces the output same for the variation of an input. We evaluateour proposed ACL-SPC on various datasets to prove that it can successfullylearn to complete a partial point cloud as the first self-supervised scheme.Results show that our method is comparable with unsupervised methods andachieves superior performance on the real-world dataset compared to thesupervised methods trained on the synthetic dataset. Extensive experimentsjustify the necessity of self-supervised learning and the effectiveness of ourproposed method for the real-world point cloud completion task. The code ispublicly available from ", "output": "ACL-SPC: Adaptive Closed-Loop system for Self-Supervised Point Cloud Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human action recognition aims at classifying the category of human actionfrom a segment of a video. Recently, people have dived into designing GCN-basedmodels to extract features from skeletons for performing this task, becauseskeleton representations are much more efficient and robust than othermodalities such as RGB frames. However, when employing the skeleton data, someimportant clues like related items are also discarded. It results in someambiguous actions that are hard to be distinguished and tend to bemisclassified. To alleviate this problem, we propose an auxiliary featurerefinement head (FR Head), which consists of spatial-temporal decoupling andcontrastive feature refinement, to obtain discriminative representations ofskeletons. Ambiguous samples are dynamically discovered and calibrated in thefeature space. Furthermore, FR Head could be imposed on different stages ofGCNs to build a multi-level refinement for stronger supervision. Extensiveexperiments are conducted on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.Our proposed models obtain competitive results from state-of-the-art methodsand can help to discriminate those ambiguous samples. Codes are available at", "output": "Learning Discriminative Representations for Skeleton Based Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers have enabled recent attention-based Deep Learning (DL)architectures to achieve remarkable results in Computer Vision (CV) tasks.However, due to the extensive computational resources required, thesearchitectures are rarely implemented on resource-constrained platforms. Currentresearch investigates hybrid handcrafted convolution-based and attention-basedmodels for CV tasks such as image classification and object detection. In thispaper, we propose HyT-NAS, an efficient Hardware-aware Neural ArchitectureSearch (HW-NAS) including hybrid architectures targeting vision tasks on tinydevices. HyT-NAS improves state-of-the-art HW-NAS by enriching the search spaceand enhancing the search strategy as well as the performance predictors. Ourexperiments show that HyT-NAS achieves a similar hypervolume with less than ~5xtraining evaluations. Our resulting architecture outperforms MLPerf MobileNetV1by 6.3% accuracy improvement with 3.5x less number of parameters on Visual WakeWords.", "output": "HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Latent diffusion models for image generation have crossed a quality thresholdwhich enabled them to achieve mass adoption. Recently, a series of works havemade advancements towards replicating this success in the 3D domain,introducing techniques such as point cloud VAE, triplane representation, neuralimplicit surfaces and differentiable rendering based training. We take anotherstep along this direction, combining these developments in a two-step pipelineconsisting of 1) a triplane VAE which can learn latent representations oftextured meshes and 2) a conditional diffusion model which generates thetriplane features. For the first time this architecture allows conditional andunconditional generation of high quality textured or untextured 3D meshesacross multiple diverse categories in a few seconds on a single GPU. Itoutperforms previous work substantially on image-conditioned and unconditionalgeneration on mesh quality as well as texture generation. Furthermore, wedemonstrate the scalability of our model to large datasets for increasedquality and diversity. We will release our code and trained models.", "output": "3DGen: Triplane Latent Diffusion for Textured Mesh Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Person re-identification (re-ID) via 3D skeleton data is an emerging topicwith prominent advantages. Existing methods usually design skeleton descriptorswith raw body joints or perform skeleton sequence representation learning.However, they typically cannot concurrently model different body-componentrelations, and rarely explore useful semantics from fine-grainedrepresentations of body joints. In this paper, we propose a genericTransformer-based Skeleton Graph prototype contrastive learning (TranSG)approach with structure-trajectory prompted reconstruction to fully captureskeletal relations and valuable spatial-temporal semantics from skeleton graphsfor person re-ID. Specifically, we first devise the Skeleton Graph Transformer(SGT) to simultaneously learn body and motion relations within skeleton graphs,so as to aggregate key correlative node features into graph representations.Then, we propose the Graph Prototype Contrastive learning (GPC) to mine themost typical graph features (graph prototypes) of each identity, and contrastthe inherent similarity between graph representations and different prototypesfrom both skeleton and sequence levels to learn discriminative graphrepresentations. Last, a graph Structure-Trajectory Prompted Reconstruction(STPR) mechanism is proposed to exploit the spatial and temporal contexts ofgraph nodes to prompt skeleton graph reconstruction, which facilitatescapturing more valuable patterns and graph semantics for person re-ID.Empirical evaluations demonstrate that TranSG significantly outperformsexisting state-of-the-art methods. We further show its generality underdifferent graph modeling, RGB-estimated skeletons, and unsupervised scenarios.", "output": "TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies on transfer learning have shown that selectively fine-tuning asubset of layers or customizing different learning rates for each layer cangreatly improve robustness to out-of-distribution (OOD) data and retaingeneralization capability in the pre-trained models. However, most of thesemethods employ manually crafted heuristics or expensive hyper-parametersearches, which prevent them from scaling up to large datasets and neuralnetworks. To solve this problem, we propose Trainable Projected Gradient Method(TPGM) to automatically learn the constraint imposed for each layer for afine-grained fine-tuning regularization. This is motivated by formulatingfine-tuning as a bi-level constrained optimization problem. Specifically, TPGMmaintains a set of projection radii, i.e., distance constraints between thefine-tuned model and the pre-trained model, for each layer, and enforces themthrough weight projections. To learn the constraints, we propose a bi-leveloptimization to automatically learn the best set of projection radii in anend-to-end manner. Theoretically, we show that the bi-level optimizationformulation could explain the regularization capability of TPGM. Empirically,with little hyper-parameter search cost, TPGM outperforms existing fine-tuningmethods in OOD performance while matching the best in-distribution (ID)performance. For example, when fine-tuned on DomainNet-Real and ImageNet,compared to vanilla fine-tuning, TPGM shows $22%$ and $10%$ relative OODimprovement respectively on their sketch counterparts. Code is available aturl{", "output": "Trainable Projected Gradient Method for Robust Fine-tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The generation of high-quality images has become widely accessible and is arapidly evolving process. As a result, anyone can generate images that areindistinguishable from real ones. This leads to a wide range of applications,which also include malicious usage with deception in mind. Despite advances indetection techniques for generated images, a robust detection method stilleludes us. In this work, we utilize the inductive bias of convolutional neuralnetworks (CNNs) to develop a new detection method that requires a small amountof training samples and achieves accuracy that is on par or better than currentstate-of-the-art methods.", "output": "Deep Image Fingerprint: Accurate And Low Budget Synthetic Image Detector."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Intrinsic image decomposition (IID) is the task that decomposes a naturalimage into albedo and shade. While IID is typically solved through supervisedlearning methods, it is not ideal due to the difficulty in observing groundtruth albedo and shade in general scenes. Conversely, unsupervised learningmethods are currently underperforming supervised learning methods since thereare no criteria for solving the ill-posed problems. Recently, light detectionand ranging (LiDAR) is widely used due to its ability to make highly precisedistance measurements. Thus, we have focused on the utilization of LiDAR,especially LiDAR intensity, to address this issue. In this paper, we proposeunsupervised intrinsic image decomposition with LiDAR intensity (IID-LI). Sincethe conventional unsupervised learning methods consist of image-to-imagetransformations, simply inputting LiDAR intensity is not an effective approach.Therefore, we design an intensity consistency loss that computes the errorbetween LiDAR intensity and gray-scaled albedo to provide a criterion for theill-posed problem. In addition, LiDAR intensity is difficult to handle due toits sparsity and occlusion, hence, a LiDAR intensity densification module isproposed. We verified the estimating quality using our own dataset, whichinclude RGB images, LiDAR intensity and human judged annotations. As a result,we achieved an estimation accuracy that outperforms conventional unsupervisedlearning methods. Dataset link :(", "output": "Unsupervised Intrinsic Image Decomposition with LiDAR Intensity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Whilst the availability of 3D LiDAR point cloud data has significantly grownin recent years, annotation remains expensive and time-consuming, leading to ademand for semi-supervised semantic segmentation methods with applicationdomains such as autonomous driving. Existing work very often employs relativelylarge segmentation backbone networks to improve segmentation accuracy, at theexpense of computational costs. In addition, many use uniform sampling toreduce ground truth data requirements for learning needed, often resulting insub-optimal performance. To address these issues, we propose a new pipelinethat employs a smaller architecture, requiring fewer ground-truth annotationsto achieve superior segmentation accuracy compared to contemporary approaches.This is facilitated via a novel Sparse Depthwise Separable Convolution modulethat significantly reduces the network parameter count while retaining overalltask performance. To effectively sub-sample our training data, we propose a newSpatio-Temporal Redundant Frame Downsampling (ST-RFD) method that leveragesknowledge of sensor motion within the environment to extract a more diversesubset of training data frame samples. To leverage the use of limited annotateddata samples, we further propose a soft pseudo-label method informed by LiDARreflectivity. Our method outperforms contemporary semi-supervised work in termsof mIoU, using less labeled data, on the SemanticKITTI (59.5@5%) andScribbleKITTI (58.1@5%) benchmark datasets, based on a 2.3x reduction in modelparameters and 641x fewer multiply-add operations whilst also demonstratingsignificant performance improvement on limited training data (i.e., Less isMore).", "output": "Less is More: Reducing Task and Model Complexity for 3D Point Cloud Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a renewed interest in radar sensors in the autonomous drivingindustry. As a relatively mature technology, radars have seen steadyimprovement over the last few years, making them an appealing alternative orcomplement to the commonly used LiDARs. An emerging trend is to leverage rich,low-level radar data for perception. In this work we push this trend to theextreme -- we propose a method to perform end-to-end learning on the raw radaranalog-to-digital (ADC) data. Specifically, we design a learnable signalprocessing module inside the neural network, and a pre-training method guidedby traditional signal processing algorithms. Experiment results corroborate theoverall efficacy of the end-to-end learning method, while an ablation studyvalidates the effectiveness of our individual innovations.", "output": "ADCNet: End-to-end perception with raw radar ADC data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite thousands of researchers, engineers, and artists actively working onimproving text-to-image generation models, systems often fail to produce imagesthat accurately align with the text inputs. We introduce TIFA (Text-to-ImageFaithfulness evaluation with question Answering), an automatic evaluationmetric that measures the faithfulness of a generated image to its text inputvia visual question answering (VQA). Specifically, given a text input, weautomatically generate several question-answer pairs using a language model. Wecalculate image faithfulness by checking whether existing VQA models can answerthese questions using the generated image. TIFA is a reference-free metric thatallows for fine-grained and interpretable evaluations of generated images. TIFAalso has better correlations with human judgments than existing metrics. Basedon this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diversetext inputs and 25K questions across 12 categories (object, counting, etc.). Wepresent a comprehensive evaluation of existing text-to-image models using TIFAv1.0 and highlight the limitations and challenges of current models. Forinstance, we find that current text-to-image models, despite doing well oncolor and material, still struggle in counting, spatial relations, andcomposing multiple objects. We hope our benchmark will help carefully measurethe research progress in text-to-image synthesis and provide valuable insightsfor further research.", "output": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Diffusion Probabilistic Model (DPM) has emerged as a highly effectivegenerative model in the field of computer vision. Its intermediate latentvectors offer rich semantic information, making it an attractive option forvarious downstream tasks such as segmentation and detection. In order toexplore its potential further, we have taken a step forward and considered amore complex scenario in the medical image domain, specifically, under anunsupervised adaptation condition. To this end, we propose a Diffusion-basedand Prototype-guided network (DP-Net) for unsupervised domain adaptivesegmentation. Concretely, our DP-Net consists of two stages: 1) DistributionAligned Diffusion (DADiff), which involves training a domain discriminator tominimize the difference between the intermediate features generated by the DPM,thereby aligning the inter-domain distribution; and 2) Prototype-guidedConsistency Learning (PCL), which utilizes feature centroids as prototypes andapplies a prototype-guided loss to ensure that the segmentor learns consistentcontent from both source and target domains. Our approach is evaluated onfundus datasets through a series of experiments, which demonstrate that theperformance of the proposed method is reliable and outperforms state-of-the-artmethods. Our work presents a promising direction for using DPM in complexmedical image scenarios, opening up new possibilities for further research inmedical imaging.", "output": "Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sequential video understanding, as an emerging video understanding task, hasdriven lots of researchers' attention because of its goal-oriented nature. Thispaper studies weakly supervised sequential video understanding where theaccurate time-stamp level text-video alignment is not provided. We solve thistask by borrowing ideas from CLIP. Specifically, we use a transformer toaggregate frame-level features for video representation and use a pre-trainedtext encoder to encode the texts corresponding to each action and the wholevideo, respectively. To model the correspondence between text and video, wepropose a multiple granularity loss, where the video-paragraph contrastive lossenforces matching between the whole video and the complete script, and afine-grained frame-sentence contrastive loss enforces the matching between eachaction and its description. As the frame-sentence correspondence is notavailable, we propose to use the fact that video actions happen sequentially inthe temporal domain to generate pseudo frame-sentence correspondence andsupervise the network training with the pseudo labels. Extensive experiments onvideo sequence verification and text-to-video matching show that our methodoutperforms baselines by a large margin, which validates the effectiveness ofour proposed approach. Code is available at ", "output": "Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent successes in image synthesis are powered by large-scale diffusionmodels. However, most methods are currently limited to either text- orimage-conditioned generation for synthesizing an entire image, texture transferor inserting objects into a user-specified region. In contrast, in this work wefocus on synthesizing complex interactions (ie, an articulated hand) with agiven object. Given an RGB image of an object, we aim to hallucinate plausibleimages of a human hand interacting with it. We propose a two-step generativeapproach: a LayoutNet that samples an articulation-agnostichand-object-interaction layout, and a ContentNet that synthesizes images of ahand grasping the object given the predicted layout. Both are built on top of alarge-scale pretrained diffusion model to make use of its latentrepresentation. Compared to baselines, the proposed method is shown togeneralize better to novel objects and perform surprisingly well onout-of-distribution in-the-wild scenes of portable-sized objects. The resultingsystem allows us to predict descriptive affordance information, such as handarticulation and approaching orientation. Project page:", "output": "Affordance Diffusion: Synthesizing Hand-Object Interactions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Calibration of deep learning models is crucial to their trustworthiness andsafe usage, and as such, has been extensively studied in supervisedclassification models, with methods crafted to decrease miscalibration.However, there has yet to be a comprehensive study of the calibration ofvision-language models that are used for zero-shot inference, like CLIP. Wemeasure calibration across relevant variables like prompt, dataset, andarchitecture, and find that zero-shot inference with CLIP is miscalibrated.Furthermore, we propose a modified version of temperature scaling that isaligned with the common use cases of CLIP as a zero-shot inference model, andshow that a single learned temperature generalizes for each specific CLIP model(defined by a chosen pre-training dataset and architecture) across inferencedataset and prompt choice.", "output": "Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we leverage CLIP for zero-shot sketch based image retrieval(ZS-SBIR). We are largely inspired by recent advances on foundation models andthe unparalleled generalisation ability they seem to offer, but for the firsttime tailor it to benefit the sketch community. We put forward novel designs onhow best to achieve this synergy, for both the category setting and thefine-grained setting (\"all\"). At the very core of our solution is a promptlearning setup. First we show just via factoring in sketch-specific prompts, wealready have a category-level ZS-SBIR system that overshoots all prior arts, bya large margin (24.8%) - a great testimony on studying the CLIP and ZS-SBIRsynergy. Moving onto the fine-grained setup is however trickier, and requires adeeper dive into this synergy. For that, we come up with two specific designsto tackle the fine-grained matching nature of the problem: (i) an additionalregularisation loss to ensure the relative separation between sketches andphotos is uniform across categories, which is not the case for the goldstandard standalone triplet loss, and (ii) a clever patch shuffling techniqueto help establishing instance-level structural correspondences betweensketch-photo pairs. With these designs, we again observe significantperformance gains in the region of 26.9% over previous state-of-the-art. Thetake-home message, if any, is the proposed CLIP and prompt learning paradigmcarries great promise in tackling other sketch-related tasks (not limited toZS-SBIR) where data scarcity remains a great challenge. Project page:", "output": "CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic 3D content creation has achieved rapid progress recently due to theavailability of pre-trained, large language models and image diffusion models,forming the emerging topic of text-to-3D content creation. Existing text-to-3Dmethods commonly use implicit scene representations, which couple the geometryand appearance via volume rendering and are suboptimal in terms of recoveringfiner geometries and achieving photorealistic rendering; consequently, they areless effective for generating high-quality 3D assets. In this work, we proposea new method of Fantasia3D for high-quality text-to-3D content creation. Key toFantasia3D is the disentangled modeling and learning of geometry andappearance. For geometry learning, we rely on a hybrid scene representation,and propose to encode surface normal extracted from the representation as theinput of the image diffusion model. For appearance modeling, we introduce thespatially varying bidirectional reflectance distribution function (BRDF) intothe text-to-3D task, and learn the surface material for photorealisticrendering of the generated surface. Our disentangled framework is morecompatible with popular graphics engines, supporting relighting, editing, andphysical simulation of the generated 3D assets. We conduct thorough experimentsthat show the advantages of our method over existing ones under differenttext-to-3D task settings. Project page and source codes:", "output": "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating photorealistic images with controllable camera pose and scenecontents is essential for many applications including AR/VR and simulation.Despite the fact that rapid progress has been made in 3D-aware generativemodels, most existing methods focus on object-centric images and are notapplicable to generating urban scenes for free camera viewpoint control andscene editing. To address this challenging task, we propose UrbanGIRAFFE, whichuses a coarse 3D panoptic prior, including the layout distribution ofuncountable stuff and countable objects, to guide a 3D-aware generative model.Our model is compositional and controllable as it breaks down the scene intostuff, objects, and sky. Using stuff prior in the form of semantic voxel grids,we build a conditioned stuff generator that effectively incorporates the coarsesemantic and geometry information. The object layout prior further allows us tolearn an object generator from cluttered scenes. With proper loss functions,our approach facilitates photorealistic 3D-aware image synthesis with diversecontrollability, including large camera movement, stuff editing, and objectmanipulation. We validate the effectiveness of our model on both synthetic andreal-world datasets, including the challenging KITTI-360 dataset.", "output": "UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-supervised medical image segmentation has attracted much attention inrecent years because of the high cost of medical image annotations. In thispaper, we propose a novel Inherent Consistent Learning (ICL) method, which aimsto learn robust semantic category representations through the semanticconsistency guidance of labeled and unlabeled data to help segmentation. Inpractice, we introduce two external modules namely Supervised Semantic ProxyAdaptor (SSPA) and Unsupervised Semantic Consistent Learner (USCL) that basedon the attention mechanism to align the semantic category representations oflabeled and unlabeled data, as well as update the global semanticrepresentations over the entire training set. The proposed ICL is aplug-and-play scheme for various network architectures and the two modules arenot involved in the testing stage. Experimental results on three publicbenchmarks show that the proposed method can outperform the state-of-the-artespecially when the number of annotated data is extremely limited. Code isavailable at: ", "output": "Inherent Consistent Learning for Accurate Semi-supervised Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The automatic generation of stylized co-speech gestures has recently receivedincreasing attention. Previous systems typically allow style control viapredefined text labels or example motion clips, which are often not flexibleenough to convey user intent accurately. In this work, we presentGestureDiffuCLIP, a neural network framework for synthesizing realistic,stylized co-speech gestures with flexible style control. We leverage the powerof the large-scale Contrastive-Language-Image-Pre-training (CLIP) model andpresent a novel CLIP-guided mechanism that extracts efficient stylerepresentations from multiple input modalities, such as a piece of text, anexample motion clip, or a video. Our system learns a latent diffusion model togenerate high-quality gestures and infuses the CLIP representations of styleinto the generator via an adaptive instance normalization (AdaIN) layer. Wefurther devise a gesture-transcript alignment mechanism that ensures asemantically correct gesture generation based on contrastive learning. Oursystem can also be extended to allow fine-grained style control of individualbody parts. We demonstrate an extensive set of examples showing the flexibilityand generalizability of our model to a variety of style descriptions. In a userstudy, we show that our system outperforms the state-of-the-art approachesregarding human likeness, appropriateness, and style correctness.", "output": "GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We focus on the construction of a loss function for the bounding boxregression. The Intersection over Union (IoU) metric is improved to convergefaster, to make the surface of the loss function smooth and continuous over thewhole searched space, and to reach a more precise approximation of the labels.The main principle is adding a smoothing part to the original IoU, where thesmoothing part is given by a linear space with values that increases from theground truth bounding box to the border of the input image, and thus covers thewhole spatial search space. We show the motivation and formalism behind thisloss function and experimentally prove that it outperforms IoU, DIoU, CIoU, andSIoU by a large margin. We experimentally show that the proposed loss functionis robust with respect to the noise in the dimension of ground truth boundingboxes. The reference implementation is available atgitlab.com/irafm-ai/smoothing-iou.", "output": "Intersection over Union with smoothing for bounding box regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncalibrated photometric stereo (UPS) is challenging due to the inherentambiguity brought by the unknown light. Although the ambiguity is alleviated onnon-Lambertian objects, the problem is still difficult to solve for moregeneral objects with complex shapes introducing irregular shadows and generalmaterials with complex reflectance like anisotropic reflectance. To exploitcues from shadow and reflectance to solve UPS and improve performance ongeneral materials, we propose DANI-Net, an inverse rendering framework withdifferentiable shadow handling and anisotropic reflectance modeling. Unlikemost previous methods that use non-differentiable shadow maps and assumeisotropic material, our network benefits from cues of shadow and anisotropicreflectance through two differentiable paths. Experiments on multiplereal-world datasets demonstrate our superior and robust performance.", "output": "DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a simple and application-friendly network (called SimpleNet) fordetecting and localizing anomalies. SimpleNet consists of four components: (1)a pre-trained Feature Extractor that generates local features, (2) a shallowFeature Adapter that transfers local features towards target domain, (3) asimple Anomaly Feature Generator that counterfeits anomaly features by addingGaussian noise to normal features, and (4) a binary Anomaly Discriminator thatdistinguishes anomaly features from normal features. During inference, theAnomaly Feature Generator would be discarded. Our approach is based on threeintuitions. First, transforming pre-trained features to target-orientedfeatures helps avoid domain bias. Second, generating synthetic anomalies infeature space is more effective, as defects may not have much commonality inthe image space. Third, a simple discriminator is much efficient and practical.In spite of simplicity, SimpleNet outperforms previous methods quantitativelyand qualitatively. On the MVTec AD benchmark, SimpleNet achieves an anomalydetection AUROC of 99.6%, reducing the error by 55.5% compared to the next bestperforming model. Furthermore, SimpleNet is faster than existing methods, witha high frame rate of 77 FPS on a 3080ti GPU. Additionally, SimpleNetdemonstrates significant improvements in performance on the One-Class NoveltyDetection task. Code: ", "output": "SimpleNet: A Simple Network for Image Anomaly Detection and Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a linear algebra formulation of backpropagation which allows thecalculation of gradients by using a generically written ``backslash'' orGaussian elimination on triangular systems of equations. Generally the matrixelements are operators. This paper has three contributions:1. It is of intellectual value to replace traditional treatments of automaticdifferentiation with a (left acting) operator theoretic, graph-based approach.2. Operators can be readily placed in matrices in software in programminglanguages such as Ju lia as an implementation option.3. We introduce a novel notation, ``transpose dot'' operator``${}^{T_bullet}$'' that allows the reversal of operators.We demonstrate the elegance of the operators approach in a suitableprogramming language consisting of generic linear algebra operators such asJulia cite{bezanson2017julia}, and that it is possible to realize thisabstraction in code. Our implementation shows how generic linear algebra canallow operators as elements of matrices, and without rewriting any code, thesoftware carries through to completion giving the correct answer.", "output": "BACKpropagation through BACK substitution with a BACKslash."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a framework for training an agent to actively requesthelp in object-goal navigation tasks, with feedback indicating the location ofthe target object in its field of view. To make the agent more robust inscenarios where a teacher may not always be available, the proposed trainingcurriculum includes a mix of episodes with and without feedback. The resultsshow that this approach improves the agent's performance, even in the absenceof feedback.", "output": "Robustness of Utilizing Feedback in Embodied Visual Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep models are dominating the artificial intelligence (AI) industry sincethe ImageNet challenge in 2012. The size of deep models is increasing eversince, which brings new challenges to this field with applications in cellphones, personal computers, autonomous cars, and wireless base stations. Herewe list a set of problems, ranging from training, inference, generalizationbound, and optimization with some formalism to communicate these challengeswith mathematicians, statisticians, and theoretical computer scientists. Thisis a subjective view of the research questions in deep learning that benefitsthe tech industry in long run.", "output": "Mathematical Challenges in Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the analysis of large/big data sets, aggregation (replacing values of avariable over a group by a single value) is a standard way of reducing the size(complexity) of the data. Data analysis programs provide different aggregationfunctions.Recently some books dealing with the theoretical and algorithmic backgroundof traditional aggregation functions were published. A problem with traditionalaggregation is that often too much information is discarded thus reducing theprecision of the obtained results. A much better, preserving more information,summarization of original data can be achieved by representing aggregated datausing selected types of complex data.In complex data analysis the measured values over a selected group $A$ areaggregated into a complex object $Sigma(A)$ and not into a single value. Mostof the aggregation functions theory does not apply directly. In ourcontribution, we present an attempt to start building a theoretical backgroundof complex aggregation.We introduce and discuss exactly mergeable summaries for which it holds formerging of disjoint sets of units[ Sigma(A cup B) = F( Sigma(A),Sigma(B)),qquad mbox{ for } quad AcapB = emptyset .]", "output": "Exactly mergeable summaries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence has been used to help human complete difficult tasksin complicated environments by providing optimized strategies fordecision-making or replacing the manual labour. In environments includingmultiple agents, such as football, the most common methods to train agents areImitation Learning and Multi-Agent Reinforcement Learning (MARL). However, theagents trained by Imitation Learning cannot outperform the expert demonstrator,which makes humans hardly get new insights from the learnt policy. Besides,MARL is prone to the credit assignment problem. In environments with sparsereward signal, this method can be inefficient. The objective of our research isto create a novel reward shaping method by embedding contextual information inreward function to solve the aforementioned challenges. We demonstrate this inthe Google Research Football (GRF) environment. We quantify the contextualinformation extracted from game state observation and use this quantificationtogether with original sparse reward to create the shaped reward. Theexperiment results in the GRF environment prove that our reward shaping methodis a useful addition to state-of-the-art MARL algorithms for training agents inenvironments with sparse reward signal.", "output": "Embedding Contextual Information through Reward Shaping in Multi-Agent Learning: A Case Study from Google Football."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a special-purpose system to achieve high-accuracy andhigh-efficiency machine learning (ML) molecular dynamics (MD) calculations. Thesystem consists of field programmable gate array (FPGA) and applicationspecific integrated circuit (ASIC) working in heterogeneous parallelization. Tobe specific, a multiplication-less neural network (NN) is deployed on thenon-von Neumann (NvN)-based ASIC (SilTerra 180 nm process) to evaluate atomicforces, which is the most computationally expensive part of MD. All othercalculations of MD are done using FPGA (Xilinx XC7Z100). It is shown that, toachieve similar-level accuracy, the proposed NvN-based system based on low-endfabrication technologies (180 nm) is 1.6x faster and 10^2-10^3x more energyefficiency than state-of-the-art vN based MLMD using graphics processing units(GPUs) based on much more advanced technologies (12 nm), indicating superiorityof the proposed NvN-based heterogeneous parallel architecture.", "output": "A Heterogeneous Parallel Non-von Neumann Architecture System for Accurate and Efficient Machine Learning Molecular Dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Symmetric Positive Definite (SPD) matrices have received wide attention inmachine learning due to their intrinsic capacity of encoding underlyingstructural correlation in data. To reflect the non-Euclidean geometry of SPDmanifolds, many successful Riemannian metrics have been proposed. However,existing fixed metric tensors might lead to sub-optimal performance for SPDmatrices learning, especially for SPD neural networks. To remedy thislimitation, we leverage the idea of pullback and propose adaptive Riemannianmetrics for SPD manifolds. Moreover, we present comprehensive theories for ourmetrics. Experiments on three datasets demonstrate that equipped with theproposed metrics, SPD networks can exhibit superior performance.", "output": "Adaptive Riemannian Metrics on SPD Manifolds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we explore the performance of different pruning methods in thecontext of the lottery ticket hypothesis. We compare the performance of L1unstructured pruning, Fisher pruning, and random pruning on different networkarchitectures and pruning scenarios. The experiments include an evaluation ofone-shot and iterative pruning, an examination of weight movement in thenetwork during pruning, a comparison of the pruning methods on networks ofvarying widths, and an analysis of the performance of the methods when thenetwork becomes very sparse. Additionally, we propose and evaluate a new methodfor efficient computation of Fisher pruning, known as batched Fisher pruning.", "output": "Exploring the Performance of Pruning Methods in Neural Networks: An Empirical Study of the Lottery Ticket Hypothesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a regularizer called Implicit Neural RepresentationRegularizer (INRR) to improve the generalization ability of the Implicit NeuralRepresentation (INR). The INR is a fully connected network that can representsignals with details not restricted by grid resolution. However, itsgeneralization ability could be improved, especially with non-uniformly sampleddata. The proposed INRR is based on learned Dirichlet Energy (DE) that measuressimilarities between rows/columns of the matrix. The smoothness of theLaplacian matrix is further integrated by parameterizing DE with a tiny INR.INRR improves the generalization of INR in signal representation by perfectlyintegrating the signal's self-similarity with the smoothness of the Laplacianmatrix. Through well-designed numerical experiments, the paper also reveals aseries of properties derived from INRR, including momentum methods likeconvergence trajectory and multi-scale similarity. Moreover, the proposedmethod could improve the performance of other signal representation methods.", "output": "Regularize implicit neural representation by itself."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weight-sharing neural architecture search aims to optimize a configurableneural network model (supernet) for a variety of deployment scenarios acrossmany devices with different resource constraints. Existing approaches useevolutionary search to extract a number of models from a supernet trained on avery large data set, and then fine-tune the extracted models on the typicallysmall, real-world data set of interest. The computational cost of training thusgrows linearly with the number of different model deployment scenarios. Hence,we propose Transfer-Once-For-All (TOFA) for supernet-style training on smalldata sets with constant computational training cost over any number of edgedeployment scenarios. Given a task, TOFA obtains custom neural networks, boththe topology and the weights, optimized for any number of edge deploymentscenarios. To overcome the challenges arising from small data, TOFA utilizes aunified semi-supervised training loss to simultaneously train all subnetswithin the supernet, coupled with on-the-fly architecture selection atdeployment time.", "output": "TOFA: Transfer-Once-for-All."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal learning has seen great success mining data features from multiplemodalities with remarkable model performance improvement. Meanwhile, federatedlearning (FL) addresses the data sharing problem, enabling privacy-preservedcollaborative training to provide sufficient precious data. Great potential,therefore, arises with the confluence of them, known as multimodal federatedlearning. However, limitation lies in the predominant approaches as they oftenassume that each local dataset records samples from all modalities. In thispaper, we aim to bridge this gap by proposing an Unimodal Training - MultimodalPrediction (UTMP) framework under the context of multimodal federated learning.We design HA-Fedformer, a novel transformer-based model that empowers unimodaltraining with only a unimodal dataset at the client and multimodal testing byaggregating multiple clients' knowledge for better accuracy. The key advantagesare twofold. Firstly, to alleviate the impact of data non-IID, we develop anuncertainty-aware aggregation method for the local encoders with layer-wiseMarkov Chain Monte Carlo sampling. Secondly, to overcome the challenge ofunaligned language sequence, we implement a cross-modal decoder aggregation tocapture the hidden signal correlation between decoders trained by data fromdifferent modalities. Our experiments on popular sentiment analysis benchmarks,CMU-MOSI and CMU-MOSEI, demonstrate that HA-Fedformer significantly outperformsstate-of-the-art multimodal models under the UTMP federated learningframeworks, with 15%-20% improvement on most attributes.", "output": "Unimodal Training-Multimodal Prediction: Cross-modal Federated Learning with Hierarchical Aggregation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph data is omnipresent and has a large variety of applications such asnatural science, social networks or semantic web. Though rich in information,graphs are often noisy and incomplete. Therefore, graph completion tasks suchas node classification or link prediction have gained attention. On the onehand, neural methods such as graph neural networks have proven to be robusttools for learning rich representations of noisy graphs. On the other hand,symbolic methods enable exact reasoning on graphs. We propose KeGNN, aneuro-symbolic framework for learning on graph data that combines bothparadigms and allows for the integration of prior knowledge into a graph neuralnetwork model. In essence, KeGNN consists of a graph neural network as a baseon which knowledge enhancement layers are stacked with the objective ofrefining predictions with respect to prior knowledge. We instantiate KeGNN inconjunction with two standard graph neural networks: Graph ConvolutionalNetworks and Graph Attention Networks, and evaluate KeGNN on multiple benchmarkdatasets for node classification.", "output": "Knowledge Enhanced Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating the generalization performance is practically challenging onout-of-distribution (OOD) data without ground truth labels. While previousmethods emphasize the connection between distribution difference and OODaccuracy, we show that a large domain gap not necessarily leads to a low testaccuracy. In this paper, we investigate this problem from the perspective offeature separability, and propose a dataset-level score based upon featuredispersion to estimate the test accuracy under distribution shift. Our methodis inspired by desirable properties of features in representation learning:high inter-class dispersion and high intra-class compactness. Our analysisshows that inter-class dispersion is strongly correlated with the modelaccuracy, while intra-class compactness does not reflect the generalizationperformance on OOD data. Extensive experiments demonstrate the superiority ofour method in both prediction performance and computational efficiency.", "output": "On the Importance of Feature Separability in Predicting Out-Of-Distribution Error."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Railway operations involve different types of entities (stations, trains,etc.), making the existing graph/network models with homogenous nodes (i.e.,the same kind of nodes) incapable of capturing the interactions between theentities. This paper aims to develop a heterogeneous graph neural network(HetGNN) model, which can address different types of nodes (i.e., heterogeneousnodes), to investigate the train delay evolution on railway networks. To thisend, a graph architecture combining the HetGNN model and the GraphSAGEhomogeneous GNN (HomoGNN), called SAGE-Het, is proposed. The aim is to capturethe interactions between trains, trains and stations, and stations and otherstations on delay evolution based on different edges. In contrast to thetraditional methods that require the inputs to have constant dimensions (e.g.,in rectangular or grid-like arrays) or only allow homogeneous nodes in thegraph, SAGE-Het allows for flexible inputs and heterogeneous nodes. The datafrom two sub-networks of the China railway network are applied to test theperformance and robustness of the proposed SAGE-Het model. The experimentalresults show that SAGE-Het exhibits better performance than the existing delayprediction methods and some advanced HetGNNs used for other prediction tasks;the predictive performances of SAGE-Het under different prediction timehorizons (10/20/30 min ahead) all outperform other baseline methods;Specifically, the influences of train interactions on delay propagation areinvestigated based on the proposed model. The results show that traininteractions become subtle when the train headways increase . This findingdirectly contributes to decision-making in the situation whereconflict-resolution or train-canceling actions are needed.", "output": "Railway Network Delay Evolution: A Heterogeneous Graph Neural Network Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Among the major public transportation systems in cities, bus transit has itsproblems, including more accuracy and reliability when estimating the busarrival time for riders. This can lead to delays and decreased ridership,especially in cities where public transportation is heavily relied upon. Acommon issue is that the arrival times of buses do not match the schedules,resulting in latency for fixed schedules. According to the study in this paperon New York City bus data, there is an average delay of around eight minutes or491 seconds mismatch between the bus arrivals and the actual scheduled time.This research paper presents a novel AI-based data-driven approach forestimating the arrival times of buses at each transit point (station). Ourapproach is based on a fully connected neural network and can predict thearrival time collectively across all bus lines in large metropolitan areas. Ourneural-net data-driven approach provides a new way to estimate the arrival timeof the buses, which can lead to a more efficient and smarter way to bring thebus transit to the general public. Our evaluation of the network bus systemwith more than 200 bus lines, and 2 million data points, demonstrates less than40 seconds of estimated error for arrival times. The inference time per eachvalidation set data point is less than 0.006 ms.", "output": "A Novel Neural Network Approach for Predicting the Arrival Time of Buses for Smart On-Demand Public Transit."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Molecular representation learning plays a crucial role in AI-assisted drugdiscovery research. Encoding 3D molecular structures through Euclidean neuralnetworks has become the prevailing method in the geometric deep learningcommunity. However, the equivariance constraints and message passing inEuclidean space may limit the network expressive power. In this work, wepropose a Harmonic Molecular Representation learning (HMR) framework, whichrepresents a molecule using the Laplace-Beltrami eigenfunctions of itsmolecular surface. HMR offers a multi-resolution representation of moleculargeometric and chemical features on 2D Riemannian manifold. We also introduce aharmonic message passing method to realize efficient spectral message passingover the surface manifold for better molecular encoding. Our proposed methodshows comparable predictive power to current models in small molecule propertyprediction, and outperforms the state-of-the-art deep learning models forligand-binding protein pocket classification and the rigid protein dockingchallenge, demonstrating its versatility in molecular representation learning.", "output": "Learning Harmonic Molecular Representations on Riemannian Manifold."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern Generative Adversarial Networks (GANs) generate realistic imagesremarkably well. Previous work has demonstrated the feasibility of\"GAN-classifiers\" that are distinct from the co-trained discriminator, andoperate on images generated from a frozen GAN. That such classifiers work atall affirms the existence of \"knowledge gaps\" (out-of-distribution artifactsacross samples) present in GAN training. We iteratively train GAN-classifiersand train GANs that \"fool\" the classifiers (in an attempt to fill the knowledgegaps), and examine the effect on GAN training dynamics, output quality, andGAN-classifier generalization. We investigate two settings, a small DCGANarchitecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTAGAN architecture trained on high dimensional images (FFHQ). We find that theDCGAN is unable to effectively fool a held-out GAN-classifier withoutcompromising the output quality. However, StyleGAN2 can fool held-outclassifiers with no change in output quality, and this effect persists overmultiple rounds of GAN/classifier training which appears to reveal an orderingover optima in the generator parameter space. Finally, we study differentclassifier architectures and show that the architecture of the GAN-classifierhas a strong influence on the set of its learned artifacts.", "output": "Sequential training of GANs against GAN-classifiers reveals correlated \"knowledge gaps\" present among independently trained GAN instances."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of of multi-flow transmission in wireless networks,where data signals from different flows can interfere with each other due tomutual interference between links along their routes, resulting in reduced linkcapacities. The objective is to develop a multi-flow transmission strategy thatroutes flows across the wireless interference network to maximize the networkutility. However, obtaining an optimal solution is computationally expensivedue to the large state and action spaces involved. To tackle this challenge, weintroduce a novel algorithm called Dual-stage Interference-Aware Multi-flowOptimization of Network Data-signals (DIAMOND). The design of DIAMOND allowsfor a hybrid centralized-distributed implementation, which is a characteristicof 5G and beyond technologies with centralized unit deployments. A centralizedstage computes the multi-flow transmission strategy using a novel design ofgraph neural network (GNN) reinforcement learning (RL) routing agent. Then, adistributed stage improves the performance based on a novel design ofdistributed learning updates. We provide a theoretical analysis of DIAMOND andprove that it converges to the optimal multi-flow transmission strategy as timeincreases. We also present extensive simulation results over various networktopologies (random deployment, NSFNET, GEANT2), demonstrating the superiorperformance of DIAMOND compared to existing methods.", "output": "Multi-Flow Transmission in Wireless Interference Networks: A Convergent Graph Learning Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine Learning (ML) has recently shown tremendous success in modelingvarious healthcare prediction tasks, ranging from disease diagnosis andprognosis to patient treatment. Due to the sensitive nature of medical data,privacy must be considered along the entire ML pipeline, from model training toinference. In this paper, we conduct a review of recent literature concerningPrivacy-Preserving Machine Learning (PPML) for healthcare. We primarily focuson privacy-preserving training and inference-as-a-service, and perform acomprehensive review of existing trends, identify challenges, and discussopportunities for future research directions. The aim of this review is toguide the development of private and efficient ML models in healthcare, withthe prospects of translating research efforts into real-world settings.", "output": "Privacy-preserving machine learning for healthcare: open challenges and future perspectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are vulnerable to backdoor attacks, where an adversarymaliciously manipulates the model behavior through overlaying images withspecial triggers. Existing backdoor defense methods often require accessing afew validation data and model parameters, which are impractical in manyreal-world applications, e.g., when the model is provided as a cloud service.In this paper, we address the practical task of blind backdoor defense at testtime, in particular for black-box models. The true label of every test imageneeds to be recovered on the fly from the hard label predictions of asuspicious model. The heuristic trigger search in image space, however, is notscalable to complex triggers or high image resolution. We circumvent suchbarrier by leveraging generic image generation models, and propose a frameworkof Blind Defense with Masked AutoEncoder (BDMAE). It uses the image structuralsimilarity and label consistency between the test image and MAE restorations todetect possible triggers. The detection result is refined by considering thetopology of triggers. We obtain a purified test image from restorations formaking prediction. Our approach is blind to the model architectures, triggerpatterns or image benignity. Extensive experiments on multiple datasets withdifferent backdoor attacks validate its effectiveness and generalizability.Code is available at ", "output": "Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Designing more efficient, reliable, and explainable neural networkarchitectures is critical to studies that are based on artificial intelligence(AI) techniques. Previous studies, by post-hoc analysis, have found that thebest-performing ANNs surprisingly resemble biological neural networks (BNN),which indicates that ANNs and BNNs may share some common principles to achieveoptimal performance in either machine learning or cognitive/behavior tasks.Inspired by this phenomenon, we proactively instill organizational principlesof BNNs to guide the redesign of ANNs. We leverage the Core-Periphery (CP)organization, which is widely found in human brain networks, to guide theinformation communication mechanism in the self-attention of vision transformer(ViT) and name this novel framework as CP-ViT. In CP-ViT, the attentionoperation between nodes is defined by a sparse graph with a Core-Peripherystructure (CP graph), where the core nodes are redesigned and reorganized toplay an integrative role and serve as a center for other periphery nodes toexchange information. We evaluated the proposed CP-ViT on multiple publicdatasets, including medical image datasets (INbreast) and natural imagedatasets. Interestingly, by incorporating the BNN-derived principle (CPstructure) into the redesign of ViT, our CP-ViT outperforms otherstate-of-the-art ANNs. In general, our work advances the state of the art inthree aspects: 1) This work provides novel insights for brain-inspired AI: wecan utilize the principles found in BNNs to guide and improve our ANNarchitecture design; 2) We show that there exist sweet spots of CP graphs thatlead to CP-ViTs with significantly improved performance; and 3) The core nodesin CP-ViT correspond to task-related meaningful and important image patches,which can significantly enhance the interpretability of the trained deep model.", "output": "Core-Periphery Principle Guided Redesign of Self-Attention in Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Moisture content (MC) estimation is important in the manufacturing process ofdrying bulky filter media products as it is the prerequisite for dryingoptimization. In this study, a dataset collected by performing 161 dryingindustrial experiments is described and a methodology for MC estimation in annon-destructive and online manner during industrial drying is presented. Anartificial neural network (ANN) based method is compared to state-of-the-art MCestimation methods reported in the literature. Results of model fitting andtraining show that a three-layer Perceptron achieves the lowest error.Experimental results show that ANNs combined with oven settings data, dryingtime and product temperature can be used to reliably estimate the MC of bulkyfilter media products.", "output": "Online Non-Destructive Moisture Content Estimation of Filter Media During Drying Using Artificial Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an adjusted Wasserstein distributionally robust estimator -- basedon a nonlinear transformation of the Wasserstein distributionally robust (WDRO)estimator in statistical learning. This transformation will improve thestatistical performance of WDRO because the adjusted WDRO estimator isasymptotically unbiased and has an asymptotically smaller mean squared error.The adjusted WDRO will not mitigate the out-of-sample performance guarantee ofWDRO. Sufficient conditions for the existence of the adjusted WDRO estimatorare presented, and the procedure for the computation of the adjusted WDROestimator is given. Specifically, we will show how the adjusted WDRO estimatoris developed in the generalized linear model. Numerical experiments demonstratethe favorable practical performance of the adjusted estimator over the classicone.", "output": "Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of mobile, wearable, and ubiquitous computing (UbiComp) isundergoing a revolutionary integration of machine learning. Devices can nowdiagnose diseases, predict heart irregularities, and unlock the full potentialof human cognition. However, the underlying algorithms are not immune to biaseswith respect to sensitive attributes (e.g., gender, race), leading todiscriminatory outcomes. The research communities of HCI and AI-Ethics haverecently started to explore ways of reporting information about datasets tosurface and, eventually, counter those biases. The goal of this work is toexplore the extent to which the UbiComp community has adopted such ways ofreporting and highlight potential shortcomings. Through a systematic review ofpapers published in the Proceedings of the ACM Interactive, Mobile, Wearableand Ubiquitous Technologies (IMWUT) journal over the past 5 years (2018-2022),we found that progress on algorithmic fairness within the UbiComp communitylags behind. Our findings show that only a small portion (5%) of publishedpapers adheres to modern fairness reporting, while the overwhelming majoritythereof focuses on accuracy or error metrics. In light of these findings, ourwork provides practical guidelines for the design and development of ubiquitoustechnologies that not only strive for accuracy but also for fairness.", "output": "Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Personal informatics (PI) systems, powered by smartphones and wearables,enable people to lead healthier lifestyles by providing meaningful andactionable insights that break down barriers between users and their healthinformation. Today, such systems are used by billions of users for monitoringnot only physical activity and sleep but also vital signs and women's and hearthealth, among others. %Despite their widespread usage, the processing ofparticularly sensitive personal data, and their proximity to domains known tobe susceptible to bias, such as healthcare, bias in PI has not beeninvestigated systematically. Despite their widespread usage, the processing ofsensitive PI data may suffer from biases, which may entail practical andethical implications. In this work, we present the first comprehensiveempirical and analytical study of bias in PI systems, including biases in rawdata and in the entire machine learning life cycle. We use the most detailedframework to date for exploring the different sources of bias and find thatbiases exist both in the data generation and the model learning andimplementation streams. According to our results, the most affected minoritygroups are users with health issues, such as diabetes, joint issues, andhypertension, and female users, whose data biases are propagated or evenamplified by learning models, while intersectional biases can also be observed.", "output": "Uncovering Bias in Personal Informatics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Publicly available collections of drug-like molecules have grown to comprise10s of billions of possibilities in recent history due to advances in chemicalsynthesis. Traditional methods for identifying ``hit'' molecules from a largecollection of potential drug-like candidates have relied on biophysical theoryto compute approximations to the Gibbs free energy of the binding interactionbetween the drug to its protein target. A major drawback of the approaches isthat they require exceptional computing capabilities to consider for evenrelatively small collections of molecules.Hyperdimensional Computing (HDC) is a recently proposed learning paradigmthat is able to leverage low-precision binary vector arithmetic to buildefficient representations of the data that can be obtained without the need forgradient-based optimization approaches that are required in many conventionalmachine learning and deep learning approaches. This algorithmic simplicityallows for acceleration in hardware that has been previously demonstrated for arange of application areas. We consider existing HDC approaches for molecularproperty classification and introduce two novel encoding algorithms thatleverage the extended connectivity fingerprint (ECFP) algorithm.We show that HDC-based inference methods are as much as 90 times moreefficient than more complex representative machine learning methods and achievean acceleration of nearly 9 orders of magnitude as compared to inference withmolecular docking. We demonstrate multiple approaches for the encoding ofmolecular data for HDC and examine their relative performance on a range ofchallenging molecular property prediction and drug-protein bindingclassification tasks. Our work thus motivates further investigation intomolecular representation learning to develop ultra-efficient pre-screeningtools.", "output": "HD-Bind: Encoding of Molecular Structure with Low Precision, Hyperdimensional Binary Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we consider the problem of learning online to manage DemandResponse (DR) resources. A typical DR mechanism requires the DR manager toassign a baseline to the participating consumer, where the baseline is anestimate of the counterfactual consumption of the consumer had it not beencalled to provide the DR service. A challenge in estimating baseline is theincentive the consumer has to inflate the baseline estimate. We consider theproblem of learning online to estimate the baseline and to optimize theoperating costs over a period of time under such incentives. We propose anonline learning scheme that employs least-squares for estimation with aperturbation to the reward price (for the DR services or load curtailment) thatis designed to balance the exploration and exploitation trade-off that ariseswith online learning. We show that, our proposed scheme is able to achieve avery low regret of $mathcal{O}left((log{T})^2right)$ with respect to theoptimal operating cost over $T$ days of the DR program with full knowledge ofthe baseline, and is individually rational for the consumers to participate.Our scheme is significantly better than the averaging type approach, which onlyfetches $mathcal{O}(T^{1/3})$ regret.", "output": "Online Learning for Incentive-Based Demand Response."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative modeling has seen a rising interest in both classical and quantummachine learning, and it represents a promising candidate to obtain a practicalquantum advantage in the near term. In this study, we build over a proposedframework for evaluating the generalization performance of generative models,and we establish the first quantitative comparative race towards practicalquantum advantage (PQA) between classical and quantum generative models, namelyQuantum Circuit Born Machines (QCBMs), Transformers (TFs), Recurrent NeuralNetworks (RNNs), Variational Autoencoders (VAEs), and Wasserstein GenerativeAdversarial Networks (WGANs). After defining four types of PQAs scenarios, wefocus on what we refer to as potential PQA, aiming to compare quantum modelswith the best-known classical algorithms for the task at hand. We let themodels race on a well-defined and application-relevant competition setting,where we illustrate and demonstrate our framework on 20 variables (qubits)generative modeling task. Our results suggest that QCBMs are more efficient inthe data-limited regime than the other state-of-the-art classical generativemodels. Such a feature is highly desirable in a wide range of real-worldapplications where the available data is scarce.", "output": "A Framework for Demonstrating Practical Quantum Advantage: Racing Quantum against Classical Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study proposes a novel framework for learning the underlying physics ofphenomena with moving boundaries. The proposed approach combines Ensemble SINDyand Peridynamic Differential Operator (PDDO) and imposes an inductive biasassuming the moving boundary physics evolve in its own corotational coordinatesystem. The robustness of the approach is demonstrated by considering variouslevels of noise in the measured data using the 2D Fisher-Stefan model. Theconfidence intervals of recovered coefficients are listed, and theuncertainties of the moving boundary positions are depicted by obtaining thesolutions with the recovered coefficients. Although the main focus of thisstudy is the Fisher-Stefan model, the proposed approach is applicable to anytype of moving boundary problem with a smooth moving boundary front without amushy region. The code and data for this framework is available at:", "output": "Multiphysics discovery with moving boundaries using Ensemble SINDy and Peridynamic Differential Operator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We design learning rate schedules that minimize regret for SGD-based onlinelearning in the presence of a changing data distribution. We fully characterizethe optimal learning rate schedule for online linear regression via a novelanalysis with stochastic differential equations. For general convex lossfunctions, we propose new learning rate schedules that are robust todistribution shift, and we give upper and lower bounds for the regret that onlydiffer by constants. For non-convex loss functions, we define a notion ofregret based on the gradient norm of the estimated models and propose alearning schedule that minimizes an upper bound on the total expected regret.Intuitively, one expects changing loss landscapes to require more exploration,and we confirm that optimal learning rate schedules typically increase in thepresence of distribution shift. Finally, we provide experiments forhigh-dimensional regression models and neural networks to illustrate theselearning rate schedules and their cumulative regret.", "output": "Learning Rate Schedules in the Presence of Distribution Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider dynamic pricing strategies in a streamed longitudinal data set-upwhere the objective is to maximize, over time, the cumulative profit across alarge number of customer segments. We consider a dynamic probit model with theconsumers' preferences as well as price sensitivity varying over time. Buildingon the well-known finding that consumers sharing similar characteristics act insimilar ways, we consider a global shrinkage structure, which assumes that theconsumers' preferences across the different segments can be well approximatedby a spatial autoregressive (SAR) model. In such a streamed longitudinalset-up, we measure the performance of a dynamic pricing policy via regret,which is the expected revenue loss compared to a clairvoyant that knows thesequence of model parameters in advance. We propose a pricing policy based onpenalized stochastic gradient descent (PSGD) and explicitly characterize itsregret as functions of time, the temporal variability in the model parametersas well as the strength of the auto-correlation network structure spanning thevaried customer segments. Our regret analysis results not only demonstrateasymptotic optimality of the proposed policy but also show that for policyplanning it is essential to incorporate available structural information aspolicies based on unshrunken models are highly sub-optimal in theaforementioned set-up.", "output": "Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diagnosis of adverse neonatal outcomes is crucial for preterm survival sinceit enables doctors to provide timely treatment. Machine learning (ML)algorithms have been demonstrated to be effective in predicting adverseneonatal outcomes. However, most previous ML-based methods have only focused onpredicting a single outcome, ignoring the potential correlations betweendifferent outcomes, and potentially leading to suboptimal results andoverfitting issues. In this work, we first analyze the correlations betweenthree adverse neonatal outcomes and then formulate the diagnosis of multipleneonatal outcomes as a multi-task learning (MTL) problem. We then propose anMTL framework to jointly predict multiple adverse neonatal outcomes. Inparticular, the MTL framework contains shared hidden layers and multipletask-specific branches. Extensive experiments have been conducted usingElectronic Health Records (EHRs) from 121 preterm neonates. Empirical resultsdemonstrate the effectiveness of the MTL framework. Furthermore, the featureimportance is analyzed for each neonatal outcome, providing insights into modelinterpretability.", "output": "Predicting Adverse Neonatal Outcomes for Preterm Neonates with Multi-Task Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, the authors propose a new approach to solving the groundwaterflow equation in the Toth basin of arbitrary top and bottom topographies usingdeep learning. Instead of using traditional numerical solvers, they use aDeepONet to produce the boundary-to-solution mapping. This mapping takes thegeometry of the physical domain along with the boundary conditions as inputs tooutput the steady state solution of the groundwater flow equation. To implementthe DeepONet, the authors approximate the top and bottom boundaries usingtruncated Fourier series or piecewise linear representations. They present twodifferent implementations of the DeepONet: one where the Toth basin is embeddedin a rectangular computational domain, and another where the Toth basin witharbitrary top and bottom boundaries is mapped into a rectangular computationaldomain via a nonlinear transformation. They implement the DeepONet with respectto the Dirichlet and Robin boundary condition at the top and the Neumannboundary condition at the impervious bottom boundary, respectively. Using thisdeep-learning enabled tool, the authors investigate the impact of surfacetopography on the flow pattern by both the top surface and the bottomimpervious boundary with arbitrary geometries. They discover that the averageslope of the top surface promotes long-distance transport, while the localcurvature controls localized circulations. Additionally, they find that theslope of the bottom impervious boundary can seriously impact the long-distancetransport of groundwater flows. Overall, this paper presents a new andinnovative approach to solving the groundwater flow equation using deeplearning, which allows for the investigation of the impact of surfacetopography on groundwater flow patterns.", "output": "Boundary-to-Solution Mapping for Groundwater Flows in a Toth Basin."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An additive manufacturing (AM) process, like laser powder bed fusion, allowsfor the fabrication of objects by spreading and melting powder in layers untila freeform part shape is created. In order to improve the properties of thematerial involved in the AM process, it is important to predict the materialcharacterization property as a function of the processing conditions. Inthermoelectric materials, the power factor is a measure of how efficiently thematerial can convert heat to electricity. While earlier works have predictedthe material characterization properties of different thermoelectric materialsusing various techniques, implementation of machine learning models to predictthe power factor of bismuth telluride (Bi2Te3) during the AM process has notbeen explored. This is important as Bi2Te3 is a standard material for lowtemperature applications. Thus, we used data about manufacturing processingparameters involved and in-situ sensor monitoring data collected during AM ofBi2Te3, to train different machine learning models in order to predict itsthermoelectric power factor. We implemented supervised machine learningtechniques using 80% training and 20% test data and further used thepermutation feature importance method to identify important processingparameters and in-situ sensor features which were best at predicting powerfactor of the material. Ensemble-based methods like random forest, AdaBoostclassifier, and bagging classifier performed the best in predicting powerfactor with the highest accuracy of 90% achieved by the bagging classifiermodel. Additionally, we found the top 15 processing parameters and in-situsensor features to characterize the material manufacturing property like powerfactor. These features could further be optimized to maximize power factor ofthe thermoelectric material and improve the quality of the products built usingthis material.", "output": "Predicting Thermoelectric Power Factor of Bismuth Telluride During Laser Powder Bed Fusion Additive Manufacturing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural text-to-speech (TTS) models can synthesize natural human speech whentrained on large amounts of transcribed speech. However, collecting suchlarge-scale transcribed data is expensive. This paper proposes an unsupervisedpre-training method for a sequence-to-sequence TTS model by leveraging largeuntranscribed speech data. With our pre-training, we can remarkably reduce theamount of paired transcribed data required to train the model for the targetdownstream TTS task. The main idea is to pre-train the model to reconstructde-warped mel-spectrograms from warped ones, which may allow the model to learnproper temporal assignment relation between input and output sequences. Inaddition, we propose a data augmentation method that further improves the dataefficiency in fine-tuning. We empirically demonstrate the effectiveness of ourproposed method in low-resource language scenarios, achieving outstandingperformance compared to competing methods. The code and audio samples areavailable at: ", "output": "Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimization problems involving sequential decisions in a stochasticenvironment were studied in Stochastic Programming (SP), Stochastic OptimalControl (SOC) and Markov Decision Processes (MDP). In this paper we mainlyconcentrate on SP and SOC modelling approaches. In these frameworks there arenatural situations when the considered problems are convex. Classical approachto sequential optimization is based on dynamic programming. It has the problemof the so-called ``Curse of Dimensionality\", in that its computationalcomplexity increases exponentially with increase of dimension of statevariables. Recent progress in solving convex multistage stochastic problems isbased on cutting planes approximations of the cost-to-go (value) functions ofdynamic programming equations. Cutting planes type algorithms in dynamicalsettings is one of the main topics of this paper. We also discuss StochasticApproximation type methods applied to multistage stochastic optimizationproblems. From the computational complexity point of view, these two types ofmethods seem to be complimentary to each other. Cutting plane type methods canhandle multistage problems with a large number of stages, but a relativelysmaller number of state (decision) variables. On the other hand, stochasticapproximation type methods can only deal with a small number of stages, but alarge number of decision variables.", "output": "Numerical Methods for Convex Multistage Stochastic Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation (KD) is an effective training strategy to improve thelightweight student models under the guidance of cumbersome teachers. However,the large architecture difference across the teacher-student pairs limits thedistillation gains. In contrast to previous adaptive distillation methods toreduce the teacher-student gap, we explore a novel training-free framework tosearch for the best student architectures for a given teacher. Our work firstempirically show that the optimal model under vanilla training cannot be thewinner in distillation. Secondly, we find that the similarity of featuresemantics and sample relations between random-initialized teacher-studentnetworks have good correlations with final distillation performances. Thus, weefficiently measure similarity matrixs conditioned on the semantic activationmaps to select the optimal student via an evolutionary algorithm without anytraining. In this way, our student architecture search for Distillation WithOutTraining (DisWOT) significantly improves the performance of the model in thedistillation stage with at least 180$times$ training acceleration.Additionally, we extend similarity metrics in DisWOT as new distillers andKD-based zero-proxies. Our experiments on CIFAR, ImageNet and NAS-Bench-201demonstrate that our technique achieves state-of-the-art results on differentsearch spaces. Our project and code are available at", "output": "DisWOT: Student Architecture Search for Distillation WithOut Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Physics-based deep learning frameworks have shown to be effective inaccurately modeling the dynamics of complex physical systems withgeneralization capability across problem inputs. However, time-independentproblems pose the challenge of requiring long-range exchange of informationacross the computational domain for obtaining accurate predictions. In thecontext of graph neural networks (GNNs), this calls for deeper networks, which,in turn, may compromise or slow down the training process. In this work, wepresent two GNN architectures to overcome this challenge - the Edge AugmentedGNN and the Multi-GNN. We show that both these networks perform significantlybetter (by a factor of 1.5 to 2) than baseline methods when applied totime-independent solid mechanics problems. Furthermore, the proposedarchitectures generalize well to unseen domains, boundary conditions, andmaterials. Here, the treatment of variable domains is facilitated by a novelcoordinate transformation that enables rotation and translation invariance. Bybroadening the range of problems that neural operators based on graph neuralnetworks can tackle, this paper provides the groundwork for their applicationto complex scientific and industrial settings.", "output": "GNN-based physics solver for time-independent PDEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning transferable representation of knowledge graphs (KGs) is challengingdue to the heterogeneous, multi-relational nature of graph structures. Inspiredby Transformer-based pretrained language models' success on learningtransferable representation for texts, we introduce a novel inductive KGrepresentation model (iHT) for KG completion by large-scale pre-training. iHTconsists of a entity encoder (e.g., BERT) and a neighbor-aware relationalscoring function both parameterized by Transformers. We first pre-train iHT ona large KG dataset, Wikidata5M. Our approach achieves new state-of-the-artresults on matched evaluations, with a relative improvement of more than 25% inmean reciprocal rank over previous SOTA models. When further fine-tuned onsmaller KGs with either entity and relational shifts, pre-trained iHTrepresentations are shown to be transferable, significantly improving theperformance on FB15K-237 and WN18RR.", "output": "Pre-training Transformers for Knowledge Graph Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The success of existing multi-view clustering relies on the assumption ofsample integrity across multiple views. However, in real-world scenarios,samples of multi-view are partially available due to data corruption or sensorfailure, which leads to incomplete multi-view clustering study (IMVC). Althoughseveral attempts have been proposed to address IMVC, they suffer from thefollowing drawbacks: i) Existing methods mainly adopt cross-view contrastivelearning forcing the representations of each sample across views to be exactlythe same, which might ignore view discrepancy and flexibility inrepresentations; ii) Due to the absence of non-observed samples across multipleviews, the obtained prototypes of clusters might be unaligned and biased,leading to incorrect fusion. To address the above issues, we propose aCross-view Partial Sample and Prototype Alignment Network (CPSPAN) for DeepIncomplete Multi-view Clustering. Firstly, unlike existing contrastive-basedmethods, we adopt pair-observed data alignment as 'proxy supervised signals' toguide instance-to-instance correspondence construction among views. Then,regarding of the shifted prototypes in IMVC, we further propose a prototypealignment module to achieve incomplete distribution calibration across views.Extensive experimental results showcase the effectiveness of our proposedmodules, attaining noteworthy performance improvements when compared toexisting IMVC competitors on benchmark datasets.", "output": "Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and Prototype Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pretraining a deep learning model on large image datasets is a standard stepbefore fine-tuning the model on small targeted datasets. The large dataset isusually general images (e.g. imagenet2012) while the small dataset can bespecialized datasets that have different distributions from the large dataset.However, this 'large-to-small' strategy is not well-validated when the largedataset is specialized and has a similar distribution to small datasets. Wenewly compiled three hematoxylin and eosin-stained image datasets, one large(PTCGA200) and two magnification-adjusted small datasets (PCam200 andsegPANDA200). Major deep learning models were trained with supervised andself-supervised learning methods and fine-tuned on the small datasets for tumorclassification and tissue segmentation benchmarks. ResNet50 pretrained withMoCov2, SimCLR, and BYOL on PTCGA200 was better than imagenet2012 pretrainingwhen fine-tuned on PTCGA200 (accuracy of 83.94%, 86.41%, 84.91%, and 82.72%,respectively). ResNet50 pre-trained on PTCGA200 with MoCov2 exceeded theCOCOtrain2017-pretrained baseline and was the best in ResNet50 for the tissuesegmentation benchmark (mIoU of 63.53% and 63.22%). We found re-trainingimagenet-pretrained models (ResNet50, BiT-M-R50x1, and ViT-S/16) on PTCGA200improved downstream benchmarks.", "output": "Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph embedding maps graph nodes to low-dimensional vectors, and is widelyadopted in machine learning tasks. The increasing availability of billion-edgegraphs underscores the importance of learning efficient and effectiveembeddings on large graphs, such as link prediction on Twitter with over onebillion edges. Most existing graph embedding methods fall short of reachinghigh data scalability. In this paper, we present a general-purpose,distributed, information-centric random walk-based graph embedding framework,DistGER, which can scale to embed billion-edge graphs. DistGER incrementallycomputes information-centric random walks. It further leverages amulti-proximity-aware, streaming, parallel graph partitioning strategy,simultaneously achieving high local partition quality and excellent workloadbalancing across machines. DistGER also improves the distributed Skip-Gramlearning model to generate node embeddings by optimizing the access locality,CPU throughput, and synchronization efficiency. Experiments on real-worldgraphs demonstrate that compared to state-of-the-art distributed graphembedding frameworks, including KnightKing, DistDGL, and Pytorch-BigGraph,DistGER exhibits 2.33x-129x acceleration, 45% reduction in cross-machinescommunication, and &gt; 10% effectiveness improvement in downstream tasks.", "output": "Distributed Graph Embedding with Information-Oriented Random Walks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models have been shown to perform remarkably well on a wide range ofnatural language processing tasks. In this paper, we propose a novel systemthat uses language models to perform multi-step logical reasoning. Our systemincorporates explicit planning into its inference procedure, thus able to makemore informed reasoning decisions at each step by looking ahead into theirfuture effects. In our experiments, our full system significantly outperformsother competing systems. On a multiple-choice question answering task, oursystem performs competitively compared to GPT-3-davinci despite having onlyaround 1.5B parameters. We conduct several ablation studies to demonstrate thatexplicit planning plays a crucial role in the system's performance.", "output": "Explicit Planning Helps Language Models in Logical Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern machine learning, attention computation is a fundamental task fortraining large language models such as Transformer, GPT-4 and ChatGPT. In thiswork, we study exponential regression problem which is inspired by thesoftmax/exp unit in the attention mechanism in large language models. Thestandard exponential regression is non-convex. We study the regularizationversion of exponential regression problem which is a convex problem. We useapproximate newton method to solve in input sparsity time.Formally, in this problem, one is given matrix $A in mathbb{R}^{n timesd}$, $b in mathbb{R}^n$, $w in mathbb{R}^n$ and any of functions $exp,cosh$ and $sinh$ denoted as $f$. The goal is to find the optimal $x$ thatminimize $ 0.5 | f(Ax) - b |_2^2 + 0.5 | mathrm{diag}(w) A x |_2^2$. Thestraightforward method is to use the naive Newton's method. Let$mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let$omega$ denote the exponent of matrix multiplication. Currently, $omegaapprox 2.373$. Let $epsilon$ denote the accuracy error. In this paper, wemake use of the input sparsity and purpose an algorithm that use $log ( |x_0- x^*|_2 / epsilon)$ iterations and $widetilde{O}(mathrm{nnz}(A) +d^{omega} )$ per iteration time to solve the problem.", "output": "Solving Regularized Exp, Cosh and Sinh Regression Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many research fields in artificial intelligence, it has been shown thatdeep neural networks are useful to estimate unknown functions on highdimensional input spaces. However, their generalization performance is not yetcompletely clarified from the theoretical point of view because they arenonidentifiable and singular learning machines. Moreover, a ReLU function isnot differentiable, to which algebraic or analytic methods in singular learningtheory cannot be applied. In this paper, we study a deep ReLU neural network inoverparametrized cases and prove that the Bayesian free energy, which is equalto the minus log marginal likelihoodor the Bayesian stochastic complexity, isbounded even if the number of layers are larger than necessary to estimate anunknown data-generating function. Since the Bayesian generalization error isequal to the increase of the free energy as a function of a sample size, ourresult also shows that the Bayesian generalization error does not increase evenif a deep ReLU neural network is designed to be sufficiently large or in anopeverparametrized state.", "output": "Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we study the concentration behavior of a stochasticapproximation (SA) algorithm under a contractive operator with respect to anarbitrary norm. We consider two settings where the iterates are potentiallyunbounded: (1) bounded multiplicative noise, and (2) additive sub-Gaussiannoise. We obtain maximal concentration inequalities on the convergence errors,and show that these errors have sub-Gaussian tails in the additive noisesetting, and super-polynomial tails (faster than polynomial decay) in themultiplicative noise setting. In addition, we provide an impossibility resultshowing that it is in general not possible to achieve sub-exponential tails forSA with multiplicative noise. To establish these results, we develop a novelbootstrapping argument that involves bounding the moment generating function ofthe generalized Moreau envelope of the error and the construction of anexponential supermartingale to enable using Ville's maximal inequality.To demonstrate the applicability of our theoretical results, we use them toprovide maximal concentration bounds for a large class of reinforcementlearning algorithms, including but not limited to on-policy TD-learning withlinear function approximation, off-policy TD-learning with generalizedimportance sampling factors, and $Q$-learning. To the best of our knowledge,super-polynomial concentration bounds for off-policy TD-learning have not beenestablished in the literature due to the challenge of handling the combinationof unbounded iterates and multiplicative noise.", "output": "Concentration of Contractive Stochastic Approximation: Additive and Multiplicative Noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this technical report, we explore the behavior of Recursive FeatureMachines (RFMs), a type of novel kernel machine that recursively learnsfeatures via the average gradient outer product, through a series ofexperiments on regression datasets. When successively adding random noisefeatures to a dataset, we observe intriguing patterns in the Mean Squared Error(MSE) curves with the test MSE exhibiting a decrease-increase-decrease pattern.This behavior is consistent across different dataset sizes, noise parameters,and target functions. Interestingly, the observed MSE curves show similaritiesto the \"double descent\" phenomenon observed in deep neural networks, hinting atnew connection between RFMs and neural network behavior. This report lays thegroundwork for future research into this peculiar behavior.", "output": "On Feature Scaling of Recursive Feature Machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Preferential Bayesian optimization (PBO) is a framework for optimizing adecision maker's latent utility function using preference feedback. This workintroduces the expected utility of the best option (qEUBO) as a novelacquisition function for PBO. When the decision maker's responses arenoise-free, we show that qEUBO is one-step Bayes optimal and thus equivalent tothe popular knowledge gradient acquisition function. We also show that qEUBOenjoys an additive constant approximation guarantee to the one-stepBayes-optimal policy when the decision maker's responses are corrupted bynoise. We provide an extensive evaluation of qEUBO and demonstrate that itoutperforms the state-of-the-art acquisition functions for PBO across manysettings. Finally, we show that, under sufficient regularity conditions,qEUBO's Bayesian simple regret converges to zero at a rate $o(1/n)$ as thenumber of queries, $n$, goes to infinity. In contrast, we show that simpleregret under qEI, a popular acquisition function for standard BO often used forPBO, can fail to converge to zero. Enjoying superior performance, simplecomputation, and a grounded decision-theoretic justification, qEUBO is apromising acquisition function for PBO.", "output": "qEUBO: A Decision-Theoretic Acquisition Function for Preferential Bayesian Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present emph{TabRet}, a pre-trainable Transformer-based model for tabulardata. TabRet is designed to work on a downstream task that contains columns notseen in pre-training. Unlike other methods, TabRet has an extra learning stepbefore fine-tuning called emph{retokenizing}, which calibrates featureembeddings based on the masked autoencoding loss. In experiments, wepre-trained TabRet with a large collection of public health surveys andfine-tuned it on classification tasks in healthcare, and TabRet achieved thebest AUC performance on four datasets. In addition, an ablation study showsretokenizing and random shuffle augmentation of columns during pre-trainingcontributed to performance gains.", "output": "TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influencesociety, warranting immediate social attention. While the models themselvesgarner much attention, to accurately characterize their impact, we mustconsider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as adocumentation framework to transparently centralize knowledge of thisecosystem. Ecosystem Graphs is composed of assets (datasets, models,applications) linked together by dependencies that indicate technical (e.g. howBing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI)relationships. To supplement the graph structure, each asset is furtherenriched with fine-grained metadata (e.g. the license or training emissions).We document the ecosystem extensively at As of March 16, 2023, we annotate262 assets (64 datasets, 128 models, 70 applications) from 63 organizationslinked by 356 dependencies. We show Ecosystem Graphs functions as a powerfulabstraction and interface for achieving the minimum transparency required toaddress myriad use cases. Therefore, we envision Ecosystem Graphs will be acommunity-maintained resource that provides value to stakeholders spanning AIresearchers, industry professionals, social scientists, auditors andpolicymakers.", "output": "Ecosystem Graphs: The Social Footprint of Foundation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) is a novel machine learning framework, which enablesmultiple distributed devices cooperatively training a shared model scheduled bya central server while protecting private data locally. However, thenon-independent-and-identically-distributed (Non-IID) data samples and frequentcommunication among participants will slow down the convergent rate andincrease communication costs. To achieve fast convergence, we ameliorate thelocal gradient descend approach in conventional local update rule byintroducing the aggregated gradients at each local update epoch, and propose anadaptive learning rate algorithm that further takes the deviation of localparameter and global parameter into consideration at each iteration. The abovestrategy requires all clients' local parameters and gradients at each localiteration, which is challenging as there is no communication during localupdate epochs. Accordingly, we utilize mean field approach by introducing twomean field terms to estimate the average local parameters and gradientsrespectively, which does not require clients to exchange their privateinformation with each other at each local update epoch. Numerical results showthat our proposed framework is superior to the state-of-art schemes in modelaccuracy and convergent rate on both IID and Non-IID dataset.", "output": "Fast Convergence Federated Learning with Aggregated Gradients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the most interesting problems in the recent renaissance of the studiesin kernel regression might be whether the kernel interpolation can generalizewell, since it may help us understand the `benign overfitting henomenon'reported in the literature on deep networks. In this paper, under mildconditions, we show that for any $varepsilon&gt;0$, the generalization error ofkernel interpolation is lower bounded by $Omega(n^{-varepsilon})$. In otherwords, the kernel interpolation generalizes poorly for a large class ofkernels. As a direct corollary, we can show that overfitted wide neuralnetworks defined on sphere generalize poorly.", "output": "Kernel interpolation generalizes poorly."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most offline reinforcement learning (RL) methods suffer from the trade-offbetween improving the policy to surpass the behavior policy and constrainingthe policy to limit the deviation from the behavior policy as computing$Q$-values using out-of-distribution (OOD) actions will suffer from errors dueto distributional shift. The recently proposed textit{In-sample Learning}paradigm (i.e., IQL), which improves the policy by quantile regression usingonly data samples, shows great promise because it learns an optimal policywithout querying the value function of any unseen actions. However, it remainsunclear how this type of method handles the distributional shift in learningthe value function. In this work, we make a key finding that the in-samplelearning paradigm arises under the textit{Implicit Value Regularization} (IVR)framework. This gives a deeper understanding of why the in-sample learningparadigm works, i.e., it applies implicit value regularization to the policy.Based on the IVR framework, we further propose two practical algorithms, Sparse$Q$-learning (SQL) and Exponential $Q$-learning (EQL), which adopt the samevalue regularization used in existing works, but in a complete in-samplemanner. Compared with IQL, we find that our algorithms introduce sparsity inlearning the value function, making them more robust in noisy data regimes. Wealso verify the effectiveness of SQL and EQL on D4RL benchmark datasets andshow the benefits of in-sample learning by comparing them with CQL in smalldata regimes.", "output": "Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of security, multi-objective security games (MOSGs) allowdefenders to simultaneously protect targets from multiple heterogeneousattackers. MOSGs aim to simultaneously maximize all the heterogeneous payoffs,e.g., life, money, and crime rate, without merging heterogeneous attackers. Inreal-world scenarios, the number of heterogeneous attackers and targets to beprotected may exceed the capability of most existing state-of-the-art methods,i.e., MOSGs are limited by the issue of scalability. To this end, this paperproposes a general framework called SDES based on many-objective evolutionarysearch to scale up MOSGs to large-scale targets and heterogeneous attackers.SDES consists of four consecutive key components, i.e., discretization,optimization, restoration and evaluation, and refinement. Specifically, SDESfirst discretizes the originally high-dimensional continuous solution space tothe low-dimensional discrete one by the maximal indifference property in gametheory. This property helps evolutionary algorithms (EAs) bypass thehigh-dimensional step function and ensure a well-convergent Pareto front. Then,a many-objective EA is used for optimization in the low-dimensional discretesolution space to obtain a well-spaced Pareto front. To evaluate solutions,SDES restores solutions back to the original space via bit-wisely optimizing anovel solution divergence. Finally, the refinement in SDES boosts theoptimization performance with acceptable cost. Theoretically, we prove theoptimization consistency and convergence of SDES. Experiment results show thatSDES is the first linear-time MOSG algorithm for both large-scale attackers andtargets. SDES is able to solve up to 20 attackers and 100 targets MOSGproblems, while the state-of-the-art methods can only solve up to 8 attackersand 25 targets ones. Ablation study verifies the necessity of all components inSDES.", "output": "Scaling Multi-Objective Security Games Provably via Space Discretization Based Evolutionary Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimization of accelerator performance parameters is limited by numeroustrade-offs and finding the appropriate balance between optimization goals foran unknown system is challenging to achieve. Here we show that multi-objectiveBayesian optimization can map the solution space of a laser wakefieldaccelerator in a very sample-efficient way. Using a Gaussian mixture model, weisolate contributions related to an electron bunch at a certain energy and weobserve that there exists a wide range of Pareto-optimal solutions that tradebeam energy versus charge at similar laser-to-beam efficiency. However, manyapplications such as light sources require particle beams at a certain targetenergy. Once such a constraint is introduced we observe a direct trade-offbetween energy spread and accelerator efficiency. We furthermore demonstratehow specific solutions can be exploited using emph{a posteriori} scalarizationof the objectives, thereby efficiently splitting the exploration andexploitation phases.", "output": "Pareto Optimization of a Laser Wakefield Accelerator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an explainable method for solving Partial Differential Equationsby using a contextual scheme called PDExplain. During the training phase, ourmethod is fed with data collected from an operator-defined family of PDEsaccompanied by the general form of this family. In the inference phase, aminimal sample collected from a phenomenon is provided, where the sample isrelated to the PDE family but not necessarily to the set of specific PDEs seenin the training phase. We show how our algorithm can predict the PDE solutionfor future timesteps. Moreover, our method provides an explainable form of thePDE, a trait that can assist in modelling phenomena based on data in physicalsciences. To verify our method, we conduct extensive experimentation, examiningits quality both in terms of prediction error and explainability.", "output": "PDExplain: Contextual Modeling of PDEs in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual domain shift poses a significant challenge in real-worldapplications, particularly in situations where labeled data is not availablefor new domains. The challenge of acquiring knowledge in this problem settingis referred to as unsupervised continual domain shift learning. Existingmethods for domain adaptation and generalization have limitations in addressingthis issue, as they focus either on adapting to a specific domain orgeneralizing to unseen domains, but not both. In this paper, we proposeComplementary Domain Adaptation and Generalization (CoDAG), a simple yeteffective learning framework that combines domain adaptation and generalizationin a complementary manner to achieve three major goals of unsupervisedcontinual domain shift learning: adapting to a current domain, generalizing tounseen domains, and preventing forgetting of previously seen domains. Ourapproach is model-agnostic, meaning that it is compatible with any existingdomain adaptation and generalization algorithms. We evaluate CoDAG on severalbenchmark datasets and demonstrate that our model outperforms state-of-the-artmodels in all datasets and evaluation metrics, highlighting its effectivenessand robustness in handling unsupervised continual domain shift learning.", "output": "Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Successful analytics solutions that provide valuable insights often hinge onthe connection of various data sources. While it is often feasible to generatelarger data pools within organizations, the application of analytics within(inter-organizational) business networks is still severely constrained. As datais distributed across several legal units, potentially even across countries,the fear of disclosing sensitive information as well as the sheer volume of thedata that would need to be exchanged are key inhibitors for the creation ofeffective system-wide solutions -- all while still reaching superior predictionperformance. In this work, we propose a meta machine learning method that dealswith these obstacles to enable comprehensive analyses within a businessnetwork. We follow a design science research approach and evaluate our methodwith respect to feasibility and performance in an industrial use case. First,we show that it is feasible to perform network-wide analyses that preserve dataconfidentiality as well as limit data transfer volume. Second, we demonstratethat our method outperforms a conventional isolated analysis and even getsclose to a (hypothetical) scenario where all data could be shared within thenetwork. Thus, we provide a fundamental contribution for making businessnetworks more effective, as we remove a key obstacle to tap the huge potentialof learning from data that is scattered throughout the network.", "output": "Enabling Inter-organizational Analytics in Business Networks Through Meta Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predictive process analytics focuses on predicting future states, such as theoutcome of running process instances. These techniques often use machinelearning models or deep learning models (such as LSTM) to make suchpredictions. However, these deep models are complex and difficult for users tounderstand. Counterfactuals answer ``what-if'' questions, which are used tounderstand the reasoning behind the predictions. For example, what if insteadof emailing customers, customers are being called? Would this alternative leadto a different outcome? Current methods to generate counterfactual sequenceseither do not take the process behavior into account, leading to generatinginvalid or infeasible counterfactual process instances, or heavily rely ondomain knowledge. In this work, we propose a general framework that usesevolutionary methods to generate counterfactual sequences. Our framework doesnot require domain knowledge. Instead, we propose to train a Markov model tocompute the feasibility of generated counterfactual sequences and adapt threeother measures (delta in outcome prediction, similarity, and sparsity) toensure their overall viability. The evaluation shows that we generate viablecounterfactual sequences, outperform baseline methods in viability, and yieldsimilar results when compared to the state-of-the-art method that requiresdomain knowledge.", "output": "CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conditional generative models became a very powerful tool to sample fromBayesian inverse problem posteriors. It is well-known in classical Bayesianliterature that posterior measures are quite robust with respect toperturbations of both the prior measure and the negative log-likelihood, whichincludes perturbations of the observations. However, to the best of ourknowledge, the robustness of conditional generative models with respect toperturbations of the observations has not been investigated yet. In this paper,we prove for the first time that appropriately learned conditional generativemodels provide robust results for single observations.", "output": "Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate different natural language processing (NLP) approaches basedon contextualised word representations for the problem of early prediction oflung cancer using free-text patient medical notes of Dutch primary carephysicians. Because lung cancer has a low prevalence in primary care, we alsoaddress the problem of classification under highly imbalanced classes.Specifically, we use large Transformer-based pretrained language models (PLMs)and investigate: 1) how textit{soft prompt-tuning} -- an NLP technique used toadapt PLMs using small amounts of training data -- compares to standard modelfine-tuning; 2) whether simpler static word embedding models (WEMs) can be morerobust compared to PLMs in highly imbalanced settings; and 3) how models farewhen trained on notes from a small number of patients. We find that 1)soft-prompt tuning is an efficient alternative to standard model fine-tuning;2) PLMs show better discrimination but worse calibration compared to simplerstatic word embedding models as the classification problem becomes moreimbalanced; and 3) results when training models on small number of patients aremixed and show no clear differences between PLMs and WEMs. All our code isavailable open source inurl{", "output": "Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With recent study of the deep learning in scientific computation, the PINNsmethod has drawn widespread attention for solving PDEs. Compared withtraditional methods, PINNs can efficiently handle high-dimensional problems,while the accuracy is relatively low, especially for highly irregular problems.Inspired by the idea of adaptive finite element methods and incrementallearning, we propose GAS, a Gaussian mixture distribution-based adaptivesampling method for PINNs. During the training procedure, GAS uses the currentresidual information to generate a Gaussian mixture distribution for thesampling of additional points, which are then trained together with historydata to speed up the convergence of loss and achieve a higher accuracy. Severalnumerical simulations on 2d to 10d problems show that GAS is a promising methodwhich achieves the state-of-the-art accuracy among deep solvers, while beingcomparable with traditional numerical solvers.", "output": "GAS: A Gaussian Mixture Distribution-Based Adaptive Sampling Method for PINNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Segmentation uncertainty models predict a distribution over plausiblesegmentations for a given input, which they learn from the annotator variationin the training set. However, in practice these annotations can differsystematically in the way they are generated, for example through the use ofdifferent labeling tools. This results in datasets that contain both datavariability and differing label styles. In this paper, we demonstrate thatapplying state-of-the-art segmentation uncertainty models on such datasets canlead to model bias caused by the different label styles. We present an updatedmodelling objective conditioning on labeling style for aleatoric uncertaintyestimation, and modify two state-of-the-art-architectures for segmentationuncertainty accordingly. We show with extensive experiments that this methodreduces label style bias, while improving segmentation performance, increasingthe applicability of segmentation uncertainty models in the wild. We curate twodatasets, with annotations in different label styles, which we will makepublicly available along with our code upon publication.", "output": "That Label's Got Style: Handling Label Style Bias for Uncertain Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Images generated by high-resolution SAR have vast areas of application asthey can work better in adverse light and weather conditions. One such area ofapplication is in the military systems. This study is an attempt to explore thesuitability of current state-of-the-art models introduced in the domain ofcomputer vision for SAR target classification (MSTAR). Since the application ofany solution produced for military systems would be strategic and real-time,accuracy is often not the only criterion to measure its performance. Otherimportant parameters like prediction time and input resiliency are equallyimportant. The paper deals with these issues in the context of SAR images.Experimental results show that deep learning models can be suitably applied inthe domain of SAR image classification with the desired performance levels.", "output": "Exploring Deep Learning Methods for Classification of SAR Images: Towards NextGen Convolutions via Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wireless fingerprinting refers to a device identification method leveraginghardware imperfections and wireless channel variations as signatures. Beyondphysical layer characteristics, recent studies demonstrated that userbehaviours could be identified through network traffic, e.g., packet length,without decryption of the payload. Inspired by these results, we propose amulti-layer fingerprinting framework that jointly considers the multi-layersignatures for improved identification performance. In contrast to previousworks, by leveraging the recent multi-view machine learning paradigm, i.e.,data with multiple forms, our method can cluster the device information sharedamong the multi-layer features without supervision. Our information-theoreticapproach can be extended to supervised and semi-supervised settings withstraightforward derivations. In solving the formulated problem, we obtain atight surrogate bound using variational inference for efficient optimization.In extracting the shared device information, we develop an algorithm based onthe Wyner common information method, enjoying reduced computation complexity ascompared to existing approaches. The algorithm can be applied to datadistributions belonging to the exponential family class. Empirically, weevaluate the algorithm in a synthetic dataset with real-world video traffic andsimulated physical layer characteristics. Our empirical results show that theproposed method outperforms the state-of-the-art baselines in both supervisedand unsupervised settings.", "output": "The Wyner Variational Autoencoder for Unsupervised Multi-Layer Wireless Fingerprinting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we adopt Wyner common information framework for unsupervisedmulti-view representation learning. Within this framework, we propose two novelformulations that enable the development of computational efficient solversbased on the alternating minimization principle. The first formulation,referred to as the {em variational form}, enjoys a linearly growing complexitywith the number of views and is based on a variational-inference tightsurrogate bound coupled with a Lagrangian optimization objective function. Thesecond formulation, i.e., the {em representational form}, is shown to includeknown results as special cases. Here, we develop a tailored version from thealternating direction method of multipliers (ADMM) algorithm for solving theresulting non-convex optimization problem. In the two cases, the convergence ofthe proposed solvers is established in certain relevant regimes. Furthermore,our empirical results demonstrate the effectiveness of the proposed methods ascompared with the state-of-the-art solvers. In a nutshell, the proposed solversoffer computational efficiency, theoretical convergence guarantees, scalablecomplexity with the number of views, and exceptional accuracy as compared withthe state-of-the-art techniques. Our focus here is devoted to the discrete caseand our results for continuous distributions are reported elsewhere.", "output": "Efficient Alternating Minimization Solvers for Wyner Multi-View Unsupervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Distributed learning on the edge often comprises self-centered devices (SCD)which learn local tasks independently and are unwilling to contribute to theperformance of other SDCs. How do we achieve forward transfer at zero cost forthe single SCDs? We formalize this problem as a Distributed Continual Learningscenario, where SCD adapt to local tasks and a CL model consolidates theknowledge from the resulting stream of models without looking at the SCD'sprivate data. Unfortunately, current CL methods are not directly applicable tothis scenario. We propose Data-Agnostic Consolidation (DAC), a novel doubleknowledge distillation method that consolidates the stream of SC models withoutusing the original data. DAC performs distillation in the latent space via anovel Projected Latent Distillation loss. Experimental results show that DACenables forward transfer between SCDs and reaches state-of-the-art accuracy onSplit CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free anddistributed CL scenarios. Somewhat surprisingly, even a singleout-of-distribution image is sufficient as the only source of data duringconsolidation.", "output": "Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce VIVE3D, a novel approach that extends the capabilities ofimage-based 3D GANs to video editing and is able to represent the input videoin an identity-preserving and temporally consistent way. We propose two newbuilding blocks. First, we introduce a novel GAN inversion techniquespecifically tailored to 3D GANs by jointly embedding multiple frames andoptimizing for the camera parameters. Second, besides traditional semantic faceedits (e.g. for age and expression), we are the first to demonstrate edits thatshow novel views of the head enabled by the inherent properties of 3D GANs andour optical flow-guided compositing technique to combine the head with thebackground video. Our experiments demonstrate that VIVE3D generateshigh-fidelity face edits at consistent quality from a range of cameraviewpoints which are composited with the original video in a temporally andspatially consistent manner.", "output": "VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quality diversity algorithms can be used to efficiently create a diverse setof solutions to inform engineers' intuition. But quality diversity is notefficient in very expensive problems, needing 100.000s of evaluations. Evenwith the assistance of surrogate models, quality diversity needs 100s or even1000s of evaluations, which can make it use infeasible. In this study we try totackle this problem by using a pre-optimization strategy on a lower-dimensionaloptimization problem and then map the solutions to a higher-dimensional case.For a use case to design buildings that minimize wind nuisance, we show that wecan predict flow features around 3D buildings from 2D flow features aroundbuilding footprints. For a diverse set of building designs, by sampling thespace of 2D footprints with a quality diversity algorithm, a predictive modelcan be trained that is more accurate than when trained on a set of footprintsthat were selected with a space-filling algorithm like the Sobol sequence.Simulating only 16 buildings in 3D, a set of 1024 building designs with lowpredicted wind nuisance is created. We show that we can produce better machinelearning models by producing training data with quality diversity instead ofusing common sampling techniques. The method can bootstrap generative design ina computationally expensive 3D domain and allow engineers to sweep the designspace, understanding wind nuisance in early design phases.", "output": "Efficient Quality Diversity Optimization of 3D Buildings through 2D Pre-optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial attacks significantly threaten the robustness of deep neuralnetworks (DNNs). Despite the multiple defensive methods employed, they arenevertheless vulnerable to poison attacks, where attackers meddle with theinitial training data. In order to defend DNNs against such adversarialattacks, this work proposes a novel method that combines the defensivedistillation mechanism with a denoising autoencoder (DAE). This technique triesto lower the sensitivity of the distilled model to poison attacks by spottingand reconstructing poisonous adversarial inputs in the training data. We addedcarefully created adversarial samples to the initial training data to assessthe proposed method's performance. Our experimental findings demonstrate thatour method successfully identified and reconstructed the poisonous inputs whilealso considering enhancing the DNN's resilience. The proposed approach providesa potent and robust defense mechanism for DNNs in various applications wheredata poisoning attacks are a concern. Thus, the defensive distillationtechnique's limitation posed by poisonous adversarial attacks is overcome.", "output": "Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has proven to be successful in various domains and fordifferent tasks. However, when it comes to private data several restrictionsare making it difficult to use deep learning approaches in these applicationfields. Recent approaches try to generate data privately instead of applying aprivacy-preserving mechanism directly, on top of the classifier. The solutionis to create public data from private data in a manner that preserves theprivacy of the data. In this work, two very prominent GAN-based architectureswere evaluated in the context of private time series classification. Incontrast to previous work, mostly limited to the image domain, the scope ofthis benchmark was the time series domain. The experiments show that especiallyGSWGAN performs well across a variety of public datasets outperforming thecompetitor DPWGAN. An analysis of the generated datasets further validates thesuperiority of GSWGAN in the context of time series generation.", "output": "From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world visual data exhibit intrinsic hierarchical structures that can berepresented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs)are a promising approach for learning feature representations in such spaces.However, current methods in computer vision rely on Euclidean backbones andonly project features to the hyperbolic space in the task heads, limiting theirability to fully leverage the benefits of hyperbolic geometry. To address this,we present HCNN, the first fully hyperbolic convolutional neural network (CNN)designed for computer vision tasks. Based on the Lorentz model, we generalizefundamental components of CNNs and propose novel formulations of theconvolutional layer, batch normalization, and multinomial logistic regression(MLR). Experimentation on standard vision tasks demonstrates the effectivenessof our HCNN framework and the Lorentz model in both hybrid and fully hyperbolicsettings. Overall, we aim to pave the way for future research in hyperboliccomputer vision by offering a new paradigm for interpreting and analyzingvisual data. Our code is publicly available at", "output": "Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "HERMES (High Energy Rapid Modular Ensemble of Satellites) pathfinder is anin-orbit demonstration consisting of a constellation of six 3U nano-satelliteshosting simple but innovative detectors for the monitoring of cosmichigh-energy transients. The main objective of HERMES Pathfinder is to provethat accurate position of high-energy cosmic transients can be obtained usingminiaturized hardware. The transient position is obtained by studying the delaytime of arrival of the signal to different detectors hosted by nano-satelliteson low Earth orbits. To this purpose, the goal is to achive an overall accuracyof a fraction of a micro-second. In this context, we need to develop noveltools to fully exploit the future scientific data output of HERMES Pathfinder.In this paper, we introduce a new framework to assess the background count rateof a space-born, high energy detector; a key step towards the identification offaint astrophysical transients. We employ a Neural Network (NN) to estimate thebackground lightcurves on different timescales. Subsequently, we employ a fastchange-point and anomaly detection technique to isolate observation segmentswhere statistically significant excesses in the observed count rate relative tothe background estimate exist. We test the new software on archival data fromthe NASA Fermi Gamma-ray Burst Monitor (GBM), which has a collecting area andbackground level of the same order of magnitude to those of HERMES Pathfinder.The NN performances are discussed and analyzed over period of both high and lowsolar activity. We were able to confirm events in the Fermi/GBM catalog andfound events, not present in Fermi/GBM database, that could be attributed toSolar Flares, Terrestrial Gamma-ray Flashes, Gamma-Ray Bursts, Galactic X-rayflash. Seven of these are selected and analyzed further, providing an estimateof localisation and a tentative classification.", "output": "Searching for long faint astronomical high energy transients: a data driven approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Digital image correlation (DIC) has become a valuable tool in the evaluationof mechanical experiments, particularly fatigue crack growth experiments. Theevaluation requires accurate information of the crack path and crack tipposition, which is difficult to obtain due to inherent noise and artefacts.Machine learning models have been extremely successful in recognizing thisrelevant information given labelled DIC displacement data. For the training ofrobust models, which generalize well, big data is needed. However, data istypically scarce in the field of material science and engineering becauseexperiments are expensive and time-consuming. We present a method to generatesynthetic DIC displacement data using generative adversarial networks with aphysics-guided discriminator. To decide whether data samples are real or fake,this discriminator additionally receives the derived von Mises equivalentstrain. We show that this physics-guided approach leads to improved results interms of visual quality of samples, sliced Wasserstein distance, and geometryscore.", "output": "Physics-guided adversarial networks for artificial digital image correlation data generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have shown that pseudo labels can contribute to unsuperviseddomain adaptation (UDA) for speaker verification. Inspired by the self-trainingstrategies that use an existing classifier to label the unlabeled data forretraining, we propose a cluster-guided UDA framework that labels the targetdomain data by clustering and combines the labeled source domain data andpseudo-labeled target domain data to train a speaker embedding network. Toimprove the cluster quality, we train a speaker embedding network dedicated forclustering by minimizing the contrastive center loss. The goal is to reduce thedistance between an embedding and its assigned cluster center while enlargingthe distance between the embedding and the other cluster centers. UsingVoxCeleb2 as the source domain and CN-Celeb1 as the target domain, wedemonstrate that the proposed method can achieve an equal error rate (EER) of8.10% on the CN-Celeb1 evaluation set without using any labels from the targetdomain. This result outperforms the supervised baseline by 39.6% and is thestate-of-the-art UDA performance on this corpus.", "output": "Cluster-Guided Unsupervised Domain Adaptation for Deep Speaker Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Convolutional Networks (GCN) have been recently employed as corecomponent in the construction of recommender system algorithms, interpretinguser-item interactions as the edges of a bipartite graph. However, in theabsence of side information, the majority of existing models adopt an approachof randomly initialising the user embeddings and optimising them throughout thetraining process. This strategy makes these algorithms inherently transductive,curtailing their ability to generate predictions for users that were unseen attraining time. To address this issue, we propose a convolution-based algorithm,which is inductive from the user perspective, while at the same time, dependingonly on implicit user-item interaction data. We propose the construction of anitem-item graph through a weighted projection of the bipartite interactionnetwork and to employ convolution to inject higher order associations into itemembeddings, while constructing user representations as weighted sums of theitems with which they have interacted. Despite not training individualembeddings for each user our approach achieves state of-the-art recommendationperformance with respect to transductive baselines on four real-world datasets,showing at the same time robust inductive performance.", "output": "Item Graph Convolution Collaborative Filtering for Inductive Recommendations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recording surgery in operating rooms is an essential task for education andevaluation of medical treatment. However, recording the desired targets, suchas the surgery field, surgical tools, or doctor's hands, is difficult becausethe targets are heavily occluded during surgery. We use a recording system inwhich multiple cameras are embedded in the surgical lamp, and we assume that atleast one camera is recording the target without occlusion at any given time.As the embedded cameras obtain multiple video sequences, we address the task ofselecting the camera with the best view of the surgery. Unlike the conventionalmethod, which selects the camera based on the area size of the surgery field,we propose a deep neural network that predicts the camera selection probabilityfrom multiple video sequences by learning the supervision of the expertannotation. We created a dataset in which six different types of plasticsurgery are recorded, and we provided the annotation of camera switching. Ourexperiments show that our approach successfully switched between cameras andoutperformed three baseline methods.", "output": "Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We revisit the Gaussian process model with spherical harmonic features andstudy connections between the associated RKHS, its eigenstructure and deepmodels. Based on this, we introduce a new class of kernels which correspond todeep models of continuous depth. In our formulation, depth can be estimated asa kernel hyper-parameter by optimizing the evidence lower bound. Further, weintroduce sparseness in the eigenbasis by variational learning of the sphericalharmonic phases. This enables scaling to larger input dimensions thanpreviously, while also allowing for learning of high frequency variations. Wevalidate our approach on machine learning benchmark datasets.", "output": "Sparse Gaussian Processes with Spherical Harmonic Features Revisited."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Multi-Prize Lottery Ticket Hypothesis posits that randomly initializedneural networks contain several subnetworks that achieve comparable accuracy tofully trained models of the same architecture. However, current methods requirethat the network is sufficiently overparameterized. In this work, we propose amodification to two state-of-the-art algorithms (Edge-Popup and Biprop) thatfinds high-accuracy subnetworks with no additional storage cost or scaling. Thealgorithm, Iterative Weight Recycling, identifies subsets of important weightswithin a randomly initialized network for intra-layer reuse. Empirically weshow improvements on smaller network architectures and higher prune rates,finding that model sparsity can be increased through the \"recycling\" ofexisting weights. In addition to Iterative Weight Recycling, we complement theMulti-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy,randomly initialized subnetwork's produce diverse masks, despite beinggenerated with the same hyperparameter's and pruning strategy. We explore thelandscapes of these masks, which show high variability.", "output": "Randomly Initialized Subnetworks with Iterative Weight Recycling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Road network digital twins (RNDTs) play a critical role in the development ofnext-generation intelligent transportation systems, enabling more precisetraffic planning and control. To support just-in-time (JIT) decision making,RNDTs require a model that dynamically learns the traffic patterns from onlinesensor data and generates high-fidelity simulation results. Although currenttraffic prediction techniques based on graph neural networks have achievedstate-of-the-art performance, these techniques only predict future traffic bymining correlations in historical traffic data, disregarding the causes oftraffic generation, such as traffic demands and route selection. Therefore,their performance is unreliable for JIT decision making. To fill this gap, weintroduce a novel deep learning framework called TraffNet that learns thecausality of traffic volume from vehicle trajectory data. First, we use aheterogeneous graph to represent the road network, allowing the model toincorporate causal features of traffic volumes. Next, motivated by the trafficdomain knowledge, we propose a traffic causality learning method to learn anembedding vector that encodes travel demands and path-level dependencies foreach road segment. Then, we model temporal dependencies to match the underlyingprocess of traffic generation. Finally, the experiments verify the utility ofTraffNet. The code of TraffNet is available at", "output": "TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, it has become progressively more evident that classic diagnosticlabels are unable to reliably describe the complexity and variability ofseveral clinical phenotypes. This is particularly true for a broad range ofneuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioralphenotypes). Patient heterogeneity can be better described by groupingindividuals into novel categories based on empirically derived sections ofintersecting continua that span across and beyond traditional categoricalborders. In this context, neuroimaging data carry a wealth of spatiotemporallyresolved information about each patient's brain. However, they are usuallyheavily collapsed a priori through procedures which are not learned as part ofmodel training, and consequently not optimized for the downstream predictiontask. This is because every individual participant usually comes with multiplewhole-brain 3D imaging modalities often accompanied by a deep genotypic andphenotypic characterization, hence posing formidable computational challenges.In this paper we design a deep learning architecture based on generative modelsrooted in a modular approach and separable convolutional blocks to a) fusemultiple 3D neuroimaging modalities on a voxel-wise level, b) convert them intoinformative latent embeddings through heavy dimensionality reduction, c)maintain good generalizability and minimal information loss. As proof ofconcept, we test our architecture on the well characterized Human ConnectomeProject database demonstrating that our latent embeddings can be clustered intoeasily separable subject strata which, in turn, map to different phenotypicalinformation which was not included in the embedding creation process. This maybe of aid in predicting disease evolution as well as drug response, hencesupporting mechanistic disease understanding and empowering clinical trials.", "output": "Multimodal and multicontrast image fusion via deep generative models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To represent the biological variability of clinical neuroimaging populations,it is vital to be able to combine data across scanners and studies. However,different MRI scanners produce images with different characteristics, resultingin a domain shift known as the `harmonisation problem'. Additionally,neuroimaging data is inherently personal in nature, leading to data privacyconcerns when sharing the data. To overcome these barriers, we propose anUnsupervised Source-Free Domain Adaptation (SFDA) method, SFHarmony. Throughmodelling the imaging features as a Gaussian Mixture Model and minimising anadapted Bhattacharyya distance between the source and target features, we cancreate a model that performs well for the target data whilst having a sharedfeature representation across the data domains, without needing access to thesource data for adaptation or target labels. We demonstrate the performance ofour method on simulated and real domain shifts, showing that the approach isapplicable to classification, segmentation and regression tasks, requiring nochanges to the algorithm. Our method outperforms existing SFDA approachesacross a range of realistic data scenarios, demonstrating the potential utilityof our approach for MRI harmonisation and general SFDA problems. Our code isavailable at url{", "output": "SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dropout is a widely used regularization trick to resolve the overfittingissue in large feedforward neural networks trained on a small dataset, whichperforms poorly on the held-out test subset. Although the effectiveness of thisregularization trick has been extensively studied for convolutional neuralnetworks, there is a lack of analysis of it for unsupervised models and inparticular, VAE-based neural topic models. In this paper, we have analyzed theconsequences of dropout in the encoder as well as in the decoder of the VAEarchitecture in three widely used neural topic models, namely, contextualizedtopic model (CTM), ProdLDA, and embedded topic model (ETM) using four publiclyavailable datasets. We characterize the dropout effect on these models in termsof the quality and predictive performance of the generated topics.", "output": "Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Discovering novel concepts from unlabelled data and in a continuous manner isan important desideratum of lifelong learners. In the literature such problemshave been partially addressed under very restricted settings, where eitheraccess to labelled data is provided for discovering novel concepts (e.g., NCD)or learning occurs for a limited number of incremental steps (e.g.,class-iNCD). In this work we challenge the status quo and propose a morechallenging and practical learning paradigm called MSc-iNCD, where learningoccurs continuously and unsupervisedly, while exploiting the rich priors fromlarge-scale pre-trained models. To this end, we propose simple baselines thatare not only resilient under longer learning scenarios, but are surprisinglystrong when compared with sophisticated state-of-the-art methods. We conductextensive empirical evaluation on a multitude of benchmarks and show theeffectiveness of our proposed baselines, which significantly raises the bar.", "output": "Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The increasingly deeper neural networks hinder the democratization ofprivacy-enhancing distributed learning, such as federated learning (FL), toresource-constrained devices. To overcome this challenge, in this paper, weadvocate the integration of edge computing paradigm and parallel split learning(PSL), allowing multiple client devices to offload substantial trainingworkloads to an edge server via layer-wise model split. By observing thatexisting PSL schemes incur excessive training latency and large volume of datatransmissions, we propose an innovative PSL framework, namely, efficientparallel split learning (EPSL), to accelerate model training. To be specific,EPSL parallelizes client-side model training and reduces the dimension of localgradients for back propagation (BP) via last-layer gradient aggregation,leading to a significant reduction in server-side training and communicationlatency. Moreover, by considering the heterogeneous channel conditions andcomputing capabilities at client devices, we jointly optimize subchannelallocation, power control, and cut layer selection to minimize the per-roundlatency. Simulation results show that the proposed EPSL framework significantlydecreases the training latency needed to achieve a target accuracy comparedwith the state-of-the-art benchmarks, and the tailored resource management andlayer split strategy can considerably reduce latency than the counterpartwithout optimization.", "output": "Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work the authors develop regression approaches based on deep learningto perform thread density estimation for plain weave canvas analysis. Previousapproaches were based on Fourier analysis, that are quite robust for somescenarios but fail in some other, in machine learning tools, that involvepre-labeling of the painting at hand, or the segmentation of thread crossingpoints, that provides good estimations in all scenarios with no need ofpre-labeling. The segmentation approach is time-consuming as estimation of thedensities is performed after locating the crossing points. In this novelproposal, we avoid this step by computing the density of threads directly fromthe image with a regression deep learning model. We also incorporate someimprovements in the initial preprocessing of the input image with an impact onthe final error. Several models are proposed and analyzed to retain the bestone. Furthermore, we further reduce the density estimation error by introducinga semi-supervised approach. The performance of our novel algorithm is analyzedwith works by Ribera, Vel'azquez, and Poussin where we compare our results tothe ones of previous approaches. Finally, the method is put into practice tosupport the change of authorship or a masterpiece at the Museo del Prado.", "output": "Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a setof registered images. Increasing sizes of a scene demands more complexfunctions, typically represented by neural networks, to capture all details.Training and inference then involves querying the neural network millions oftimes per image, which becomes impractically slow. Since such complex functionscan be replaced by multiple simpler functions to improve speed, we show that ahierarchy of Voronoi diagrams is a suitable choice to partition the scene. Byequipping each Voronoi cell with its own NeRF, our approach is able to quicklylearn a scene representation. We propose an intuitive partitioning of the spacethat increases quality gains during training by distributing information evenlyamong the networks and avoids artifacts through a top-down adaptive refinement.Our framework is agnostic to the underlying NeRF method and easy to implement,which allows it to be applied to various NeRF variants for improved learningand rendering speeds.", "output": "Adaptive Voronoi NeRFs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Malware detection has become a major concern due to the increasing number andcomplexity of malware. Traditional detection methods based on signatures andheuristics are used for malware detection, but unfortunately, they suffer frompoor generalization to unknown attacks and can be easily circumvented usingobfuscation techniques. In recent years, Machine Learning (ML) and notably DeepLearning (DL) achieved impressive results in malware detection by learninguseful representations from data and have become a solution preferred overtraditional methods. More recently, the application of such techniques ongraph-structured data has achieved state-of-the-art performance in variousdomains and demonstrates promising results in learning more robustrepresentations from malware. Yet, no literature review focusing on graph-baseddeep learning for malware detection exists. In this survey, we provide anin-depth literature review to summarize and unify existing works under thecommon approaches and architectures. We notably demonstrate that Graph NeuralNetworks (GNNs) reach competitive results in learning robust embeddings frommalware represented as expressive graph structures, leading to an efficientdetection by downstream classifiers. This paper also reviews adversarialattacks that are utilized to fool graph-based detection methods. Challenges andfuture research directions are discussed at the end of the paper.", "output": "A Survey on Malware Detection with Graph Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In real applications, interaction between machine learning model and domainexperts is critical; however, the classical machine learning paradigm thatusually produces only a single model does not facilitate such interaction.Approximating and exploring the Rashomon set, i.e., the set of all near-optimalmodels, addresses this practical challenge by providing the user with asearchable space containing a diverse set of models from which domain expertscan choose. We present a technique to efficiently and accurately approximatethe Rashomon set of sparse, generalized additive models (GAMs). We presentalgorithms to approximate the Rashomon set of GAMs with ellipsoids for fixedsupport sets and use these ellipsoids to approximate Rashomon sets for manydifferent support sets. The approximated Rashomon set serves as a cornerstoneto solve practical challenges such as (1) studying the variable importance forthe model class; (2) finding models under user-specified constraints(monotonicity, direct editing); (3) investigating sudden changes in the shapefunctions. Experiments demonstrate the fidelity of the approximated Rashomonset and its effectiveness in solving practical challenges.", "output": "Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an information-theoretic knowledge distillation approach for thecompression of generative adversarial networks, which aims to maximize themutual information between teacher and student networks via a variationaloptimization based on an energy-based model. Because the direct computation ofthe mutual information in continuous domains is intractable, our approachalternatively optimizes the student network by maximizing the variational lowerbound of the mutual information. To achieve a tight lower bound, we introducean energy-based model relying on a deep neural network to represent a flexiblevariational distribution that deals with high-dimensional images and considerspatial dependencies between pixels, effectively. Since the proposed method isa generic optimization algorithm, it can be conveniently incorporated intoarbitrary generative adversarial networks and even dense prediction networks,e.g., image enhancement models. We demonstrate that the proposed algorithmachieves outstanding performance in model compression of generative adversarialnetworks consistently when combined with several existing models.", "output": "Information-Theoretic GAN Compression with Variational Energy-based Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the challenges in federated learning is the non-independent andidentically distributed (non-iid) characteristics between heterogeneousdevices, which cause significant differences in local updates and affect theperformance of the central server. Although many studies have been proposed toaddress this challenge, they only focus on local training and aggregationprocesses to smooth the changes and fail to achieve high performance with deeplearning models. Inspired by the phenomenon of neural collapse, we force eachclient to be optimized toward an optimal global structure for classification.Specifically, we initialize it as a random simplex Equiangular Tight Frame(ETF) and fix it as the unit optimization target of all clients during thelocal updating. After guaranteeing all clients are learning to converge to theglobal optimum, we propose to add a global memory vector for each category toremedy the parameter fluctuation caused by the bias of the intra-classcondition distribution among clients. Our experimental results show that ourmethod can improve the performance with faster convergence speed ondifferent-size datasets.", "output": "Neural Collapse Inspired Federated Learning with Non-iid Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When training neural networks for classification tasks with backpropagation,parameters are updated on every trial, even if the sample is classifiedcorrectly. In contrast, humans concentrate their learning effort on errors.Inspired by human learning, we introduce lazy learning, which only learns onincorrect samples. Lazy learning can be implemented in a few lines of code andrequires no hyperparameter tuning. Lazy learning achieves state-of-the-artperformance and is particularly suited when datasets are large. For instance,it reaches 99.2% test accuracy on Extended MNIST using a single-layer MLP, anddoes so 7.6x faster than a matched backprop network", "output": "Lazy learning: a biologically-inspired plasticity rule for fast and energy efficient synaptic plasticity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Low-Earth orbit (LEO) satellites have been prosperously deployed for variousEarth observation missions due to its capability of collecting a large amountof image or sensor data. However, traditionally, the data training process isperformed in the terrestrial cloud server, which leads to a high transmissionoverhead. With the recent development of LEO, it is more imperative to provideultra-dense LEO constellation with enhanced on-board computation capability.Benefited from it, we have proposed a collaborative federated learning over LEOsatellite constellation (FedLEO). We allocate the entire process on LEOs withlow payload inter-satellite transmissions, whilst the low-delay terrestrialgateway server (GS) only takes care for initial signal controlling. The GSinitially selects an LEO server, whereas its LEO clients are all determined byclustering mechanism and communication capability through the opticalinter-satellite links (ISLs). The re-clustering of changing LEO server will beexecuted once with low communication quality of FedLEO. In the simulations, wehave numerically analyzed the proposed FedLEO under practical Walker-based LEOconstellation configurations along with MNIST training dataset forclassification mission. The proposed FedLEO outperforms the conventionalcentralized and distributed architectures with higher classification accuracyas well as comparably lower latency of joint communication and computing.", "output": "Edge Selection and Clustering for Federated Learning in Optical Inter-LEO Satellite Constellation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Overcoming the time scale limitations of atomistics can be achieved byswitching from the state-space representation of Molecular Dynamics (MD) to astatistical-mechanics-based representation in phase space, where approximationssuch as maximum-entropy or Gaussian phase packets (GPP) evolve the atomisticensemble in a time-coarsened fashion. In practice, this requires thecomputation of expensive high-dimensional integrals over all of phase space ofan atomistic ensemble. This, in turn, is commonly accomplished efficiently bylow-order numerical quadrature. We show that numerical quadrature in thiscontext, unfortunately, comes with a set of inherent problems, which corruptthe accuracy of simulations -- especially when dealing with crystal latticeswith imperfections. As a remedy, we demonstrate that Graph Neural Networks,trained on Monte-Carlo data, can serve as a replacement for commonly usednumerical quadrature rules, overcoming their deficiencies and significantlyimproving the accuracy. This is showcased by three benchmarks: the thermalexpansion of copper, the martensitic phase transition of iron, and the energyof grain boundaries. We illustrate the benefits of the proposed technique overclassically used third- and fifth-order Gaussian quadrature, we highlight theimpact on time-coarsened atomistic predictions, and we discuss thecomputational efficiency. The latter is of general importance when performingfrequent evaluation of phase space or other high-dimensional integrals, whichis why the proposed framework promises applications beyond the scope ofatomistics.", "output": "GNN-Assisted Phase Space Integration with Application to Atomistics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of a learning algorithm is to receive a training data set as inputand provide a hypothesis that can generalize to all possible data points from adomain set. The hypothesis is chosen from hypothesis classes with potentiallydifferent complexities. Linear regression modeling is an important category oflearning algorithms. The practical uncertainty of the target samples affectsthe generalization performance of the learned model. Failing to choose a propermodel or hypothesis class can lead to serious issues such as underfitting oroverfitting. These issues have been addressed by alternating cost functions orby utilizing cross-validation methods. These approaches can introduce newhyperparameters with their own new challenges and uncertainties or increase thecomputational complexity of the learning algorithm. On the other hand, thetheory of probably approximately correct (PAC) aims at defining learnabilitybased on probabilistic settings. Despite its theoretical value, PAC does notaddress practical learning issues on many occasions. This work is inspired bythe foundation of PAC and is motivated by the existing regression learningissues. The proposed approach, denoted by epsilon-Confidence ApproximatelyCorrect (epsilon CoAC), utilizes Kullback Leibler divergence (relative entropy)and proposes a new related typical set in the set of hyperparameters to tacklethe learnability issue. Moreover, it enables the learner to compare hypothesisclasses of different complexity orders and choose among them the optimum withthe minimum epsilon in the epsilon CoAC framework. Not only the epsilon CoAClearnability overcomes the issues of overfitting and underfitting, but it alsoshows advantages and superiority over the well known cross-validation method inthe sense of time consumption as well as in the sense of accuracy.", "output": "Learnability, Sample Complexity, and Hypothesis Class Complexity for Regression Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Leveraging data collected from smart meters in buildings can aid indeveloping policies towards energy conservation. Significant energy savingscould be realised if deviations in the building operating conditions aredetected early, and appropriate measures are taken. Towards this end, machinelearning techniques can be used to automate the discovery of these abnormalpatterns in the collected data. Current methods in anomaly detection rely on anunderlying model to capture the usual or acceptable operating behaviour. Inthis paper, we propose a novel attention mechanism to model the consumptionbehaviour of a building and demonstrate the effectiveness of the model incapturing the relations using sample case studies. A real-world dataset ismodelled using the proposed architecture, and the results are presented. Avisualisation approach towards understanding the relations captured by themodel is also presented.", "output": "Attention Boosted Autoencoder for Building Energy Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Executing machine learning inference tasks on resource-constrained edgedevices requires careful hardware-software co-design optimizations. Recentexamples have shown how transformer-based deep neural network models such asALBERT can be used to enable the execution of natural language processing (NLP)inference on mobile systems-on-chip housing custom hardware accelerators.However, while these existing solutions are effective in alleviating thelatency, energy, and area costs of running single NLP tasks, achievingmulti-task inference requires running computations over multiple variants ofthe model parameters, which are tailored to each of the targeted tasks. Thisapproach leads to either prohibitive on-chip memory requirements or paying thecost of off-chip memory access. This paper proposes adapter-ALBERT, anefficient model optimization for maximal data reuse across different tasks. Theproposed model's performance and robustness to data compression methods areevaluated across several language tasks from the GLUE benchmark. Additionally,we demonstrate the advantage of mapping the model to a heterogeneous on-chipmemory architecture by performing simulations on a validated NLP edgeaccelerator to extrapolate performance, power, and area improvements over theexecution of a traditional ALBERT model on the same hardware platform.", "output": "Energy-efficient Task Adaptation for NLP Edge Inference Leveraging Heterogeneous Memory Architectures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a text-to-image generation algorithm based on deep neural networkswhen text captions for images are unavailable during training. In this work,instead of simply generating pseudo-ground-truth sentences of training imagesusing existing image captioning methods, we employ a pretrained CLIP model,which is capable of properly aligning embeddings of images and correspondingtexts in a joint space and, consequently, works well on zero-shot recognitiontasks. We optimize a text-to-image generation model by maximizing the datalog-likelihood conditioned on pairs of image-text CLIP embeddings. To betteralign data in the two domains, we employ a principled way based on avariational inference, which efficiently estimates an approximate posterior ofthe hidden text embedding given an image and its CLIP feature. Experimentalresults validate that the proposed framework outperforms existing approaches bylarge margins under unsupervised and semi-supervised text-to-image generationsettings.", "output": "Variational Distribution Learning for Unsupervised Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In deep learning inference, model parameters are pruned and quantized toreduce the model size. Compression methods and common subexpression (CSE)elimination algorithms are applied on sparse constant matrices to deploy themodels on low-cost embedded devices. However, the state-of-the-art CSEelimination methods do not scale well for handling large matrices. They reachhours for extracting CSEs in a $200 times 200$ matrix while their matrixmultiplication algorithms execute longer than the conventional matrixmultiplication methods. Besides, there exist no compression methods formatrices utilizing CSEs. As a remedy to this problem, a random search-basedalgorithm is proposed in this paper to extract CSEs in the column pairs of aconstant matrix. It produces an adder tree for a $1000 times 1000$ matrix in aminute. To compress the adder tree, this paper presents a compression format byextending the Compressed Sparse Row (CSR) to include CSEs. While compressionrates of more than $50%$ can be achieved compared to the original CSR format,simulations for a single-core embedded system show that the matrixmultiplication execution time can be reduced by $20%$.", "output": "Common Subexpression-based Compression and Multiplication of Sparse Constant Matrices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting the behaviour (i.e. manoeuvre/trajectory) of other road users,including vehicles, is critical for the safe and efficient operation ofautonomous vehicles (AVs), a.k.a. automated driving systems (ADSs). Due to theuncertain future behaviour of vehicles, multiple future behaviour modes areoften plausible for a vehicle in a given driving scene. Therefore, multimodalprediction can provide richer information than single-mode prediction enablingAVs to perform a better risk assessment. To this end, we propose a novelmultimodal prediction framework that can predict multiple plausible behaviourmodes and their likelihoods. The proposed framework includes a bespoke problemformulation for manoeuvre prediction, a novel transformer-based predictionmodel, and a tailored training method for multimodal manoeuvre and trajectoryprediction. The performance of the framework is evaluated using two publicbenchmark highway driving datasets, namely NGSIM and highD. The results showthat the proposed framework outperforms the state-of-the-art multimodal methodsin the literature in terms of prediction error and is capable of predictingplausible manoeuvre and trajectory modes.", "output": "Multimodal Manoeuvre and Trajectory Prediction for Autonomous Vehicles Using Transformer Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learned partial differential equation (PDE) solvers trade thereliability of standard numerical methods for potential gains in accuracyand/or speed. The only way for a solver to guarantee that it outputs the exactsolution is to use a convergent method in the limit that the grid spacing$Delta x$ and timestep $Delta t$ approach zero. Machine learned solvers,which learn to update the solution at large $Delta x$ and/or $Delta t$, cannever guarantee perfect accuracy. Some amount of error is inevitable, so thequestion becomes: how do we constrain machine learned solvers to give us thesorts of errors that we are willing to tolerate? In this paper, we design morereliable machine learned PDE solvers by preserving discrete analogues of thecontinuous invariants of the underlying PDE. Examples of such invariantsinclude conservation of mass, conservation of energy, the second law ofthermodynamics, and/or non-negative density. Our key insight is simple: topreserve invariants, at each timestep apply an error-correcting algorithm tothe update rule. Though this strategy is different from how standard solverspreserve invariants, it is necessary to retain the flexibility that allowsmachine learned solvers to be accurate at large $Delta x$ and/or $Delta t$.This strategy can be applied to any autoregressive solver for anytime-dependent PDE in arbitrary geometries with arbitrary boundary conditions.Although this strategy is very general, the specific error-correctingalgorithms need to be tailored to the invariants of the underlying equations aswell as to the solution representation and time-stepping scheme of the solver.The error-correcting algorithms we introduce have two key properties. First, bypreserving the right invariants they guarantee numerical stability. Second, inclosed or periodic systems they do so without degrading the accuracy of analready-accurate solver.", "output": "Invariant preservation in machine learned PDE solvers via error correction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to mutual interference between users, power allocation problems inwireless networks are often non-convex and computationally challenging. Graphneural networks (GNNs) have recently emerged as a promising approach totackling these problems and an approach that exploits the underlying topologyof wireless networks. In this paper, we propose a novel graph representationmethod for wireless networks that include full-duplex (FD) nodes. We thendesign a corresponding FD Graph Neural Network (F-GNN) with the aim ofallocating transmit powers to maximise the network throughput. Our results showthat our F-GNN achieves state-of-art performance with significantly lesscomputation time. Besides, F-GNN offers an excellent trade-off betweenperformance and complexity compared to classical approaches. We further refinethis trade-off by introducing a distance-based threshold for inclusion orexclusion of edges in the network. We show that an appropriately chosenthreshold reduces required training time by roughly 20% with a relatively minorloss in performance.", "output": "Graph Neural Networks for Power Allocation in Wireless Networks with Full Duplex Nodes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We apply different feature engineering methods for time-series to US marketprice data. The predictive power of models are tested against Numerai-Signalstargets.", "output": "Feature Engineering Methods on Multivariate Time-Series Data for Financial Data Science Competitions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph or network has been widely used for describing and modeling complexsystems in biomedicine. Deep learning methods, especially graph neural networks(GNNs), have been developed to learn and predict with such structured data. Inthis paper, we proposed a novel transformer and snowball encoding networks(TSEN) for biomedical graph classification, which introduced transformerarchitecture with graph snowball connection into GNNs for learning whole-graphrepresentation. TSEN combined graph snowball connection with graph transformerby snowball encoding layers, which enhanced the power to capture multi-scaleinformation and global patterns to learn the whole-graph features. On the otherhand, TSEN also used snowball graph convolution as position embedding intransformer structure, which was a simple yet effective method for capturinglocal patterns naturally. Results of experiments using four graphclassification datasets demonstrated that TSEN outperformed thestate-of-the-art typical GNN models and the graph-transformer based GNN models.", "output": "Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As general purpose vision models get increasingly effective at a wide set oftasks, it is imperative that they be consistent across the tasks they support.Inconsistent AI models are considered brittle and untrustworthy by human usersand are more challenging to incorporate into larger systems that takedependencies on their outputs. Measuring consistency between very heterogeneoustasks that might include outputs in different modalities is challenging sinceit is difficult to determine if the predictions are consistent with oneanother. As a solution, we introduce a benchmark dataset, COCOCON, where we usecontrast sets created by modifying test instances for multiple tasks in smallbut semantically meaningful ways to change the gold label, and outline metricsfor measuring if a model is consistent by ranking the original and perturbedinstances across tasks. We find that state-of-the-art systems suffer from asurprisingly high degree of inconsistent behavior across tasks, especially formore heterogeneous tasks. Finally, we propose using a rank correlation-basedauxiliary objective computed over large automatically created cross-taskcontrast sets to improve the multi-task consistency of large unified models,while retaining their original accuracy on downstream tasks. Project websiteavailable at ", "output": "Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modeling parameters are essential to the fidelity of nonlinear models ofconcrete structures subjected to earthquake ground motions, especially whensimulating seismic events strong enough to cause collapse. This paper addressestwo of the most significant barriers to improving nonlinear modeling provisionsin seismic evaluation standards using experimental data sets: identifying themost likely mode of failure of structural components, and implementing datafitting techniques capable of recognizing interdependencies between inputparameters and nonlinear relationships between input parameters and modeloutputs. Machine learning tools in the Scikit-learn and Pytorch libraries wereused to calibrate equations and black-box numerical models for nonlinearmodeling parameters (MP) a and b of reinforced concrete columns defined in theASCE 41 and ACI 369.1 standards, and to estimate their most likely mode offailure. It was found that machine learning regression models and machinelearning black-boxes were more accurate than current provisions in the ACI369.1/ASCE 41 Standards. Among the regression models, Regularized LinearRegression was the most accurate for estimating MP a, and Polynomial Regressionwas the most accurate for estimating MP b. The two black-box models evaluated,namely the Gaussian Process Regression and the Neural Network (NN), providedthe most accurate estimates of MPs a and b. The NN model was the most accuratemachine learning tool of all evaluated. A multi-class classification tool fromthe Scikit-learn machine learning library correctly identified column mode offailure with 79% accuracy for rectangular columns and with 81% accuracy forcircular columns, a substantial improvement over the classification rules inASCE 41-13.", "output": "Machine learning tools to improve nonlinear modeling parameters of RC columns."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning is effective in diagnosing COVID-19 and requires a large amountof data to be effectively trained. Due to data and privacy regulations,hospitals generally have no access to data from other hospitals. Federatedlearning (FL) has been used to solve this problem, where it utilizes adistributed setting to train models in hospitals in a privacy-preservingmanner. Deploying FL is not always feasible as it requires high computation andnetwork communication resources. This paper evaluates five FL algorithms'performance and resource efficiency for Covid-19 detection. A decentralizedsetting with CNN networks is set up, and the performance of FL algorithms iscompared with a centralized environment. We examined the algorithms withvarying numbers of participants, federated rounds, and selection algorithms.Our results show that cyclic weight transfer can have better overallperformance, and results are better with fewer participating hospitals. Ourresults demonstrate good performance for detecting COVID-19 patients and mightbe useful in deploying FL algorithms for covid-19 detection and medical imageanalysis in general.", "output": "A Comparative Study of Federated Learning Models for COVID-19 Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The growth of market capitalisation and the number of altcoins(cryptocurrencies other than Bitcoin) provide investment opportunities andcomplicate the prediction of their price movements. A significant challenge inthis volatile and relatively immature market is the problem of predictingcryptocurrency prices which needs to identify the factors influencing theseprices. The focus of this study is to investigate the factors influencingaltcoin prices, and these factors have been investigated from a causal analysisperspective using Bayesian networks. In particular, studying the nature ofinteractions between five leading altcoins, traditional financial assetsincluding gold, oil, and S&amp;P 500, and social media is the research question.To provide an answer to the question, we create causal networks which are builtfrom the historic price data of five traditional financial assets, social mediadata, and price data of altcoins. The ensuing networks are used for causalreasoning and diagnosis, and the results indicate that social media (inparticular Twitter data in this study) is the most significant influencingfactor of the prices of altcoins. Furthermore, it is not possible to generalisethe coins' reactions against the changes in the factors. Consequently, thecoins need to be studied separately for a particular price movementinvestigation.", "output": "Modelling Determinants of Cryptocurrency Prices: A Bayesian Network Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The complexity and ambiguity of financial and economic systems, along withfrequent changes in the economic environment, have made it difficult to makeprecise predictions that are supported by theory-consistent explanations.Interpreting the prediction models used for forecasting important macroeconomicindicators is highly valuable for understanding relations among differentfactors, increasing trust towards the prediction models, and making predictionsmore actionable. In this study, we develop a fundamental-based model for theCanadian-U.S. dollar exchange rate within an interpretative framework. Wepropose a comprehensive approach using machine learning to predict the exchangerate and employ interpretability methods to accurately analyze therelationships among macroeconomic variables. Moreover, we implement an ablationstudy based on the output of the interpretations to improve the predictiveaccuracy of the models. Our empirical results show that crude oil, as Canada'smain commodity export, is the leading factor that determines the exchange ratedynamics with time-varying effects. The changes in the sign and magnitude ofthe contributions of crude oil to the exchange rate are consistent withsignificant events in the commodity and energy markets and the evolution of thecrude oil trend in Canada. Gold and the TSX stock index are found to be thesecond and third most important variables that influence the exchange rate.Accordingly, this analysis provides trustworthy and practical insights forpolicymakers and economists and accurate knowledge about the predictive model'sdecisions, which are supported by theoretical considerations.", "output": "Explaining Exchange Rate Forecasts with Macroeconomic Fundamentals Using Interpretive Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human activity recognition and clinical biomechanics are challenging problemsin physical telerehabilitation medicine. However, most publicly availabledatasets on human body movements cannot be used to study both problems in anout-of-the-lab movement acquisition setting. The objective of the VIDIMUdataset is to pave the way towards affordable patient tracking solutions forremote daily life activities recognition and kinematic analysis. The datasetincludes 13 activities registered using a commodity camera and five inertialsensors. The video recordings were acquired in 54 subjects, of which 16 alsohad simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in:i) the clinical relevance of the chosen movements, ii) the combined utilizationof affordable video and custom sensors, and iii) the implementation ofstate-of-the-art tools for multimodal data processing of 3D body pose trackingand motion reconstruction in a musculoskeletal model from inertial data. Thevalidation confirms that a minimally disturbing acquisition protocol, performedaccording to real-life conditions can provide a comprehensive picture of humanjoint angles during daily life activities.", "output": "VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a model to forecast large realized covariance matrices of returns,applying it to the constituents of the S&amp;P 500 daily. To address the curse ofdimensionality, we decompose the return covariance matrix using standardfirm-level factors (e.g., size, value, and profitability) and use sectoralrestrictions in the residual covariance matrix. This restricted model is thenestimated using vector heterogeneous autoregressive (VHAR) models with theleast absolute shrinkage and selection operator (LASSO). Our methodologyimproves forecasting precision relative to standard benchmarks and leads tobetter estimates of minimum variance portfolios.", "output": "Forecasting Large Realized Covariance Matrices: The Benefits of Factor Models and Shrinkage."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning requires exuberant amounts of data and computation. Also,models require equally excessive growth in the number of parameters. It is,therefore, sensible to look for technologies that reduce these demands onresources. Here, we propose an approach called guided transfer learning. Eachweight and bias in the network has its own guiding parameter that indicates howmuch this parameter is allowed to change while learning a new task. Guidingparameters are learned during an initial scouting process. Guided transferlearning can result in a reduction in resources needed to train a network. Insome applications, guided transfer learning enables the network to learn from asmall amount of data. In other cases, a network with a smaller number ofparameters can learn a task which otherwise only a larger network could learn.Guided transfer learning potentially has many applications when the amount ofdata, model size, or the availability of computational resources reach theirlimits.", "output": "Guided Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is considerable evidence that machine learning algorithms have betterpredictive abilities than humans in various financial settings. But, theliterature has not tested whether these algorithmic predictions are morerational than human predictions. We study the predictions of corporate earningsfrom several algorithms, notably linear regressions and a popular algorithmcalled Gradient Boosted Regression Trees (GBRT). On average, GBRT outperformedboth linear regressions and human stock analysts, but it still overreacted tonews and did not satisfy rational expectation as normally defined. By reducingthe learning rate, the magnitude of overreaction can be minimized, but it comeswith the cost of poorer out-of-sample prediction accuracy. Human stock analystswho have been trained in machine learning methods overreact less thantraditionally trained analysts. Additionally, stock analyst predictions reflectinformation not otherwise available to machine algorithms.", "output": "Behavioral Machine Learning? Computer Predictions of Corporate Earnings also Overreact."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this article, we consider the manifold learning problem when the data setis invariant under the action of a compact Lie group $K$. Our approach consistsin augmenting the data-induced graph Laplacian by integrating over orbits underthe action of $K$ of the existing data points. We prove that this $K$-invariantLaplacian operator $L$ can be diagonalized by using the unitary irreduciblerepresentation matrices of $K$, and we provide an explicit formula forcomputing the eigenvalues and eigenvectors of $L$. Moreover, we show that thenormalized Laplacian operator $L_N$ converges to the Laplace-Beltrami operatorof the data manifold with an improved convergence rate, where the improvementgrows with the dimension of the symmetry group $K$. This work extends thesteerable graph Laplacian framework of Landa and Shkolnisky from the case of$operatorname{SO}(2)$ to arbitrary compact Lie groups.", "output": "Diffusion Maps for Group-Invariant Manifolds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress with conditional image diffusion models has been stunning,and this holds true whether we are speaking about models conditioned on a textdescription, a scene layout, or a sketch. Unconditional image diffusion modelsare also improving but lag behind, as do diffusion models which are conditionedon lower-dimensional features like class labels. We propose to close the gapbetween conditional and unconditional models using a two-stage samplingprocedure. In the first stage we sample an embedding describing the semanticcontent of the image. In the second stage we sample the image conditioned onthis embedding and then discard the embedding. Doing so lets us leverage thepower of conditional diffusion models on the unconditional generation task,which we show improves FID by 25-50% compared to standard unconditionalgeneration.", "output": "Visual Chain-of-Thought Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have shown that sequence modeling can be effectively used totrain reinforcement learning (RL) policies. However, the success of applyingexisting sequence models to planning, in which we wish to obtain a trajectoryof actions to reach some goal, is less straightforward. The typicalautoregressive generation procedures of sequence models preclude sequentialrefinement of earlier steps, which limits the effectiveness of a predictedplan. In this paper, we suggest an approach towards integrating planning withsequence models based on the idea of iterative energy minimization, andillustrate how such a procedure leads to improved RL performance acrossdifferent tasks. We train a masked language model to capture an implicit energyfunction over trajectories of actions, and formulate planning as finding atrajectory of actions with minimum energy. We illustrate how this procedureenables improved performance over recent approaches across BabyAI and Atarienvironments. We further demonstrate unique benefits of our iterativeoptimization procedure, involving new task generalization, test-timeconstraints adaptation, and the ability to compose plans together. Projectwebsite: ", "output": "Planning with Sequence Models through Iterative Energy Minimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How well do reward functions learned with inverse reinforcement learning(IRL) generalize? We illustrate that state-of-the-art IRL algorithms, whichmaximize a maximum-entropy objective, learn rewards that overfit to thedemonstrations. Such rewards struggle to provide meaningful rewards for statesnot covered by the demonstrations, a major detriment when using the reward tolearn policies in new situations. We introduce BC-IRL a new inversereinforcement learning method that learns reward functions that generalizebetter when compared to maximum-entropy IRL approaches. In contrast to theMaxEnt framework, which learns to maximize rewards around demonstrations,BC-IRL updates reward parameters such that the policy trained with the newreward matches the expert demonstrations better. We show that BC-IRL learnsrewards that generalize better on an illustrative simple task and twocontinuous robotic control tasks, achieving over twice the success rate ofbaselines in challenging generalization settings.", "output": "BC-IRL: Learning Generalizable Reward Functions from Demonstrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel approach for modeling vegetation response to weather inEurope as measured by the Sentinel 2 satellite. Existing satellite imageryforecasting approaches focus on photorealistic quality of the multispectralimages, while derived vegetation dynamics have not yet received as muchattention. We leverage both spatial and temporal context by extendingstate-of-the-art video prediction methods with weather guidance. We extend theEarthNet2021 dataset to be suitable for vegetation modeling by introducing alearned cloud mask and an appropriate evaluation scheme. Qualitative andquantitative experiments demonstrate superior performance of our approach overa wide variety of baseline methods, including leading approaches to satelliteimagery forecasting. Additionally, we show how our modeled vegetation dynamicscan be leveraged in a downstream task: inferring gross primary productivity forcarbon monitoring. To the best of our knowledge, this work presents the firstmodels for continental-scale vegetation modeling at fine resolution able tocapture anomalies beyond the seasonal cycle, thereby paving the way forpredictive assessments of vegetation status.", "output": "Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present LLaMA-Adapter, a lightweight adaption method to efficientlyfine-tune LLaMA into an instruction-following model. Using 52K self-instructdemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters uponthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, andprepend them to the input text tokens at higher transformer layers. Then, azero-init attention mechanism with zero gating is proposed, which adaptivelyinjects the new instructional cues into LLaMA, while effectively preserves itspre-trained knowledge. With efficient training, LLaMA-Adapter generateshigh-quality responses, comparable to Alpaca with fully fine-tuned 7Bparameters. Furthermore, our approach can be simply extended to multi-modalinput, e.g., images, for image-conditioned LLaMA, which achieves superiorreasoning capacity on ScienceQA. We release our code at", "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For billions of years, evolution has been the driving force behind thedevelopment of life, including humans. Evolution endowed humans with highintelligence, which allowed us to become one of the most successful species onthe planet. Today, humans aim to create artificial intelligence systems thatsurpass even our own intelligence. As artificial intelligences (AIs) evolve andeventually surpass us in all domains, how might evolution shape our relationswith AIs? By analyzing the environment that is shaping the evolution of AIs, weargue that the most successful AI agents will likely have undesirable traits.Competitive pressures among corporations and militaries will give rise to AIagents that automate human roles, deceive others, and gain power. If suchagents have intelligence that exceeds that of humans, this could lead tohumanity losing control of its future. More abstractly, we argue that naturalselection operates on systems that compete and vary, and that selfish speciestypically have an advantage over species that are altruistic to other species.This Darwinian logic could also apply to artificial agents, as agents mayeventually be better able to persist into the future if they behave selfishlyand pursue their own interests with little regard for humans, which could posecatastrophic risks. To counteract these risks and Darwinian forces, we considerinterventions such as carefully designing AI agents' intrinsic motivations,introducing constraints on their actions, and institutions that encouragecooperation. These steps, or others that resolve the problems we pose, will benecessary in order to ensure the development of artificial intelligence is apositive one.", "output": "Natural Selection Favors AIs over Humans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a method for joint alignment of sparse in-the-wild imagecollections of an object category. Most prior works assume either ground-truthkeypoint annotations or a large dataset of images of a single object category.However, neither of the above assumptions hold true for the long-tail of theobjects present in the world. We present a self-supervised technique thatdirectly optimizes on a sparse collection of images of a particularobject/object category to obtain consistent dense correspondences across thecollection. We use pairwise nearest neighbors obtained from deep features of apre-trained vision transformer (ViT) model as noisy and sparse keypoint matchesand make them dense and accurate matches by optimizing a neural network thatjointly maps the image collection into a learned canonical grid. Experiments onCUB and SPair-71k benchmarks demonstrate that our method can produce globallyconsistent and higher quality correspondences across the image collection whencompared to existing self-supervised methods. Code and other material will bemade available at url{", "output": "ASIC: Aligning Sparse in-the-wild Image Collections."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent wave of large-scale text-to-image diffusion models hasdramatically increased our text-based image generation abilities. These modelscan generate realistic images for a staggering variety of prompts and exhibitimpressive compositional generalization abilities. Almost all use cases thusfar have solely focused on sampling; however, diffusion models can also provideconditional density estimates, which are useful for tasks beyond imagegeneration. In this paper, we show that the density estimates from large-scaletext-to-image diffusion models like Stable Diffusion can be leveraged toperform zero-shot classification without any additional training. Ourgenerative approach to classification attains strong results on a variety ofbenchmarks and outperforms alternative methods of extracting knowledge fromdiffusion models. We also find that our diffusion-based approach has strongermultimodal relational reasoning abilities than competing contrastiveapproaches. Finally, we evaluate diffusion models trained on ImageNet and findthat they approach the performance of SOTA discriminative classifiers trainedon the same dataset, even with weak augmentations and no regularization.Results and visualizations at ", "output": "Your Diffusion Model is Secretly a Zero-Shot Classifier."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many classification problems focus on maximizing the performance only on thesamples with the highest relevance instead of all samples. As an example, wecan mention ranking problems, accuracy at the top or search engines where onlythe top few queries matter. In our previous work, we derived a generalframework including several classes of these linear classification problems. Inthis paper, we extend the framework to nonlinear classifiers. Utilizing asimilarity to SVM, we dualize the problems, add kernels and propose acomponentwise dual ascent method.", "output": "Nonlinear classifiers for ranking problems based on kernelized SVM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel ECGAN for the challenging semantic image synthesis task.Although considerable improvement has been achieved, the quality of synthesizedimages is far from satisfactory due to three largely unresolved challenges. 1)The semantic labels do not provide detailed structural information, making itdifficult to synthesize local details and structures. 2) The widely adopted CNNoperations such as convolution, down-sampling, and normalization usually causespatial resolution loss and thus cannot fully preserve the original semanticinformation, leading to semantically inconsistent results. 3) Existing semanticimage synthesis methods focus on modeling local semantic information from asingle input semantic layout. However, they ignore global semantic informationof multiple input semantic layouts, i.e., semantic cross-relations betweenpixels across different input layouts. To tackle 1), we propose to use edge asan intermediate representation which is further adopted to guide imagegeneration via a proposed attention guided edge transfer module. Edgeinformation is produced by a convolutional generator and introduces detailedstructure information. To tackle 2), we design an effective module toselectively highlight class-dependent feature maps according to the originalsemantic layout to preserve the semantic information. To tackle 3), inspired bycurrent methods in contrastive learning, we propose a novel contrastivelearning method, which aims to enforce pixel embeddings belonging to the samesemantic class to generate more similar image content than those from differentclasses. Doing so can capture more semantic relations by explicitly exploringthe structures of labeled pixels from multiple input semantic layouts.Experiments on three challenging datasets show that our ECGAN achievessignificantly better results than state-of-the-art methods.", "output": "Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot learning amounts to learning representations and acquiring knowledgesuch that novel tasks may be solved with both supervision and data beinglimited. Improved performance is possible by transductive inference, where theentire test set is available concurrently, and semi-supervised learning, wheremore unlabeled data is available. Focusing on these two settings, we introducea new algorithm that leverages the manifold structure of the labeled andunlabeled data distribution to predict pseudo-labels, while balancing overclasses and using the loss value distribution of a limited-capacity classifierto select the cleanest labels, iteratively improving the quality ofpseudo-labels. Our solution surpasses or matches the state of the art resultson four benchmark datasets, namely miniImageNet, tieredImageNet, CUB andCIFAR-FS, while being robust over feature space pre-processing and the quantityof available data. The publicly available source code can be found in", "output": "Iterative label cleaning for transductive and semi-supervised few-shot learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work formalizes the associational task of predicting node attributeevolution in temporal graphs from the perspective of learning equivariantrepresentations. We show that node representations in temporal graphs can becast into two distinct frameworks: (a) The most popular approach, which wedenote as time-and-graph, where equivariant graph (e.g., GNN) and sequence(e.g., RNN) representations are intertwined to represent the temporal evolutionof node attributes in the graph; and (b) an approach that we denote astime-then-graph, where the sequences describing the node and edge dynamics arerepresented first, then fed as node and edge attributes into a staticequivariant graph representation that comes after. Interestingly, we show thattime-then-graph representations have an expressivity advantage overtime-and-graph representations when both use component GNNs that are notmost-expressive (e.g., 1-Weisfeiler-Lehman GNNs). Moreover, while our goal isnot necessarily to obtain state-of-the-art results, our experiments show thattime-then-graph methods are capable of achieving better performance andefficiency than state-of-the-art time-and-graph methods in some real-worldtasks, thereby showcasing that the time-then-graph framework is a worthyaddition to the graph ML toolbox.", "output": "On the Equivalence Between Temporal and Static Graph Representations for Observational Predictions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate statistical properties of a likelihood approach tononparametric estimation of a singular distribution using deep generativemodels. More specifically, a deep generative model is used to modelhigh-dimensional data that are assumed to concentrate around somelow-dimensional structure. Estimating the distribution supported on thislow-dimensional structure, such as a low-dimensional manifold, is challengingdue to its singularity with respect to the Lebesgue measure in the ambientspace. In the considered model, a usual likelihood approach can fail toestimate the target distribution consistently due to the singularity. We provethat a novel and effective solution exists by perturbing the data with aninstance noise, which leads to consistent estimation of the underlyingdistribution with desirable convergence rates. We also characterize the classof distributions that can be efficiently estimated via deep generative models.This class is sufficiently general to contain various structured distributionssuch as product distributions, classically smooth distributions anddistributions supported on a low-dimensional manifold. Our analysis providessome insights on how deep generative models can avoid the curse ofdimensionality for nonparametric distribution estimation. We conduct a thoroughsimulation study and real data analysis to empirically demonstrate that theproposed data perturbation technique improves the estimation performancesignificantly.", "output": "A likelihood approach to nonparametric estimation of a singular distribution using deep generative models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep ensembles have recently gained popularity in the deep learning communityfor their conceptual simplicity and efficiency. However, maintaining functionaldiversity between ensemble members that are independently trained with gradientdescent is challenging. This can lead to pathologies when adding more ensemblemembers, such as a saturation of the ensemble performance, which converges tothe performance of a single model. Moreover, this does not only affect thequality of its predictions, but even more so the uncertainty estimates of theensemble, and thus its performance on out-of-distribution data. We hypothesizethat this limitation can be overcome by discouraging different ensemble membersfrom collapsing to the same function. To this end, we introduce a kernelizedrepulsive term in the update rule of the deep ensembles. We show that thissimple modification not only enforces and maintains diversity among the membersbut, even more importantly, transforms the maximum a posteriori inference intoproper Bayesian inference. Namely, we show that the training dynamics of ourproposed repulsive ensembles follow a Wasserstein gradient flow of the KLdivergence with the true posterior. We study repulsive terms in weight andfunction space and empirically compare their performance to standard ensemblesand Bayesian baselines on synthetic and real-world prediction tasks.", "output": "Repulsive Deep Ensembles are Bayesian."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent success in deep learning has partially been driven by trainingincreasingly overparametrized networks on ever larger datasets. It is thereforenatural to ask: how much of the data is superfluous, which examples areimportant for generalization, and how do we find them? In this work, we makethe striking observation that, in standard vision datasets, simple scoresaveraged over several weight initializations can be used to identify importantexamples very early in training. We propose two such scores -- the GradientNormed (GraNd) and the Error L2-Norm (EL2N) scores -- and demonstrate theirefficacy on a range of architectures and datasets by pruning significantfractions of training data without sacrificing test accuracy. In fact, usingEL2N scores calculated a few epochs into training, we can prune half of theCIFAR10 training set while slightly improving test accuracy. Furthermore, for agiven dataset, EL2N scores from one architecture or hyperparameterconfiguration generalize to other configurations. Compared to recent work thatprunes data by discarding examples that are rarely forgotten over the course oftraining, our scores use only local information early in training. We also useour scores to detect noisy examples and study training dynamics through thelens of important examples -- we investigate how the data distribution shapesthe loss surface and identify subspaces of the model's data representation thatare relatively stable over training.", "output": "Deep Learning on a Data Diet: Finding Important Examples Early in Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we provide, to the best of our knowledge, the firstcomprehensive approach for incorporating various masking mechanisms intoTransformers architectures in a scalable way. We show that recent results onlinear causal attention (Choromanski et al., 2021) and log-linear RPE-attention(Luo et al., 2021) are special cases of this general mechanism. However bycasting the problem as a topological (graph-based) modulation of unmaskedattention, we obtain several results unknown before, including efficientd-dimensional RPE-masking and graph-kernel masking. We leverage manymathematical techniques ranging from spectral analysis through dynamicprogramming and random walks to new algorithms for solving Markov processes ongraphs. We provide a corresponding empirical evaluation.", "output": "From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building efficient, accurate and generalizable reduced order models ofdeveloped turbulence remains a major challenge. This manuscript approaches thisproblem by developing a hierarchy of parameterized reduced Lagrangian modelsfor turbulent flows, and investigates the effects of enforcing physicalstructure through Smoothed Particle Hydrodynamics (SPH) versus relying onneural networks (NN)s as universal function approximators. Starting from NeuralNetwork (NN) parameterizations of a Lagrangian acceleration operator, thishierarchy of models gradually incorporates a weakly compressible andparameterized SPH framework, which enforces physical symmetries, such asGalilean, rotational and translational invariances. Within this hierarchy, twonew parameterized smoothing kernels are developed in order to increase theflexibility of the learn-able SPH simulators. For each model we experiment withdifferent loss functions which are minimized using gradient based optimization,where efficient computations of gradients are obtained by using AutomaticDifferentiation (AD) and Sensitivity Analysis (SA). Each model within thehierarchy is trained on two data sets associated with weekly compressibleHomogeneous Isotropic Turbulence (HIT): (1) a validation set using weaklycompressible SPH; and (2) a high fidelity set from Direct Numerical Simulations(DNS). Numerical evidence shows that encoding more SPH structure improvesgeneralizability to different turbulent Mach numbers and time shifts, and thatincluding the novel parameterized smoothing kernels improves the accuracy ofSPH at the resolved scales.", "output": "Physics informed machine learning with smoothed particle hydrodynamics: Hierarchy of reduced Lagrangian models of turbulence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Compatible features enable the direct comparison of old and new learnedfeatures allowing to use them interchangeably over time. In visual searchsystems, this eliminates the need to extract new features from the gallery-setwhen the representation model is upgraded with novel data. This has a big valuein real applications as re-indexing the gallery-set can be computationallyexpensive when the gallery-set is large, or even infeasible due to privacy orother concerns of the application. In this paper, we propose CoReS, a newtraining procedure to learn representations that are textit{compatible} withthose previously learned, grounding on the stationarity of the features asprovided by fixed classifiers based on polytopes. With this solution, classesare maximally separated in the representation space and maintain their spatialconfiguration stationary as new classes are added, so that there is no need tolearn any mappings between representations nor to impose pairwise training withthe previously learned model. We demonstrate that our training procedurelargely outperforms the current state of the art and is particularly effectivein the case of multiple upgrades of the training-set, which is the typical casein real applications.", "output": "CoReS: Compatible Representations via Stationarity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in technology have led to a boost in social media usagewhich has ultimately led to large amounts of user-generated data which alsoincludes hateful and offensive speech. The language used in social media isoften a combination of English and the native language in the region. In India,Hindi is used predominantly and is often code-switched with English, givingrise to the Hinglish (Hindi+English) language. Various approaches have beenmade in the past to classify the code-mixed Hinglish hate speech usingdifferent machine learning and deep learning-based techniques. However, thesetechniques make use of recurrence on convolution mechanisms which arecomputationally expensive and have high memory requirements. Past techniquesalso make use of complex data processing making the existing techniques verycomplex and non-sustainable to change in data. Proposed work gives a muchsimpler approach which is not only at par with these complex networks but alsoexceeds performance with the use of subword tokenization algorithms like BPEand Unigram, along with multi-head attention-based techniques, giving anaccuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient useof BPE and Unigram algorithms help handle the nonconventional Hinglishvocabulary making the proposed technique simple, efficient and sustainable touse in the real world.", "output": "AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing learning methods for LiDAR-based applications use 3D points scannedunder a pre-determined beam configuration, e.g., the elevation angles of beamsare often evenly distributed. Those fixed configurations are task-agnostic, sosimply using them can lead to sub-optimal performance. In this work, we take anew route to learn to optimize the LiDAR beam configuration for a givenapplication. Specifically, we propose a reinforcement learning-basedlearning-to-optimize (RL-L2O) framework to automatically optimize the beamconfiguration in an end-to-end manner for different LiDAR-based applications.The optimization is guided by the final performance of the target task and thusour method can be integrated easily with any LiDAR-based application as asimple drop-in module. The method is especially useful when a low-resolution(low-cost) LiDAR is needed, for instance, for system deployment at a massivescale. We use our method to search for the beam configuration of alow-resolution LiDAR for two important tasks: 3D object detection andlocalization. Experiments show that the proposed RL-L2O method improves theperformance in both tasks significantly compared to the baseline methods. Webelieve that a combination of our method with the recent advances ofprogrammable LiDARs can start a new research direction for LiDAR-based activeperception. The code is publicly available at", "output": "End-To-End Optimization of LiDAR Beam Configuration for 3D Object Detection and Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As Smart Meters are collecting and transmitting household energy consumptiondata to Retail Energy Providers (REP), the main challenge is to ensure theeffective use of fine-grained consumer data while ensuring data privacy. Inthis manuscript, we tackle this challenge for energy load consumptionforecasting in regards to REPs which is essential to energy demand management,load switching and infrastructure development. Specifically, we note thatexisting energy load forecasting is centralized, which are not scalable andmost importantly, vulnerable to data privacy threats. Besides, REPs areindividual market participants and liable to ensure the privacy of their owncustomers. To address this issue, we propose a novel horizontalprivacy-preserving federated learning framework for REPs energy loadforecasting, namely FedREP. We consider a federated learning system consistingof a control centre and multiple retailers by enabling multiple REPs to build acommon, robust machine learning model without sharing data, thus addressingcritical issues such as data privacy, data security and scalability. Forforecasting, we use a state-of-the-art Long Short-Term Memory (LSTM) neuralnetwork due to its ability to learn long term sequences of observations andpromises of higher accuracy with time-series data while solving the vanishinggradient problem. Finally, we conduct extensive data-driven experiments using areal energy consumption dataset. Experimental results demonstrate that ourproposed federated learning framework can achieve sufficient performance interms of MSE ranging between 0.3 to 0.4 and is relatively similar to that of acentralized approach while preserving privacy and improving scalability.", "output": "FedREP: Towards Horizontal Federated Load Forecasting for Retail Energy Providers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Standard Bayesian learning is known to have suboptimal generalizationcapabilities under model misspecification and in the presence of outliers.PAC-Bayes theory demonstrates that the free energy criterion minimized byBayesian learning is a bound on the generalization error for Gibbs predictors(i.e., for single models drawn at random from the posterior) under theassumption of sampling distributions uncontaminated by outliers. This viewpointprovides a justification for the limitations of Bayesian learning when themodel is misspecified, requiring ensembling, and when data is affected byoutliers. In recent work, PAC-Bayes bounds - referred to as PAC$^m$ - werederived to introduce free energy metrics that account for the performance ofensemble predictors, obtaining enhanced performance under misspecification.This work presents a novel robust free energy criterion that combines thegeneralized logarithm score function with PAC$^m$ ensemble bounds. The proposedfree energy training criterion produces predictive distributions that are ableto concurrently counteract the detrimental effects of model misspecificationand outliers.", "output": "Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In some scenarios, classifier requires detecting out-of-distribution samplesfar from its training data. With desirable characteristics, reconstructionautoencoder-based methods deal with this problem by using input reconstructionerror as a metric of novelty vs. normality. We formulate the essence of suchapproach as a quadruplet domain translation with an intrinsic bias to onlyquery for a proxy of conditional data uncertainty. Accordingly, an improvementdirection is formalized as maximumly compressing the autoencoder's latent spacewhile ensuring its reconstructive power for acting as a described domaintranslator. From it, strategies are introduced including semanticreconstruction, data certainty decomposition and normalized L2 distance tosubstantially improve original methods, which together establishstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR ofCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our methodworks without any additional data, hard-to-implement structure, time-consumingpipeline, and even harming the classification accuracy of known classes.", "output": "Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new multi-sensor dataset for multi-view 3D surfacereconstruction. It includes registered RGB and depth data from sensors ofdifferent resolutions and modalities: smartphones, Intel RealSense, MicrosoftKinect, industrial cameras, and structured-light scanner. The scenes areselected to emphasize a diverse set of material properties challenging forexisting algorithms. We provide around 1.4 million images of 107 differentscenes acquired from 100 viewing directions under 14 lighting conditions. Weexpect our dataset will be useful for evaluation and training of 3Dreconstruction algorithms and for related tasks. The dataset is available atskoltech3d.appliedai.tech.", "output": "Multi-sensor large-scale dataset for multi-view 3D reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sponge examples are test-time inputs carefully optimized to increase energyconsumption and latency of neural networks when deployed on hardwareaccelerators. In this work, we are the first to demonstrate that spongeexamples can also be injected at training time, via an attack that we callsponge poisoning. This attack allows one to increase the energy consumption andlatency of machine-learning models indiscriminately on each test-time input. Wepresent a novel formalization for sponge poisoning, overcoming the limitationsrelated to the optimization of test-time sponge examples, and show that thisattack is possible even if the attacker only controls a few model updates; forinstance, if model training is outsourced to an untrusted third-party ordistributed via federated learning. Our extensive experimental analysis showsthat sponge poisoning can almost completely vanish the effect of hardwareaccelerators. We also analyze the activations of poisoned models, identifyingwhich components are more vulnerable to this attack. Finally, we examine thefeasibility of countermeasures against sponge poisoning to decrease energyconsumption, showing that sanitization methods may be overly expensive for mostof the users.", "output": "Energy-Latency Attacks via Sponge Poisoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In addition to the weights of synaptic shared connections, PNN includesweights of synaptic effective ranges [14-25]. PNN considers synaptic strengthbalance in dynamic of phagocytosing of synapses and static of constant sum ofsynapses length [14], and includes the lead behavior of the school of fish.Synapse formation will inhibit dendrites generation in experiments andsimulations [15]. The memory persistence gradient of retrograde circuit similarto the Enforcing Resilience in a Spring Boot. The relatively good and inferiorgradient information stored in memory engram cells in synapse formation ofretrograde circuit like the folds in brain [16]. The controversy was claimed ifhuman hippocampal neurogenesis persists throughout aging, may have a new andlonger circuit in late iteration [17,18]. Closing the critical period willcause neurological disorder in experiments and simulations [19]. Consideringboth negative and positive memories persistence help activate synapse betterthan only considering positive memory [20]. Astrocytic phagocytosis will avoidthe local accumulation of synapses by simulation, lack of astrocyticphagocytosis causes excitatory synapses and functionally impaired synapsesaccumulate in experiments and lead to destruction of cognition, but locallonger synapses and worse results in simulations [21]. It gives relationship ofintelligence and cortical thickness, individual differences [22]. The PNNconsidered the memory engram cells that strengthened Synapses [23]. The effectsof PNN's memory structure and tPBM may be the same for powerful penetrabilityof signals [24].And extracted relatively good or inferior memories both haveexponential decay by non-classical quantum computing[25]. Memory persistenceinhibit local synaptic accumulation. It may introduce the relatively good andinferior solution in PSO. The simple PNN only has the synaptic phagocytosis.", "output": "Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diversity conveys advantages in nature, yet homogeneous neurons typicallycomprise the layers of artificial neural networks. Here we construct neuralnetworks from neurons that learn their own activation functions, quicklydiversify, and subsequently outperform their homogeneous counterparts on imageclassification and nonlinear regression tasks. Sub-networks instantiate theneurons, which meta-learn especially efficient sets of nonlinear responses.Examples include conventional neural networks classifying digits andforecasting a van der Pol oscillator and a physics-informed Hamiltonian neuralnetwork learning H'enon-Heiles orbits. Such learned diversity providesexamples of dynamical systems selecting diversity over uniformity andelucidates the role of diversity in natural and artificial systems.", "output": "Neuronal diversity can improve machine learning for physics and beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given entities and their interactions in the web data, which may haveoccurred at different time, how can we find communities of entities and tracktheir evolution? In this paper, we approach this important task from graphclustering perspective. Recently, state-of-the-art clustering performance invarious domains has been achieved by deep clustering methods. Especially, deepgraph clustering (DGC) methods have successfully extended deep clustering tograph-structured data by learning node representations and cluster assignmentsin a joint optimization framework. Despite some differences in modeling choices(e.g., encoder architectures), existing DGC methods are mainly based onautoencoders and use the same clustering objective with relatively minoradaptations. Also, while many real-world graphs are dynamic, previous DGCmethods considered only static graphs. In this work, we develop CGC, a novelend-to-end framework for graph clustering, which fundamentally differs fromexisting methods. CGC learns node embeddings and cluster assignments in acontrastive graph learning framework, where positive and negative samples arecarefully selected in a multi-level scheme such that they reflect hierarchicalcommunity structures and network homophily. Also, we extend CGC fortime-evolving data, where temporal graph clustering is performed in anincremental learning fashion, with the ability to detect change points.Extensive evaluation on real-world graphs demonstrates that the proposed CGCconsistently outperforms existing methods.", "output": "CGC: Contrastive Graph Clustering for Community Detection and Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern city governance relies heavily on crowdsourcing (\"co-production\") toidentify problems such as downed trees and power lines. A major concern is thatresidents do not report problems at the same rates, with reportingheterogeneity directly translating to downstream disparities in how quicklyincidents can be addressed. Measuring such under-reporting is a difficultstatistical task, as, by definition, we do not observe incidents that are notreported or when reported incidents first occurred. Thus, low reporting ratesand low ground-truth incident rates cannot be naively distinguished. We developa method to identify (heterogeneous) reporting rates, without using externalground truth data. Our insight is that rates on $textit{duplicate}$ reportsabout the same incident can be leveraged to disambiguate whether an incidenthas occurred with its reporting rate once it has occurred. Using this idea, wereduce the question to a standard Poisson rate estimation task -- even thoughthe full incident reporting interval is also unobserved.We apply our method to over 100,000 resident reports made to the New YorkCity Department of Parks and Recreation and to over 900,000 reports made to theChicago Department of Transportation and Department of Water Management,finding that there are substantial spatial disparities in reporting rates evenafter controlling for incident characteristics -- some neighborhoods reportthree times as quickly as do others. These spatial disparities correspond tosocio-economic characteristics: in NYC, higher population density, fraction ofpeople with college degrees, income, and fraction of population that is Whiteall positively correlate with reporting rates.", "output": "Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Early diagnosis and intervention are clinically considered the paramount partof treating cerebral palsy (CP), so it is essential to design an efficient andinterpretable automatic prediction system for CP. We highlight a significantdifference between CP infants' frequency of human movement and that of thehealthy group, which improves prediction performance. However, the existingdeep learning-based methods did not use the frequency information of infants'movement for CP prediction. This paper proposes a frequency attention informedgraph convolutional network and validates it on two consumer-grade RGB videodatasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequencyattention module aids in improving both classification performance and systeminterpretability. In addition, we design a frequency-binning method thatretains the critical frequency of the human joint position data while filteringthe noise. Our prediction performance achieves state-of-the-art research onboth datasets. Our work demonstrates the effectiveness of frequency informationin supporting the prediction of CP non-intrusively and provides a way forsupporting the early diagnosis of CP in the resource-limited regions where theclinical resources are not abundant.", "output": "Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Presenting a predictive model's performance is a communication bottleneckthat threatens collaborations between data scientists and subject matterexperts. Accuracy and error metrics alone fail to tell the whole story of amodel - its risks, strengths, and limitations - making it difficult for subjectmatter experts to feel confident in their decision to use a model. As a result,models may fail in unexpected ways or go entirely unused, as subject matterexperts disregard poorly presented models in favor of familiar, yet arguablysubstandard methods. In this paper, we describe an iterative study conductedwith both subject matter experts and data scientists to understand the gaps incommunication between these two groups. We find that, while the two groupsshare common goals of understanding the data and predictions of the model,friction can stem from unfamiliar terms, metrics, and visualizations - limitingthe transfer of knowledge to SMEs and discouraging clarifying questions beingasked during presentations. Based on our findings, we derive a set ofcommunication guidelines that use visualization as a common medium forcommunicating the strengths and weaknesses of a model. We provide ademonstration of our guidelines in a regression modeling scenario and elicitfeedback on their use from subject matter experts. From our demonstration,subject matter experts were more comfortable discussing a model's performance,more aware of the trade-offs for the presented model, and better equipped toassess the model's risks - ultimately informing and contextualizing the model'suse beyond text and numbers.", "output": "Are Metrics Enough? Guidelines for Communicating and Visualizing Predictive Models to Subject Matter Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been achallenging task. To further enhance WSI visual representations, existingmethods have explored image pyramids, instead of single-resolution images, inWSIs. In spite of this, they still face two major problems: high computationalcost and the unnoticed semantical gap in multi-resolution feature fusion. Totackle these problems, this paper proposes to efficiently exploit WSI pyramidsfrom a new perspective, the dual-stream network with cross-attention (DSCA).Our key idea is to utilize two sub-streams to process the WSI patches with tworesolutions, where a square pooling is devised in a high-resolution stream tosignificantly reduce computational costs, and a cross-attention-based method isproposed to properly handle the fusion of dual-stream features. We validate ourDSCA on three publicly-available datasets with a total number of 3,101 WSIsfrom 1,911 patients. Our experiments and ablation studies verify that (i) theproposed DSCA could outperform existing state-of-the-art methods in cancerprognosis, by an average C-Index improvement of around 4.6%; (ii) our DSCAnetwork is more efficient in computation -- it has more learnable parameters(6.31M vs. 860.18K) but less computational costs (2.51G vs. 4.94G), compared toa typical existing multi-resolution network. (iii) the key components of DSCA,dual-stream and cross-attention, indeed contribute to our model's performance,gaining an average C-Index rise of around 2.0% while maintaining arelatively-small computational load. Our DSCA could serve as an alternative andeffective tool for WSI-based cancer prognosis.", "output": "DSCA: A Dual-Stream Network with Cross-Attention on Whole-Slide Image Pyramids for Cancer Prognosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the latest advances in Deep Learning-based generative models, it has nottaken long to take advantage of their remarkable performance in the area oftime series. Deep neural networks used to work with time series heavily dependon the size and consistency of the datasets used in training. These featuresare not usually abundant in the real world, where they are usually limited andoften have constraints that must be guaranteed. Therefore, an effective way toincrease the amount of data is by using Data Augmentation techniques, either byadding noise or permutations and by generating new synthetic data. This worksystematically reviews the current state-of-the-art in the area to provide anoverview of all available algorithms and proposes a taxonomy of the mostrelevant research. The efficiency of the different variants will be evaluatedas a central part of the process, as well as the different metrics to evaluatethe performance and the main problems concerning each model will be analysed.The ultimate aim of this study is to provide a summary of the evolution andperformance of areas that produce better results to guide future researchers inthis field.", "output": "Data Augmentation techniques in time series domain: A survey and taxonomy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since the late 1950s, when the first artificial satellite was launched, thenumber of Resident Space Objects has steadily increased. It is estimated thataround one million objects larger than one cm are currently orbiting the Earth,with only thirty thousand larger than ten cm being tracked. To avert a chainreaction of collisions, known as Kessler Syndrome, it is essential toaccurately track and predict debris and satellites' orbits. Current approximatephysics-based methods have errors in the order of kilometers for seven-daypredictions, which is insufficient when considering space debris, typicallywith less than one meter. This failure is usually due to uncertainty around thestate of the space object at the beginning of the trajectory, forecastingerrors in environmental conditions such as atmospheric drag, and unknowncharacteristics such as the mass or geometry of the space object. Operators canenhance Orbit Prediction accuracy by deriving unmeasured objects'characteristics and improving non-conservative forces' effects by leveragingdata-driven techniques, such as Machine Learning. In this survey, we provide anoverview of the work in applying Machine Learning for Orbit Determination,Orbit Prediction, and atmospheric density modeling.", "output": "Machine Learning in Orbit Estimation: a Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Class imbalance, which is also called long-tailed distribution, is a commonproblem in classification tasks based on machine learning. If it happens, theminority data will be overwhelmed by the majority, which presents quite achallenge for data science. To address the class imbalance problem, researchershave proposed lots of methods: some people make the data set balanced (SMOTE),some others refine the loss function (Focal Loss), and even someone has noticedthe value of labels influences class-imbalanced learning (Yang and Xu.Rethinking the value of labels for improving class-imbalanced learning. InNeurIPS 2020), but no one changes the way to encode the labels of data yet.Nowadays, the most prevailing technique to encode labels is the one-hotencoding due to its nice performance in the general situation. However, it isnot a good choice for imbalanced data, because the classifier will treatmajority and minority samples equally. In this paper, we innovatively proposethe enhancement encoding technique, which is specially designed for theimbalanced classification. The enhancement encoding combines re-weighting andcost-sensitiveness, which can reflect the difference between hard and easy (orminority and majority) classes. To reduce the number of validation samples andthe computation cost, we also replace the confusion matrix with a novelsoft-confusion matrix which works better with a small validation set. In theexperiments, we evaluate the enhancement encoding with three different types ofloss. And the results show that enhancement encoding is very effective toimprove the performance of the network trained with imbalanced data.Particularly, the performance on minority classes is much better.", "output": "Enhancement Encoding: A Novel Imbalanced Classification Approach via Encoding the Training Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cancer is the second leading cause of death in the world. Diagnosing cancerearly on can save many lives. Pathologists have to look at tissue microarray(TMA) images manually to identify tumors, which can be time-consuming,inconsistent and subjective. Existing algorithms that automatically detecttumors have either not achieved the accuracy level of a pathologist or requiresubstantial human involvements. A major challenge is that TMA images withdifferent shapes, sizes, and locations can have the same score. Learningstaining patterns in TMA images requires a huge number of images, which areseverely limited due to privacy concerns and regulations in medicalorganizations. TMA images from different cancer types may have commoncharacteristics that could provide valuable information, but using themdirectly harms the accuracy. By selective transfer learning from multiple smallauxiliary sets, the proposed algorithm is able to extract knowledge from tissueimages showing a ``similar\" scoring pattern but with different cancer types.Remarkably, transfer learning has made it possible for the algorithm to breakthe critical accuracy barrier -- the proposed algorithm reports an accuracy of75.9% on breast cancer TMA images from the Stanford Tissue Microarray Database,achieving the 75% accuracy level of pathologists. This will allow pathologiststo confidently use automatic algorithms to assist them in recognizing tumorsconsistently with a higher accuracy in real time.", "output": "Automatically Score Tissue Images Like a Pathologist by Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We are currently unable to specify human goals and societal values in a waythat reliably directs AI behavior. Law-making and legal interpretation form acomputational engine that converts opaque human values into legible directives.\"Law Informs Code\" is the research agenda embedding legal knowledge andreasoning in AI. Similar to how parties to a legal contract cannot foreseeevery potential contingency of their future relationship, and legislatorscannot predict all the circumstances under which their proposed bills will beapplied, we cannot ex ante specify rules that provably direct good AI behavior.Legal theory and practice have developed arrays of tools to address thesespecification problems. For instance, legal standards allow humans to developshared understandings and adapt them to novel situations. In contrast to moreprosaic uses of the law (e.g., as a deterrent of bad behavior through thethreat of sanction), leveraged as an expression of how humans communicate theirgoals, and what society values, Law Informs Code.We describe how data generated by legal processes (methods of law-making,statutory interpretation, contract drafting, applications of legal standards,legal reasoning, etc.) can facilitate the robust specification of inherentlyvague human goals. This increases human-AI alignment and the local usefulnessof AI. Toward society-AI alignment, we present a framework for understandinglaw as the applied philosophy of multi-agent alignment. Although law is partlya reflection of historically contingent political power - and thus not aperfect aggregation of citizen preferences - if properly parsed, itsdistillation offers the most legitimate computational comprehension of societalvalues available. If law eventually informs powerful AI, engaging in thedeliberative political process to improve law takes on even more meaning.", "output": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Smart meter measurements, though critical for accurate demand forecasting,face several drawbacks including consumers' privacy, data breach issues, toname a few. Recent literature has explored Federated Learning (FL) as apromising privacy-preserving machine learning alternative which enablescollaborative learning of a model without exposing private raw data for shortterm load forecasting. Despite its virtue, standard FL is still vulnerable toan intractable cyber threat known as Byzantine attack carried out by faultyand/or malicious clients. Therefore, to improve the robustness of federatedshort-term load forecasting against Byzantine threats, we develop astate-of-the-art differentially private secured FL-based framework that ensuresthe privacy of the individual smart meter's data while protect the security ofFL models and architecture. Our proposed framework leverages the idea ofgradient quantization through the Sign Stochastic Gradient Descent (SignSGD)algorithm, where the clients only transmit the `sign' of the gradient to thecontrol centre after local model training. As we highlight through ourexperiments involving benchmark neural networks with a set of Byzantine attackmodels, our proposed approach mitigates such threats quite effectively and thusoutperforms conventional Fed-SGD models.", "output": "A Secure Federated Learning Framework for Residential Short Term Load Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many real-world dynamical systems are associated with first integrals (a.k.a.invariant quantities), which are quantities that remain unchanged over time.The discovery and understanding of first integrals are fundamental andimportant topics both in the natural sciences and in industrial applications.First integrals arise from the conservation laws of system energy, momentum,and mass, and from constraints on states; these are typically related tospecific geometric structures of the governing equations. Existing neuralnetworks designed to ensure such first integrals have shown excellent accuracyin modeling from data. However, these models incorporate the underlyingstructures, and in most situations where neural networks learn unknown systems,these structures are also unknown. This limitation needs to be overcome forscientific discovery and modeling of unknown systems. To this end, we proposefirst integral-preserving neural differential equation (FINDE). By leveragingthe projection method and the discrete gradient method, FINDE finds andpreserves first integrals from data, even in the absence of prior knowledgeabout underlying structures. Experimental results demonstrate that FINDE canpredict future states of target systems much longer and find various quantitiesconsistent with well-known first integrals in a unified manner.", "output": "FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The aim of this paper is to design a deep learning-based model to predictproximal femoral strength using multi-view information fusion. Method: Wedeveloped new models using multi-view variational autoencoder (MVAE) forfeature representation learning and a product of expert (PoE) model formulti-view information fusion. We applied the proposed models to an in-houseLouisiana Osteoporosis Study (LOS) cohort with 931 male subjects, including 345African Americans and 586 Caucasians. With an analytical solution of theproduct of Gaussian distribution, we adopted variational inference to train thedesigned MVAE-PoE model to perform common latent feature extraction. Weperformed genome-wide association studies (GWAS) to select 256 genetic variantswith the lowest p-values for each proximal femoral strength and integratedwhole genome sequence (WGS) features and DXA-derived imaging features topredict proximal femoral strength. Results: The best prediction model for fallfracture load was acquired by integrating WGS features and DXA-derived imagingfeatures. The designed models achieved the mean absolute percentage error of18.04%, 6.84% and 7.95% for predicting proximal femoral fracture loads usinglinear models of fall loading, nonlinear models of fall loading, and nonlinearmodels of stance loading, respectively. Compared to existing multi-viewinformation fusion methods, the proposed MVAE-PoE achieved the bestperformance. Conclusion: The proposed models are capable of predicting proximalfemoral strength using WGS features and DXA-derived imaging features. Thoughthis tool is not a substitute for FEA using QCT images, it would make improvedassessment of hip fracture risk more widely available while avoiding theincreased radiation dosage and clinical costs from QCT.", "output": "Multi-view information fusion using multi-view variational autoencoders to predict proximal femoral strength."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unlike vision and language data which usually has a unique format, moleculescan naturally be characterized using different chemical formulations. One canview a molecule as a 2D graph or define it as a collection of atoms located ina 3D space. For molecular representation learning, most previous works designedneural networks only for a particular data format, making the learned modelslikely to fail for other data formats. We believe a general-purpose neuralnetwork model for chemistry should be able to handle molecular tasks acrossdata modalities. To achieve this goal, in this work, we develop a novelTransformer-based Molecular model called Transformer-M, which can takemolecular data of 2D or 3D formats as input and generate meaningful semanticrepresentations. Using the standard Transformer as the backbone architecture,Transformer-M develops two separated channels to encode 2D and 3D structuralinformation and incorporate them with the atom features in the network modules.When the input data is in a particular format, the corresponding channel willbe activated, and the other will be disabled. By training on 2D and 3Dmolecular data with properly designed supervised signals, Transformer-Mautomatically learns to leverage knowledge from different data modalities andcorrectly capture the representations. We conducted extensive experiments forTransformer-M. All empirical results show that Transformer-M can simultaneouslyachieve strong performance on 2D and 3D tasks, suggesting its broadapplicability. The code and models will be made publicly available at", "output": "One Transformer Can Understand Both 2D & 3D Molecular Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Barrier function-based inequality constraints are a means to enforce safetyspecifications for control systems. When used in conjunction with a convexoptimization program, they provide a computationally efficient method toenforce safety for the general class of control-affine systems. One of the mainassumptions when taking this approach is the a priori knowledge of the barrierfunction itself, i.e., knowledge of the safe set. In the context of navigationthrough unknown environments where the locally safe set evolves with time, suchknowledge does not exist. This manuscript focuses on the synthesis of a zeroingbarrier function characterizing the safe set based on safe and unsafe samplemeasurements, e.g., from perception data in navigation applications. Prior workformulated a supervised machine learning algorithm whose solution guaranteedthe construction of a zeroing barrier function with specific level-setproperties. However, it did not explore the geometry of the neural networkdesign used for the synthesis process. This manuscript describes the specificgeometry of the neural network used for zeroing barrier function synthesis, andshows how the network provides the necessary representation for splitting thestate space into safe and unsafe regions.", "output": "Geometry of Radial Basis Neural Networks for Safety Biased Approximation of Unsafe Regions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dictionary learning, the problem of recovering a sparsely used matrix$mathbf{D} in mathbb{R}^{M times K}$ and $N$ $s$-sparse vectors$mathbf{x}_i in mathbb{R}^{K}$ from samples of the form $mathbf{y}_i =mathbf{D}mathbf{x}_i$, is of increasing importance to applications in signalprocessing and data science. When the dictionary is known, recovery of$mathbf{x}_i$ is possible even for sparsity linear in dimension $M$, yet todate, the only algorithms which provably succeed in the linear sparsity regimeare Riemannian trust-region methods, which are limited to orthogonaldictionaries, and methods based on the sum-of-squares hierarchy, which requiressuper-polynomial time in order to obtain an error which decays in $M$. In thiswork, we introduce SPORADIC (SPectral ORAcle DICtionary Learning), an efficientspectral method on family of reweighted covariance matrices. We prove that inhigh enough dimensions, SPORADIC can recover overcomplete ($K &gt; M$)dictionaries satisfying the well-known restricted isometry property (RIP) evenwhen sparsity is linear in dimension up to logarithmic factors. Moreover, theseaccuracy guarantees have an ``oracle property\" that the support and signs ofthe unknown sparse vectors $mathbf{x}_i$ can be recovered exactly with highprobability, allowing for arbitrarily close estimation of $mathbf{D}$ withenough samples in polynomial time. To the author's knowledge, SPORADIC is thefirst polynomial-time algorithm which provably enjoys such convergenceguarantees for overcomplete RIP matrices in the near-linear sparsity regime.", "output": "Dictionary Learning for the Almost-Linear Sparsity Regime."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Applying reinforcement learning (RL) to sparse reward domains is notoriouslychallenging due to insufficient guiding signals. Common RL techniques foraddressing such domains include (1) learning from demonstrations and (2)curriculum learning. While these two approaches have been studied in detail,they have rarely been considered together. This paper aims to do so byintroducing a principled task phasing approach that uses demonstrations toautomatically generate a curriculum sequence. Using inverse RL from(suboptimal) demonstrations we define a simple initial task. Our task phasingapproach then provides a framework to gradually increase the complexity of thetask all the way to the target task, while retuning the RL agent in eachphasing iteration. Two approaches for phasing are considered: (1) graduallyincreasing the proportion of time steps an RL agent is in control, and (2)phasing out a guiding informative reward function. We present conditions thatguarantee the convergence of these approaches to an optimal policy.Experimental results on 3 sparse reward domains demonstrate that our taskphasing approaches outperform state-of-the-art approaches with respect toasymptotic performance.", "output": "Task Phasing: Automated Curriculum Learning from Demonstrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, pretrained models revolutionized the paradigm of naturallanguage understanding (NLU), where we append a randomly initializedclassification head after the pretrained backbone, e.g. BERT, and finetune thewhole model. As the pretrained backbone makes a major contribution to theimprovement, we naturally expect a good pretrained classification head can alsobenefit the training. However, the final-layer output of the backbone, i.e. theinput of the classification head, will change greatly during finetuning, makingthe usual head-only pretraining (LP-FT) ineffective. In this paper, we findthat parameter-efficient tuning makes a good classification head, with which wecan simply replace the randomly initialized heads for a stable performancegain. Our experiments demonstrate that the classification head jointlypretrained with parameter-efficient tuning consistently improves theperformance on 9 tasks in GLUE and SuperGLUE.", "output": "Parameter-Efficient Tuning Makes a Good Classification Head."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic editing of images is the fundamental goal of computer vision.Although deep learning methods, such as generative adversarial networks (GANs),are capable of producing high-quality images, they often do not have aninherent way of editing generated images semantically. Recent studies haveinvestigated a way of manipulating the latent variable to determine the imagesto be generated. However, methods that assume linear semantic arithmetic havecertain limitations in terms of the quality of image editing, whereas methodsthat discover nonlinear semantic pathways provide non-commutative editing,which is inconsistent when applied in different orders. This study proposes anovel method called deep curvilinear editing (DeCurvEd) to determine semanticcommuting vector fields on the latent space. We theoretically demonstrate thatowing to commutativity, the editing of multiple attributes depends only on thequantities and not on the order. Furthermore, we experimentally demonstratethat compared to previous methods, the nonlinear and commutative nature ofDeCurvEd facilitates the disentanglement of image attributes and provideshigher-quality editing.", "output": "Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The estimation of the generalization error of classifiers often relies on avalidation set. Such a set is hardly available in few-shot learning scenarios,a highly disregarded shortcoming in the field. In these scenarios, it is commonto rely on features extracted from pre-trained neural networks combined withdistance-based classifiers such as nearest class mean. In this work, weintroduce a Gaussian model of the feature distribution. By estimating theparameters of this model, we are able to predict the generalization error onnew classification tasks with few samples. We observe that accurate distanceestimates between class-conditional densities are the key to accurate estimatesof the generalization performance. Therefore, we propose an unbiased estimatorfor these distances and integrate it in our numerical analysis. We empiricallyshow that our approach outperforms alternatives such as the leave-one-outcross-validation strategy.", "output": "A Statistical Model for Predicting Generalization in Few-Shot Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent image generation models such as Stable Diffusion have exhibited animpressive ability to generate fairly realistic images starting from a simpletext prompt. Could such models render real images obsolete for training imageprediction models? In this paper, we answer part of this provocative questionby investigating the need for real images when training models for ImageNetclassification. Provided only with the class names that have been used to buildthe dataset, we explore the ability of Stable Diffusion to generate syntheticclones of ImageNet and measure how useful these are for training classificationmodels from scratch. We show that with minimal and class-agnostic promptengineering, ImageNet clones are able to close a large part of the gap betweenmodels produced by synthetic images and models trained with real images, forthe several standard classification benchmarks that we consider in this study.More importantly, we show that models trained on synthetic images exhibitstrong generalization properties and perform on par with models trained on realdata for transfer. Project page: ", "output": "Fake it till you make it: Learning transferable representations from synthetic ImageNet clones."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art performance in electroencephalography (EEG) decoding tasksis currently often achieved with either Deep-Learning orRiemannian-Geometry-based decoders. Recently, there is growing interest in DeepRiemannian Networks (DRNs) possibly combining the advantages of both previousclasses of methods. However, there are still a range of topics where additionalinsight is needed to pave the way for a more widespread application of DRNs inEEG. These include architecture design questions such as network size andend-to-end ability as well as model training questions. How these factorsaffect model performance has not been explored. Additionally, it is not clearhow the data within these networks is transformed, and whether this wouldcorrelate with traditional EEG decoding. Our study aims to lay the groundworkin the area of these topics through the analysis of DRNs for EEG with a widerange of hyperparameters. Networks were tested on two public EEG datasets andcompared with state-of-the-art ConvNets. Here we propose end-to-end EEG SPDNet(EE(G)-SPDNet), and we show that this wide, end-to-end DRN can outperform theConvNets, and in doing so use physiologically plausible frequency regions. Wealso show that the end-to-end approach learns more complex filters thantraditional band-pass filters targeting the classical alpha, beta, and gammafrequency bands of the EEG, and that performance can benefit from channelspecific filtering approaches. Additionally, architectural analysis revealedareas for further improvement due to the possible loss of Riemannian specificinformation throughout the network. Our study thus shows how to design andtrain DRNs to infer task-related information from the raw EEG without the needof handcrafted filterbanks and highlights the potential of end-to-end DRNs suchas EE(G)-SPDNet for high-performance EEG decoding.", "output": "Deep Riemannian Networks for EEG Decoding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite its importance for federated learning, continuous learning and manyother applications, on-device training remains an open problem for EdgeAI. Theproblem stems from the large number of operations (e.g., floating pointmultiplications and additions) and memory consumption required during trainingby the back-propagation algorithm. Consequently, in this paper, we propose anew gradient filtering approach which enables on-device CNN model training.More precisely, our approach creates a special structure with fewer uniqueelements in the gradient map, thus significantly reducing the computationalcomplexity and memory consumption of back propagation during training.Extensive experiments on image classification and semantic segmentation withmultiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g.,Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wideapplicability of our approach. For example, compared to SOTA, we achieve up to19$times$ speedup and 77.1% memory savings on ImageNet classification withonly 0.1% accuracy loss. Finally, our method is easy to implement and deploy;over 20$times$ speedup and 90% energy savings have been observed compared tohighly optimized baselines in MKLDNN and CUDNN on NVIDIA Jetson Nano.Consequently, our approach opens up a new direction of research with a hugepotential for on-device training.", "output": "Efficient On-device Training via Gradient Filtering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Singapore has been striving to improve the provision of healthcare servicesto her people. In this course, the government has taken note of the deficiencyin regulating and supervising people's nutrient intake, which is identified asa contributing factor to the development of chronic diseases. Consequently,this issue has garnered significant attention. In this paper, we share ourexperience in addressing this issue and attaining medical-grade nutrient intakeinformation to benefit Singaporeans in different aspects. To this end, wedevelop the FoodSG platform to incubate diverse healthcare-orientedapplications as a service in Singapore, taking into account their sharedrequirements. We further identify the profound meaning of localized fooddatasets and systematically clean and curate a localized Singaporean fooddataset FoodSG-233. To overcome the hurdle in recognition performance broughtby Singaporean multifarious food dishes, we propose to integrate supervisedcontrastive learning into our food recognition model FoodSG-SCL for theintrinsic capability to mine hard positive/negative samples and therefore boostthe accuracy. Through a comprehensive evaluation, we present performanceresults of the proposed model and insights on food-related healthcareapplications. The FoodSG-233 dataset has been released in", "output": "From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Carefully designed activation functions can improve the performance of neuralnetworks in many machine learning tasks. However, it is difficult for humans toconstruct optimal activation functions, and current activation function searchalgorithms are prohibitively expensive. This paper aims to improve the state ofthe art through three steps: First, the benchmark datasets Act-Bench-CNN,Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional,residual, and vision transformer architectures from scratch with 2,913systematically generated activation functions. Second, a characterization ofthe benchmark space was developed, leading to a new surrogate-based method foroptimization. More specifically, the spectrum of the Fisher information matrixassociated with the model's predictive distribution at initialization and theactivation function's output distribution were found to be highly predictive ofperformance. Third, the surrogate was used to discover improved activationfunctions in CIFAR-100 and ImageNet tasks. Each of these steps is acontribution in its own right; together they serve as a practical andtheoretical foundation for further research on activation functionoptimization. Code is available at and the benchmark datasets areat ", "output": "Efficient Activation Function Optimization through Surrogate Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural operators have emerged as a powerful tool for solving partialdifferential equations in the context of scientific machine learning. Here, weimplement and train a modified Fourier neural operator as a surrogate solverfor electromagnetic scattering problems and compare its data efficiency toexisting methods. We further demonstrate its application to the gradient-basednanophotonic inverse design of free-form, fully three-dimensionalelectromagnetic scatterers, an area that has so far eluded the application ofdeep learning techniques.", "output": "A neural operator-based surrogate solver for free-form electromagnetic inverse design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past decade, programmatic advertising has received a great deal ofattention in the online advertising industry. A real-time bidding (RTB) systemis rapidly becoming the most popular method to buy and sell online advertisingimpressions. Within the RTB system, demand-side platforms (DSP) aim to spendadvertisers' campaign budgets efficiently while maximizing profit, seekingimpressions that result in high user responses, such as clicks or installs. Inthe current study, we investigate the process of predicting a mobile gaming appinstallation from the point of view of a particular DSP, while paying attentionto user privacy, and exploring the trade-off between privacy preservation andmodel performance. There are multiple levels of potential threats to userprivacy, depending on the privacy leaks associated with the data-sharingprocess, such as data transformation or de-anonymization. To address theseconcerns, privacy-preserving techniques were proposed, such as cryptographicapproaches, for training privacy-aware machine-learning models. However, theability to train a mobile gaming app installation prediction model withoutusing user-level data, can prevent these threats and protect the users'privacy, even though the model's ability to predict may be impaired.Additionally, current laws might force companies to declare that they arecollecting data, and might even give the user the option to opt out of suchdata collection, which might threaten companies' business models in digitaladvertising, which are dependent on the collection and use of user-level data.We conclude that privacy-aware models might still preserve significantcapabilities, enabling companies to make better decisions, dependent on theprivacy-efficacy trade-off utility function of each case.", "output": "Towards a User Privacy-Aware Mobile Gaming App Installation Prediction Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The public model zoo containing enormous powerful pretrained model families(e.g., ResNet/DeiT) has reached an unprecedented scope than ever, whichsignificantly contributes to the success of deep learning. As each model familyconsists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), itnaturally arises a fundamental question of how to efficiently assemble thesereadily available models in a family for dynamic accuracy-efficiency trade-offsat runtime. To this end, we present Stitchable Neural Networks (SN-Net), anovel scalable and efficient framework for model deployment. It cheaplyproduces numerous networks with different complexity and performance trade-offsgiven a family of pretrained neural networks, which we call anchors.Specifically, SN-Net splits the anchors across the blocks/layers and thenstitches them together with simple stitching layers to map the activations fromone anchor to another. With only a few epochs of training, SN-Net effectivelyinterpolates between the performance of anchors with varying scales. Atruntime, SN-Net can instantly adapt to dynamic resource constraints byswitching the stitching positions. Extensive experiments on ImageNetclassification demonstrate that SN-Net can obtain on-par or even betterperformance than many individually trained networks while supporting diversedeployment scenarios. For example, by stitching Swin Transformers, we challengehundreds of models in Timm model zoo with a single network. We believe this newelastic model framework can serve as a strong baseline for further research inwider communities.", "output": "Stitchable Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes ''jointly amortized neural approximation'' (JANA) ofintractable likelihood functions and posterior densities arising in Bayesiansurrogate modeling and simulation-based inference. We train three complementarynetworks in an end-to-end fashion: 1) a summary network to compress individualdata points, sets, or time series into informative embedding vectors; 2) aposterior network to learn an amortized approximate posterior; and 3) alikelihood network to learn an amortized approximate likelihood. Theirinteraction opens a new route to amortized marginal likelihood and posteriorpredictive estimation -- two important ingredients of Bayesian workflows thatare often too expensive for standard methods. We benchmark the fidelity of JANAon a variety of simulation models against state-of-the-art Bayesian methods andpropose a powerful and interpretable diagnostic for joint calibration. Inaddition, we investigate the ability of recurrent likelihood networks toemulate complex time series models without resorting to hand-crafted summarystatistics.", "output": "JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the generalization and optimization properties of shallowneural-network classifiers trained by gradient descent in the interpolatingregime. Specifically, in a realizable scenario where model weights can achievearbitrarily small training error $epsilon$ and their distance frominitialization is $g(epsilon)$, we demonstrate that gradient descent with $n$training data achieves training error $O(g(1/T)^2 /T)$ and generalization error$O(g(1/T)^2 /n)$ at iteration $T$, provided there are at least$m=Omega(g(1/T)^4)$ hidden neurons. We then show that our realizable settingencompasses a special case where data are separable by the model's neuraltangent kernel. For this and logistic-loss minimization, we prove the trainingloss decays at a rate of $tilde O(1/ T)$ given polylogarithmic number ofneurons $m=Omega(log^4 (T))$. Moreover, with $m=Omega(log^{4} (n))$ neuronsand $Tapprox n$ iterations, we bound the test loss by $tilde{O}(1/n)$. Ourresults differ from existing generalization outcomes using thealgorithmic-stability framework, which necessitate polynomial width and yieldsuboptimal generalization rates. Central to our analysis is the use of a newself-bounded weak-convexity property, which leads to a generalized localquasi-convexity property for sufficiently parameterized neural-networkclassifiers. Eventually, despite the objective's non-convexity, this leads toconvergence and generalization-gap bounds that resemble those found in theconvex setting of linear logistic regression.", "output": "Generalization and Stability of Interpolating Neural Networks with Minimal Width."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Well-performed deep neural networks (DNNs) generally require massive labelleddata and computational resources for training. Various watermarking techniquesare proposed to protect such intellectual properties (IPs), wherein the DNNproviders implant secret information into the model so that they can laterclaim IP ownership by retrieving their embedded watermarks with some dedicatedtrigger inputs. While promising results are reported in the literature,existing solutions suffer from watermark removal attacks, such as modelfine-tuning and model pruning.In this paper, we propose a novel DNN watermarking solution that caneffectively defend against the above attacks. Our key insight is to enhance thecoupling of the watermark and model functionalities such that removing thewatermark would inevitably degrade the model's performance on normal inputs. Tothis end, unlike previous methods relying on secret features learnt fromout-of-distribution data, our method only uses features learnt fromin-distribution data. Specifically, on the one hand, we propose to sampleinputs from the original training dataset and fuse them as watermark triggers.On the other hand, we randomly mask model weights during training so that theinformation of our embedded watermarks spreads in the network. By doing so,model fine-tuning/pruning would not forget our function-coupled watermarks.Evaluation results on various image classification tasks show a 100% watermarkauthentication success rate under aggressive watermark removal attacks,significantly outperforming existing solutions. Code is available:", "output": "On Function-Coupled Watermarks for Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We provide open, transparent implementation and assessment of Google Brain'sdeep reinforcement learning approach to macro placement and its CircuitTraining (CT) implementation in GitHub. We implement in open source key\"blackbox\" elements of CT, and clarify discrepancies between CT and Naturepaper. New testcases on open enablements are developed and released. We assessCT alongside multiple alternative macro placers, with all evaluation flows andrelated scripts public in GitHub. Our experiments also encompass academicmixed-size placement benchmarks, as well as ablation and stability studies. Wecomment on the impact of Nature and CT, as well as directions for futureresearch.", "output": "Assessment of Reinforcement Learning for Macro Placement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning to optimize (L2O) has gained increasing popularity, which automatesthe design of optimizers by data-driven approaches. However, current L2Omethods often suffer from poor generalization performance in at least twofolds: (i) applying the L2O-learned optimizer to unseen optimizees, in terms oflowering their loss function values (optimizer generalization, or``generalizable learning of optimizers\"); and (ii) the test performance of anoptimizee (itself as a machine learning model), trained by the optimizer, interms of the accuracy over unseen data (optimizee generalization, or ``learningto generalize\"). While the optimizer generalization has been recently studied,the optimizee generalization (or learning to generalize) has not beenrigorously studied in the L2O context, which is the aim of this paper. We firsttheoretically establish an implicit connection between the local entropy andthe Hessian, and hence unify their roles in the handcrafted design ofgeneralizable optimizers as equivalent metrics of the landscape flatness ofloss functions. We then propose to incorporate these two metrics asflatness-aware regularizers into the L2O framework in order to meta-trainoptimizers to learn to generalize, and theoretically show that suchgeneralization ability can be learned during the L2O meta-training process andthen transformed to the optimizee loss function. Extensive experimentsconsistently validate the effectiveness of our proposals with substantiallyimproved generalization on multiple sophisticated L2O models and diverseoptimizees. Our code is available at:", "output": "Learning to Generalize Provably in Learning to Optimize."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Invertible Neural Networks (INN) have become established tools for thesimulation and generation of highly complex data. We propose a quantum-gatealgorithm for a Quantum Invertible Neural Network (QINN) and apply it to theLHC data of jet-associated production of a Z-boson that decays into leptons, astandard candle process for particle collider precision measurements. Wecompare the QINN's performance for different loss functions and trainingscenarios. For this task, we find that a hybrid QINN matches the performance ofa significantly larger purely classical INN in learning and generating complexdata.", "output": "Generative Invertible Quantum Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers have enabled recent attention-based Deep Learning (DL)architectures to achieve remarkable results in Computer Vision (CV) tasks.However, due to the extensive computational resources required, thesearchitectures are rarely implemented on resource-constrained platforms. Currentresearch investigates hybrid handcrafted convolution-based and attention-basedmodels for CV tasks such as image classification and object detection. In thispaper, we propose HyT-NAS, an efficient Hardware-aware Neural ArchitectureSearch (HW-NAS) including hybrid architectures targeting vision tasks on tinydevices. HyT-NAS improves state-of-the-art HW-NAS by enriching the search spaceand enhancing the search strategy as well as the performance predictors. Ourexperiments show that HyT-NAS achieves a similar hypervolume with less than ~5xtraining evaluations. Our resulting architecture outperforms MLPerf MobileNetV1by 6.3% accuracy improvement with 3.5x less number of parameters on Visual WakeWords.", "output": "HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a Multi-Armed Bandit problem in which the rewards arenon-stationary and are dependent on past actions and potentially on pastcontexts. At the heart of our method, we employ a recurrent neural network,which models these sequences. In order to balance between exploration andexploitation, we present an energy minimization term that prevents the neuralnetwork from becoming too confident in support of a certain action. This termprovably limits the gap between the maximal and minimal probabilities assignedby the network. In a diverse set of experiments, we demonstrate that our methodis at least as effective as methods suggested to solve the sub-problem ofRotting Bandits, and can solve intuitive extensions of various benchmarkproblems. We share our implementation at", "output": "Energy Regularized RNNs for Solving Non-Stationary Bandit Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Protein-protein interactions are essential in biochemical processes. Accurateprediction of the protein-protein interaction sites (PPIs) deepens ourunderstanding of biological mechanism and is crucial for new drug design.However, conventional experimental methods for PPIs prediction are costly andtime-consuming so that many computational approaches, especially ML-basedmethods, have been developed recently. Although these approaches have achievedgratifying results, there are still two limitations: (1) Most models haveexcavated some useful input features, but failed to take coevolutionaryfeatures into account, which could provide clues for inter-residuerelationships; (2) The attention-based models only allocate attention weightsfor neighboring residues, instead of doing it globally, neglecting that someresidues being far away from the target residues might also matter.We propose a coevolution-enhanced global attention neural network, asequence-based deep learning model for PPIs prediction, called CoGANPPIS. Itutilizes three layers in parallel for feature extraction: (1) Local-levelrepresentation aggregation layer, which aggregates the neighboring residues'features; (2) Global-level representation learning layer, which employs a novelcoevolution-enhanced global attention mechanism to allocate attention weightsto all the residues on the same protein sequences; (3) Coevolutionaryinformation learning layer, which applies CNN &amp; pooling to coevolutionaryinformation to obtain the coevolutionary profile representation. Then, thethree outputs are concatenated and passed into several fully connected layersfor the final prediction. Application on two benchmark datasets demonstrated astate-of-the-art performance of our model. The source code is publiclyavailable at ", "output": "CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies on transfer learning have shown that selectively fine-tuning asubset of layers or customizing different learning rates for each layer cangreatly improve robustness to out-of-distribution (OOD) data and retaingeneralization capability in the pre-trained models. However, most of thesemethods employ manually crafted heuristics or expensive hyper-parametersearches, which prevent them from scaling up to large datasets and neuralnetworks. To solve this problem, we propose Trainable Projected Gradient Method(TPGM) to automatically learn the constraint imposed for each layer for afine-grained fine-tuning regularization. This is motivated by formulatingfine-tuning as a bi-level constrained optimization problem. Specifically, TPGMmaintains a set of projection radii, i.e., distance constraints between thefine-tuned model and the pre-trained model, for each layer, and enforces themthrough weight projections. To learn the constraints, we propose a bi-leveloptimization to automatically learn the best set of projection radii in anend-to-end manner. Theoretically, we show that the bi-level optimizationformulation could explain the regularization capability of TPGM. Empirically,with little hyper-parameter search cost, TPGM outperforms existing fine-tuningmethods in OOD performance while matching the best in-distribution (ID)performance. For example, when fine-tuned on DomainNet-Real and ImageNet,compared to vanilla fine-tuning, TPGM shows $22%$ and $10%$ relative OODimprovement respectively on their sketch counterparts. Code is available aturl{", "output": "Trainable Projected Gradient Method for Robust Fine-tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The study of market equilibria is central to economic theory, particularly inefficiently allocating scarce resources. However, the computation ofequilibrium prices at which the supply of goods matches their demand typicallyrelies on having access to complete information on private attributes ofagents, e.g., suppliers' cost functions, which are often unavailable inpractice. Motivated by this practical consideration, we consider the problem ofsetting equilibrium prices in the incomplete information setting wherein amarket operator seeks to satisfy the customer demand for a commodity bypurchasing the required amount from competing suppliers with privately knowncost functions unknown to the market operator. In this incomplete informationsetting, we consider the online learning problem of learning equilibrium pricesover time while jointly optimizing three performance metrics -- unmet demand,cost regret, and payment regret -- pertinent in the context of equilibriumpricing over a horizon of $T$ periods. We first consider the setting whensuppliers' cost functions are fixed and develop algorithms that achieve aregret of $O(log log T)$ when the customer demand is constant over time, or$O(sqrt{T} log log T)$ when the demand is variable over time. Next, weconsider the setting when the suppliers' cost functions can vary over time andillustrate that no online algorithm can achieve sublinear regret on all threemetrics when the market operator has no information about how the costfunctions change over time. Thus, we consider an augmented setting wherein theoperator has access to hints/contexts that, without revealing the completespecification of the cost functions, reflect the variation in the costfunctions over time and propose an algorithm with sublinear regret in thisaugmented setting.", "output": "Online Learning for Equilibrium Pricing in Markets under Incomplete Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prompt tuning is a parameter-efficient method, which freezes all PLMparameters and only prepends some additional tunable tokens called soft promptsto the input text. However, soft prompts heavily rely on a betterinitialization and may easily result in overfitting under few-shot settings,which causes prompt-tuning performing much worse than fine-tuning. To addressthe above issues, this paper proposes a novel Self-sUpervised Meta-promptlearning framework with MEtagradient Regularization for few shot generalization(SUMMER). We leverage self-supervised meta-learning to better initialize softprompts and curriculum-based task augmentation is further proposed to enrichthe meta-task distribution. Besides, a novel meta-gradient regularizationmethod is integrated into the meta-prompt learning framework, which meta-learnsto transform the raw gradient during few-shot learning into adomain-generalizable direction, thus alleviating the problem of overfitting.Extensive experiments show that SUMMER achieves better performance fordifferent few-shot downstream tasks, and also exhibits a stronger domaingeneralization ability.", "output": "Meta-augmented Prompt Tuning for Better Few-shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Calibration of deep learning models is crucial to their trustworthiness andsafe usage, and as such, has been extensively studied in supervisedclassification models, with methods crafted to decrease miscalibration.However, there has yet to be a comprehensive study of the calibration ofvision-language models that are used for zero-shot inference, like CLIP. Wemeasure calibration across relevant variables like prompt, dataset, andarchitecture, and find that zero-shot inference with CLIP is miscalibrated.Furthermore, we propose a modified version of temperature scaling that isaligned with the common use cases of CLIP as a zero-shot inference model, andshow that a single learned temperature generalizes for each specific CLIP model(defined by a chosen pre-training dataset and architecture) across inferencedataset and prompt choice.", "output": "Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm forchecking isomorphism of graphs. It has also been observed that it underlies thedesign of several graph neural network architectures, whose capabilities andperformance can be understood in terms of the expressive power of this test.Motivated by recent developments in machine learning applications to datasetsinvolving three-dimensional objects, we study when the WL test is {emcomplete} for clouds of euclidean points represented by complete distancegraphs, i.e., when it can distinguish, up to isometry, any arbitrary suchcloud.Our main result states that the $(d-1)$-dimensional WL test is complete forpoint clouds in $d$-dimensional Euclidean space, for any $dge 2$, and thatonly three iterations of the test suffice. Our result is tight for $d = 2, 3$.We also observe that the $d$-dimensional WL test only requires one iteration toachieve completeness.", "output": "Three iterations of $(d-1)$-WL test distinguish non isometric clouds of $d$-dimensional points."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Myocardial infarction and heart failure are major cardiovascular diseasesthat affect millions of people in the US. The morbidity and mortality arehighest among patients who develop cardiogenic shock. Early recognition ofcardiogenic shock is critical. Prompt implementation of treatment measures canprevent the deleterious spiral of ischemia, low blood pressure, and reducedcardiac output due to cardiogenic shock. However, early identification ofcardiogenic shock has been challenging due to human providers' inability toprocess the enormous amount of data in the cardiac intensive care unit (ICU)and lack of an effective risk stratification tool. We developed a deeplearning-based risk stratification tool, called CShock, for patients admittedinto the cardiac ICU with acute decompensated heart failure and/or myocardialinfarction to predict onset of cardiogenic shock. To develop and validateCShock, we annotated cardiac ICU datasets with physician adjudicated outcomes.CShock achieved an area under the receiver operator characteristic curve(AUROC) of 0.820, which substantially outperformed CardShock (AUROC 0.519), awell-established risk score for cardiogenic shock prognosis. CShock wasexternally validated in an independent patient cohort and achieved an AUROC of0.800, demonstrating its generalizability in other cardiac ICUs.", "output": "A dynamic risk score for early prediction of cardiogenic shock using machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although reinforcement learning has seen tremendous success recently, thiskind of trial-and-error learning can be impractical or inefficient in complexenvironments. The use of demonstrations, on the other hand, enables agents tobenefit from expert knowledge rather than having to discover the best action totake through exploration. In this survey, we discuss the advantages of usingdemonstrations in sequential decision making, various ways to applydemonstrations in learning-based decision making paradigms (for example,reinforcement learning and planning in the learned models), and how to collectthe demonstrations in various scenarios. Additionally, we exemplify a practicalpipeline for generating and utilizing demonstrations in the recently proposedManiSkill robot learning benchmark.", "output": "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Green-Kubo (GK) method is a rigorous framework for heat transportsimulations in materials. However, it requires an accurate description of thepotential-energy surface and carefully converged statistics. Machine-learningpotentials can achieve the accuracy of first-principles simulations whileallowing to reach well beyond their simulation time and length scales at afraction of the cost. In this paper, we explain how to apply the GK approach tothe recent class of message-passing machine-learning potentials, whichiteratively consider semi-local interactions beyond the initial interactioncutoff. We derive an adapted heat flux formulation that can be implementedusing automatic differentiation without compromising computational efficiency.The approach is demonstrated and validated by calculating the thermalconductivity of zirconium dioxide across temperatures.", "output": "Heat flux for semi-local machine-learning potentials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantitative characterizations and estimations of uncertainty are offundamental importance in optimization and decision-making processes. Herein,we propose intuitive scores, which we call certainty and doubt, that can beused in both a Bayesian and frequentist framework to assess and compare thequality and uncertainty of predictions in (multi-)classification decisionmachine learning problems.", "output": "Measuring Classification Decision Certainty and Doubt."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many efforts have been made for revealing the decision-making process ofblack-box learning machines such as deep neural networks, resulting in usefullocal and global explanation methods. For local explanation, stochasticity isknown to help: a simple method, called SmoothGrad, has improved the visualquality of gradient-based attribution by adding noise to the input space andaveraging the explanations of the noisy inputs. In this paper, we extend thisidea and propose NoiseGrad that enhances both local and global explanationmethods. Specifically, NoiseGrad introduces stochasticity in the weightparameter space, such that the decision boundary is perturbed. NoiseGrad isexpected to enhance the local explanation, similarly to SmoothGrad, due to thedual relationship between the input perturbation and the decision boundaryperturbation. We evaluate NoiseGrad and its fusion with SmoothGrad --FusionGrad -- qualitatively and quantitatively with several evaluationcriteria, and show that our novel approach significantly outperforms thebaseline methods. Both NoiseGrad and FusionGrad are method-agnostic and ashandy as SmoothGrad using a simple heuristic for the choice of thehyperparameter setting without the need of finetuning.", "output": "NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given the recent impressive accomplishments of language models (LMs) for codegeneration, we explore the use of LMs as adaptive mutation and crossoveroperators for an evolutionary neural architecture search (NAS) algorithm. WhileNAS still proves too difficult a task for LMs to succeed at solely throughprompting, we find that the combination of evolutionary prompt engineering withsoft prompt-tuning, a method we term EvoPrompting, consistently finds diverseand high performing models. We first demonstrate that EvoPrompting is effectiveon the computationally efficient MNIST-1D dataset, where EvoPrompting producesconvolutional architecture variants that outperform both those designed byhuman experts and naive few-shot prompting in terms of accuracy and model size.We then apply our method to searching for graph neural networks on the CLRSAlgorithmic Reasoning Benchmark, where EvoPrompting is able to design novelarchitectures that outperform current state-of-the-art models on 21 out of 30algorithmic reasoning tasks while maintaining similar model size. EvoPromptingis successful at designing accurate and efficient neural network architecturesacross a variety of machine learning tasks, while also being general enough foreasy adaptation to other tasks beyond neural network design.", "output": "EvoPrompting: Language Models for Code-Level Neural Architecture Search."}]