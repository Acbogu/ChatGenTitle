[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation is a powerful technique for transferring knowledgefrom a pre-trained teacher model to a student model. However, the truepotential of knowledge transfer has not been fully explored. Existingapproaches primarily focus on distilling individual information orinstance-level relationships, overlooking the valuable information embedded intoken-level relationships, which may be particularly affected by the long-taileffects. To address the above limitations, we propose a novel method calledKnowledge Distillation with Token-level Relationship Graph (TRG) that leveragesthe token-wise relational knowledge to enhance the performance of knowledgedistillation. By employing TRG, the student model can effectively emulatehigher-level semantic information from the teacher model, resulting in improveddistillation results. To further enhance the learning process, we introduce atoken-wise contextual loss called contextual loss, which encourages the studentmodel to capture the inner-instance semantic contextual of the teacher model.We conduct experiments to evaluate the effectiveness of the proposed methodagainst several state-of-the-art approaches. Empirical results demonstrate thesuperiority of TRG across various visual classification tasks, including thoseinvolving imbalanced data. Our method consistently outperforms the existingbaselines, establishing a new state-of-the-art performance in the field ofknowledge distillation.", "output": "Knowledge Distillation via Token-level Relationship Graph."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Detecting duplicate patient participation in clinical trials is a majorchallenge because repeated patients can undermine the credibility and accuracyof the trial's findings and result in significant health and financial risks.Developing accurate automated speaker verification (ASV) models is crucial toverify the identity of enrolled individuals and remove duplicates, but the sizeand quality of data influence ASV performance. However, there has been limitedinvestigation into the factors that can affect ASV capabilities in clinicalenvironments. In this paper, we bridge the gap by conducting analysis of howparticipant demographic characteristics, audio quality criteria, and severitylevel of Alzheimer's disease (AD) impact the performance of ASV utilizing adataset of speech recordings from 659 participants with varying levels of AD,obtained through multiple speech tasks. Our results indicate that ASVperformance: 1) is slightly better on male speakers than on female speakers; 2)degrades for individuals who are above 70 years old; 3) is comparatively betterfor non-native English speakers than for native English speakers; 4) isnegatively affected by clinician interference, noisy background, and unclearparticipant speech; 5) tends to decrease with an increase in the severity levelof AD. Our study finds that voice biometrics raise fairness concerns as certainsubgroups exhibit different ASV performances owing to their inherent voicecharacteristics. Moreover, the performance of ASV is influenced by the qualityof speech recordings, which underscores the importance of improving the datacollection settings in clinical trials.", "output": "Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the fundamental challenges in causal inference is to estimate thecausal effect of a treatment on its outcome of interest from observationaldata. However, causal effect estimation often suffers from the impacts ofconfounding bias caused by unmeasured confounders that affect both thetreatment and the outcome. The instrumental variable (IV) approach is apowerful way to eliminate the confounding bias from latent confounders.However, the existing IV-based estimators require a nominated IV, and for aconditional IV (CIV) the corresponding conditioning set too, for causal effectestimation. This limits the application of IV-based estimators. In this paper,by leveraging the advantage of disentangled representation learning, we proposea novel method, named DVAE.CIV, for learning and disentangling therepresentations of CIV and the representations of its conditioning set forcausal effect estimations from data with latent confounders. Extensiveexperimental results on both synthetic and real-world datasets demonstrate thesuperiority of the proposed DVAE.CIV method against the existing causal effectestimators.", "output": "Learning Conditional Instrumental Variable Representation for Causal Effect Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Design activity -- constructing an artifact description satisfying givengoals and constraints -- distinguishes humanity from other animals andtraditional machines, and endowing machines with design abilities at the humanlevel or beyond has been a long-term pursuit. Though machines have alreadydemonstrated their abilities in designing new materials, proteins, and computerprograms with advanced artificial intelligence (AI) techniques, the searchspace for designing such objects is relatively small, and thus, \"Can machinesdesign like humans?\" remains an open question. To explore the boundary ofmachine design, here we present a new AI approach to automatically design acentral processing unit (CPU), the brain of a computer, and one of the world'smost intricate devices humanity have ever designed. This approach generates thecircuit logic, which is represented by a graph structure called BinarySpeculation Diagram (BSD), of the CPU design from only external input-outputobservations instead of formal program code. During the generation of BSD,Monte Carlo-based expansion and the distance of Boolean functions are used toguarantee accuracy and efficiency, respectively. By efficiently exploring asearch space of unprecedented size 10^{10^{540}}, which is the largest one ofall machine-designed objects to our best knowledge, and thus pushing the limitsof machine design, our approach generates an industrial-scale RISC-V CPU withinonly 5 hours. The taped-out CPU successfully runs the Linux operating systemand performs comparably against the human-designed Intel 80486SX CPU. Inaddition to learning the world's first CPU only from input-output observations,which may reform the semiconductor industry by significantly reducing thedesign cycle, our approach even autonomously discovers human knowledge of thevon Neumann architecture.", "output": "Pushing the Limits of Machine Design: Automated CPU Design with AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Objective: COVID-19 has spread worldwide and made a huge influence across theworld. Modeling the infectious spread situation of COVID-19 is essential tounderstand the current condition and to formulate intervention measurements.Epidemiological equations based on the SEIR model simulate disease development.The traditional parameter estimation method to solve SEIR equations could notprecisely fit real-world data due to different situations, such as socialdistancing policies and intervention strategies. Additionally, learning-basedmodels achieve outstanding fitting performance, but cannot visualizemechanisms. Methods: Thus, we propose a deep dynamic epidemiological (DDE)method that combines epidemiological equations and deep-learning advantages toobtain high accuracy and visualization. The DDE contains deep networks to fitthe effect function to simulate the ever-changing situations based on theneural ODE method in solving variants' equations, ensuring the fittingperformance of multi-level areas. Results: We introduce four SEIR variants tofit different situations in different countries and regions. We compare our DDEmethod with traditional parameter estimation methods (Nelder-Mead, BFGS,Powell, Truncated Newton Conjugate-Gradient, Neural ODE) in fitting thereal-world data in the cases of countries (the USA, Columbia, South Africa) andregions (Wuhan in China, Piedmont in Italy). Our DDE method achieves the bestMean Square Error and Pearson coefficient in all five areas. Further, comparedwith the state-of-art learning-based approaches, the DDE outperforms alltechniques, including LSTM, RNN, GRU, Random Forest, Extremely Random Trees,and Decision Tree. Conclusion: DDE presents outstanding predictive ability andvisualized display of the changes in infection rates in different regions andcountries.", "output": "Deep Dynamic Epidemiological Modelling for COVID-19 Forecasting in Multi-level Districts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpreting machine learning models remains a challenge, hindering theiradoption in clinical settings. This paper proposes leveraging LocalInterpretable Model-Agnostic Explanations (LIME) to provide interpretabledescriptions of black box classification models in high-stakes sepsisdetection. By analyzing misclassified instances, significant featurescontributing to suboptimal performance are identified. The analysis revealsregions where the classifier performs poorly, allowing the calculation of errorrates within these regions. This knowledge is crucial for cautiousdecision-making in sepsis detection and other critical applications. Theproposed approach is demonstrated using the eICU dataset, effectivelyidentifying and visualizing regions where the classifier underperforms. Byenhancing interpretability, our method promotes the adoption of machinelearning models in clinical practice, empowering informed decision-making andmitigating risks in critical scenarios.", "output": "Investigating Poor Performance Regions of Black Boxes: LIME-based Exploration in Sepsis Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We show that any randomized first-order algorithm which minimizes a$d$-dimensional, $1$-Lipschitz convex function over the unit ball must eitheruse $Omega(d^{2-delta})$ bits of memory or make $Omega(d^{1+delta/6-o(1)})$queries, for any constant $deltain (0,1)$ and when the precision $epsilon$is quasipolynomially small in $d$. Our result implies that cutting planemethods, which use $tilde{O}(d^2)$ bits of memory and $tilde{O}(d)$ queries,are Pareto-optimal among randomized first-order algorithms, and quadraticmemory is required to achieve optimal query complexity for convex optimization.", "output": "Memory-Query Tradeoffs for Randomized Convex Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Complex, long-horizon planning and its combinatorial nature pose steepchallenges for learning-based agents. Difficulties in such settings areexacerbated in low data regimes where over-fitting stifles generalization andcompounding errors hurt accuracy. In this work, we explore the use of an oftenunused source of auxiliary supervision: language. Inspired by recent advancesin transformer-based models, we train agents with an instruction predictionloss that encourages learning temporally extended representations that operateat a high level of abstraction. Concretely, we demonstrate that instructionmodeling significantly improves performance in planning environments whentraining with a limited number of demonstrations on the BabyAI and Crafterbenchmarks. In further analysis we find that instruction modeling is mostimportant for tasks that require complex reasoning, while understandablyoffering smaller gains in environments that require simple plans. More detailsand code can be found at ", "output": "Improving Long-Horizon Imitation Through Instruction Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper investigates whether current large language models exhibit biasesin logical reasoning, similar to humans. Specifically, we focus on syllogisticreasoning, a well-studied form of inference in the cognitive science of humandeduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO,originally designed for psychological experiments that assess human logicalabilities in syllogistic reasoning. The dataset consists of syllogisticinferences in both English and Japanese. We examine three types of biasesobserved in human syllogistic reasoning: belief biases, conversion errors, andatmosphere effects. Our findings demonstrate that current large language modelsstruggle more with problems involving these three types of biases.", "output": "Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce and define the novel problem of multi-distribution informationretrieval (IR) where given a query, systems need to retrieve passages fromwithin multiple collections, each drawn from a different distribution. Some ofthese collections and distributions might not be available at training time. Toevaluate methods for multi-distribution retrieval, we design three benchmarksfor this task from existing single-distribution datasets, namely, a datasetbased on question answering and two based on entity matching. We propose simplemethods for this task which allocate the fixed retrieval budget (top-kpassages) strategically across domains to prevent the known domains fromconsuming most of the budget. We show that our methods lead to an average of3.8+ and up to 8.0 points improvements in Recall@100 across the datasets andthat improvements are consistent when fine-tuning different base retrievalmodels. Our benchmarks are made publicly available.", "output": "Resources and Evaluations for Multi-Distribution Dense Information Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is increasing attention being given to how to regulate AI systems. Asgoverning bodies grapple with what values to encapsulate into regulation, weconsider the technical half of the question: To what extent can AI experts vetan AI system for adherence to regulatory requirements? We investigate thisquestion through two public sector procurement checklists, identifying what wecan do now, what we should be able to do with technical innovation in AI, andwhat requirements necessitate a more interdisciplinary approach.", "output": "Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the great success of pre-trained language models, it is still achallenge to use these models for continual learning, especially for theclass-incremental learning (CIL) setting due to catastrophic forgetting (CF).This paper reports our finding that if we formulate CIL as a continual labelgeneration problem, CF is drastically reduced and the generalizablerepresentations of pre-trained models can be better retained. We thus propose anew CIL method (VAG) that also leverages the sparsity of vocabulary to focusthe generation and creates pseudo-replay samples by using label semantics.Experimental results show that VAG outperforms baselines by a large margin.", "output": "Class-Incremental Learning based on Label Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The availability of accurate localization is critical for multi-robotexploration strategies; noisy or inconsistent localization causes failure inmeeting exploration objectives. We aim to achieve high localization accuracywith contemporary exploration map belief and vice versa without needing globallocalization information. This paper proposes a novel simultaneous explorationand localization (SEAL) approach, which uses Gaussian Processes (GP)-basedinformation fusion for maximum exploration while performing communication graphoptimization for relative localization. Both these cross-dependent objectiveswere integrated through the Rao-Blackwellization technique. Distributedlinearized convex hull optimization is used to select the next-best unexploredregion for distributed exploration. SEAL outperformed cutting-edge methods onexploration and localization performance in extensive ROS-Gazebo simulations,illustrating the practicality of the approach in real-world applications.", "output": "SEAL: Simultaneous Exploration and Localization in Multi-Robot Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autoencoders have been extensively used in the development of recent anomalydetection techniques. The premise of their application is based on the notionthat after training the autoencoder on normal training data, anomalous inputswill exhibit a significant reconstruction error. Consequently, this enables aclear differentiation between normal and anomalous samples. In practice,however, it is observed that autoencoders can generalize beyond the normalclass and achieve a small reconstruction error on some of the anomaloussamples. To improve the performance, various techniques propose additionalcomponents and more sophisticated training procedures. In this work, we proposea remarkably straightforward alternative: instead of adding neural networkcomponents, involved computations, and cumbersome training, we complement thereconstruction loss with a computationally light term that regulates the normof representations in the latent space. The simplicity of our approachminimizes the requirement for hyperparameter tuning and customization for newapplications which, paired with its permissive data modality constraint,enhances the potential for successful adoption across a broad range ofapplications. We test the method on various visual and tabular benchmarks anddemonstrate that the technique matches and frequently outperforms alternatives.We also provide a theoretical analysis and numerical simulations that helpdemonstrate the underlying process that unfolds during training and how it canhelp with anomaly detection. This mitigates the black-box nature ofautoencoder-based anomaly detection algorithms and offers an avenue for furtherinvestigation of advantages, fail cases, and potential new directions.", "output": "Targeted collapse regularized autoencoder for anomaly detection: black hole at the center."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Code contains security and functional bugs. The process of identifying andlocalizing them is difficult and relies on human labor. In this work, wepresent a novel approach (FLAG) to assist human debuggers. FLAG is based on thelexical capabilities of generative AI, specifically, Large Language Models(LLMs). Here, we input a code file then extract and regenerate each line withinthat file for self-comparison. By comparing the original code with anLLM-generated alternative, we can flag notable differences as anomalies forfurther inspection, with features such as distance from comments and LLMconfidence also aiding this classification. This reduces the inspection searchspace for the designer. Unlike other automated approaches in this area, FLAG islanguage-agnostic, can work on incomplete (and even non-compiling) code andrequires no creation of security properties, functional tests or definition ofrules. In this work, we explore the features that help LLMs in thisclassification and evaluate the performance of FLAG on known bugs. We use 121benchmarks across C, Python and Verilog; with each benchmark containing a knownsecurity or functional weakness. We conduct the experiments using two state ofthe art LLMs in OpenAI's code-davinci-002 and gpt-3.5-turbo, but our approachmay be used by other models. FLAG can identify 101 of the defects and helpsreduce the search space to 12-17% of source code.", "output": "FLAG: Finding Line Anomalies (in code) with Generative AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous agents acting in real-world environments often need to reason withunknown novelties interfering with their plan execution. Novelty is anunexpected phenomenon that can alter the core characteristics, composition, anddynamics of the environment. Novelty can occur at any time in any sufficientlycomplex environment without any prior notice or explanation. Previous studiesshow that novelty has catastrophic impact on agent performance. Intelligentagents reason with an internal model of the world to understand the intricaciesof their environment and to successfully execute their plans. The introductionof novelty into the environment usually renders their internal model inaccurateand the generated plans no longer applicable. Novelty is particularly prevalentin the real world where domain-specific and even predicted novelty-specificapproaches are used to mitigate the novelty's impact. In this work, wedemonstrate that a domain-independent AI agent designed to detect,characterize, and accommodate novelty in smaller-scope physics-based games suchas Angry Birds and Cartpole can be adapted to successfully perform and reasonwith novelty in realistic high-fidelity simulator of the military domain.", "output": "Novelty Accommodating Multi-Agent Planning in High Fidelity Simulated Open World."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rare diseases (RDs) are collectively common and affect 300 million peopleworldwide. Accurate phenotyping is critical for informing diagnosis andtreatment, but RD phenotypes are often embedded in unstructured text andtime-consuming to extract manually. While natural language processing (NLP)models can perform named entity recognition (NER) to automate extraction, amajor bottleneck is the development of a large, annotated corpus for modeltraining. Recently, prompt learning emerged as an NLP paradigm that can lead tomore generalizable results without any (zero-shot) or few labeled samples(few-shot). Despite growing interest in ChatGPT, a revolutionary large languagemodel capable of following complex human prompts and generating high-qualityresponses, none have studied its NER performance for RDs in the zero- andfew-shot settings. To this end, we engineered novel prompts aimed at extractingRD phenotypes and, to the best of our knowledge, are the first the establish abenchmark for evaluating ChatGPT's performance in these settings. We comparedits performance to the traditional fine-tuning approach and conducted anin-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted inhigher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in thezero- and few-shot settings, respectively). Despite this, ChatGPT achievedsimilar or higher accuracy for certain entities (i.e., rare diseases and signs)in the one-shot setting (F1 of 0.776 and 0.725). This suggests that withappropriate prompt engineering, ChatGPT has the potential to match oroutperform fine-tuned language models for certain entity types with just onelabeled sample. While the proliferation of large language models may provideopportunities for supporting RD diagnosis and treatment, researchers andclinicians should critically evaluate model outputs and be well-informed oftheir limitations.", "output": "Identifying and Extracting Rare Disease Phenotypes with Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How does language inform our downstream thinking? In particular, how dohumans make meaning from language -- and how can we leverage a theory oflinguistic meaning to build machines that think in more human-like ways? Inthis paper, we propose textit{rational meaning construction}, a computationalframework for language-informed thinking that combines neural models oflanguage with probabilistic models for rational inference. We frame linguisticmeaning as a context-sensitive mapping from natural language into atextit{probabilistic language of thought} (PLoT) -- a general-purpose symbolicsubstrate for probabilistic, generative world modeling. Our architectureintegrates two powerful computational tools that have not previously cometogether: we model thinking with textit{probabilistic programs}, an expressiverepresentation for flexible commonsense reasoning; and we model meaningconstruction with textit{large language models} (LLMs), which supportbroad-coverage translation from natural language utterances to code expressionsin a probabilistic programming language. We illustrate our framework in actionthrough examples covering four core domains from cognitive science:probabilistic reasoning, logical and relational reasoning, visual and physicalreasoning, and social reasoning about agents and their plans. In each, we showthat LLMs can generate context-sensitive translations that capturepragmatically-appropriate linguistic meanings, while Bayesian inference withthe generated programs supports coherent and robust commonsense reasoning. Weextend our framework to integrate cognitively-motivated symbolic modules toprovide a unified commonsense thinking interface from language. Finally, weexplore how language can drive the construction of world models themselves.", "output": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Memristive reservoirs draw inspiration from a novel class of neuromorphichardware known as nanowire networks. These systems display emergent brain-likedynamics, with optimal performance demonstrated at dynamical phase transitions.In these networks, a limited number of electrodes are available to modulatesystem dynamics, in contrast to the global controllability offered byneuromorphic hardware through random access memories. We demonstrate that thelearn-to-learn framework can effectively address this challenge in the contextof optimization. Using the framework, we successfully identify the optimalhyperparameters for the reservoir. This finding aligns with previous research,which suggests that the optimal performance of a memristive reservoir occurs atthe `edge of formation' of a conductive pathway. Furthermore, our results showthat these systems can mimic membrane potential behavior observed in spikingneurons, and may serve as an interface between spike-based and continuousprocesses.", "output": "Memristive Reservoirs Learn to Learn."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Soft object manipulation tasks in domestic scenes pose a significantchallenge for existing robotic skill learning techniques due to their complexdynamics and variable shape characteristics. Since learning new manipulationskills from human demonstration is an effective way for robot applications,developing prior knowledge of the representation and dynamics of soft objectsis necessary. In this regard, we propose a pre-trained soft object manipulationskill learning model, namely SoftGPT, that is trained using large amounts ofexploration data, consisting of a three-dimensional heterogeneous graphrepresentation and a GPT-based dynamics model. For each downstream task, agoal-oriented policy agent is trained to predict the subsequent actions, andSoftGPT generates the consequences of these actions. Integrating these twoapproaches establishes a thinking process in the robot's mind that providesrollout for facilitating policy learning. Our results demonstrate thatleveraging prior knowledge through this thinking process can efficiently learnvarious soft object manipulation skills, with the potential for direct learningfrom human demonstrations.", "output": "SoftGPT: Learn Goal-oriented Soft Object Manipulation Skills by Generative Pre-trained Heterogeneous Graph Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this technical survey, we comprehensively summarize the latestadvancements in the field of recommender systems. The objective of this studyis to provide an overview of the current state-of-the-art in the field andhighlight the latest trends in the development of recommender systems. Thestudy starts with a comprehensive summary of the main taxonomy of recommendersystems, including personalized and group recommender systems, and then delvesinto the category of knowledge-based recommender systems. In addition, thesurvey analyzes the robustness, data bias, and fairness issues in recommendersystems, summarizing the evaluation metrics used to assess the performance ofthese systems. Finally, the study provides insights into the latest trends inthe development of recommender systems and highlights the new directions forfuture research in the field.", "output": "Recent Developments in Recommender Systems: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graphs represent real-world entities and their relations in asemantically-rich structure supported by ontologies. Exploring this data withmachine learning methods often relies on knowledge graph embeddings, whichproduce latent representations of entities that preserve structural and localgraph neighbourhood properties, but sacrifice explainability. However, in taskssuch as link or relation prediction, understanding which specific featuresbetter explain a relation is crucial to support complex or criticalapplications.We propose SEEK, a novel approach for explainable representations to supportrelation prediction in knowledge graphs. It is based on identifying relevantshared semantic aspects (i.e., subgraphs) between entities and learningrepresentations for each subgraph, producing a multi-faceted and explainablerepresentation.We evaluate SEEK on two real-world highly complex relation prediction tasks:protein-protein interaction prediction and gene-disease association prediction.Our extensive analysis using established benchmarks demonstrates that SEEKachieves significantly better performance than standard learning representationmethods while identifying both sufficient and necessary explanations based onshared semantic aspects.", "output": "Explainable Representations for Relation Prediction in Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning algorithms are powerful tools for data driven tasks such asimage classification and feature detection, however their vulnerability toadversarial examples - input samples manipulated to fool the algorithm -remains a serious challenge. The integration of machine learning with quantumcomputing has the potential to yield tools offering not only better accuracyand computational efficiency, but also superior robustness against adversarialattacks. Indeed, recent work has employed quantum mechanical phenomena todefend against adversarial attacks, spurring the rapid development of the fieldof quantum adversarial machine learning (QAML) and potentially yielding a newsource of quantum advantage. Despite promising early results, there remainchallenges towards building robust real-world QAML tools. In this review wediscuss recent progress in QAML and identify key challenges. We also suggestfuture research directions which could determine the route to practicality forQAML approaches as quantum computing hardware scales up and noise levels arereduced.", "output": "Towards quantum enhanced adversarial robustness in machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vector embeddings have become ubiquitous tools for many language-relatedtasks. A leading embedding model is OpenAI's text-ada-002 which can embedapproximately 6,000 words into a 1,536-dimensional vector. While powerful,text-ada-002 is not open source and is only available via API. We trained asimple neural network to convert open-source 768-dimensional MPNet embeddingsinto text-ada-002 embeddings. We compiled a subset of 50,000 online foodreviews. We calculated MPNet and text-ada-002 embeddings for each review andtrained a simple neural network to for 75 epochs. The neural network wasdesigned to predict the corresponding text-ada-002 embedding for a given MPNETembedding. Our model achieved an average cosine similarity of 0.932 on 10,000unseen reviews in our held-out test dataset. We manually assessed the qualityof our predicted embeddings for vector search over text-ada-002-embeddedreviews. While not as good as real text-ada-002 embeddings, predictedembeddings were able to retrieve highly relevant reviews. Our final model,Vec2Vec, is lightweight (&lt;80 MB) and fast. Future steps include training aneural network with a more sophisticated architecture and a larger dataset ofpaired embeddings to achieve greater performance. The ability to convertbetween and align embedding spaces may be helpful for interoperability,limiting dependence on proprietary models, protecting data privacy, reducingcosts, and offline operations.", "output": "Vec2Vec: A Compact Neural Network Approach for Transforming Text Embeddings with High Fidelity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomaly detection plays an increasingly important role in various fields forcritical tasks such as intrusion detection in cybersecurity, financial riskdetection, and human health monitoring. A variety of anomaly detection methodshave been proposed, and a category based on the isolation forest mechanismstands out due to its simplicity, effectiveness, and efficiency, e.g., iForestis often employed as a state-of-the-art detector for real deployment. While themajority of isolation forests use the binary structure, a framework LSHiForesthas demonstrated that the multi-fork isolation tree structure can lead tobetter detection performance. However, there is no theoretical work answeringthe fundamentally and practically important question on the optimal treestructure for an isolation forest with respect to the branching factor. In thispaper, we establish a theory on isolation efficiency to answer the question anddetermine the optimal branching factor for an isolation tree. Based on thetheoretical underpinning, we design a practical optimal isolation forestOptIForest incorporating clustering based learning to hash which enables moreinformation to be learned from data for better isolation quality. The rationaleof our approach relies on a better bias-variance trade-off achieved by biasreduction in OptIForest. Extensive experiments on a series of benchmarkingdatasets for comparative and ablation studies demonstrate that our approach canefficiently and robustly achieve better detection performance in general thanthe state-of-the-arts including the deep learning based methods.", "output": "OptIForest: Optimal Isolation Forest for Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a novel deep reinforcement learning (RL) approach calledMovement Prmitive-based Planning Policy (MP3). By integrating movementprimitives (MPs) into the deep RL framework, MP3 enables the generation ofsmooth trajectories throughout the whole learning process while effectivelylearning from sparse and non-Markovian rewards. Additionally, MP3 maintains thecapability to adapt to changes in the environment during execution. Althoughmany early successes in robot RL have been achieved by combining RL with MPs,these approaches are often limited to learning single stroke-based motions,lacking the ability to adapt to task variations or adjust motions duringexecution. Building upon our previous work, which introduced an episode-basedRL method for the non-linear adaptation of MP parameters to different taskvariations, this paper extends the approach to incorporating replanningstrategies. This allows adaptation of the MP parameters throughout motionexecution, addressing the lack of online motion adaptation in stochasticdomains requiring feedback. We compared our approach against state-of-the-artdeep RL and RL with MPs methods. The results demonstrated improved performancein sophisticated, sparse reward settings and in domains requiring replanning.", "output": "MP3: Movement Primitive-Based (Re-)Planning Policy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional Neural Networks (CNNs) are widely employed to solve variousproblems, e.g., image classification. Due to their compute- and data-intensivenature, CNN accelerators have been developed as ASICs or on FPGAs. Increasingcomplexity of applications has caused resource costs and energy requirements ofthese accelerators to grow. Spiking Neural Networks (SNNs) are an emergingalternative to CNN implementations, promising higher resource and energyefficiency. The main research question addressed in this paper is whether SNNaccelerators truly meet these expectations of reduced energy requirementscompared to their CNN equivalents. For this purpose, we analyze multiple SNNhardware accelerators for FPGAs regarding performance and energy efficiency. Wepresent a novel encoding scheme of spike event queues and a novel memoryorganization technique to improve SNN energy efficiency further. Bothtechniques have been integrated into a state-of-the-art SNN architecture andevaluated for MNIST, SVHN, and CIFAR-10 datasets and corresponding networkarchitectures on two differently sized modern FPGA platforms. For small-scalebenchmarks such as MNIST, SNN designs provide rather no or little latency andenergy efficiency advantages over corresponding CNN implementations. For morecomplex benchmarks such as SVHN and CIFAR-10, the trend reverses.", "output": "To Spike or Not to Spike? A Quantitative Comparison of SNN and CNN FPGA Implementations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, we have witnessed generative retrieval increasingly gainingattention in the information retrieval (IR) field, which retrieves documents bydirectly generating their identifiers. So far, much effort has been devoted todeveloping effective generative retrieval models. There has been less attentionpaid to the robustness perspective. When a new retrieval paradigm enters intothe real-world application, it is also critical to measure theout-of-distribution (OOD) generalization, i.e., how would generative retrievalmodels generalize to new distributions. To answer this question, firstly, wedefine OOD robustness from three perspectives in retrieval problems: 1) Thequery variations; 2) The unforeseen query types; and 3) The unforeseen tasks.Based on this taxonomy, we conduct empirical studies to analyze the OODrobustness of several representative generative retrieval models against denseretrieval models. The empirical results indicate that the OOD robustness ofgenerative retrieval models requires enhancement. We hope studying the OODrobustness of generative retrieval models would be advantageous to the IRcommunity.", "output": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based onadversarial learning that adopts MFCCs as input and generates raw speechwaveforms. Benefiting the GAN model capabilities, it produces speech withhigher intelligibility than a rule-based MFCC-based speech synthesizer WORLD.We evaluated the model based on a popular intrusive objective speechintelligibility measure (STOI) and quality (NISQA score). Experimental resultsshow that our proposed system outperforms Librosa MFCC- inversion (by anincrease of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and arise of about 10% in intelligibility and about 4% in naturalness in comparisonwith conventional rule-based vocoder WORLD that used in the CycleGAN-VC family.However, WORLD needs additional data like F0. Finally, using perceptual loss indiscriminators based on STOI could improve the quality more. WebMUSHRA-basedsubjective tests also show the quality of the proposed approach.", "output": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent research in representation learning utilizes large databases ofproteins or molecules to acquire knowledge of drug and protein structuresthrough unsupervised learning techniques. These pre-trained representationshave proven to significantly enhance the accuracy of subsequent tasks, such aspredicting the affinity between drugs and target proteins. In this study, wedemonstrate that by incorporating knowledge graphs from diverse sources andmodalities into the sequences or SMILES representation, we can further enrichthe representation and achieve state-of-the-art results on establishedbenchmark datasets. We provide preprocessed and integrated data obtained from 7public sources, which encompass over 30M triples. Additionally, we makeavailable the pre-trained models based on this data, along with the reportedoutcomes of their performance on three widely-used benchmark datasets fordrug-target binding affinity prediction found in the Therapeutic Data Commons(TDC) benchmarks. Additionally, we make the source code for training models onbenchmark datasets publicly available. Our objective in releasing thesepre-trained models, accompanied by clean data for model pretraining andbenchmark results, is to encourage research in knowledge-enhancedrepresentation learning.", "output": "Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Limit order books are a fundamental and widespread market mechanism. Thispaper investigates the use of conditional generative models for order booksimulation. For developing a trading agent, this approach has drawn recentattention as an alternative to traditional backtesting due to its ability toreact to the presence of the trading agent. Using a state-of-the-art CGAN (fromColetta et al. (2022)), we explore its dependence upon input features, whichhighlights both strengths and weaknesses. To do this, we use \"adversarialattacks\" on the model's features and its mechanism. We then show how theseinsights can be used to improve the CGAN, both in terms of its realism androbustness. We finish by laying out a roadmap for future work.", "output": "Conditional Generators for Limit Order Book Environments: Explainability, Challenges, and Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of 'explainable' artificial intelligence (XAI) has produced highlycited methods that seek to make the decisions of complex machine learning (ML)methods 'understandable' to humans, for example by attributing 'importance'scores to input features. Yet, a lack of formal underpinning leaves it unclearas to what conclusions can safely be drawn from the results of a given XAImethod and has also so far hindered the theoretical verification and empiricalvalidation of XAI methods. This means that challenging non-linear problems,typically solved by deep neural networks, presently lack appropriate remedies.Here, we craft benchmark datasets for three different non-linear classificationscenarios, in which the important class-conditional features are known bydesign, serving as ground truth explanations. Using novel quantitative metrics,we benchmark the explanation performance of a wide set of XAI methods acrossthree deep learning model architectures. We show that popular XAI methods areoften unable to significantly outperform random performance baselines and edgedetection methods. Moreover, we demonstrate that explanations derived fromdifferent model architectures can be vastly different; thus, prone tomisinterpretation even under controlled conditions.", "output": "XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the modern world, we are permanently using, leveraging, interacting with,and relying upon systems of ever higher sophistication, ranging from our cars,recommender systems in e-commerce, and networks when we go online, tointegrated circuits when using our PCs and smartphones, the power grid toensure our energy supply, security-critical software when accessing our bankaccounts, and spreadsheets for financial planning and decision making. Thecomplexity of these systems coupled with our high dependency on them impliesboth a non-negligible likelihood of system failures, and a high potential thatsuch failures have significant negative effects on our everyday life. For thatreason, it is a vital requirement to keep the harm of emerging failures to aminimum, which means minimizing the system downtime as well as the cost ofsystem repair. This is where model-based diagnosis comes into play.Model-based diagnosis is a principled, domain-independent approach that canbe generally applied to troubleshoot systems of a wide variety of types,including all the ones mentioned above, and many more. It exploits andorchestrates i.a. techniques for knowledge representation, automated reasoning,heuristic problem solving, intelligent search, optimization, stochastics,statistics, decision making under uncertainty, machine learning, as well ascalculus, combinatorics and set theory to detect, localize, and fix faults inabnormally behaving systems.In this thesis, we will give an introduction to the topic of model-baseddiagnosis, point out the major challenges in the field, and discuss a selectionof approaches from our research addressing these issues.", "output": "Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past few years, due to the rapid development of machine learning(ML) models for weather forecasting, state-of-the-art ML models have shownsuperior performance compared to the European Centre for Medium-Range WeatherForecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at aspatial resolution of 0.25 degree. However, the challenge remains to performcomparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previousstudies have demonstrated the importance of mitigating the accumulation offorecast errors for effective long-term forecasts. Despite numerous efforts toreduce accumulation errors, including autoregressive multi-time step loss,using a single model is found to be insufficient to achieve optimal performancein both short and long lead times. Therefore, we present FuXi, a cascaded MLweather forecasting system that provides 15-day global forecasts with atemporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi isdeveloped using 39 years of the ECMWF ERA5 reanalysis dataset. The performanceevaluation, based on latitude-weighted root mean square error (RMSE) andanomaly correlation coefficient (ACC), demonstrates that FuXi has comparableforecast performance to ECMWF EM in 15-day forecasts, making FuXi the firstML-based weather forecasting system to accomplish this achievement.", "output": "FuXi: A cascade machine learning forecasting system for 15-day global weather forecast."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce AudioPaLM, a large language model for speech understanding andgeneration. AudioPaLM fuses text-based and speech-based language models, PaLM-2[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unifiedmultimodal architecture that can process and generate text and speech withapplications including speech recognition and speech-to-speech translation.AudioPaLM inherits the capability to preserve paralinguistic information suchas speaker identity and intonation from AudioLM and the linguistic knowledgepresent only in text large language models such as PaLM-2. We demonstrate thatinitializing AudioPaLM with the weights of a text-only large language modelimproves speech processing, successfully leveraging the larger quantity of texttraining data used in pretraining to assist with the speech tasks. Theresulting model significantly outperforms existing systems for speechtranslation tasks and has the ability to perform zero-shot speech-to-texttranslation for many languages for which input/target language combinationswere not seen in training. AudioPaLM also demonstrates features of audiolanguage models, such as transferring a voice across languages based on a shortspoken prompt. We release examples of our method at", "output": "AudioPaLM: A Large Language Model That Can Speak and Listen."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep reinforcement learning (DRL) has seen remarkable success in the controlof single robots. However, applying DRL to robot swarms presents significantchallenges. A critical challenge is non-stationarity, which occurs when two ormore robots update individual or shared policies concurrently, thereby engagingin an interdependent training process with no guarantees of convergence.Circumventing non-stationarity typically involves training the robots withglobal information about other agents' states and/or actions. In contrast, inthis paper we explore how to remove the need for global information. We poseour problem as a Partially Observable Markov Decision Process, due to theabsence of global knowledge on other agents. Using collective transport as atestbed scenario, we study two approaches to multi-agent training. In thefirst, the robots exchange no messages, and are trained to rely on implicitcommunication through push-and-pull on the object to transport. In the secondapproach, we introduce Global State Prediction (GSP), a network trained toforma a belief over the swarm as a whole and predict its future states. Weprovide a comprehensive study over four well-known deep reinforcement learningalgorithms in environments with obstacles, measuring performance as thesuccessful transport of the object to the goal within a desired time-frame.Through an ablation study, we show that including GSP boosts performance andincreases robustness when compared with methods that use global knowledge.", "output": "Decentralized Multi-Agent Reinforcement Learning with Global State Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer models have been widely adopted in various domains over the lastyears, and especially large language models have advanced the field of AIsignificantly. Due to their size, the capability of these networks hasincreased tremendously, but this has come at the cost of a significant increasein necessary compute. Quantization is one of the most effective ways to reducethe computational time and memory consumption of neural networks. Many studieshave shown, however, that modern transformer models tend to learn strongoutliers in their activations, making them difficult to quantize. To retainacceptable performance, the existence of these outliers requires activations tobe in higher bitwidth or the use of different numeric formats, extrafine-tuning, or other workarounds. We show that strong outliers are related tovery specific behavior of attention heads that try to learn a \"no-op\" or just apartial update of the residual. To achieve the exact zeros needed in theattention matrix for a no-update, the input to the softmax is pushed to belarger and larger during training, causing outliers in other parts of thenetwork. Based on these observations, we propose two simple (independent)modifications to the attention mechanism - clipped softmax and gated attention.We empirically show that models pre-trained using our methods learnsignificantly smaller outliers while maintaining and sometimes even improvingthe floating-point task performance. This enables us to quantize transformersto full INT8 quantization of the activations without any additional effort. Wedemonstrate the effectiveness of our methods on both language models (BERT,OPT) and vision transformers.", "output": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNNs) have demonstrated success in modeling relationaldata, especially for data that exhibits homophily: when a connection betweennodes tends to imply that they belong to the same class. However, while thisassumption is true in many relevant situations, there are important real-worldscenarios that violate this assumption, and this has spurred research intoimproving GNNs for these cases. In this work, we propose Evolving ComputationGraphs (ECGs), a novel method for enhancing GNNs on heterophilic datasets. Ourapproach builds on prior theoretical insights linking node degree, highhomophily, and inter vs intra-class embedding similarity by rewiring the GNNs'computation graph towards adding edges that connect nodes that are likely to bein the same class. We utilise weaker classifiers to identify these edges,ultimately improving GNN performance on non-homophilic data as a result. Weevaluate ECGs on a diverse set of recently-proposed heterophilous datasets anddemonstrate improvements over the relevant baselines. ECG presents a simple,intuitive and elegant approach for improving GNN performance on heterophilicdatasets without requiring prior domain knowledge.", "output": "Evolving Computation Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Implicit Neural Representations (INRs) have emerged as a promising method forrepresenting diverse data modalities, including 3D shapes, images, and audio.While recent research has demonstrated successful applications of INRs in imageand 3D shape compression, their potential for audio compression remains largelyunexplored. Motivated by this, we present a preliminary investigation into theuse of INRs for audio compression. Our study introduces Siamese SIREN, a novelapproach based on the popular SIREN architecture. Our experimental resultsindicate that Siamese SIREN achieves superior audio reconstruction fidelitywhile utilizing fewer network parameters compared to previous INRarchitectures.", "output": "Siamese SIREN: Audio Compression with Implicit Neural Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of quantitative trading, it is common practice to transform rawhistorical stock data into indicative signals for the market trend. Suchsignals are called alpha factors. Alphas in formula forms are moreinterpretable and thus favored by practitioners concerned with risk. Inpractice, a set of formulaic alphas is often used together for better modelingprecision, so we need to find synergistic formulaic alpha sets that work welltogether. However, most traditional alpha generators mine alphas one by oneseparately, overlooking the fact that the alphas would be combined later. Inthis paper, we propose a new alpha-mining framework that prioritizes mining asynergistic set of alphas, i.e., it directly uses the performance of thedownstream combination model to optimize the alpha generator. Our frameworkalso leverages the strong exploratory capabilities of reinforcementlearning~(RL) to better explore the vast search space of formulaic alphas. Thecontribution to the combination models' performance is assigned to be thereturn used in the RL process, driving the alpha generator to find betteralphas that improve upon the current set. Experimental evaluations onreal-world stock market data demonstrate both the effectiveness and theefficiency of our framework for stock trend forecasting. The investmentsimulation results show that our framework is able to achieve higher returnscompared to previous approaches.", "output": "Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As generative language models are deployed in ever-wider contexts, concernsabout their political values have come to the forefront with critique from allparts of the political spectrum that the models are biased and lack neutrality.However, the question of what neutrality is and whether it is desirable remainsunderexplored. In this paper, I examine neutrality through an audit of Delphi[<a href=\"/abs/2110.07574\">arXiv:2110.07574</a>], a large language model designed for crowdsourced ethics. Ianalyse how Delphi responds to politically controversial questions compared todifferent US political subgroups. I find that Delphi is poorly calibrated withrespect to confidence and exhibits a significant political skew. Based on theseresults, I examine the question of neutrality from a data-feminist lens, interms of how notions of neutrality shift power and further marginalise unheardvoices. These findings can hopefully contribute to a more reflexive debateabout the normative questions of alignment and what role we want generativemodels to play in society.", "output": "Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is an increasing interest in learning reward functions that model humanintent and human preferences. However, many frameworks use blackbox learningmethods that, while expressive, are difficult to interpret. We propose andevaluate a novel approach for learning expressive and interpretable rewardfunctions from preferences using Differentiable Decision Trees (DDTs) for bothlow- and high-dimensional state inputs. We explore and discuss the viability oflearning interpretable reward functions using DDTs by evaluating our algorithmon Cartpole, Visual Gridworld environments, and Atari games. We provideevidence that that the tree structure of our learned reward function is usefulin determining the extent to which a reward function is aligned with humanpreferences. We visualize the learned reward DDTs and find that they arecapable of learning interpretable reward functions but that the discrete natureof the trees hurts the performance of reinforcement learning at test time.However, we also show evidence that using soft outputs (averaged over all leafnodes) results in competitive performance when compared with larger capacitydeep neural network reward functions.", "output": "Can Differentiable Decision Trees Learn Interpretable Reward Functions?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advancements in reinforcement learning (RL) have demonstrated superhumanperformance in complex tasks such as Starcraft, Go, Chess etc. However,knowledge transfer from Artificial \"Experts\" to humans remain a significantchallenge. A promising avenue for such transfer would be the use of curricula.Recent methods in curricula generation focuses on training RL agentsefficiently, yet such methods rely on surrogate measures to track studentprogress, and are not suited for training robots in the real world (or moreambitiously humans). In this paper, we introduce a method named ParameterizedEnvironment Response Model (PERM) that shows promising results in training RLagents in parameterized environments. Inspired by Item Response Theory, PERMseeks to model difficulty of environments and ability of RL agents directly.Given that RL agents and humans are trained more efficiently under the \"zone ofproximal development\", our method generates a curriculum by matching thedifficulty of an environment to the current ability of the student. Inaddition, PERM can be trained offline and does not employ non-stationarymeasures of student ability, making it suitable for transfer between students.We demonstrate PERM's ability to represent the environment parameter space, andtraining with RL agents with PERM produces a strong performance indeterministic environments. Lastly, we show that our method is transferablebetween students, without any sacrifice in training quality.", "output": "Transferable Curricula through Difficulty Conditioned Generators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Artificial Intelligence (GenAI) has emerged as a powerfultechnology capable of autonomously producing highly realistic content invarious domains, such as text, images, audio, and videos. With its potentialfor positive applications in creative arts, content generation, virtualassistants, and data synthesis, GenAI has garnered significant attention andadoption. However, the increasing adoption of GenAI raises concerns about itspotential misuse for crafting convincing phishing emails, generatingdisinformation through deepfake videos, and spreading misinformation viaauthentic-looking social media posts, posing a new set of challenges and risksin the realm of cybersecurity. To combat the threats posed by GenAI, we proposeleveraging the Cyber Kill Chain (CKC) to understand the lifecycle ofcyberattacks, as a foundational model for cyber defense. This paper aims toprovide a comprehensive analysis of the risk areas introduced by the offensiveuse of GenAI techniques in each phase of the CKC framework. We also analyze thestrategies employed by threat actors and examine their utilization throughoutdifferent phases of the CKC, highlighting the implications for cyber defense.Additionally, we propose GenAI-enabled defense strategies that are bothattack-aware and adaptive. These strategies encompass various techniques suchas detection, deception, and adversarial training, among others, aiming toeffectively mitigate the risks posed by GenAI-induced cyber threats.", "output": "Impacts and Risk of Generative AI Technology on Cyber Defense."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most offline reinforcement learning (RL) algorithms return a target policymaximizing a trade-off between (1) the expected performance gain over thebehavior policy that collected the dataset, and (2) the risk stemming from theout-of-distribution-ness of the induced state-action occupancy. It follows thatthe performance of the target policy is strongly related to the performance ofthe behavior policy and, thus, the trajectory return distribution of thedataset. We show that in mixed datasets consisting of mostly low-returntrajectories and minor high-return trajectories, state-of-the-art offline RLalgorithms are overly restrained by low-return trajectories and fail to exploithigh-performing trajectories to the fullest. To overcome this issue, we showthat, in deterministic MDPs with stochastic initial states, the datasetsampling can be re-weighted to induce an artificial dataset whose behaviorpolicy has a higher return. This re-weighted sampling strategy may be combinedwith any offline RL algorithm. We further analyze that the opportunity forperformance improvement over the behavior policy correlates with thepositive-sided variance of the returns of the trajectories in the dataset. Weempirically show that while CQL, IQL, and TD3+BC achieve only a part of thispotential policy improvement, these same algorithms combined with ourreweighted sampling strategy fully exploit the dataset. Furthermore, weempirically demonstrate that, despite its theoretical limitation, the approachmay still be efficient in stochastic environments. The code is available at", "output": "Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new dataset condensation framework termed Squeeze, Recover andRelabel (SRe$^2$L) that decouples the bilevel optimization of model andsynthetic data during training, to handle varying scales of datasets, modelarchitectures and image resolutions for effective dataset condensation. Theproposed method demonstrates flexibility across diverse dataset scales andexhibits multiple advantages in terms of arbitrary resolutions of synthesizedimages, low training cost and memory consumption with high-resolution training,and the ability to scale up to arbitrary evaluation network architectures.Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1Kdatasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8%validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming allprevious state-of-the-art methods by margins of 14.5% and 32.9%, respectively.Our approach also outperforms MTT by approximately 52$times$ (ConvNet-4) and16$times$ (ResNet-18) faster in speed with less memory consumption of11.6$times$ and 6.4$times$ during data synthesis. Our code and condenseddatasets of 50, 200 IPC with 4K recovery budget are available at", "output": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Within the realm of service robotics, researchers have placed a great amountof effort into learning, understanding, and representing motions asmanipulations for task execution by robots. The task of robot learning andproblem-solving is very broad, as it integrates a variety of tasks such asobject detection, activity recognition, task/motion planning, localization,knowledge representation and retrieval, and the intertwining ofperception/vision and machine learning techniques. In this paper, we solelyfocus on knowledge representations and notably how knowledge is typicallygathered, represented, and reproduced to solve problems as done by researchersin the past decades. In accordance with the definition of knowledgerepresentations, we discuss the key distinction between such representationsand useful learning models that have extensively been introduced and studied inrecent years, such as machine learning, deep learning, probabilistic modelling,and semantic graphical structures. Along with an overview of such tools, wediscuss the problems which have existed in robot learning and how they havebeen built and used as solutions, technologies or developments (if any) whichhave contributed to solving them. Finally, we discuss key principles thatshould be considered when designing an effective knowledge representation.", "output": "A Survey of Knowledge Representation in Service Robotics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning (ML) is increasingly often used to inform high-stakesdecisions. As complex ML models (e.g., deep neural networks) are oftenconsidered black boxes, a wealth of procedures has been developed to shed lighton their inner workings and the ways in which their predictions come about,defining the field of 'explainable AI' (XAI). Saliency methods rank inputfeatures according to some measure of 'importance'. Such methods are difficultto validate since a formal definition of feature importance is, thus far,lacking. It has been demonstrated that some saliency methods can highlightfeatures that have no statistical association with the prediction target(suppressor variables). To avoid misinterpretations due to such behavior, wepropose the actual presence of such an association as a necessary condition andobjective preliminary definition for feature importance. We carefully crafted aground-truth dataset in which all statistical dependencies are well-defined andlinear, serving as a benchmark to study the problem of suppressor variables. Weevaluate common explanation methods including LRP, DTD, PatternNet,PatternAttribution, LIME, Anchors, SHAP, and permutation-based methods withrespect to our objective definition. We show that most of these methods areunable to distinguish important features from suppressors in this setting.", "output": "Scrutinizing XAI using linear ground-truth data with suppressor variables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Currently, there are no convincing proxies for the fundamentals ofcryptocurrency assets. We propose a new market-to-fundamental ratio, theprice-to-utility (PU) ratio, utilizing unique blockchain accounting methods. Wethen proxy various existing fundamental-to-market ratios by Bitcoin historicaldata and find they have little predictive power for short-term bitcoin returns.However, PU ratio effectively predicts long-term bitcoin returns thanalternative methods. Furthermore, we verify the explainability of PU ratiousing machine learning. Finally, we present an automated trading strategyadvised by the PU ratio that outperforms the conventional buy-and-hold andmarket-timing strategies. Our research contributes to explainable AI in financefrom three facets: First, our market-to-fundamental ratio is based on classicmonetary theory and the unique UTXO model of Bitcoin accounting rather than adhoc; Second, the empirical evidence testifies the buy-low and sell-highimplications of the ratio; Finally, we distribute the trading algorithms asopen-source software via Python Package Index for future research, which isexceptional in finance research.", "output": "Cryptocurrency Valuation: An Explainable AI Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Input gradients have a pivotal role in a variety of applications, includingadversarial attack algorithms for evaluating model robustness, explainable AItechniques for generating Saliency Maps, and counterfactual explanations.However, Saliency Maps generated by traditional neural networks are often noisyand provide limited insights. In this paper, we demonstrate that, on thecontrary, the Saliency Maps of 1-Lipschitz neural networks, learnt with thedual loss of an optimal transportation problem, exhibit desirable XAIproperties: They are highly concentrated on the essential parts of the imagewith low noise, significantly outperforming state-of-the-art explanationapproaches across various models and metrics. We also prove that these mapsalign unprecedentedly well with human explanations on ImageNet. To explain theparticularly beneficial properties of the Saliency Map for such models, weprove this gradient encodes both the direction of the transportation plan andthe direction towards the nearest adversarial attack. Following the gradientdown to the decision boundary is no longer considered an adversarial attack,but rather a counterfactual explanation that explicitly transports the inputfrom one class to another. Thus, Learning with such a loss jointly optimizesthe classification objective and the alignment of the gradient , i.e. theSaliency Map, to the transportation plan direction. These networks werepreviously known to be certifiably robust by design, and we demonstrate thatthey scale well for large problems and models, and are tailored forexplainability using a fast and straightforward method.", "output": "On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Articulatory features are inherently invariant to acoustic signal distortionand have been successfully incorporated into automatic speech recognition (ASR)systems designed for normal speech. Their practical application to atypicaltask domains such as elderly and disordered speech across languages is oftenlimited by the difficulty in collecting such specialist data from targetspeakers. This paper presents a cross-domain and cross-lingual A2A inversionapproach that utilizes the parallel audio and ultrasound tongue imaging (UTI)data of the 24-hour TaL corpus in A2A model pre-training before beingcross-domain and cross-lingual adapted to three datasets across two languages:the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora;and the English TORGO dysarthric speech data, to produce UTI based articulatoryfeatures. Experiments conducted on three tasks suggested incorporating thegenerated articulatory features consistently outperformed the baseline TDNN andConformer ASR systems constructed using acoustic features only by statisticallysignificant word or character error rate reductions up to 4.75%, 2.59% and2.07% absolute (14.69%, 10.64% and 22.72% relative) after data augmentation,speaker adaptation and cross system multi-pass decoding were applied.", "output": "Exploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The effectiveness of machine learning in evaluating the creditworthiness ofloan applicants has been demonstrated for a long time. However, there isconcern that the use of automated decision-making processes may result inunequal treatment of groups or individuals, potentially leading todiscriminatory outcomes. This paper seeks to address this issue by evaluatingthe effectiveness of 12 leading bias mitigation methods across 5 differentfairness metrics, as well as assessing their accuracy and potentialprofitability for financial institutions. Through our analysis, we haveidentified the challenges associated with achieving fairness while maintainingaccuracy and profitabiliy, and have highlighted both the most successful andleast successful mitigation methods. Ultimately, our research serves to bridgethe gap between experimental machine learning and its practical applications inthe finance industry.", "output": "Algorithmic decision making methods for fair credit scoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven learning based methods have recently been particularly successfulat learning robust locomotion controllers for a variety of unstructuredterrains. Prior work has shown that incorporating good locomotion priors in theform of trajectory generators (TGs) is effective at efficiently learningcomplex locomotion skills. However, defining a good, single TG astasks/environments become increasingly more complex remains a challengingproblem as it requires extensive tuning and risks reducing the effectiveness ofthe prior. In this paper, we present Evolved Environmental TrajectoryGenerators (EETG), a method that learns a diverse set of specialised locomotionpriors using Quality-Diversity algorithms while maintaining a single policywithin the Policies Modulating TG (PMTG) architecture. The results demonstratethat EETG enables a quadruped robot to successfully traverse a wide range ofenvironments, such as slopes, stairs, rough terrain, and balance beams. Ourexperiments show that learning a diverse set of specialized TG priors issignificantly (5 times) more efficient than using a single, fixed prior whendealing with a wide range of environments.", "output": "Efficient Learning of Locomotion Skills through the Discovery of Diverse Environmental Trajectory Generator Priors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Natural agents can effectively learn from multiple data sources that differin size, quality, and types of measurements. We study this heterogeneity in thecontext of offline reinforcement learning (RL) by introducing a new,practically motivated semi-supervised setting. Here, an agent has access to twosets of trajectories: labelled trajectories containing state, action and rewardtriplets at every timestep, along with unlabelled trajectories that containonly state and reward information. For this setting, we develop and study asimple meta-algorithmic pipeline that learns an inverse dynamics model on thelabelled data to obtain proxy-labels for the unlabelled data, followed by theuse of any offline RL algorithm on the true and proxy-labelled trajectories.Empirically, we find this simple pipeline to be highly successful -- on severalD4RL benchmarks~cite{fu2020d4rl}, certain offline RL algorithms can match theperformance of variants trained on a fully labelled dataset even when we labelonly 10% of trajectories which are highly suboptimal. To strengthen ourunderstanding, we perform a large-scale controlled empirical studyinvestigating the interplay of data-centric properties of the labelled andunlabelled datasets, with algorithmic design choices (e.g., choice of inversedynamics, offline RL algorithm) to identify general trends and best practicesfor training RL agents on semi-supervised offline datasets.", "output": "Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The next step for intelligent dialog agents is to escape their role as silentbystanders and become proactive. Well-defined proactive behavior may improvehuman-machine cooperation, as the agent takes a more active role duringinteraction and takes off responsibility from the user. However, proactivity isa double-edged sword because poorly executed pre-emptive actions may have adevastating effect not only on the task outcome but also on the relationshipwith the user. For designing adequate proactive dialog strategies, we propose anovel approach including both social as well as task-relevant features in thedialog. Here, the primary goal is to optimize proactive behavior so that it istask-oriented - this implies high task success and efficiency - while alsobeing socially effective by fostering user trust. Including both aspects in thereward function for training a proactive dialog agent using reinforcementlearning showed the benefit of our approach for more successful human-machinecooperation.", "output": "Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The prevalence of tree-like structures, encompassing hierarchical structuresand power law distributions, exists extensively in real-world applications,including recommendation systems, ecosystems, financial networks, socialnetworks, etc. Recently, the exploitation of hyperbolic space for tree-likenessmodeling has garnered considerable attention owing to its exponential growthvolume. Compared to the flat Euclidean space, the curved hyperbolic spaceprovides a more amenable and embeddable room, especially for datasetsexhibiting implicit tree-like architectures. However, the intricate nature ofreal-world tree-like data presents a considerable challenge, as it frequentlydisplays a heterogeneous composition of tree-like, flat, and circular regions.The direct embedding of such heterogeneous structures into a homogeneousembedding space (i.e., hyperbolic space) inevitably leads to heavy distortions.To mitigate the aforementioned shortage, this study endeavors to explore thecurvature between discrete structure and continuous learning space, aiming atencoding the message conveyed by the network topology in the learning process,thereby improving tree-likeness modeling. To the end, a curvature-awarehyperbolic graph convolutional neural network, {kappa}HGCN, is proposed, whichutilizes the curvature to guide message passing and improve long-rangepropagation. Extensive experiments on node classification and link predictiontasks verify the superiority of the proposal as it consistently outperformsvarious competitive models by a large margin.", "output": "\\{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mathematical reasoning is a fundamental aspect of human intelligence and isapplicable in various fields, including science, engineering, finance, andeveryday life. The development of artificial intelligence (AI) systems capableof solving math problems and proving theorems has garnered significant interestin the fields of machine learning and natural language processing. For example,mathematics serves as a testbed for aspects of reasoning that are challengingfor powerful deep learning models, driving new algorithmic and modelingadvances. On the other hand, recent advances in large-scale neural languagemodels have opened up new benchmarks and opportunities to use deep learning formathematical reasoning. In this survey paper, we review the key tasks,datasets, and methods at the intersection of mathematical reasoning and deeplearning over the past decade. We also evaluate existing benchmarks andmethods, and discuss future research directions in this domain.", "output": "A Survey of Deep Learning for Mathematical Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Following the traditional paradigm of convolutional neural networks (CNNs),modern CNNs manage to keep pace with more recent, for exampletransformer-based, models by not only increasing model depth and width but alsothe kernel size. This results in large amounts of learnable model parametersthat need to be handled during training. While following the convolutionalparadigm with the according spatial inductive bias, we question thesignificance of emph{learned} convolution filters. In fact, our findingsdemonstrate that many contemporary CNN architectures can achieve high testaccuracies without ever updating randomly initialized (spatial) convolutionfilters. Instead, simple linear combinations (implemented through efficient$1times 1$ convolutions) suffice to effectively recombine even random filtersinto expressive network operators. Furthermore, these combinations of randomfilters can implicitly regularize the resulting operations, mitigatingoverfitting and enhancing overall performance and robustness. Conversely,retaining the ability to learn filter updates can impair network performance.Lastly, although we only observe relatively small gains from learning $3times3$ convolutions, the learning gains increase proportionally with kernel size,owing to the non-idealities of the independent and identically distributed(textit{i.i.d.}) nature of default initialization techniques.", "output": "The Power of Linear Combinations: Learning with Random Convolutions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a new model, independent linear Markov game, for multi-agentreinforcement learning with a large state space and a large number of agents.This is a class of Markov games with independent linear function approximation,where each agent has its own function approximation for the state-action valuefunctions that are marginalized by other players' policies. We design newalgorithms for learning the Markov coarse correlated equilibria (CCE) andMarkov correlated equilibria (CE) with sample complexity bounds that only scalepolynomially with each agent's own function class complexity, thus breaking thecurse of multiagents. In contrast, existing works for Markov games withfunction approximation have sample complexity bounds scale with the size of theemph{joint action space} when specialized to the canonical tabular Markov gamesetting, which is exponentially large in the number of agents. Our algorithmsrely on two key technical innovations: (1) utilizing policy replay to tacklenon-stationarity incurred by multiple agents and the use of functionapproximation; (2) separating learning Markov equilibria and exploration in theMarkov games, which allows us to use the full-information no-regret learningoracle instead of the stronger bandit-feedback no-regret learning oracle usedin the tabular setting. Furthermore, we propose an iterative-best-response typealgorithm that can learn pure Markov Nash equilibria in independent linearMarkov potential games. In the tabular case, by adapting the policy replaymechanism for independent linear Markov games, we propose an algorithm with$widetilde{O}(epsilon^{-2})$ sample complexity to learn Markov CCE, whichimproves the state-of-the-art result $widetilde{O}(epsilon^{-3})$ inDaskalakis et al. 2022, where $epsilon$ is the desired accuracy, and alsosignificantly improves other problem parameters.", "output": "Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNNs) have shown high potential for a variety ofreal-world, challenging applications, but one of the major obstacles in GNNresearch is the lack of large-scale flexible datasets. Most existing publicdatasets for GNNs are relatively small, which limits the ability of GNNs togeneralize to unseen data. The few existing large-scale graph datasets providevery limited labeled data. This makes it difficult to determine if the GNNmodel's low accuracy for unseen data is inherently due to insufficient trainingdata or if the model failed to generalize. Additionally, datasets used to trainGNNs need to offer flexibility to enable a thorough study of the impact ofvarious factors while training GNN models.In this work, we introduce the Illinois Graph Benchmark (IGB), a researchdataset tool that the developers can use to train, scrutinize andsystematically evaluate GNN models with high fidelity. IGB includes bothhomogeneous and heterogeneous academic graphs of enormous sizes, with more than40% of their nodes labeled. Compared to the largest graph datasets publiclyavailable, the IGB provides over 162X more labeled data for deep learningpractitioners and developers to create and evaluate models with higheraccuracy. The IGB dataset is a collection of academic graphs designed to beflexible, enabling the study of various GNN architectures, embedding generationtechniques, and analyzing system performance issues for node classificationtasks. IGB is open-sourced, supports DGL and PyG frameworks, and comes withreleases of the raw text that we believe foster emerging language models andGNN research projects. An early public version of IGB is available at", "output": "IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic recognition of disordered and elderly speech remains a highlychallenging task to date due to the difficulty in collecting such data in largequantities. This paper explores a series of approaches to integrate domainadapted SSL pre-trained models into TDNN and Conformer ASR systems fordysarthric and elderly speech recognition: a) input feature fusion betweenstandard acoustic frontends and domain adapted wav2vec2.0 speechrepresentations; b) frame-level joint decoding of TDNN systems separatelytrained using standard acoustic features alone and with additional wav2vec2.0features; and c) multi-pass decoding involving the TDNN/Conformer systemoutputs to be rescored using domain adapted wav2vec2.0 models. In addition,domain adapted wav2vec2.0 representations are utilized inacoustic-to-articulatory (A2A) inversion to construct multi-modal dysarthricand elderly speech recognition systems. Experiments conducted on the UASpeechdysarthric and DementiaBank Pitt elderly speech corpora suggest TDNN andConformer ASR systems integrated domain adapted wav2vec2.0 models consistentlyoutperform the standalone wav2vec2.0 models by statistically significant WERreductions of 8.22% and 3.43% absolute (26.71% and 15.88% relative) on the twotasks respectively. The lowest published WERs of 22.56% (52.53% on very lowintelligibility, 39.09% on unseen words) and 18.17% are obtained on theUASpeech test set of 16 dysarthric speakers, and the DementiaBank Pitt test setrespectively.", "output": "Exploring Self-supervised Pre-trained ASR Models For Dysarthric and Elderly Speech Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While artificial intelligence (AI) models have achieved human or evensuperhuman performance in many well-defined applications, they still struggleto show signs of broad and flexible intelligence. The Abstraction and ReasoningCorpus (ARC), a visual intelligence benchmark introduced by Franc{c}oisChollet, aims to assess how close AI systems are to human-like cognitiveabilities. Most current approaches rely on carefully handcrafteddomain-specific program searches to brute-force solutions for the tasks presentin ARC. In this work, we propose a general learning-based framework for solvingARC. It is centered on transforming tasks from the vision to the languagedomain. This composition of language and vision allows for pre-trained modelsto be leveraged at each stage, enabling a shift from handcrafted priors towardsthe learned priors of the models. While not yet beating state-of-the-art modelson ARC, we demonstrate the potential of our approach, for instance, by solvingsome ARC tasks that have not been solved previously.", "output": "Abstract Visual Reasoning Enabled by Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world time-series datasets are often multivariate with complex dynamics.To capture this complexity, high capacity architectures like recurrent- orattention-based sequential deep learning models have become popular. However,recent work demonstrates that simple univariate linear models can outperformsuch deep learning models on several commonly used academic benchmarks.Extending them, in this paper, we investigate the capabilities of linear modelsfor time-series forecasting and present Time-Series Mixer (TSMixer), a novelarchitecture designed by stacking multi-layer perceptrons (MLPs). TSMixer isbased on mixing operations along both the time and feature dimensions toextract information efficiently. On popular academic benchmarks, thesimple-to-implement TSMixer is comparable to specialized state-of-the-artmodels that leverage the inductive biases of specific benchmarks. On thechallenging and large scale M5 benchmark, a real-world retail dataset, TSMixerdemonstrates superior performance compared to the state-of-the-artalternatives. Our results underline the importance of efficiently utilizingcross-variate and auxiliary information for improving the performance of timeseries forecasting. We present various analyses to shed light into thecapabilities of TSMixer. The design paradigms utilized in TSMixer are expectedto open new horizons for deep learning-based time series forecasting. Theimplementation is available at", "output": "TSMixer: An all-MLP Architecture for Time Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bandit learning has been an increasingly popular design choice forrecommender system. Despite the strong interest in bandit learning from thecommunity, there remains multiple bottlenecks that prevent many bandit learningapproaches from productionalization. One major bottleneck is how to test theeffectiveness of bandit algorithm with fairness and without data leakage.Different from supervised learning algorithms, bandit learning algorithmsemphasize greatly on the data collection process through their explorativenature. Such explorative behavior may induce unfair evaluation in a classic A/Btest setting. In this work, we apply upper confidence bound (UCB) to our largescale short video recommender system and present a test framework for theproduction bandit learning life-cycle with a new set of metrics. Extensiveexperiment results show that our experiment design is able to fairly evaluatethe performance of bandit learning in the recommender system.", "output": "Evaluating Online Bandit Exploration In Large-Scale Recommender System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graph reasoning (KGR) -- answering complex logical queries overlarge knowledge graphs -- represents an important artificial intelligence task,entailing a range of applications (e.g., cyber threat hunting). However,despite its surging popularity, the potential security risks of KGR are largelyunexplored, which is concerning, given the increasing use of such capability insecurity-critical domains.This work represents a solid initial step towards bridging the striking gap.We systematize the security threats to KGR according to the adversary'sobjectives, knowledge, and attack vectors. Further, we present ROAR, a newclass of attacks that instantiate a variety of such threats. Through empiricalevaluation in representative use cases (e.g., medical decision support, cyberthreat hunting, and commonsense reasoning), we demonstrate that ROAR is highlyeffective to mislead KGR to suggest pre-defined answers for target queries, yetwith negligible impact on non-target ones. Finally, we explore potentialcountermeasures against ROAR, including filtering of potentially poisoningknowledge and training with adversarially augmented queries, which leads toseveral promising research directions.", "output": "On the Security Risks of Knowledge Graph Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-modal contrastive learning in vision language pretraining (VLP) facesthe challenge of (partial) false negatives. In this paper, we study thisproblem from the perspective of Mutual Information (MI) optimization. It iscommon sense that InfoNCE loss used in contrastive learning will maximize thelower bound of MI between anchors and their positives, while we theoreticallyprove that MI involving negatives also matters when noises commonly exist.Guided by a more general lower bound form for optimization, we propose acontrastive learning strategy regulated by progressively refined cross-modalsimilarity, to more accurately optimize MI between an image/text anchor and itsnegative texts/images instead of improperly minimizing it. Our method performscompetitively on four downstream cross-modal tasks and systematically balancesthe beneficial and harmful effects of (partial) false negative samples undertheoretical guidance.", "output": "Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We would like to explore how morphemes can affect the performance of alanguage model. We trained GPT-2 and Bert model with StateMorph for bothFinnish and Russian, which is a morpheme segmenting algorithm. As a comparison,we also trained a model with BPE and Morfessor. Our preliminary result showsthat StateMorph can help the model to converge more efficiently and achieve abetter validation score.", "output": "Investigating the effect of sub-word segmentation on the performance of transformer language models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite a surge collection of XAI methods, users still struggle to obtainrequired AI explanations. Previous research suggests chatbots as dynamicsolutions, but the effective design of conversational XAI agents for practicalhuman needs remains under-explored. This paper focuses on Conversational XAIfor AI-assisted scientific writing tasks. Drawing from human linguistictheories and formative studies, we identify four design rationales:\"multifaceted\", \"controllability\", \"mix-initiative\", \"context-awaredrill-down\". We incorporate them into an interactive prototype, ConvXAI, whichfacilitates heterogeneous AI explanations for scientific writing throughdialogue. In two studies with 21 users, ConvXAI outperforms a GUI-basedbaseline on improving human-perceived understanding and writing improvement.The paper further discusses the practical human usage patterns in interactingwith ConvXAI for scientific co-writing.", "output": "ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Subject-driven text-to-image generation models create novel renditions of aninput subject based on text prompts. Existing models suffer from lengthyfine-tuning and difficulties preserving the subject fidelity. To overcome theselimitations, we introduce BLIP-Diffusion, a new subject-driven image generationmodel that supports multimodal control which consumes inputs of subject imagesand text prompts. Unlike other subject-driven generation models, BLIP-Diffusionintroduces a new multimodal encoder which is pre-trained to provide subjectrepresentation. We first pre-train the multimodal encoder following BLIP-2 toproduce visual representation aligned with the text. Then we design a subjectrepresentation learning task which enables a diffusion model to leverage suchvisual representation and generates new subject renditions. Compared withprevious methods such as DreamBooth, our model enables zero-shot subject-drivengeneration, and efficient fine-tuning for customized subject with up to 20xspeedup. We also demonstrate that BLIP-Diffusion can be flexibly combined withexisting techniques such as ControlNet and prompt-to-prompt to enable novelsubject-driven generation and editing applications. Code and models will bereleased at Projectpage at ", "output": "BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evolution of semantic segmentation has long been dominated by learningmore discriminative image representations for classifying each pixel. Despitethe prominent advancements, the priors of segmentation masks themselves, e.g.,geometric and semantic constraints, are still under-explored. In this paper, wepropose to ameliorate the semantic segmentation quality of existingdiscriminative approaches with a mask prior modeled by a recently-developeddenoising diffusion generative model. Beginning with a unified architecturethat adapts diffusion models for mask prior modeling, we focus this work on aspecific instantiation with discrete diffusion and identify a variety of keydesign choices for its successful application. Our exploratory analysisrevealed several important findings, including: (1) a simple integration ofdiffusion models into semantic segmentation is not sufficient, and apoorly-designed diffusion process might lead to degradation in segmentationperformance; (2) during the training, the object to which noise is added ismore important than the type of noise; (3) during the inference, the strictdiffusion denoising scheme may not be essential and can be relaxed to a simplerscheme that even works better. We evaluate the proposed prior modeling withseveral off-the-shelf segmentors, and our experimental results on ADE20K andCityscapes demonstrate that our approach could achieve competitivelyquantitative performance and more appealing visual quality.", "output": "Denoising Diffusion Semantic Segmentation with Mask Prior Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work studies how brain-inspired neural ensembles equipped with localHebbian plasticity can perform active inference (AIF) in order to controldynamical agents. A generative model capturing the environment dynamics islearned by a network composed of two distinct Hebbian ensembles: a posteriornetwork, which infers latent states given the observations, and a statetransition network, which predicts the next expected latent state given currentstate-action pairs. Experimental studies are conducted using the Mountain Carenvironment from the OpenAI gym suite, to study the effect of the variousHebbian network parameters on the task performance. It is shown that theproposed Hebbian AIF approach outperforms the use of Q-learning, while notrequiring any replay buffer, as in typical reinforcement learning systems.These results motivate further investigations of Hebbian learning for thedesign of AIF networks that can learn environment dynamics without the need forrevisiting past buffered experiences.", "output": "Active Inference in Hebbian Learning Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies the fair range clustering problem in which the data pointsare from different demographic groups and the goal is to pick $k$ centers withthe minimum clustering cost such that each group is at least minimallyrepresented in the centers set and no group dominates the centers set. Moreprecisely, given a set of $n$ points in a metric space $(P,d)$ where each pointbelongs to one of the $ell$ different demographics (i.e., $P = P_1 uplus P_2uplus cdots uplus P_ell$) and a set of $ell$ intervals $[alpha_1,beta_1], cdots, [alpha_ell, beta_ell]$ on desired number of centers fromeach group, the goal is to pick a set of $k$ centers $C$ with minimum$ell_p$-clustering cost (i.e., $(sum_{vin P} d(v,C)^p)^{1/p}$) such that foreach group $iin ell$, $|Ccap P_i| in [alpha_i, beta_i]$. In particular,the fair range $ell_p$-clustering captures fair range $k$-center, $k$-medianand $k$-means as its special cases. In this work, we provide efficient constantfactor approximation algorithms for fair range $ell_p$-clustering for allvalues of $pin [1,infty)$.", "output": "Approximation Algorithms for Fair Range Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deploying Large language models (LLMs) can pose hazards from harmful outputssuch as toxic or dishonest speech. Prior work has introduced tools that elicitharmful outputs in order to identify and mitigate these risks. While this is avaluable step toward securing language models, these approaches typically relyon a pre-existing classifier for undesired outputs. This limits theirapplication to situations where the type of harmful behavior is known withprecision beforehand. However, this skips a central challenge of red teaming:developing a contextual understanding of the behaviors that a model canexhibit. Furthermore, when such a classifier already exists, red teaming haslimited marginal value because the classifier could simply be used to filtertraining data or model outputs. In this work, we consider red teaming under theassumption that the adversary is working from a high-level, abstractspecification of undesired behavior. The red team is expected to refine/extendthis specification and identify methods to elicit this behavior from the model.Our red teaming framework consists of three steps: 1) Exploring the model'sbehavior in the desired context; 2) Establishing a measurement of undesiredbehavior (e.g., a classifier trained to reflect human evaluations); and 3)Exploiting the model's flaws using this measure and an established red teamingmethodology. We apply this approach to red team GPT-2 and GPT-3 models tosystematically discover classes of prompts that elicit toxic and dishoneststatements. In doing so, we also construct and release the CommonClaim datasetof 20,000 statements that have been labeled by human subjects ascommon-knowledge-true, common-knowledge-false, or neither. Code is available at CommonClaimis available at ", "output": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpreting the meaning of legal open-textured terms is a key task of legalprofessionals. An important source for this interpretation is how the term wasapplied in previous court cases. In this paper, we evaluate the performance ofGPT-4 in generating factually accurate, clear and relevant explanations ofterms in legislation. We compare the performance of a baseline setup, whereGPT-4 is directly asked to explain a legal term, to an augmented approach,where a legal information retrieval module is used to provide relevant contextto the model, in the form of sentences from case law. We found that the directapplication of GPT-4 yields explanations that appear to be of very high qualityon their surface. However, detailed analysis uncovered limitations in terms ofthe factual accuracy of the explanations. Further, we found that theaugmentation leads to improved quality, and appears to eliminate the issue ofhallucination, where models invent incorrect statements. These findings openthe door to the building of systems that can autonomously retrieve relevantsentences from case law and condense them into a useful explanation for legalscholars, educators or practicing lawyers alike.", "output": "Explaining Legal Concepts with Augmented Large Language Models (GPT-4)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have shown remarkable aptitude in codegeneration but still struggle on challenging programming tasks. Self-repair --in which the model debugs and fixes mistakes in its own code -- has recentlybecome a popular way to boost performance in these settings. However, only verylimited studies on how and when self-repair works effectively exist in theliterature, and one might wonder to what extent a model is really capable ofproviding accurate feedback on why the code is wrong when that code wasgenerated by the same model. In this paper, we analyze GPT-3.5 and GPT-4'sability to perform self-repair on APPS, a challenging dataset consisting ofdiverse coding challenges. To do so, we first establish a new evaluationstrategy dubbed pass@t that measures the pass rate of the tasks against thetotal number of tokens sampled from the model, enabling a fair comparison topurely sampling-based approaches. With this evaluation strategy, we find thatthe effectiveness of self-repair is only seen in GPT-4. We also observe thatself-repair is bottlenecked by the feedback stage; using GPT-4 to give feedbackon the programs generated by GPT-3.5 and using expert human programmers to givefeedback on the programs generated by GPT-4, we unlock significant performancegains.", "output": "Demystifying GPT Self-Repair for Code Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image restoration is a low-level visual task, and most CNN methods aredesigned as black boxes, lacking transparency and intrinsic aesthetics. Manyunsupervised approaches ignore the degradation of visible information inlow-light scenes, which will seriously affect the aggregation of complementaryinformation and also make the fusion algorithm unable to produce satisfactoryfusion results under extreme conditions. In this paper, we proposeEnlighten-anything, which is able to enhance and fuse the semantic intent ofSAM segmentation with low-light images to obtain fused images with good visualperception. The generalization ability of unsupervised learning is greatlyimproved, and experiments on LOL dataset are conducted to show that our methodimproves 3db in PSNR over baseline and 8 in SSIM. Zero-shot learning of SAMintroduces a powerful aid for unsupervised low-light enhancement. The sourcecode of Enlighten Anything can be obtained from", "output": "Enlighten Anything: When Segment Anything Model Meets Low-Light Image Enhancement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Foley sound generation aims to synthesise the background sound for multimediacontent. Previous models usually employ a large development set with labels asinput (e.g., single numbers or one-hot vector). In this work, we propose adiffusion model based system for Foley sound generation with text conditions.To alleviate the data scarcity issue, our model is initially pre-trained withlarge-scale datasets and fine-tuned to this task via transfer learning usingthe contrastive language-audio pertaining (CLAP) technique. We have observedthat the feature embedding extracted by the text encoder can significantlyaffect the performance of the generation model. Hence, we introduce a trainablelayer after the encoder to improve the text embedding produced by the encoder.In addition, we further refine the generated waveform by generating multiplecandidate audio clips simultaneously and selecting the best one, which isdetermined in terms of the similarity score between the embedding of thecandidate clips and the embedding of the target text label. Using the proposedmethod, our system ranks ${1}^{st}$ among the systems submitted to DCASEChallenge 2023 Task 7. The results of the ablation studies illustrate that theproposed techniques significantly improve sound generation performance. Thecodes for implementing the proposed system are available online.", "output": "Text-Driven Foley Sound Generation With Latent Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we focus on inferring whether the given user command is clear,ambiguous, or infeasible in the context of interactive robotic agents utilizinglarge language models (LLMs). To tackle this problem, we first present anuncertainty estimation method for LLMs to classify whether the command iscertain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the commandis classified as uncertain, we further distinguish it between ambiguous orinfeasible commands leveraging LLMs with situational aware context in azero-shot manner. For ambiguous commands, we disambiguate the command byinteracting with users via question generation with LLMs. We believe thatproper recognition of the given commands could lead to a decrease inmalfunction and undesired actions of the robot, enhancing the reliability ofinteractive robot agents. We present a dataset for robotic situationalawareness, consisting pair of high-level commands, scene descriptions, andlabels of command type (i.e., clear, ambiguous, or infeasible). We validate theproposed method on the collected dataset, pick-and-place tabletop simulation.Finally, we demonstrate the proposed approach in real-world human-robotinteraction experiments, i.e., handover scenarios.", "output": "CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rehabilitation treatment helps to heal minor sports and occupationalinjuries. In a traditional rehabilitation process, a therapist will assigncertain actions to a patient to perform in between hospital visits, and it willrely on the patient to remember actions correctly and the schedule to performthem. Unfortunately, many patients forget to perform actions or fail to recallactions in detail. As a consequence, the rehabilitation treatment is hamperedor, in the worst case, the patient may suffer from additional injury caused byperforming incorrect actions. To resolve these issues, we propose a HRNet-basedrehabilitation monitoring system, which can remind a patient when to performthe actions and display the actions for the patient to follow via the patient'ssmartphone. In addition, it helps the therapist to monitor the progress of therehabilitation for the patient. Our system consists of an iOS app and severalcomponents at the server side. The app is in charge of displaying andcollecting action videos. The server computes the similarity score between thetherapist's actions and the patient's in the videos to keep track of the numberof repetitions of each action. Theses stats will be shown to both of thepatient and therapist. The extensive experiments show that the F1-Score of thesimilarity calculation is as high as 0.9 and the soft accuracy of the number ofrepetitions is higher than 90%.", "output": "A HRNet-based Rehabilitation Monitoring System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of variational quantum algorithms is crucial for theapplication of NISQ computers. Such algorithms require short quantum circuits,which are more amenable to implementation on near-term hardware, and many suchmethods have been developed. One of particular interest is the so-called thevariational diagonalization method, which constitutes an important algorithmicsubroutine, and it can be used directly for working with data encoded inquantum states. In particular, it can be applied to discern the features ofquantum states, such as entanglement properties of a system, or in quantummachine learning algorithms. In this work, we tackle the problem of designing avery shallow quantum circuit, required in the quantum state diagonalizationtask, by utilizing reinforcement learning. To achieve this, we utilize a novelencoding method that can be used to tackle the problem of circuit depthoptimization using a reinforcement learning approach. We demonstrate that ourapproach provides a solid approximation to the diagonalization task while usinga small number of gates. The circuits proposed by the reinforcement learningmethods are shallower than the standard variational quantum statediagonalization algorithm, and thus can be used in situations where the depthof quantum circuits is limited by the hardware capabilities.", "output": "Enhancing variational quantum state diagonalization using reinforcement learning techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce AdCraft, a novel benchmark environment for the ReinforcementLearning (RL) community distinguished by its stochastic and non-stationaryproperties. The environment simulates bidding and budgeting dynamics withinSearch Engine Marketing (SEM), a digital marketing technique utilizing paidadvertising to enhance the visibility of websites on search engine resultspages (SERPs). The performance of SEM advertisement campaigns depends onseveral factors, including keyword selection, ad design, bid management, budgetadjustments, and performance monitoring. Deep RL recently emerged as apotential strategy to optimize campaign profitability within the complex anddynamic landscape of SEM but it requires substantial data, which may be costlyor infeasible to acquire in practice. Our customizable environment enablespractitioners to assess and enhance the robustness of RL algorithms pertinentto SEM bid and budget management without such costs. Through a series ofexperiments within the environment, we demonstrate the challenges imposed bysparsity and non-stationarity on agent convergence and performance. We hopethese challenges further encourage discourse and development around effectivestrategies for managing real-world uncertainties.", "output": "AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) for physical design of silicon chips in a Google2021 Nature paper stirred controversy due to poorly documented claims thatraised eyebrows and attracted critical media coverage. The Nature paperwithheld most inputs needed to produce reported results and some critical stepsin the methodology. But two separate evaluations filled in the gaps anddemonstrated that Google RL lags behind human designers, behind a well-knownalgorithm (Simulated Annealing), and also behind generally-available commercialsoftware. Crosschecked data indicate that the integrity of the Nature paper issubstantially undermined owing to errors in the conduct, analysis andreporting.", "output": "The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Obsessive-compulsive disorder (OCD) presents itself as a highly debilitatingdisorder. The disorder has common associations with the prefrontal cortex andthe glutamate receptor known as Metabotropic Glutamate Receptor 5 (mGluR5).This receptor has been observed to demonstrate higher levels of signaling frompositron emission tomography scans measured by its distribution volume ratiosin mice. Despite this evidence, studies are unable to fully verify theinvolvement of mGluR5 as more empirical data is needed. Computational modelingmethods were used as a means of validation for previous hypotheses involvingmGluR5. The inadequacies in relation to the causal factor of OCD were answeredby utilizing T1 resting-state magnetic resonance imaging (TRS-MRI) scans ofpatients suffering from schizophrenia, major depressive disorder, andobsessive-compulsive disorder. Because comorbid cases often occur within thesedisorders, cross-comparative abilities become necessary to find distinctivecharacteristics. Two-dimensional convolutional neural networks alongsideResNet50 and MobileNet models were constructed and evaluated for efficiency.Activation heatmaps of TRS-MRI scans were outputted, allowing fortranscriptomics analysis. Though, a lack of ability to predict OCD casesprevented gene expression analysis. Across all models, there was an 88.75%validation accuracy for MDD, and 82.08% validation accuracy for SZD under theframework of ResNet50 as well as novel computation. OCD yielded an accuracyrate of ~54.4%. These results provided further evidence for the p factortheorem regarding mental disorders. Future work involves the application oftransfer learning to bolster accuracy rates.", "output": "Modeling T1 Resting-State MRI Variants Using Convolutional Neural Networks in Diagnosis of OCD."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative models capable of capturing nuanced clinical features in medicalimages hold great promise for facilitating clinical data sharing, enhancingrare disease datasets, and efficiently synthesizing annotated medical images atscale. Despite their potential, assessing the quality of synthetic medicalimages remains a challenge. While modern generative models can synthesizevisually-realistic medical images, the clinical validity of these images may becalled into question. Domain-agnostic scores, such as FID score, precision, andrecall, cannot incorporate clinical knowledge and are, therefore, not suitablefor assessing clinical sensibility. Additionally, there are numerousunpredictable ways in which generative models may fail to synthesize clinicallyplausible images, making it challenging to anticipate potential failures andmanually design scores for their detection. To address these challenges, thispaper introduces a pathologist-in-the-loop framework for generatingclinically-plausible synthetic medical images. Starting with a diffusion modelpretrained using real images, our framework comprises three steps: (1)evaluating the generated images by expert pathologists to assess whether theysatisfy clinical desiderata, (2) training a reward model that predicts thepathologist feedback on new samples, and (3) incorporating expert knowledgeinto the diffusion model by using the reward model to inform a finetuningobjective. We show that human feedback significantly improves the quality ofsynthetic images in terms of fidelity, diversity, utility in downstreamapplications, and plausibility as evaluated by experts.", "output": "Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work addresses the challenge of producing chip level predictions onsatellite imagery when only label proportions at a coarser spatial geometry areavailable, typically from statistical or aggregated data from administrativedivisions (such as municipalities or communes). This kind of tabular data isusually widely available in many regions of the world and application areasand, thus, its exploitation may contribute to leverage the endemic scarcity offine grained labelled data in Earth Observation (EO). This can be framed as aLearning from Label Proportions (LLP) problem setup. LLP applied to EO data isstill an emerging field and performing comparative studies in applied scenariosremains a challenge due to the lack of standardized datasets. In this work,first, we show how simple deep learning and probabilistic methods generallyperform better than standard more complex ones, providing a surprising level offiner grained spatial detail when trained with much coarser label proportions.Second, we provide a set of benchmarking datasets enabling comparative LLPapplied to EO, providing both fine grained labels and aggregated data accordingto existing administrative divisions. Finally, we argue how this approach mightbe valuable when considering on-orbit inference and training. Source code isavailable at ", "output": "Lightweight learning from label proportions on satellite imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, the main objective is to develop an algorithm capable ofidentifying and delineating tumor regions in breast ultrasound (BUS) andmammographic images. The technique employs two advanced deep learningarchitectures, namely U-Net and pretrained SAM, for tumor segmentation. TheU-Net model is specifically designed for medical image segmentation andleverages its deep convolutional neural network framework to extract meaningfulfeatures from input images. On the other hand, the pretrained SAM architectureincorporates a mechanism to capture spatial dependencies and generatesegmentation results. Evaluation is conducted on a diverse dataset containingannotated tumor regions in BUS and mammographic images, covering both benignand malignant tumors. This dataset enables a comprehensive assessment of thealgorithm's performance across different tumor types. Results demonstrate thatthe U-Net model outperforms the pretrained SAM architecture in accuratelyidentifying and segmenting tumor regions in both BUS and mammographic images.The U-Net exhibits superior performance in challenging cases involvingirregular shapes, indistinct boundaries, and high tumor heterogeneity. Incontrast, the pretrained SAM architecture exhibits limitations in accuratelyidentifying tumor areas, particularly for malignant tumors and objects withweak boundaries or complex shapes. These findings highlight the importance ofselecting appropriate deep learning architectures tailored for medical imagesegmentation. The U-Net model showcases its potential as a robust and accuratetool for tumor detection, while the pretrained SAM architecture suggests theneed for further improvements to enhance segmentation performance.", "output": "Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the proliferation of generative models, achieving fast samplingduring inference without compromising sample diversity and quality remainschallenging. Existing models such as Denoising Diffusion Probabilistic Models(DDPM) deliver high-quality, diverse samples but are slowed by an inherentlyhigh number of iterative steps. The Denoising Diffusion Generative AdversarialNetworks (DDGAN) attempted to circumvent this limitation by integrating a GANmodel for larger jumps in the diffusion process. However, DDGAN encounteredscalability limitations when applied to large datasets. To address theselimitations, we introduce a novel approach that tackles the problem by matchingimplicit and explicit factors. More specifically, our approach involvesutilizing an implicit model to match the marginal distributions of noisy dataand the explicit conditional distribution of the forward diffusion. Thiscombination allows us to effectively match the joint denoising distributions.Unlike DDPM but similar to DDGAN, we do not enforce a parametric distributionfor the reverse step, enabling us to take large steps during inference. Similarto the DDPM but unlike DDGAN, we take advantage of the exact form of thediffusion process. We demonstrate that our proposed method obtains comparablegenerative performance to diffusion-based models and vastly superior results tomodels with a small number of sampling steps.", "output": "Semi-Implicit Denoising Diffusion Models (SIDDMs)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FFCV, a library for easy and fast machine learning model training.FFCV speeds up model training by eliminating (often subtle) data bottlenecksfrom the training process. In particular, we combine techniques such as anefficient file storage format, caching, data pre-loading, asynchronous datatransfer, and just-in-time compilation to (a) make data loading and transfersignificantly more efficient, ensuring that GPUs can reach full utilization;and (b) offload as much data processing as possible to the CPU asynchronously,freeing GPU cycles for training. Using FFCV, we train ResNet-18 and ResNet-50on the ImageNet dataset with competitive tradeoff between accuracy and trainingtime. For example, we are able to train an ImageNet ResNet-50 model to 75% inonly 20 mins on a single machine. We demonstrate FFCV's performance,ease-of-use, extensibility, and ability to adapt to resource constraintsthrough several case studies. Detailed installation instructions,documentation, and Slack support channel are available at  .", "output": "FFCV: Accelerating Training by Removing Data Bottlenecks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this technical report, we present the 1st place solution for the 2023Waymo Open Dataset Pose Estimation challenge. Due to the difficulty ofacquiring large-scale 3D human keypoint annotation, previous methods havecommonly relied on 2D image features and 2D sequential annotations for 3D humanpose estimation. In contrast, our proposed method, named LPFormer, uses onlyLiDAR as its input along with its corresponding 3D annotations. LPFormerconsists of two stages: the first stage detects the human bounding box andextracts multi-level feature representations, while the second stage employs atransformer-based network to regress the human keypoints using these features.Experimental results on the Waymo Open Dataset demonstrate the top performance,and improvements even compared to previous multi-modal solutions.", "output": "LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Direct matching of 2D keypoints in an input image to a 3D point cloud of thescene without requiring visual descriptors has garnered increased interest dueto its lower memory requirements, inherent privacy preservation, and reducedneed for expensive 3D model maintenance compared to visual descriptor-basedmethods. However, existing algorithms often compromise on performance,resulting in a significant deterioration compared to their descriptor-basedcounterparts. In this paper, we introduce DGC-GNN, a novel algorithm thatemploys a global-to-local Graph Neural Network (GNN) that progressivelyexploits geometric and color cues to represent keypoints, thereby improvingmatching robustness. Our global-to-local procedure encodes both Euclidean andangular relations at a coarse level, forming the geometric embedding to guidethe local point matching. We evaluate DGC-GNN on both indoor and outdoordatasets, demonstrating that it not only doubles the accuracy of thestate-of-the-art descriptor-free algorithm but, also, substantially narrows theperformance gap between descriptor-based and descriptor-free methods. The codeand trained models will be made publicly available.", "output": "DGC-GNN: Descriptor-free Geometric-Color Graph Neural Network for 2D-3D Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent focus in video captioning has been on designing architectures that canconsume both video and text modalities, and using large-scale video datasetswith text transcripts for pre-training, such as HowTo100M. Though theseapproaches have achieved significant improvement, the audio modality is oftenignored in video captioning. In this work, we present an audio-visualframework, which aims to fully exploit the potential of the audio modality forcaptioning. Instead of relying on text transcripts extracted via automaticspeech recognition (ASR), we argue that learning with raw audio signals can bemore beneficial, as audio has additional information including acoustic events,speaker identity, etc. Our contributions are twofold. First, we observed thatthe model overspecializes to the audio modality when pre-training with bothvideo and audio modality, since the ground truth (i.e., text transcripts) canbe solely predicted using audio. We proposed a Modality Balanced Pre-training(MBP) loss to mitigate this issue and significantly improve the performance ondownstream tasks. Second, we slice and dice different design choices of thecross-modal module, which may become an information bottleneck and generateinferior results. We proposed new local-global fusion mechanisms to improveinformation exchange across audio and video. We demonstrate significantimprovements by leveraging the audio modality on four datasets, and evenoutperform the state of the art on some metrics without relying on the textmodality as the input.", "output": "Exploring the Role of Audio in Video Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modeling the spatial radiance distribution of light rays in a scene has beenextensively explored for applications, including view synthesis. Spectrum andpolarization, the wave properties of light, are often neglected due to theirintegration into three RGB spectral bands and their non-perceptibility to humanvision. Despite this, these properties encompass substantial material andgeometric information about a scene. In this work, we propose to modelspectro-polarimetric fields, the spatial Stokes-vector distribution of anylight ray at an arbitrary wavelength. We present Neural Spectro-polarimetricFields (NeSpoF), a neural representation that models the physically-validStokes vector at given continuous variables of position, direction, andwavelength. NeSpoF manages inherently noisy raw measurements, showcases memoryefficiency, and preserves physically vital signals, factors that are crucialfor representing the high-dimensional signal of a spectro-polarimetric field.To validate NeSpoF, we introduce the first multi-viewhyperspectral-polarimetric image dataset, comprised of both synthetic andreal-world scenes. These were captured using our compacthyperspectral-polarimetric imaging system, which has been calibrated forrobustness against system imperfections. We demonstrate the capabilities ofNeSpoF on diverse scenes.", "output": "Neural Spectro-polarimetric Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D content manipulation is an important computer vision task with manyreal-world applications (e.g., product design, cartoon generation, and 3DAvatar editing). Recently proposed 3D GANs can generate diverse photorealistic3D-aware contents using Neural Radiance fields (NeRF). However, manipulation ofNeRF still remains a challenging problem since the visual quality tends todegrade after manipulation and suboptimal control handles such as 2D semanticmaps are used for manipulations. While text-guided manipulations have shownpotential in 3D editing, such approaches often lack locality. To overcome theseproblems, we propose Local Editing NeRF (LENeRF), which only requires textinputs for fine-grained and localized manipulation. Specifically, we presentthree add-on modules of LENeRF, the Latent Residual Mapper, the Attention FieldNetwork, and the Deformation Network, which are jointly used for localmanipulations of 3D features by estimating a 3D attention field. The 3Dattention field is learned in an unsupervised way, by distilling the zero-shotmask generation capability of CLIP to the 3D space with multi-view guidance. Weconduct diverse experiments and thorough evaluations both quantitatively andqualitatively.", "output": "Local 3D Editing via 3D Distillation of CLIP Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The assessment of iris uniqueness plays a crucial role in analyzing thecapabilities and limitations of iris recognition systems. Among the variousmethodologies proposed, Daugman's approach to iris uniqueness stands out as oneof the most widely accepted. According to Daugman, uniqueness refers to theiris recognition system's ability to enroll an increasing number of classeswhile maintaining a near-zero probability of collision between new and enrolledclasses. Daugman's approach involves creating distinct IrisCode templates foreach iris class within the system and evaluating the sustainable populationunder a fixed Hamming distance between codewords. In our previous work [23], weutilized Rate-Distortion Theory (as it pertains to the limits oferror-correction codes) to establish boundaries for the maximum possiblepopulation of iris classes supported by Daugman's IrisCode, given theconstraint of a fixed Hamming distance between codewords. Building upon thatresearch, we propose a novel methodology to evaluate the scalability of an irisrecognition system, while also measuring iris quality. We achieve this byemploying a sphere-packing bound for Gaussian codewords and adopting a approachsimilar to Daugman's, which utilizes relative entropy as a distance measurebetween iris classes. To demonstrate the efficacy of our methodology, weillustrate its application on two small datasets of iris images. We determinethe sustainable maximum population for each dataset based on the quality of theimages. By providing these illustrations, we aim to assist researchers incomprehending the limitations inherent in their recognition systems, dependingon the quality of their iris databases.", "output": "Uniqueness of Iris Pattern Based on AR Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rapid and accurate building damage assessments from high-resolution satelliteimagery following a natural disaster is essential to inform and optimize firstresponder efforts. However, performing such building damage assessments in anautomated manner is non-trivial due to the challenges posed by variations indisaster-specific damage, diversity in satellite imagery, and the dearth ofextensive, labeled datasets. To circumvent these issues, this paper introducesa human-in-the-loop workflow for rapidly training building damage assessmentmodels after a natural disaster. This article details a case study using thisworkflow, executed in partnership with the American Red Cross during a tornadoevent in Rolling Fork, Mississippi in March, 2023. The output from ourhuman-in-the-loop modeling process achieved a precision of 0.86 and recall of0.80 for damaged buildings when compared to ground truth data collectedpost-disaster. This workflow was implemented end-to-end in under 2 hours persatellite imagery scene, highlighting its potential for real-time deployment.", "output": "Rapid building damage assessment workflow: An implementation for the 2023 Rolling Fork, Mississippi tornado event."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Certifiably robust defenses against adversarial patches for image classifiersensure correct prediction against any changes to a constrained neighborhood ofpixels. PatchCleanser <a href=\"/abs/2108.09135\">arXiv:2108.09135</a> [cs.CV], the state-of-the-art certifieddefense, uses a double-masking strategy for robust classification. The successof this strategy relies heavily on the model's invariance to image pixelmasking. In this paper, we take a closer look at model training schemes toimprove this invariance. Instead of using Random Cutout <a href=\"/abs/1708.04552\">arXiv:1708.04552v2</a>[cs.CV] augmentations like PatchCleanser, we introduce the notion of worst-casemasking, i.e., selecting masked images which maximize classification loss.However, finding worst-case masks requires an exhaustive search, which might beprohibitively expensive to do on-the-fly during training. To solve thisproblem, we propose a two-round greedy masking strategy (Greedy Cutout) whichfinds an approximate worst-case mask location with much less compute. We showthat the models trained with our Greedy Cutout improves certified robustaccuracy over Random Cutout in PatchCleanser across a range of datasets andarchitectures. Certified robust accuracy on ImageNet with a ViT-B16-224 modelincreases from 58.1% to 62.3% against a 3% square patch applied anywhere onthe image.", "output": "Revisiting Image Classifier Training for Improved Certified Robust Defense against Adversarial Patches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The emergence of different sensors (Near-Infrared, Depth, etc.) is a remedyfor the limited application scenarios of traditional RGB camera. The RGB-Xtasks, which rely on RGB input and another type of data input to resolvespecific problems, have become a popular research topic in multimedia. Acrucial part in two-branch RGB-X deep neural networks is how to fuseinformation across modalities. Given the tremendous information inside RGB-Xnetworks, previous works typically apply naive fusion (e.g., average or maxfusion) or only focus on the feature fusion at the same scale(s). While in thispaper, we propose a novel method called RXFOOD for the fusion of featuresacross different scales within the same modality branch and from differentmodality branches simultaneously in a unified attention mechanism. An EnergyExchange Module is designed for the interaction of each feature map's energymatrix, who reflects the inter-relationship of different positions anddifferent channels inside a feature map. The RXFOOD method can be easilyincorporated to any dual-branch encoder-decoder network as a plug-in module,and help the original backbone network better focus on important positions andchannels for object of interest detection. Experimental results on RGB-NIRsalient object detection, RGB-D salient object detection, and RGBFrequencyimage manipulation detection demonstrate the clear effectiveness of theproposed RXFOOD.", "output": "RXFOOD: Plug-in RGB-X Fusion for Object of Interest Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Subject-driven image generation aims at generating images containingcustomized subjects, which has recently drawn enormous attention from theresearch community. However, the previous works cannot precisely control thebackground and position of the target subject. In this work, we aspire to fillthe void and propose two novel subject-driven sub-tasks, i.e., SubjectReplacement and Subject Addition. The new tasks are challenging in multipleaspects: replacing a subject with a customized one can change its shape,texture, and color, while adding a target subject to a designated position in aprovided scene necessitates a context-aware posture. To conquer these two noveltasks, we first manually curate a new dataset DreamEditBench containing 22different types of subjects, and 440 source images with different difficultylevels. We plan to host DreamEditBench as a platform and hire trainedevaluators for standard human evaluation. We also devise an innovative methodDreamEditor to resolve these tasks by performing iterative generation, whichenables a smooth adaptation to the customized subject. In this project, weconduct automatic and human evaluations to understand the performance ofDreamEditor and baselines on DreamEditBench. For Subject Replacement, we foundthat the existing models are sensitive to the shape and color of the originalsubject. The model failure rate will dramatically increase when the source andtarget subjects are highly different. For Subject Addition, we found that theexisting models cannot easily blend the customized subjects into the backgroundsmoothly, leading to noticeable artifacts in the generated image. We hopeDreamEditBench can become a standard platform to enable future investigationstoward building more controllable subject-driven image editing. Our projecthomepage is ", "output": "DreamEdit: Subject-driven Image Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Multimodal Learning for Earth and Environment Workshop (MultiEarth 2023)aims to harness the substantial amount of remote sensing data gathered overextensive periods for the monitoring and analysis of Earth's ecosystems'health.The subtask, Multimodal SAR-to-EO Image Translation, involves the use of robustSAR data, even under adverse weather and lighting conditions, transforming itinto high-quality, clear, and visually appealing EO data. In the context of theSAR2EO task, the presence of clouds or obstructions in EO data can potentiallypose a challenge. To address this issue, we propose the Clean CollectorAlgorithm (CCA), designed to take full advantage of this cloudless SAR data andeliminate factors that may hinder the data learning process. Subsequently, weapplied pix2pixHD for the SAR-to-EO translation and Restormer for imageenhancement. In the final evaluation, the team 'CDRL' achieved an MAE of0.07313, securing the top rank on the leaderboard.", "output": "1st Place Solution to MultiEarth 2023 Challenge on Multimodal SAR-to-EO Image Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autoencoders have been extensively used in the development of recent anomalydetection techniques. The premise of their application is based on the notionthat after training the autoencoder on normal training data, anomalous inputswill exhibit a significant reconstruction error. Consequently, this enables aclear differentiation between normal and anomalous samples. In practice,however, it is observed that autoencoders can generalize beyond the normalclass and achieve a small reconstruction error on some of the anomaloussamples. To improve the performance, various techniques propose additionalcomponents and more sophisticated training procedures. In this work, we proposea remarkably straightforward alternative: instead of adding neural networkcomponents, involved computations, and cumbersome training, we complement thereconstruction loss with a computationally light term that regulates the normof representations in the latent space. The simplicity of our approachminimizes the requirement for hyperparameter tuning and customization for newapplications which, paired with its permissive data modality constraint,enhances the potential for successful adoption across a broad range ofapplications. We test the method on various visual and tabular benchmarks anddemonstrate that the technique matches and frequently outperforms alternatives.We also provide a theoretical analysis and numerical simulations that helpdemonstrate the underlying process that unfolds during training and how it canhelp with anomaly detection. This mitigates the black-box nature ofautoencoder-based anomaly detection algorithms and offers an avenue for furtherinvestigation of advantages, fail cases, and potential new directions.", "output": "Targeted collapse regularized autoencoder for anomaly detection: black hole at the center."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum image computing draws a lot of attention due to storing andprocessing image data faster than classical. With increasing the image size,the number of connections also increases, leading to the circuit complex.Therefore, efficient quantum image representation and compression issues arestill challenging. The encoding of images for representation and compression inquantum systems is different from classical ones. In quantum, encoding ofposition is more concerned which is the major difference from the classical. Inthis paper, a novel zero-discarded state connection novel enhance quantumrepresentation (ZSCNEQR) approach is introduced to reduce complexity further bydiscarding '0' in the location representation information. In the controloperational gate, only input '1' contribute to its output thus, discarding zeromakes the proposed ZSCNEQR circuit more efficient. The proposed ZSCNEQRapproach significantly reduced the required bit for both representation andcompression. The proposed method requires 11.76% less qubits compared to therecent existing method. The results show that the proposed approach is highlyeffective for representing and compressing images compared to the two relevantexisting methods in terms of rate-distortion performance.", "output": "Efficient quantum image representation and compression circuit using zero-discarded state preparation approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual foundation models like CLIP excel in learning feature representationsfrom extensive datasets through self-supervised methods, demonstratingremarkable transfer learning and generalization capabilities. A growing numberof applications based on visual foundation models are emerging, includinginnovative solutions such as BLIP-2. These applications employ pre-trained CLIPmodels as upstream feature extractors and train various downstream modules toaccomplish diverse tasks. In situations involving system upgrades that requireupdating the upstream foundation model, it becomes essential to re-train alldownstream modules to adapt to the new foundation model, which is inflexibleand inefficient. In this paper, we introduce a parameter-efficient andtask-agnostic adapter, dubbed TaCA, that facilitates compatibility acrossdistinct foundation models while ensuring enhanced performance for the newmodels. TaCA allows downstream applications to seamlessly integratebetter-performing foundation models without necessitating retraining. Weconduct extensive experimental validation of TaCA using different scales ofmodels with up to one billion parameters on various tasks such as video-textretrieval, video recognition, and visual question answering. The resultsconsistently demonstrate the emergent ability of TaCA on hot-plugging upgradesfor visual foundation models. Codes and models will be available at", "output": "TaCA: Upgrading Your Visual Foundation Model with Task-agnostic Compatible Adapter."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies the challenging continual learning (CL) setting of ClassIncremental Learning (CIL). CIL learns a sequence of tasks consisting ofdisjoint sets of concepts or classes. At any time, a single model is built thatcan be applied to predict/classify test instances of any classes learned thusfar without providing any task related information for each test instance.Although many techniques have been proposed for CIL, they are mostly empirical.It has been shown recently that a strong CIL system needs a strong within-taskprediction (WP) and a strong out-of-distribution (OOD) detection for each task.However, it is still not known whether CIL is actually learnable. This papershows that CIL is learnable. Based on the theory, a new CIL algorithm is alsoproposed. Experimental results demonstrate its effectiveness.", "output": "Learnability and Algorithm for Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pancreas segmentation is challenging due to the small proportion and highlychangeable anatomical structure. It motivates us to propose a novelsegmentation framework, namely Curriculum Knowledge Switching (CKS) framework,which decomposes detecting pancreas into three phases with different difficultyextent: straightforward, difficult, and challenging. The framework switchesfrom straightforward to challenging phases and thereby gradually learns todetect pancreas. In addition, we adopt the momentum update parameter updatingmechanism during switching, ensuring the loss converges gradually when theinput dataset changes. Experimental results show that different neural networkbackbones with the CKS framework achieved state-of-the-art performance on theNIH dataset as measured by the DSC metric.", "output": "Curriculum Knowledge Switching for Pancreas Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hand tracking is an important aspect of human-computer interaction and has awide range of applications in extended reality devices. However, current handmotion capture methods suffer from various limitations. For instance,visual-based hand pose estimation is susceptible to self-occlusion and changesin lighting conditions, while IMU-based tracking gloves experience significantdrift and are not resistant to external magnetic field interference. To addressthese issues, we propose a novel and low-cost hand-tracking glove that utilizesseveral MEMS-ultrasonic sensors attached to the fingers, to measure thedistance matrix among the sensors. Our lightweight deep network thenreconstructs the hand pose from the distance matrix. Our experimental resultsdemonstrate that this approach is both accurate, size-agnostic, and robust toexternal interference. We also show the design logic for the sensor selection,sensor configurations, circuit diagram, as well as model architecture.", "output": "Hand Pose Estimation with Mems-Ultrasonic Sensors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks employ spurious correlations in their predictions, resultingin decreased performance when these correlations do not hold. Recent workssuggest fixing pretrained representations and training a classification headthat does not use spurious features. We investigate how spurious features arerepresented in pretrained representations and explore strategies for removinginformation about spurious features. Considering the Waterbirds dataset and afew pretrained representations, we find that even with full knowledge ofspurious features, their removal is not straightforward due to entangledrepresentation. To address this, we propose a linear autoencoder trainingmethod to separate the representation into core, spurious, and other features.We propose two effective spurious feature removal approaches that are appliedto the encoding and significantly improve classification performance measuredby worst group accuracy.", "output": "Identifying and Disentangling Spurious Features in Pretrained Image Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have explored the fundamental role of depth estimation inmulti-view stereo (MVS) and semantic scene completion (SSC). They generallyconstruct 3D cost volumes to explore geometric correspondence in depth, andestimate such volumes in a single step relying directly on the ground truthapproximation. However, such problem cannot be thoroughly handled in one stepdue to complex empirical distributions, especially in challenging regions likeocclusions, reflections, etc. In this paper, we formulate the depth estimationtask as a multi-step distribution approximation process, and introduce a newparadigm of modeling the Volumetric Probability Distribution progressively(step-by-step) following a Markov chain with Diffusion models (VPDD).Specifically, to constrain the multi-step generation of volume in VPDD, weconstruct a meta volume guidance and a confidence-aware contextual guidance asconditional geometry priors to facilitate the distribution approximation. Forthe sampling process, we further investigate an online filtering strategy tomaintain consistency in volume representations for stable training. Experimentsdemonstrate that our plug-and-play VPDD outperforms the state-of-the-arts fortasks of MVS and SSC, and can also be easily extended to different baselines toget improvement. It is worth mentioning that we are the first camera-based workthat surpasses LiDAR-based methods on the SemanticKITTI dataset.", "output": "One at A Time: Multi-step Volumetric Probability Distribution Diffusion for Depth Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transfer-based attacks generate adversarial examples on the surrogate model,which can mislead other black-box models without any access, making itpromising to attack real-world applications. Recently, several works have beenproposed to boost adversarial transferability, in which the surrogate model isusually overlooked. In this work, we identify that non-linear layers (e.g.,ReLU, max-pooling, etc.) truncate the gradient during backward propagation,making the gradient w.r.t.input image imprecise to the loss function. Wehypothesize and empirically validate that such truncation undermines thetransferability of adversarial examples. Based on these findings, we propose anovel method called Backward Propagation Attack (BPA) to increase the relevancebetween the gradient w.r.t. input image and loss function so as to generateadversarial examples with higher transferability. Specifically, BPA adopts anon-monotonic function as the derivative of ReLU and incorporates softmax withtemperature to smooth the derivative of max-pooling, thereby mitigating theinformation loss during the backward propagation of gradients. Empiricalresults on the ImageNet dataset demonstrate that not only does our methodsubstantially boost the adversarial transferability, but it also is general toexisting transfer-based attacks.", "output": "Rethinking the Backward Propagation for Adversarial Transferability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes a novel face-swapping framework FlowFace++, utilizingexplicit semantic flow supervision and end-to-end architecture to facilitateshape-aware face-swapping. Specifically, our work pretrains a facial shapediscriminator to supervise the face swapping network. The discriminator isshape-aware and relies on a semantic flow-guided operation to explicitlycalculate the shape discrepancies between the target and source faces, thusoptimizing the face swapping network to generate highly realistic results. Theface swapping network is a stack of a pre-trained face-masked autoencoder(MAE), a cross-attention fusion module, and a convolutional decoder. The MAEprovides a fine-grained facial image representation space, which is unified forthe target and source faces and thus facilitates final realistic results. Thecross-attention fusion module carries out the source-to-target face swapping ina fine-grained latent space while preserving other attributes of the targetimage (e.g. expression, head pose, hair, background, illumination, etc).Lastly, the convolutional decoder further synthesizes the swapping resultsaccording to the face-swapping latent embedding from the cross-attention fusionmodule. Extensive quantitative and qualitative experiments on in-the-wild facesdemonstrate that our FlowFace++ outperforms the state-of-the-art significantly,particularly while the source face is obstructed by uneven lighting or angleoffset.", "output": "FlowFace++: Explicit Semantic Flow-supervised End-to-End Face Swapping."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, foundation models have been introduced demonstrating various tasksin the field of computer vision. These models such as Segment Anything Model(SAM) are generalized models trained using huge datasets. Currently, ongoingresearch focuses on exploring the effective utilization of these generalizedmodels for specific domains, such as medical imaging. However, in medicalimaging, the lack of training samples due to privacy concerns and other factorspresents a major challenge for applying these generalized models to medicalimage segmentation task. To address this issue, the effective fine tuning ofthese models is crucial to ensure their optimal utilization. In this study, wepropose to combine a complementary Convolutional Neural Network (CNN) alongwith the standard SAM network for medical image segmentation. To reduce theburden of fine tuning large foundation model and implement cost-efficienttrainnig scheme, we focus only on fine-tuning the additional CNN network andSAM decoder part. This strategy significantly reduces trainnig time andachieves competitive results on publicly available dataset. The code isavailable at ", "output": "Ladder Fine-tuning approach for SAM integrating complementary network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When a JPEG image is compressed using the loss compression method with a highcompression rate, a blocking phenomenon can occur in the image, making itnecessary to restore the image to its original quality. In particular,restoring compressed images that are unrecognizable presents an innovativechallenge. Therefore, this paper aims to address the restoration of JPEG imagesthat have suffered significant loss due to maximum compression using aGAN-based net-work method. The generator in this network is based on the U-Netarchitecture and features a newly presented hourglass structure that canpreserve the charac-teristics of deep layers. Additionally, the networkincorporates two loss functions, LF Loss and HF Loss, to generate natural andhigh-performance images. HF Loss uses a pretrained VGG-16 network and isconfigured using a specific layer that best represents features, which canenhance performance for the high-frequency region. LF Loss, on the other hand,is used to handle the low-frequency region. These two loss functions facilitatethe generation of images by the generator that can deceive the discriminatorwhile accurately generating both high and low-frequency regions. The resultsshow that the blocking phe-nomenon in lost compressed images was removed, andrecognizable identities were generated. This study represents a significantimprovement over previous research in terms of image restoration performance.", "output": "Restoration of the JPEG Maximum Lossy Compressed Face Images with Hourglass Block based on Early Stopping Discriminator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Editing a local region or a specific object in a 3D scene represented by aNeRF is challenging, mainly due to the implicit nature of the scenerepresentation. Consistently blending a new realistic object into the sceneadds an additional level of difficulty. We present Blended-NeRF, a robust andflexible framework for editing a specific region of interest in an existingNeRF scene, based on text prompts or image patches, along with a 3D ROI box.Our method leverages a pretrained language-image model to steer the synthesistowards a user-provided text prompt or image patch, along with a 3D MLP modelinitialized on an existing NeRF scene to generate the object and blend it intoa specified region in the original scene. We allow local editing by localizinga 3D ROI box in the input scene, and seamlessly blend the content synthesizedinside the ROI with the existing scene using a novel volumetric blendingtechnique. To obtain natural looking and view-consistent results, we leverageexisting and new geometric priors and 3D augmentations for improving the visualfidelity of the final result.We test our framework both qualitatively and quantitatively on a variety ofreal 3D scenes and text prompts, demonstrating realistic multi-view consistentresults with much flexibility and diversity compared to the baselines. Finally,we show the applicability of our framework for several 3D editing applications,including adding new objects to a scene, removing/replacing/altering existingobjects, and texture conversion.", "output": "Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D reconstruction plays an increasingly important role in modernphotogrammetric systems. Conventional satellite or aerial-based remote sensing(RS) platforms can provide the necessary data sources for the 3D reconstructionof large-scale landforms and cities. Even with low-altitude UAVs (UnmannedAerial Vehicles), 3D reconstruction in complicated situations, such as urbancanyons and indoor scenes, is challenging due to the frequent tracking failuresbetween camera frames and high data collection costs. Recently, sphericalimages have been extensively exploited due to the capability of recordingsurrounding environments from one camera exposure. Classical 3D reconstructionpipelines, however, cannot be used for spherical images. Besides, there existfew software packages for 3D reconstruction of spherical images. Based on theimaging geometry of spherical cameras, this study investigates the algorithmsfor the relative orientation using spherical correspondences, absoluteorientation using 3D correspondences between scene and spherical points, andthe cost functions for BA (bundle adjustment) optimization. In addition, anincremental SfM (Structure from Motion) workflow has been proposed forspherical images using the above-mentioned algorithms. The proposed solution isfinally verified by using three spherical datasets captured by bothconsumer-grade and professional spherical cameras. The results demonstrate thatthe proposed SfM workflow can achieve the successful 3D reconstruction ofcomplex scenes and provide useful clues for the implementation in open-sourcesoftware packages. The source code of the designed SfM workflow would be madepublicly available.", "output": "3D Reconstruction of Spherical Images based on Incremental Structure from Motion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rapid development of deep neural networks(DNNs), many robust blindwatermarking algorithms and frameworks have been proposed and achieved goodresults. At present, the watermark attack algorithm can not compete with thewatermark addition algorithm. And many watermark attack algorithms only careabout interfering with the normal extraction of the watermark, and thewatermark attack will cause great visual loss to the image. To this end, wepropose DiffWA, a conditional diffusion model with distance guidance forwatermark attack, which can restore the image while removing the embeddedwatermark. The core of our method is training an image-to-image conditionaldiffusion model on unwatermarked images and guiding the conditional model usinga distance guidance when sampling so that the model will generate unwatermarkedimages which is similar to original images. We conducted experiments onCIFAR-10 using our proposed models. The results shows that the model can removethe watermark with good effect and make the bit error rate of watermarkextraction higher than 0.4. At the same time, the attacked image will maintaingood visual effect with PSNR more than 31 and SSIM more than 0.97 compared withthe original image.", "output": "DiffWA: Diffusion Models for Watermark Attack."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal learning assumes all modality combinations of interest areavailable during training to learn cross-modal correspondences. In this paper,we challenge this modality-complete assumption for multimodal learning andinstead strive for generalization to unseen modality combinations duringinference. We pose the problem of unseen modality interaction and introduce afirst solution. It exploits a feature projection module to project themultidimensional features of different modalities into a common space with richinformation reserved. This allows the information to be accumulated with asimple summation operation across available modalities. To reduce overfittingto unreliable modality combinations during training, we further improve themodel learning with pseudo-supervision indicating the reliability of amodality's prediction. We demonstrate that our approach is effective fordiverse tasks and modalities by evaluating it for multimodal videoclassification, robot state regression, and multimedia retrieval.", "output": "Learning Unseen Modality Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Enhancing the resolution of Biogenic Volatile Organic Compound (BVOC)emission maps is a critical task in remote sensing. Recently, someSuper-Resolution (SR) methods based on Deep Learning (DL) have been proposed,leveraging data from numerical simulations for their training process. However,when dealing with data derived from satellite observations, the reconstructionis particularly challenging due to the scarcity of measurements to train SRalgorithms with. In our work, we aim at super-resolving low resolution emissionmaps derived from satellite observations by leveraging the information ofemission maps obtained through numerical simulations. To do this, we combine aSR method based on DL with Domain Adaptation (DA) techniques, harmonizing thedifferent aggregation strategies and spatial information used in simulated andobserved domains to ensure compatibility. We investigate the effectiveness ofDA strategies at different stages by systematically varying the number ofsimulated and observed emissions used, exploring the implications of datascarcity on the adaptation strategies. To the best of our knowledge, there areno prior investigations of DA in satellite-derived BVOC maps enhancement. Ourwork represents a first step toward the development of robust strategies forthe reconstruction of observed BVOC emissions.", "output": "Super-Resolution of BVOC Emission Maps Via Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of 'explainable' artificial intelligence (XAI) has produced highlycited methods that seek to make the decisions of complex machine learning (ML)methods 'understandable' to humans, for example by attributing 'importance'scores to input features. Yet, a lack of formal underpinning leaves it unclearas to what conclusions can safely be drawn from the results of a given XAImethod and has also so far hindered the theoretical verification and empiricalvalidation of XAI methods. This means that challenging non-linear problems,typically solved by deep neural networks, presently lack appropriate remedies.Here, we craft benchmark datasets for three different non-linear classificationscenarios, in which the important class-conditional features are known bydesign, serving as ground truth explanations. Using novel quantitative metrics,we benchmark the explanation performance of a wide set of XAI methods acrossthree deep learning model architectures. We show that popular XAI methods areoften unable to significantly outperform random performance baselines and edgedetection methods. Moreover, we demonstrate that explanations derived fromdifferent model architectures can be vastly different; thus, prone tomisinterpretation even under controlled conditions.", "output": "XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning from visual observation (LfVO), aiming at recovering policies fromonly visual observation data, is promising yet a challenging problem. ExistingLfVO approaches either only adopt inefficient online learning schemes orrequire additional task-specific information like goal states, making them notsuited for open-ended tasks. To address these issues, we propose a two-stageframework for learning from visual observation. In the first stage, weintroduce and pretrain State-to-Go (STG) Transformer offline to predict anddifferentiate latent transitions of demonstrations. Subsequently, in the secondstage, the STG Transformer provides intrinsic rewards for downstreamreinforcement learning tasks where an agent learns merely from intrinsicrewards. Empirical results on Atari and Minecraft show that our proposed methodoutperforms baselines and in some tasks even achieves performance comparable tothe policy learned from environmental rewards. These results shed light on thepotential of utilizing video-only data to solve difficult visual reinforcementlearning tasks rather than relying on complete offline datasets containingstates, actions, and rewards. The project's website and code can be found at", "output": "Learning from Visual Observation via Offline Pretrained State-to-Go Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model compression techniques reduce the computational load and memoryconsumption of deep neural networks. After the compression operation, e.g.parameter pruning, the model is normally fine-tuned on the original trainingdataset to recover from the performance drop caused by compression. However,the training data is not always available due to privacy issues or otherfactors. In this work, we present a data-free fine-tuning approach for pruningthe backbone of deep neural networks. In particular, the pruned networkbackbone is trained with synthetically generated images, and our proposedintermediate supervision to mimic the unpruned backbone's output feature map.Afterwards, the pruned backbone can be combined with the original network headto make predictions. We generate synthetic images by back-propagating gradientsto noise images while relying on L1-pruning for the backbone pruning. In ourexperiments, we show that our approach is task-independent due to pruning onlythe backbone. By evaluating our approach on 2D human pose estimation, objectdetection, and image classification, we demonstrate promising performancecompared to the unpruned model. Our code is available at", "output": "Data-Free Backbone Fine-Tuning for Pruned Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose a built-in Deep Learning Physics Optimization (DLPO)framework to set up a shape optimization study of the Duisburg Test Case (DTC)container vessel. We present two different applications: (1) sensitivityanalysis to detect the most promising generic basis hull shapes, and (2)multi-objective optimization to quantify the trade-off between optimal hullforms. DLPO framework allows for the evaluation of design iterationsautomatically in an end-to-end manner. We achieved these results by couplingExtrality's Deep Learning Physics (DLP) model to a CAD engine and an optimizer.Our proposed DLP model is trained on full 3D volume data coming from RANSsimulations, and it can provide accurate and high-quality 3D flow predictionsin real-time, which makes it a good evaluator to perform optimization of newcontainer vessel designs w.r.t the hydrodynamic efficiency. In particular, itis able to recover the forces acting on the vessel by integration on the hullsurface with a mean relative error of 3.84% pm 2.179% on the totalresistance. Each iteration takes only 20 seconds, thus leading to a drasticsaving of time and engineering efforts, while delivering valuable insight intothe performance of the vessel, including RANS-like detailed flow information.We conclude that DLPO framework is a promising tool to accelerate the shipdesign process and lead to more efficient ships with better hydrodynamicperformance.", "output": "Multi-Objective Hull Form Optimization with CAD Engine-based Deep Learning Physics for 3D Flow Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer models have been widely adopted in various domains over the lastyears, and especially large language models have advanced the field of AIsignificantly. Due to their size, the capability of these networks hasincreased tremendously, but this has come at the cost of a significant increasein necessary compute. Quantization is one of the most effective ways to reducethe computational time and memory consumption of neural networks. Many studieshave shown, however, that modern transformer models tend to learn strongoutliers in their activations, making them difficult to quantize. To retainacceptable performance, the existence of these outliers requires activations tobe in higher bitwidth or the use of different numeric formats, extrafine-tuning, or other workarounds. We show that strong outliers are related tovery specific behavior of attention heads that try to learn a \"no-op\" or just apartial update of the residual. To achieve the exact zeros needed in theattention matrix for a no-update, the input to the softmax is pushed to belarger and larger during training, causing outliers in other parts of thenetwork. Based on these observations, we propose two simple (independent)modifications to the attention mechanism - clipped softmax and gated attention.We empirically show that models pre-trained using our methods learnsignificantly smaller outliers while maintaining and sometimes even improvingthe floating-point task performance. This enables us to quantize transformersto full INT8 quantization of the activations without any additional effort. Wedemonstrate the effectiveness of our methods on both language models (BERT,OPT) and vision transformers.", "output": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a deep-learning-based approach to writer retrieval andidentification for papyri, with a focus on identifying fragments associatedwith a specific writer and those corresponding to the same image. We present anovel neural network architecture that combines a residual backbone with afeature mixing stage to improve retrieval performance, and the final descriptoris derived from a projection layer. The methodology is evaluated on twobenchmarks: PapyRow, where we achieve a mAP of 26.6 % and 24.9 % on writer andpage retrieval, and HisFragIR20, showing state-of-the-art performance (44.0 %and 29.3 % mAP). Furthermore, our network has an accuracy of 28.7 % for writeridentification. Additionally, we conduct experiments on the influence of twobinarization techniques on fragments and show that binarizing does not enhanceperformance. Our code and models are available to the community.", "output": "Feature Mixing for Writer Retrieval and Identification on Papyri Fragments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While a large amount of work has focused on designing adversarial attacksagainst image classifiers, only a few methods exist to attack semanticsegmentation models. We show that attacking segmentation models presentstask-specific challenges, for which we propose novel solutions. Our finalevaluation protocol outperforms existing methods, and shows that those canoverestimate the robustness of the models. Additionally, so far adversarialtraining, the most successful way for obtaining robust image classifiers, couldnot be successfully applied to semantic segmentation. We argue that this isbecause the task to be learned is more challenging, and requires significantlyhigher computational effort than for image classification. As a remedy, we showthat by taking advantage of recent advances in robust ImageNet classifiers, onecan train adversarially robust segmentation models at limited computationalcost by fine-tuning robust backbones.", "output": "Robust Semantic Segmentation: Strong Adversarial Attacks and Fast Training of Robust Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative diffusion models, including Stable Diffusion and Midjourney, cangenerate visually appealing, diverse, and high-resolution images for variousapplications. These models are trained on billions of internet-sourced images,raising significant concerns about the potential unauthorized use ofcopyright-protected images. In this paper, we examine whether it is possible todetermine if a specific image was used in the training set, a problem known inthe cybersecurity community and referred to as a membership inference attack.Our focus is on Stable Diffusion, and we address the challenge of designing afair evaluation framework to answer this membership question. We propose amethodology to establish a fair evaluation setup and apply it to StableDiffusion, enabling potential extensions to other generative models. Utilizingthis evaluation setup, we execute membership attacks (both known and newlyintroduced). Our research reveals that previously proposed evaluation setups donot provide a full understanding of the effectiveness of membership inferenceattacks. We conclude that the membership inference attack remains a significantchallenge for large diffusion models (often deployed as black-box systems),indicating that related privacy and copyright issues will persist in theforeseeable future.", "output": "Towards More Realistic Membership Inference Attacks on Large Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image segmentation and registration are said to be challenging when appliedto dynamic contrast enhanced MRI sequences (DCE-MRI). The contrast agent causesrapid changes in intensity in the region of interest and elsewhere, which canlead to false positive predictions for segmentation tasks and confound theimage registration similarity metric. While it is widely assumed that contrastchanges increase the difficulty of these tasks, to our knowledge no work hasquantified these effects. In this paper we examine the effect of training withdifferent ratios of contrast enhanced (CE) data on two popular tasks:segmentation with nnU-Net and Mask R-CNN and registration using VoxelMorph andVTN. We experimented further by strategically using the available datasetsthrough pretraining and fine tuning with different splits of data. We foundthat to create a generalisable model, pretraining with CE data and fine tuningwith non-CE data gave the best result. This interesting find could be expandedto other deep learning based image processing tasks with DCE-MRI and providesignificant improvements to the models performance.", "output": "Can a single image processing algorithm work equally well across all phases of DCE-MRI?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-quality panoramic images with a Field of View (FoV) of 360-degree areessential for contemporary panoramic computer vision tasks. However,conventional imaging systems come with sophisticated lens designs and heavyoptical components. This disqualifies their usage in many mobile and wearableapplications where thin and portable, minimalist imaging systems are desired.In this paper, we propose a Panoramic Computational Imaging Engine (PCIE) toaddress minimalist and high-quality panoramic imaging. With less than threespherical lenses, a Minimalist Panoramic Imaging Prototype (MPIP) isconstructed based on the design of the Panoramic Annular Lens (PAL), but withlow-quality imaging results due to aberrations and small image plane size. Wepropose two pipelines, i.e. Aberration Correction (AC) and Super-Resolution andAberration Correction (SR&amp;AC), to solve the image quality problems of MPIP,with imaging sensors of small and large pixel size, respectively. To provide auniversal network for the two pipelines, we leverage the information from thePoint Spread Function (PSF) of the optical system and design a PSF-awareAberration-image Recovery Transformer (PART), in which the self-attentioncalculation and feature extraction are guided via PSF-aware mechanisms. Wetrain PART on synthetic image pairs from simulation and put forward the PALHQdataset to fill the gap of real-world high-quality PAL images for low-levelvision. A comprehensive variety of experiments on synthetic and real-worldbenchmarks demonstrates the impressive imaging results of PCIE and theeffectiveness of plug-and-play PSF-aware mechanisms. We further deliverheuristic experimental findings for minimalist and high-quality panoramicimaging. Our dataset and code will be available at", "output": "Minimalist and High-Quality Panoramic Imaging with PSF-aware Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel method to compute the relative pose of multi-camerasystems using two affine correspondences (ACs). Existing solutions to themulti-camera relative pose estimation are either restricted to special cases ofmotion, have too high computational complexity, or require too many pointcorrespondences (PCs). Thus, these solvers impede an efficient or accuraterelative pose estimation when applying RANSAC as a robust estimator. This papershows that the 6DOF relative pose estimation problem using ACs permits afeasible minimal solution, when exploiting the geometric constraints betweenACs and multi-camera systems using a special parameterization. We present aproblem formulation based on two ACs that encompass two common types of ACsacross two views, i.e., inter-camera and intra-camera. Moreover, the frameworkfor generating the minimal solvers can be extended to solve various relativepose estimation problems, e.g., 5DOF relative pose estimation with knownrotation angle prior. Experiments on both virtual and real multi-camera systemsprove that the proposed solvers are more efficient than the state-of-the-artalgorithms, while resulting in a better relative pose accuracy. Source code isavailable at ", "output": "Affine Correspondences between Multi-Camera Systems for Relative Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cerebral Microbleeds (CMBs) are chronic deposits of small blood products inthe brain tissues, which have explicit relation to various cerebrovasculardiseases depending on their anatomical location, including cognitive decline,intracerebral hemorrhage, and cerebral infarction. However, manual detection ofCMBs is a time-consuming and error-prone process because of their sparse andtiny structural properties. The detection of CMBs is commonly affected by thepresence of many CMB mimics that cause a high false-positive rate (FPR), suchas calcification and pial vessels. This paper proposes a novel 3D deep learningframework that does not only detect CMBs but also inform their anatomicallocation in the brain (i.e., lobar, deep, and infratentorial regions). For theCMB detection task, we propose a single end-to-end model by leveraging theU-Net as a backbone with Region Proposal Network (RPN). To significantly reducethe FPs within the same single model, we develop a new scheme, containingFeature Fusion Module (FFM) that detects small candidates utilizing contextualinformation and Hard Sample Prototype Learning (HSPL) that mines CMB mimics andgenerates additional loss term called concentration loss using ConvolutionalPrototype Learning (CPL). The anatomical localization task does not only tellto which region the CMBs belong but also eliminate some FPs from the detectiontask by utilizing anatomical information. The results show that the proposedRPN that utilizes the FFM and HSPL outperforms the vanilla RPN and achieves asensitivity of 94.66% vs. 93.33% and an average number of false positives persubject (FPavg) of 0.86 vs. 14.73. Also, the anatomical localization taskfurther improves the detection performance by reducing the FPavg to 0.56 whilemaintaining the sensitivity of 94.66%.", "output": "Toward Automated Detection of Microbleeds with Anatomical Scale Localization: A Complete Clinical Diagnosis Support Using Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clustering aims to group similar objects together while separating dissimilarones apart. Thereafter, structures hidden in data can be identified to helpunderstand data in an unsupervised manner. Traditional clustering methods suchas k-means provide only a single clustering for one data set. Deep clusteringmethods such as auto-encoder based clustering methods have shown a betterperformance, but still provide a single clustering. However, a given datasetmight have multiple clustering structures and each represents a uniqueperspective of the data. Therefore, some multiple clustering methods have beendeveloped to discover multiple independent structures hidden in data. Althoughdeep multiple clustering methods provide better performance, how to efficientlycapture the alternative perspectives in data is still a problem. In this paper,we propose AugDMC, a novel data Augmentation guided Deep Multiple Clusteringmethod, to tackle the challenge. Specifically, AugDMC leverages dataaugmentations to automatically extract features related to a certain aspect ofthe data using a self-supervised prototype-based representation learning, wheredifferent aspects of the data can be preserved under different dataaugmentations. Moreover, a stable optimization strategy is proposed toalleviate the unstable problem from different augmentations. Thereafter,multiple clusterings based on different aspects of the data can be obtained.Experimental results on three real-world datasets compared withstate-of-the-art methods validate the effectiveness of the proposed method.", "output": "AugDMC: Data Augmentation Guided Deep Multiple Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Metric Learning (DML) models rely on strong representations andsimilarity-based measures with specific loss functions. Proxy-based losses haveshown great performance compared to pair-based losses in terms of convergencespeed. However, proxies that are assigned to different classes may end up beingclosely located in the embedding space and hence having a hard time todistinguish between positive and negative items. Alternatively, they may becomehighly correlated and hence provide redundant information with the model. Toaddress these issues, we propose a novel approach that introduces SoftOrthogonality (SO) constraint on proxies. The constraint ensures the proxies tobe as orthogonal as possible and hence control their positions in the embeddingspace. Our approach leverages Data-Efficient Image Transformer (DeiT) as anencoder to extract contextual features from images along with a DML objective.The objective is made of the Proxy Anchor loss along with the SOregularization. We evaluate our method on four public benchmarks forcategory-level image retrieval and demonstrate its effectiveness withcomprehensive experimental results and ablation studies. Our evaluationsdemonstrate the superiority of our proposed approach over state-of-the-artmethods by a significant margin.", "output": "Deep Metric Learning with Soft Orthogonal Proxies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-object tracking algorithms have made significant advancements due tothe recent developments in object detection. However, most existing methodsprimarily focus on tracking pedestrians or vehicles, which exhibit relativelysimple and regular motion patterns. Consequently, there is a scarcity ofalgorithms that address the tracking of targets with irregular or non-linearmotion, such as multi-athlete tracking. Furthermore, popular trackingalgorithms often rely on the Kalman filter for object motion modeling, whichfails to track objects when their motion contradicts the linear motionassumption of the Kalman filter. Due to this reason, we proposed a novel onlineand robust multi-object tracking approach, named Iterative Scale-UpExpansionIoU and Deep Features for multi-object tracking. Unlike conventionalmethods, we abandon the use of the Kalman filter and propose utilizing theiterative scale-up expansion IoU. This approach achieves superior trackingperformance without requiring additional training data or adopting a morerobust detector, all while maintaining a lower computational cost compared toother appearance-based methods. Our proposed method demonstrates remarkableeffectiveness in tracking irregular motion objects, achieving a score of 75.3%in HOTA. It outperforms all state-of-the-art online tracking algorithms on theSportsMOT dataset, covering various kinds of sport scenarios.", "output": "Iterative Scale-Up ExpansionIoU and Deep Features Association for Multi-Object Tracking in Sports."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Investigators, funders, and the public desire knowledge on topics and trendsin publicly funded research but current efforts in manual categorization arelimited in scale and understanding. We developed a semi-automated approach toextract and name research topics, and applied this to $1.9B of NCI fundingover 21 years in the radiological sciences to determine micro- and macro-scaleresearch topics and funding trends. Our method relies on sequential clusteringof existing biomedical-based word embeddings, naming using subject matterexperts, and visualization to discover trends at a macroscopic scale aboveindividual topics. We present results using 15 and 60 cluster topics, where wefound that 2D projection of grant embeddings reveals two dominant axes:physics-biology and therapeutic-diagnostic. For our dataset, we found thatfunding for therapeutics- and physics-based research have outpaced diagnostics-and biology-based research, respectively. We hope these results may (1) giveinsight to funders on the appropriateness of their funding allocation, (2)assist investigators in contextualizing their work and explore neighboringresearch domains, and (3) allow the public to review where their tax dollarsare being allocated.", "output": "Semi-automated extraction of research topics and trends from NCI funding in radiological sciences from 2000-2020."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in large-scale text-to-image diffusion models haveenabled many applications in image editing. However, none of these methods havebeen able to edit the layout of single existing images. To address this gap, wepropose the first framework for layout editing of a single image whilepreserving its visual properties, thus allowing for continuous editing on asingle image. Our approach is achieved through two key modules. First, topreserve the characteristics of multiple objects within an image, wedisentangle the concepts of different objects and embed them into separatetextual tokens using a novel method called masked textual inversion. Next, wepropose a training-free optimization method to perform layout control for apre-trained diffusion model, which allows us to regenerate images with learnedconcepts and align them with user-specified layouts. As the first framework toedit the layout of existing images, we demonstrate that our method is effectiveand outperforms other baselines that were modified to support this task. Ourcode will be freely available for public use upon acceptance.", "output": "Continuous Layout Editing of Single Images with Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image restoration involves recovering a high-quality clean image from itsdegraded version. Deep learning-based methods have significantly improved imagerestoration performance, however, they have limited generalization ability todifferent degradation types and levels. This restricts their real-worldapplication since it requires training individual models for each specificdegradation and knowing the input degradation type to apply the relevant model.We present a prompt-based learning approach, PromptIR, for All-In-One imagerestoration that can effectively restore images from various types and levelsof degradation. In particular, our method uses prompts to encodedegradation-specific information, which is then used to dynamically guide therestoration network. This allows our method to generalize to differentdegradation types and levels, while still achieving state-of-the-art results onimage denoising, deraining, and dehazing. Overall, PromptIR offers a genericand efficient plugin module with few lightweight prompts that can be used torestore images of various types and levels of degradation with no priorinformation on the corruptions present in the image. Our code and pretrainedmodels are available here: ", "output": "PromptIR: Prompting for All-in-One Blind Image Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability of generative models to produce highly realistic synthetic faceimages has raised security and ethical concerns. As a first line of defenseagainst such fake faces, deep learning based forensic classifiers have beendeveloped. While these forensic models can detect whether a face image issynthetic or real with high accuracy, they are also vulnerable to adversarialattacks. Although such attacks can be highly successful in evading detection byforensic classifiers, they introduce visible noise patterns that are detectablethrough careful human scrutiny. Additionally, these attacks assume access tothe target model(s) which may not always be true. Attempts have been made todirectly perturb the latent space of GANs to produce adversarial fake facesthat can circumvent forensic classifiers. In this work, we go one step furtherand show that it is possible to successfully generate adversarial fake faceswith a specified set of attributes (e.g., hair color, eye size, race, gender,etc.). To achieve this goal, we leverage the state-of-the-art generative modelStyleGAN with disentangled representations, which enables a range ofmodifications without leaving the manifold of natural images. We propose aframework to search for adversarial latent codes within the feature space ofStyleGAN, where the search can be guided either by a text prompt or a referenceimage. We also propose a meta-learning based optimization strategy to achievetransferable performance on unknown target models. Extensive experimentsdemonstrate that the proposed approach can produce semantically manipulatedadversarial fake faces, which are true to the specified attribute set and cansuccessfully fool forensic face classifiers, while remaining undetectable byhumans. Code: ", "output": "Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new dataset condensation framework termed Squeeze, Recover andRelabel (SRe$^2$L) that decouples the bilevel optimization of model andsynthetic data during training, to handle varying scales of datasets, modelarchitectures and image resolutions for effective dataset condensation. Theproposed method demonstrates flexibility across diverse dataset scales andexhibits multiple advantages in terms of arbitrary resolutions of synthesizedimages, low training cost and memory consumption with high-resolution training,and the ability to scale up to arbitrary evaluation network architectures.Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1Kdatasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8%validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming allprevious state-of-the-art methods by margins of 14.5% and 32.9%, respectively.Our approach also outperforms MTT by approximately 52$times$ (ConvNet-4) and16$times$ (ResNet-18) faster in speed with less memory consumption of11.6$times$ and 6.4$times$ during data synthesis. Our code and condenseddatasets of 50, 200 IPC with 4K recovery budget are available at", "output": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent learning-based approaches have achieved impressive results in thefield of single-shot camera localization. However, how best to fuse multiplemodalities (e.g., image and depth) and to deal with degraded or missing inputare less well studied. In particular, we note that previous approaches towardsdeep fusion do not perform significantly better than models employing a singlemodality. We conjecture that this is because of the naive approaches to featurespace fusion through summation or concatenation which do not take into accountthe different strengths of each modality. To address this, we propose anend-to-end framework, termed VMLoc, to fuse different sensor inputs into acommon latent space through a variational Product-of-Experts (PoE) followed byattention-based fusion. Unlike previous multimodal variational works directlyadapting the objective function of vanilla variational auto-encoder, we showhow camera localization can be accurately estimated through an unbiasedobjective function based on importance weighting. Our model is extensivelyevaluated on RGB-D datasets and the results prove the efficacy of our model.The source code is available at ", "output": "VMLoc: Variational Fusion For Learning-Based Multimodal Camera Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video frame interpolation (VFI) is one of the fundamental research areas invideo processing and there has been extensive research on novel and enhancedinterpolation algorithms. The same is not true for quality assessment of theinterpolated content. In this paper, we describe a subjective quality study forVFI based on a newly developed video database, BVI-VFI. BVI-VFI contains 36reference sequences at three different frame rates and 180 distorted videosgenerated using five conventional and learning based VFI algorithms. Subjectiveopinion scores have been collected from 60 human participants, and thenemployed to evaluate eight popular quality metrics, including PSNR, SSIM andLPIPS which are all commonly used for assessing VFI methods. The resultsindicate that none of these metrics provide acceptable correlation with theperceived quality on interpolated content, with the best-performing metric,LPIPS, offering a SROCC value below 0.6. Our findings show that there is anurgent need to develop a bespoke perceptual quality metric for VFI. The BVI-VFIdataset is publicly available and can be accessed at", "output": "A Subjective Quality Study for Video Frame Interpolation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a new deformable convolution-based video frameinterpolation (VFI) method, using a coarse to fine 3D CNN to enhance themulti-flow prediction. This model first extracts spatio-temporal features atmultiple scales using a 3D CNN, and estimates multi-flows using these featuresin a coarse-to-fine manner. The estimated multi-flows are then used to warp theoriginal input frames as well as context maps, and the warped results are fusedby a synthesis network to produce the final output. This VFI approach has beenfully evaluated against 12 state-of-the-art VFI methods on three commonly usedtest databases. The results evidently show the effectiveness of the proposedmethod, which offers superior interpolation performance over other state of theart algorithms, with PSNR gains up to 0.19dB.", "output": "Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work aims to address the challenges in domain adaptation of 3D objectdetection using infrastructure LiDARs. We design a model DASE-ProPillars thatcan detect vehicles in infrastructure-based LiDARs in real-time. Our model usesPointPillars as the baseline model with additional modules to improve the 3Ddetection performance. To prove the effectiveness of our proposed modules inDASE-ProPillars, we train and evaluate the model on two datasets, the opensource A9-Dataset and a semi-synthetic infrastructure dataset created withinthe Regensburg Next project. We do several sets of experiments for each modulein the DASE-ProPillars detector that show that our model outperforms theSE-ProPillars baseline on the real A9 test set and a semi-synthetic A9 testset, while maintaining an inference speed of 45 Hz (22 ms). We apply domainadaptation from the semi-synthetic A9-Dataset to the semi-synthetic datasetfrom the Regensburg Next project by applying transfer learning and achieve a 3DmAP@0.25 of 93.49% on the Car class of the target test set using 40 recallpositions.", "output": "Real-Time and Robust 3D Object Detection Within Road-Side LiDARs Using Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robust detection of vulnerable road users is a safety critical requirementfor the deployment of autonomous vehicles in heterogeneous traffic. One of themost complex outstanding challenges is that of partial occlusion where a targetobject is only partially available to the sensor due to obstruction by anotherforeground object. A number of leading pedestrian detection benchmarks provideannotation for partial occlusion, however each benchmark varies greatly intheir definition of the occurrence and severity of occlusion. Recent researchdemonstrates that a high degree of subjectivity is used to classify occlusionlevel in these cases and occlusion is typically categorized into 2 to 3 broadcategories such as partially and heavily occluded. This can lead to inaccurateor inconsistent reporting of pedestrian detection model performance dependingon which benchmark is used. This research introduces a novel, objectivebenchmark for partially occluded pedestrian detection to facilitate theobjective characterization of pedestrian detection models. Characterization iscarried out on seven popular pedestrian detection models for a range ofocclusion levels from 0-99%, in order to demonstrate the efficacy and increasedanalysis capabilities of the proposed characterization method. Resultsdemonstrate that pedestrian detection performance degrades, and the number offalse negative detections increase as pedestrian occlusion level increases. Ofthe seven popular pedestrian detection routines characterized, CenterNet hasthe greatest overall performance, followed by SSDlite. RetinaNet has the lowestoverall detection performance across the range of occlusion levels.", "output": "The Impact of Partial Occlusion on Pedestrian Detectability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Input gradients have a pivotal role in a variety of applications, includingadversarial attack algorithms for evaluating model robustness, explainable AItechniques for generating Saliency Maps, and counterfactual explanations.However, Saliency Maps generated by traditional neural networks are often noisyand provide limited insights. In this paper, we demonstrate that, on thecontrary, the Saliency Maps of 1-Lipschitz neural networks, learnt with thedual loss of an optimal transportation problem, exhibit desirable XAIproperties: They are highly concentrated on the essential parts of the imagewith low noise, significantly outperforming state-of-the-art explanationapproaches across various models and metrics. We also prove that these mapsalign unprecedentedly well with human explanations on ImageNet. To explain theparticularly beneficial properties of the Saliency Map for such models, weprove this gradient encodes both the direction of the transportation plan andthe direction towards the nearest adversarial attack. Following the gradientdown to the decision boundary is no longer considered an adversarial attack,but rather a counterfactual explanation that explicitly transports the inputfrom one class to another. Thus, Learning with such a loss jointly optimizesthe classification objective and the alignment of the gradient , i.e. theSaliency Map, to the transportation plan direction. These networks werepreviously known to be certifiably robust by design, and we demonstrate thatthey scale well for large problems and models, and are tailored forexplainability using a fast and straightforward method.", "output": "On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-Language Pretraining (VLP) models have recently successfullyfacilitated many cross-modal downstream tasks. Most existing works evaluatedtheir systems by comparing the fine-tuned downstream task performance. However,only average downstream task accuracy provides little information about thepros and cons of each VLP method, let alone provides insights on how thecommunity can improve the systems in the future. Inspired by the CheckList fortesting natural language processing, we exploit VL-CheckList, a novel frameworkto understand the capabilities of VLP models. The proposed method divides theimage-texting ability of a VLP model into three categories: objects,attributes, and relations, and uses a novel taxonomy to further break downthese three aspects. We conduct comprehensive studies to analyze seven recentlypopular VLP models via the proposed framework. Results confirm theeffectiveness of the proposed method by revealing fine-grained differencesamong the compared models that were not visible from downstream task-onlyevaluation. Further results show promising research direction in buildingbetter VLP models. Our data and code are available at:", "output": "VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video frame interpolation (VFI) serves as a useful tool for many videoprocessing applications. Recently, it has also been applied in the videocompression domain for enhancing both conventional video codecs andlearning-based compression architectures. While there has been an increasedfocus on the development of enhanced frame interpolation algorithms in recentyears, the perceptual quality assessment of interpolated content remains anopen field of research. In this paper, we present a bespoke full referencevideo quality metric for VFI, FloLPIPS, that builds on the popular perceptualimage quality metric, LPIPS, which captures the perceptual degradation inextracted image feature space. In order to enhance the performance of LPIPS forevaluating interpolated content, we re-designed its spatial feature aggregationstep by using the temporal distortion (through comparing optical flows) toweight the feature difference maps. Evaluated on the BVI-VFI database, whichcontains 180 test sequences with various frame interpolation artefacts,FloLPIPS shows superior correlation performance (with statistical significance)with subjective ground truth over 12 popular quality assessors. To facilitatefurther research in VFI quality assessment, our code is publicly available at", "output": "FloLPIPS: A Bespoke Video Quality Metric for Frame Interpoation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In low-level video analyses, effective representations are important toderive the correspondences between video frames. These representations havebeen learned in a self-supervised fashion from unlabeled images or videos,using carefully designed pretext tasks in some recent studies. However, theprevious work concentrates on either spatial-discriminative features ortemporal-repetitive features, with little attention to the synergy betweenspatial and temporal cues. To address this issue, we propose aspatial-then-temporal self-supervised learning method. Specifically, we firstlyextract spatial features from unlabeled images via contrastive learning, andsecondly enhance the features by exploiting the temporal cues in unlabeledvideos via reconstructive learning. In the second step, we design a globalcorrelation distillation loss to ensure the learning not to forget the spatialcues, and a local correlation distillation loss to combat the temporaldiscontinuity that harms the reconstruction. The proposed method outperformsthe state-of-the-art self-supervised methods, as established by theexperimental results on a series of correspondence-based video analysis tasks.Also, we performed ablation studies to verify the effectiveness of the two-stepdesign as well as the distillation losses.", "output": "Spatial-then-Temporal Self-Supervised Learning for Video Correspondence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel clustering algorithm, visClust, that is based on lowerdimensional data representations and visual interpretation. Thereto, we designa transformation that allows the data to be represented by a binary integerarray enabling the further use of image processing methods to select apartition. Qualitative and quantitative analyses show that the algorithmobtains high accuracy (measured with an adjusted one-sided Rand-Index) andrequires low runtime and RAM. We compare the results to 6 state-of-the-artalgorithms, confirming the quality of visClust by outperforming in mostexperiments. Moreover, the algorithm asks for just one obligatory inputparameter while allowing optimization via optional parameters. The code is madeavailable on GitHub.", "output": "visClust: A visual clustering algorithm based on orthogonal projections."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have witnessed the great success of deep learning algorithms inthe geoscience and remote sensing realm. Nevertheless, the security androbustness of deep learning models deserve special attention when addressingsafety-critical remote sensing tasks. In this paper, we provide a systematicanalysis of backdoor attacks for remote sensing data, where both sceneclassification and semantic segmentation tasks are considered. While most ofthe existing backdoor attack algorithms rely on visible triggers like squaredpatches with well-designed patterns, we propose a novel wavelet transform-basedattack (WABA) method, which can achieve invisible attacks by injecting thetrigger image into the poisoned image in the low-frequency domain. In this way,the high-frequency information in the trigger image can be filtered out in theattack, resulting in stealthy data poisoning. Despite its simplicity, theproposed method can significantly cheat the current state-of-the-art deeplearning models with a high attack success rate. We further analyze howdifferent trigger images and the hyper-parameters in the wavelet transformwould influence the performance of the proposed method. Extensive experimentson four benchmark remote sensing datasets demonstrate the effectiveness of theproposed method for both scene classification and semantic segmentation tasksand thus highlight the importance of designing advanced backdoor defensealgorithms to address this threat in remote sensing scenarios. The code will beavailable online at url{", "output": "Backdoor Attacks for Remote Sensing Data with Wavelet Transform."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a method for in-hand 3D scanning of an unknown object with amonocular camera. Our method relies on a neural implicit surface representationthat captures both the geometry and the appearance of the object, however, bycontrast with most NeRF-based methods, we do not assume that the camera-objectrelative poses are known. Instead, we simultaneously optimize both the objectshape and the pose trajectory. As direct optimization over all shape and poseparameters is prone to fail without coarse-level initialization, we propose anincremental approach that starts by splitting the sequence into carefullyselected overlapping segments within which the optimization is likely tosucceed. We reconstruct the object shape and track its poses independentlywithin each segment, then merge all the segments before performing a globaloptimization. We show that our method is able to reconstruct the shape andcolor of both textured and challenging texture-less objects, outperformsclassical methods that rely only on appearance features, and that itsperformance is close to recent methods that assume known camera poses.", "output": "In-Hand 3D Object Scanning from an RGB Sequence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in artificial intelligence (AI) have significantlyintensified research in the geoscience and remote sensing (RS) field. AIalgorithms, especially deep learning-based ones, have been developed andapplied widely to RS data analysis. The successful application of AI coversalmost all aspects of Earth observation (EO) missions, from low-level visiontasks like super-resolution, denoising and inpainting, to high-level visiontasks like scene classification, object detection and semantic segmentation.While AI techniques enable researchers to observe and understand the Earth moreaccurately, the vulnerability and uncertainty of AI models deserve furtherattention, considering that many geoscience and RS tasks are highlysafety-critical. This paper reviews the current development of AI security inthe geoscience and RS field, covering the following five important aspects:adversarial attack, backdoor attack, federated learning, uncertainty andexplainability. Moreover, the potential opportunities and trends are discussedto provide insights for future research. To the best of the authors' knowledge,this paper is the first attempt to provide a systematic review of AIsecurity-related research in the geoscience and RS community. Available codeand datasets are also listed in the paper to move this vibrant field ofresearch forward.", "output": "AI Security for Geoscience and Remote Sensing: Challenges and Future Trends."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mathematical reasoning is a fundamental aspect of human intelligence and isapplicable in various fields, including science, engineering, finance, andeveryday life. The development of artificial intelligence (AI) systems capableof solving math problems and proving theorems has garnered significant interestin the fields of machine learning and natural language processing. For example,mathematics serves as a testbed for aspects of reasoning that are challengingfor powerful deep learning models, driving new algorithmic and modelingadvances. On the other hand, recent advances in large-scale neural languagemodels have opened up new benchmarks and opportunities to use deep learning formathematical reasoning. In this survey paper, we review the key tasks,datasets, and methods at the intersection of mathematical reasoning and deeplearning over the past decade. We also evaluate existing benchmarks andmethods, and discuss future research directions in this domain.", "output": "A Survey of Deep Learning for Mathematical Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to distinguish between different movie scenes is critical forunderstanding the storyline of a movie. However, accurately detecting moviescenes is often challenging as it requires the ability to reason over very longmovie segments. This is in contrast to most existing video recognition models,which are typically designed for short-range video analysis. This work proposesa State-Space Transformer model that can efficiently capture dependencies inlong movie videos for accurate movie scene detection. Our model, dubbedTranS4mer, is built using a novel S4A building block, which combines thestrengths of structured state-space sequence (S4) and self-attention (A)layers. Given a sequence of frames divided into movie shots (uninterruptedperiods where the camera position does not change), the S4A block first appliesself-attention to capture short-range intra-shot dependencies. Afterward, thestate-space operation in the S4A block is used to aggregate long-rangeinter-shot cues. The final TranS4mer model, which can be trained end-to-end, isobtained by stacking the S4A blocks one after the other multiple times. Ourproposed TranS4mer outperforms all prior methods in three movie scene detectiondatasets, including MovieNet, BBC, and OVSD, while also being $2times$ fasterand requiring $3times$ less GPU memory than standard Transformer models. Wewill release our code and models.", "output": "Efficient Movie Scene Detection using State-Space Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Following the traditional paradigm of convolutional neural networks (CNNs),modern CNNs manage to keep pace with more recent, for exampletransformer-based, models by not only increasing model depth and width but alsothe kernel size. This results in large amounts of learnable model parametersthat need to be handled during training. While following the convolutionalparadigm with the according spatial inductive bias, we question thesignificance of emph{learned} convolution filters. In fact, our findingsdemonstrate that many contemporary CNN architectures can achieve high testaccuracies without ever updating randomly initialized (spatial) convolutionfilters. Instead, simple linear combinations (implemented through efficient$1times 1$ convolutions) suffice to effectively recombine even random filtersinto expressive network operators. Furthermore, these combinations of randomfilters can implicitly regularize the resulting operations, mitigatingoverfitting and enhancing overall performance and robustness. Conversely,retaining the ability to learn filter updates can impair network performance.Lastly, although we only observe relatively small gains from learning $3times3$ convolutions, the learning gains increase proportionally with kernel size,owing to the non-idealities of the independent and identically distributed(textit{i.i.d.}) nature of default initialization techniques.", "output": "The Power of Linear Combinations: Learning with Random Convolutions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a novel visual simultaneous localization and mapping(SLAM) system called Hybrid Depth-augmented Panoramic Visual SLAM (HDPV-SLAM),that employs a panoramic camera and a tilted multi-beam LiDAR scanner togenerate accurate and metrically-scaled trajectories. RGB-D SLAM was the designbasis for HDPV-SLAM, which added depth information to visual features. It aimsto solve the two major issues hindering the performance of similar SLAMsystems. The first obstacle is the sparseness of LiDAR depth, which makes itdifficult to correlate it with the extracted visual features of the RGB image.A deep learning-based depth estimation module for iteratively densifying sparseLiDAR depth was suggested to address this issue. The second issue pertains tothe difficulties in depth association caused by a lack of horizontal overlapbetween the panoramic camera and the tilted LiDAR sensor. To surmount thisdifficulty, we present a hybrid depth association module that optimallycombines depth information estimated by two independent procedures,feature-based triangulation and depth estimation. During a phase of featuretracking, this hybrid depth association module aims to maximize the use of moreaccurate depth information between the triangulated depth with visual featurestracked and the deep learning-based corrected depth. We evaluated the efficacyof HDPV-SLAM using the 18.95 km-long York University and Teledyne Optech (YUTO)MMS dataset. The experimental results demonstrate that the two proposed modulescontribute substantially to the performance of HDPV-SLAM, which surpasses thatof the state-of-the-art (SOTA) SLAM systems.", "output": "HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "GAN-based image restoration inverts the generative process to repair imagescorrupted by known degradations. Existing unsupervised methods must becarefully tuned for each task and degradation level. In this work, we makeStyleGAN image restoration robust: a single set of hyperparameters works acrossa wide range of degradation levels. This makes it possible to handlecombinations of several degradations, without the need to retune. Our proposedapproach relies on a 3-phase progressive latent space extension and aconservative optimizer, which avoids the need for any additional regularizationterms. Extensive experiments demonstrate robustness on inpainting, upsampling,denoising, and deartifacting at varying degradations levels, outperformingother StyleGAN-based inversion techniques. Our approach also favorably comparesto diffusion-based restoration by yielding much more realistic inversionresults. Code is available at ", "output": "Robust Unsupervised StyleGAN Image Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Biogenic Volatile Organic Compounds (BVOCs) play a critical role inbiosphere-atmosphere interactions, being a key factor in the physical andchemical properties of the atmosphere and climate. Acquiring large andfine-grained BVOC emission maps is expensive and time-consuming, so mostavailable BVOC data are obtained on a loose and sparse sampling grid or onsmall regions. However, high-resolution BVOC data are desirable in manyapplications, such as air quality, atmospheric chemistry, and climatemonitoring. In this work, we investigate the possibility of enhancing BVOCacquisitions, further explaining the relationships between the environment andthese compounds. We do so by comparing the performances of severalstate-of-the-art neural networks proposed for image Super-Resolution (SR),adapting them to overcome the challenges posed by the large dynamic range ofthe emission and reduce the impact of outliers in the prediction. Moreover, wealso consider realistic scenarios, considering both temporal and geographicalconstraints. Finally, we present possible future developments regarding SRgeneralization, considering the scale-invariance property and super-resolvingemissions from unseen compounds.", "output": "Super-Resolution of BVOC Maps by Adapting Deep Learning Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human-centric perceptions (e.g., pose estimation, human parsing, pedestriandetection, person re-identification, etc.) play a key role in industrialapplications of visual models. While specific human-centric tasks have theirown relevant semantic aspect to focus on, they also share the same underlyingsemantic structure of the human body. However, few works have attempted toexploit such homogeneity and design a general-propose model for human-centrictasks. In this work, we revisit a broad range of human-centric tasks and unifythem in a minimalist manner. We propose UniHCP, a Unified Model forHuman-Centric Perceptions, which unifies a wide range of human-centric tasks ina simplified end-to-end manner with the plain vision transformer architecture.With large-scale joint training on 33 human-centric datasets, UniHCP canoutperform strong baselines on several in-domain and downstream tasks by directevaluation. When adapted to a specific task, UniHCP achieves new SOTAs on awide range of human-centric tasks, e.g., 69.8 mIoU on CIHP for human parsing,86.18 mA on PA-100K for attribute prediction, 90.3 mAP on Market1501 for ReID,and 85.8 JI on CrowdHuman for pedestrian detection, performing better thanspecialized models tailored for each task.", "output": "UniHCP: A Unified Model for Human-Centric Perceptions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpretability is highly desired for deep neural network-based classifiers,especially when addressing high-stake decisions in medical imaging. Commonlyused post-hoc interpretability methods have the limitation that they canproduce plausible but different interpretations of a given model, leading toambiguity about which one to choose. To address this problem, a noveldecision-theory-motivated approach is investigated to establish aself-interpretable model, given a pretrained deep binary black-box medicalimage classifier. This approach involves utilizing a self-interpretableencoder-decoder model in conjunction with a single-layer fully connectednetwork with unity weights. The model is trained to estimate the test statisticof the given trained black-box deep binary classifier to maintain a similaraccuracy. The decoder output image, referred to as an equivalency map, is animage that represents a transformed version of the to-be-classified image that,when processed by the fixed fully connected layer, produces the same teststatistic value as the original classifier. The equivalency map provides avisualization of the transformed image features that directly contribute to thetest statistic value and, moreover, permits quantification of their relativecontributions. Unlike the traditional post-hoc interpretability methods, theproposed method is self-interpretable, quantitative, and fundamentally based ondecision theory. Detailed quantitative and qualitative analysis have beenperformed with three different medical image binary classification tasks.", "output": "Revisiting model self-interpretability in a decision-theoretic way for binary medical image classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since the number of incident energies is limited, it is difficult to directlyacquire hyperspectral images (HSI) with high spatial resolution. Consideringthe high dimensionality and correlation of HSI, super-resolution (SR) of HSIremains a challenge in the absence of auxiliary high-resolution images.Furthermore, it is very important to extract the spatial features effectivelyand make full use of the spectral information. This paper proposes a novel HSIsuper-resolution algorithm, termed dual-domain network based on hybridconvolution (SRDNet). Specifically, a dual-domain network is designed to fullyexploit the spatial-spectral and frequency information among the hyper-spectraldata. To capture inter-spectral self-similarity, a self-attention learningmechanism (HSL) is devised in the spatial domain. Meanwhile the pyramidstructure is applied to increase the acceptance field of attention, whichfurther reinforces the feature representation ability of the network. Moreover,to further improve the perceptual quality of HSI, a frequency loss(HFL) isintroduced to optimize the model in the frequency domain. The dynamic weightingmechanism drives the network to gradually refine the generated frequency andexcessive smoothing caused by spatial loss. Finally, In order to better fullyobtain the mapping relationship between high-resolution space andlow-resolution space, a hybrid module of 2D and 3D units with progressiveupsampling strategy is utilized in our method. Experiments on a widely usedbenchmark dataset illustrate that the proposed SRDNet method enhances thetexture information of HSI and is superior to state-of-the-art methods.", "output": "Hyperspectral Image Super-Resolution via Dual-domain Network Based on Hybrid Convolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-modal contrastive learning in vision language pretraining (VLP) facesthe challenge of (partial) false negatives. In this paper, we study thisproblem from the perspective of Mutual Information (MI) optimization. It iscommon sense that InfoNCE loss used in contrastive learning will maximize thelower bound of MI between anchors and their positives, while we theoreticallyprove that MI involving negatives also matters when noises commonly exist.Guided by a more general lower bound form for optimization, we propose acontrastive learning strategy regulated by progressively refined cross-modalsimilarity, to more accurately optimize MI between an image/text anchor and itsnegative texts/images instead of improperly minimizing it. Our method performscompetitively on four downstream cross-modal tasks and systematically balancesthe beneficial and harmful effects of (partial) false negative samples undertheoretical guidance.", "output": "Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Biogenic Volatile Organic Compounds (BVOCs) emitted from the terrestrialecosystem into the Earth's atmosphere are an important component of atmosphericchemistry. Due to the scarcity of measurement, a reliable enhancement of BVOCsemission maps can aid in providing denser data for atmospheric chemical,climate, and air quality models. In this work, we propose a strategy tosuper-resolve coarse BVOC emission maps by simultaneously exploiting thecontributions of different compounds. To this purpose, we first accuratelyinvestigate the spatial inter-connections between several BVOC species. Then,we exploit the found similarities to build a Multi-Image Super-Resolution(MISR) system, in which a number of emission maps associated with diversecompounds are aggregated to boost Super-Resolution (SR) performance. We comparedifferent configurations regarding the species and the number of joined BVOCs.Our experimental results show that incorporating BVOCs' relationship into theprocess can substantially improve the accuracy of the super-resolved maps.Interestingly, the best results are achieved when we aggregate the emissionmaps of strongly uncorrelated compounds. This peculiarity seems to confirm whatwas already guessed for other data-domains, i.e., joined uncorrelatedinformation are more helpful than correlated ones to boost MISR performance.Nonetheless, the proposed work represents the first attempt in SR of BVOCemissions through the fusion of multiple different compounds.", "output": "Multi-BVOC Super-Resolution Exploiting Compounds Inter-Connection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Subject-driven text-to-image generation models create novel renditions of aninput subject based on text prompts. Existing models suffer from lengthyfine-tuning and difficulties preserving the subject fidelity. To overcome theselimitations, we introduce BLIP-Diffusion, a new subject-driven image generationmodel that supports multimodal control which consumes inputs of subject imagesand text prompts. Unlike other subject-driven generation models, BLIP-Diffusionintroduces a new multimodal encoder which is pre-trained to provide subjectrepresentation. We first pre-train the multimodal encoder following BLIP-2 toproduce visual representation aligned with the text. Then we design a subjectrepresentation learning task which enables a diffusion model to leverage suchvisual representation and generates new subject renditions. Compared withprevious methods such as DreamBooth, our model enables zero-shot subject-drivengeneration, and efficient fine-tuning for customized subject with up to 20xspeedup. We also demonstrate that BLIP-Diffusion can be flexibly combined withexisting techniques such as ControlNet and prompt-to-prompt to enable novelsubject-driven generation and editing applications. Code and models will bereleased at Projectpage at ", "output": "BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evolution of semantic segmentation has long been dominated by learningmore discriminative image representations for classifying each pixel. Despitethe prominent advancements, the priors of segmentation masks themselves, e.g.,geometric and semantic constraints, are still under-explored. In this paper, wepropose to ameliorate the semantic segmentation quality of existingdiscriminative approaches with a mask prior modeled by a recently-developeddenoising diffusion generative model. Beginning with a unified architecturethat adapts diffusion models for mask prior modeling, we focus this work on aspecific instantiation with discrete diffusion and identify a variety of keydesign choices for its successful application. Our exploratory analysisrevealed several important findings, including: (1) a simple integration ofdiffusion models into semantic segmentation is not sufficient, and apoorly-designed diffusion process might lead to degradation in segmentationperformance; (2) during the training, the object to which noise is added ismore important than the type of noise; (3) during the inference, the strictdiffusion denoising scheme may not be essential and can be relaxed to a simplerscheme that even works better. We evaluate the proposed prior modeling withseveral off-the-shelf segmentors, and our experimental results on ADE20K andCityscapes demonstrate that our approach could achieve competitivelyquantitative performance and more appealing visual quality.", "output": "Denoising Diffusion Semantic Segmentation with Mask Prior Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image restoration is a low-level visual task, and most CNN methods aredesigned as black boxes, lacking transparency and intrinsic aesthetics. Manyunsupervised approaches ignore the degradation of visible information inlow-light scenes, which will seriously affect the aggregation of complementaryinformation and also make the fusion algorithm unable to produce satisfactoryfusion results under extreme conditions. In this paper, we proposeEnlighten-anything, which is able to enhance and fuse the semantic intent ofSAM segmentation with low-light images to obtain fused images with good visualperception. The generalization ability of unsupervised learning is greatlyimproved, and experiments on LOL dataset are conducted to show that our methodimproves 3db in PSNR over baseline and 8 in SSIM. Zero-shot learning of SAMintroduces a powerful aid for unsupervised low-light enhancement. The sourcecode of Enlighten Anything can be obtained from", "output": "Enlighten Anything: When Segment Anything Model Meets Low-Light Image Enhancement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High dynamic range (HDR) imaging is still a significant yet challengingproblem due to the limited dynamic range of generic image sensors. Mostexisting learning-based HDR reconstruction methods take a set ofbracketed-exposure sRGB images to extend the dynamic range, and thus arecomputational- and memory-inefficient by requiring the Image Signal Processor(ISP) to produce multiple sRGB images from the raw ones. In this paper, wepropose to broaden the dynamic range from the raw inputs and perform only oneISP processing for the reconstructed HDR raw image. Our key insights arethreefold: (1) we design a new computational raw HDR data formation pipelineand construct the first real-world raw HDR dataset, RealRaw-HDR; (2) we developa lightweight-efficient HDR model, RepUNet, using the structuralre-parameterization technique; (3) we propose a plug-and-play motion alignmentloss to mitigate motion misalignment between short- and long-exposure images.Extensive experiments demonstrate that our approach achieves state-of-the-artperformance in both visual quality and quantitative metrics.", "output": "Efficient HDR Reconstruction from Real-World Raw Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rehabilitation treatment helps to heal minor sports and occupationalinjuries. In a traditional rehabilitation process, a therapist will assigncertain actions to a patient to perform in between hospital visits, and it willrely on the patient to remember actions correctly and the schedule to performthem. Unfortunately, many patients forget to perform actions or fail to recallactions in detail. As a consequence, the rehabilitation treatment is hamperedor, in the worst case, the patient may suffer from additional injury caused byperforming incorrect actions. To resolve these issues, we propose a HRNet-basedrehabilitation monitoring system, which can remind a patient when to performthe actions and display the actions for the patient to follow via the patient'ssmartphone. In addition, it helps the therapist to monitor the progress of therehabilitation for the patient. Our system consists of an iOS app and severalcomponents at the server side. The app is in charge of displaying andcollecting action videos. The server computes the similarity score between thetherapist's actions and the patient's in the videos to keep track of the numberof repetitions of each action. Theses stats will be shown to both of thepatient and therapist. The extensive experiments show that the F1-Score of thesimilarity calculation is as high as 0.9 and the soft accuracy of the number ofrepetitions is higher than 90%.", "output": "A HRNet-based Rehabilitation Monitoring System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent accelerations in multi-modal applications have been made possible withthe plethora of image and text data available online. However, the scarcity ofanalogous data in the medical field, specifically in histopathology, has haltedcomparable progress. To enable similar representation learning forhistopathology, we turn to YouTube, an untapped resource of videos, offering$1,087$ hours of valuable educational histopathology videos from expertclinicians. From YouTube, we curate Quilt: a large-scale vision-languagedataset consisting of $768,826$ image and text pairs. Quilt was automaticallycurated using a mixture of models, including large language models, handcraftedalgorithms, human knowledge databases, and automatic speech recognition. Incomparison, the most comprehensive datasets curated for histopathology amassonly around $200$K samples. We combine Quilt with datasets from other sources,including Twitter, research papers, and the internet in general, to create aneven larger dataset: Quilt-1M, with $1$M paired image-text samples, marking itas the largest vision-language histopathology dataset to date. We demonstratethe value of Quilt-1M by fine-tuning a pre-trained CLIP model. Our modeloutperforms state-of-the-art models on both zero-shot and linear probing tasksfor classifying new histopathology images across $13$ diverse patch-leveldatasets of $8$ different sub-pathologies and cross-modal retrieval tasks.", "output": "Quilt-1M: One Million Image-Text Pairs for Histopathology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The advent of data-driven technology solutions is accompanied by anincreasing concern with data privacy. This is of particular importance forhuman-centered image recognition tasks, such as pedestrian detection,re-identification, and tracking. To highlight the importance of privacy issuesand motivate future research, we motivate and introduce the Pedestrian DatasetDe-Identification (PDI) task. PDI evaluates the degree of de-identification anddownstream task training performance for a given de-identification method. As afirst baseline, we propose IncogniMOT, a two-stage full-body de-identificationpipeline based on image synthesis via generative adversarial networks. Thefirst stage replaces target pedestrians with synthetic identities. To improvedownstream task performance, we then apply stage two, which blends and adaptsthe synthetic image parts into the data. To demonstrate the effectiveness ofIncogniMOT, we generate a fully de-identified version of the MOT17 pedestriantracking dataset and analyze its application as training data for pedestrianre-identification, detection, and tracking models. Furthermore, we show how ourdata is able to narrow the synthetic-to-real performance gap in aprivacy-conscious manner.", "output": "Data-Driven but Privacy-Conscious: Pedestrian Dataset De-identification via Full-Body Person Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large multimodal language models (LMMs) have achieved significant success ingeneral domains. However, due to the significant differences between medicalimages and text and general web content, the performance of LMMs in medicalscenarios is limited. In ophthalmology, clinical diagnosis relies on multiplemodalities of medical images, but unfortunately, multimodal ophthalmic largelanguage models have not been explored to date. In this paper, we study andconstruct an ophthalmic large multimodal model. Firstly, we use fundus imagesas an entry point to build a disease assessment and diagnosis pipeline toachieve common ophthalmic disease diagnosis and lesion segmentation. Then, weestablish a new ophthalmic multimodal instruction-following and dialoguefine-tuning dataset based on disease-related knowledge data and publiclyavailable real-world medical dialogue. We introduce visual ability into thelarge language model to complete the ophthalmic large language and visionassistant (OphGLM). Our experimental results demonstrate that the OphGLM modelperforms exceptionally well, and it has the potential to revolutionize clinicalapplications in ophthalmology. The dataset, code, and models will be madepublicly available at ", "output": "OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a large publicly available multi-center lumbar spinemagnetic resonance imaging (MRI) dataset with reference segmentations ofvertebrae, intervertebral discs (IVDs), and spinal canal. The dataset includes447 sagittal T1 and T2 MRI series from 218 patients with a history of low backpain. It was collected from four different hospitals and was divided into atraining (179 patients) and validation (39 patients) set. An iterative dataannotation approach was used by training a segmentation algorithm on a smallpart of the dataset, enabling semi-automatic segmentation of the remainingimages. The algorithm provided an initial segmentation, which was subsequentlyreviewed, manually corrected, and added to the training data. We providereference performance values for this baseline algorithm and nnU-Net, whichperformed comparably. We set up a continuous segmentation challenge to allowfor a fair comparison of different segmentation algorithms. This study mayencourage wider collaboration in the field of spine segmentation, and improvethe diagnostic value of lumbar spine MRI.", "output": "Lumbar spine segmentation in MR images: a dataset and a public benchmark."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, diseases are increasing in numbers and severity by the hour.Immunity diseases, affecting 8% of the world population in 2017 according tothe World Health Organization (WHO), is a field in medicine worth attention dueto the high rate of disease occurrence classified under this category. Thiswork presents an up-to-date review of state-of-the-art immune diseaseshealthcare solutions. We focus on tackling the issue with modern solutions suchas Deep Learning to detect anomalies in the early stages hence providing healthpractitioners with efficient tools. We rely on advanced deep learningtechniques such as Convolutional Neural Networks (CNN) to fulfill our objectiveof providing an efficient tool while providing a proficient analysis of thissolution. The proposed solution was tested and evaluated by the immunologydepartment in the Principal Military Hospital of Instruction of Tunis, whichconsidered it a very helpful tool.", "output": "Interpretation of immunofluorescence slides by deep learning techniques: anti-nuclear antibodies case study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Obsessive-compulsive disorder (OCD) presents itself as a highly debilitatingdisorder. The disorder has common associations with the prefrontal cortex andthe glutamate receptor known as Metabotropic Glutamate Receptor 5 (mGluR5).This receptor has been observed to demonstrate higher levels of signaling frompositron emission tomography scans measured by its distribution volume ratiosin mice. Despite this evidence, studies are unable to fully verify theinvolvement of mGluR5 as more empirical data is needed. Computational modelingmethods were used as a means of validation for previous hypotheses involvingmGluR5. The inadequacies in relation to the causal factor of OCD were answeredby utilizing T1 resting-state magnetic resonance imaging (TRS-MRI) scans ofpatients suffering from schizophrenia, major depressive disorder, andobsessive-compulsive disorder. Because comorbid cases often occur within thesedisorders, cross-comparative abilities become necessary to find distinctivecharacteristics. Two-dimensional convolutional neural networks alongsideResNet50 and MobileNet models were constructed and evaluated for efficiency.Activation heatmaps of TRS-MRI scans were outputted, allowing fortranscriptomics analysis. Though, a lack of ability to predict OCD casesprevented gene expression analysis. Across all models, there was an 88.75%validation accuracy for MDD, and 82.08% validation accuracy for SZD under theframework of ResNet50 as well as novel computation. OCD yielded an accuracyrate of ~54.4%. These results provided further evidence for the p factortheorem regarding mental disorders. Future work involves the application oftransfer learning to bolster accuracy rates.", "output": "Modeling T1 Resting-State MRI Variants Using Convolutional Neural Networks in Diagnosis of OCD."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate epidemic forecasting plays a vital role for governments indeveloping effective prevention measures for suppressing epidemics. Most of thepresent spatio-temporal models cannot provide a general framework for stable,and accurate forecasting of epidemics with diverse evolution trends.Incorporating epidemiological domain knowledge ranging from single-patch tomulti-patch into neural networks is expected to improve forecasting accuracy.However, relying solely on single-patch knowledge neglects inter-patchinteractions, while constructing multi-patch knowledge is challenging withoutpopulation mobility data. To address the aforementioned problems, we propose anovel hybrid model called Metapopulation-based Spatio-Temporal AttentionNetwork (MPSTAN). This model aims to improve the accuracy of epidemicforecasting by incorporating multi-patch epidemiological knowledge into aspatio-temporal model and adaptively defining inter-patch interactions.Moreover, we incorporate inter-patch epidemiological knowledge into both themodel construction and loss function to help the model learn epidemictransmission dynamics. Extensive experiments conducted on two representativedatasets with different epidemiological evolution trends demonstrate that ourproposed model outperforms the baselines and provides more accurate and stableshort- and long-term forecasting. We confirm the effectiveness of domainknowledge in the learning model and investigate the impact of different ways ofintegrating domain knowledge on forecasting. We observe that using domainknowledge in both model construction and loss functions leads to more efficientforecasting, and selecting appropriate domain knowledge can improve accuracyfurther.", "output": "MPSTAN: Metapopulation-based Spatio-Temporal Attention Network for Epidemic Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative models capable of capturing nuanced clinical features in medicalimages hold great promise for facilitating clinical data sharing, enhancingrare disease datasets, and efficiently synthesizing annotated medical images atscale. Despite their potential, assessing the quality of synthetic medicalimages remains a challenge. While modern generative models can synthesizevisually-realistic medical images, the clinical validity of these images may becalled into question. Domain-agnostic scores, such as FID score, precision, andrecall, cannot incorporate clinical knowledge and are, therefore, not suitablefor assessing clinical sensibility. Additionally, there are numerousunpredictable ways in which generative models may fail to synthesize clinicallyplausible images, making it challenging to anticipate potential failures andmanually design scores for their detection. To address these challenges, thispaper introduces a pathologist-in-the-loop framework for generatingclinically-plausible synthetic medical images. Starting with a diffusion modelpretrained using real images, our framework comprises three steps: (1)evaluating the generated images by expert pathologists to assess whether theysatisfy clinical desiderata, (2) training a reward model that predicts thepathologist feedback on new samples, and (3) incorporating expert knowledgeinto the diffusion model by using the reward model to inform a finetuningobjective. We show that human feedback significantly improves the quality ofsynthetic images in terms of fidelity, diversity, utility in downstreamapplications, and plausibility as evaluated by experts.", "output": "Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation is a powerful technique for transferring knowledgefrom a pre-trained teacher model to a student model. However, the truepotential of knowledge transfer has not been fully explored. Existingapproaches primarily focus on distilling individual information orinstance-level relationships, overlooking the valuable information embedded intoken-level relationships, which may be particularly affected by the long-taileffects. To address the above limitations, we propose a novel method calledKnowledge Distillation with Token-level Relationship Graph (TRG) that leveragesthe token-wise relational knowledge to enhance the performance of knowledgedistillation. By employing TRG, the student model can effectively emulatehigher-level semantic information from the teacher model, resulting in improveddistillation results. To further enhance the learning process, we introduce atoken-wise contextual loss called contextual loss, which encourages the studentmodel to capture the inner-instance semantic contextual of the teacher model.We conduct experiments to evaluate the effectiveness of the proposed methodagainst several state-of-the-art approaches. Empirical results demonstrate thesuperiority of TRG across various visual classification tasks, including thoseinvolving imbalanced data. Our method consistently outperforms the existingbaselines, establishing a new state-of-the-art performance in the field ofknowledge distillation.", "output": "Knowledge Distillation via Token-level Relationship Graph."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mental distress like depression and anxiety contribute to the largestproportion of the global burden of diseases. Automated diagnosis systems ofsuch disorders, empowered by recent innovations in Artificial Intelligence, canpave the way to reduce the sufferings of the affected individuals. Developmentof such systems requires information-rich and balanced corpora. In this work,we introduce a novel mental distress analysis audio dataset DEPAC, labeledbased on established thresholds on depression and anxiety standard screeningtools. This large dataset comprises multiple speech tasks per individual, aswell as relevant demographic information. Alongside, we present a feature setconsisting of hand-curated acoustic and linguistic features, which were foundeffective in identifying signs of mental illnesses in human speech. Finally, wejustify the quality and effectiveness of our proposed audio corpus and featureset in predicting depression severity by comparing the performance of baselinemachine learning models built on this dataset with baseline models trained onother well-known depression corpora.", "output": "DEPAC: a Corpus for Depression and Anxiety Detection from Speech."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Detecting duplicate patient participation in clinical trials is a majorchallenge because repeated patients can undermine the credibility and accuracyof the trial's findings and result in significant health and financial risks.Developing accurate automated speaker verification (ASV) models is crucial toverify the identity of enrolled individuals and remove duplicates, but the sizeand quality of data influence ASV performance. However, there has been limitedinvestigation into the factors that can affect ASV capabilities in clinicalenvironments. In this paper, we bridge the gap by conducting analysis of howparticipant demographic characteristics, audio quality criteria, and severitylevel of Alzheimer's disease (AD) impact the performance of ASV utilizing adataset of speech recordings from 659 participants with varying levels of AD,obtained through multiple speech tasks. Our results indicate that ASVperformance: 1) is slightly better on male speakers than on female speakers; 2)degrades for individuals who are above 70 years old; 3) is comparatively betterfor non-native English speakers than for native English speakers; 4) isnegatively affected by clinician interference, noisy background, and unclearparticipant speech; 5) tends to decrease with an increase in the severity levelof AD. Our study finds that voice biometrics raise fairness concerns as certainsubgroups exhibit different ASV performances owing to their inherent voicecharacteristics. Moreover, the performance of ASV is influenced by the qualityof speech recordings, which underscores the importance of improving the datacollection settings in clinical trials.", "output": "Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study aims at comparing several deep learning-based forecasters in thetask of volatility prediction using multivariate data, proceeding from simpleror shallower to deeper and more complex models and compare them to the naiveprediction and variations of classical GARCH models. Specifically, thevolatility of five assets (i.e., S&amp;P500, NASDAQ100, gold, silver, and oil) waspredicted with the GARCH models, Multi-Layer Perceptrons, recurrent neuralnetworks, Temporal Convolutional Networks, and the Temporal Fusion Transformer.In most cases the Temporal Fusion Transformer followed by variants of TemporalConvolutional Network outperformed classical approaches and shallow networks.These experiments were repeated, and the difference between competing modelswas shown to be statistically significant, therefore encouraging their use inpractice.", "output": "Comparing deep learning models for volatility prediction using multivariate data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the fundamental challenges in causal inference is to estimate thecausal effect of a treatment on its outcome of interest from observationaldata. However, causal effect estimation often suffers from the impacts ofconfounding bias caused by unmeasured confounders that affect both thetreatment and the outcome. The instrumental variable (IV) approach is apowerful way to eliminate the confounding bias from latent confounders.However, the existing IV-based estimators require a nominated IV, and for aconditional IV (CIV) the corresponding conditioning set too, for causal effectestimation. This limits the application of IV-based estimators. In this paper,by leveraging the advantage of disentangled representation learning, we proposea novel method, named DVAE.CIV, for learning and disentangling therepresentations of CIV and the representations of its conditioning set forcausal effect estimations from data with latent confounders. Extensiveexperimental results on both synthetic and real-world datasets demonstrate thesuperiority of the proposed DVAE.CIV method against the existing causal effectestimators.", "output": "Learning Conditional Instrumental Variable Representation for Causal Effect Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Objective: COVID-19 has spread worldwide and made a huge influence across theworld. Modeling the infectious spread situation of COVID-19 is essential tounderstand the current condition and to formulate intervention measurements.Epidemiological equations based on the SEIR model simulate disease development.The traditional parameter estimation method to solve SEIR equations could notprecisely fit real-world data due to different situations, such as socialdistancing policies and intervention strategies. Additionally, learning-basedmodels achieve outstanding fitting performance, but cannot visualizemechanisms. Methods: Thus, we propose a deep dynamic epidemiological (DDE)method that combines epidemiological equations and deep-learning advantages toobtain high accuracy and visualization. The DDE contains deep networks to fitthe effect function to simulate the ever-changing situations based on theneural ODE method in solving variants' equations, ensuring the fittingperformance of multi-level areas. Results: We introduce four SEIR variants tofit different situations in different countries and regions. We compare our DDEmethod with traditional parameter estimation methods (Nelder-Mead, BFGS,Powell, Truncated Newton Conjugate-Gradient, Neural ODE) in fitting thereal-world data in the cases of countries (the USA, Columbia, South Africa) andregions (Wuhan in China, Piedmont in Italy). Our DDE method achieves the bestMean Square Error and Pearson coefficient in all five areas. Further, comparedwith the state-of-art learning-based approaches, the DDE outperforms alltechniques, including LSTM, RNN, GRU, Random Forest, Extremely Random Trees,and Decision Tree. Conclusion: DDE presents outstanding predictive ability andvisualized display of the changes in infection rates in different regions andcountries.", "output": "Deep Dynamic Epidemiological Modelling for COVID-19 Forecasting in Multi-level Districts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Precision psychiatry is an ermerging field that aims to provideindividualized approaches to mental health care. Multivariate analysis andmachine learning are used to create outcome prediction models based on clinicaldata such as demographics, symptom assessments, genetic information, and brainimaging. While much emphasis has been placed on technical innovation, thecomplex and varied nature of mental health presents significant challenges tothe successful implementation of these models. From this perspective, I reviewten challenges in the field of precision psychiatry, including the need forstudies on real-world populations and realistic clinical outcome definitions,consideration of treatment-related factors such as placebo effects andnon-adherence to prescriptions. Fairness, prospective validation in comparisonto current practice and implementation studies of prediction models are otherkey issues that are currently understudied. A shift is proposed fromretrospective studies based on linear and static concepts of disease towardsprospective research that considers the importance of contextual factors andthe dynamic and complex nature of mental health.", "output": "Precision psychiatry: predicting predictability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advancements in adapting deep convolution architectures for Spiking NeuralNetworks (SNNs) have significantly enhanced image classification performanceand reduced computational burdens. However, the inability ofMultiplication-Free Inference (MFI) to harmonize with attention and transformermechanisms, which are critical to superior performance on high-resolutionvision tasks, imposes limitations on these gains. To address this, our researchexplores a new pathway, drawing inspiration from the progress made inMulti-Layer Perceptrons (MLPs). We propose an innovative spiking MLParchitecture that uses batch normalization to retain MFI compatibility andintroduces a spiking patch encoding layer to reinforce local feature extractioncapabilities. As a result, we establish an efficient multi-stage spiking MLPnetwork that effectively blends global receptive fields with local featureextraction for comprehensive spike-based computation. Without relying onpre-training or sophisticated SNN training techniques, our network secures atop-1 accuracy of 66.39% on the ImageNet-1K dataset, surpassing the directlytrained spiking ResNet-34 by 2.67%. Furthermore, we curtail computationalcosts, model capacity, and simulation steps. An expanded version of our networkchallenges the performance of the spiking VGG-16 network with a 71.64% top-1accuracy, all while operating with a model capacity 2.1 times smaller. Ourfindings accentuate the potential of our deep SNN architecture in seamlesslyintegrating global and local learning abilities. Interestingly, the trainedreceptive field in our network mirrors the activity patterns of cortical cells.", "output": "Efficient Deep Spiking Multi-Layer Perceptrons with Multiplication-Free Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current approaches to neural network verification focus on specificationsthat target small regions around known input data points, such as localrobustness. Thus, using these approaches, we can not obtain guarantees forinputs that are not close to known inputs. Yet, it is highly likely that aneural network will encounter such truly unseen inputs during its application.We study global specifications that - when satisfied - provide guarantees forall potential inputs. We introduce a hyperproperty formalism that allows forexpressing global specifications such as monotonicity, Lipschitz continuity,global robustness, and dependency fairness. Our formalism enables verifyingglobal specifications using existing neural network verification approaches byleveraging capabilities for verifying general computational graphs. Thereby, weextend the scope of guarantees that can be provided using existing methods.Recent success in verifying specific global specifications shows that attainingstrong guarantees for all potential data points is feasible.", "output": "Verifying Global Neural Network Specifications using Hyperproperties."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Assessing the predictive uncertainty of deep neural networks is crucial forsafety-related applications of deep learning. Although Bayesian deep learningoffers a principled framework for estimating model uncertainty, the approachesthat are commonly used to approximate the posterior often fail to deliverreliable estimates of predictive uncertainty. In this paper we propose a novelcriterion for predictive uncertainty, that a model's predictive variance shouldbe grounded in the empirical density of the input. It should produce higheruncertainty for inputs that are improbable in the training data and loweruncertainty for those inputs that are more probable. To operationalize thiscriterion, we develop the density uncertainty layer, an architectural elementfor a stochastic neural network that guarantees that the density uncertaincriterion is satisfied. We study neural networks with density uncertaintylayers on the CIFAR-10 and CIFAR-100 uncertainty benchmarks. Compared toexisting approaches, we find that density uncertainty layers provide reliableuncertainty estimates and robust out-of-distribution detection performance.", "output": "Density Uncertainty Layers for Reliable Uncertainty Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stochastic gradient descent (SGD) is perhaps the most prevalent optimizationmethod in modern machine learning. Contrary to the empirical practice ofsampling from the datasets without replacement and with (possible) reshufflingat each epoch, the theoretical counterpart of SGD usually relies on theassumption of sampling with replacement. It is only very recently that SGD withsampling without replacement -- shuffled SGD -- has been analyzed. For convexfinite sum problems with $n$ components and under the $L$-smoothness assumptionfor each component function, there are matching upper and lower bounds, undersufficiently small -- $mathcal{O}(frac{1}{nL})$ -- step sizes. Yet thosebounds appear too pessimistic -- in fact, the predicted performance isgenerally no better than for full gradient descent -- and do not agree with theempirical observations. In this work, to narrow the gap between the theory andpractice of shuffled SGD, we sharpen the focus from general finite sum problemsto empirical risk minimization with linear predictors. This allows us to take aprimal-dual perspective and interpret shuffled SGD as a primal-dual method withcyclic coordinate updates on the dual side. Leveraging this perspective, weprove a fine-grained complexity bound that depends on the data matrix and isnever worse than what is predicted by the existing bounds. Notably, our boundcan predict much faster convergence than the existing analyses -- by a factorof the order of $sqrt{n}$ in some cases. We empirically demonstrate that oncommon machine learning datasets our bound is indeed much tighter. We furthershow how to extend our analysis to convex nonsmooth problems, with similarimprovements.", "output": "Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpreting machine learning models remains a challenge, hindering theiradoption in clinical settings. This paper proposes leveraging LocalInterpretable Model-Agnostic Explanations (LIME) to provide interpretabledescriptions of black box classification models in high-stakes sepsisdetection. By analyzing misclassified instances, significant featurescontributing to suboptimal performance are identified. The analysis revealsregions where the classifier performs poorly, allowing the calculation of errorrates within these regions. This knowledge is crucial for cautiousdecision-making in sepsis detection and other critical applications. Theproposed approach is demonstrated using the eICU dataset, effectivelyidentifying and visualizing regions where the classifier underperforms. Byenhancing interpretability, our method promotes the adoption of machinelearning models in clinical practice, empowering informed decision-making andmitigating risks in critical scenarios.", "output": "Investigating Poor Performance Regions of Black Boxes: LIME-based Exploration in Sepsis Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We view large language models (LLMs) as stochastic emph{language layers} ina network, where the learnable parameters are the natural languageemph{prompts} at each layer. We stack two such layers, feeding the output ofone layer to the next. We call the stacked architecture a emph{Deep LanguageNetwork} (DLN). We first show how to effectively perform prompt optimizationfor a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs(DLN-2), where two prompts must be learnt. We consider the output of the firstlayer as a latent variable to marginalize, and devise a variational inferencealgorithm for joint prompt training. A DLN-2 reaches higher performance than asingle layer, sometimes comparable to few-shot GPT-4 even when each LLM in thenetwork is smaller and less powerful. The DLN code is open source: .", "output": "Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, the main objective is to develop an algorithm capable ofidentifying and delineating tumor regions in breast ultrasound (BUS) andmammographic images. The technique employs two advanced deep learningarchitectures, namely U-Net and pretrained SAM, for tumor segmentation. TheU-Net model is specifically designed for medical image segmentation andleverages its deep convolutional neural network framework to extract meaningfulfeatures from input images. On the other hand, the pretrained SAM architectureincorporates a mechanism to capture spatial dependencies and generatesegmentation results. Evaluation is conducted on a diverse dataset containingannotated tumor regions in BUS and mammographic images, covering both benignand malignant tumors. This dataset enables a comprehensive assessment of thealgorithm's performance across different tumor types. Results demonstrate thatthe U-Net model outperforms the pretrained SAM architecture in accuratelyidentifying and segmenting tumor regions in both BUS and mammographic images.The U-Net exhibits superior performance in challenging cases involvingirregular shapes, indistinct boundaries, and high tumor heterogeneity. Incontrast, the pretrained SAM architecture exhibits limitations in accuratelyidentifying tumor areas, particularly for malignant tumors and objects withweak boundaries or complex shapes. These findings highlight the importance ofselecting appropriate deep learning architectures tailored for medical imagesegmentation. The U-Net model showcases its potential as a robust and accuratetool for tumor detection, while the pretrained SAM architecture suggests theneed for further improvements to enhance segmentation performance.", "output": "Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the proliferation of generative models, achieving fast samplingduring inference without compromising sample diversity and quality remainschallenging. Existing models such as Denoising Diffusion Probabilistic Models(DDPM) deliver high-quality, diverse samples but are slowed by an inherentlyhigh number of iterative steps. The Denoising Diffusion Generative AdversarialNetworks (DDGAN) attempted to circumvent this limitation by integrating a GANmodel for larger jumps in the diffusion process. However, DDGAN encounteredscalability limitations when applied to large datasets. To address theselimitations, we introduce a novel approach that tackles the problem by matchingimplicit and explicit factors. More specifically, our approach involvesutilizing an implicit model to match the marginal distributions of noisy dataand the explicit conditional distribution of the forward diffusion. Thiscombination allows us to effectively match the joint denoising distributions.Unlike DDPM but similar to DDGAN, we do not enforce a parametric distributionfor the reverse step, enabling us to take large steps during inference. Similarto the DDPM but unlike DDGAN, we take advantage of the exact form of thediffusion process. We demonstrate that our proposed method obtains comparablegenerative performance to diffusion-based models and vastly superior results tomodels with a small number of sampling steps.", "output": "Semi-Implicit Denoising Diffusion Models (SIDDMs)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FFCV, a library for easy and fast machine learning model training.FFCV speeds up model training by eliminating (often subtle) data bottlenecksfrom the training process. In particular, we combine techniques such as anefficient file storage format, caching, data pre-loading, asynchronous datatransfer, and just-in-time compilation to (a) make data loading and transfersignificantly more efficient, ensuring that GPUs can reach full utilization;and (b) offload as much data processing as possible to the CPU asynchronously,freeing GPU cycles for training. Using FFCV, we train ResNet-18 and ResNet-50on the ImageNet dataset with competitive tradeoff between accuracy and trainingtime. For example, we are able to train an ImageNet ResNet-50 model to 75% inonly 20 mins on a single machine. We demonstrate FFCV's performance,ease-of-use, extensibility, and ability to adapt to resource constraintsthrough several case studies. Detailed installation instructions,documentation, and Slack support channel are available at  .", "output": "FFCV: Accelerating Training by Removing Data Bottlenecks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We show that any randomized first-order algorithm which minimizes a$d$-dimensional, $1$-Lipschitz convex function over the unit ball must eitheruse $Omega(d^{2-delta})$ bits of memory or make $Omega(d^{1+delta/6-o(1)})$queries, for any constant $deltain (0,1)$ and when the precision $epsilon$is quasipolynomially small in $d$. Our result implies that cutting planemethods, which use $tilde{O}(d^2)$ bits of memory and $tilde{O}(d)$ queries,are Pareto-optimal among randomized first-order algorithms, and quadraticmemory is required to achieve optimal query complexity for convex optimization.", "output": "Memory-Query Tradeoffs for Randomized Convex Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Turbulent flow simulation plays a crucial role in various applications,including aircraft and ship design, industrial process optimization, andweather prediction. In this paper, we propose an advanced data-driven methodfor simulating turbulent flow, representing a significant improvement overexisting approaches.Our methodology combines the strengths of Video Prediction Transformer (VPTR)(Ye &amp; Bilodeau, 2022) and Multigrid Architecture (MgConv, MgResnet) (Ke et al.,2017). VPTR excels in capturing complex spatiotemporal dependencies andhandling large input data, making it a promising choice for turbulent flowprediction. Meanwhile, Multigrid Architecture utilizes multiple grids withdifferent resolutions to capture the multiscale nature of turbulent flows,resulting in more accurate and efficient simulations.Through our experiments, we demonstrate the effectiveness of our proposedapproach, named MGxTransformer, in accurately predicting velocity, temperature,and turbulence intensity for incompressible turbulent flows across variousgeometries and flow conditions. Our results exhibit superior accuracy comparedto other baselines, while maintaining computational efficiency.", "output": "Neural Multigrid Memory For Computational Fluid Dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We compute how small input perturbations affect the output of deep neuralnetworks, exploring an analogy between deep networks and dynamical systems,where the growth or decay of local perturbations is characterised byfinite-time Lyapunov exponents. We show that the maximal exponent formsgeometrical structures in input space, akin to coherent structures in dynamicalsystems. Ridges of large positive exponents divide input space into differentregions that the network associates with different classes. These ridgesvisualise the geometry that deep networks construct in input space, sheddinglight on the fundamental mechanisms underlying their learning capabilities.", "output": "Finite-time Lyapunov exponents of deep neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Complex, long-horizon planning and its combinatorial nature pose steepchallenges for learning-based agents. Difficulties in such settings areexacerbated in low data regimes where over-fitting stifles generalization andcompounding errors hurt accuracy. In this work, we explore the use of an oftenunused source of auxiliary supervision: language. Inspired by recent advancesin transformer-based models, we train agents with an instruction predictionloss that encourages learning temporally extended representations that operateat a high level of abstraction. Concretely, we demonstrate that instructionmodeling significantly improves performance in planning environments whentraining with a limited number of demonstrations on the BabyAI and Crafterbenchmarks. In further analysis we find that instruction modeling is mostimportant for tasks that require complex reasoning, while understandablyoffering smaller gains in environments that require simple plans. More detailsand code can be found at ", "output": "Improving Long-Horizon Imitation Through Instruction Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The growth of network-connected devices is creating an explosion of data,known as big data, and posing significant challenges to efficient dataanalysis. This data is generated continuously, creating a dynamic flow known asa data stream. The characteristics of a data stream may change dynamically, andthis change is known as concept drift. Consequently, a method for handling datastreams must efficiently reduce their volume while dynamically adapting tothese changing characteristics. This paper proposes a simple online vectorquantization method for concept drift. The proposed method identifies andreplaces units with low win probability through remove-birth updating, thusachieving a rapid adaptation to concept drift. Furthermore, the results of thisstudy show that the proposed method can generate minimal dead units even in thepresence of concept drift. This study also suggests that some metricscalculated from the proposed method will be helpful for drift detection.", "output": "An efficient and straightforward online quantization method for a data stream through remove-birth updating."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, studies such ascite{carmon2019unlabeled,gowal2021improving,xing2022artificial} havedemonstrated that incorporating additional real or generated data withpseudo-labels can enhance adversarial training through a two-stage trainingapproach. In this paper, we perform a theoretical analysis of the asymptoticbehavior of this method in high-dimensional linear regression. While adouble-descent phenomenon can be observed in ridgeless training, with anappropriate $mathcal{L}_2$ regularization, the two-stage adversarial trainingachieves a better performance. Finally, we derive a shortcut cross-validationformula specifically tailored for the two-stage training method.", "output": "Adversarial Training with Generated Data in High-Dimensional Regression: An Asymptotic Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When analyzing real-world data it is common to work with event ensembles,which comprise sets of observations that collectively constrain the parametersof an underlying model of interest. Such models often have a hierarchicalstructure, where \"local\" parameters impact individual events and \"global\"parameters influence the entire dataset. We introduce practical approaches foroptimal dataset-wide probabilistic inference in cases where the likelihood isintractable, but simulations can be realized via forward modeling. We constructneural estimators for the likelihood(-ratio) or posterior and show thatexplicitly accounting for the model's hierarchical structure can lead totighter parameter constraints. We ground our discussion using case studies fromthe physical sciences, focusing on examples from particle physics (particlecollider data) and astrophysics (strong gravitational lensing observations).", "output": "Hierarchical Neural Simulation-Based Inference Over Event Ensembles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rapid and accurate building damage assessments from high-resolution satelliteimagery following a natural disaster is essential to inform and optimize firstresponder efforts. However, performing such building damage assessments in anautomated manner is non-trivial due to the challenges posed by variations indisaster-specific damage, diversity in satellite imagery, and the dearth ofextensive, labeled datasets. To circumvent these issues, this paper introducesa human-in-the-loop workflow for rapidly training building damage assessmentmodels after a natural disaster. This article details a case study using thisworkflow, executed in partnership with the American Red Cross during a tornadoevent in Rolling Fork, Mississippi in March, 2023. The output from ourhuman-in-the-loop modeling process achieved a precision of 0.86 and recall of0.80 for damaged buildings when compared to ground truth data collectedpost-disaster. This workflow was implemented end-to-end in under 2 hours persatellite imagery scene, highlighting its potential for real-time deployment.", "output": "Rapid building damage assessment workflow: An implementation for the 2023 Rolling Fork, Mississippi tornado event."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement Learning (RL) algorithms have shown tremendous success insimulation environments, but their application to real-world problems facessignificant challenges, with safety being a major concern. In particular,enforcing state-wise constraints is essential for many challenging tasks suchas autonomous driving and robot manipulation. However, existing safe RLalgorithms under the framework of Constrained Markov Decision Process (CMDP) donot consider state-wise constraints. To address this gap, we propose State-wiseConstrained Policy Optimization (SCPO), the first general-purpose policy searchalgorithm for state-wise constrained reinforcement learning. SCPO providesguarantees for state-wise constraint satisfaction in expectation. Inparticular, we introduce the framework of Maximum Markov Decision Process, andprove that the worst-case safety violation is bounded under SCPO. Wedemonstrate the effectiveness of our approach on training neural networkpolicies for extensive robot locomotion tasks, where the agent must satisfy avariety of state-wise safety constraints. Our results show that SCPOsignificantly outperforms existing methods and can handle state-wiseconstraints in high-dimensional robotics tasks.", "output": "State-wise Constrained Policy Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern foundation model architectures rely on attention mechanisms toeffectively capture context. However, these methods require linear or quadraticmemory in terms of the number of inputs/datapoints, limiting theirapplicability in low-compute domains. In this work, we propose Constant MemoryAttention Block (CMAB), a novel general-purpose attention block that computesits output in constant memory and performs updates in constant computation.Highlighting CMABs efficacy, we introduce methods for Neural Processes andTemporal Point Processes. Empirically, we show our proposed methods achieveresults competitive with state-of-the-art while being significantly more memoryefficient.", "output": "Constant Memory Attention Block."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks are typically sensitive to small input perturbations, leadingto unexpected or brittle behaviour. We present RobustNeuralNetworks.jl: a Juliapackage for neural network models that are constructed to naturally satisfy aset of user-defined robustness constraints. The package is based on therecently proposed Recurrent Equilibrium Network (REN) and Lipschitz-BoundedDeep Network (LBDN) model classes, and is designed to interface directly withJulia's most widely-used machine learning package, Flux.jl. We discuss thetheory behind our model parameterization, give an overview of the package, andprovide a tutorial demonstrating its use in image classification, reinforcementlearning, and nonlinear state-observer design.", "output": "RobustNeuralNetworks.jl: a Package for Machine Learning and Data-Driven Control with Certified Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the great success of pre-trained language models, it is still achallenge to use these models for continual learning, especially for theclass-incremental learning (CIL) setting due to catastrophic forgetting (CF).This paper reports our finding that if we formulate CIL as a continual labelgeneration problem, CF is drastically reduced and the generalizablerepresentations of pre-trained models can be better retained. We thus propose anew CIL method (VAG) that also leverages the sparsity of vocabulary to focusthe generation and creates pseudo-replay samples by using label semantics.Experimental results show that VAG outperforms baselines by a large margin.", "output": "Class-Incremental Learning based on Label Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The high communication cost of sending model updates from the clients to theserver is a significant bottleneck for scalable federated learning (FL). Amongexisting approaches, state-of-the-art bitrate-accuracy tradeoffs have beenachieved using stochastic compression methods -- in which the client $n$ sendsa sample from a client-only probability distribution $q_{phi^{(n)}}$, and theserver estimates the mean of the clients' distributions using these samples.However, such methods do not take full advantage of the FL setup where theserver, throughout the training process, has side information in the form of apre-data distribution $p_{theta}$ that is close to the client's distribution$q_{phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploitthis closeness between the clients' distributions $q_{phi^{(n)}}$'s and theside information $p_{theta}$ at the server, and propose a framework thatrequires approximately $D_{KL}(q_{phi^{(n)}}|| p_{theta})$ bits ofcommunication. We show that our method can be integrated into many existingstochastic compression frameworks such as FedPM, Federated SGLD, and QSGD toattain the same (and often higher) test accuracy with up to $50$ timesreduction in the bitrate.", "output": "Communication-Efficient Federated Learning through Importance Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autoencoders have been extensively used in the development of recent anomalydetection techniques. The premise of their application is based on the notionthat after training the autoencoder on normal training data, anomalous inputswill exhibit a significant reconstruction error. Consequently, this enables aclear differentiation between normal and anomalous samples. In practice,however, it is observed that autoencoders can generalize beyond the normalclass and achieve a small reconstruction error on some of the anomaloussamples. To improve the performance, various techniques propose additionalcomponents and more sophisticated training procedures. In this work, we proposea remarkably straightforward alternative: instead of adding neural networkcomponents, involved computations, and cumbersome training, we complement thereconstruction loss with a computationally light term that regulates the normof representations in the latent space. The simplicity of our approachminimizes the requirement for hyperparameter tuning and customization for newapplications which, paired with its permissive data modality constraint,enhances the potential for successful adoption across a broad range ofapplications. We test the method on various visual and tabular benchmarks anddemonstrate that the technique matches and frequently outperforms alternatives.We also provide a theoretical analysis and numerical simulations that helpdemonstrate the underlying process that unfolds during training and how it canhelp with anomaly detection. This mitigates the black-box nature ofautoencoder-based anomaly detection algorithms and offers an avenue for furtherinvestigation of advantages, fail cases, and potential new directions.", "output": "Targeted collapse regularized autoencoder for anomaly detection: black hole at the center."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This report gives a summary of two problems about graph convolutionalnetworks (GCNs): over-smoothing and heterophily challenges, and outlines futuredirections to explore.", "output": "On Addressing the Limitations of Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies the challenging continual learning (CL) setting of ClassIncremental Learning (CIL). CIL learns a sequence of tasks consisting ofdisjoint sets of concepts or classes. At any time, a single model is built thatcan be applied to predict/classify test instances of any classes learned thusfar without providing any task related information for each test instance.Although many techniques have been proposed for CIL, they are mostly empirical.It has been shown recently that a strong CIL system needs a strong within-taskprediction (WP) and a strong out-of-distribution (OOD) detection for each task.However, it is still not known whether CIL is actually learnable. This papershows that CIL is learnable. Based on the theory, a new CIL algorithm is alsoproposed. Experimental results demonstrate its effectiveness.", "output": "Learnability and Algorithm for Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop a fitted value iteration (FVI) method to compute bicausal optimaltransport (OT) where couplings have an adapted structure. Based on the dynamicprogramming formulation, FVI adopts a function class to approximate the valuefunctions in bicausal OT. Under the concentrability condition and approximatecompleteness assumption, we prove the sample complexity using (local)Rademacher complexity. Furthermore, we demonstrate that multilayer neuralnetworks with appropriate structures satisfy the crucial assumptions requiredin sample complexity proofs. Numerical experiments reveal that FVI outperformslinear programming and adapted Sinkhorn methods in scalability as the timehorizon increases, while still maintaining acceptable accuracy.", "output": "Fitted Value Iteration Methods for Bicausal Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sentiment analysis is a vital tool for uncovering insights from financialarticles, news, and social media, shaping our understanding of marketmovements. Despite the impressive capabilities of large language models (LLMs)in financial natural language processing (NLP), they still struggle withaccurately interpreting numerical values and grasping financial context,limiting their effectiveness in predicting financial sentiment. In this paper,we introduce a simple yet effective instruction tuning approach to addressthese issues. By transforming a small portion of supervised financial sentimentanalysis data into instruction data and fine-tuning a general-purpose LLM withthis method, we achieve remarkable advancements in financial sentimentanalysis. In the experiment, our approach outperforms state-of-the-artsupervised sentiment analysis models, as well as widely used LLMs like ChatGPTand LLaMAs, particularly in scenarios where numerical understanding andcontextual comprehension are vital.", "output": "Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we have developed an incremental machine learning (ML) methodthat efficiently obtains the optimal model when a small number of instances orfeatures are added or removed. This problem holds practical importance in modelselection, such as cross-validation (CV) and feature selection. Among the classof ML methods known as linear estimators, there exists an efficient modelupdate framework called the low-rank update that can effectively handle changesin a small number of rows and columns within the data matrix. However, for MLmethods beyond linear estimators, there is currently no comprehensive frameworkavailable to obtain knowledge about the updated solution within a specificcomputational complexity. In light of this, our study introduces a methodcalled the Generalized Low-Rank Update (GLRU) which extends the low-rank updateframework of linear estimators to ML methods formulated as a certain class ofregularized empirical risk minimization, including commonly used methods suchas SVM and logistic regression. The proposed GLRU method not only expands therange of its applicability but also provides information about the updatedsolutions with a computational complexity proportional to the amount of datasetchanges. To demonstrate the effectiveness of the GLRU method, we conductexperiments showcasing its efficiency in performing cross-validation andfeature selection compared to other baseline methods.", "output": "Generalized Low-Rank Update: Model Parameter Bounds for Low-Rank Training Data Modifications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks employ spurious correlations in their predictions, resultingin decreased performance when these correlations do not hold. Recent workssuggest fixing pretrained representations and training a classification headthat does not use spurious features. We investigate how spurious features arerepresented in pretrained representations and explore strategies for removinginformation about spurious features. Considering the Waterbirds dataset and afew pretrained representations, we find that even with full knowledge ofspurious features, their removal is not straightforward due to entangledrepresentation. To address this, we propose a linear autoencoder trainingmethod to separate the representation into core, spurious, and other features.We propose two effective spurious feature removal approaches that are appliedto the encoding and significantly improve classification performance measuredby worst group accuracy.", "output": "Identifying and Disentangling Spurious Features in Pretrained Image Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study problem of estimating a sparse regression vector withcorrect support in the presence of outlier samples. The inconsistency oflasso-type methods is well known in this scenario. We propose a combinatorialversion of outlier-robust lasso which also identifies clean samples.Subsequently, we use these clean samples to make a good estimation. We alsoprovide a novel invex relaxation for the combinatorial problem and provideprovable theoretical guarantees for this relaxation. Finally, we conductexperiments to validate our theory and compare our results against standardlasso.", "output": "Outlier-robust Estimation of a Sparse Linear Model Using Invexity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graphs represent real-world entities and their relations in asemantically-rich structure supported by ontologies. Exploring this data withmachine learning methods often relies on knowledge graph embeddings, whichproduce latent representations of entities that preserve structural and localgraph neighbourhood properties, but sacrifice explainability. However, in taskssuch as link or relation prediction, understanding which specific featuresbetter explain a relation is crucial to support complex or criticalapplications.We propose SEEK, a novel approach for explainable representations to supportrelation prediction in knowledge graphs. It is based on identifying relevantshared semantic aspects (i.e., subgraphs) between entities and learningrepresentations for each subgraph, producing a multi-faceted and explainablerepresentation.We evaluate SEEK on two real-world highly complex relation prediction tasks:protein-protein interaction prediction and gene-disease association prediction.Our extensive analysis using established benchmarks demonstrates that SEEKachieves significantly better performance than standard learning representationmethods while identifying both sufficient and necessary explanations based onshared semantic aspects.", "output": "Explainable Representations for Relation Prediction in Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning algorithms are powerful tools for data driven tasks such asimage classification and feature detection, however their vulnerability toadversarial examples - input samples manipulated to fool the algorithm -remains a serious challenge. The integration of machine learning with quantumcomputing has the potential to yield tools offering not only better accuracyand computational efficiency, but also superior robustness against adversarialattacks. Indeed, recent work has employed quantum mechanical phenomena todefend against adversarial attacks, spurring the rapid development of the fieldof quantum adversarial machine learning (QAML) and potentially yielding a newsource of quantum advantage. Despite promising early results, there remainchallenges towards building robust real-world QAML tools. In this review wediscuss recent progress in QAML and identify key challenges. We also suggestfuture research directions which could determine the route to practicality forQAML approaches as quantum computing hardware scales up and noise levels arereduced.", "output": "Towards quantum enhanced adversarial robustness in machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vector embeddings have become ubiquitous tools for many language-relatedtasks. A leading embedding model is OpenAI's text-ada-002 which can embedapproximately 6,000 words into a 1,536-dimensional vector. While powerful,text-ada-002 is not open source and is only available via API. We trained asimple neural network to convert open-source 768-dimensional MPNet embeddingsinto text-ada-002 embeddings. We compiled a subset of 50,000 online foodreviews. We calculated MPNet and text-ada-002 embeddings for each review andtrained a simple neural network to for 75 epochs. The neural network wasdesigned to predict the corresponding text-ada-002 embedding for a given MPNETembedding. Our model achieved an average cosine similarity of 0.932 on 10,000unseen reviews in our held-out test dataset. We manually assessed the qualityof our predicted embeddings for vector search over text-ada-002-embeddedreviews. While not as good as real text-ada-002 embeddings, predictedembeddings were able to retrieve highly relevant reviews. Our final model,Vec2Vec, is lightweight (&lt;80 MB) and fast. Future steps include training aneural network with a more sophisticated architecture and a larger dataset ofpaired embeddings to achieve greater performance. The ability to convertbetween and align embedding spaces may be helpful for interoperability,limiting dependence on proprietary models, protecting data privacy, reducingcosts, and offline operations.", "output": "Vec2Vec: A Compact Neural Network Approach for Transforming Text Embeddings with High Fidelity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The execution of large deep neural networks (DNN) at mobile edge devicesrequires considerable consumption of critical resources, such as energy, whileimposing demands on hardware capabilities. In approaches based on edgecomputing the execution of the models is offloaded to a compute-capable devicepositioned at the edge of 5G infrastructures. The main issue of the latterclass of approaches is the need to transport information-rich signals overwireless links with limited and time-varying capacity. The recent splitcomputing paradigm attempts to resolve this impasse by distributing theexecution of DNN models across the layers of the systems to reduce the amountof data to be transmitted while imposing minimal computing load on mobiledevices. In this context, we propose a novel split computing approach based onslimmable ensemble encoders. The key advantage of our design is the ability toadapt computational load and transmitted data size in real-time with minimaloverhead and time. This is in contrast with existing approaches, where the sameadaptation requires costly context switching and model loading. Moreover, ourmodel outperforms existing solutions in terms of compression efficacy andexecution time, especially in the context of weak mobile devices. We present acomprehensive comparison with the most advanced split computing solutions, aswell as an experimental evaluation on GPU-less devices.", "output": "Slimmable Encoders for Flexible Split DNNs in Bandwidth and Resource Constrained IoT Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop an approach to efficiently grow neural networks, within whichparameterization and optimization strategies are designed by considering theireffects on the training dynamics. Unlike existing growing methods, which followsimple replication heuristics or utilize auxiliary gradient-based localoptimization, we craft a parameterization scheme which dynamically stabilizesweight, activation, and gradient scaling as the architecture evolves, andmaintains the inference functionality of the network. To address theoptimization difficulty resulting from imbalanced training effort distributedto subnetworks fading in at different growth phases, we propose a learning rateadaption mechanism that rebalances the gradient contribution of these separatesubcomponents. Experimental results show that our method achieves comparable orbetter accuracy than training large fixed-size models, while saving asubstantial portion of the original computation budget for training. Wedemonstrate that these gains translate into real wall-clock training speedups.", "output": "Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomaly detection plays an increasingly important role in various fields forcritical tasks such as intrusion detection in cybersecurity, financial riskdetection, and human health monitoring. A variety of anomaly detection methodshave been proposed, and a category based on the isolation forest mechanismstands out due to its simplicity, effectiveness, and efficiency, e.g., iForestis often employed as a state-of-the-art detector for real deployment. While themajority of isolation forests use the binary structure, a framework LSHiForesthas demonstrated that the multi-fork isolation tree structure can lead tobetter detection performance. However, there is no theoretical work answeringthe fundamentally and practically important question on the optimal treestructure for an isolation forest with respect to the branching factor. In thispaper, we establish a theory on isolation efficiency to answer the question anddetermine the optimal branching factor for an isolation tree. Based on thetheoretical underpinning, we design a practical optimal isolation forestOptIForest incorporating clustering based learning to hash which enables moreinformation to be learned from data for better isolation quality. The rationaleof our approach relies on a better bias-variance trade-off achieved by biasreduction in OptIForest. Extensive experiments on a series of benchmarkingdatasets for comparative and ablation studies demonstrate that our approach canefficiently and robustly achieve better detection performance in general thanthe state-of-the-arts including the deep learning based methods.", "output": "OptIForest: Optimal Isolation Forest for Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic singing voice understanding tasks, such as singer identification,singing voice transcription, and singing technique classification, benefit fromdata-driven approaches that utilize deep learning techniques. These approacheswork well even under the rich diversity of vocal and noisy samples owing totheir representation ability. However, the limited availability of labeled dataremains a significant obstacle to achieving satisfactory performance. In recentyears, self-supervised learning models (SSL models) have been trained usinglarge amounts of unlabeled data in the field of speech processing and musicclassification. By fine-tuning these models for the target tasks, comparableperformance to conventional supervised learning can be achieved with limitedtraining data. Therefore, in this paper, we investigate the effectiveness ofSSL models for various singing voice recognition tasks. We report the resultsof experiments comparing SSL models for three different tasks (i.e., singeridentification, singing voice transcription, and singing techniqueclassification) as initial exploration and aim to discuss these findings.Experimental results show that each SSL model achieves comparable performanceand sometimes outperforms compared to state-of-the-art methods on each task. Wealso conducted a layer-wise analysis to further understand the behavior of theSSL models.", "output": "Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A pooling operation is essential for effective graph-level representationlearning, where the node drop pooling has become one mainstream graph poolingtechnology. However, current node drop pooling methods usually keep the top-knodes according to their significance scores, which ignore the graph diversityin terms of the node features and the graph structures, thus resulting insuboptimal graph-level representations. To address the aforementioned issue, wepropose a novel plug-and-play score scheme and refer to it as MID, whichconsists of a textbf{M}ultidimensional score space with two operations,textit{i.e.}, fltextbf{I}pscore and textbf{D}ropscore. Specifically, themultidimensional score space depicts the significance of nodes through multiplecriteria; the flipscore encourages the maintenance of dissimilar node features;and the dropscore forces the model to notice diverse graph structures insteadof being stuck in significant local structures. To evaluate the effectivenessof our proposed MID, we perform extensive experiments by applying it to a widevariety of recent node drop pooling methods, including TopKPool, SAGPool,GSAPool, and ASAP. Specifically, the proposed MID can efficiently andconsistently achieve about 2.8% average improvements over the above fourmethods on seventeen real-world graph classification datasets, including foursocial datasets (IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, and COLLAB), andthirteen biochemical datasets (D&amp;D, PROTEINS, NCI1, MUTAG, PTC-MR, NCI109,ENZYMES, MUTAGENICITY, FRANKENSTEIN, HIV, BBBP, TOXCAST, and TOX21). Code isavailable at~url{", "output": "On Exploring Node-feature and Graph-structure Diversities for Node Drop Graph Pooling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a novel deep reinforcement learning (RL) approach calledMovement Prmitive-based Planning Policy (MP3). By integrating movementprimitives (MPs) into the deep RL framework, MP3 enables the generation ofsmooth trajectories throughout the whole learning process while effectivelylearning from sparse and non-Markovian rewards. Additionally, MP3 maintains thecapability to adapt to changes in the environment during execution. Althoughmany early successes in robot RL have been achieved by combining RL with MPs,these approaches are often limited to learning single stroke-based motions,lacking the ability to adapt to task variations or adjust motions duringexecution. Building upon our previous work, which introduced an episode-basedRL method for the non-linear adaptation of MP parameters to different taskvariations, this paper extends the approach to incorporating replanningstrategies. This allows adaptation of the MP parameters throughout motionexecution, addressing the lack of online motion adaptation in stochasticdomains requiring feedback. We compared our approach against state-of-the-artdeep RL and RL with MPs methods. The results demonstrated improved performancein sophisticated, sparse reward settings and in domains requiring replanning.", "output": "MP3: Movement Primitive-Based (Re-)Planning Policy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have shown that line search methods can speed up StochasticGradient Descent (SGD) and Adam in modern over-parameterized settings. However,existing line searches may take steps that are smaller than necessary sincethey require a monotone decrease of the (mini-)batch objective function. Weexplore nonmonotone line search methods to relax this condition and possiblyaccept larger step sizes. Despite the lack of a monotonic decrease, we provethe same fast rates of convergence as in the monotone case. Our experimentsshow that nonmonotone methods improve the speed of convergence andgeneralization properties of SGD/Adam even beyond the previous monotone linesearches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtainedby combining a nonmonotone line search with a Polyak initial step size.Furthermore, we develop a new resetting technique that in the majority of theiterations reduces the amount of backtracks to zero while still maintaining alarge initial step size. To the best of our knowledge, a first runtimecomparison shows that the epoch-wise advantage of line-search-based methodsgets reflected in the overall computational time.", "output": "Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline reinforcement learning (RL) aims to learn a policy using onlypre-collected and fixed data. Although avoiding the time-consuming onlineinteractions in RL, it poses challenges for out-of-distribution (OOD) stateactions and often suffers from data inefficiency for training. Despite manyefforts being devoted to addressing OOD state actions, the latter (datainefficiency) receives little attention in offline RL. To address this, thispaper proposes the cross-domain offline RL, which assumes offline dataincorporate additional source-domain data from varying transition dynamics(environments), and expects it to contribute to the offline data efficiency. Todo so, we identify a new challenge of OOD transition dynamics, beyond thecommon OOD state actions issue, when utilizing cross-domain offline data. Then,we propose our method BOSA, which employs two support-constrained objectives toaddress the above OOD issues. Through extensive experiments in the cross-domainoffline RL setting, we demonstrate BOSA can greatly improve offline dataefficiency: using only 10% of the target data, BOSA could achieve {74.4%} ofthe SOTA offline RL performance that uses 100% of the target data.Additionally, we also show BOSA can be effortlessly plugged into model-basedoffline RL and noising data augmentation techniques (used for generatingsource-domain data), which naturally avoids the potential dynamics mismatchbetween target-domain data and newly generated source-domain data.", "output": "Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, we have witnessed generative retrieval increasingly gainingattention in the information retrieval (IR) field, which retrieves documents bydirectly generating their identifiers. So far, much effort has been devoted todeveloping effective generative retrieval models. There has been less attentionpaid to the robustness perspective. When a new retrieval paradigm enters intothe real-world application, it is also critical to measure theout-of-distribution (OOD) generalization, i.e., how would generative retrievalmodels generalize to new distributions. To answer this question, firstly, wedefine OOD robustness from three perspectives in retrieval problems: 1) Thequery variations; 2) The unforeseen query types; and 3) The unforeseen tasks.Based on this taxonomy, we conduct empirical studies to analyze the OODrobustness of several representative generative retrieval models against denseretrieval models. The empirical results indicate that the OOD robustness ofgenerative retrieval models requires enhancement. We hope studying the OODrobustness of generative retrieval models would be advantageous to the IRcommunity.", "output": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Editing a local region or a specific object in a 3D scene represented by aNeRF is challenging, mainly due to the implicit nature of the scenerepresentation. Consistently blending a new realistic object into the sceneadds an additional level of difficulty. We present Blended-NeRF, a robust andflexible framework for editing a specific region of interest in an existingNeRF scene, based on text prompts or image patches, along with a 3D ROI box.Our method leverages a pretrained language-image model to steer the synthesistowards a user-provided text prompt or image patch, along with a 3D MLP modelinitialized on an existing NeRF scene to generate the object and blend it intoa specified region in the original scene. We allow local editing by localizinga 3D ROI box in the input scene, and seamlessly blend the content synthesizedinside the ROI with the existing scene using a novel volumetric blendingtechnique. To obtain natural looking and view-consistent results, we leverageexisting and new geometric priors and 3D augmentations for improving the visualfidelity of the final result.We test our framework both qualitatively and quantitatively on a variety ofreal 3D scenes and text prompts, demonstrating realistic multi-view consistentresults with much flexibility and diversity compared to the baselines. Finally,we show the applicability of our framework for several 3D editing applications,including adding new objects to a scene, removing/replacing/altering existingobjects, and texture conversion.", "output": "Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Decentralized deep learning requires dealing with non-iid data acrossclients, which may also change over time due to temporal shifts. While non-iiddata has been extensively studied in distributed settings, temporal shifts havereceived no attention. To the best of our knowledge, we are first with tacklingthe novel and challenging problem of decentralized learning with non-iid anddynamic data. We propose a novel algorithm that can automatically discover andadapt to the evolving concepts in the network, without any prior knowledge orestimation of the number of concepts. We evaluate our algorithm on standardbenchmark datasets and demonstrate that it outperforms previous methods fordecentralized learning.", "output": "Concept-aware clustering for decentralized deep learning under temporal shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the problem of identifying the optimal policy with a fixedconfidence level in a multi-armed bandit setup, when emph{the arms are subjectto linear constraints}. Unlike the standard best-arm identification problemwhich is well studied, the optimal policy in this case may not be deterministicand could mix between several arms. This changes the geometry of the problemwhich we characterize via an information-theoretic lower bound. We introducetwo asymptotically optimal algorithms for this setting, one based on theTrack-and-Stop method and the other based on a game-theoretic approach. Boththese algorithms try to track an optimal allocation based on the lower boundand computed by a weighted projection onto the boundary of a normal cone.Finally, we provide empirical results that validate our bounds and visualizehow constraints change the hardness of the problem.", "output": "Pure Exploration in Bandits with Linear Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Full waveform inversion (FWI) has the potential to provide high-resolutionsubsurface model estimations. However, due to limitations in observation, e.g.,regional noise, limited shots or receivers, and band-limited data, it is hardto obtain the desired high-resolution model with FWI. To address thischallenge, we propose a new paradigm for FWI regularized by generativediffusion models. Specifically, we pre-train a diffusion model in a fullyunsupervised manner on a prior velocity model distribution that represents ourexpectations of the subsurface and then adapt it to the seismic observations byincorporating the FWI into the sampling process of the generative diffusionmodels. What makes diffusion models uniquely appropriate for such animplementation is that the generative process retains the form and dimensionsof the velocity model. Numerical examples demonstrate that our method canoutperform the conventional FWI with only negligible additional computationalcost. Even in cases of very sparse observations or observations with strongnoise, the proposed method could still reconstruct a high-quality subsurfacemodel. Thus, we can incorporate our prior expectations of the solutions in anefficient manner. We further test this approach on field data, whichdemonstrates the effectiveness of the proposed method.", "output": "A prior regularized full waveform inversion using generative diffusion models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recommender systems are designed to predict user preferences over collectionsof items. These systems process users' previous interactions to decide whichitems should be ranked higher to satisfy their desires. An ensemble recommendersystem can achieve great recommendation performance by effectively combiningthe decisions generated by individual models. In this paper, we propose a novelensemble recommender system that combines predictions made by different modelsinto a unified hypergraph ranking framework. This is the first time thathypergraph ranking has been employed to model an ensemble of recommendersystems. Hypergraphs are generalizations of graphs where multiple vertices canbe connected via hyperedges, efficiently modeling high-order relations. Wedifferentiate real and predicted connections between users and items byassigning different hyperedge weights to individual recommender systems. Weperform experiments using four datasets from the fields of movie, music andnews media recommendation. The obtained results show that the ensemblehypergraph ranking method generates more accurate recommendations compared tothe individual models and a weighted hybrid approach. The assignment ofdifferent hyperedge weights to the ensemble hypergraph further improves theperformance compared to a setting with identical hyperedge weights.", "output": "HypeRS: Building a Hypergraph-driven ensemble Recommender System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent research in representation learning utilizes large databases ofproteins or molecules to acquire knowledge of drug and protein structuresthrough unsupervised learning techniques. These pre-trained representationshave proven to significantly enhance the accuracy of subsequent tasks, such aspredicting the affinity between drugs and target proteins. In this study, wedemonstrate that by incorporating knowledge graphs from diverse sources andmodalities into the sequences or SMILES representation, we can further enrichthe representation and achieve state-of-the-art results on establishedbenchmark datasets. We provide preprocessed and integrated data obtained from 7public sources, which encompass over 30M triples. Additionally, we makeavailable the pre-trained models based on this data, along with the reportedoutcomes of their performance on three widely-used benchmark datasets fordrug-target binding affinity prediction found in the Therapeutic Data Commons(TDC) benchmarks. Additionally, we make the source code for training models onbenchmark datasets publicly available. Our objective in releasing thesepre-trained models, accompanied by clean data for model pretraining andbenchmark results, is to encourage research in knowledge-enhancedrepresentation learning.", "output": "Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spaces with locally varying scale of measurement, like multidimensionalstructures with differently scaled dimensions, are pretty common in statisticsand machine learning. Nevertheless, it is still understood as an open questionhow to exploit the entire information encoded in them properly. We address thisproblem by considering an order based on (sets of) expectations of randomvariables mapping into such non-standard spaces. This order contains stochasticdominance and expectation order as extreme cases when no, or respectivelyperfect, cardinal structure is given. We derive a (regularized) statisticaltest for our proposed generalized stochastic dominance (GSD) order,operationalize it by linear optimization, and robustify it by impreciseprobability models. Our findings are illustrated with data frommultidimensional poverty measurement, finance, and medicine.", "output": "Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Limit order books are a fundamental and widespread market mechanism. Thispaper investigates the use of conditional generative models for order booksimulation. For developing a trading agent, this approach has drawn recentattention as an alternative to traditional backtesting due to its ability toreact to the presence of the trading agent. Using a state-of-the-art CGAN (fromColetta et al. (2022)), we explore its dependence upon input features, whichhighlights both strengths and weaknesses. To do this, we use \"adversarialattacks\" on the model's features and its mechanism. We then show how theseinsights can be used to improve the CGAN, both in terms of its realism androbustness. We finish by laying out a roadmap for future work.", "output": "Conditional Generators for Limit Order Book Environments: Explainability, Challenges, and Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of 'explainable' artificial intelligence (XAI) has produced highlycited methods that seek to make the decisions of complex machine learning (ML)methods 'understandable' to humans, for example by attributing 'importance'scores to input features. Yet, a lack of formal underpinning leaves it unclearas to what conclusions can safely be drawn from the results of a given XAImethod and has also so far hindered the theoretical verification and empiricalvalidation of XAI methods. This means that challenging non-linear problems,typically solved by deep neural networks, presently lack appropriate remedies.Here, we craft benchmark datasets for three different non-linear classificationscenarios, in which the important class-conditional features are known bydesign, serving as ground truth explanations. Using novel quantitative metrics,we benchmark the explanation performance of a wide set of XAI methods acrossthree deep learning model architectures. We show that popular XAI methods areoften unable to significantly outperform random performance baselines and edgedetection methods. Moreover, we demonstrate that explanations derived fromdifferent model architectures can be vastly different; thus, prone tomisinterpretation even under controlled conditions.", "output": "XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurately predicting the elastic properties of crystalline solids is vitalfor computational materials science. However, traditional atomistic scale abinitio approaches are computationally intensive, especially for studyingcomplex materials with a large number of atoms in a unit cell. We introduce anovel data-driven approach to efficiently predict the elastic properties ofcrystal structures using SE(3)-equivariant graph neural networks (GNNs). Thisapproach yields important scalar elastic moduli with the accuracy comparable torecent data-driven studies. Importantly, our symmetry-aware GNNs model alsoenables the prediction of the strain energy density (SED) and the associatedelastic constants, the fundamental tensorial quantities that are significantlyinfluenced by a material's crystallographic group. The model consistentlydistinguishes independent elements of SED tensors, in accordance with thesymmetry of the crystal structures. Finally, our deep learning model possessesmeaningful latent features, offering an interpretable prediction of the elasticproperties.", "output": "StrainNet: Predicting crystal structure elastic properties using SE(3)-equivariant graph neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cascade systems comprise a two-model sequence, with a lightweight modelprocessing all samples and a heavier, higher-accuracy model conditionallyrefining harder samples to improve accuracy. By placing the light model on thedevice side and the heavy model on a server, model cascades constitute a widelyused distributed inference approach. With the rapid expansion of intelligentindoor environments, such as smart homes, the new setting of Multi-DeviceCascade is emerging where multiple and diverse devices are to simultaneouslyuse a shared heavy model on the same server, typically located within or closeto the consumer environment. This work presents MultiTASC, amulti-tenancy-aware scheduler that adaptively controls the forwarding decisionfunctions of the devices in order to maximize the system throughput, whilesustaining high accuracy and low latency. By explicitly considering deviceheterogeneity, our scheduler improves the latency service-level objective (SLO)satisfaction rate by 20-25 percentage points (pp) over state-of-the-art cascademethods in highly heterogeneous setups, while serving over 40 devices,showcasing its scalability.", "output": "MultiTASC: A Multi-Tenancy-Aware Scheduler for Cascaded DNN Inference at the Consumer Edge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The storage, management, and application of massive spatio-temporal data arewidely applied in various practical scenarios, including public safety.However, due to the unique spatio-temporal distribution characteristics ofre-al-world data, most existing methods have limitations in terms of thespatio-temporal proximity of data and load balancing in distributed storage.There-fore, this paper proposes an efficient partitioning method of large-scalepublic safety spatio-temporal data based on information loss constraints(IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporalpoint da-ta by combining the spatio-temporal partitioning module (STPM) withthe graph partitioning module (GPM). This approach can significantly reduce thescale of data while maintaining the model's accuracy, in order to improve thepartitioning efficiency. It can also ensure the load balancing of distributedstorage while maintaining spatio-temporal proximity of the data partitioningresults. This method provides a new solution for distributed storage ofmas-sive spatio-temporal data. The experimental results on multiple real-worldda-tasets demonstrate the effectiveness and superiority of IFL-LSTP.", "output": "Efficient Partitioning Method of Large-Scale Public Safety Spatio-Temporal Data based on Information Loss Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning is a distributed machine learning technology, whichrealizes the balance between data privacy protection and data sharingcomputing. To protect data privacy, feder-ated learning learns shared models bylocally executing distributed training on participating devices and aggregatinglocal models into global models. There is a problem in federated learning, thatis, the negative impact caused by the non-independent and identicaldistribu-tion of data across different user terminals. In order to alleviatethis problem, this paper pro-poses a strengthened federation aggregation methodbased on adaptive OPTICS clustering. Specifically, this method perceives theclustering environment as a Markov decision process, and models the adjustmentprocess of parameter search direction, so as to find the best clus-teringparameters to achieve the best federated aggregation method. The corecontribution of this paper is to propose an adaptive OPTICS clusteringalgorithm for federated learning. The algorithm combines OPTICS clustering andadaptive learning technology, and can effective-ly deal with the problem ofnon-independent and identically distributed data across different userterminals. By perceiving the clustering environment as a Markov decisionprocess, the goal is to find the best parameters of the OPTICS cluster withoutartificial assistance, so as to obtain the best federated aggregation methodand achieve better performance. The reliability and practicability of thismethod have been verified on the experimental data, and its effec-tiveness andsuperiority have been proved.", "output": "Reinforcement Federated Learning Method Based on Adaptive OPTICS Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning from visual observation (LfVO), aiming at recovering policies fromonly visual observation data, is promising yet a challenging problem. ExistingLfVO approaches either only adopt inefficient online learning schemes orrequire additional task-specific information like goal states, making them notsuited for open-ended tasks. To address these issues, we propose a two-stageframework for learning from visual observation. In the first stage, weintroduce and pretrain State-to-Go (STG) Transformer offline to predict anddifferentiate latent transitions of demonstrations. Subsequently, in the secondstage, the STG Transformer provides intrinsic rewards for downstreamreinforcement learning tasks where an agent learns merely from intrinsicrewards. Empirical results on Atari and Minecraft show that our proposed methodoutperforms baselines and in some tasks even achieves performance comparable tothe policy learned from environmental rewards. These results shed light on thepotential of utilizing video-only data to solve difficult visual reinforcementlearning tasks rather than relying on complete offline datasets containingstates, actions, and rewards. The project's website and code can be found at", "output": "Learning from Visual Observation via Offline Pretrained State-to-Go Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we present a method for single-channel wind noise reductionusing our previously proposed diffusion-based stochastic regeneration modelcombining predictive and generative modelling. We introduce a non-additivespeech in noise model to account for the non-linear deformation of the membranecaused by the wind flow and possible clipping. We show that our stochasticregeneration model outperforms other neural-network-based wind noise reductionmethods as well as purely predictive and generative models, on a dataset usingsimulated and real-recorded wind noise. We further show that the proposedmethod generalizes well by testing on an unseen dataset with real-recorded windnoise. Audio samples, data generation scripts and code for the proposed methodscan be found online (", "output": "Wind Noise Reduction with a Diffusion-based Stochastic Regeneration Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past few years, due to the rapid development of machine learning(ML) models for weather forecasting, state-of-the-art ML models have shownsuperior performance compared to the European Centre for Medium-Range WeatherForecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at aspatial resolution of 0.25 degree. However, the challenge remains to performcomparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previousstudies have demonstrated the importance of mitigating the accumulation offorecast errors for effective long-term forecasts. Despite numerous efforts toreduce accumulation errors, including autoregressive multi-time step loss,using a single model is found to be insufficient to achieve optimal performancein both short and long lead times. Therefore, we present FuXi, a cascaded MLweather forecasting system that provides 15-day global forecasts with atemporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi isdeveloped using 39 years of the ECMWF ERA5 reanalysis dataset. The performanceevaluation, based on latitude-weighted root mean square error (RMSE) andanomaly correlation coefficient (ACC), demonstrates that FuXi has comparableforecast performance to ECMWF EM in 15-day forecasts, making FuXi the firstML-based weather forecasting system to accomplish this achievement.", "output": "FuXi: A cascade machine learning forecasting system for 15-day global weather forecast."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuablefor developing various optoelectronic devices such as QD lasers and singlephoton sources. The applications strongly rely on the density and quality ofthese dots, which has motivated studies of the growth process control torealize high-quality epi-wafers and devices. Establishing the processparameters in molecular beam epitaxy (MBE) for a specific density of QDs is amultidimensional optimization challenge, usually addressed throughtime-consuming and iterative trial-and-error. Meanwhile, reflective high-energyelectron diffraction (RHEED) has been widely used to capture a wealth of growthinformation in situ. However, it still faces the challenges of extractinginformation from noisy and overlapping images. Here, based on 3D ResNet, wedeveloped a machine learning (ML) model specially designed for training RHEEDvideos instead of static images and providing real-time feedback on surfacemorphologies for process control. We demonstrated that ML from previous growthcould predict the post-growth density of QDs, by successfully tuning the QDdensities in near-real time from 1.5E10 cm-2 down to 3.8E8 cm-2 or up to 1.4E11 cm-2. Compared to traditional methods, our approach, with in-situ tuningcapabilities and excellent reliability, can dramatically expedite the materialoptimization process and improve the reproducibility of MBE growth,constituting significant progress for thin film growth techniques. The conceptsand methodologies proved feasible in this work are promising to be applied to avariety of material growth processes, which will revolutionize semiconductormanufacturing for microelectronic and optoelectronic industries.", "output": "Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have seen many successful applications of machine learning (ML)to facilitate fluid dynamic computations. As simulations grow, generating newtraining datasets for traditional offline learning creates I/O and storagebottlenecks. Additionally, performing inference at runtime requires non-trivialcoupling of ML framework libraries with simulation codes. This work offers asolution to both limitations by simplifying this coupling and enabling in situtraining and inference workflows on heterogeneous clusters. LeveragingSmartSim, the presented framework deploys a database to store data and MLmodels in memory, thus circumventing the file system. On the Polarissupercomputer, we demonstrate perfect scaling efficiency to the full machinesize of the data transfer and inference costs thanks to a novel co-locateddeployment of the database. Moreover, we train an autoencoder in situ from aturbulent flow simulation, showing that the framework overhead is negligiblerelative to a solver time step and training epoch.", "output": "In Situ Framework for Coupling Simulation and Machine Learning with Application to CFD."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The insurance industry is heavily reliant on predictions of risks based oncharacteristics of potential customers. Although the use of said models iscommon, researchers have long pointed out that such practices perpetuatediscrimination based on sensitive features such as gender or race. Given thatsuch discrimination can often be attributed to historical data biases, anelimination or at least mitigation is desirable. With the shift from moretraditional models to machine-learning based predictions, calls for greatermitigation have grown anew, as simply excluding sensitive variables in thepricing process can be shown to be ineffective. In this article, we firstinvestigate why predictions are a necessity within the industry and whycorrecting biases is not as straightforward as simply identifying a sensitivevariable. We then propose to ease the biases through the use of Wassersteinbarycenters instead of simple scaling. To demonstrate the effects andeffectiveness of the approach we employ it on real data and discuss itsimplications.", "output": "Mitigating Discrimination in Insurance with Wasserstein Barycenters."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose a built-in Deep Learning Physics Optimization (DLPO)framework to set up a shape optimization study of the Duisburg Test Case (DTC)container vessel. We present two different applications: (1) sensitivityanalysis to detect the most promising generic basis hull shapes, and (2)multi-objective optimization to quantify the trade-off between optimal hullforms. DLPO framework allows for the evaluation of design iterationsautomatically in an end-to-end manner. We achieved these results by couplingExtrality's Deep Learning Physics (DLP) model to a CAD engine and an optimizer.Our proposed DLP model is trained on full 3D volume data coming from RANSsimulations, and it can provide accurate and high-quality 3D flow predictionsin real-time, which makes it a good evaluator to perform optimization of newcontainer vessel designs w.r.t the hydrodynamic efficiency. In particular, itis able to recover the forces acting on the vessel by integration on the hullsurface with a mean relative error of 3.84% pm 2.179% on the totalresistance. Each iteration takes only 20 seconds, thus leading to a drasticsaving of time and engineering efforts, while delivering valuable insight intothe performance of the vessel, including RANS-like detailed flow information.We conclude that DLPO framework is a promising tool to accelerate the shipdesign process and lead to more efficient ships with better hydrodynamicperformance.", "output": "Multi-Objective Hull Form Optimization with CAD Engine-based Deep Learning Physics for 3D Flow Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Novel Class Discovery (NCD) is the problem of trying to discover novelclasses in an unlabeled set, given a labeled set of different but relatedclasses. The majority of NCD methods proposed so far only deal with image data,despite tabular data being among the most widely used type of data in practicalapplications. To interpret the results of clustering or NCD algorithms, datascientists need to understand the domain- and application-specific attributesof tabular data. This task is difficult and can often only be performed by adomain expert. Therefore, this interface allows a domain expert to easily runstate-of-the-art algorithms for NCD in tabular data. With minimal knowledge indata science, interpretable results can be generated.", "output": "An Interactive Interface for Novel Class Discovery in Tabular Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep reinforcement learning (DRL) has seen remarkable success in the controlof single robots. However, applying DRL to robot swarms presents significantchallenges. A critical challenge is non-stationarity, which occurs when two ormore robots update individual or shared policies concurrently, thereby engagingin an interdependent training process with no guarantees of convergence.Circumventing non-stationarity typically involves training the robots withglobal information about other agents' states and/or actions. In contrast, inthis paper we explore how to remove the need for global information. We poseour problem as a Partially Observable Markov Decision Process, due to theabsence of global knowledge on other agents. Using collective transport as atestbed scenario, we study two approaches to multi-agent training. In thefirst, the robots exchange no messages, and are trained to rely on implicitcommunication through push-and-pull on the object to transport. In the secondapproach, we introduce Global State Prediction (GSP), a network trained toforma a belief over the swarm as a whole and predict its future states. Weprovide a comprehensive study over four well-known deep reinforcement learningalgorithms in environments with obstacles, measuring performance as thesuccessful transport of the object to the goal within a desired time-frame.Through an ablation study, we show that including GSP boosts performance andincreases robustness when compared with methods that use global knowledge.", "output": "Decentralized Multi-Agent Reinforcement Learning with Global State Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer models have been widely adopted in various domains over the lastyears, and especially large language models have advanced the field of AIsignificantly. Due to their size, the capability of these networks hasincreased tremendously, but this has come at the cost of a significant increasein necessary compute. Quantization is one of the most effective ways to reducethe computational time and memory consumption of neural networks. Many studieshave shown, however, that modern transformer models tend to learn strongoutliers in their activations, making them difficult to quantize. To retainacceptable performance, the existence of these outliers requires activations tobe in higher bitwidth or the use of different numeric formats, extrafine-tuning, or other workarounds. We show that strong outliers are related tovery specific behavior of attention heads that try to learn a \"no-op\" or just apartial update of the residual. To achieve the exact zeros needed in theattention matrix for a no-update, the input to the softmax is pushed to belarger and larger during training, causing outliers in other parts of thenetwork. Based on these observations, we propose two simple (independent)modifications to the attention mechanism - clipped softmax and gated attention.We empirically show that models pre-trained using our methods learnsignificantly smaller outliers while maintaining and sometimes even improvingthe floating-point task performance. This enables us to quantize transformersto full INT8 quantization of the activations without any additional effort. Wedemonstrate the effectiveness of our methods on both language models (BERT,OPT) and vision transformers.", "output": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While a large amount of work has focused on designing adversarial attacksagainst image classifiers, only a few methods exist to attack semanticsegmentation models. We show that attacking segmentation models presentstask-specific challenges, for which we propose novel solutions. Our finalevaluation protocol outperforms existing methods, and shows that those canoverestimate the robustness of the models. Additionally, so far adversarialtraining, the most successful way for obtaining robust image classifiers, couldnot be successfully applied to semantic segmentation. We argue that this isbecause the task to be learned is more challenging, and requires significantlyhigher computational effort than for image classification. As a remedy, we showthat by taking advantage of recent advances in robust ImageNet classifiers, onecan train adversarially robust segmentation models at limited computationalcost by fine-tuning robust backbones.", "output": "Robust Semantic Segmentation: Strong Adversarial Attacks and Fast Training of Robust Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNNs) have demonstrated success in modeling relationaldata, especially for data that exhibits homophily: when a connection betweennodes tends to imply that they belong to the same class. However, while thisassumption is true in many relevant situations, there are important real-worldscenarios that violate this assumption, and this has spurred research intoimproving GNNs for these cases. In this work, we propose Evolving ComputationGraphs (ECGs), a novel method for enhancing GNNs on heterophilic datasets. Ourapproach builds on prior theoretical insights linking node degree, highhomophily, and inter vs intra-class embedding similarity by rewiring the GNNs'computation graph towards adding edges that connect nodes that are likely to bein the same class. We utilise weaker classifiers to identify these edges,ultimately improving GNN performance on non-homophilic data as a result. Weevaluate ECGs on a diverse set of recently-proposed heterophilous datasets anddemonstrate improvements over the relevant baselines. ECG presents a simple,intuitive and elegant approach for improving GNN performance on heterophilicdatasets without requiring prior domain knowledge.", "output": "Evolving Computation Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Auto-encoders (AEs) have the potential to be effective and generic tools fornew physics searches at colliders, requiring little to no model-dependentassumptions. New hypothetical physics signals can be considered anomalies thatdeviate from the well-known background processes generally expected to describethe whole dataset. We present a search formulated as an anomaly detection (AD)problem, using an AE to define a criterion to decide about the physics natureof an event. In this work, we perform an AD search for manifestations of a darkversion of strong force using raw detector images, which are large and verysparse, without leveraging any physics-based pre-processing or assumption onthe signals. We propose a dual-encoder design which can learn a compact latentspace through conditioning. In the context of multiple AD metrics, we present aclear improvement over competitive baselines and prior approaches. It is thefirst time that an AE is shown to exhibit excellent discrimination againstmultiple dark shower models, illustrating the suitability of this method as aperformant, model-independent algorithm to deploy, e.g., in the trigger stageof LHC experiments such as ATLAS and CMS.", "output": "Triggering Dark Showers with Conditional Dual Auto-Encoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Implicit Neural Representations (INRs) have emerged as a promising method forrepresenting diverse data modalities, including 3D shapes, images, and audio.While recent research has demonstrated successful applications of INRs in imageand 3D shape compression, their potential for audio compression remains largelyunexplored. Motivated by this, we present a preliminary investigation into theuse of INRs for audio compression. Our study introduces Siamese SIREN, a novelapproach based on the popular SIREN architecture. Our experimental resultsindicate that Siamese SIREN achieves superior audio reconstruction fidelitywhile utilizing fewer network parameters compared to previous INRarchitectures.", "output": "Siamese SIREN: Audio Compression with Implicit Neural Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "PyKoopman is a Python package for the data-driven approximation of theKoopman operator associated with a dynamical system. The Koopman operator is aprincipled linear embedding of nonlinear dynamics and facilitates theprediction, estimation, and control of strongly nonlinear dynamics using linearsystems theory. In particular, PyKoopman provides tools for data-driven systemidentification for unforced and actuated systems that build on theequation-free dynamic mode decomposition (DMD) and its variants. In this work,we provide a brief description of the mathematical underpinnings of the Koopmanoperator, an overview and demonstration of the features implemented inPyKoopman (with code examples), practical advice for users, and a list ofpotential extensions to PyKoopman. Software is available at<a href=\" http URL</a>", "output": "PyKoopman: A Python Package for Data-Driven Approximation of the Koopman Operator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of quantitative trading, it is common practice to transform rawhistorical stock data into indicative signals for the market trend. Suchsignals are called alpha factors. Alphas in formula forms are moreinterpretable and thus favored by practitioners concerned with risk. Inpractice, a set of formulaic alphas is often used together for better modelingprecision, so we need to find synergistic formulaic alpha sets that work welltogether. However, most traditional alpha generators mine alphas one by oneseparately, overlooking the fact that the alphas would be combined later. Inthis paper, we propose a new alpha-mining framework that prioritizes mining asynergistic set of alphas, i.e., it directly uses the performance of thedownstream combination model to optimize the alpha generator. Our frameworkalso leverages the strong exploratory capabilities of reinforcementlearning~(RL) to better explore the vast search space of formulaic alphas. Thecontribution to the combination models' performance is assigned to be thereturn used in the RL process, driving the alpha generator to find betteralphas that improve upon the current set. Experimental evaluations onreal-world stock market data demonstrate both the effectiveness and theefficiency of our framework for stock trend forecasting. The investmentsimulation results show that our framework is able to achieve higher returnscompared to previous approaches.", "output": "Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum algorithms have the potential to enhance machine learning across avariety of domains and applications. In this work, we show how quantum machinelearning can be used to improve financial forecasting. First, we use classicaland quantum Determinantal Point Processes to enhance Random Forest models forchurn prediction, improving precision by almost 6%. Second, we design quantumneural network architectures with orthogonal and compound layers for creditrisk assessment, which match classical performance with significantly fewerparameters. Our results demonstrate that leveraging quantum ideas caneffectively enhance the performance of machine learning, both today asquantum-inspired classical ML solutions, and even more in the future, with theadvent of better quantum hardware.", "output": "Improved Financial Forecasting via Quantum Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of recovering hidden communities in the LabeledStochastic Block Model (LSBM) with a finite number of clusters, where clustersizes grow linearly with the total number $n$ of items. In the LSBM, a label is(independently) observed for each pair of items. Our objective is to devise anefficient algorithm that recovers clusters using the observed labels. To thisend, we revisit instance-specific lower bounds on the expected number ofmisclassified items satisfied by any clustering algorithm. We presentInstance-Adaptive Clustering (IAC), the first algorithm whose performancematches these lower bounds both in expectation and with high probability. IACconsists of a one-time spectral clustering algorithm followed by an iterativelikelihood-based cluster assignment improvement. This approach is based on theinstance-specific lower bound and does not require any model parameters,including the number of clusters. By performing the spectral clustering onlyonce, IAC maintains an overall computational complexity of $mathcal{O}(ntext{polylog}(n))$. We illustrate the effectiveness of our approach throughnumerical experiments.", "output": "Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper will analyze and implement a time series dynamic neural network topredict daily closing stock prices. Neural networks possess unsurpassedabilities in identifying underlying patterns in chaotic, non-linear, andseemingly random data, thus providing a mechanism to predict stock pricemovements much more precisely than many current techniques. Contemporarymethods for stock analysis, including fundamental, technical, and regressiontechniques, are conversed and paralleled with the performance of neuralnetworks. Also, the Efficient Market Hypothesis (EMH) is presented andcontrasted with Chaos theory using neural networks. This paper will refute theEMH and support Chaos theory. Finally, recommendations for using neuralnetworks in stock price prediction will be presented.", "output": "Stock Price Prediction using Dynamic Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Change detection is of fundamental importance when analyzing data streams.Detecting changes both quickly and accurately enables monitoring and predictionsystems to react, e.g., by issuing an alarm or by updating a learningalgorithm. However, detecting changes is challenging when observations arehigh-dimensional. In high-dimensional data, change detectors should not only beable to identify when changes happen, but also in which subspace they occur.Ideally, one should also quantify how severe they are. Our approach, ABCD, hasthese properties. ABCD learns an encoder-decoder model and monitors itsaccuracy over a window of adaptive size. ABCD derives a change score based onBernstein's inequality to detect deviations in terms of accuracy, whichindicate changes. Our experiments demonstrate that ABCD outperforms its bestcompetitor by at least 8% and up to 23% in F1-score on average. It can alsoaccurately estimate changes' subspace, together with a severity measure thatcorrelates with the ground truth.", "output": "Adaptive Bernstein Change Detector for High-Dimensional Data Streams."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this letter, we investigate a joint power and beamforming design problemfor rate-splitting multiple access (RSMA)-based aerial communications withenergy harvesting, where a self-sustainable aerial base station serves multipleusers by utilizing the harvested energy. Considering maximizing the sum-ratefrom the long-term perspective, we utilize a deep reinforcement learning (DRL)approach, namely the soft actor-critic algorithm, to restrict the maximumtransmission power at each time based on the stochastic property of the channelenvironment, harvested energy, and battery power information. Moreover, fordesigning precoders and power allocation among all the private/common streamsof the RSMA, we employ sequential least squares programming (SLSQP) using theHan-Powell quasi-Newton method to maximize the sum-rate for the giventransmission power via DRL. Numerical results show the superiority of theproposed scheme over several baseline methods in terms of the average sum-rateperformance.", "output": "Sum-Rate Maximization of RSMA-based Aerial Communications with Energy Harvesting: A Reinforcement Learning Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning often needs to deal with the exponential growth ofstates and actions when exploring optimal control in high-dimensional spaces(often known as the curse of dimensionality). In this work, we address thisissue by learning the inherent structure of action-wise similar MDP toappropriately balance the performance degradation versus sample/computationalcomplexity. In particular, we partition the action spaces into multiple groupsbased on the similarity in transition distribution and reward function, andbuild a linear decomposition model to capture the difference between theintra-group transition kernel and the intra-group rewards. Both our theoreticalanalysis and experiments reveal a emph{surprising and counter-intuitiveresult}: while a more refined grouping strategy can reduce the approximationerror caused by treating actions in the same group as identical, it also leadsto increased estimation error when the size of samples or the computationresources is limited. This finding highlights the grouping strategy as a newdegree of freedom that can be optimized to minimize the overall performanceloss. To address this issue, we formulate a general optimization problem fordetermining the optimal grouping strategy, which strikes a balance betweenperformance loss and sample/computational complexity. We further propose acomputationally efficient method for selecting a nearly-optimal groupingstrategy, which maintains its computational complexity independent of the sizeof the action space.", "output": "Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative diffusion models, including Stable Diffusion and Midjourney, cangenerate visually appealing, diverse, and high-resolution images for variousapplications. These models are trained on billions of internet-sourced images,raising significant concerns about the potential unauthorized use ofcopyright-protected images. In this paper, we examine whether it is possible todetermine if a specific image was used in the training set, a problem known inthe cybersecurity community and referred to as a membership inference attack.Our focus is on Stable Diffusion, and we address the challenge of designing afair evaluation framework to answer this membership question. We propose amethodology to establish a fair evaluation setup and apply it to StableDiffusion, enabling potential extensions to other generative models. Utilizingthis evaluation setup, we execute membership attacks (both known and newlyintroduced). Our research reveals that previously proposed evaluation setups donot provide a full understanding of the effectiveness of membership inferenceattacks. We conclude that the membership inference attack remains a significantchallenge for large diffusion models (often deployed as black-box systems),indicating that related privacy and copyright issues will persist in theforeseeable future.", "output": "Towards More Realistic Membership Inference Attacks on Large Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For a random variable $X$, we are interested in the blind extraction of itsfinest mutual independence pattern $mu ( X )$. We introduce a specific kind ofindependence that we call dichotomic. If $Delta ( X )$ stands for the set ofall patterns of dichotomic independence that hold for $X$, we show that $mu (X )$ can be obtained as the intersection of all elements of $Delta ( X )$. Wethen propose a method to estimate $Delta ( X )$ when the data are independentand identically (i.i.d.) realizations of a multivariate normal distribution. If$hat{Delta} ( X )$ is the estimated set of valid patterns of dichotomicindependence, we estimate $mu ( X )$ as the intersection of all patterns of$hat{Delta} ( X )$. The method is tested on simulated data, showing itsadvantages and limits. We also consider an application to a toy example as wellas to experimental data.", "output": "Inferring the finest pattern of mutual independence from data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image segmentation and registration are said to be challenging when appliedto dynamic contrast enhanced MRI sequences (DCE-MRI). The contrast agent causesrapid changes in intensity in the region of interest and elsewhere, which canlead to false positive predictions for segmentation tasks and confound theimage registration similarity metric. While it is widely assumed that contrastchanges increase the difficulty of these tasks, to our knowledge no work hasquantified these effects. In this paper we examine the effect of training withdifferent ratios of contrast enhanced (CE) data on two popular tasks:segmentation with nnU-Net and Mask R-CNN and registration using VoxelMorph andVTN. We experimented further by strategically using the available datasetsthrough pretraining and fine tuning with different splits of data. We foundthat to create a generalisable model, pretraining with CE data and fine tuningwith non-CE data gave the best result. This interesting find could be expandedto other deep learning based image processing tasks with DCE-MRI and providesignificant improvements to the models performance.", "output": "Can a single image processing algorithm work equally well across all phases of DCE-MRI?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is an increasing interest in learning reward functions that model humanintent and human preferences. However, many frameworks use blackbox learningmethods that, while expressive, are difficult to interpret. We propose andevaluate a novel approach for learning expressive and interpretable rewardfunctions from preferences using Differentiable Decision Trees (DDTs) for bothlow- and high-dimensional state inputs. We explore and discuss the viability oflearning interpretable reward functions using DDTs by evaluating our algorithmon Cartpole, Visual Gridworld environments, and Atari games. We provideevidence that that the tree structure of our learned reward function is usefulin determining the extent to which a reward function is aligned with humanpreferences. We visualize the learned reward DDTs and find that they arecapable of learning interpretable reward functions but that the discrete natureof the trees hurts the performance of reinforcement learning at test time.However, we also show evidence that using soft outputs (averaged over all leafnodes) results in competitive performance when compared with larger capacitydeep neural network reward functions.", "output": "Can Differentiable Decision Trees Learn Interpretable Reward Functions?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advancements in reinforcement learning (RL) have demonstrated superhumanperformance in complex tasks such as Starcraft, Go, Chess etc. However,knowledge transfer from Artificial \"Experts\" to humans remain a significantchallenge. A promising avenue for such transfer would be the use of curricula.Recent methods in curricula generation focuses on training RL agentsefficiently, yet such methods rely on surrogate measures to track studentprogress, and are not suited for training robots in the real world (or moreambitiously humans). In this paper, we introduce a method named ParameterizedEnvironment Response Model (PERM) that shows promising results in training RLagents in parameterized environments. Inspired by Item Response Theory, PERMseeks to model difficulty of environments and ability of RL agents directly.Given that RL agents and humans are trained more efficiently under the \"zone ofproximal development\", our method generates a curriculum by matching thedifficulty of an environment to the current ability of the student. Inaddition, PERM can be trained offline and does not employ non-stationarymeasures of student ability, making it suitable for transfer between students.We demonstrate PERM's ability to represent the environment parameter space, andtraining with RL agents with PERM produces a strong performance indeterministic environments. Lastly, we show that our method is transferablebetween students, without any sacrifice in training quality.", "output": "Transferable Curricula through Difficulty Conditioned Generators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cyberattacks are increasingly threatening networked systems, often with theemergence of new types of unknown (zero-day) attacks and the rise of vulnerabledevices. While Machine Learning (ML)-based Intrusion Detection Systems (IDSs)have been shown to be extremely promising in detecting these attacks, the needto learn large amounts of labelled data often limits the applicability ofML-based IDSs to cybersystems that only have access to private local data. Toaddress this issue, this paper proposes a novel Decentralized and OnlineFederated Learning Intrusion Detection (DOF-ID) architecture. DOF-ID is acollaborative learning system that allows each IDS used for a cybersystem tolearn from experience gained in other cybersystems in addition to its own localdata without violating the data privacy of other systems. As the performanceevaluation results using public Kitsune and Bot-IoT datasets show, DOF-IDsignificantly improves the intrusion detection performance in all collaboratingnodes simultaneously with acceptable computation time for online learning.", "output": "Decentralized Online Federated G-Network Learning for Lightweight Intrusion Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a novel Self-Supervised Intrusion Detection (SSID)framework, which enables a fully online Machine Learning (ML) based IntrusionDetection System (IDS) that requires no human intervention or prior off-linelearning. The proposed framework analyzes and labels incoming traffic packetsbased only on the decisions of the IDS itself using an Auto-Associative DeepRandom Neural Network, and on an online estimate of its statistically measuredtrustworthiness. The SSID framework enables IDS to adapt rapidly totime-varying characteristics of the network traffic, and eliminates the needfor offline data collection. This approach avoids human errors in datalabeling, and human labor and computational costs of model training and datacollection. The approach is experimentally evaluated on public datasets andcompared with well-known ML models, showing that this SSID framework is veryuseful and advantageous as an accurate and online learning ML-based IDS for IoTsystems.", "output": "Online Self-Supervised Learning in Machine Learning Intrusion Detection for the Internet of Things."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unlike classical lexical overlap metrics such as BLEU, most currentevaluation metrics for machine translation (for example, COMET or BERTScore)are based on black-box large language models. They often achieve strongcorrelations with human judgments, but recent research indicates that thelower-quality classical metrics remain dominant, one of the potential reasonsbeing that their decision processes are more transparent. To foster morewidespread acceptance of novel high-quality metrics, explainability thusbecomes crucial. In this concept paper, we identify key properties as well askey goals of explainable machine translation metrics and provide acomprehensive synthesis of recent techniques, relating them to our establishedgoals and properties. In this context, we also discuss the lateststate-of-the-art approaches to explainable metrics based on generative modelssuch as ChatGPT and GPT4. Finally, we contribute a vision of next-generationapproaches, including natural language explanations. We hope that our work canhelp catalyze and guide future research on explainable evaluation metrics and,mediately, also contribute to better and more transparent machine translationsystems.", "output": "Towards Explainable Evaluation Metrics for Machine Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Complementarity Determining Region (CDR) structure prediction of loops inantibody engineering has gained a lot of attraction by researchers. Whendesigning antibodies, a main challenge is to predict the CDR structure of theH3 loop. Compared with the other CDR loops, that is the H1 and H2 loops, theCDR structure of the H3 loop is more challenging due to its varying length andflexible structure. In this paper, we propose a Multi-task learning model withLoop Specific Attention, namely MLSA. In particular, to the best of ourknowledge we are the first to jointly learn the three CDR loops, via a novelmulti-task learning strategy. In addition, to account for the structural andfunctional similarities and differences of the three CDR loops, we propose aloop specific attention mechanism to control the influence of each CDR loop onthe training of MLSA. Our experimental evaluation on widely used benchmark datashows that the proposed MLSA method significantly reduces the prediction errorof the CDR structure of the H3 loop, by at least 19%, when compared with otherbaseline strategies. Finally, for reproduction purposes we make theimplementation of MLSA publicly available at", "output": "Multi-Task Learning with Loop Specific Attention for CDR Structure Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Collaborative filtering (CF) has become a popular method for developingrecommender systems (RS) where ratings of a user for new items is predictedbased on her past preferences and available preference information of otherusers. Despite the popularity of CF-based methods, their performance is oftengreatly limited by the sparsity of observed entries. In this study, we explorethe data augmentation and refinement aspects of Maximum Margin MatrixFactorization (MMMF), a widely accepted CF technique for the ratingpredictions, which have not been investigated before. We exploit the inherentcharacteristics of CF algorithms to assess the confidence level of individualratings and propose a semi-supervised approach for rating augmentation based onself-training. We hypothesize that any CF algorithm's predictions with lowconfidence are due to some deficiency in the training data and hence, theperformance of the algorithm can be improved by adopting a systematic dataaugmentation strategy. We iteratively use some of the ratings predicted withhigh confidence to augment the training data and remove low-confidence entriesthrough a refinement process. By repeating this process, the system learns toimprove prediction accuracy. Our method is experimentally evaluated on severalstate-of-the-art CF algorithms and leads to informative rating augmentation,improving the performance of the baseline approaches.", "output": "Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a contextual bandit problem with $S $ contexts and $A $ actions.In each round $t=1,2,dots$ the learner observes a random context and choosesan action based on its past experience. The learner then observes a randomreward whose mean is a function of the context and the action for the round.Under the assumption that the contexts can be lumped into $rle min{S ,A }$groups such that the mean reward for the various actions is the same for anytwo contexts that are in the same group, we give an algorithm that outputs an$epsilon$-optimal policy after using at most $widetilde O(r (S +A)/epsilon^2)$ samples with high probability and provide a matching$widetildeOmega(r (S +A )/epsilon^2)$ lower bound. In the regretminimization setting, we give an algorithm whose cumulative regret up to time$T$ is bounded by $widetilde O(sqrt{r^3(S +A )T})$. To the best of ourknowledge, we are the first to show the near-optimal sample complexity in thePAC setting and $widetilde O(sqrt{{poly}(r)(S+K)T})$ minimax regret in theonline setting for this problem. We also show our algorithms can be applied tomore general low-rank bandits and get improved regret bounds in some scenarios.", "output": "Context-lumpable stochastic bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a versatile privacy framework for quantum systems, termed quantumpufferfish privacy (QPP). Inspired by classical pufferfish privacy, ourformulation generalizes and addresses limitations of quantum differentialprivacy by offering flexibility in specifying private information, feasiblemeasurements, and domain knowledge. We show that QPP can be equivalentlyformulated in terms of the Datta-Leditzky information spectrum divergence, thusproviding the first operational interpretation thereof. We reformulate thisdivergence as a semi-definite program and derive several properties of it,which are then used to prove convexity, composability, and post-processing ofQPP mechanisms. Parameters that guarantee QPP of the depolarization mechanismare also derived. We analyze the privacy-utility tradeoff of general QPPmechanisms and, again, study the depolarization mechanism as an explicitinstance. The QPP framework is then applied to privacy auditing for identifyingprivacy violations via a hypothesis testing pipeline that leverages quantumalgorithms. Connections to quantum fairness and other quantum divergences arealso explored and several variants of QPP are examined.", "output": "Quantum Pufferfish Privacy: A Flexible Privacy Framework for Quantum Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the complexity of learning mixtures of separated Gaussians withcommon unknown bounded covariance matrix. Specifically, we focus on learningGaussian mixture models (GMMs) on $mathbb{R}^d$ of the form $P= sum_{i=1}^kw_i mathcal{N}(boldsymbol mu_i,mathbf Sigma_i)$, where $mathbf Sigma_i =mathbf Sigma preceq mathbf I$ and $min_{i neq j} | boldsymbol mu_i -boldsymbol mu_j|_2 geq k^epsilon$ for some $epsilon&gt;0$. Known learningalgorithms for this family of GMMs have complexity $(dk)^{O(1/epsilon)}$. Inthis work, we prove that any Statistical Query (SQ) algorithm for this problemrequires complexity at least $d^{Omega(1/epsilon)}$. In the special casewhere the separation is on the order of $k^{1/2}$, we additionally obtainfine-grained SQ lower bounds with the correct exponent. Our SQ lower boundsimply similar lower bounds for low-degree polynomial tests. Conceptually, ourresults provide evidence that known algorithms for this problem are nearly bestpossible.", "output": "SQ Lower Bounds for Learning Bounded Covariance GMMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predictive models that satisfy group fairness criteria in aggregate formembers of a protected class, but do not guarantee subgroup fairness, couldproduce biased predictions for individuals at the intersection of two or moreprotected classes. To address this risk, we propose Conditional Bias Scan(CBS), a flexible auditing framework for detecting intersectional biases inclassification models. CBS identifies the subgroup for which there is the mostsignificant bias against the protected class, as compared to the equivalentsubgroup in the non-protected class, and can incorporate multiple commonly usedfairness definitions for both probabilistic and binarized predictions. We showthat this methodology can detect previously unidentified intersectional andcontextual biases in the COMPAS pre-trial risk assessment tool and has higherbias detection power compared to similar methods that audit for subgroupfairness.", "output": "Auditing Predictive Models for Intersectional Biases."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Emotion recognition has become an important research topic in the field ofhuman-computer interaction. Studies on sound and videos to understand emotionsfocused mainly on analyzing facial expressions and classified 6 basic emotions.In this study, the performance of different sequence models in multi-modalemotion recognition was compared. The sound and images were first processed bymulti-layered CNN models, and the outputs of these models were fed into varioussequence models. The sequence model is GRU, Transformer, LSTM and Max Pooling.Accuracy, precision, and F1 Score values of all models were calculated. Themulti-modal CREMA-D dataset was used in the experiments. As a result of thecomparison of the CREMA-D dataset, GRU-based architecture with 0.640 showed thebest result in F1 score, LSTM-based architecture with 0.699 in precisionmetric, while sensitivity showed the best results over time with MaxPooling-based architecture with 0.620. As a result, it has been observed thatthe sequence models compare performances close to each other.", "output": "A Comparison of Time-based Models for Multimodal Emotion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most offline reinforcement learning (RL) algorithms return a target policymaximizing a trade-off between (1) the expected performance gain over thebehavior policy that collected the dataset, and (2) the risk stemming from theout-of-distribution-ness of the induced state-action occupancy. It follows thatthe performance of the target policy is strongly related to the performance ofthe behavior policy and, thus, the trajectory return distribution of thedataset. We show that in mixed datasets consisting of mostly low-returntrajectories and minor high-return trajectories, state-of-the-art offline RLalgorithms are overly restrained by low-return trajectories and fail to exploithigh-performing trajectories to the fullest. To overcome this issue, we showthat, in deterministic MDPs with stochastic initial states, the datasetsampling can be re-weighted to induce an artificial dataset whose behaviorpolicy has a higher return. This re-weighted sampling strategy may be combinedwith any offline RL algorithm. We further analyze that the opportunity forperformance improvement over the behavior policy correlates with thepositive-sided variance of the returns of the trajectories in the dataset. Weempirically show that while CQL, IQL, and TD3+BC achieve only a part of thispotential policy improvement, these same algorithms combined with ourreweighted sampling strategy fully exploit the dataset. Furthermore, weempirically demonstrate that, despite its theoretical limitation, the approachmay still be efficient in stochastic environments. The code is available at", "output": "Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Molecule property prediction has gained significant attention in recentyears. The main bottleneck is the label insufficiency caused by expensive labexperiments. In order to alleviate this issue and to better leverage textualknowledge for tasks, this study investigates the feasibility of employingnatural language instructions to accomplish molecule-related tasks in azero-shot setting. We discover that existing molecule-text models performpoorly in this setting due to inadequate treatment of instructions and limitedcapacity for graphs. To overcome these issues, we propose GIMLET, which unifieslanguage models for both graph and text data. By adopting generalized positionembedding, our model is extended to encode both graph structures andinstruction text without additional graph encoding modules. GIMLET alsodecouples encoding of the graph from tasks instructions in the attentionmechanism, enhancing the generalization of graph features across novel tasks.We construct a dataset consisting of more than two thousand molecule tasks withcorresponding instructions derived from task descriptions. We pretrain GIMLETon the molecule tasks along with instructions, enabling the model to transfereffectively to a broad range of tasks. Experimental results demonstrate thatGIMLET significantly outperforms molecule-text baselines in instruction-basedzero-shot learning, even achieving closed results to supervised GNN models ontasks such as toxcast and muv.", "output": "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability of generative models to produce highly realistic synthetic faceimages has raised security and ethical concerns. As a first line of defenseagainst such fake faces, deep learning based forensic classifiers have beendeveloped. While these forensic models can detect whether a face image issynthetic or real with high accuracy, they are also vulnerable to adversarialattacks. Although such attacks can be highly successful in evading detection byforensic classifiers, they introduce visible noise patterns that are detectablethrough careful human scrutiny. Additionally, these attacks assume access tothe target model(s) which may not always be true. Attempts have been made todirectly perturb the latent space of GANs to produce adversarial fake facesthat can circumvent forensic classifiers. In this work, we go one step furtherand show that it is possible to successfully generate adversarial fake faceswith a specified set of attributes (e.g., hair color, eye size, race, gender,etc.). To achieve this goal, we leverage the state-of-the-art generative modelStyleGAN with disentangled representations, which enables a range ofmodifications without leaving the manifold of natural images. We propose aframework to search for adversarial latent codes within the feature space ofStyleGAN, where the search can be guided either by a text prompt or a referenceimage. We also propose a meta-learning based optimization strategy to achievetransferable performance on unknown target models. Extensive experimentsdemonstrate that the proposed approach can produce semantically manipulatedadversarial fake faces, which are true to the specified attribute set and cansuccessfully fool forensic face classifiers, while remaining undetectable byhumans. Code: ", "output": "Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new dataset condensation framework termed Squeeze, Recover andRelabel (SRe$^2$L) that decouples the bilevel optimization of model andsynthetic data during training, to handle varying scales of datasets, modelarchitectures and image resolutions for effective dataset condensation. Theproposed method demonstrates flexibility across diverse dataset scales andexhibits multiple advantages in terms of arbitrary resolutions of synthesizedimages, low training cost and memory consumption with high-resolution training,and the ability to scale up to arbitrary evaluation network architectures.Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1Kdatasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8%validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming allprevious state-of-the-art methods by margins of 14.5% and 32.9%, respectively.Our approach also outperforms MTT by approximately 52$times$ (ConvNet-4) and16$times$ (ResNet-18) faster in speed with less memory consumption of11.6$times$ and 6.4$times$ during data synthesis. Our code and condenseddatasets of 50, 200 IPC with 4K recovery budget are available at", "output": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sampling schemes are fundamental tools in statistics, survey design, andalgorithm design. A fundamental result in differential privacy is that adifferentially private mechanism run on a simple random sample of a populationprovides stronger privacy guarantees than the same algorithm run on the entirepopulation. However, in practice, sampling designs are often more complex thanthe simple, data-independent sampling schemes that are addressed in prior work.In this work, we extend the study of privacy amplification results to morecomplex, data-dependent sampling schemes. We find that not only do thesesampling schemes often fail to amplify privacy, they can actually result inprivacy degradation. We analyze the privacy implications of the pervasivecluster sampling and stratified sampling paradigms, as well as provide someinsight into the study of more general sampling designs.", "output": "Controlling Privacy Loss in Sampling Schemes: an Analysis of Stratified and Cluster Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents CounterNet, a novel end-to-end learning framework whichintegrates Machine Learning (ML) model training and the generation ofcorresponding counterfactual (CF) explanations into a single end-to-endpipeline. Counterfactual explanations offer a contrastive case, i.e., theyattempt to find the smallest modification to the feature values of an instancethat changes the prediction of the ML model on that instance to a predefinedoutput. Prior techniques for generating CF explanations suffer from two majorlimitations: (i) all of them are post-hoc methods designed for use withproprietary ML models -- as a result, their procedure for generating CFexplanations is uninformed by the training of the ML model, which leads tomisalignment between model predictions and explanations; and (ii) most of themrely on solving separate time-intensive optimization problems to find CFexplanations for each input data point (which negatively impacts theirruntime). This work makes a novel departure from the prevalent post-hocparadigm (of generating CF explanations) by presenting CounterNet, anend-to-end learning framework which integrates predictive model training andthe generation of counterfactual (CF) explanations into a single pipeline.Unlike post-hoc methods, CounterNet enables the optimization of the CFexplanation generation only once together with the predictive model. We adopt ablock-wise coordinate descent procedure which helps in effectively trainingCounterNet's network. Our extensive experiments on multiple real-world datasetsshow that CounterNet generates high-quality predictions, and consistentlyachieves 100% CF validity and low proximity scores (thereby achieving awell-balanced cost-invalidity trade-off) for any new input instance, and runs3X faster than existing state-of-the-art baselines.", "output": "CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In real scenarios, state observations that an agent observes may containmeasurement errors or adversarial noises, misleading the agent to takesuboptimal actions or even collapse while training. In this paper, we study thetraining robustness of distributional Reinforcement Learning (RL), a class ofstate-of-the-art methods that estimate the whole distribution, as opposed toonly the expectation, of the total return. Firstly, we validate the contractionof distributional Bellman operators in the State-Noisy Markov Decision Process(SN-MDP), a typical tabular case that incorporates both random and adversarialstate observation noises. In the noisy setting with function approximation, wethen analyze the vulnerability of least squared loss in expectation-based RLwith either linear or nonlinear function approximation. By contrast, wetheoretically characterize the bounded gradient norm of distributional RL lossbased on the categorical parameterization equipped with the KL divergence. Theresulting stable gradients while the optimization in distributional RL accountsfor its better training robustness against state observation noises. Finally,extensive experiments on the suite of environments verified that distributionalRL is less vulnerable against both random and adversarial noisy stateobservations compared with its expectation-based counterpart.", "output": "Exploring the Training Robustness of Distributional Reinforcement Learning against Noisy State Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study sparse linear regression over a network of agents, modeled as anundirected graph (with no centralized node). The estimation problem isformulated as the minimization of the sum of the local LASSO loss functionsplus a quadratic penalty of the consensus constraint -- the latter beinginstrumental to obtain distributed solution methods. While penalty-basedconsensus methods have been extensively studied in the optimization literature,their statistical and computational guarantees in the high dimensional settingremain unclear. This work provides an answer to this open problem. Ourcontribution is two-fold. First, we establish statistical consistency of theestimator: under a suitable choice of the penalty parameter, the optimalsolution of the penalized problem achieves near optimal minimax rate$mathcal{O}(s log d/N)$ in $ell_2$-loss, where $s$ is the sparsity value,$d$ is the ambient dimension, and $N$ is the total sample size in the network-- this matches centralized sample rates. Second, we show that theproximal-gradient algorithm applied to the penalized problem, which naturallyleads to distributed implementations, converges linearly up to a tolerance ofthe order of the centralized statistical error -- the rate scales as$mathcal{O}(d)$, revealing an unavoidable speed-accuracy dilemma.Numericalresults demonstrate the tightness of the derived sample rate and convergencerate scalings.", "output": "Distributed Sparse Regression via Penalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning (ML) is increasingly often used to inform high-stakesdecisions. As complex ML models (e.g., deep neural networks) are oftenconsidered black boxes, a wealth of procedures has been developed to shed lighton their inner workings and the ways in which their predictions come about,defining the field of 'explainable AI' (XAI). Saliency methods rank inputfeatures according to some measure of 'importance'. Such methods are difficultto validate since a formal definition of feature importance is, thus far,lacking. It has been demonstrated that some saliency methods can highlightfeatures that have no statistical association with the prediction target(suppressor variables). To avoid misinterpretations due to such behavior, wepropose the actual presence of such an association as a necessary condition andobjective preliminary definition for feature importance. We carefully crafted aground-truth dataset in which all statistical dependencies are well-defined andlinear, serving as a benchmark to study the problem of suppressor variables. Weevaluate common explanation methods including LRP, DTD, PatternNet,PatternAttribution, LIME, Anchors, SHAP, and permutation-based methods withrespect to our objective definition. We show that most of these methods areunable to distinguish important features from suppressors in this setting.", "output": "Scrutinizing XAI using linear ground-truth data with suppressor variables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks have emerged as a leading architecture for manygraph-level tasks, such as graph classification and graph generation. As anessential component of the architecture, graph pooling is indispensable forobtaining a holistic graph-level representation of the whole graph. Although agreat variety of methods have been proposed in this promising andfast-developing research field, to the best of our knowledge, little effort hasbeen made to systematically summarize these works. To set the stage for thedevelopment of future works, in this paper, we attempt to fill this gap byproviding a broad review of recent methods for graph pooling. Specifically, 1)we first propose a taxonomy of existing graph pooling methods with amathematical summary for each category; 2) then, we provide an overview of thelibraries related to graph pooling, including the commonly used datasets, modelarchitectures for downstream tasks, and open-source implementations; 3) next,we further outline the applications that incorporate the idea of graph poolingin a variety of domains; 4) finally, we discuss certain critical challengesfacing current studies and share our insights on future potential directionsfor research on the improvement of graph pooling.", "output": "Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Input gradients have a pivotal role in a variety of applications, includingadversarial attack algorithms for evaluating model robustness, explainable AItechniques for generating Saliency Maps, and counterfactual explanations.However, Saliency Maps generated by traditional neural networks are often noisyand provide limited insights. In this paper, we demonstrate that, on thecontrary, the Saliency Maps of 1-Lipschitz neural networks, learnt with thedual loss of an optimal transportation problem, exhibit desirable XAIproperties: They are highly concentrated on the essential parts of the imagewith low noise, significantly outperforming state-of-the-art explanationapproaches across various models and metrics. We also prove that these mapsalign unprecedentedly well with human explanations on ImageNet. To explain theparticularly beneficial properties of the Saliency Map for such models, weprove this gradient encodes both the direction of the transportation plan andthe direction towards the nearest adversarial attack. Following the gradientdown to the decision boundary is no longer considered an adversarial attack,but rather a counterfactual explanation that explicitly transports the inputfrom one class to another. Thus, Learning with such a loss jointly optimizesthe classification objective and the alignment of the gradient , i.e. theSaliency Map, to the transportation plan direction. These networks werepreviously known to be certifiably robust by design, and we demonstrate thatthey scale well for large problems and models, and are tailored forexplainability using a fast and straightforward method.", "output": "On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-Language Pretraining (VLP) models have recently successfullyfacilitated many cross-modal downstream tasks. Most existing works evaluatedtheir systems by comparing the fine-tuned downstream task performance. However,only average downstream task accuracy provides little information about thepros and cons of each VLP method, let alone provides insights on how thecommunity can improve the systems in the future. Inspired by the CheckList fortesting natural language processing, we exploit VL-CheckList, a novel frameworkto understand the capabilities of VLP models. The proposed method divides theimage-texting ability of a VLP model into three categories: objects,attributes, and relations, and uses a novel taxonomy to further break downthese three aspects. We conduct comprehensive studies to analyze seven recentlypopular VLP models via the proposed framework. Results confirm theeffectiveness of the proposed method by revealing fine-grained differencesamong the compared models that were not visible from downstream task-onlyevaluation. Further results show promising research direction in buildingbetter VLP models. Our data and code are available at:", "output": "VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study separations between two fundamental models (or emph{Ans\"atze}) ofantisymmetric functions, that is, functions $f$ of the form $f(x_{sigma(1)},ldots, x_{sigma(N)}) = text{sign}(sigma)f(x_1, ldots, x_N)$, where$sigma$ is any permutation. These arise in the context of quantum chemistry,and are the basic modeling tool for wavefunctions of Fermionic systems.Specifically, we consider two popular antisymmetric Ans\"atze: the Slaterrepresentation, which leverages the alternating structure of determinants, andthe Jastrow ansatz, which augments Slater determinants with a product by anarbitrary symmetric function. We construct an antisymmetric function in $N$dimensions that can be efficiently expressed in Jastrow form, yet provablycannot be approximated by Slater determinants unless there are exponentially(in $N^2$) many terms. This represents the first explicit quantitativeseparation between these two Ans\"atze.", "output": "Towards Antisymmetric Neural Ansatz Separation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Frouros is an open-source Python library capable of detecting drift inmachine learning systems. It provides a combination of classical and morerecent algorithms for drift detection: both concept and data drift. We havedesigned it with the objective of making it compatible with any machinelearning framework and easily adaptable to real-world use cases. The library isdeveloped following a set of best development and continuous integrationpractices to ensure ease of maintenance and extensibility. The source code isavailable at ", "output": "Frouros: A Python library for drift detection in machine learning systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Property inference attacks allow an adversary to extract global properties ofthe training dataset from a machine learning model. Such attacks have privacyimplications for data owners sharing their datasets to train machine learningmodels. Several existing approaches for property inference attacks against deepneural networks have been proposed, but they all rely on the attacker traininga large number of shadow models, which induces a large computational overhead.In this paper, we consider the setting of property inference attacks in whichthe attacker can poison a subset of the training dataset and query the trainedtarget model. Motivated by our theoretical analysis of model confidences underpoisoning, we design an efficient property inference attack, SNAP, whichobtains higher attack success and requires lower amounts of poisoning than thestate-of-the-art poisoning-based property inference attack by Mahloujifar etal. For example, on the Census dataset, SNAP achieves 34% higher success ratethan Mahloujifar et al. while being 56.5x faster. We also extend our attack toinfer whether a certain property was present at all during training andestimate the exact proportion of a property of interest efficiently. Weevaluate our attack on several properties of varying proportions from fourdatasets and demonstrate SNAP's generality and effectiveness. An open-sourceimplementation of SNAP can be found at ", "output": "SNAP: Efficient Extraction of Private Properties with Poisoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Obesity is a serious issue in the modern society and is often associated tosignificantly reduced quality of life. Current research conducted to exploreobesity-related neurological evidences using electroencephalography (EEG) dataare limited to traditional approaches. In this study, we developed a novelmachine learning model to identify brain networks of obese females using alphaband functional connectivity features derived from EEG data. An overallclassification accuracy of 0.937 is achieved. Our finding suggests that theobese brain is characterized by a dysfunctional network in which the areas thatresponsible for processing self-referential information and environmentalcontext information are impaired.", "output": "Finding neural signatures for obesity through feature selection on source-localized EEG."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Centralised training with decentralised execution (CT-DE) serves as thefoundation of many leading multi-agent reinforcement learning (MARL)algorithms. Despite its popularity, it suffers from a critical drawback due toits reliance on learning from a single sample of the joint-action at a givenstate. As agents explore and update their policies during training, thesesingle samples may poorly represent the actual joint-policy of the system ofagents leading to high variance gradient estimates that hinder learning. Toaddress this problem, we propose an enhancement tool that accommodates anyactor-critic MARL method. Our framework, Performance Enhancing ReinforcementLearning Apparatus (PERLA), introduces a sampling technique of the agents'joint-policy into the critics while the agents train. This leads to TD updatesthat closely approximate the true expected value under the current joint-policyrather than estimates from a single sample of the joint-action at a givenstate. This produces low variance and precise estimates of expected returns,minimising the variance in the critic estimators which typically hinderslearning. Moreover, as we demonstrate, by eliminating much of the criticvariance from the single sampling of the joint policy, PERLA enables CT-DEmethods to scale more efficiently with the number of agents. Theoretically, weprove that PERLA reduces variance in value estimates similar to that ofdecentralised training while maintaining the benefits of centralised training.Empirically, we demonstrate PERLA's superior performance and ability to reduceestimator variance in a range of benchmarks including Multi-agent Mujoco, andStarCraft II Multi-agent Challenge.", "output": "Taming Multi-Agent Reinforcement Learning with Estimator Variance Reduction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The effectiveness of machine learning in evaluating the creditworthiness ofloan applicants has been demonstrated for a long time. However, there isconcern that the use of automated decision-making processes may result inunequal treatment of groups or individuals, potentially leading todiscriminatory outcomes. This paper seeks to address this issue by evaluatingthe effectiveness of 12 leading bias mitigation methods across 5 differentfairness metrics, as well as assessing their accuracy and potentialprofitability for financial institutions. Through our analysis, we haveidentified the challenges associated with achieving fairness while maintainingaccuracy and profitabiliy, and have highlighted both the most successful andleast successful mitigation methods. Ultimately, our research serves to bridgethe gap between experimental machine learning and its practical applications inthe finance industry.", "output": "Algorithmic decision making methods for fair credit scoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven learning based methods have recently been particularly successfulat learning robust locomotion controllers for a variety of unstructuredterrains. Prior work has shown that incorporating good locomotion priors in theform of trajectory generators (TGs) is effective at efficiently learningcomplex locomotion skills. However, defining a good, single TG astasks/environments become increasingly more complex remains a challengingproblem as it requires extensive tuning and risks reducing the effectiveness ofthe prior. In this paper, we present Evolved Environmental TrajectoryGenerators (EETG), a method that learns a diverse set of specialised locomotionpriors using Quality-Diversity algorithms while maintaining a single policywithin the Policies Modulating TG (PMTG) architecture. The results demonstratethat EETG enables a quadruped robot to successfully traverse a wide range ofenvironments, such as slopes, stairs, rough terrain, and balance beams. Ourexperiments show that learning a diverse set of specialized TG priors issignificantly (5 times) more efficient than using a single, fixed prior whendealing with a wide range of environments.", "output": "Efficient Learning of Locomotion Skills through the Discovery of Diverse Environmental Trajectory Generator Priors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Natural agents can effectively learn from multiple data sources that differin size, quality, and types of measurements. We study this heterogeneity in thecontext of offline reinforcement learning (RL) by introducing a new,practically motivated semi-supervised setting. Here, an agent has access to twosets of trajectories: labelled trajectories containing state, action and rewardtriplets at every timestep, along with unlabelled trajectories that containonly state and reward information. For this setting, we develop and study asimple meta-algorithmic pipeline that learns an inverse dynamics model on thelabelled data to obtain proxy-labels for the unlabelled data, followed by theuse of any offline RL algorithm on the true and proxy-labelled trajectories.Empirically, we find this simple pipeline to be highly successful -- on severalD4RL benchmarks~cite{fu2020d4rl}, certain offline RL algorithms can match theperformance of variants trained on a fully labelled dataset even when we labelonly 10% of trajectories which are highly suboptimal. To strengthen ourunderstanding, we perform a large-scale controlled empirical studyinvestigating the interplay of data-centric properties of the labelled andunlabelled datasets, with algorithmic design choices (e.g., choice of inversedynamics, offline RL algorithm) to identify general trends and best practicesfor training RL agents on semi-supervised offline datasets.", "output": "Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The library scikit-fda is a Python package for Functional Data Analysis(FDA). It provides a comprehensive set of tools for representation,preprocessing, and exploratory analysis of functional data. The library isbuilt upon and integrated in Python's scientific ecosystem. In particular, itconforms to the scikit-learn application programming interface so as to takeadvantage of the functionality for machine learning provided by this package:pipelines, model selection, and hyperparameter tuning, among others. Thescikit-fda package has been released as free and open-source software under a3-Clause BSD license and is open to contributions from the FDA community. Thelibrary's extensive documentation includes step-by-step tutorials and detailedexamples of use.", "output": "scikit-fda: A Python Package for Functional Data Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mitigating the climate crisis requires a rapid transition towardslower-carbon energy. Catalyst materials play a crucial role in theelectrochemical reactions involved in numerous industrial processes key to thistransition, such as renewable energy storage and electrofuel synthesis. Toreduce the energy spent on such activities, we must quickly discover moreefficient catalysts to drive electrochemical reactions. Machine learning (ML)holds the potential to efficiently model materials properties from largeamounts of data, accelerating electrocatalyst design. The Open Catalyst ProjectOC20 dataset was constructed to that end. However, ML models trained on OC20are still neither scalable nor accurate enough for practical applications. Inthis paper, we propose task-specific innovations applicable to mostarchitectures, enhancing both computational efficiency and accuracy. Thisincludes improvements in (1) the graph creation step, (2) atom representations,(3) the energy prediction head, and (4) the force prediction head. We describethese contributions and evaluate them thoroughly on multiple architectures.Overall, our proposed PhAST improvements increase energy MAE by 4 to 42$%$while dividing compute time by 3 to 8$times$ depending on the targetedtask/model. PhAST also enables CPU training, leading to 40$times$ speedups inhighly parallelized settings. Python package:url{", "output": "PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The next step for intelligent dialog agents is to escape their role as silentbystanders and become proactive. Well-defined proactive behavior may improvehuman-machine cooperation, as the agent takes a more active role duringinteraction and takes off responsibility from the user. However, proactivity isa double-edged sword because poorly executed pre-emptive actions may have adevastating effect not only on the task outcome but also on the relationshipwith the user. For designing adequate proactive dialog strategies, we propose anovel approach including both social as well as task-relevant features in thedialog. Here, the primary goal is to optimize proactive behavior so that it istask-oriented - this implies high task success and efficiency - while alsobeing socially effective by fostering user trust. Including both aspects in thereward function for training a proactive dialog agent using reinforcementlearning showed the benefit of our approach for more successful human-machinecooperation.", "output": "Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The prevalence of tree-like structures, encompassing hierarchical structuresand power law distributions, exists extensively in real-world applications,including recommendation systems, ecosystems, financial networks, socialnetworks, etc. Recently, the exploitation of hyperbolic space for tree-likenessmodeling has garnered considerable attention owing to its exponential growthvolume. Compared to the flat Euclidean space, the curved hyperbolic spaceprovides a more amenable and embeddable room, especially for datasetsexhibiting implicit tree-like architectures. However, the intricate nature ofreal-world tree-like data presents a considerable challenge, as it frequentlydisplays a heterogeneous composition of tree-like, flat, and circular regions.The direct embedding of such heterogeneous structures into a homogeneousembedding space (i.e., hyperbolic space) inevitably leads to heavy distortions.To mitigate the aforementioned shortage, this study endeavors to explore thecurvature between discrete structure and continuous learning space, aiming atencoding the message conveyed by the network topology in the learning process,thereby improving tree-likeness modeling. To the end, a curvature-awarehyperbolic graph convolutional neural network, {kappa}HGCN, is proposed, whichutilizes the curvature to guide message passing and improve long-rangepropagation. Extensive experiments on node classification and link predictiontasks verify the superiority of the proposal as it consistently outperformsvarious competitive models by a large margin.", "output": "\\{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mathematical reasoning is a fundamental aspect of human intelligence and isapplicable in various fields, including science, engineering, finance, andeveryday life. The development of artificial intelligence (AI) systems capableof solving math problems and proving theorems has garnered significant interestin the fields of machine learning and natural language processing. For example,mathematics serves as a testbed for aspects of reasoning that are challengingfor powerful deep learning models, driving new algorithmic and modelingadvances. On the other hand, recent advances in large-scale neural languagemodels have opened up new benchmarks and opportunities to use deep learning formathematical reasoning. In this survey paper, we review the key tasks,datasets, and methods at the intersection of mathematical reasoning and deeplearning over the past decade. We also evaluate existing benchmarks andmethods, and discuss future research directions in this domain.", "output": "A Survey of Deep Learning for Mathematical Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although we are currently in the era of noisy intermediate scale quantumdevices, several studies are being conducted with the aim of bringing machinelearning to the quantum domain. Currently, quantum variational circuits are oneof the main strategies used to build such models. However, despite itswidespread use, we still do not know what are the minimum resources needed tocreate a quantum machine learning model. In this article, we analyze how theexpressiveness of the parametrization affects the cost function. Weanalytically show that the more expressive the parametrization is, the more thecost function will tend to concentrate around a value that depends both on thechosen observable and on the number of qubits used. For this, we initiallyobtain a relationship between the expressiveness of the parametrization and themean value of the cost function. Afterwards, we relate the expressivity of theparametrization with the variance of the cost function. Finally, we show somenumerical simulation results that confirm our theoretical-analyticalpredictions. To the best of our knowledge, this is the first time that thesetwo important aspects of quantum neural networks are explicitly connected.", "output": "The quantum cost function concentration dependency on the parametrization expressivity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recruitment of trustworthy and high-quality workers is an importantresearch issue for MCS. Previous studies either assume that the qualities ofworkers are known in advance, or assume that the platform knows the qualitiesof workers once it receives their collected data. In reality, to reduce costsand thus maximize revenue, many strategic workers do not perform their sensingtasks honestly and report fake data to the platform, which is called False dataattacks. And it is very hard for the platform to evaluate the authenticity ofthe received data. In this paper, an incentive mechanism named Semi-supervisionbased Combinatorial Multi-Armed Bandit reverse Auction (SCMABA) is proposed tosolve the recruitment problem of multiple unknown and strategic workers in MCS.First, we model the worker recruitment as a multi-armed bandit reverse auctionproblem and design an UCB-based algorithm to separate the exploration andexploitation, regarding the Sensing Rates (SRs) of recruited workers as thegain of the bandit. Next, a Semi-supervised Sensing Rate Learning (SSRL)approach is proposed to quickly and accurately obtain the workers' SRs, whichconsists of two phases, supervision and self-supervision. Last, SCMABA isdesigned organically combining the SRs acquisition mechanism with multi-armedbandit reverse auction, where supervised SR learning is used in theexploration, and the self-supervised one is used in the exploitation. Wetheoretically prove that our SCMABA achieves truthfulness and individualrationality and exhibits outstanding performances of the SCMABA mechanismthrough in-depth simulations of real-world data traces.", "output": "A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Following the traditional paradigm of convolutional neural networks (CNNs),modern CNNs manage to keep pace with more recent, for exampletransformer-based, models by not only increasing model depth and width but alsothe kernel size. This results in large amounts of learnable model parametersthat need to be handled during training. While following the convolutionalparadigm with the according spatial inductive bias, we question thesignificance of emph{learned} convolution filters. In fact, our findingsdemonstrate that many contemporary CNN architectures can achieve high testaccuracies without ever updating randomly initialized (spatial) convolutionfilters. Instead, simple linear combinations (implemented through efficient$1times 1$ convolutions) suffice to effectively recombine even random filtersinto expressive network operators. Furthermore, these combinations of randomfilters can implicitly regularize the resulting operations, mitigatingoverfitting and enhancing overall performance and robustness. Conversely,retaining the ability to learn filter updates can impair network performance.Lastly, although we only observe relatively small gains from learning $3times3$ convolutions, the learning gains increase proportionally with kernel size,owing to the non-idealities of the independent and identically distributed(textit{i.i.d.}) nature of default initialization techniques.", "output": "The Power of Linear Combinations: Learning with Random Convolutions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The advances in variational inference are providing promising paths inBayesian estimation problems. These advances make variational phylogeneticinference an alternative approach to Markov Chain Monte Carlo methods forapproximating the phylogenetic posterior. However, one of the main drawbacks ofsuch approaches is the modelling of the prior through fixed distributions,which could bias the posterior approximation if they are distant from thecurrent data distribution. In this paper, we propose an approach and animplementation framework to relax the rigidity of the prior densities bylearning their parameters using a gradient-based method and a neuralnetwork-based parameterization. We applied this approach for branch lengths andevolutionary parameters estimation under several Markov chain substitutionmodels. The results of performed simulations show that the approach is powerfulin estimating branch lengths and evolutionary model parameters. They also showthat a flexible prior model could provide better results than a predefinedprior model. Finally, the results highlight that using neural networks improvesthe initialization of the optimization of the prior density parameters.", "output": "Prior Density Learning in Variational Bayesian Phylogenetic Parameters Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a new model, independent linear Markov game, for multi-agentreinforcement learning with a large state space and a large number of agents.This is a class of Markov games with independent linear function approximation,where each agent has its own function approximation for the state-action valuefunctions that are marginalized by other players' policies. We design newalgorithms for learning the Markov coarse correlated equilibria (CCE) andMarkov correlated equilibria (CE) with sample complexity bounds that only scalepolynomially with each agent's own function class complexity, thus breaking thecurse of multiagents. In contrast, existing works for Markov games withfunction approximation have sample complexity bounds scale with the size of theemph{joint action space} when specialized to the canonical tabular Markov gamesetting, which is exponentially large in the number of agents. Our algorithmsrely on two key technical innovations: (1) utilizing policy replay to tacklenon-stationarity incurred by multiple agents and the use of functionapproximation; (2) separating learning Markov equilibria and exploration in theMarkov games, which allows us to use the full-information no-regret learningoracle instead of the stronger bandit-feedback no-regret learning oracle usedin the tabular setting. Furthermore, we propose an iterative-best-response typealgorithm that can learn pure Markov Nash equilibria in independent linearMarkov potential games. In the tabular case, by adapting the policy replaymechanism for independent linear Markov games, we propose an algorithm with$widetilde{O}(epsilon^{-2})$ sample complexity to learn Markov CCE, whichimproves the state-of-the-art result $widetilde{O}(epsilon^{-3})$ inDaskalakis et al. 2022, where $epsilon$ is the desired accuracy, and alsosignificantly improves other problem parameters.", "output": "Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We focus on decentralized stochastic non-convex optimization, where $n$agents work together to optimize a composite objective function which is a sumof a smooth term and a non-smooth convex term. To solve this problem, wepropose two single-time scale algorithms: Prox-DASA and Prox-DASA-GT. Thesealgorithms can find $epsilon$-stationary points in$mathcal{O}(n^{-1}epsilon^{-2})$ iterations using constant batch sizes (i.e.,$mathcal{O}(1)$). Unlike prior work, our algorithms achieve comparablecomplexity without requiring large batch sizes, more complex per-iterationoperations (such as double loops), or stronger assumptions. Our theoreticalfindings are supported by extensive numerical experiments, which demonstratethe superiority of our algorithms over previous approaches. Our code isavailable at ", "output": "A One-Sample Decentralized Proximal Algorithm for Non-Convex Stochastic Composite Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a symmetric mixture of linear regressions with random samplesfrom the pairwise comparison design, which can be seen as a noisy version of atype of Euclidean distance geometry problem. We analyze theexpectation-maximization (EM) algorithm locally around the ground truth andestablish that the sequence converges linearly, providing an $ell_infty$-normguarantee on the estimation error of the iterates. Furthermore, we show thatthe limit of the EM sequence achieves the sharp rate of estimation in the$ell_2$-norm, matching the information-theoretically optimal constant. We alsoargue through simulation that convergence from a random initialization is muchmore delicate in this setting, and does not appear to occur in general. Ourresults show that the EM algorithm can exhibit several unique behaviors whenthe covariate distribution is suitably structured.", "output": "Sharp analysis of EM for learning mixtures of pairwise differences."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNNs) have shown high potential for a variety ofreal-world, challenging applications, but one of the major obstacles in GNNresearch is the lack of large-scale flexible datasets. Most existing publicdatasets for GNNs are relatively small, which limits the ability of GNNs togeneralize to unseen data. The few existing large-scale graph datasets providevery limited labeled data. This makes it difficult to determine if the GNNmodel's low accuracy for unseen data is inherently due to insufficient trainingdata or if the model failed to generalize. Additionally, datasets used to trainGNNs need to offer flexibility to enable a thorough study of the impact ofvarious factors while training GNN models.In this work, we introduce the Illinois Graph Benchmark (IGB), a researchdataset tool that the developers can use to train, scrutinize andsystematically evaluate GNN models with high fidelity. IGB includes bothhomogeneous and heterogeneous academic graphs of enormous sizes, with more than40% of their nodes labeled. Compared to the largest graph datasets publiclyavailable, the IGB provides over 162X more labeled data for deep learningpractitioners and developers to create and evaluate models with higheraccuracy. The IGB dataset is a collection of academic graphs designed to beflexible, enabling the study of various GNN architectures, embedding generationtechniques, and analyzing system performance issues for node classificationtasks. IGB is open-sourced, supports DGL and PyG frameworks, and comes withreleases of the raw text that we believe foster emerging language models andGNN research projects. An early public version of IGB is available at", "output": "IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel approach based on sparse Gaussian processes (SGPs) toaddress the sensor placement problem for monitoring spatially (orspatiotemporally) correlated phenomena such as temperature and precipitation.Existing Gaussian process (GP) based sensor placement approaches use GPs withknown kernel function parameters to model a phenomenon and subsequentlyoptimize the sensor locations in a discretized representation of theenvironment. In our approach, we fit an SGP with known kernel functionparameters to randomly sampled unlabeled locations in the environment and showthat the learned inducing points of the SGP inherently solve the sensorplacement problem in continuous spaces. Using SGPs avoids discretizing theenvironment and reduces the computation cost from cubic to linear complexity.When restricted to a candidate set of sensor placement locations, we can usegreedy sequential selection algorithms on the SGP's optimization bound to findgood solutions. We also present an approach to efficiently map our continuousspace solutions to discrete solution spaces using the assignment problem, whichgives us discrete sensor placements optimized in unison. Moreover, wegeneralize our approach to model sensors with non-point field-of-view andintegrated observations by leveraging the inherent properties of GPs and SGPs.Our experimental results on three real-world datasets show that our approachesgenerate solution placements that result in reconstruction quality that isconsistently on par or better than the prior state-of-the-art approach whilebeing significantly faster. Our computationally efficient approaches willenable both large-scale sensor placement, and fast sensor placement forinformative path planning problems.", "output": "Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While artificial intelligence (AI) models have achieved human or evensuperhuman performance in many well-defined applications, they still struggleto show signs of broad and flexible intelligence. The Abstraction and ReasoningCorpus (ARC), a visual intelligence benchmark introduced by Franc{c}oisChollet, aims to assess how close AI systems are to human-like cognitiveabilities. Most current approaches rely on carefully handcrafteddomain-specific program searches to brute-force solutions for the tasks presentin ARC. In this work, we propose a general learning-based framework for solvingARC. It is centered on transforming tasks from the vision to the languagedomain. This composition of language and vision allows for pre-trained modelsto be leveraged at each stage, enabling a shift from handcrafted priors towardsthe learned priors of the models. While not yet beating state-of-the-art modelson ARC, we demonstrate the potential of our approach, for instance, by solvingsome ARC tasks that have not been solved previously.", "output": "Abstract Visual Reasoning Enabled by Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world time-series datasets are often multivariate with complex dynamics.To capture this complexity, high capacity architectures like recurrent- orattention-based sequential deep learning models have become popular. However,recent work demonstrates that simple univariate linear models can outperformsuch deep learning models on several commonly used academic benchmarks.Extending them, in this paper, we investigate the capabilities of linear modelsfor time-series forecasting and present Time-Series Mixer (TSMixer), a novelarchitecture designed by stacking multi-layer perceptrons (MLPs). TSMixer isbased on mixing operations along both the time and feature dimensions toextract information efficiently. On popular academic benchmarks, thesimple-to-implement TSMixer is comparable to specialized state-of-the-artmodels that leverage the inductive biases of specific benchmarks. On thechallenging and large scale M5 benchmark, a real-world retail dataset, TSMixerdemonstrates superior performance compared to the state-of-the-artalternatives. Our results underline the importance of efficiently utilizingcross-variate and auxiliary information for improving the performance of timeseries forecasting. We present various analyses to shed light into thecapabilities of TSMixer. The design paradigms utilized in TSMixer are expectedto open new horizons for deep learning-based time series forecasting. Theimplementation is available at", "output": "TSMixer: An all-MLP Architecture for Time Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An increasing share of energy is produced from renewable sources by manysmall producers. The efficiency of those sources is volatile and, to someextent, random, exacerbating the problem of energy market balancing. In manycountries, this balancing is done on the day-ahead (DA) energy markets. Thispaper considers automated trading on the DA energy market by a medium sizeprosumer. We model this activity as a Markov Decision Process and formalize aframework in which an applicable in real-life strategy can be optimized withoff-line data. We design a trading strategy that is fed with the availableenvironmental information that can impact future prices, including weatherforecasts. We use state-of-the-art reinforcement learning (RL) algorithms tooptimize this strategy. For comparison, we also synthesize a simple parametrictrading strategy and optimize it with an evolutionary algorithm. Results showthat our RL-based strategy generates the highest market profits.", "output": "On-line reinforcement learning for optimization of real-life energy trading strategy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bandit learning has been an increasingly popular design choice forrecommender system. Despite the strong interest in bandit learning from thecommunity, there remains multiple bottlenecks that prevent many bandit learningapproaches from productionalization. One major bottleneck is how to test theeffectiveness of bandit algorithm with fairness and without data leakage.Different from supervised learning algorithms, bandit learning algorithmsemphasize greatly on the data collection process through their explorativenature. Such explorative behavior may induce unfair evaluation in a classic A/Btest setting. In this work, we apply upper confidence bound (UCB) to our largescale short video recommender system and present a test framework for theproduction bandit learning life-cycle with a new set of metrics. Extensiveexperiment results show that our experiment design is able to fairly evaluatethe performance of bandit learning in the recommender system.", "output": "Evaluating Online Bandit Exploration In Large-Scale Recommender System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of emph{pruning} a classification tree, that is,selecting a suitable subtree that balances bias and variance, in commonsituations with inhomogeneous training data. Namely, assuming access to mostlydata from a distribution $P_{X, Y}$, but little data from a desireddistribution $Q_{X, Y}$ with different $X$-marginals, we present the firstefficient procedure for optimal pruning in such situations, whencross-validation and other penalized variants are grossly inadequate.Optimality is derived with respect to a notion of emph{average discrepancy}$P_{X} to Q_{X}$ (averaged over $X$ space) which significantly relaxes arecent notion -- termed emph{transfer-exponent} -- shown to tightly capturethe limits of classification under such a distribution shift. Our relaxednotion can be viewed as a measure of emph{relative dimension} betweendistributions, as it relates to existing notions of information such as theMinkowski and Renyi dimensions.", "output": "Classification Tree Pruning Under Covariate Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We would like to explore how morphemes can affect the performance of alanguage model. We trained GPT-2 and Bert model with StateMorph for bothFinnish and Russian, which is a morpheme segmenting algorithm. As a comparison,we also trained a model with BPE and Morfessor. Our preliminary result showsthat StateMorph can help the model to converge more efficiently and achieve abetter validation score.", "output": "Investigating the effect of sub-word segmentation on the performance of transformer language models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present CosmoPower-JAX, a JAX-based implementation of the CosmoPowerframework, which accelerates cosmological inference by building neuralemulators of cosmological power spectra. We show how, using the automaticdifferentiation, batch evaluation and just-in-time compilation features of JAX,and running the inference pipeline on graphics processing units (GPUs),parameter estimation can be accelerated by orders of magnitude with advancedgradient-based sampling techniques. These can be used to efficiently explorehigh-dimensional parameter spaces, such as those needed for the analysis ofnext-generation cosmological surveys. We showcase the accuracy andcomputational efficiency of CosmoPower-JAX on two simulated Stage IVconfigurations. We first consider a single survey performing a cosmic shearanalysis totalling 37 model parameters. We validate the contours derived withCosmoPower-JAX and a Hamiltonian Monte Carlo sampler against those derived witha nested sampler and without emulators, obtaining a speed-up factor of$mathcal{O}(10^3)$. We then consider a combination of three Stage IV surveys,each performing a joint cosmic shear and galaxy clustering (3x2pt) analysis,for a total of 157 model parameters. Even with such a high-dimensionalparameter space, CosmoPower-JAX provides converged posterior contours in 3days, as opposed to the estimated 6 years required by standard methods.CosmoPower-JAX is fully written in Python, and we make it publicly available tohelp the cosmological community meet the accuracy requirements set bynext-generation surveys.", "output": "CosmoPower-JAX: high-dimensional Bayesian inference with differentiable cosmological emulators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Augmenting large language models (LLMs) with external tools has emerged as apromising approach to solving complex problems. However, traditional methods,which finetune LLMs with tool demonstration data, can be both costly andrestricted to a predefined set of tools. Recent in-context learning paradigmalleviates these issues, but the limited context length only allows for a fewshots of demonstrations, leading to suboptimal understandings of the tools.Moreover, when there are numerous tools to choose from, in-context learningcould completely fail to work. In this paper, we propose an alternativeapproach, $textbf{ToolkenGPT}$, which combines the benefits of both sides. Ourapproach represents each $underline{tool}$ as a to$underline{ken}$($textit{toolken}$) and learns an embedding for it, enabling tool calls in thesame way as generating a regular word token. Once a toolken is triggered, theLLM is prompted to complete arguments for the tool to execute. ToolkenGPToffers the flexibility to plug in an arbitrary number of tools by expanding theset of toolkens on the fly. In addition, it improves tool use by allowingextensive demonstration data for learning the toolken embeddings. In diversedomains, including numerical reasoning, knowledge-based question answering, andembodied plan generation, our approach effectively augments LLMs with tools andsubstantially outperforms various latest baselines. ToolkenGPT demonstrates thepromising ability to use relevant tools from a large tool set in complexscenarios.", "output": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Biogenic Volatile Organic Compounds (BVOCs) emitted from the terrestrialecosystem into the Earth's atmosphere are an important component of atmosphericchemistry. Due to the scarcity of measurement, a reliable enhancement of BVOCsemission maps can aid in providing denser data for atmospheric chemical,climate, and air quality models. In this work, we propose a strategy tosuper-resolve coarse BVOC emission maps by simultaneously exploiting thecontributions of different compounds. To this purpose, we first accuratelyinvestigate the spatial inter-connections between several BVOC species. Then,we exploit the found similarities to build a Multi-Image Super-Resolution(MISR) system, in which a number of emission maps associated with diversecompounds are aggregated to boost Super-Resolution (SR) performance. We comparedifferent configurations regarding the species and the number of joined BVOCs.Our experimental results show that incorporating BVOCs' relationship into theprocess can substantially improve the accuracy of the super-resolved maps.Interestingly, the best results are achieved when we aggregate the emissionmaps of strongly uncorrelated compounds. This peculiarity seems to confirm whatwas already guessed for other data-domains, i.e., joined uncorrelatedinformation are more helpful than correlated ones to boost MISR performance.Nonetheless, the proposed work represents the first attempt in SR of BVOCemissions through the fusion of multiple different compounds.", "output": "Multi-BVOC Super-Resolution Exploiting Compounds Inter-Connection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy-based models (EBMs) are versatile density estimation models thatdirectly parameterize an unnormalized log density. Although very flexible, EBMslack a specified normalization constant of the model, making the likelihood ofthe model computationally intractable. Several approximate samplers andvariational inference techniques have been proposed to estimate the likelihoodgradients for training. These techniques have shown promising results ingenerating samples, but little attention has been paid to the statisticalaccuracy of the estimated density, such as determining the relative importanceof different classes in a dataset. In this work, we propose a new maximumlikelihood training algorithm for EBMs that uses a different type of generativemodel, normalizing flows (NF), which have recently been proposed to facilitatesampling. Our method fits an NF to an EBM during training so that anNF-assisted sampling scheme provides an accurate gradient for the EBMs at alltimes, ultimately leading to a fast sampler for generating new data.", "output": "Balanced Training of Energy-Based Models with Adaptive Flow Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evaluation of noisy binary classifiers on unlabeled data is treated as astreaming task: given a data sketch of the decisions by an ensemble, estimatethe true prevalence of the labels as well as each classifier's accuracy onthem. Two fully algebraic evaluators are constructed to do this. Both are basedon the assumption that the classifiers make independent errors. The first isbased on majority voting. The second, the main contribution of the paper, isguaranteed to be correct. But how do we know the classifiers are independent onany given test? This principal/agent monitoring paradox is ameliorated byexploiting the failures of the independent evaluator to return sensibleestimates. A search for nearly error independent trios is empirically carriedout on the texttt{adult}, texttt{mushroom}, and texttt{two-norm} datasets byusing the algebraic failure modes to reject evaluation ensembles as toocorrelated. The searches are refined by constructing a surface in evaluationspace that contains the true value point. The algebra of arbitrarily correlatedclassifiers permits the selection of a polynomial subset free of anycorrelation variables. Candidate evaluation ensembles are rejected if theirdata sketches produce independent estimates too far from the constructedsurface. The results produced by the surviving ensembles can sometimes be asgood as 1%. But handling even small amounts of correlation remains achallenge. A Taylor expansion of the estimates produced when independence isassumed but the classifiers are, in fact, slightly correlated helps clarify howthe independent evaluator has algebraic `blind spots'.", "output": "Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have achieved state-of-the-art performance in generatingmany different kinds of data, including images, text, and videos. Despite theirsuccess, there has been limited research on how the underlying diffusionprocess and the final convergent prior can affect generative performance; thisresearch has also been limited to continuous data types and a score-baseddiffusion framework. To fill this gap, we explore how different discretediffusion kernels (which converge to different prior distributions) affect theperformance of diffusion models for graphs. To this end, we developed a novelformulation of a family of discrete diffusion kernels which are easilyadjustable to converge to different Bernoulli priors, and we study the effectof these different kernels on generative performance. We show that the qualityof generated graphs is sensitive to the prior used, and that the optimal choicecannot be explained by obvious statistics or metrics, which challenges theintuitions which previous works have suggested.", "output": "Complex Preferences for Different Convergent Priors in Discrete Graph Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Network designs are quite diverse, from VGG-style to ResNet-style, andfrom Convolutional Neural Networks to Transformers. Towards the design ofefficient accelerators, many works have adopted a dataflow-based, inter-layerpipelined architecture, with a customised hardware towards each layer,achieving ultra high throughput and low latency. The deployment of neuralnetworks to such dataflow architecture accelerators is usually hindered by theavailable on-chip memory as it is desirable to preload the weights of neuralnetworks on-chip to maximise the system performance. To address this, networksare usually compressed before the deployment through methods such as pruning,quantization and tensor decomposition. In this paper, a framework for mappingCNNs onto FPGAs based on a novel tensor decomposition method called Mixed-TD isproposed. The proposed method applies layer-specific Singular ValueDecomposition (SVD) and Canonical Polyadic Decomposition (CPD) in a mixedmanner, achieving 1.73x to 10.29x throughput per DSP to state-of-the-art CNNs.Our work is open-sourced: ", "output": "Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpretable time series prediction is crucial for safety-critical areassuch as healthcare and autonomous driving. Most existing methods focus oninterpreting predictions by assigning important scores to segments of timeseries. In this paper, we take a different and more challenging route and aimat developing a self-interpretable model, dubbed Counterfactual Time Series(CounTS), which generates counterfactual and actionable explanations for timeseries predictions. Specifically, we formalize the problem of time seriescounterfactual explanations, establish associated evaluation protocols, andpropose a variational Bayesian deep learning model equipped with counterfactualinference capability of time series abduction, action, and prediction. Comparedwith state-of-the-art baselines, our self-interpretable model can generatebetter counterfactual explanations while maintaining comparable predictionaccuracy.", "output": "Self-Interpretable Time Series Prediction with Counterfactual Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Physics-informed neural networks (PINNs) provide a framework to buildsurrogate models for dynamical systems governed by differential equations.During the learning process, PINNs incorporate a physics-based regularizationterm within the loss function to enhance generalization performance. Sincesimulating dynamics controlled by partial differential equations (PDEs) can becomputationally expensive, PINNs have gained popularity in learning parametricsurrogates for fluid flow problems governed by Navier-Stokes equations. In thiswork, we introduce RANS-PINN, a modified PINN framework, to predict flow fields(i.e., velocity and pressure) in high Reynolds number turbulent flow regime. Toaccount for the additional complexity introduced by turbulence, RANS-PINNemploys a 2-equation eddy viscosity model based on a Reynolds-averagedNavier-Stokes (RANS) formulation. Furthermore, we adopt a novel trainingapproach that ensures effective initialization and balance among the variouscomponents of the loss function. The effectiveness of RANS-PINN framework isthen demonstrated using a parametric PINN.", "output": "RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies the fair range clustering problem in which the data pointsare from different demographic groups and the goal is to pick $k$ centers withthe minimum clustering cost such that each group is at least minimallyrepresented in the centers set and no group dominates the centers set. Moreprecisely, given a set of $n$ points in a metric space $(P,d)$ where each pointbelongs to one of the $ell$ different demographics (i.e., $P = P_1 uplus P_2uplus cdots uplus P_ell$) and a set of $ell$ intervals $[alpha_1,beta_1], cdots, [alpha_ell, beta_ell]$ on desired number of centers fromeach group, the goal is to pick a set of $k$ centers $C$ with minimum$ell_p$-clustering cost (i.e., $(sum_{vin P} d(v,C)^p)^{1/p}$) such that foreach group $iin ell$, $|Ccap P_i| in [alpha_i, beta_i]$. In particular,the fair range $ell_p$-clustering captures fair range $k$-center, $k$-medianand $k$-means as its special cases. In this work, we provide efficient constantfactor approximation algorithms for fair range $ell_p$-clustering for allvalues of $pin [1,infty)$.", "output": "Approximation Algorithms for Fair Range Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deploying Large language models (LLMs) can pose hazards from harmful outputssuch as toxic or dishonest speech. Prior work has introduced tools that elicitharmful outputs in order to identify and mitigate these risks. While this is avaluable step toward securing language models, these approaches typically relyon a pre-existing classifier for undesired outputs. This limits theirapplication to situations where the type of harmful behavior is known withprecision beforehand. However, this skips a central challenge of red teaming:developing a contextual understanding of the behaviors that a model canexhibit. Furthermore, when such a classifier already exists, red teaming haslimited marginal value because the classifier could simply be used to filtertraining data or model outputs. In this work, we consider red teaming under theassumption that the adversary is working from a high-level, abstractspecification of undesired behavior. The red team is expected to refine/extendthis specification and identify methods to elicit this behavior from the model.Our red teaming framework consists of three steps: 1) Exploring the model'sbehavior in the desired context; 2) Establishing a measurement of undesiredbehavior (e.g., a classifier trained to reflect human evaluations); and 3)Exploiting the model's flaws using this measure and an established red teamingmethodology. We apply this approach to red team GPT-2 and GPT-3 models tosystematically discover classes of prompts that elicit toxic and dishoneststatements. In doing so, we also construct and release the CommonClaim datasetof 20,000 statements that have been labeled by human subjects ascommon-knowledge-true, common-knowledge-false, or neither. Code is available at CommonClaimis available at ", "output": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) enables multiple clients to collaboratively learn amachine learning model without exchanging their own local data. In this way,the server can exploit the computational power of all clients and train themodel on a larger set of data samples among all clients. Although such amechanism is proven to be effective in various fields, existing works generallyassume that each client preserves sufficient data for training. In practice,however, certain clients may only contain a limited number of samples (i.e.,few-shot samples). For example, the available photo data taken by a specificuser with a new mobile device is relatively rare. In this scenario, existing FLefforts typically encounter a significant performance drop on these clients.Therefore, it is urgent to develop a few-shot model that can generalize toclients with limited data under the FL scenario. In this paper, we refer tothis novel problem as federated few-shot learning. Nevertheless, the problemremains challenging due to two major reasons: the global data variance amongclients (i.e., the difference in data distributions among clients) and thelocal data insufficiency in each client (i.e., the lack of adequate local datafor training). To overcome these two challenges, we propose a novel federatedfew-shot learning framework with two separately updated models and dedicatedtraining strategies to reduce the adverse impact of global data variance andlocal data insufficiency. Extensive experiments on four prevalent datasets thatcover news articles and images validate the effectiveness of our frameworkcompared with the state-of-the-art baselines. Our code is provided at", "output": "Federated Few-shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Foley sound generation aims to synthesise the background sound for multimediacontent. Previous models usually employ a large development set with labels asinput (e.g., single numbers or one-hot vector). In this work, we propose adiffusion model based system for Foley sound generation with text conditions.To alleviate the data scarcity issue, our model is initially pre-trained withlarge-scale datasets and fine-tuned to this task via transfer learning usingthe contrastive language-audio pertaining (CLAP) technique. We have observedthat the feature embedding extracted by the text encoder can significantlyaffect the performance of the generation model. Hence, we introduce a trainablelayer after the encoder to improve the text embedding produced by the encoder.In addition, we further refine the generated waveform by generating multiplecandidate audio clips simultaneously and selecting the best one, which isdetermined in terms of the similarity score between the embedding of thecandidate clips and the embedding of the target text label. Using the proposedmethod, our system ranks ${1}^{st}$ among the systems submitted to DCASEChallenge 2023 Task 7. The results of the ablation studies illustrate that theproposed techniques significantly improve sound generation performance. Thecodes for implementing the proposed system are available online.", "output": "Text-Driven Foley Sound Generation With Latent Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of variational quantum algorithms is crucial for theapplication of NISQ computers. Such algorithms require short quantum circuits,which are more amenable to implementation on near-term hardware, and many suchmethods have been developed. One of particular interest is the so-called thevariational diagonalization method, which constitutes an important algorithmicsubroutine, and it can be used directly for working with data encoded inquantum states. In particular, it can be applied to discern the features ofquantum states, such as entanglement properties of a system, or in quantummachine learning algorithms. In this work, we tackle the problem of designing avery shallow quantum circuit, required in the quantum state diagonalizationtask, by utilizing reinforcement learning. To achieve this, we utilize a novelencoding method that can be used to tackle the problem of circuit depthoptimization using a reinforcement learning approach. We demonstrate that ourapproach provides a solid approximation to the diagonalization task while usinga small number of gates. The circuits proposed by the reinforcement learningmethods are shallower than the standard variational quantum statediagonalization algorithm, and thus can be used in situations where the depthof quantum circuits is limited by the hardware capabilities.", "output": "Enhancing variational quantum state diagonalization using reinforcement learning techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent accelerations in multi-modal applications have been made possible withthe plethora of image and text data available online. However, the scarcity ofanalogous data in the medical field, specifically in histopathology, has haltedcomparable progress. To enable similar representation learning forhistopathology, we turn to YouTube, an untapped resource of videos, offering$1,087$ hours of valuable educational histopathology videos from expertclinicians. From YouTube, we curate Quilt: a large-scale vision-languagedataset consisting of $768,826$ image and text pairs. Quilt was automaticallycurated using a mixture of models, including large language models, handcraftedalgorithms, human knowledge databases, and automatic speech recognition. Incomparison, the most comprehensive datasets curated for histopathology amassonly around $200$K samples. We combine Quilt with datasets from other sources,including Twitter, research papers, and the internet in general, to create aneven larger dataset: Quilt-1M, with $1$M paired image-text samples, marking itas the largest vision-language histopathology dataset to date. We demonstratethe value of Quilt-1M by fine-tuning a pre-trained CLIP model. Our modeloutperforms state-of-the-art models on both zero-shot and linear probing tasksfor classifying new histopathology images across $13$ diverse patch-leveldatasets of $8$ different sub-pathologies and cross-modal retrieval tasks.", "output": "Quilt-1M: One Million Image-Text Pairs for Histopathology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine Learning as a Service (MLaaS) platforms have gained popularity due totheir accessibility, cost-efficiency, scalability, and rapid developmentcapabilities. However, recent research has highlighted the vulnerability ofcloud-based models in MLaaS to model extraction attacks. In this paper, weintroduce FDINET, a novel defense mechanism that leverages the featuredistribution of deep neural network (DNN) models. Concretely, by analyzing thefeature distribution from the adversary's queries, we reveal that the featuredistribution of these queries deviates from that of the model's training set.Based on this key observation, we propose Feature Distortion Index (FDI), ametric designed to quantitatively measure the feature distribution deviation ofreceived queries. The proposed FDINET utilizes FDI to train a binary detectorand exploits FDI similarity to identify colluding adversaries from distributedextraction attacks. We conduct extensive experiments to evaluate FDINET againstsix state-of-the-art extraction attacks on four benchmark datasets and fourpopular model architectures. Empirical results demonstrate the followingfindings FDINET proves to be highly effective in detecting model extraction,achieving a 100% detection accuracy on DFME and DaST. FDINET is highlyefficient, using just 50 queries to raise an extraction alarm with an averageconfidence of 96.08% for GTSRB. FDINET exhibits the capability to identifycolluding adversaries with an accuracy exceeding 91%. Additionally, itdemonstrates the ability to detect two types of adaptive attacks.", "output": "FDINet: Protecting against DNN Model Extraction via Feature Distortion Index."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The concentration of power in a few digital technology companies has become asubject of increasing interest in both academic and non-academic discussions.One of the most noteworthy contributions to the debate is Lina Khan's Amazon'sAntitrust Paradox. In this work, Khan contends that Amazon has systematicallyexerted its dominance in online retail to eliminate competitors andsubsequently charge above-market prices. This work contributed to Khan'sappointment as the chair of the US Federal Trade Commission (FTC), one of themost influential antitrust organizations. Today, several ongoing antitrustlawsuits in the US and Europe involve major technology companies like Apple,Google/Alphabet, and Facebook/Meta. In the realm of generative AI, we are onceagain witnessing the same companies taking the lead in technologicaladvancements, leaving little room for others to compete. This article examinesthe market dominance of these corporations in the technology stack behindgenerative AI from an antitrust law perspective.", "output": "Exploring Antitrust and Platform Power in Generative AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Structure-based drug design (SBDD), which utilizes the three-dimensionalgeometry of proteins to identify potential drug candidates, is becomingincreasingly vital in drug discovery. However, traditional methods based onphysiochemical modeling and experts' domain knowledge are time-consuming andlaborious. The recent advancements in geometric deep learning, which integratesand processes 3D geometric data, coupled with the availability of accurateprotein 3D structure predictions from tools like AlphaFold, have significantlypropelled progress in structure-based drug design. In this paper, wesystematically review the recent progress of geometric deep learning forstructure-based drug design. We start with a brief discussion of the mainstreamtasks in structure-based drug design, commonly used 3D protein representationsand representative predictive/generative models. Then we delve into detailedreviews for each task (binding site prediction, binding pose generation,emph{de novo} molecule generation, linker design, and binding affinityprediction), including the problem setup, representative methods, datasets, andevaluation metrics. Finally, we conclude this survey with the currentchallenges and highlight potential opportunities of geometric deep learning forstructure-based drug design.", "output": "A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce AdCraft, a novel benchmark environment for the ReinforcementLearning (RL) community distinguished by its stochastic and non-stationaryproperties. The environment simulates bidding and budgeting dynamics withinSearch Engine Marketing (SEM), a digital marketing technique utilizing paidadvertising to enhance the visibility of websites on search engine resultspages (SERPs). The performance of SEM advertisement campaigns depends onseveral factors, including keyword selection, ad design, bid management, budgetadjustments, and performance monitoring. Deep RL recently emerged as apotential strategy to optimize campaign profitability within the complex anddynamic landscape of SEM but it requires substantial data, which may be costlyor infeasible to acquire in practice. Our customizable environment enablespractitioners to assess and enhance the robustness of RL algorithms pertinentto SEM bid and budget management without such costs. Through a series ofexperiments within the environment, we demonstrate the challenges imposed bysparsity and non-stationarity on agent convergence and performance. We hopethese challenges further encourage discourse and development around effectivestrategies for managing real-world uncertainties.", "output": "AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work focuses on developing methods for approximating the solutionoperators of a class of parametric partial differential equations via neuraloperators. Neural operators have several challenges, including the issue ofgenerating appropriate training data, cost-accuracy trade-offs, and nontrivialhyperparameter tuning. The unpredictability of the accuracy of neural operatorsimpacts their applications in downstream problems of inference, optimization,and control. A framework is proposed based on the linear variational problemthat gives the correction to the prediction furnished by neural operators. Theoperator associated with the corrector problem is referred to as the correctoroperator. Numerical results involving a nonlinear diffusion model in twodimensions with PCANet-type neural operators show almost two orders of increasein the accuracy of approximations when neural operators are corrected using theproposed scheme. Further, topology optimization involving a nonlinear diffusionmodel is considered to highlight the limitations of neural operators and theefficacy of the correction scheme. Optimizers with neural operator surrogatesare seen to make significant errors (as high as 80 percent). However, theerrors are much lower (below 7 percent) when neural operators are correctedfollowing the proposed method.", "output": "Corrector Operator to Enhance Accuracy and Reliability of Neural Operator Surrogates of Nonlinear Variational Boundary-Value Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bayesian deep learning (BDL) is a promising approach to achievewell-calibrated predictions on distribution-shifted data. Nevertheless, thereexists no large-scale survey that evaluates recent SOTA methods on diverse,realistic, and challenging benchmark tasks in a systematic manner. To provide aclear picture of the current state of BDL research, we evaluate modern BDLalgorithms on real-world datasets from the WILDS collection containingchallenging classification and regression tasks, with a focus on generalizationcapability and calibration under distribution shift. We compare the algorithmson a wide range of large, convolutional and transformer-based neural networkarchitectures. In particular, we investigate a signed version of the expectedcalibration error that reveals whether the methods are over- orunder-confident, providing further insight into the behavior of the methods.Further, we provide the first systematic evaluation of BDL for fine-tuninglarge pre-trained models, where training from scratch is prohibitivelyexpensive. Finally, given the recent success of Deep Ensembles, we extendpopular single-mode posterior approximations to multiple modes by the use ofensembles. While we find that ensembling single-mode approximations generallyimproves the generalization capability and calibration of the models by asignificant margin, we also identify a failure mode of ensembles whenfinetuning large transformer-based language models. In this setting,variational inference based approaches such as last-layer Bayes By Backpropoutperform other methods in terms of accuracy by a large margin, while modernapproximate inference algorithms such as SWAG achieve the best calibration.", "output": "Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In stochastic zeroth-order optimization, a problem of practical relevance isunderstanding how to fully exploit the local geometry of the underlyingobjective function. We consider a fundamental setting in which the objectivefunction is quadratic, and provide the first tight characterization of theoptimal Hessian-dependent sample complexity. Our contribution is twofold.First, from an information-theoretic point of view, we prove tight lower boundson Hessian-dependent complexities by introducing a concept called energyallocation, which captures the interaction between the searching algorithm andthe geometry of objective functions. A matching upper bound is obtained bysolving the optimal energy spectrum. Then, algorithmically, we show theexistence of a Hessian-independent algorithm that universally achieves theasymptotic optimal sample complexities for all Hessian instances. The optimalsample complexities achieved by our algorithm remain valid for heavy-tailednoise distributions, which are enabled by a truncation method.", "output": "Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks possess formidable representational power, rendering theminvaluable in solving complex quantum many-body systems. While they excel atanalyzing static solutions, nonequilibrium processes, including criticaldynamics during a quantum phase transition, pose a greater challenge for neuralnetworks. To address this, we utilize neural networks and machine learningalgorithms to investigate the time evolutions, universal statistics, andcorrelations of topological defects in a one-dimensional transverse-fieldquantum Ising model. Specifically, our analysis involves computing the energyof the system during a quantum phase transition following a linear quench ofthe transverse magnetic field strength. The excitation energies satisfy apower-law relation to the quench rate, indicating a proportional relationshipbetween the excitation energy and the kink numbers. Moreover, we establish auniversal power-law relationship between the first three cumulants of the kinknumbers and the quench rate, indicating a binomial distribution of the kinks.Finally, the normalized kink-kink correlations are also investigated and it isfound that the numerical values are consistent with the analytic formula.", "output": "Learning topological defects formation with neural networks in a quantum phase transition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) for physical design of silicon chips in a Google2021 Nature paper stirred controversy due to poorly documented claims thatraised eyebrows and attracted critical media coverage. The Nature paperwithheld most inputs needed to produce reported results and some critical stepsin the methodology. But two separate evaluations filled in the gaps anddemonstrated that Google RL lags behind human designers, behind a well-knownalgorithm (Simulated Annealing), and also behind generally-available commercialsoftware. Crosschecked data indicate that the integrity of the Nature paper issubstantially undermined owing to errors in the conduct, analysis andreporting.", "output": "The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement."}]