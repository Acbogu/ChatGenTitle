[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several decision problems that are encountered in various business domainscan be modeled as mathematical programs, i.e. optimization problems. Theprocess of conducting such modeling often requires the involvement of expertstrained in operations research and advanced algorithms. Surprisingly, despitethe significant advances in the methods for program and code synthesis, AutoML,learning to optimize etc., there has been little or no attention paid toautomating the task of synthesizing mathematical programs. We imagine ascenario where the specifications for modeling, i.e. the objective andconstraints are expressed in an unstructured form in natural language (NL) andthe mathematical program has to be synthesized from such an NL specification.In this work we evaluate the efficacy of employing CodeT5 with dataaugmentation and post-processing of beams. We utilize GPT-3 with backtranslation for generation of synthetic examples. Further we apply rules oflinear programming to score beams and correct beams based on common errorpatterns. We observe that with these enhancements CodeT5 base gives anexecution accuracy of 0.73 which is significantly better than zero-shotexecution accuracy of 0.41 by ChatGPT and 0.36 by Codex.", "output": "Synthesis of Mathematical programs from Natural Language Specifications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, deep learning models have demonstrated remarkable success invarious domains, such as computer vision, natural language processing, andspeech recognition. However, the generalization capabilities of these modelscan be negatively impacted by the limitations of their feature fusiontechniques. This paper introduces an innovative approach, Adaptive FeatureFusion (AFF), to enhance the generalization of deep learning models bydynamically adapting the fusion process of feature representations.The proposed AFF framework is designed to incorporate fusion layers intoexisting deep learning architectures, enabling seamless integration andimproved performance. By leveraging a combination of data-driven andmodel-based fusion strategies, AFF is able to adaptively fuse features based onthe underlying data characteristics and model requirements. This paper presentsa detailed description of the AFF framework, including the design andimplementation of fusion layers for various architectures.Extensive experiments are conducted on multiple benchmark datasets, with theresults demonstrating the superiority of the AFF approach in comparison totraditional feature fusion techniques. The analysis showcases the effectivenessof AFF in enhancing generalization capabilities, leading to improvedperformance across different tasks and applications.Finally, the paper discusses various real-world use cases where AFF can beemployed, providing insights into its practical applicability. The conclusionhighlights the potential for future research directions, including theexploration of advanced fusion strategies and the extension of AFF to othermachine learning paradigms.", "output": "Adaptive Feature Fusion: Enhancing Generalization in Deep Learning Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, reinforcement learning (RL) has emerged as a popularapproach for solving sequence-based tasks in machine learning. However, findingsuitable alternatives to RL remains an exciting and innovative research area.One such alternative that has garnered attention is the Non-Axiomatic ReasoningSystem (NARS), which is a general-purpose cognitive reasoning framework. Inthis paper, we delve into the potential of NARS as a substitute for RL insolving sequence-based tasks. To investigate this, we conduct a comparativeanalysis of the performance of ONA as an implementation of NARS and$Q$-Learning in various environments that were created using the Open AI gym.The environments have different difficulty levels, ranging from simple tocomplex. Our results demonstrate that NARS is a promising alternative to RL,with competitive performance in diverse environments, particularly innon-deterministic ones.", "output": "Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Why do explainable AI (XAI) explanations in radiology, despite their promiseof transparency, still fail to gain human trust? Current XAI approaches providejustification for predictions, however, these do not meet practitioners' needs.These XAI explanations lack intuitive coverage of the evidentiary basis for agiven classification, posing a significant barrier to adoption. We posit thatXAI explanations that mirror human processes of reasoning and justificationwith evidence may be more useful and trustworthy than traditional visualexplanations like heat maps. Using a radiology case study, we demonstrate howradiology practitioners get other practitioners to see a diagnosticconclusion's validity. Machine-learned classifications lack this evidentiarygrounding and consequently fail to elicit trust and adoption by potentialusers. Insights from this study may generalize to guiding principles forhuman-centered explanation design based on human reasoning and justification ofevidence.", "output": "Explainable AI And Visual Reasoning: Insights From Radiology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a formal context, an ordinal factor is a subset of its incidencerelation that forms a chain in the concept lattice, i.e., a part of the datasetthat corresponds to a linear order. To visualize the data in a formal context,Ganter and Glodeanu proposed a biplot based on two ordinal factors. For thebiplot to be useful, it is important that these factors comprise as much datapoints as possible, i.e., that they cover a large part of the incidencerelation. In this work, we investigate such ordinal two-factorizations. First,we investigate for formal contexts that omit ordinal two-factorizations thedisjointness of the two factors. Then, we show that deciding on the existenceof two-factorizations of a given size is an NP-complete problem which makescomputing maximal factorizations computationally expensive. Finally, we providethe algorithm Ord2Factor that allows us to compute large ordinaltwo-factorizations.", "output": "Maximal Ordinal Two-Factorizations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph collaborative filtering (GCF) is a popular technique for capturinghigh-order collaborative signals in recommendation systems. However, GCF'sbipartite adjacency matrix, which defines the neighbors being aggregated basedon user-item interactions, can be noisy for users/items with abundantinteractions and insufficient for users/items with scarce interactions.Additionally, the adjacency matrix ignores user-user and item-itemcorrelations, which can limit the scope of beneficial neighbors beingaggregated.In this work, we propose a new graph adjacency matrix that incorporatesuser-user and item-item correlations, as well as a properly designed user-iteminteraction matrix that balances the number of interactions across all users.To achieve this, we pre-train a graph-based recommendation method to obtainusers/items embeddings, and then enhance the user-item interaction matrix viatop-K sampling. We also augment the symmetric user-user and item-itemcorrelation components to the adjacency matrix. Our experiments demonstratethat the enhanced user-item interaction matrix with improved neighbors andlower density leads to significant benefits in graph-based recommendation.Moreover, we show that the inclusion of user-user and item-item correlationscan improve recommendations for users with both abundant and insufficientinteractions. The code is in url{", "output": "Graph Collaborative Signals Denoising and Augmentation for Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Decision-focused (DF) model-based reinforcement learning has recently beenintroduced as a powerful algorithm which can focus on learning the MDP dynamicswhich are most relevant for obtaining high rewards. While this approachincreases the performance of agents by focusing the learning towards optimizingfor the reward directly, it does so by learning less accurate dynamics (from aMLE standpoint), and may thus be brittle to changes in the reward function. Inthis work, we develop the robust decision-focused (RDF) algorithm whichleverages the non-identifiability of DF solutions to learn models whichmaximize expected returns while simultaneously learning models which are robustto changes in the reward function. We demonstrate on a variety of toy exampleand healthcare simulators that RDF significantly increases the robustness of DFto changes in the reward function, without decreasing the overall return theagent obtains.", "output": "Robust Decision-Focused Learning for Reward Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wikidata is a knowledge graph increasingly adopted by many communities fordiverse applications. Wikidata statements are annotated with qualifier-valuepairs that are used to depict information, such as the validity context of thestatement, its causality, provenances, etc. Handling the qualifiers inreasoning is a challenging problem. When defining inference rules (inparticular, rules on ontological properties (x subclass of y, z instance of x,etc.)), one must consider the qualifiers, as most of them participate in thesemantics of the statements. This poses a complex problem because a) there is amassive number of qualifiers, and b) the qualifiers of the inferred statementare often a combination of the qualifiers in the rule condition. In this work,we propose to address this problem by a) defining a categorization of thequalifiers b) formalizing the Wikidata model with a many-sorted logicallanguage; the sorts of this language are the qualifier categories. We couplethis logic with an algebraic specification that provides a means foreffectively handling qualifiers in inference rules. The work supports theexpression of all current Wikidata ontological properties. Finally, we discussthe methodology for practically implementing the work and present a prototypeimplementation.", "output": "Handling Wikidata Qualifiers in Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We are developing a virtual coaching system that helps patients adhere tobehavior change interventions (BCI). Our proposed system predicts whether apatient will perform the targeted behavior and uses counterfactual exampleswith feature control to guide personalizsation of BCI. We evaluated ourprediction model using simulated patient data with varying levels ofreceptivity to intervention.", "output": "Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The success of contextual word representations and advances in neuralinformation retrieval have made dense vector-based retrieval a standardapproach for passage and document ranking. While effective and efficient,dual-encoders are brittle to variations in query distributions and noisyqueries. Data augmentation can make models more robust but introduces overheadto training set generation and requires retraining and index regeneration. Wepresent Contrastive Alignment POst Training (CAPOT), a highly efficientfinetuning method that improves model robustness without requiring indexregeneration, the training set optimization, or alteration. CAPOT enablesrobust retrieval by freezing the document encoder while the query encoderlearns to align noisy queries with their unaltered root. We evaluate CAPOTnoisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,finding CAPOT has a similar impact as data augmentation with none of itsoverhead.", "output": "CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in self-supervised learning have demonstrated thateffective visual representations can be learned from unlabeled images. This hasled to increased interest in applying self-supervised learning to the medicaldomain, where unlabeled images are abundant and labeled images are difficult toobtain. However, most self-supervised learning approaches are modeled as imagelevel discriminative or generative proxy tasks, which may not capture the finerlevel representations necessary for dense prediction tasks like multi-organsegmentation. In this paper, we propose a novel contrastive learning frameworkthat integrates Localized Region Contrast (LRC) to enhance existingself-supervised pre-training methods for medical image segmentation. Ourapproach involves identifying Super-pixels by Felzenszwalb's algorithm andperforming local contrastive learning using a novel contrastive sampling loss.Through extensive experiments on three multi-organ segmentation datasets, wedemonstrate that integrating LRC to an existing self-supervised method in alimited annotation setting significantly improves segmentation performance.Moreover, we show that LRC can also be applied to fully-supervised pre-trainingmethods to further boost performance.", "output": "Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The integration of artificial intelligence (AI) technology in the musicindustry is driving a significant change in the way music is being composed,produced and mixed. This study investigates the current state of AI in themixing workflows and its adoption by different user groups. Throughsemi-structured interviews, a questionnaire-based study, and analyzing webforums, the study confirms three user groups comprising amateurs, pro-ams, andprofessionals. Our findings show that while AI mixing tools can simplify theprocess and provide decent results for amateurs, pro-ams seek precise controland customization options, while professionals desire control and customizationoptions in addition to assistive and collaborative technologies. The studyprovides strategies for designing effective AI mixing tools for different usergroups and outlines future directions.", "output": "Adoption of AI Technology in the Music Mixing Workflow: An Investigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scholars in the humanities rely heavily on ancient manuscripts to studyhistory, religion, and socio-political structures in the past. Many effortshave been devoted to digitizing these precious manuscripts using OCRtechnology, but most manuscripts were blemished over the centuries so that anOptical Character Recognition (OCR) program cannot be expected to capture fadedgraphs and stains on pages. This work presents a neural spelling correctionmodel built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisyoutput. This paper is divided into four sections: dataset, model architecture,training and analysis. First, we feature-engineered our raw Tibetan etextcorpus into two sets of structured data frames -- a set of paired toy data anda set of paired real data. Then, we implemented a Confidence Score mechanisminto the Transformer architecture to perform spelling correction tasks.According to the Loss and Character Error Rate, our Transformer + Confidencescore mechanism architecture proves to be superior to Transformer, LSTM-2-LSTMand GRU-2-GRU architectures. Finally, to examine the robustness of our model,we analyzed erroneous tokens, visualized Attention and Self-Attention heatmapsin our model.", "output": "Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised approaches for learning representations invariant to commontransformations are used quite often for object recognition. Learninginvariances makes models more robust and practical to use in real-worldscenarios. Since data transformations that do not change the intrinsicproperties of the object cause the majority of the complexity in recognitiontasks, models that are invariant to these transformations help reduce theamount of training data required. This further increases the model's efficiencyand simplifies training. In this paper, we investigate the generalization ofinvariant representations on out-of-distribution data and try to answer thequestion: Do model representations invariant to some transformations in aparticular seen domain also remain invariant in previously unseen domains?Through extensive experiments, we demonstrate that the invariant model learnsunstructured latent representations that are robust to distribution shifts,thus making invariance a desirable property for training inresource-constrained settings.", "output": "Domain Generalization In Robust Invariant Representation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Harnessing logical reasoning ability is a comprehensive natural languageunderstanding endeavor. With the release of Generative Pretrained Transformer 4(GPT-4), highlighted as \"advanced\" at reasoning tasks, we are eager to learnthe GPT-4 performance on various logical reasoning tasks. This report analysesmultiple logical reasoning datasets, with popular benchmarks like LogiQA andReClor, and newly-released datasets like AR-LSAT. We test the multi-choicereading comprehension and natural language inference tasks with benchmarksrequiring logical reasoning. We further construct a logical reasoningout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.We also make a performance comparison between ChatGPT and GPT-4. Experimentresults show that ChatGPT performs significantly better than the RoBERTafine-tuning method on most logical reasoning benchmarks. GPT-4 shows evenhigher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 dorelatively well on well-known datasets like LogiQA and ReClor. However, theperformance drops significantly when handling newly released andout-of-distribution datasets. Logical reasoning remains challenging for ChatGPTand GPT-4, especially on out-of-distribution and natural language inferencedatasets.", "output": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Believable proxies of human behavior can empower interactive applicationsranging from immersive environments to rehearsal spaces for interpersonalcommunication to prototyping tools. In this paper, we introduce generativeagents--computational software agents that simulate believable human behavior.Generative agents wake up, cook breakfast, and head to work; artists paint,while authors write; they form opinions, notice each other, and initiateconversations; they remember and reflect on days past as they plan the nextday. To enable generative agents, we describe an architecture that extends alarge language model to store a complete record of the agent's experiencesusing natural language, synthesize those memories over time into higher-levelreflections, and retrieve them dynamically to plan behavior. We instantiategenerative agents to populate an interactive sandbox environment inspired byThe Sims, where end users can interact with a small town of twenty five agentsusing natural language. In an evaluation, these generative agents producebelievable individual and emergent social behaviors: for example, starting withonly a single user-specified notion that one agent wants to throw a Valentine'sDay party, the agents autonomously spread invitations to the party over thenext two days, make new acquaintances, ask each other out on dates to theparty, and coordinate to show up for the party together at the right time. Wedemonstrate through ablation that the components of our agentarchitecture--observation, planning, and reflection--each contribute criticallyto the believability of agent behavior. By fusing large language models withcomputational, interactive agents, this work introduces architectural andinteraction patterns for enabling believable simulations of human behavior.", "output": "Generative Agents: Interactive Simulacra of Human Behavior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Safe navigation of drones in the presence of adversarial physical attacksfrom multiple pursuers is a challenging task. This paper proposes a novelapproach, asynchronous multi-stage deep reinforcement learning (AMS-DRL), totrain an adversarial neural network that can learn from the actions of multiplepursuers and adapt quickly to their behavior, enabling the drone to avoidattacks and reach its target. Our approach guarantees convergence by ensuringNash Equilibrium among agents from the game-theory analysis. We evaluate ourmethod in extensive simulations and show that it outperforms baselines withhigher navigation success rates. We also analyze how parameters such as therelative maximum speed affect navigation performance. Furthermore, we haveconducted physical experiments and validated the effectiveness of the trainedpolicies in real-time flights. A success rate heatmap is introduced toelucidate how spatial geometry influences navigation outcomes. Project website:", "output": "AMS-DRL: Learning Multi-Pursuit Evasion for Safe Targeted Navigation of Drones."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of knowledge graph (KG) applications has led to a rising needfor entity alignment (EA) between heterogeneous KGs that are extracted fromvarious sources. Recently, graph neural networks (GNNs) have been widelyadopted in EA tasks due to GNNs' impressive ability to capture structureinformation. However, we have observed that the oversimplified settings of theexisting common EA datasets are distant from real-world scenarios, whichobstructs a full understanding of the advancements achieved by recent methods.This phenomenon makes us ponder: Do existing GNN-based EA methods really makegreat progress?In this paper, to study the performance of EA methods in realistic settings,we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGsand general KGs) which are different with regard to the scale and structure,and share fewer overlapping entities. First, we sweep the unreasonablesettings, and propose two new HHKG datasets that closely mimic real-world EAscenarios. Then, based on the proposed datasets, we conduct extensiveexperiments to evaluate previous representative EA methods, and revealinteresting findings about the progress of GNN-based EA methods. We find thatthe structural information becomes difficult to exploit but still valuable inaligning HHKGs. This phenomenon leads to inferior performance of existing EAmethods, especially GNN-based methods. Our findings shed light on the potentialproblems resulting from an impulsive application of GNN-based methods as apanacea for all EA datasets. Finally, we introduce a simple but effectivemethod: Simple-HHEA, which comprehensively utilizes entity name, structure, andtemporal information. Experiment results show Simple-HHEA outperforms previousmodels on HHKG datasets. The datasets and source code will be available at", "output": "Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In deep learning, mini-batch training is commonly used to optimize networkparameters. However, the traditional mini-batch method may not learn theunder-represented samples and complex patterns in the data, leading to a longertime for generalization. To address this problem, a variant of the traditionalalgorithm has been proposed, which trains the network focusing on mini-batcheswith high loss. The study evaluates the effectiveness of the proposed trainingusing various deep neural networks trained on three benchmark datasets(CIFAR-10, CIFAR-100, and STL-10). The deep neural networks used in the studyare ResNet-18, ResNet-50, Efficient Net B4, EfficientNetV2-S, andMobilenetV3-S. The experimental results showed that the proposed method cansignificantly improve the test accuracy and speed up the convergence comparedto the traditional mini-batch training method. Furthermore, we introduce ahyper-parameter delta ({delta}) that decides how many mini-batches areconsidered for training. Experiments on various values of {delta} found thatthe performance of the proposed method for smaller {delta} values generallyresults in similar test accuracy and faster generalization. We show that theproposed method generalizes in 26.47% less number of epochs than thetraditional mini-batch method in EfficientNet-B4 on STL-10. The proposed methodalso improves the test top-1 accuracy by 7.26% in ResNet-18 on CIFAR-100.", "output": "Can we learn better with hard samples?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Flower breed detection and giving details of that breed with the suggestionof cultivation processes and the way of taking care is important for flowercultivation, breed invention, and the flower business. Among all the localflowers in Bangladesh, the rose is one of the most popular and demandedflowers. Roses are the most desirable flower not only in Bangladesh but alsothroughout the world. Roses can be used for many other purposes apart fromdecoration. As roses have a great demand in the flower business so rose breeddetection will be very essential. However, there is no remarkable work forbreed detection of a particular flower unlike the classification of differentflowers. In this research, we have proposed a model to detect rose breeds fromimages using transfer learning techniques. For such work in flowers, resourcesare not enough in image processing and classification, so we needed a largedataset of the massive number of images to train our model. we have used 1939raw images of five different breeds and we have generated 9306 images for thetraining dataset and 388 images for the testing dataset to validate the modelusing augmentation. We have applied four transfer learning models in thisresearch, which are Inception V3, ResNet50, Xception, and VGG16. Among thesefour models, VGG16 achieved the highest accuracy of 99%, which is an excellentoutcome. Breed detection of a rose by using transfer learning methods is thefirst work on breed detection of a particular flower that is publicly availableaccording to the study.", "output": "Local Rose Breeds Detection System Using Transfer Learning Techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Carrot is a famous nutritional vegetable and developed all over the world.Different diseases of Carrot has become a massive issue in the carrotproduction circle which leads to a tremendous effect on the economic growth inthe agricultural sector. An automatic carrot disease detection system can helpto identify malicious carrots and can provide a guide to cure carrot disease inan earlier stage, resulting in a less economical loss in the carrot productionsystem. The proposed research study has developed a web application Carrot Curebased on Convolutional Neural Network (CNN), which can identify a defectivecarrot and provide a proper curative solution. Images of carrots affected bycavity spot and leaf bright as well as healthy images were collected. Further,this research work has employed Convolutional Neural Network to include birthneural purposes and a Fully Convolutional Neural Network model (FCNN) forinfection order. Different avenues regarding different convolutional modelswith colorful layers are explored and the proposed Convolutional model hasachieved the perfection of 99.8%, which will be useful for the drovers todistinguish carrot illness and boost their advantage.", "output": "Carrot Cure: A CNN based Application to Detect Carrot Disease."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper describes our submission to Task 10 at SemEval 2023-ExplainableDetection of Online Sexism (EDOS), divided into three subtasks. The recent risein social media platforms has seen an increase in disproportionate levels ofsexism experienced by women on social media platforms. This has made detectingand explaining online sexist content more important than ever to make socialmedia safer and more accessible for women. Our approach consists ofexperimenting and finetuning BERT-based models and using a Majority Votingensemble model that outperforms individual baseline model scores. Our systemachieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319for Task C.", "output": "SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Orchestrating a high-quality data preparation program is essential forsuccessful machine learning (ML), but it is known to be time and effortconsuming. Despite the impressive capabilities of large language models likeChatGPT in generating programs by interacting with users through naturallanguage prompts, there are still limitations. Specifically, a user mustprovide specific prompts to iteratively guide ChatGPT in improving datapreparation programs, which requires a certain level of expertise inprogramming, the dataset used and the ML task. Moreover, once a program hasbeen generated, it is non-trivial to revisit a previous version or make changesto the program without starting the process over again. In this paper, wepresent ChatPipe, a novel system designed to facilitate seamless interactionbetween users and ChatGPT. ChatPipe provides users with effectiverecommendation on next data preparation operations, and guides ChatGPT togenerate program for the operations. Also, ChatPipe enables users to easilyroll back to previous versions of the program, which facilitates more efficientexperimentation and testing. We have developed a web application for ChatPipeand prepared several real-world ML tasks from Kaggle. These tasks can showcasethe capabilities of ChatPipe and enable VLDB attendees to easily experimentwith our novel features to rapidly orchestrate a high-quality data preparationprogram.", "output": "ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has achieved impressive performance in many domains, such ascomputer vision and natural language processing, but its advantage overclassical shallow methods on tabular datasets remains questionable. It isespecially challenging to surpass the performance of tree-like ensembles, suchas XGBoost or Random Forests, on small-sized datasets (less than 1k samples).To tackle this challenge, we introduce HyperTab, a hypernetwork-based approachto solving small sample problems on tabular datasets. By combining theadvantages of Random Forests and neural networks, HyperTab generates anensemble of neural networks, where each target model is specialized to processa specific lower-dimensional view of the data. Since each view plays the roleof data augmentation, we virtually increase the number of training sampleswhile keeping the number of trainable parameters unchanged, which preventsmodel overfitting. We evaluated HyperTab on more than 40 tabular datasets of avarying number of samples and domains of origin, and compared its performancewith shallow and deep learning models representing the currentstate-of-the-art. We show that HyperTab consistently outranks other methods onsmall data (with a statistically significant difference) and scores comparableto them on larger datasets.We make a python package with the code available to download at", "output": "HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of deep learning has witnessed significant progress, particularlyin computer vision (CV), natural language processing (NLP), and speech. The useof large-scale models trained on vast amounts of data holds immense promise forpractical applications, enhancing industrial productivity and facilitatingsocial development. With the increasing demands on computational capacity,though numerous studies have explored the efficient training, a comprehensivesummarization on acceleration techniques of training deep learning models isstill much anticipated. In this survey, we present a detailed review fortraining acceleration. We consider the fundamental update formulation and splitits basic components into five main perspectives: (1) data-centric: includingdataset regularization, data sampling, and data-centric curriculum learningtechniques, which can significantly reduce the computational complexity of thedata samples; (2) model-centric, including acceleration of basic modules,compression training, model initialization and model-centric curriculumlearning techniques, which focus on accelerating the training via reducing thecalculations on parameters; (3) optimization-centric, including the selectionof learning rate, the employment of large batchsize, the designs of efficientobjectives, and model average techniques, which pay attention to the trainingpolicy and improving the generality for the large-scale models; (4) budgetedtraining, including some distinctive acceleration methods on source-constrainedsituations; (5) system-centric, including some efficient open-sourcedistributed libraries/systems which provide adequate hardware support for theimplementation of acceleration algorithms. By presenting this comprehensivetaxonomy, our survey presents a comprehensive review to understand the generalmechanisms within each component and their joint interaction.", "output": "On Efficient Training of Large-Scale Deep Learning Models: A Literature Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classical map-based navigation methods are commonly used for robotnavigation, but they often struggle in crowded environments due to the FrozenRobot Problem (FRP). Deep reinforcement learning-based methods address the FRPproblem, however, suffer from the issues of generalization and scalability. Toovercome these challenges, we propose a method that uses Collision Probability(CP) to help the robot navigate safely through crowds. The inclusion of CP inthe observation space gives the robot a sense of the level of danger of themoving crowd. The robot will navigate through the crowd when it appears safebut will take a detour when the crowd is moving aggressively. By focusing onthe most dangerous obstacle, the robot will not be confused when the crowddensity is high, ensuring scalability of the model. Our approach was developedusing deep reinforcement learning (DRL) and trained using the Gazebo simulatorin a non cooperative crowd environment with obstacles moving at randomizedspeeds and directions. We then evaluated our model on four differentcrowd-behavior scenarios with varying densities of crowds. The results shownthat our method achieved a 100% success rate in all test settings. We comparedour approach with a current state-of-the-art DRLbased approach, and ourapproach has performed significantly better. Importantly, our method is highlygeneralizable and requires no fine-tuning after being trained once. We furtherdemonstrated the crowd navigation capability of our model in real-world tests.", "output": "Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This research sets out to assess the viability of using game engines togenerate synthetic training data for machine learning in the context of palletsegmentation. Using synthetic data has been proven in prior research to be aviable means of training neural networks and saves hours of manual labour dueto the reduced need for manual image annotation. Machine vision for palletdetection can benefit from synthetic data as the industry increases thedevelopment of autonomous warehousing technologies. As per our methodology, wedeveloped a tool capable of automatically generating large amounts of annotatedtraining data from 3D models at pixel-perfect accuracy and a much faster ratethan manual approaches. Regarding image segmentation, a Mask R-CNN pipeline wasused, which achieved an AP50 of 86% for individual pallets.", "output": "Pallet Detection from Synthetic Data Using Game Engines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Smart farming is a growing field as technology advances. Plantcharacteristics are crucial indicators for monitoring plant growth. Researchhas been done to estimate characteristics like leaf area index, leaf disease,and plant height. However, few methods have been applied to non-destructivemeasurements of leaf size. In this paper, an automated non-destructiveimaged-based measuring system is presented, which uses 2D and 3D data obtainedusing a Zivid 3D camera, creating 3D virtual representations (digital twins) ofthe tomato plants. Leaves are detected from corresponding 2D RGB images andmapped to their 3D point cloud using the detected leaf masks, which then passthe leaf point cloud to the plane fitting algorithm to extract the leaf size toprovide data for growth monitoring. The performance of the measurement platformhas been measured through a comprehensive trial on real-world tomato plantswith quantified performance metrics compared to ground truth measurements.Three tomato leaf and height datasets (including 50+ 3D point cloud files oftomato plants) were collected and open-sourced in this project. The proposedleaf size estimation method demonstrates an RMSE value of 4.47mm and an R^2value of 0.87. The overall measurement system (leaf detection and sizeestimation algorithms combine) delivers an RMSE value of 8.13mm and an R^2value of 0.899.", "output": "Look how they have grown: Non-destructive Leaf Detection and Size Estimation of Tomato Plants for 3D Growth Monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Active Object Tracking (AOT) aims to maintain a specific relation between thetracker and object(s) by autonomously controlling the motion system of atracker given observations. AOT has wide-ranging applications, such as inmobile robots and autonomous driving. However, building a generalizable activetracker that works robustly across different scenarios remains a challenge,especially in unstructured environments with cluttered obstacles and diverselayouts. We argue that constructing a state representation capable of modelingthe geometry structure of the surroundings and the dynamics of the target iscrucial for achieving this goal. To address this challenge, we present RSPT, aframework that forms a structure-aware motion representation by Reconstructingthe Surroundings and Predicting the target Trajectory. Additionally, we enhancethe generalization of the policy network by training in an asymmetric duelingmechanism. We evaluate RSPT on various simulated scenarios and show that itoutperforms existing methods in unseen environments, particularly those withcomplex obstacles and layouts. We also demonstrate the successful transfer ofRSPT to real-world settings. Project Website:", "output": "RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quality-Diversity (QD) algorithms are designed to generate collections ofhigh-performing solutions while maximizing their diversity in a givendescriptor space. However, in the presence of unpredictable noise, the fitnessand descriptor of the same solution can differ significantly from oneevaluation to another, leading to uncertainty in the estimation of such values.Given the elitist nature of QD algorithms, they commonly end up with manydegenerate solutions in such noisy settings. In this work, we introduce ArchiveReproducibility Improvement Algorithm (ARIA); a plug-and-play approach thatimproves the reproducibility of the solutions present in an archive. We proposeit as a separate optimization module, relying on natural evolution strategies,that can be executed on top of any QD algorithm. Our module mutates solutionsto (1) optimize their probability of belonging to their niche, and (2) maximizetheir fitness. The performance of our method is evaluated on various tasks,including a classical optimization problem and two high-dimensional controltasks in simulated robotic environments. We show that our algorithm enhancesthe quality and descriptor space coverage of any given archive by at least 50%.", "output": "Don't Bet on Luck Alone: Enhancing Behavioral Reproducibility of Quality-Diversity Solutions in Uncertain Domains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the recent years, machine learning has made great advancements that havebeen at the root of many breakthroughs in different application domains.However, it is still an open issue how make them applicable to high-stakes orsafety-critical application domains, as they can often be brittle andunreliable. In this paper, we argue that requirements definition andsatisfaction can go a long way to make machine learning models even morefitting to the real world, especially in critical domains. To this end, wepresent two problems in which (i) requirements arise naturally, (ii) machinelearning models are or can be fruitfully deployed, and (iii) neglecting therequirements can have dramatic consequences. We show how the requirementsspecification can be fruitfully integrated into the standard machine learningdevelopment pipeline, proposing a novel pyramid development process in whichrequirements definition may impact all the subsequent phases in the pipeline,and viceversa.", "output": "Machine Learning with Requirements: a Manifesto."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fluid mechanics is a fundamental field in engineering and science. Solvingthe Navier-Stokes equation (NSE) is critical for understanding the behavior offluids. However, the NSE is a complex partial differential equation that isdifficult to solve, and classical numerical methods can be computationallyexpensive. In this paper, we present an innovative approach for solving the NSEusing Physics Informed Neural Networks (PINN) and several novel techniques thatimprove their performance. The first model is based on an assumption thatinvolves approximating the velocity component by employing the derivative of astream function. This assumption serves to simplify the system and guaranteesthat the velocity adheres to the divergence-free equation. We also developed asecond more flexible model that approximates the solution without anyassumptions. The proposed models can effectively solve two-dimensional NSE.Moreover, we successfully applied the second model to solve thethree-dimensional NSE. The results show that the models can efficiently andaccurately solve the NSE in three dimensions. These approaches offer severaladvantages, including high trainability, flexibility, and efficiency.", "output": "EPINN-NSE: Enhanced Physics-Informed Neural Networks for Solving Navier-Stokes Equations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The popularity of encryption mechanisms poses a great challenge to malicioustraffic detection. The reason is traditional detection techniques cannot workwithout the decryption of encrypted traffic. Currently, research on encryptedmalicious traffic detection without decryption has focused on featureextraction and the choice of machine learning or deep learning algorithms. Inthis paper, we first provide an in-depth analysis of traffic features andcompare different state-of-the-art traffic feature creation approaches, whileproposing a novel concept for encrypted traffic feature which is specificallydesigned for encrypted malicious traffic analysis. In addition, we propose aframework for encrypted malicious traffic detection. The framework is atwo-layer detection framework which consists of both deep learning andtraditional machine learning algorithms. Through comparative experiments, itoutperforms classical deep learning and traditional machine learningalgorithms, such as ResNet and Random Forest. Moreover, to provide sufficienttraining data for the deep learning model, we also curate a dataset composedentirely of public datasets. The composed dataset is more comprehensive thanusing any public dataset alone. Lastly, we discuss the future directions ofthis research.", "output": "Feature Mining for Encrypted Malicious Traffic Detection with Deep Learning and Other Machine Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Single occupancy vehicles are the most attractive transportation alternativefor many commuters, leading to increased traffic congestion and air pollution.Advancements in information technologies create opportunities for smartsolutions that incentivize ridesharing and mode shift to higher occupancyvehicles (HOVs) to achieve the car lighter vision of cities. In this study, wepresent HumanLight, a novel decentralized adaptive traffic signal controlalgorithm designed to optimize people throughput at intersections. Our proposedcontroller is founded on reinforcement learning with the reward functionembedding the transportation-inspired concept of pressure at the person-level.By rewarding HOV commuters with travel time savings for their efforts to mergeinto a single ride, HumanLight achieves equitable allocation of green times.Apart from adopting FRAP, a state-of-the-art (SOTA) base model, HumanLightintroduces the concept of active vehicles, loosely defined as vehicles inproximity to the intersection within the action interval window. The proposedalgorithm showcases significant headroom and scalability in different networkconfigurations considering multimodal vehicle splits at various scenarios ofHOV adoption. Improvements in person delays and queues range from 15% to over55% compared to vehicle-level SOTA controllers. We quantify the impact ofincorporating active vehicles in the formulation of our RL model for differentnetwork structures. HumanLight also enables regulation of the aggressiveness ofthe HOV prioritization. The impact of parameter setting on the generated phaseprofile is investigated as a key component of acyclic signal controllersaffecting pedestrian waiting times. HumanLight's scalable, decentralized designcan reshape the resolution of traffic management to be more human-centric andempower policies that incentivize ridesharing and public transit systems.", "output": "HumanLight: Incentivizing Ridesharing via Human-centric Deep Reinforcement Learning in Traffic Signal Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We explore the metric and preference learning problem in Hilbert spaces. Weobtain a novel representer theorem for the simultaneous task of metric andpreference learning. Our key observation is that the representer theorem can beformulated with respect to the norm induced by the inner product inherent inthe problem structure. Additionally, we demonstrate how our framework can beapplied to the task of metric learning from triplet comparisons and show thatit leads to a simple and self-contained representer theorem for this task. Inthe case of Reproducing Kernel Hilbert Spaces (RKHS), we demonstrate that thesolution to the learning problem can be expressed using kernel terms, akin toclassical representer theorems.", "output": "Representer Theorems for Metric and Preference Learning: A Geometric Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As quantum chemical properties have a significant dependence on theirgeometries, graph neural networks (GNNs) using 3D geometric information haveachieved high prediction accuracy in many tasks. However, they often require 3Dgeometries obtained from high-level quantum mechanical calculations, which arepractically infeasible, limiting their applicability in real-world problems. Totackle this, we propose a method to accurately predict the properties withrelatively easy-to-obtain geometries (e.g., optimized geometries from themolecular force field). In this method, the input geometry, regarded as thecorrupted geometry of the correct one, gradually approaches the correct one asit passes through the stacked denoising layers. We investigated the performanceof the proposed method using 3D message-passing architectures for twoprediction tasks: molecular properties and chemical reaction property. Thereduction of positional errors through the denoising process contributed toperformance improvement by increasing the mutual information between thecorrect and corrupted geometries. Moreover, our analysis of the correlationbetween denoising power and predictive accuracy demonstrates the effectivenessof the denoising process.", "output": "Predicting quantum chemical property with easy-to-obtain geometry via positional denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, the U.S. Department of Energy (DOE), Office of Science, Biologicaland Environmental Research (BER), and Advanced Scientific Computing Research(ASCR) programs organized and held the Artificial Intelligence for Earth SystemPredictability (AI4ESP) workshop series. From this workshop, a criticalconclusion that the DOE BER and ASCR community came to is the requirement todevelop a new paradigm for Earth system predictability focused on enablingartificial intelligence (AI) across the field, lab, modeling, and analysisactivities, called ModEx. The BER's `Model-Experimentation', ModEx, is aniterative approach that enables process models to generate hypotheses. Thedeveloped hypotheses inform field and laboratory efforts to collect measurementand observation data, which are subsequently used to parameterize, drive, andtest model (e.g., process-based) predictions. A total of 17 technical sessionswere held in this AI4ESP workshop series. This paper discusses the topic of the`AI Architectures and Co-design' session and associated outcomes. The AIArchitectures and Co-design session included two invited talks, two plenarydiscussion panels, and three breakout rooms that covered specific topics,including: (1) DOE HPC Systems, (2) Cloud HPC Systems, and (3) Edge computingand Internet of Things (IoT). We also provide forward-looking ideas andperspectives on potential research in this co-design area that can be achievedby synergies with the other 16 session topics. These ideas include topics suchas: (1) reimagining co-design, (2) data acquisition to distribution, (3)heterogeneous HPC solutions for integration of AI/ML and other data analyticslike uncertainty quantification with earth system modeling and simulation, and(4) AI-enabled sensor integration into earth system measurements andobservations. Such perspectives are a distinguishing aspect of this paper.", "output": "Perspectives on AI Architectures and Co-design for Earth System Predictability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixed Integer Programming (MIP) is NP-hard, and yet modern solvers oftensolve large real-world problems within minutes. This success can partially beattributed to heuristics. Since their behavior is highly instance-dependent,relying on hard-coded rules derived from empirical testing on a largeheterogeneous corpora of benchmark instances might lead to sub-optimalperformance. In this work, we propose an online learning approach that adaptsthe application of heuristics towards the single instance at hand. We replacethe commonly used static heuristic handling with an adaptive frameworkexploiting past observations about the heuristic's behavior to make futuredecisions. In particular, we model the problem of controlling LargeNeighborhood Search and Diving - two broad and complex classes of heuristics -as a multi-armed bandit problem. Going beyond existing work in the literature,we control two different classes of heuristics simultaneously by a singlelearning agent. We verify our approach numerically and show consistent nodereductions over the MIPLIB 2017 Benchmark set. For harder instances that takeat least 1000 seconds to solve, we observe a speedup of 4%.", "output": "Online Learning for Scheduling MIP Heuristics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep multimodal learning has achieved great progress in recent years.However, current fusion approaches are static in nature, i.e., they process andfuse multimodal inputs with identical computation, without accounting fordiverse computational demands of different multimodal data. In this work, wepropose dynamic multimodal fusion (DynMM), a new approach that adaptively fusesmultimodal data and generates data-dependent forward paths during inference. Tothis end, we propose a gating function to provide modality-level orfusion-level decisions on-the-fly based on multimodal features and aresource-aware loss function that encourages computational efficiency. Resultson various multimodal tasks demonstrate the efficiency and wide applicabilityof our approach. For instance, DynMM can reduce the computation costs by 46.5%with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improvesegmentation performance with over 21% savings in computation (NYU Depth V2semantic segmentation) when compared with static fusion approaches. We believeour approach opens a new direction towards dynamic multimodal network design,with applications to a wide range of multimodal tasks.", "output": "Dynamic Multimodal Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Two-branch network architecture has shown its efficiency and effectiveness inreal-time semantic segmentation tasks. However, direct fusion ofhigh-resolution details and low-frequency context has the drawback of detailedfeatures being easily overwhelmed by surrounding contextual information. Thisovershoot phenomenon limits the improvement of the segmentation accuracy ofexisting two-branch models. In this paper, we make a connection betweenConvolutional Neural Networks (CNN) and Proportional-Integral-Derivative (PID)controllers and reveal that a two-branch network is equivalent to aProportional-Integral (PI) controller, which inherently suffers from similarovershoot issues. To alleviate this problem, we propose a novel three-branchnetwork architecture: PIDNet, which contains three branches to parse detailed,context and boundary information, respectively, and employs boundary attentionto guide the fusion of detailed and context branches. Our family of PIDNetsachieve the best trade-off between inference speed and accuracy and theiraccuracy surpasses all the existing models with similar inference speed on theCityscapes and CamVid datasets. Specifically, PIDNet-S achieves 78.6% mIOU withinference speed of 93.2 FPS on Cityscapes and 80.1% mIOU with speed of 153.7FPS on CamVid.", "output": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sentence completion (SC) questions present a sentence with one or more blanksthat need to be filled in, three to five possible words or phrases as options.SC questions are widely used for students learning English as a Second Language(ESL). In this paper, we present a large-scale SC dataset, textsc{SC-Ques},which is made up of 289,148 ESL SC questions from real-world standardizedEnglish examinations. Furthermore, we build a comprehensive benchmark ofautomatically solving the SC questions by training the large-scale pre-trainedlanguage models on the proposed textsc{SC-Ques} dataset. We conduct detailedanalysis of the baseline models performance, limitations and trade-offs. Thedata and our code are available for research purposes from:url{", "output": "SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural radiance fields (NeRFs) have recently emerged as a promising approachfor 3D reconstruction and novel view synthesis. However, NeRF-based methodsencode shape, reflectance, and illumination implicitly and this makes itchallenging for users to manipulate these properties in the rendered imagesexplicitly. Existing approaches only enable limited editing of the scene anddeformation of the geometry. Furthermore, no existing work enables accuratescene illumination after object deformation. In this work, we introduce SPIDR,a new hybrid neural SDF representation. SPIDR combines point cloud and neuralimplicit representations to enable the reconstruction of higher quality objectsurfaces for geometry deformation and lighting estimation. meshes and surfacesfor object deformation and lighting estimation. To more accurately captureenvironment illumination for scene relighting, we propose a novel neuralimplicit model to learn environment light. To enable more accurate illuminationupdates after deformation, we use the shadow mapping technique to approximatethe light visibility updates caused by geometry editing. We demonstrate theeffectiveness of SPIDR in enabling high quality geometry editing with moreaccurate updates to the illumination of the scene.", "output": "SPIDR: SDF-based Neural Point Fields for Illumination and Deformation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interval Markov Decision Processes (IMDPs) are finite-state uncertain Markovmodels, where the transition probabilities belong to intervals. Recently, therehas been a surge of research on employing IMDPs as abstractions of stochasticsystems for control synthesis. However, due to the absence of algorithms forsynthesis over IMDPs with continuous action-spaces, the action-space is assumeddiscrete a-priori, which is a restrictive assumption for many applications.Motivated by this, we introduce continuous-action IMDPs (caIMDPs), where thebounds on transition probabilities are functions of the action variables, andstudy value iteration for maximizing expected cumulative rewards. Specifically,we decompose the max-min problem associated to value iteration to$|mathcal{Q}|$ max problems, where $|mathcal{Q}|$ is the number of states ofthe caIMDP. Then, exploiting the simple form of these max problems, we identifycases where value iteration over caIMDPs can be solved efficiently (e.g., withlinear or convex programming). We also gain other interesting insights: e.g.,in certain cases where the action set $mathcal{A}$ is a polytope, synthesisover a discrete-action IMDP, where the actions are the vertices of$mathcal{A}$, is sufficient for optimality. We demonstrate our results on anumerical example. Finally, we include a short discussion on employing caIMDPsas abstractions for control synthesis.", "output": "Interval Markov Decision Processes with Continuous Action-Spaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the current context where online platforms have been effectivelyweaponized in a variety of geo-political events and social issues, Internetmemes make fair content moderation at scale even more difficult. Existing workon meme classification and tracking has focused on black-box methods that donot explicitly consider the semantics of the memes or the context of theircreation. In this paper, we pursue a modular and explainable architecture forInternet meme understanding. We design and implement multimodal classificationmethods that perform example- and prototype-based reasoning over trainingcases, while leveraging both textual and visual SOTA models to represent theindividual cases. We study the relevance of our modular and explainable modelsin detecting harmful memes on two existing tasks: Hate Speech Detection andMisogyny Classification. We compare the performance between example- andprototype-based methods, and between text, vision, and multimodal models,across different categories of harmfulness (e.g., stereotype andobjectification). We devise a user-friendly interface that facilitates thecomparative analysis of examples retrieved by all of our models for any givenmeme, informing the community about the strengths and limitations of theseexplainable methods.", "output": "Multimodal and Explainable Internet Meme Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D shapes have complementary abstractions from low-level geometry topart-based hierarchies to languages, which convey different levels ofinformation. This paper presents a unified framework to translate between pairsof shape abstractions: $textit{Text}$ $Longleftrightarrow$ $textit{PointCloud}$ $Longleftrightarrow$ $textit{Program}$. We propose $textbf{NeuralShape Compiler}$ to model the abstraction transformation as a conditionalgeneration process. It converts 3D shapes of three abstract types into unifieddiscrete shape code, transforms each shape code into code of other abstracttypes through the proposed $textit{ShapeCode Transformer}$, and decodes themto output the target shape abstraction. Point Cloud code is obtained in aclass-agnostic way by the proposed $textit{Point}$VQVAE. On Text2Shape,ShapeGlot, ABO, Genre, and Program Synthetic datasets, Neural Shape Compilershows strengths in $textit{Text}$ $Longrightarrow$ $textit{Point Cloud}$,$textit{Point Cloud}$ $Longrightarrow$ $textit{Text}$, $textit{PointCloud}$ $Longrightarrow$ $textit{Program}$, and Point Cloud Completion tasks.Additionally, Neural Shape Compiler benefits from jointly training on allheterogeneous data and tasks.", "output": "Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a modality-agnostic neural compression algorithm based on afunctional view of data and parameterised as an Implicit Neural Representation(INR). Bridging the gap between latent coding and sparsity, we obtain compactlatent representations non-linearly mapped to a soft gating mechanism. Thisallows the specialisation of a shared INR network to each data item throughsubnetwork selection. After obtaining a dataset of such latent representations,we directly optimise the rate/distortion trade-off in a modality-agnostic spaceusing neural compression. Variational Compression of Implicit NeuralRepresentations (VC-INR) shows improved performance given the samerepresentational capacity pre quantisation while also outperforming previousquantisation schemes used for other INR techniques. Our experiments demonstratestrong results over a large set of diverse modalities using the same algorithmwithout any modality-specific inductive biases. We show results on images,climate data, 3D shapes and scenes as well as audio and video, introducingVC-INR as the first INR-based method to outperform codecs as well-known anddiverse as JPEG 2000, MP3 and AVC/HEVC on their respective modalities.", "output": "Modality-Agnostic Variational Compression of Implicit Neural Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous robots are required to reason about the behaviour of dynamicagents in their environment. The creation of models to describe theserelationships is typically accomplished through the application of causaldiscovery techniques. However, as it stands observational causal discoverytechniques struggle to adequately cope with conditions such as causal sparsityand non-stationarity typically seen during online usage in autonomous agentdomains. Meanwhile, interventional techniques are not always feasible due todomain restrictions. In order to better explore the issues facing observationaltechniques and promote further discussion of these topics we carry out abenchmark across 10 contemporary observational temporal causal discoverymethods in the domain of autonomous driving. By evaluating these methods uponcausal scenes drawn from real world datasets in addition to those generatedsynthetically we highlight where improvements need to be made in order tofacilitate the application of causal discovery techniques to the aforementioneduse-cases. Finally, we discuss potential directions for future work that couldhelp better tackle the difficulties currently experienced by state of the arttechniques.", "output": "Evaluating Temporal Observation-Based Causal Discovery Techniques Applied to Road Driver Behaviour."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper reviews the state-of-the-art of language models architectures andstrategies for \"complex\" question-answering (QA, CQA, CPS) with a focus onhybridization. Large Language Models (LLM) are good at leveraging public dataon standard problems but once you want to tackle more specific complexquestions or problems (e.g. How does the concept of personal freedom varybetween different cultures ? What is the best mix of power generation methodsto reduce climate change ?) you may need specific architecture, knowledge,skills, methods, sensitive data protection, explainability, human approval andversatile feedback... Recent projects like ChatGPT and GALACTICA have allowednon-specialists to grasp the great potential as well as the equally stronglimitations of LLM in complex QA. In this paper, we start by reviewing requiredskills and evaluation techniques. We integrate findings from the robustcommunity edited research papers BIG, BLOOM and HELM which open source,benchmark and analyze limits and challenges of LLM in terms of tasks complexityand strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) asa baseline. We discuss some challenges associated with complex QA, includingdomain adaptation, decomposition and efficient multi-step QA, long form andnon-factoid QA, safety and multi-sensitivity data protection, multimodalsearch, hallucinations, explainability and truthfulness, temporal reasoning. Weanalyze current solutions and promising research trends, using elements suchas: hybrid LLM architectural patterns, training and prompting strategies,active human reinforcement learning supervised with AI, neuro-symbolic andstructured knowledge grounding, program synthesis, iterated decomposition andothers.", "output": "Complex QA and language models hybrid architectures, Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Friend recall is an important way to improve Daily Active Users (DAU) inonline games. The problem is to generate a proper lost friend ranking listessentially. Traditional friend recall methods focus on rules like friendintimacy or training a classifier for predicting lost players' returnprobability, but ignore feature information of (active) players and historicalfriend recall events. In this work, we treat friend recall as a link predictionproblem and explore several link prediction methods which can use features ofboth active and lost players, as well as historical events. Furthermore, wepropose a novel Edge Transformer model and pre-train the model via maskedauto-encoders. Our method achieves state-of-the-art results in the offlineexperiments and online A/B Tests of three Tencent games.", "output": "Friend Ranking in Online Games via Pre-training Edge Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We revisit in this paper the discrete-time linear quadratic regulator (LQR)problem from the perspective of receding-horizon policy gradient (RHPG), anewly developed model-free learning framework for control applications. Weprovide a fine-grained sample complexity analysis for RHPG to learn a controlpolicy that is both stabilizing and $epsilon$-close to the optimal LQRsolution, and our algorithm does not require knowing a stabilizing controlpolicy for initialization. Combined with the recent application of RHPG inlearning the Kalman filter, we demonstrate the general applicability of RHPG inlinear control and estimation with streamlined analyses.", "output": "Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-agent Reinforcement Learning (MARL) based traffic signal controlbecomes a popular research topic in recent years. Most existing MARL approachestend to learn the optimum control strategies in a decentralised manner byconsidering communication among neighbouring intersections. However, thenon-stationary property in MARL may lead to extremely slow or even failure ofconvergence, especially when the number of intersections becomes large. One ofthe existing methods is to partition the whole network into several regions,each of which utilizes a centralized RL framework to speed up the convergencerate. However, there are two challenges for this strategy: the first one is howto get a flexible partition and the second one is how to search for the optimaljoint actions for a region of intersections. In this paper, we propose a noveltraining framework where our region partitioning rule is based on the adjacencybetween the intersections and propose Dynamic Branching Dueling Q-Network(DBDQ) to search for optimal joint action efficiently and to maximize theregional reward. The experimental results with both real datasets and syntheticdatasets demonstrate the superiority of our framework over other existingframeworks.", "output": "Large-Scale Regional Traffic Signal Control Using Dynamic Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stable Diffusion model has been extensively employed in the study ofarchi-tectural image generation, but there is still an opportunity to enhancein terms of the controllability of the generated image content. A multi-networkcombined text-to-building facade image generating method is proposed in thiswork. We first fine-tuned the Stable Diffusion model on the CMP Fa-cadesdataset using the LoRA (Low-Rank Adaptation) approach, then we ap-ply theControlNet model to further control the output. Finally, we contrast-ed thefacade generating outcomes under various architectural style text con-tents andcontrol strategies. The results demonstrate that the LoRA training approachsignificantly decreases the possibility of fine-tuning the Stable Dif-fusionlarge model, and the addition of the ControlNet model increases thecontrollability of the creation of text to building facade images. Thispro-vides a foundation for subsequent studies on the generation ofarchitectural images.", "output": "Text Semantics to Image Generation: A method of building facades design base on Stable Diffusion model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the last few years, many works have tried to explain the predictions ofdeep learning models. Few methods, however, have been proposed to verify theaccuracy or faithfulness of these explanations. Recently, influence functions,which is a method that approximates the effect that leave-one-out training hason the loss function, has been shown to be fragile. The proposed reason fortheir fragility remains unclear. Although previous work suggests the use ofregularization to increase robustness, this does not hold in all cases. In thiswork, we seek to investigate the experiments performed in the prior work in aneffort to understand the underlying mechanisms of influence function fragility.First, we verify influence functions using procedures from the literature underconditions where the convexity assumptions of influence functions are met.Then, we relax these assumptions and study the effects of non-convexity byusing deeper models and more complex datasets. Here, we analyze the key metricsand procedures that are used to validate influence functions. Our resultsindicate that the validation procedures may cause the observed fragility.", "output": "Revisiting the Fragility of Influence Functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are widely used prediction algorithms whose performanceoften improves as the number of weights increases, leading toover-parametrization. We consider a two-layered neural network whose firstlayer is frozen while the last layer is trainable, known as the random featuremodel. We study over-parametrization in the context of a student-teacherframework by deriving a set of differential equations for the learningdynamics. For any finite ratio of hidden layer size and input dimension, thestudent cannot generalize perfectly, and we compute the non-zero asymptoticgeneralization error. Only when the student's hidden layer size isexponentially larger than the input dimension, an approach to perfectgeneralization is possible.", "output": "Online Learning for the Random Feature Model in the Student-Teacher Framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D object detection from visual sensors is a cornerstone capability ofrobotic systems. State-of-the-art methods focus on reasoning and decodingobject bounding boxes from multi-view camera input. In this work we gainintuition from the integral role of multi-view consistency in 3D sceneunderstanding and geometric learning. To this end, we introduce VEDet, a novel3D object detection framework that exploits 3D multi-view geometry to improvelocalization through viewpoint awareness and equivariance. VEDet leverages aquery-based transformer architecture and encodes the 3D scene by augmentingimage features with positional encodings from their 3D perspective geometry. Wedesign view-conditioned queries at the output level, which enables thegeneration of multiple virtual frames during training to learn viewpointequivariance by enforcing multi-view consistency. The multi-view geometryinjected at the input level as positional encodings and regularized at the losslevel provides rich geometric cues for 3D object detection, leading tostate-of-the-art performance on the nuScenes benchmark. The code and model aremade available at ", "output": "Viewpoint Equivariance for Multi-View 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The quality of texts generated by natural language generation (NLG) systemsis hard to measure automatically. Conventional reference-based metrics, such asBLEU and ROUGE, have been shown to have relatively low correlation with humanjudgments, especially for tasks that require creativity and diversity. Recentstudies suggest using large language models (LLMs) as reference-free metricsfor NLG evaluation, which have the benefit of being applicable to new tasksthat lack human references. However, these LLM-based evaluators still havelower human correspondence than medium-size neural evaluators. In this work, wepresent G-Eval, a framework of using large language models withchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality ofNLG outputs. We experiment with two generation tasks, text summarization anddialogue generation. We show that G-Eval with GPT-4 as the backbone modelachieves a Spearman correlation of 0.514 with human on summarization task,outperforming all previous methods by a large margin. We also proposepreliminary analysis on the behavior of LLM-based evaluators, and highlight thepotential issue of LLM-based evaluators having a bias towards the LLM-generatedtexts.", "output": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Equivariance w.r.t. geometric transformations in neural networks improvesdata efficiency, parameter efficiency and robustness to out-of-domainperspective shifts. When equivariance is not designed into a neural network,the network can still learn equivariant functions from the data. We quantifythis learned equivariance, by proposing an improved measure for equivariance.We find evidence for a correlation between learned translation equivariance andvalidation accuracy on ImageNet. We therefore investigate what can increase thelearned equivariance in neural networks, and find that data augmentation,reduced model capacity and inductive bias in the form of convolutions inducehigher learned equivariance in neural networks.", "output": "What Affects Learned Equivariance in Deep Image Recognition Models?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical data often exhibits long-tail distributions with heavy classimbalance, which naturally leads to difficulty in classifying the minorityclasses (i.e., boundary regions or rare objects). Recent work has significantlyimproved semi-supervised medical image segmentation in long-tailed scenarios byequipping them with unsupervised contrastive criteria. However, it remainsunclear how well they will perform in the labeled portion of data where classdistribution is also highly imbalanced. In this work, we present ACTION++, animproved contrastive learning framework with adaptive anatomical contrast forsemi-supervised medical segmentation. Specifically, we propose an adaptivesupervised contrastive loss, where we first compute the optimal locations ofclass centers uniformly distributed on the embedding space (i.e., off-line),and then perform online contrastive matching training by encouraging differentclass features to adaptively match these distinct and uniformly distributedclass centers. Moreover, we argue that blindly adopting a constant temperature$tau$ in the contrastive loss on long-tailed medical data is not optimal, andpropose to use a dynamic $tau$ via a simple cosine schedule to yield betterseparation between majority and minority classes. Empirically, we evaluateACTION++ on ACDC and LA benchmarks and show that it achieves state-of-the-artacross two semi-supervised settings. Theoretically, we analyze the performanceof adaptive anatomical contrast and confirm its superiority in labelefficiency.", "output": "ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we study how the generalization performance of a givendirection changes with its sampling ratio in Multilingual Neural MachineTranslation (MNMT). By training over 200 multilingual models with various modelsizes, directions, and total numbers of tasks, we find that scalarization leadsto a multitask trade-off front that deviates from the traditional Pareto frontwhen there exists data imbalance in the training corpus. That is, theperformance of certain translation directions does not improve with theincrease of its weight in the multi-task optimization objective, which poses agreat challenge to improve the overall performance of all directions. Based onour observations, we propose the Double Power Law to predict the uniqueperformance trade-off front in MNMT, which is robust across various languages,data adequacy, and the number of tasks. Finally, we formulate the sample ratioselection problem in MNMT as an optimization problem based on the Double PowerLaw, which achieves better performance than temperature searching and gradientmanipulation methods using up to half of the total training budget in ourexperiments.", "output": "On the Pareto Front of Multilingual Neural Machine Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, by constructing extremely hard examples of CSP (with largedomains) and SAT (with long clauses), we prove that such examples cannot besolved without exhaustive search, which implies a weaker conclusion P $neq$NP. This constructive approach for proving impossibility results is verydifferent (and missing) from those currently used in computational complexitytheory, but is similar to that used by Kurt G\"{o}del in proving his famouslogical impossibility results. Just as shown by G\"{o}del's results thatproving formal unprovability is feasible in mathematics, the results of thispaper show that proving computational hardness is not hard in mathematics.Specifically, proving lower bounds for many problems, such as 3-SAT, can bechallenging because these problems have various effective strategies availablefor avoiding exhaustive search. However, in cases of extremely hard examples,exhaustive search may be the only viable option, and proving its necessitybecomes more straightforward. Consequently, it makes the separation between SAT(with long clauses) and 3-SAT much easier than that between 3-SAT and 2-SAT.Finally, the main results of this paper demonstrate that the fundamentaldifference between the syntax and the semantics revealed by G\"{o}del's resultsalso exists in CSP and SAT.", "output": "SAT Requires Exhaustive Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, deep learning models have demonstrated remarkable success invarious domains, such as computer vision, natural language processing, andspeech recognition. However, the generalization capabilities of these modelscan be negatively impacted by the limitations of their feature fusiontechniques. This paper introduces an innovative approach, Adaptive FeatureFusion (AFF), to enhance the generalization of deep learning models bydynamically adapting the fusion process of feature representations.The proposed AFF framework is designed to incorporate fusion layers intoexisting deep learning architectures, enabling seamless integration andimproved performance. By leveraging a combination of data-driven andmodel-based fusion strategies, AFF is able to adaptively fuse features based onthe underlying data characteristics and model requirements. This paper presentsa detailed description of the AFF framework, including the design andimplementation of fusion layers for various architectures.Extensive experiments are conducted on multiple benchmark datasets, with theresults demonstrating the superiority of the AFF approach in comparison totraditional feature fusion techniques. The analysis showcases the effectivenessof AFF in enhancing generalization capabilities, leading to improvedperformance across different tasks and applications.Finally, the paper discusses various real-world use cases where AFF can beemployed, providing insights into its practical applicability. The conclusionhighlights the potential for future research directions, including theexploration of advanced fusion strategies and the extension of AFF to othermachine learning paradigms.", "output": "Adaptive Feature Fusion: Enhancing Generalization in Deep Learning Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data augmentation is a promising technique for unsupervised anomaly detectionin industrial applications, where the availability of positive samples is oftenlimited due to factors such as commercial competition and sample collectiondifficulties. In this paper, how to effectively select and apply dataaugmentation methods for unsupervised anomaly detection is studied. The impactof various data augmentation methods on different anomaly detection algorithmsis systematically investigated through experiments. The experimental resultsshow that the performance of different industrial image anomaly detection(termed as IAD) algorithms is not significantly affected by the specific dataaugmentation method employed and that combining multiple data augmentationmethods does not necessarily yield further improvements in the accuracy ofanomaly detection, although it can achieve excellent results on specificmethods. These findings provide useful guidance on selecting appropriate dataaugmentation methods for different requirements in IAD.", "output": "What makes a good data augmentation for few-shot unsupervised image anomaly detection?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural operator learning as a means of mapping between complex functionspaces has garnered significant attention in the field of computational scienceand engineering (CS&amp;E). In this paper, we apply Neural operator learning to thetime-of-flight ultrasound computed tomography (USCT) problem. We learn themapping between time-of-flight (TOF) data and the heterogeneous sound speedfield using a full-wave solver to generate the training data. This novelapplication of operator learning circumnavigates the need to solve thecomputationally intensive iterative inverse problem. The operator learns thenon-linear mapping offline and predicts the heterogeneous sound field with asingle forward pass through the model. This is the first time operator learninghas been used for ultrasound tomography and is the first step in potentialreal-time predictions of soft tissue distribution for tumor identification inbeast imaging.", "output": "Neural Operator Learning for Ultrasound Tomography Inversion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adopting contrastive image-text pretrained models like CLIP towards videoclassification has gained attention due to its cost-effectiveness andcompetitive performance. However, recent works in this area face a trade-off.Finetuning the pretrained model to achieve strong supervised performanceresults in low zero-shot generalization. Similarly, freezing the backbone toretain zero-shot capability causes significant drop in supervised accuracy.Because of this, recent works in literature typically train separate models forsupervised and zero-shot action recognition. In this work, we propose amultimodal prompt learning scheme that works to balance the supervised andzero-shot performance under a single unified training. Our prompting approachon the vision side caters for three aspects: 1) Global video-level prompts tomodel the data distribution; 2) Local frame-level prompts to provide per-framediscriminative conditioning; and 3) a summary prompt to extract a condensedvideo representation. Additionally, we define a prompting scheme on the textside to augment the textual context. Through this prompting scheme, we canachieve state-of-the-art zero-shot performance on Kinetics-600, HMDB51 andUCF101 while remaining competitive in the supervised setting. By keeping thepretrained backbone frozen, we optimize a much lower number of parameters andretain the existing general representation which helps achieve the strongzero-shot performance. Our codes/models are released at", "output": "Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image inpainting refers to the task of generating a complete, natural imagebased on a partially revealed reference image. Recently, many researchinterests have been focused on addressing this problem using fixed diffusionmodels. These approaches typically directly replace the revealed region of theintermediate or final generated images with that of the reference image or itsvariants. However, since the unrevealed regions are not directly modified tomatch the context, it results in incoherence between revealed and unrevealedregions. To address the incoherence problem, a small number of methodsintroduce a rigorous Bayesian framework, but they tend to introduce mismatchesbetween the generated and the reference images due to the approximation errorsin computing the posterior distributions. In this paper, we propose COPAINT,which can coherently inpaint the whole image without introducing mismatches.COPAINT also uses the Bayesian framework to jointly modify both revealed andunrevealed regions, but approximates the posterior distribution in a way thatallows the errors to gradually drop to zero throughout the denoising steps,thus strongly penalizing any mismatches with the reference image. Ourexperiments verify that COPAINT can outperform the existing diffusion-basedmethods under both objective and subjective metrics. The codes are available at", "output": "Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tools to generate high quality synthetic speech signal that is perceptuallyindistinguishable from speech recorded from human speakers are easilyavailable. Several approaches have been proposed for detecting syntheticspeech. Many of these approaches use deep learning methods as a black boxwithout providing reasoning for the decisions they make. This limits theinterpretability of these approaches. In this paper, we propose DisentangledSpectrogram Variational Auto Encoder (DSVAE) which is a two staged trainedvariational autoencoder that processes spectrograms of speech usingdisentangled representation learning to generate interpretable representationsof a speech signal for detecting synthetic speech. DSVAE also creates anactivation map to highlight the spectrogram regions that discriminate syntheticand bona fide human speech signals. We evaluated the representations obtainedfrom DSVAE using the ASVspoof2019 dataset. Our experimental results show highaccuracy (&gt;98%) on detecting synthetic speech from 6 known and 10 out of 11unknown speech synthesizers. We also visualize the representation obtained fromDSVAE for 17 different speech synthesizers and verify that they are indeedinterpretable and discriminate bona fide and synthetic speech from each of thesynthesizers.", "output": "DSVAE: Interpretable Disentangled Representation for Synthetic Speech Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ubiquitous multi-camera setup on modern autonomous vehicles provides anopportunity to construct surround-view depth. Existing methods, however, eitherperform independent monocular depth estimations on each camera or rely oncomputationally heavy self attention mechanisms. In this paper, we propose anovel guided attention architecture, EGA-Depth, which can improve both theefficiency and accuracy of self-supervised multi-camera depth estimation. Morespecifically, for each camera, we use its perspective view as the query tocross-reference its neighboring views to derive informative features for thiscamera view. This allows the model to perform attention only across views withconsiderable overlaps and avoid the costly computations of standardself-attention. Given its efficiency, EGA-Depth enables us to exploithigher-resolution visual features, leading to improved accuracy. Furthermore,EGA-Depth can incorporate more frames from previous time steps as it scaleslinearly w.r.t. the number of views and frames. Extensive experiments on twochallenging autonomous driving benchmarks nuScenes and DDAD demonstrate theefficacy of our proposed EGA-Depth and show that it achieves the newstate-of-the-art in self-supervised multi-camera depth estimation.", "output": "EGA-Depth: Efficient Guided Attention for Self-Supervised Multi-Camera Depth Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the problem of automatically placing an object into abackground image for image compositing. Given a background image and asegmented object, the goal is to train a model to predict plausible placements(location and scale) of the object for compositing. The quality of thecomposite image highly depends on the predicted location/scale. Existing workseither generate candidate bounding boxes or apply sliding-window search usingglobal representations from background and object images, which fail to modellocal information in background images. However, local clues in backgroundimages are important to determine the compatibility of placing the objects withcertain locations/scales. In this paper, we propose to learn the correlationbetween object features and all local background features with a transformermodule so that detailed information can be provided on all possiblelocation/scale configurations. A sparse contrastive loss is further proposed totrain our model with sparse supervision. Our new formulation generates a 3Dheatmap indicating the plausibility of all location/scale combinations in onenetwork forward pass, which is over 10 times faster than the previoussliding-window method. It also supports interactive search when users provide apre-defined location or scale. The proposed method can be trained with explicitannotation or in a self-supervised manner using an off-the-shelf inpaintingmodel, and it outperforms state-of-the-art methods significantly. The userstudy shows that the trained model generalizes well to real-world images withdiverse challenging scenes and object categories.", "output": "TopNet: Transformer-based Object Placement Network for Image Compositing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent diffusion-based generators can produce high-quality images based onlyon textual prompts. However, they do not correctly interpret instructions thatspecify the spatial layout of the composition. We propose a simple approachthat can achieve robust layout control without requiring training orfine-tuning the image generator. Our technique, which we call layout guidance,manipulates the cross-attention layers that the model uses to interface textualand visual information and steers the reconstruction in the desired directiongiven, e.g., a user-specified layout. In order to determine how to best guideattention, we study the role of different attention maps when generating imagesand experiment with two alternative strategies, forward and backward guidance.We evaluate our method quantitatively and qualitatively with severalexperiments, validating its effectiveness. We further demonstrate itsversatility by extending layout guidance to the task of editing the layout andcontext of a given real image.", "output": "Training-Free Layout Control with Cross-Attention Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce S$^2$VS, a video similarity learning approach withself-supervision. Self-Supervised Learning (SSL) is typically used to traindeep models on a proxy task so as to have strong transferability on targettasks after fine-tuning. Here, in contrast to prior work, SSL is used toperform video similarity learning and address multiple retrieval and detectiontasks at once with no use of labeled data. This is achieved by learning viainstance-discrimination with task-tailored augmentations and the widely usedInfoNCE loss together with an additional loss operating jointly onself-similarity and hard-negative similarity. We benchmark our method on taskswhere video relevance is defined with varying granularity, ranging from videocopies to videos depicting the same incident or event. We learn a singleuniversal model that achieves state-of-the-art performance on all tasks,surpassing previously proposed methods that use labeled data. The code andpretrained models are publicly available at:url{", "output": "Self-Supervised Video Similarity Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Underwater imagery often exhibits distorted coloration as a result oflight-water interactions, which complicates the study of benthic environmentsin marine biology and geography. In this research, we propose an algorithm torestore the true color (albedo) in underwater imagery by jointly learning theeffects of the medium and neural scene representations. Our approach modelswater effects as a combination of light attenuation with distance andbackscattered light. The proposed neural scene representation is based on aneural reflectance field model, which learns albedos, normals, and volumedensities of the underwater environment. We introduce a logistic regressionmodel to separate water from the scene and apply distinct light physics duringtraining. Our method avoids the need to estimate complex backscatter effects inwater by employing several approximations, enhancing sampling efficiency andnumerical stability during training. The proposed technique integratesunderwater light effects into a volume rendering framework with end-to-enddifferentiability. Experimental results on both synthetic and real-world datademonstrate that our method effectively restores true color from underwaterimagery, outperforming existing approaches in terms of color consistency.", "output": "Beyond NeRF Underwater: Learning Neural Reflectance Fields for True Color Correction of Marine Imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-modal retrieval methods are the preferred tool to search databases forthe text that best matches a query image and vice versa. However, image-textretrieval models commonly learn to memorize spurious correlations in thetraining data, such as frequent object co-occurrence, instead of looking at theactual underlying reasons for the prediction in the image. For image-textretrieval, this manifests in retrieved sentences that mention objects that arenot present in the query image. In this work, we introduce ODmAP@k, an objectdecorrelation metric that measures a model's robustness to spuriouscorrelations in the training data. We use automatic image and textmanipulations to control the presence of such object correlations in designatedtest data. Additionally, our data synthesis technique is used to tackle modelbiases due to spurious correlations of semantically unrelated objects in thetraining data. We apply our proposed pipeline, which involves the finetuning ofimage-text retrieval frameworks on carefully designed synthetic data, to threestate-of-the-art models for image-text retrieval. This results in significantimprovements for all three models, both in terms of the standard retrievalperformance and in terms of our object decorrelation metric. The code isavailable at ", "output": "Exposing and Mitigating Spurious Correlations for Cross-Modal Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data hiding such as steganography and invisible watermarking has importantapplications in copyright protection, privacy-preserved communication andcontent provenance. Existing works often fall short in either preserving imagequality, or robustness against perturbations or are too complex to train. Wepropose RoSteALS, a practical steganography technique leveraging frozenpretrained autoencoders to free the payload embedding from learning thedistribution of cover images. RoSteALS has a light-weight secret encoder ofjust 300k parameters, is easy to train, has perfect secret recovery performanceand comparable image quality on three benchmarks. Additionally, RoSteALS can beadapted for novel cover-less steganography applications in which the coverimage can be sampled from noise or conditioned on text prompts via a denoisingdiffusion process. Our model and code are available aturl{", "output": "RoSteALS: Robust Steganography using Autoencoder Latent Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in self-supervised learning have demonstrated thateffective visual representations can be learned from unlabeled images. This hasled to increased interest in applying self-supervised learning to the medicaldomain, where unlabeled images are abundant and labeled images are difficult toobtain. However, most self-supervised learning approaches are modeled as imagelevel discriminative or generative proxy tasks, which may not capture the finerlevel representations necessary for dense prediction tasks like multi-organsegmentation. In this paper, we propose a novel contrastive learning frameworkthat integrates Localized Region Contrast (LRC) to enhance existingself-supervised pre-training methods for medical image segmentation. Ourapproach involves identifying Super-pixels by Felzenszwalb's algorithm andperforming local contrastive learning using a novel contrastive sampling loss.Through extensive experiments on three multi-organ segmentation datasets, wedemonstrate that integrating LRC to an existing self-supervised method in alimited annotation setting significantly improves segmentation performance.Moreover, we show that LRC can also be applied to fully-supervised pre-trainingmethods to further boost performance.", "output": "Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual Place Recognition (VPR) estimates the location of query images bymatching them with images in a reference database. Conventional methodsgenerally adopt aggregated CNN features for global retrieval and RANSAC-basedgeometric verification for reranking. However, RANSAC only employs geometricinformation but ignores other possible information that could be useful forreranking, e.g. local feature correlations, and attention values. In thispaper, we propose a unified place recognition framework that handles bothretrieval and reranking with a novel transformer model, named $R^{2}$Former.The proposed reranking module takes feature correlation, attention value, andxy coordinates into account, and learns to determine whether the image pair isfrom the same location. The whole pipeline is end-to-end trainable and thereranking module alone can also be adopted on other CNN or transformerbackbones as a generic component. Remarkably, $R^{2}$Former significantlyoutperforms state-of-the-art methods on major VPR datasets with much lessinference time and memory consumption. It also achieves the state-of-the-art onthe hold-out MSLS challenge set and could serve as a simple yet strong solutionfor real-world large-scale applications. Experiments also show visiontransformer tokens are comparable and sometimes better than CNN local featureson local matching. The code is released at", "output": "$R^{2}$Former: Unified $R$etrieval and $R$eranking Transformer for Place Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in personalized image generation allow a pre-trainedtext-to-image model to learn a new concept from a set of images. However,existing personalization approaches usually require heavy test-time finetuningfor each concept, which is time-consuming and difficult to scale. We proposeInstantBooth, a novel approach built upon pre-trained text-to-image models thatenables instant text-guided image personalization without any test-timefinetuning. We achieve this with several major components. First, we learn thegeneral concept of the input images by converting them to a textual token witha learnable image encoder. Second, to keep the fine details of the identity, welearn rich visual feature representation by introducing a few adapter layers tothe pre-trained model. We train our components only on text-image pairs withoutusing paired images of the same concept. Compared to test-time finetuning-basedmethods like DreamBooth and Textual-Inversion, our model can generatecompetitive results on unseen concepts concerning language-image alignment,image fidelity, and identity preservation while being 100 times faster.", "output": "InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present an end-to-end unsupervised anomaly detectionframework for 3D point clouds. To the best of our knowledge, this is the firstwork to tackle the anomaly detection task on a general object represented by a3D point cloud. We propose a deep variational autoencoder-based unsupervisedanomaly detection network adapted to the 3D point cloud and an anomaly scorespecifically for 3D point clouds. To verify the effectiveness of the model, weconducted extensive experiments on the ShapeNet dataset. Through quantitativeand qualitative evaluation, we demonstrate that the proposed method outperformsthe baseline method. Our code is available at", "output": "Toward Unsupervised 3D Point Cloud Anomaly Detection using Variational Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Small object detection requires the detection head to scan a large number ofpositions on image feature maps, which is extremely hard for computation- andenergy-efficient lightweight generic detectors. To accurately detect smallobjects with limited computation, we propose a two-stage lightweight detectionframework with extremely low computation complexity, termed as TinyDet. Itenables high-resolution feature maps for dense anchoring to better cover smallobjects, proposes a sparsely-connected convolution for computation reduction,enhances the early stage features in the backbone, and addresses the featuremisalignment problem for accurate small object detection. On the COCObenchmark, our TinyDet-M achieves 30.3 AP and 13.5 AP^s with only 991 MFLOPs,which is the first detector that has an AP over 30 with less than 1 GFLOPs;besides, TinyDet-S and TinyDet-L achieve promising performance under differentcomputation limitation.", "output": "TinyDet: Accurate Small Object Detection in Lightweight Generic Detectors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sequence generation models have recently made significant progress inunifying various vision tasks. Although some auto-regressive models havedemonstrated promising results in end-to-end text spotting, they use specificdetection formats while ignoring various text shapes and are limited in themaximum number of text instances that can be detected. To overcome theselimitations, we propose a UNIfied scene Text Spotter, called UNITS. Our modelunifies various detection formats, including quadrilaterals and polygons,allowing it to detect text in arbitrary shapes. Additionally, we applystarting-point prompting to enable the model to extract texts from an arbitrarystarting point, thereby extracting more texts beyond the number of instances itwas trained on. Experimental results demonstrate that our method achievescompetitive performance compared to state-of-the-art methods. Further analysisshows that UNITS can extract a larger number of texts than it was trained on.We provide the code for our method at ", "output": "Towards Unified Scene Text Spotting based on Sequence Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Linear probing (LP) (and $k$-NN) on the upstream dataset with labels (e.g.,ImageNet) and transfer learning (TL) to various downstream datasets arecommonly employed to evaluate the quality of visual representations learned viaself-supervised learning (SSL). Although existing SSL methods have shown goodperformances under those evaluation protocols, we observe that the performancesare very sensitive to the hyperparameters involved in LP and TL. We argue thatthis is an undesirable behavior since truly generic representations should beeasily adapted to any other visual recognition task, i.e., the learnedrepresentations should be robust to the settings of LP and TL hyperparameters.In this work, we try to figure out the cause of performance sensitivity byconducting extensive experiments with state-of-the-art SSL methods. First, wefind that input normalization for LP is crucial to eliminate performancevariations according to the hyperparameters. Specifically, batch normalizationbefore feeding inputs to a linear classifier considerably improves thestability of evaluation, and also resolves inconsistency of $k$-NN and LPmetrics. Second, for TL, we demonstrate that a weight decay parameter in SSLsignificantly affects the transferability of learned representations, whichcannot be identified by LP or $k$-NN evaluations on the upstream dataset. Webelieve that the findings of this study will be beneficial for the community bydrawing attention to the shortcomings in the current SSL evaluation schemes andunderscoring the need to reconsider them.", "output": "Rethinking Evaluation Protocols of Visual Representations Learned via Self-supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many applications require grouping instances contained in diverse documentdatasets into classes. Most widely used methods do not employ deep learning anddo not exploit the inherently multimodal nature of documents. Notably, recordlinkage is typically conceptualized as a string-matching problem. This studydevelops CLIPPINGS, (Contrastively Linking Pooled Pre-trained Embeddings), amultimodal framework for record linkage. CLIPPINGS employs end-to-end trainingof symmetric vision and language bi-encoders, aligned through contrastivelanguage-image pre-training, to learn a metric space where the pooledimage-text representation for a given instance is close to representations inthe same class and distant from representations in different classes. Atinference time, instances can be linked by retrieving their nearest neighborfrom an offline exemplar embedding index or by clustering theirrepresentations. The study examines two challenging applications: constructingcomprehensive supply chains for mid-20th century Japan through linking firmlevel financial records - with each firm name represented by its crop in thedocument image and the corresponding OCR - and detecting which image-captionpairs in a massive corpus of historical U.S. newspapers came from the sameunderlying photo wire source. CLIPPINGS outperforms widely used string matchingmethods by a wide margin and also outperforms unimodal methods. Moreover, apurely self-supervised model trained on only image-OCR pairs also outperformspopular string-matching methods without requiring any labels.", "output": "Linking Representations with Multimodal Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformer (ViT) has shown great potential for various visual tasksdue to its ability to model long-range dependency. However, ViT requires alarge amount of computing resource to compute the global self-attention. Inthis work, we propose a ladder self-attention block with multiple branches anda progressive shift mechanism to develop a light-weight transformer backbonethat requires less computing resources (e.g. a relatively small number ofparameters and FLOPs), termed Progressive Shift Ladder Transformer (PSLT).First, the ladder self-attention block reduces the computational cost bymodelling local self-attention in each branch. In the meanwhile, theprogressive shift mechanism is proposed to enlarge the receptive field in theladder self-attention block by modelling diverse local self-attention for eachbranch and interacting among these branches. Second, the input feature of theladder self-attention block is split equally along the channel dimension foreach branch, which considerably reduces the computational cost in the ladderself-attention block (with nearly 1/3 the amount of parameters and FLOPs), andthe outputs of these branches are then collaborated by a pixel-adaptive fusion.Therefore, the ladder self-attention block with a relatively small number ofparameters and FLOPs is capable of modelling long-range interactions. Based onthe ladder self-attention block, PSLT performs well on several vision tasks,including image classification, objection detection and personre-identification. On the ImageNet-1k dataset, PSLT achieves a top-1 accuracyof 79.9% with 9.2M parameters and 1.9G FLOPs, which is comparable to severalexisting models with more than 20M parameters and 4G FLOPs. Code is availableat ", "output": "PSLT: A Light-weight Vision Transformer with Ladder Self-Attention and Progressive Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic imaging addresses the recovery of a time-varying 2D or 3D object ateach time instant using its undersampled measurements. In particular, in thecase of dynamic tomography, only a single projection at a single view angle maybe available at a time, making the problem severely ill-posed. In this work, wepropose an approach, RED-PSM, which combines for the first time two powerfultechniques to address this challenging imaging problem. The first, arepartially separable models, which have been used to efficiently introduce alow-rank prior for the spatio-temporal object. The second is the recentRegularization by Denoising (RED), which provides a flexible framework toexploit the impressive performance of state-of-the-art image denoisingalgorithms, for various inverse problems. We propose a partially separableobjective with RED and an optimization scheme with variable splitting and ADMM,and prove convergence of our objective to a value corresponding to a stationarypoint satisfying the first order optimality conditions. Convergence isaccelerated by a particular projection-domain-based initialization. Wedemonstrate the performance and computational improvements of our proposedRED-PSM with a learned image denoiser by comparing it to a recentdeep-prior-based method TD-DIP.", "output": "RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In deep learning, mini-batch training is commonly used to optimize networkparameters. However, the traditional mini-batch method may not learn theunder-represented samples and complex patterns in the data, leading to a longertime for generalization. To address this problem, a variant of the traditionalalgorithm has been proposed, which trains the network focusing on mini-batcheswith high loss. The study evaluates the effectiveness of the proposed trainingusing various deep neural networks trained on three benchmark datasets(CIFAR-10, CIFAR-100, and STL-10). The deep neural networks used in the studyare ResNet-18, ResNet-50, Efficient Net B4, EfficientNetV2-S, andMobilenetV3-S. The experimental results showed that the proposed method cansignificantly improve the test accuracy and speed up the convergence comparedto the traditional mini-batch training method. Furthermore, we introduce ahyper-parameter delta ({delta}) that decides how many mini-batches areconsidered for training. Experiments on various values of {delta} found thatthe performance of the proposed method for smaller {delta} values generallyresults in similar test accuracy and faster generalization. We show that theproposed method generalizes in 26.47% less number of epochs than thetraditional mini-batch method in EfficientNet-B4 on STL-10. The proposed methodalso improves the test top-1 accuracy by 7.26% in ResNet-18 on CIFAR-100.", "output": "Can we learn better with hard samples?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While recent AI-based draping networks have significantly advanced theability to simulate the appearance of clothes worn by 3D human models, thehandling of multi-layered garments remains a challenging task. This paperpresents a model for draping multi-layered garments that are unseen during thetraining process. Our proposed framework consists of three stages: garmentembedding, single-layered garment draping, and untangling. The model representsa garment independent to its topological structure by mapping it onto the $UV$map of a human body model, allowing for the ability to handle previously unseengarments. In the single-layered garment draping phase, the model sequentiallydrapes all garments in each layer on the body without considering interactionsbetween them. The untangling phase utilizes a GNN-based network to model theinteraction between the garments of different layers, enabling the simulationof complex multi-layered clothing. The proposed model demonstrates strongperformance on both unseen synthetic and real garment reconstruction data on adiverse range of human body shapes and poses.", "output": "Multi-Layered Unseen Garments Draping Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The universal model emerges as a promising trend for medical imagesegmentation, paving up the way to build medical imaging large model (MILM).One popular strategy to build universal models is to encode each task as aone-hot vector and generate dynamic convolutional layers at the end of thedecoder to extract the interested target. Although successful, it ignores thecorrelations among tasks and meanwhile is too late to make the model 'aware' ofthe ongoing task. To address both issues, we propose a prompt-driven UniversalSegmentation model (UniSeg) for multi-task medical image segmentation usingdiverse modalities and domains. We first devise a learnable universal prompt todescribe the correlations among all tasks and then convert this prompt andimage features into a task-specific prompt, which is fed to the decoder as apart of its input. Thus, we make the model 'aware' of the ongoing task earlyand boost the task-specific training of the whole decoder. Our results indicatethat the proposed UniSeg outperforms other universal models and single-taskmodels on 11 upstream tasks. Moreover, UniSeg also beats other pre-trainedmodels on two downstream datasets, providing the community with a high-qualitypre-trained model for 3D medical image segmentation. Code and model areavailable at ", "output": "UniSeg: A Prompt-driven Universal Segmentation Model as well as A Strong Representation Learner."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene graph generation aims to construct a semantic graph structure from animage such that its nodes and edges respectively represent objects and theirrelationships. One of the major challenges for the task lies in the presence ofdistracting objects and relationships in images; contextual reasoning isstrongly distracted by irrelevant objects or backgrounds and, more importantly,a vast number of irrelevant candidate relations. To tackle the issue, wepropose the Selective Quad Attention Network (SQUAT) that learns to selectrelevant object pairs and disambiguate them via diverse contextualinteractions. SQUAT consists of two main components: edge selection and quadattention. The edge selection module selects relevant object pairs, i.e., edgesin the scene graph, which helps contextual reasoning, and the quad attentionmodule then updates the edge features using both edge-to-node and edge-to-edgecross-attentions to capture contextual information between objects and objectpairs. Experiments demonstrate the strong performance and robustness of SQUAT,achieving the state of the art on the Visual Genome and Open Images v6benchmarks.", "output": "Devil's on the Edges: Selective Quad Attention for Scene Graph Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Flower breed detection and giving details of that breed with the suggestionof cultivation processes and the way of taking care is important for flowercultivation, breed invention, and the flower business. Among all the localflowers in Bangladesh, the rose is one of the most popular and demandedflowers. Roses are the most desirable flower not only in Bangladesh but alsothroughout the world. Roses can be used for many other purposes apart fromdecoration. As roses have a great demand in the flower business so rose breeddetection will be very essential. However, there is no remarkable work forbreed detection of a particular flower unlike the classification of differentflowers. In this research, we have proposed a model to detect rose breeds fromimages using transfer learning techniques. For such work in flowers, resourcesare not enough in image processing and classification, so we needed a largedataset of the massive number of images to train our model. we have used 1939raw images of five different breeds and we have generated 9306 images for thetraining dataset and 388 images for the testing dataset to validate the modelusing augmentation. We have applied four transfer learning models in thisresearch, which are Inception V3, ResNet50, Xception, and VGG16. Among thesefour models, VGG16 achieved the highest accuracy of 99%, which is an excellentoutcome. Breed detection of a rose by using transfer learning methods is thefirst work on breed detection of a particular flower that is publicly availableaccording to the study.", "output": "Local Rose Breeds Detection System Using Transfer Learning Techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face morphing attack detection is emerging as an increasingly challengingproblem owing to advancements in high-quality and realistic morphing attackgeneration. Reliable detection of morphing attacks is essential because theseattacks are targeted for border control applications. This paper presents amultispectral framework for differential morphing-attack detection (D-MAD). TheD-MAD methods are based on using two facial images that are captured from theePassport (also called the reference image) and the trusted device (forexample, Automatic Border Control (ABC) gates) to detect whether the face imagepresented in ePassport is morphed. The proposed multispectral D-MAD frameworkintroduce a multispectral image captured as a trusted capture to capture sevendifferent spectral bands to detect morphing attacks. Extensive experiments wereconducted on the newly created datasets with 143 unique data subjects that werecaptured using both visible and multispectral cameras in multiple sessions. Theresults indicate the superior performance of the proposed multispectralframework compared to visible images.", "output": "Multispectral Imaging for Differential Face Morphing Attack Detection: A Preliminary Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Carrot is a famous nutritional vegetable and developed all over the world.Different diseases of Carrot has become a massive issue in the carrotproduction circle which leads to a tremendous effect on the economic growth inthe agricultural sector. An automatic carrot disease detection system can helpto identify malicious carrots and can provide a guide to cure carrot disease inan earlier stage, resulting in a less economical loss in the carrot productionsystem. The proposed research study has developed a web application Carrot Curebased on Convolutional Neural Network (CNN), which can identify a defectivecarrot and provide a proper curative solution. Images of carrots affected bycavity spot and leaf bright as well as healthy images were collected. Further,this research work has employed Convolutional Neural Network to include birthneural purposes and a Fully Convolutional Neural Network model (FCNN) forinfection order. Different avenues regarding different convolutional modelswith colorful layers are explored and the proposed Convolutional model hasachieved the perfection of 99.8%, which will be useful for the drovers todistinguish carrot illness and boost their advantage.", "output": "Carrot Cure: A CNN based Application to Detect Carrot Disease."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work explores the use of 3D generative models to synthesize trainingdata for 3D vision tasks. The key requirements of the generative models arethat the generated data should be photorealistic to match the real-worldscenarios, and the corresponding 3D attributes should be aligned with givensampling labels. However, we find that the recent NeRF-based 3D GANs hardlymeet the above requirements due to their designed generation pipeline and thelack of explicit 3D supervision. In this work, we propose Lift3D, an inverted2D-to-3D generation framework to achieve the data generation objectives. Lift3Dhas several merits compared to prior methods: (1) Unlike previous 3D GANs thatthe output resolution is fixed after training, Lift3D can generalize to anycamera intrinsic with higher resolution and photorealistic output. (2) Bylifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3Dinformation of generated objects, thus offering accurate 3D annotations fordownstream tasks. We evaluate the effectiveness of our framework by augmentingautonomous driving datasets. Experimental results demonstrate that our datageneration framework can effectively improve the performance of 3D objectdetectors. Project page: ", "output": "Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The past few years has witnessed the dominance of Graph ConvolutionalNetworks (GCNs) over human motion prediction, while their performance is stillfar from satisfactory. Recently, MLP-Mixers show competitive results on top ofbeing more efficient and simple. To extract features, GCNs typically follow anaggregate-and-update paradigm, while Mixers rely on token mixing and channelmixing operations. The two research paths have been independently establishedin the community. In this paper, we develop a novel perspective by unifyingMixers and GCNs. We show that a mixer layer can be seen as a graphconvolutional layer applied to a fully-connected graph with parameterizedadjacency. Extending this theoretical finding to the practical side, we proposeMeta-Mixing Network (M$^2$-Net). Assisted with a novel zero aggregationoperation, our network is capable of capturing both the structure-agnostic andthe structure-sensitive dependencies in a collaborative manner. Not only is itcomputationally efficient, but most importantly, it also achievesstate-of-the-art performance. An extensive evaluation on the Human3.6M, AMASS,and 3DPW datasets shows that M$^2$-Net consistently outperforms all otherapproaches. We hope our work brings the community one step further towardstruly predictable human motion. Our code will be publicly available.", "output": "A Mixer Layer is Worth One Graph Convolution: Unifying MLP-Mixers and GCNs for Human Motion Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence (AI)-based models can help in diagnosing COVID-19from lung CT scans and X-ray images; however, these models require largeamounts of data for training and validation. Many researchers studiedGenerative Adversarial Networks (GANs) for producing synthetic lung CT scansand X-Ray images to improve the performance of AI-based models. It is not wellexplored how good GAN-based methods performed to generate reliable syntheticdata. This work analyzes 43 published studies that reported GANs for syntheticdata generation. Many of these studies suffered data bias, lack ofreproducibility, and lack of feedback from the radiologists or other domainexperts. A common issue in these studies is the unavailability of the sourcecode, hindering reproducibility. The included studies reported rescaling of theinput images to train the existing GANs architecture without providing clinicalinsights on how the rescaling was motivated. Finally, even though GAN-basedmethods have the potential for data augmentation and improving the training ofAI-based models, these methods fall short in terms of their use in clinicalpractice. This paper highlights research hotspots in countering the datascarcity problem, identifies various issues as well as potentials, and providesrecommendations to guide future research. These recommendations might be usefulto improve acceptability for the GAN-based approaches for data augmentation asGANs for data augmentation are increasingly becoming popular in the AI andmedical imaging research community.", "output": "Leveraging GANs for data scarcity of COVID-19: Beyond the hype."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pathological image analysis is an important process for detectingabnormalities such as cancer from cell images. However, since the image size isgenerally very large, the cost of providing detailed annotations is high, whichmakes it difficult to apply machine learning techniques. One way to improve theperformance of identifying abnormalities while keeping the annotation cost lowis to use only labels for each slide, or to use information from anotherdataset that has already been labeled. However, such weak supervisoryinformation often does not provide sufficient performance. In this paper, weproposed a new task setting to improve the classification performance of thetarget dataset without increasing annotation costs. And to solve this problem,we propose a pipeline that uses multiple instance learning (MIL) and domainadaptation (DA) methods. Furthermore, in order to combine the supervisoryinformation of both methods effectively, we propose a method to createpseudo-labels with high confidence. We conducted experiments on thepathological image dataset we created for this study and showed that theproposed method significantly improves the classification performance comparedto existing methods.", "output": "Domain Adaptive Multiple Instance Learning for Instance-level Prediction of Pathological Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most of the existing blind image Super-Resolution (SR) methods assume thatthe blur kernels are space-invariant. However, the blur involved in realapplications are usually space-variant due to object motion, out-of-focus,etc., resulting in severe performance drop of the advanced SR methods. Toaddress this problem, we firstly introduce two new datasets with out-of-focusblur, i.e., NYUv2-BSR and Cityscapes-BSR, to support further researches ofblind SR with space-variant blur. Based on the datasets, we design a novelCross-MOdal fuSion network (CMOS) that estimate both blur and semanticssimultaneously, which leads to improved SR results. It involves a featureGrouping Interactive Attention (GIA) module to make the two modalities interactmore effectively and avoid inconsistency. GIA can also be used for theinteraction of other features because of the universality of its structure.Qualitative and quantitative experiments compared with state-of-the-art methodson above datasets and real-world images demonstrate the superiority of ourmethod, e.g., obtaining PSNR/SSIM by +1.91/+0.0048 on NYUv2-BSR than MANet.", "output": "Better \"CMOS\" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to Synthetic Aperture Radar (SAR) imaging characteristics, SAR vehiclerecognition faces the problem of extracting discriminative and robust targetfeatures from a small dataset. Deep learning has shown impressive performanceon the MSTAR dataset. However, data bias in a small dataset, such as backgroundcorrelation, impairs the causality of these methods, i.e., discriminativefeatures contain target and background differences. Moreover, differentoperating conditions of SAR lead to target signatures and background cluttervariations in imaging results. However, many deep learning-based methods onlyverify robustness to target or background variations in the currentexperimental setting. In this paper, we propose a novel domain alignmentframework named Hierarchical Disentanglement-Alignment Network (HDANet) toenhance features' causality and robustness. Concisely, HDANet consists of threeparts: The first part uses data augmentation to generate signature variationsfor domain alignment. The second part disentangles the target features througha multitask-assisted mask to prevent non-causal clutter from interfering withsubsequent alignment and recognition. Thirdly, a contrastive loss is employedfor domain alignment to extract robust target features, and the SimSiamstructure is applied to mitigate conflicts between contrastive loss and featurediscrimination. Finally, the proposed method shows high robustness acrossMSTAR's multiple target, sensor, and environment variants. Noteworthy, we add anew scene variant to verify the robustness to target and background variations.Moreover, the saliency map and Shapley value qualitatively and quantitativelydemonstrate causality. Our code is available inurl{", "output": "Hierarchical Disentanglement-Alignment Network for Robust SAR Vehicle Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dental template and parametric dental models are important tools for variousapplications in digital dentistry. However, constructing an unbiased dentaltemplate and accurate parametric dental models remains a challenging task dueto the complex anatomical and morphological dental structures and also lowvolume ratio of the teeth. In this study, we develop an unbiased dentaltemplate by constructing an accurate dental atlas from CBCT images withguidance of teeth segmentation. First, to address the challenges, we propose toenhance the CBCT images and their segmentation images, including imagecropping, image masking and segmentation intensity reassigning. Then, wefurther use the segmentation images to perform co-registration with the CBCTimages to generate an accurate dental atlas, from which an unbiased dentaltemplate can be generated. By leveraging the unbiased dental template, weconstruct parametric dental models by estimating point-to-point correspondencesbetween the dental models and employing Principal Component Analysis todetermine shape subspaces of the parametric dental models. A total of 159 CBCTimages of real subjects are collected to perform the constructions.Experimental results demonstrate effectiveness of our proposed method inconstructing unbiased dental template and parametric dental model. Thedeveloped dental template and parametric dental models are available at", "output": "Construction of unbiased dental template and parametric dental model for precision digital dentistry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised multi-frame depth estimation achieves high accuracy bycomputing matching costs of pixel correspondences between adjacent frames,injecting geometric information into the network. These pixel-correspondencecandidates are computed based on the relative pose estimates between theframes. Accurate pose predictions are essential for precise matching costcomputation as they influence the epipolar geometry. Furthermore, improveddepth estimates can, in turn, be used to align pose estimates.Inspired by traditional structure-from-motion (SfM) principles, we proposethe DualRefine model, which tightly couples depth and pose estimation through afeedback loop. Our novel update pipeline uses a deep equilibrium modelframework to iteratively refine depth estimates and a hidden state of featuremaps by computing local matching costs based on epipolar geometry. Importantly,we used the refined depth estimates and feature maps to compute pose updates ateach step. This update in the pose estimates slowly alters the epipolargeometry during the refinement process. Experimental results on the KITTIdataset demonstrate competitive depth prediction and odometry predictionperformance surpassing published self-supervised baselines.", "output": "DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image segmentation is a fundamental task in the field of imaging and vision.Supervised deep learning for segmentation has achieved unparalleled successwhen sufficient training data with annotated labels are available. However,annotation is known to be expensive to obtain, especially for histopathologyimages where the target regions are usually with high morphology variations andirregular shapes. Thus, weakly supervised learning with sparse annotations ofpoints is promising to reduce the annotation workload. In this work, we proposea contrast-based variational model to generate segmentation results, whichserve as reliable complementary supervision to train a deep segmentation modelfor histopathology images. The proposed method considers the commoncharacteristics of target regions in histopathology images and can be trainedin an end-to-end manner. It can generate more regionally consistent andsmoother boundary segmentation, and is more robust to unlabeled `novel'regions. Experiments on two different histology datasets demonstrate itseffectiveness and efficiency in comparison to previous models.", "output": "Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pretraining on large-scale datasets can boost the performance of objectdetectors while the annotated datasets for object detection are hard to scaleup due to the high labor cost. What we possess are numerous isolatedfiled-specific datasets, thus, it is appealing to jointly pretrain modelsacross aggregation of datasets to enhance data volume and diversity. In thispaper, we propose a strong framework for utilizing Multiple datasets topretrain DETR-like detectors, termed METR, without the need for manual labelspaces integration. It converts the typical multi-classification in objectdetection into binary classification by introducing a pre-trained languagemodel. Specifically, we design a category extraction module for extractingpotential categories involved in an image and assign these categories intodifferent queries by language embeddings. Each query is only responsible forpredicting a class-specific object. Besides, to adapt our novel detectionparadigm, we propose a group bipartite matching strategy that limits the groundtruths to match queries assigned to the same category. Extensive experimentsdemonstrate that METR achieves extraordinary results on either multi-task jointtraining or the pretrain &amp; finetune paradigm. Notably, our pre-trained modelshave high flexible transferability and increase the performance upon variousDETR-like detectors on COCO val2017 benchmark. Codes will be available afterthis paper is published.", "output": "Language-aware Multiple Datasets Detection Pretraining for DETRs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This research sets out to assess the viability of using game engines togenerate synthetic training data for machine learning in the context of palletsegmentation. Using synthetic data has been proven in prior research to be aviable means of training neural networks and saves hours of manual labour dueto the reduced need for manual image annotation. Machine vision for palletdetection can benefit from synthetic data as the industry increases thedevelopment of autonomous warehousing technologies. As per our methodology, wedeveloped a tool capable of automatically generating large amounts of annotatedtraining data from 3D models at pixel-perfect accuracy and a much faster ratethan manual approaches. Regarding image segmentation, a Mask R-CNN pipeline wasused, which achieved an AP50 of 86% for individual pallets.", "output": "Pallet Detection from Synthetic Data Using Game Engines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image keypoints and descriptors play a crucial role in many visualmeasurement tasks. In recent years, deep neural networks have been widely usedto improve the performance of keypoint and descriptor extraction. However, theconventional convolution operations do not provide the geometric invariancerequired for the descriptor. To address this issue, we propose the SparseDeformable Descriptor Head (SDDH), which learns the deformable positions ofsupporting features for each keypoint and constructs deformable descriptors.Furthermore, SDDH extracts descriptors at sparse keypoints instead of a densedescriptor map, which enables efficient extraction of descriptors with strongexpressiveness. In addition, we relax the neural reprojection error (NRE) lossfrom dense to sparse to train the extracted sparse descriptors. Experimentalresults show that the proposed network is both efficient and powerful invarious visual measurement tasks, including image matching, 3D reconstruction,and visual relocalization.", "output": "ALIKED: A Lighter Keypoint and Descriptor Extraction Network via Deformable Transformation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Smart farming is a growing field as technology advances. Plantcharacteristics are crucial indicators for monitoring plant growth. Researchhas been done to estimate characteristics like leaf area index, leaf disease,and plant height. However, few methods have been applied to non-destructivemeasurements of leaf size. In this paper, an automated non-destructiveimaged-based measuring system is presented, which uses 2D and 3D data obtainedusing a Zivid 3D camera, creating 3D virtual representations (digital twins) ofthe tomato plants. Leaves are detected from corresponding 2D RGB images andmapped to their 3D point cloud using the detected leaf masks, which then passthe leaf point cloud to the plane fitting algorithm to extract the leaf size toprovide data for growth monitoring. The performance of the measurement platformhas been measured through a comprehensive trial on real-world tomato plantswith quantified performance metrics compared to ground truth measurements.Three tomato leaf and height datasets (including 50+ 3D point cloud files oftomato plants) were collected and open-sourced in this project. The proposedleaf size estimation method demonstrates an RMSE value of 4.47mm and an R^2value of 0.87. The overall measurement system (leaf detection and sizeestimation algorithms combine) delivers an RMSE value of 8.13mm and an R^2value of 0.899.", "output": "Look how they have grown: Non-destructive Leaf Detection and Size Estimation of Tomato Plants for 3D Growth Monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Active Object Tracking (AOT) aims to maintain a specific relation between thetracker and object(s) by autonomously controlling the motion system of atracker given observations. AOT has wide-ranging applications, such as inmobile robots and autonomous driving. However, building a generalizable activetracker that works robustly across different scenarios remains a challenge,especially in unstructured environments with cluttered obstacles and diverselayouts. We argue that constructing a state representation capable of modelingthe geometry structure of the surroundings and the dynamics of the target iscrucial for achieving this goal. To address this challenge, we present RSPT, aframework that forms a structure-aware motion representation by Reconstructingthe Surroundings and Predicting the target Trajectory. Additionally, we enhancethe generalization of the policy network by training in an asymmetric duelingmechanism. We evaluate RSPT on various simulated scenarios and show that itoutperforms existing methods in unseen environments, particularly those withcomplex obstacles and layouts. We also demonstrate the successful transfer ofRSPT to real-world settings. Project Website:", "output": "RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The standard class-incremental continual learning setting assumes a set oftasks seen one after the other in a fixed and predefined order. This is notvery realistic in federated learning environments where each client worksindependently in an asynchronous manner getting data for the different tasks intime-frames and orders totally uncorrelated with the other ones. We introduce anovel federated learning setting (AFCL) where the continual learning ofmultiple tasks happens at each client with different orderings and inasynchronous time slots. We tackle this novel task using prototype-basedlearning, a representation loss, fractal pre-training, and a modifiedaggregation policy. Our approach, called FedSpace, effectively tackles thistask as shown by the results on the CIFAR-100 dataset using 3 differentfederated splits with 50, 100, and 500 clients, respectively. The code andfederated splits are available at ", "output": "Asynchronous Federated Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we introduce a rule-based, compositional, and hierarchicalmodeling of action using Therbligs as our atoms. Introducing these atomsprovides us with a consistent, expressive, contact-centered representation ofaction. Over the atoms we introduce a differentiable method of rule-basedreasoning to regularize for logical consistency. Our approach is complementaryto other approaches in that the Therblig-based representations produced by ourarchitecture augment rather than replace existing architectures'representations. We release the first Therblig-centered annotations over twopopular video datasets - EPIC Kitchens 100 and 50-Salads. We also broadlydemonstrate benefits to adopting Therblig representations through evaluation onthe following tasks: action segmentation, action anticipation, and actionrecognition - observing an average 10.5%/7.53%/6.5% relative improvement,respectively, over EPIC Kitchens and an average 8.9%/6.63%/4.8% relativeimprovement, respectively, over 50 Salads. Code and data will be made publiclyavailable.", "output": "Therbligs in Action: Video Understanding through Motion Primitives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D interacting hand pose estimation from a single RGB image is a challengingtask, due to serious self-occlusion and inter-occlusion towards hands,confusing similar appearance patterns between 2 hands, ill-posed joint positionmapping from 2D to 3D, etc.. To address these, we propose to extend A2J-thestate-of-the-art depth-based 3D single hand pose estimation method-to RGBdomain under interacting hand condition. Our key idea is to equip A2J withstrong local-global aware ability to well capture interacting hands' local finedetails and global articulated clues among joints jointly. To this end, A2J isevolved under Transformer's non-local encoding-decoding framework to buildA2J-Transformer. It holds 3 main advantages over A2J. First, self-attentionacross local anchor points is built to make them global spatial context awareto better capture joints' articulation clues for resisting occlusion. Secondly,each anchor point is regarded as learnable query with adaptive feature learningfor facilitating pattern fitting capacity, instead of having the same localrepresentation with the others. Last but not least, anchor point locates in 3Dspace instead of 2D as in A2J, to leverage 3D pose prediction. Experiments onchallenging InterHand 2.6M demonstrate that, A2J-Transformer can achievestate-of-the-art model-free performance (3.38mm MPJPE advancement in 2-handcase) and can also be applied to depth domain with strong generalization.", "output": "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As bird's-eye-view (BEV) semantic segmentation is simple-to-visualize andeasy-to-handle, it has been applied in autonomous driving to provide thesurrounding information to downstream tasks. Inferring BEV semanticsegmentation conditioned on multi-camera-view images is a popular scheme in thecommunity as cheap devices and real-time processing. The recent workimplemented this task by learning the content and position relationship via thevision Transformer (ViT). However, the quadratic complexity of ViT confines therelationship learning only in the latent layer, leaving the scale gap to impedethe representation of fine-grained objects. And their plain fusion method ofmulti-view features does not conform to the information absorption intention inrepresenting BEV features. To tackle these issues, we propose a novelcross-scale hierarchical Transformer with correspondence-augmented attentionfor semantic segmentation inferring. Specifically, we devise a hierarchicalframework to refine the BEV feature representation, where the last size is onlyhalf of the final segmentation. To save the computation increase caused by thishierarchical framework, we exploit the cross-scale Transformer to learn featurerelationships in a reversed-aligning way, and leverage the residual connectionof BEV features to facilitate information transmission between scales. Wepropose correspondence-augmented attention to distinguish conducive andinconducive correspondences. It is implemented in a simple yet effective way,amplifying attention scores before the Softmax operation, so that theposition-view-related and the position-view-disrelated attention scores arehighlighted and suppressed. Extensive experiments demonstrate that our methodhas state-of-the-art performance in inferring BEV semantic segmentationconditioned on multi-camera-view images.", "output": "A Cross-Scale Hierarchical Transformer with Correspondence-Augmented Attention for inferring Bird's-Eye-View Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel framework for probing and improving relational,compositional and contextual understanding of large visual-language models(V+L). While large V+L models have achieved success in various downstreamtasks, it is not clear if they have a conceptual grasp of the content. Wepropose a novel benchmarking dataset for probing three aspects of contentunderstanding. Our probes are grounded in cognitive science and help determineif a V+L model can, for example, determine if snow garnished with a man isimplausible, or if it can identify beach furniture by knowing it is located ona beach. We have experimented with 5 well known models, such as CLIP and ViLT,and found that they mostly fail to demonstrate a conceptual understanding. Thatsaid, we find interesting insights such as cross-attention helps learningconceptual understanding. We use these insights to propose a new finetuningtechnique that rewards the three conceptual understanding measures we proposed.We hope that the presented benchmarks will help the community assess andimprove the conceptual understanding capabilities of large V+L models.", "output": "Probing Conceptual Understanding of Large Visual-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Product Retrieval (PR) and Grounding (PG), aiming to seek image andobject-level products respectively according to a textual query, have attractedgreat interest recently for better shopping experience. Owing to the lack ofrelevant datasets, we collect two large-scale benchmark datasets from TaobaoMall and Live domains with about 474k and 101k image-query pairs for PR, andmanually annotate the object bounding boxes in each image for PG. As annotatingboxes is expensive and time-consuming, we attempt to transfer knowledge fromannotated domain to unannotated for PG to achieve un-supervised DomainAdaptation (PG-DA). We propose a {bf D}omain {bf A}daptive Produc{bf t}S{bf e}eker ({bf DATE}) framework, regarding PR and PG as Product Seekingproblem at different levels, to assist the query {bf date} the product.Concretely, we first design a semantics-aggregated feature extractor for eachmodality to obtain concentrated and comprehensive features for followingefficient retrieval and fine-grained grounding tasks. Then, we present twocooperative seekers to simultaneously search the image for PR and localize theproduct for PG. Besides, we devise a domain aligner for PG-DA to alleviateuni-modal marginal and multi-modal conditional distribution shift betweensource and target domains, and design a pseudo box generator to dynamicallyselect reliable instances and generate bounding boxes for further knowledgetransfer. Extensive experiments show that our DATE achieves satisfactoryperformance in fully-supervised PR, PG and un-supervised PG-DA. Ourdesensitized datasets will be publicly availableherefootnote{url{", "output": "DATE: Domain Adaptive Product Seeker for E-commerce."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image captioning models are known to perpetuate and amplify harmful societalbias in the training set. In this work, we aim to mitigate such gender bias inimage captioning models. While prior work has addressed this problem by forcingmodels to focus on people to reduce gender misclassification, it converselygenerates gender-stereotypical words at the expense of predicting the correctgender. From this observation, we hypothesize that there are two types ofgender bias affecting image captioning models: 1) bias that exploits context topredict gender, and 2) bias in the probability of generating certain (oftenstereotypical) words because of gender. To mitigate both types of genderbiases, we propose a framework, called LIBRA, that learns from syntheticallybiased samples to decrease both types of biases, correcting gendermisclassification and changing gender-stereotypical words to more neutral ones.", "output": "Model-Agnostic Gender Debiased Image Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Our work focuses on the Multi-Object Navigation (MultiON) task, where anagent needs to navigate to multiple objects in a given sequence. Wesystematically investigate the inherent modularity of this task by dividing ourapproach to contain four modules: (a) an object detection module trained toidentify objects from RGB images, (b) a map building module to build a semanticmap of the observed objects, (c) an exploration module enabling the agent toexplore its surroundings, and finally (d) a navigation module to move toidentified target objects. We focus on the navigation and the explorationmodules in this work. We show that we can effectively leverage a PointGoalnavigation model in the MultiON task instead of learning to navigate fromscratch. Our experiments show that a PointGoal agent-based navigation moduleoutperforms analytical path planning on the MultiON task. We also compareexploration strategies and surprisingly find that a random exploration strategysignificantly outperforms more advanced exploration methods. We additionallycreate MultiON 2.0, a new large-scale dataset as a test-bed for our approach.", "output": "Reduce, Reuse, Recycle: Modular Multi-Object Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapid evolvement of deepfake creation technologies is seriously threatingmedia information trustworthiness. The consequences impacting targetedindividuals and institutions can be dire. In this work, we study the evolutionsof deep learning architectures, particularly CNNs and Transformers. Weidentified eight promising deep learning architectures, designed and developedour deepfake detection models and conducted experiments over well-establisheddeepfake datasets. These datasets included the latest second and thirdgeneration deepfake datasets. We evaluated the effectiveness of our developedsingle model detectors in deepfake detection and cross datasets evaluations. Weachieved 88.74%, 99.53%, 97.68%, 99.73% and 92.02% accuracy and 99.95%, 100%,99.88%, 99.99% and 97.61% AUC, in the detection of FF++ 2020, Google DFD,Celeb-DF, Deeper Forensics and DFDC deepfakes, respectively. We also identifiedand showed the unique strengths of CNNs and Transformers models and analysedthe observed relationships among the different deepfake datasets, to aid futuredevelopments in this area.", "output": "Deepfake Detection with Deep Learning: Convolutional Neural Networks versus Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient automatic segmentation of multi-level (i.e. main and branch)pulmonary arteries (PA) in CTPA images plays a significant role in clinicalapplications. However, most existing methods concentrate only on main PA orbranch PA segmentation separately and ignore segmentation efficiency. Besides,there is no public large-scale dataset focused on PA segmentation, which makesit highly challenging to compare the different methods. To benchmarkmulti-level PA segmentation algorithms, we organized the firsttextbf{P}ulmonary textbf{AR}tery textbf{SE}gmentation (PARSE) challenge. Onthe one hand, we focus on both the main PA and the branch PA segmentation. Onthe other hand, for better clinical application, we assign the same scoreweight to segmentation efficiency (mainly running time and GPU memoryconsumption during inference) while ensuring PA segmentation accuracy. Wepresent a summary of the top algorithms and offer some suggestions forefficient and accurate multi-level PA automatic segmentation. We provide thePARSE challenge as open-access for the community to benchmark future algorithmdevelopments at url{", "output": "Efficient automatic segmentation for multi-level pulmonary arteries: The PARSE challenge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Single domain generalization aims to learn a model from a single trainingdomain (source domain) and apply it to multiple unseen test domains (targetdomains). Existing methods focus on expanding the distribution of the trainingdomain to cover the target domains, but without estimating the domain shiftbetween the source and target domains. In this paper, we propose a new learningparadigm, namely simulate-analyze-reduce, which first simulates the domainshift by building an auxiliary domain as the target domain, then learns toanalyze the causes of domain shift, and finally learns to reduce the domainshift for model adaptation. Under this paradigm, we propose a meta-causallearning method to learn meta-knowledge, that is, how to infer the causes ofdomain shift between the auxiliary and source domains during training. We usethe meta-knowledge to analyze the shift between the target and source domainsduring testing. Specifically, we perform multiple transformations on sourcedata to generate the auxiliary domain, perform counterfactual inference tolearn to discover the causal factors of the shift between the auxiliary andsource domains, and incorporate the inferred causality into factor-aware domainalignments. Extensive experiments on several benchmarks of image classificationshow the effectiveness of our method.", "output": "Meta-causal Learning for Single Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, contrastive learning approaches (e.g., CLIP (Radford et al., 2021))have received huge success in multimodal learning, where the model tries tominimize the distance between the representations of different views (e.g.,image and its caption) of the same data point while keeping the representationsof different data points away from each other. However, from a theoreticalperspective, it is unclear how contrastive learning can learn therepresentations from different views efficiently, especially when the data isnot isotropic. In this work, we analyze the training dynamics of a simplemultimodal contrastive learning model and show that contrastive pairs areimportant for the model to efficiently balance the learned representations. Inparticular, we show that the positive pairs will drive the model to align therepresentations at the cost of increasing the condition number, while thenegative pairs will reduce the condition number, keeping the learnedrepresentations balanced.", "output": "On the Importance of Contrastive Loss in Multimodal Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Structural health monitoring (SHM) tasks like damage detection are crucialfor decision-making regarding maintenance and deterioration. For example, crackdetection in SHM is crucial for bridge maintenance as crack progression canlead to structural instability. However, most AI/ML models in the literaturehave low latency and late inference time issues while performing in real-timeenvironments. This study aims to explore the integration of edge-AI in the SHMdomain for real-time bridge inspections. Based on edge-AI literature, itscapabilities will be valuable integration for a real-time decision supportsystem in SHM tasks such that real-time inferences can be performed on physicalsites. This study will utilize commercial edge-AI platforms, such as GoogleCoral Dev Board or Kneron KL520, to develop and analyze the effectiveness ofedge-AI devices. Thus, this study proposes an edge AI framework for thestructural health monitoring domain. An edge-AI-compatible deep learning modelis developed to validate the framework to perform real-time crackclassification. The effectiveness of this model will be evaluated based on itsaccuracy, the confusion matrix generated, and the inference time observed in areal-time setting.", "output": "Integrating Edge-AI in Structural Health Monitoring domain."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in detecting arbitrary objects in the real world are trainedand evaluated on object detection datasets with a relatively restrictedvocabulary. To facilitate the development of more general visual objectdetection, we propose V3Det, a vast vocabulary visual detection dataset withprecisely annotated bounding boxes on massive images. V3Det has severalappealing properties: 1) Vast Vocabulary: It contains bounding boxes of objectsfrom 13,029 categories on real-world images, which is 10 times larger than theexisting large vocabulary object detection dataset, e.g., LVIS. 2) HierarchicalCategory Organization: The vast vocabulary of V3Det is organized by ahierarchical category tree which annotates the inclusion relationship amongcategories, encouraging the exploration of category relationships in vast andopen vocabulary object detection. 3) Rich Annotations: V3Det comprisesprecisely annotated objects in 245k images and professional descriptions ofeach category written by human experts and a powerful chatbot. By offering avast exploration space, V3Det enables extensive benchmarks on both vast andopen vocabulary object detection, leading to new observations, practices, andinsights for future research. It has the potential to serve as a cornerstonedataset for developing more general visual perception systems.", "output": "V3Det: Vast Vocabulary Visual Detection Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Causal Video Question Answering (CVidQA) queries not only association ortemporal relations but also causal relations in a video. Existing questionsynthesis methods pre-trained question generation (QG) systems on readingcomprehension datasets with text descriptions as inputs. However, QG modelsonly learn to ask association questions (e.g., ``what is someone doing...'')and result in inferior performance due to the poor transfer of associationknowledge to CVidQA, which focuses on causal questions like ``why is someonedoing ...''. Observing this, we proposed to exploit causal knowledge togenerate question-answer pairs, and proposed a novel framework, CausalKnowledge Extraction from Language Models (CaKE-LM), leveraging causalcommonsense knowledge from language models to tackle CVidQA. To extractknowledge from LMs, CaKE-LM generates causal questions containing two eventswith one triggering another (e.g., ``score a goal'' triggers ``soccer playerkicking ball'') by prompting LM with the action (soccer player kicking ball) toretrieve the intention (to score a goal). CaKE-LM significantly outperformsconventional methods by 4% to 6% of zero-shot CVidQA accuracy on NExT-QA andCausal-VidQA datasets. We also conduct comprehensive analyses and provide keyfindings for future research.", "output": "Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anatomically consistent field-of-view (FOV) completion to recover truncatedbody sections has important applications in quantitative analyses of computedtomography (CT) with limited FOV. Existing solution based on conditionalgenerative models relies on the fidelity of synthetic truncation patterns attraining phase, which poses limitations for the generalizability of the methodto potential unknown types of truncation. In this study, we evaluate azero-shot method based on a pretrained unconditional generative diffusionprior, where truncation pattern with arbitrary forms can be specified atinference phase. In evaluation on simulated chest CT slices with synthetic FOVtruncation, the method is capable of recovering anatomically consistent bodysections and subcutaneous adipose tissue measurement error caused by FOVtruncation. However, the correction accuracy is inferior to the conditionallytrained counterpart.", "output": "Zero-shot CT Field-of-view Completion with Unconditional Generative Diffusion Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Removing clutter from scenes is essential in many applications, ranging fromprivacy-concerned content filtering to data augmentation. In this work, wepresent an automatic system that removes clutter from 3D scenes and inpaintswith coherent geometry and texture. We propose techniques for its two keycomponents: 3D segmentation from shared properties and 3D inpainting, both ofwhich are important porblems. The definition of 3D scene clutter(frequently-moving objects) is not well captured by commonly-studied objectcategories in computer vision. To tackle the lack of well-defined clutterannotations, we group noisy fine-grained labels, leverage virtual rendering,and impose an instance-level area-sensitive loss. Once clutter is removed, weinpaint geometry and texture in the resulting holes by merging inpainted RGB-Dimages. This requires novel voting and pruning strategies that guaranteemulti-view consistency across individually inpainted images for meshreconstruction. Experiments on ScanNet and Matterport dataset show that ourmethod outperforms baselines for clutter segmentation and 3D inpainting, bothvisually and quantitatively.", "output": "Clutter Detection and Removal in 3D Scenes with View-Consistent Inpainting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we address the well-known image quality assessment problem butin contrast from existing approaches that predict image quality independentlyfor every images, we propose to jointly model different images depicting thesame content to improve the precision of quality estimation. This proposal ismotivated by the idea that multiple distorted images can provide information todisambiguate image features related to content and quality. To this aim, wecombine the feature representations from the different images to estimate apseudo-reference that we use to enhance score prediction. Our experiments showthat at test-time, our method successfully combines the features from multipleimages depicting the same new content, improving estimation quality.", "output": "Test your samples jointly: Pseudo-reference for image quality evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans, even at a very early age, can learn visual concepts and understandgeometry and layout through active interaction with the environment, andgeneralize their compositions to complete tasks described by natural languagesin novel scenes. To mimic such capability, we propose Embodied Concept Learner(ECL) in an interactive 3D environment. Specifically, a robot agent can groundvisual concepts, build semantic maps and plan actions to complete tasks bylearning purely from human demonstrations and language instructions, withoutaccess to ground-truth semantic and depth supervisions from simulations. ECLconsists of: (i) an instruction parser that translates the natural languagesinto executable programs; (ii) an embodied concept learner that grounds visualconcepts based on language descriptions; (iii) a map constructor that estimatesdepth and constructs semantic maps by leveraging the learned concepts; and (iv)a program executor with deterministic policies to execute each program. ECL hasseveral appealing benefits thanks to its modularized design. Firstly, itenables the robotic agent to learn semantics and depth unsupervisedly actinglike babies, e.g., ground concepts through active interaction and perceivedepth by disparities when moving forward. Secondly, ECL is fully transparentand step-by-step interpretable in long-term planning. Thirdly, ECL could bebeneficial for the embodied instruction following (EIF), outperforming previousworks on the ALFRED benchmark when the semantic label is not provided. Also,the learned concept can be reused for other downstream tasks, such as reasoningof object states. Project page: <a href=\" http URL</a>", "output": "Embodied Concept Learner: Self-supervised Learning of Concepts and Mapping through Instruction Following."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human visual recognition is a sparse process, where only a few salient visualcues are attended to rather than traversing every detail uniformly. However,most current vision networks follow a dense paradigm, processing every singlevisual unit (e.g,, pixel or patch) in a uniform manner. In this paper, wechallenge this dense paradigm and present a new method, coined SparseFormer, toimitate human's sparse visual recognition in an end-to-end manner. SparseFormerlearns to represent images using a highly limited number of tokens (down to 49)in the latent space with sparse feature sampling procedure instead ofprocessing dense units in the original pixel space. Therefore, SparseFormercircumvents most of dense operations on the image space and has much lowercomputational costs. Experiments on the ImageNet classification benchmarkdataset show that SparseFormer achieves performance on par with canonical orwell-established models while offering better accuracy-throughput tradeoff.Moreover, the design of our network can be easily extended to the videoclassification with promising performance at lower computational costs. We hopethat our work can provide an alternative way for visual modeling and inspirefurther research on sparse neural architectures. The code will be publiclyavailable at ", "output": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Detecting abnormal events in video is commonly framed as a one-classclassification task, where training videos contain only normal events, whiletest videos encompass both normal and abnormal events. In this scenario,anomaly detection is an open-set problem. However, some studies assimilateanomaly detection to action recognition. This is a closed-set scenario thatfails to test the capability of systems at detecting new anomaly types. To thisend, we propose UBnormal, a new supervised open-set benchmark composed ofmultiple virtual scenes for video anomaly detection. Unlike existing data sets,we introduce abnormal events annotated at the pixel level at training time, forthe first time enabling the use of fully-supervised learning methods forabnormal event detection. To preserve the typical open-set formulation, we makesure to include disjoint sets of anomaly types in our training and testcollections of videos. To our knowledge, UBnormal is the first video anomalydetection benchmark to allow a fair head-to-head comparison between one-classopen-set models and supervised closed-set models, as shown in our experiments.Moreover, we provide empirical evidence showing that UBnormal can enhance theperformance of a state-of-the-art anomaly detection framework on two prominentdata sets, Avenue and ShanghaiTech. Our benchmark is freely available at", "output": "UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A core component of the recent success of self-supervised learning iscropping data augmentation, which selects sub-regions of an image to be used aspositive views in the self-supervised loss. The underlying assumption is thatrandomly cropped and resized regions of a given image share information aboutthe objects of interest, which the learned representation will capture. Thisassumption is mostly satisfied in datasets such as ImageNet where there is alarge, centered object, which is highly likely to be present in random crops ofthe full image. However, in other datasets such as OpenImages or COCO, whichare more representative of real world uncurated data, there are typicallymultiple small objects in an image. In this work, we show that self-supervisedlearning based on the usual random cropping performs poorly on such datasets.We propose replacing one or both of the random crops with crops obtained froman object proposal algorithm. This encourages the model to learn both objectand scene level semantic representations. Using this approach, which we callobject-aware cropping, results in significant improvements over scene croppingon classification and object detection benchmarks. For example, on OpenImages,our approach achieves an improvement of 8.8% mAP over random scene-levelcropping using MoCo-v2 based pre-training. We also show significantimprovements on COCO and PASCAL-VOC object detection and segmentation tasksover the state-of-the-art self-supervised learning approaches. Our approach isefficient, simple and general, and can be used in most existing contrastive andnon-contrastive self-supervised learning frameworks.", "output": "Object-Aware Cropping for Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The shipping industry is one of the strongest anthropogenic emitters of$text{NO}_text{x}$ -- substance harmful both to human health and theenvironment. The rapid growth of the industry causes societal pressure oncontrolling the emission levels produced by ships. All the methods currentlyused for ship emission monitoring are costly and require proximity to a ship,which makes global and continuous emission monitoring impossible. A promisingapproach is the application of remote sensing. Studies showed that some of the$text{NO}_text{2}$ plumes from individual ships can visually be distinguishedusing the TROPOspheric Monitoring Instrument on board the Copernicus Sentinel 5Precursor (TROPOMI/S5P). To deploy a remote sensing-based global emissionmonitoring system, an automated procedure for the estimation of$text{NO}_text{2}$ emissions from individual ships is needed. The extremelylow signal-to-noise ratio of the available data as well as the absence ofground truth makes the task very challenging. Here, we present a methodologyfor the automated segmentation of $text{NO}_text{2}$ plumes produced byseagoing ships using supervised machine learning on TROPOMI/S5P data. We showthat the proposed approach leads to a more than a 20% increase in the averageprecision score in comparison to the methods used in previous studies andresults in a high correlation of 0.834 with the theoretically derived shipemission proxy. This work is a crucial step toward the development of anautomated procedure for global ship emission monitoring using remote sensingdata.", "output": "Supervised segmentation of NO2 plumes from individual ships using TROPOMI satellite data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) are built on the assumption of treating imagepatches as ``visual tokens\" and learn patch-to-patch attention. The patchembedding based tokenizer has a semantic gap with respect to its counterpart,the textual tokenizer. The patch-to-patch attention suffers from the quadraticcomplexity issue, and also makes it non-trivial to explain learned ViTs. Toaddress these issues in ViT, this paper proposes to learn Patch-to-Clusterattention (PaCa) in ViT. Queries in our PaCa-ViT starts with patches, whilekeys and values are directly based on clustering (with a predefined smallnumber of clusters). The clusters are learned end-to-end, leading to bettertokenizers and inducing joint clustering-for-attention andattention-for-clustering for better and interpretable models. The quadraticcomplexity is relaxed to linear complexity. The proposed PaCa module is used indesigning efficient and interpretable ViT backbones and semantic segmentationhead networks. In experiments, the proposed methods are tested on ImageNet-1kimage classification, MS-COCO object detection and instance segmentation andMIT-ADE20k semantic segmentation. Compared with the prior art, it obtainsbetter performance in all the three benchmarks than the SWin and the PVTs bysignificant margins in ImageNet-1k and MIT-ADE20k. It is also significantlymore efficient than PVT models in MS-COCO and MIT-ADE20k due to the linearcomplexity. The learned clusters are semantically meaningful. Code and modelcheckpoints are available at ", "output": "PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep multimodal learning has achieved great progress in recent years.However, current fusion approaches are static in nature, i.e., they process andfuse multimodal inputs with identical computation, without accounting fordiverse computational demands of different multimodal data. In this work, wepropose dynamic multimodal fusion (DynMM), a new approach that adaptively fusesmultimodal data and generates data-dependent forward paths during inference. Tothis end, we propose a gating function to provide modality-level orfusion-level decisions on-the-fly based on multimodal features and aresource-aware loss function that encourages computational efficiency. Resultson various multimodal tasks demonstrate the efficiency and wide applicabilityof our approach. For instance, DynMM can reduce the computation costs by 46.5%with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improvesegmentation performance with over 21% savings in computation (NYU Depth V2semantic segmentation) when compared with static fusion approaches. We believeour approach opens a new direction towards dynamic multimodal network design,with applications to a wide range of multimodal tasks.", "output": "Dynamic Multimodal Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Implicit fields have recently shown increasing success in representing andlearning 3D shapes accurately. Signed distance fields and occupancy fields aredecades old and still the preferred representations, both with well-studiedproperties, despite their restriction to closed surfaces. With neural networks,several other variations and training principles have been proposed with thegoal to represent all classes of shapes. In this paper, we develop a novel andyet a fundamental representation considering unit vectors in 3D space and callit Vector Field (VF): at each point in $mathbb{R}^3$, VF is directed at theclosest point on the surface. We theoretically demonstrate that VF can beeasily transformed to surface density by computing the flux density. Unlikeother standard representations, VF directly encodes an important physicalproperty of the surface, its normal. We further show the advantages of VFrepresentation, in learning open, closed, or multi-layered as well as piecewiseplanar surfaces. We compare our method on several datasets including ShapeNetwhere the proposed new neural implicit field shows superior accuracy inrepresenting any type of shape, outperforming other standard methods. Code isavailable at ", "output": "Neural Vector Fields for Implicit Surface Representation and Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Previous multi-task dense prediction studies developed complex pipelines suchas multi-modal distillations in multiple stages or searching for taskrelational contexts for each task. The core insight beyond these methods is tomaximize the mutual effects of each task. Inspired by the recent query-basedTransformers, we propose a simple pipeline named Multi-Query Transformer(MQTransformer) that is equipped with multiple queries from different tasks tofacilitate the reasoning among multiple tasks and simplify the cross-taskinteraction pipeline. Instead of modeling the dense per-pixel context amongdifferent tasks, we seek a task-specific proxy to perform cross-task reasoningvia multiple queries where each query encodes the task-related context. TheMQTransformer is composed of three key components: shared encoder, cross-taskquery attention module and shared decoder. We first model each task with atask-relevant query. Then both the task-specific feature output by the featureextractor and the task-relevant query are fed into the shared encoder, thusencoding the task-relevant query from the task-specific feature. Secondly, wedesign a cross-task query attention module to reason the dependencies amongmultiple task-relevant queries; this enables the module to only focus on thequery-level interaction. Finally, we use a shared decoder to gradually refinethe image features with the reasoned query features from different tasks.Extensive experiment results on two dense prediction datasets (NYUD-v2 andPASCAL-Context) show that the proposed method is an effective approach andachieves state-of-the-art results. Code and models are available at", "output": "Multi-Task Learning with Multi-Query Transformer for Dense Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Two-branch network architecture has shown its efficiency and effectiveness inreal-time semantic segmentation tasks. However, direct fusion ofhigh-resolution details and low-frequency context has the drawback of detailedfeatures being easily overwhelmed by surrounding contextual information. Thisovershoot phenomenon limits the improvement of the segmentation accuracy ofexisting two-branch models. In this paper, we make a connection betweenConvolutional Neural Networks (CNN) and Proportional-Integral-Derivative (PID)controllers and reveal that a two-branch network is equivalent to aProportional-Integral (PI) controller, which inherently suffers from similarovershoot issues. To alleviate this problem, we propose a novel three-branchnetwork architecture: PIDNet, which contains three branches to parse detailed,context and boundary information, respectively, and employs boundary attentionto guide the fusion of detailed and context branches. Our family of PIDNetsachieve the best trade-off between inference speed and accuracy and theiraccuracy surpasses all the existing models with similar inference speed on theCityscapes and CamVid datasets. Specifically, PIDNet-S achieves 78.6% mIOU withinference speed of 93.2 FPS on Cityscapes and 80.1% mIOU with speed of 153.7FPS on CamVid.", "output": "PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We have seen a great progress in video action recognition in recent years.There are several models based on convolutional neural network (CNN) and somerecent transformer based approaches which provide top performance on existingbenchmarks. In this work, we perform a large-scale robustness analysis of theseexisting models for video action recognition. We focus on robustness againstreal-world distribution shift perturbations instead of adversarialperturbations. We propose four different benchmark datasets, HMDB51-P,UCF101-P, Kinetics400-P, and SSv2-P to perform this analysis. We studyrobustness of six state-of-the-art action recognition models against 90different perturbations. The study reveals some interesting findings, 1)transformer based models are consistently more robust compared to CNN basedmodels, 2) Pretraining improves robustness for Transformer based models morethan CNN based models, and 3) All of the studied models are robust to temporalperturbations for all datasets but SSv2; suggesting the importance of temporalinformation for action recognition varies based on the dataset and activities.Next, we study the role of augmentations in model robustness and present areal-world dataset, UCF101-DS, which contains realistic distribution shifts, tofurther validate some of these findings. We believe this study will serve as abenchmark for future research in robust video action recognition.", "output": "Large-scale Robustness Analysis of Video Action Recognition Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most anomaly detection (AD) models are learned using only normal samples inan unsupervised way, which may result in ambiguous decision boundary andinsufficient discriminability. In fact, a few anomaly samples are oftenavailable in real-world applications, the valuable knowledge of known anomaliesshould also be effectively exploited. However, utilizing a few known anomaliesduring training may cause another issue that the model may be biased by thoseknown anomalies and fail to generalize to unseen anomalies. In this paper, wetackle supervised anomaly detection, i.e., we learn AD models using a fewavailable anomalies with the objective to detect both the seen and unseenanomalies. We propose a novel explicit boundary guided semi-push-pullcontrastive learning mechanism, which can enhance model's discriminabilitywhile mitigating the bias issue. Our approach is based on two core designs:First, we find an explicit and compact separating boundary as the guidance forfurther feature learning. As the boundary only relies on the normal featuredistribution, the bias problem caused by a few known anomalies can bealleviated. Second, a boundary guided semi-push-pull loss is developed to onlypull the normal features together while pushing the abnormal features apartfrom the separating boundary beyond a certain margin region. In this way, ourmodel can form a more explicit and discriminative decision boundary todistinguish known and also unseen anomalies from normal samples moreeffectively. Code will be available at ", "output": "Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Structure from Action (SfA), a framework to discover 3D partgeometry and joint parameters of unseen articulated objects via a sequence ofinferred interactions. Our key insight is that 3D interaction and perceptionshould be considered in conjunction to construct 3D articulated CAD models,especially for categories not seen during training. By selecting informativeinteractions, SfA discovers parts and reveals occluded surfaces, like theinside of a closed drawer. By aggregating visual observations in 3D, SfAaccurately segments multiple parts, reconstructs part geometry, and infers alljoint parameters in a canonical coordinate frame. Our experiments demonstratethat a SfA model trained in simulation can generalize to many unseen objectcategories with diverse structures and to real-world objects. Empirically, SfAoutperforms a pipeline of state-of-the-art components by 25.4 3D IoU percentagepoints on unseen categories, while matching already performant joint estimationbaselines.", "output": "Structure from Action: Learning Interactions for Articulated Object 3D Structure Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated analysis of chest radiography using deep learning has tremendouspotential to enhance the clinical diagnosis of diseases in patients. However,deep learning models typically require large amounts of annotated data toachieve high performance -- often an obstacle to medical domain adaptation. Inthis paper, we build a data-efficient learning framework that utilizesradiology reports to improve medical image classification performance withlimited labeled data (fewer than 1000 examples). Specifically, we examineimage-captioning pretraining to learn high-quality medical imagerepresentations that train on fewer examples. Following joint pretraining of aconvolutional encoder and transformer decoder, we transfer the learned encoderto various classification tasks. Averaged over 9 pathologies, we find that ourmodel achieves higher classification performance than ImageNet-supervised andin-domain supervised pretraining when labeled training data is limited.", "output": "RadTex: Learning Efficient Radiograph Representations from Text Reports."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-resolution satellite images can provide abundant, detailed spatialinformation for land cover classification, which is particularly important forstudying the complicated built environment. However, due to the complex landcover patterns, the costly training sample collections, and the severedistribution shifts of satellite imageries, few studies have appliedhigh-resolution images to land cover mapping in detailed categories at largescale. To fill this gap, we present a large-scale land cover dataset,Five-Billion-Pixels. It contains more than 5 billion labeled pixels of 150high-resolution Gaofen-2 (4 m) satellite images, annotated in a 24-categorysystem covering artificial-constructed, agricultural, and natural classes. Inaddition, we propose a deep-learning-based unsupervised domain adaptationapproach that can transfer classification models trained on labeled dataset(referred to as the source domain) to unlabeled data (referred to as the targetdomain) for large-scale land cover mapping. Specifically, we introduce anend-to-end Siamese network employing dynamic pseudo-label assignment and classbalancing strategy to perform adaptive domain joint learning. To validate thegeneralizability of our dataset and the proposed approach across differentsensors and different geographical regions, we carry out land cover mapping onfive megacities in China and six cities in other five Asian countries severallyusing: PlanetScope (3 m), Gaofen-1 (8 m), and Sentinel-2 (10 m) satelliteimages. Over a total study area of 60,000 square kilometers, the experimentsshow promising results even though the input images are entirely unlabeled. Theproposed approach, trained with the Five-Billion-Pixels dataset, enableshigh-quality and detailed land cover mapping across the whole country of Chinaand some other Asian countries at meter-resolution.", "output": "Enabling Country-Scale Land Cover Mapping with Meter-Resolution Satellite Imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Different conditional video prediction tasks, like video future frameprediction and video frame interpolation, are normally solved by task-relatedmodels even though they share many common underlying characteristics.Furthermore, almost all conditional video prediction models can only achievediscrete prediction. In this paper, we propose a unified model that addressesthese two issues at the same time. We show that conditional video predictioncan be formulated as a neural process, which maps input spatio-temporalcoordinates to target pixel values given context spatio-temporal coordinatesand context pixel values. Specifically, we feed the implicit neuralrepresentation of coordinates and context pixel features into aTransformer-based non-autoregressive conditional video prediction model. Ourtask-specific models outperform previous work for video future frame predictionand video interpolation on multiple datasets. Importantly, the model is able tointerpolate or predict with an arbitrary high frame rate, i.e., continuousprediction. Our source code is available at url{", "output": "A unified model for continuous conditional video prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural radiance fields (NeRFs) have recently emerged as a promising approachfor 3D reconstruction and novel view synthesis. However, NeRF-based methodsencode shape, reflectance, and illumination implicitly and this makes itchallenging for users to manipulate these properties in the rendered imagesexplicitly. Existing approaches only enable limited editing of the scene anddeformation of the geometry. Furthermore, no existing work enables accuratescene illumination after object deformation. In this work, we introduce SPIDR,a new hybrid neural SDF representation. SPIDR combines point cloud and neuralimplicit representations to enable the reconstruction of higher quality objectsurfaces for geometry deformation and lighting estimation. meshes and surfacesfor object deformation and lighting estimation. To more accurately captureenvironment illumination for scene relighting, we propose a novel neuralimplicit model to learn environment light. To enable more accurate illuminationupdates after deformation, we use the shadow mapping technique to approximatethe light visibility updates caused by geometry editing. We demonstrate theeffectiveness of SPIDR in enabling high quality geometry editing with moreaccurate updates to the illumination of the scene.", "output": "SPIDR: SDF-based Neural Point Fields for Illumination and Deformation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In video action recognition, shortcut static features can interfere with thelearning of motion features, resulting in poor out-of-distribution (OOD)generalization. The video background is clearly a source of static bias, butthe video foreground, such as the clothing of the actor, can also providestatic bias. In this paper, we empirically verify the existence of foregroundstatic bias by creating test videos with conflicting signals from the staticand moving portions of the video. To tackle this issue, we propose a simple yeteffective technique, StillMix, to learn robust action representations.Specifically, StillMix identifies bias-inducing video frames using a 2Dreference network and mixes them with videos for training, serving as effectivebias suppression even when we cannot explicitly extract the source of biaswithin each video frame or enumerate types of bias. Finally, to preciselyevaluate static bias, we synthesize two new benchmarks, SCUBA for static cuesin the background, and SCUFO for static cues in the foreground. With extensiveexperiments, we demonstrate that StillMix mitigates both types of static biasand improves video representations for downstream applications.", "output": "Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Different video understanding tasks are typically treated in isolation, andeven with distinct types of curated data (e.g., classifying sports in onedataset, tracking animals in another). However, in wearable cameras, theimmersive egocentric perspective of a person engaging with the world aroundthem presents an interconnected web of video understanding tasks -- hand-objectmanipulations, navigation in the space, or human-human interactions -- thatunfold continuously, driven by the person's goals. We argue that this calls fora much more unified approach. We propose EgoTask Translation (EgoT2), whichtakes a collection of models optimized on separate tasks and learns totranslate their outputs for improved performance on any or all of them at once.Unlike traditional transfer or multi-task learning, EgoT2's flipped designentails separate task-specific backbones and a task translator shared acrossall tasks, which captures synergies between even heterogeneous tasks andmitigates task competition. Demonstrating our model on a wide array of videotasks from Ego4D, we show its advantages over existing transfer paradigms andachieve top-ranked results on four of the Ego4D 2022 benchmark challenges.", "output": "Egocentric Video Task Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D shapes have complementary abstractions from low-level geometry topart-based hierarchies to languages, which convey different levels ofinformation. This paper presents a unified framework to translate between pairsof shape abstractions: $textit{Text}$ $Longleftrightarrow$ $textit{PointCloud}$ $Longleftrightarrow$ $textit{Program}$. We propose $textbf{NeuralShape Compiler}$ to model the abstraction transformation as a conditionalgeneration process. It converts 3D shapes of three abstract types into unifieddiscrete shape code, transforms each shape code into code of other abstracttypes through the proposed $textit{ShapeCode Transformer}$, and decodes themto output the target shape abstraction. Point Cloud code is obtained in aclass-agnostic way by the proposed $textit{Point}$VQVAE. On Text2Shape,ShapeGlot, ABO, Genre, and Program Synthetic datasets, Neural Shape Compilershows strengths in $textit{Text}$ $Longrightarrow$ $textit{Point Cloud}$,$textit{Point Cloud}$ $Longrightarrow$ $textit{Text}$, $textit{PointCloud}$ $Longrightarrow$ $textit{Program}$, and Point Cloud Completion tasks.Additionally, Neural Shape Compiler benefits from jointly training on allheterogeneous data and tasks.", "output": "Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present MovingParts, a NeRF-based method for dynamic scene reconstructionand part discovery. We consider motion as an important cue for identifyingparts, that all particles on the same part share the common motion pattern.From the perspective of fluid simulation, existing deformation-based methodsfor dynamic NeRF can be seen as parameterizing the scene motion under theEulerian view, i.e., focusing on specific locations in space through which thefluid flows as time passes. However, it is intractable to extract the motion ofconstituting objects or parts using the Eulerian view representation. In thiswork, we introduce the dual Lagrangian view and enforce representations underthe Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangianview, we parameterize the scene motion by tracking the trajectory of particleson objects. The Lagrangian view makes it convenient to discover parts byfactorizing the scene motion as a composition of part-level rigid motions.Experimentally, our method can achieve fast and high-quality dynamic scenereconstruction from even a single moving camera, and the induced part-basedrepresentation allows direct applications of part tracking, animation, 3D sceneediting, etc.", "output": "MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video summarization aims to distill the most important information from asource video to produce either an abridged clip or a textual narrative.Traditionally, different methods have been proposed depending on whether theoutput is a video or text, thus ignoring the correlation between the twosemantically related tasks of visual summarization and textual summarization.We propose a new joint video and text summarization task. The goal is togenerate both a shortened video clip along with the corresponding textualsummary from a long video, collectively referred to as a cross-modal summary.The generated shortened video clip and text narratives should be semanticallywell aligned. To this end, we first build a large-scale human-annotated dataset-- VideoXum (X refers to different modalities). The dataset is reannotatedbased on ActivityNet. After we filter out the videos that do not meet thelength requirements, 14,001 long videos remain in our new dataset. Each videoin our reannotated dataset has human-annotated video summaries and thecorresponding narrative summaries. We then design a novel end-to-end model --VTSUM-BILP to address the challenges of our proposed task. Moreover, we proposea new metric called VT-CLIPScore to help evaluate the semantic consistency ofcross-modality summary. The proposed model achieves promising performance onthis new task and establishes a benchmark for future research.", "output": "VideoXum: Cross-modal Visual and Textural Summarization of Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stable Diffusion model has been extensively employed in the study ofarchi-tectural image generation, but there is still an opportunity to enhancein terms of the controllability of the generated image content. A multi-networkcombined text-to-building facade image generating method is proposed in thiswork. We first fine-tuned the Stable Diffusion model on the CMP Fa-cadesdataset using the LoRA (Low-Rank Adaptation) approach, then we ap-ply theControlNet model to further control the output. Finally, we contrast-ed thefacade generating outcomes under various architectural style text con-tents andcontrol strategies. The results demonstrate that the LoRA training approachsignificantly decreases the possibility of fine-tuning the Stable Dif-fusionlarge model, and the addition of the ControlNet model increases thecontrollability of the creation of text to building facade images. Thispro-vides a foundation for subsequent studies on the generation ofarchitectural images.", "output": "Text Semantics to Image Generation: A method of building facades design base on Stable Diffusion model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D object detection from visual sensors is a cornerstone capability ofrobotic systems. State-of-the-art methods focus on reasoning and decodingobject bounding boxes from multi-view camera input. In this work we gainintuition from the integral role of multi-view consistency in 3D sceneunderstanding and geometric learning. To this end, we introduce VEDet, a novel3D object detection framework that exploits 3D multi-view geometry to improvelocalization through viewpoint awareness and equivariance. VEDet leverages aquery-based transformer architecture and encodes the 3D scene by augmentingimage features with positional encodings from their 3D perspective geometry. Wedesign view-conditioned queries at the output level, which enables thegeneration of multiple virtual frames during training to learn viewpointequivariance by enforcing multi-view consistency. The multi-view geometryinjected at the input level as positional encodings and regularized at the losslevel provides rich geometric cues for 3D object detection, leading tostate-of-the-art performance on the nuScenes benchmark. The code and model aremade available at ", "output": "Viewpoint Equivariance for Multi-View 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study masked autoencoder (MAE) pretraining on videos formatching-based downstream tasks, including visual object tracking (VOT) andvideo object segmentation (VOS). A simple extension of MAE is to randomly maskout frame patches in videos and reconstruct the frame pixels. However, we findthat this simple baseline heavily relies on spatial cues while ignoringtemporal relations for frame reconstruction, thus leading to sub-optimaltemporal matching representations for VOT and VOS. To alleviate this problem,we propose DropMAE, which adaptively performs spatial-attention dropout in theframe reconstruction to facilitate temporal correspondence learning in videos.We show that our DropMAE is a strong and efficient temporal matching learner,which achieves better finetuning results on matching-based tasks than theImageNetbased MAE with 2X faster pre-training speed. Moreover, we also findthat motion diversity in pre-training videos is more important than scenediversity for improving the performance on VOT and VOS. Our pre-trained DropMAEmodel can be directly loaded in existing ViT-based trackers for fine-tuningwithout further modifications. Notably, DropMAE sets new state-of-the-artperformance on 8 out of 9 highly competitive video tracking and segmentationdatasets. Our code and pre-trained models are available at", "output": "DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of dynamic scene graph generation (SGG) from videos is complicatedand challenging due to the inherent dynamics of a scene, temporal fluctuationof model predictions, and the long-tailed distribution of the visualrelationships in addition to the already existing challenges in image-basedSGG. Existing methods for dynamic SGG have primarily focused on capturingspatio-temporal context using complex architectures without addressing thechallenges mentioned above, especially the long-tailed distribution ofrelationships. This often leads to the generation of biased scene graphs. Toaddress these challenges, we introduce a new framework called TEMPURA: TEmporalconsistency and Memory Prototype guided UnceRtainty Attenuation for unbiaseddynamic SGG. TEMPURA employs object-level temporal consistencies viatransformer-based sequence modeling, learns to synthesize unbiased relationshiprepresentations using memory-guided training, and attenuates the predictiveuncertainty of visual relations using a Gaussian Mixture Model (GMM). Extensiveexperiments demonstrate that our method achieves significant (up to 10% in somecases) performance gain over existing methods highlighting its superiority ingenerating more unbiased scene graphs.", "output": "Unbiased Scene Graph Generation in Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in deep learning have been pushing image denoising techniquesto a new level. In self-supervised image denoising, blind-spot network (BSN) isone of the most common methods. However, most of the existing BSN algorithmsuse a dot-based central mask, which is recognized as inefficient for imageswith large-scale spatially correlated noise. In this paper, we give thedefinition of large-noise and propose a multi-mask strategy using multipleconvolutional kernels masked in different shapes to further break the noisespatial correlation. Furthermore, we propose a novel self-supervised imagedenoising method that combines the multi-mask strategy with BSN (MM-BSN). Weshow that different masks can cause significant performance differences, andthe proposed MM-BSN can efficiently fuse the features extracted by multi-maskedlayers, while recovering the texture structures destroyed by multi-masking andinformation transmission. Our MM-BSN can be used to address the problem oflarge-noise denoising, which cannot be efficiently handled by other BSNmethods. Extensive experiments on public real-world datasets demonstrate thatthe proposed MM-BSN achieves state-of-the-art performance among self-supervisedand even unpaired image denoising methods for sRGB images denoising, withoutany labelling effort or prior knowledge. Code can be found in", "output": "MM-BSN: Self-Supervised Image Denoising for Real-World with Multi-Mask based on Blind-Spot Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Equivariance w.r.t. geometric transformations in neural networks improvesdata efficiency, parameter efficiency and robustness to out-of-domainperspective shifts. When equivariance is not designed into a neural network,the network can still learn equivariant functions from the data. We quantifythis learned equivariance, by proposing an improved measure for equivariance.We find evidence for a correlation between learned translation equivariance andvalidation accuracy on ImageNet. We therefore investigate what can increase thelearned equivariance in neural networks, and find that data augmentation,reduced model capacity and inductive bias in the form of convolutions inducehigher learned equivariance in neural networks.", "output": "What Affects Learned Equivariance in Deep Image Recognition Models?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Embodied Navigation Trajectory Learner (ENTL), a method forextracting long sequence representations for embodied navigation. Our approachunifies world modeling, localization and imitation learning into a singlesequence prediction task. We train our model using vector-quantized predictionsof future states conditioned on current states and actions. ENTL's genericarchitecture enables sharing of the spatio-temporal sequence encoder formultiple challenging embodied tasks. We achieve competitive performance onnavigation tasks using significantly less data than strong baselines whileperforming auxiliary tasks such as localization and future frame prediction (aproxy for world modeling). A key property of our approach is that the model ispre-trained without any explicit reward signal, which makes the resulting modelgeneralizable to multiple tasks and environments.", "output": "ENTL: Embodied Navigation Trajectory Learner."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical data often exhibits long-tail distributions with heavy classimbalance, which naturally leads to difficulty in classifying the minorityclasses (i.e., boundary regions or rare objects). Recent work has significantlyimproved semi-supervised medical image segmentation in long-tailed scenarios byequipping them with unsupervised contrastive criteria. However, it remainsunclear how well they will perform in the labeled portion of data where classdistribution is also highly imbalanced. In this work, we present ACTION++, animproved contrastive learning framework with adaptive anatomical contrast forsemi-supervised medical segmentation. Specifically, we propose an adaptivesupervised contrastive loss, where we first compute the optimal locations ofclass centers uniformly distributed on the embedding space (i.e., off-line),and then perform online contrastive matching training by encouraging differentclass features to adaptively match these distinct and uniformly distributedclass centers. Moreover, we argue that blindly adopting a constant temperature$tau$ in the contrastive loss on long-tailed medical data is not optimal, andpropose to use a dynamic $tau$ via a simple cosine schedule to yield betterseparation between majority and minority classes. Empirically, we evaluateACTION++ on ACDC and LA benchmarks and show that it achieves state-of-the-artacross two semi-supervised settings. Theoretically, we analyze the performanceof adaptive anatomical contrast and confirm its superiority in labelefficiency.", "output": "ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hate speech is a societal problem that has significantly grown through theInternet. New forms of digital content such as image memes have given rise tospread of hate using multimodal means, being far more difficult to analyse anddetect compared to the unimodal case. Accurate automatic processing, analysisand understanding of this kind of content will facilitate the endeavor ofhindering hate speech proliferation through the digital world. To this end, wepropose MemeFier, a deep learning-based architecture for fine-grainedclassification of Internet image memes, utilizing a dual-stage modality fusionmodule. The first fusion stage produces feature vectors containing modalityalignment information that captures non-trivial connections between the textand image of a meme. The second fusion stage leverages the power of aTransformer encoder to learn inter-modality correlations at the token level andyield an informative representation. Additionally, we consider externalknowledge as an additional input, and background image caption supervision as aregularizing component. Extensive experiments on three widely adoptedbenchmarks, i.e., Facebook Hateful Memes, Memotion7k and MultiOFF, indicatethat our approach competes and in some cases surpasses state-of-the-art. Ourcode is available on ", "output": "MemeFier: Dual-stage Modality Fusion for Image Meme Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-language navigation is a task that requires an agent to followinstructions to navigate in environments. It becomes increasingly crucial inthe field of embodied AI, with potential applications in autonomous navigation,search and rescue, and human-robot interaction. In this paper, we propose toaddress a more practical yet challenging counterpart setting - vision-languagenavigation in continuous environments (VLN-CE). To develop a robust VLN-CEagent, we propose a new navigation framework, ETPNav, which focuses on twocritical skills: 1) the capability to abstract environments and generatelong-range navigation plans, and 2) the ability of obstacle-avoiding control incontinuous environments. ETPNav performs online topological mapping ofenvironments by self-organizing predicted waypoints along a traversed path,without prior environmental experience. It privileges the agent to break downthe navigation procedure into high-level planning and low-level control.Concurrently, ETPNav utilizes a transformer-based cross-modal planner togenerate navigation plans based on topological maps and instructions. The planis then performed through an obstacle-avoiding controller that leverages atrial-and-error heuristic to prevent navigation from getting stuck inobstacles. Experimental results demonstrate the effectiveness of the proposedmethod. ETPNav yields more than 10% and 20% improvements over priorstate-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code isavailable at ", "output": "ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-camera 3D object detection for autonomous driving is a challengingproblem that has garnered notable attention from both academia and industry. Anobstacle encountered in vision-based techniques involves the precise extractionof geometry-conscious features from RGB images. Recent approaches have utilizedgeometric-aware image backbones pretrained on depth-relevant tasks to acquirespatial information. However, these approaches overlook the critical aspect ofview transformation, resulting in inadequate performance due to themisalignment of spatial knowledge between the image backbone and viewtransformation. To address this issue, we propose a novel geometric-awarepretraining framework called GAPretrain. Our approach incorporates spatial andstructural cues to camera networks by employing the geometric-rich modality asguidance during the pretraining phase. The transference of modal-specificattributes across different modalities is non-trivial, but we bridge this gapby using a unified bird's-eye-view (BEV) representation and structural hintsderived from LiDAR point clouds to facilitate the pretraining process.GAPretrain serves as a plug-and-play solution that can be flexibly applied tomultiple state-of-the-art detectors. Our experiments demonstrate theeffectiveness and generalization ability of the proposed method. We achieve46.2 mAP and 55.5 NDS on the nuScenes val set using the BEVFormer method, witha gain of 2.7 and 2.1 points, respectively. We also conduct experiments onvarious image backbones and view transformations to validate the efficacy ofour approach. Code will be released at", "output": "Geometric-aware Pretraining for Vision-centric 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several decision problems that are encountered in various business domainscan be modeled as mathematical programs, i.e. optimization problems. Theprocess of conducting such modeling often requires the involvement of expertstrained in operations research and advanced algorithms. Surprisingly, despitethe significant advances in the methods for program and code synthesis, AutoML,learning to optimize etc., there has been little or no attention paid toautomating the task of synthesizing mathematical programs. We imagine ascenario where the specifications for modeling, i.e. the objective andconstraints are expressed in an unstructured form in natural language (NL) andthe mathematical program has to be synthesized from such an NL specification.In this work we evaluate the efficacy of employing CodeT5 with dataaugmentation and post-processing of beams. We utilize GPT-3 with backtranslation for generation of synthetic examples. Further we apply rules oflinear programming to score beams and correct beams based on common errorpatterns. We observe that with these enhancements CodeT5 base gives anexecution accuracy of 0.73 which is significantly better than zero-shotexecution accuracy of 0.41 by ChatGPT and 0.36 by Codex.", "output": "Synthesis of Mathematical programs from Natural Language Specifications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The past decade has witnessed rapid progress in AI research since thebreakthrough in deep learning. AI technology has been applied in almost everyfield; therefore, technical and non-technical end-users must understand thesetechnologies to exploit them. However existing materials are designed forexperts, but non-technical users need appealing materials that deliver complexideas in easy-to-follow steps. One notable tool that fits such a profile isscrollytelling, an approach to storytelling that provides readers with anatural and rich experience at the reader's pace, along with in-depthinteractive explanations of complex concepts. Hence, this work proposes a novelvisualization design for creating a scrollytelling that can effectively explainan AI concept to non-technical users. As a demonstration of our design, wecreated a scrollytelling to explain the Siamese Neural Network for the visualsimilarity matching problem. Our approach helps create a visualization valuablefor a short-timeline situation like a sales pitch. The results show that thevisualization based on our novel design helps improve non-technical users'perception and machine learning concept knowledge acquisition compared totraditional materials like online articles.", "output": "VISHIEN-MAAT: Scrollytelling visualization design for explaining Siamese Neural Network concept to non-technical users."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, reinforcement learning (RL) has emerged as a popularapproach for solving sequence-based tasks in machine learning. However, findingsuitable alternatives to RL remains an exciting and innovative research area.One such alternative that has garnered attention is the Non-Axiomatic ReasoningSystem (NARS), which is a general-purpose cognitive reasoning framework. Inthis paper, we delve into the potential of NARS as a substitute for RL insolving sequence-based tasks. To investigate this, we conduct a comparativeanalysis of the performance of ONA as an implementation of NARS and$Q$-Learning in various environments that were created using the Open AI gym.The environments have different difficulty levels, ranging from simple tocomplex. Our results demonstrate that NARS is a promising alternative to RL,with competitive performance in diverse environments, particularly innon-deterministic ones.", "output": "Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Shapelets that discriminate time series using local features (subsequences)are promising for time series clustering. Existing time series clusteringmethods may fail to capture representative shapelets because they discovershapelets from a large pool of uninformative subsequences, and thus result inlow clustering accuracy. This paper proposes a Semi-supervised Clustering ofTime Series Using Representative Shapelets (SS-Shapelets) method, whichutilizes a small number of labeled and propagated pseudo-labeled time series tohelp discover representative shapelets, thereby improving the clusteringaccuracy. In SS-Shapelets, we propose two techniques to discover representativeshapelets for the effective clustering of time series. 1) A textit{salientsubsequence chain} ($SSC$) that can extract salient subsequences (as candidateshapelets) of a labeled/pseudo-labeled time series, which helps remove massiveuninformative subsequences from the pool. 2) A textit{linear discriminantselection} ($LDS$) algorithm to identify shapelets that can capturerepresentative local features of time series in different classes, forconvenient clustering. Experiments on UCR time series datasets demonstrate thatSS-shapelets discovers representative shapelets and achieves higher clusteringaccuracy than counterpart semi-supervised time series clustering methods.", "output": "SS-shapelets: Semi-supervised Clustering of Time Series Using Representative Shapelets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural operator learning as a means of mapping between complex functionspaces has garnered significant attention in the field of computational scienceand engineering (CS&amp;E). In this paper, we apply Neural operator learning to thetime-of-flight ultrasound computed tomography (USCT) problem. We learn themapping between time-of-flight (TOF) data and the heterogeneous sound speedfield using a full-wave solver to generate the training data. This novelapplication of operator learning circumnavigates the need to solve thecomputationally intensive iterative inverse problem. The operator learns thenon-linear mapping offline and predicts the heterogeneous sound field with asingle forward pass through the model. This is the first time operator learninghas been used for ultrasound tomography and is the first step in potentialreal-time predictions of soft tissue distribution for tumor identification inbeast imaging.", "output": "Neural Operator Learning for Ultrasound Tomography Inversion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider adaptive decision-making problems where an agent optimizes acumulative performance objective by repeatedly choosing among a finite set ofoptions. Compared to the classical prediction-with-expert-advice set-up, weconsider situations where losses are constrained and derive algorithms thatexploit the additional structure in optimal and computationally efficient ways.Our algorithm and our analysis is instance dependent, that is, suboptimalchoices of the environment are exploited and reflected in our regret bounds.The constraints handle general dependencies between losses (even across time),and are flexible enough to also account for a loss budget, which theenvironment is not allowed to exceed. The performance of the resultingalgorithms is highlighted in two numerical examples, which include a nonlinearand online system identification task.", "output": "Adaptive Decision-Making with Constraints and Dependent Losses: Performance Guarantees and Applications to Online and Nonlinear Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image inpainting refers to the task of generating a complete, natural imagebased on a partially revealed reference image. Recently, many researchinterests have been focused on addressing this problem using fixed diffusionmodels. These approaches typically directly replace the revealed region of theintermediate or final generated images with that of the reference image or itsvariants. However, since the unrevealed regions are not directly modified tomatch the context, it results in incoherence between revealed and unrevealedregions. To address the incoherence problem, a small number of methodsintroduce a rigorous Bayesian framework, but they tend to introduce mismatchesbetween the generated and the reference images due to the approximation errorsin computing the posterior distributions. In this paper, we propose COPAINT,which can coherently inpaint the whole image without introducing mismatches.COPAINT also uses the Bayesian framework to jointly modify both revealed andunrevealed regions, but approximates the posterior distribution in a way thatallows the errors to gradually drop to zero throughout the denoising steps,thus strongly penalizing any mismatches with the reference image. Ourexperiments verify that COPAINT can outperform the existing diffusion-basedmethods under both objective and subjective metrics. The codes are available at", "output": "Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models have gained considerable interest for their impressiveperformance on various tasks. Among these models, ChatGPT developed by OpenAIhas become extremely popular among early adopters who even regard it as adisruptive technology in many fields like customer service, education,healthcare, and finance. It is essential to comprehend the opinions of theseinitial users as it can provide valuable insights into the potential strengths,weaknesses, and success or failure of the technology in different areas. Thisresearch examines the responses generated by ChatGPT from differentConversational QA corpora. The study employed BERT similarity scores to comparethese responses with correct answers and obtain Natural Language Inference(NLI)labels. Evaluation scores were also computed and compared to determine theoverall performance of GPT-3 &amp; GPT-4. Additionally, the study identifiedinstances where ChatGPT provided incorrect answers to questions, providinginsights into areas where the model may be prone to error.", "output": "ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking about."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multilabel ranking is a central task in machine learning with widespreadapplications to web search, news stories, recommender systems, etc. However,the most fundamental question of learnability in a multilabel ranking settingremains unanswered. In this paper, we characterize the learnability ofmultilabel ranking problems in both the batch and online settings for a largefamily of ranking losses. Along the way, we also give the first equivalenceclass of ranking losses based on learnability.", "output": "On the Learnability of Multilabel Ranking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a formal context, an ordinal factor is a subset of its incidencerelation that forms a chain in the concept lattice, i.e., a part of the datasetthat corresponds to a linear order. To visualize the data in a formal context,Ganter and Glodeanu proposed a biplot based on two ordinal factors. For thebiplot to be useful, it is important that these factors comprise as much datapoints as possible, i.e., that they cover a large part of the incidencerelation. In this work, we investigate such ordinal two-factorizations. First,we investigate for formal contexts that omit ordinal two-factorizations thedisjointness of the two factors. Then, we show that deciding on the existenceof two-factorizations of a given size is an NP-complete problem which makescomputing maximal factorizations computationally expensive. Finally, we providethe algorithm Ord2Factor that allows us to compute large ordinaltwo-factorizations.", "output": "Maximal Ordinal Two-Factorizations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we have shown autonomous long-term prediction with aspintronic physical reservoir. Due to the short-term memory property of themagnetization dynamics, non-linearity arises in the reservoir states whichcould be used for long-term prediction tasks using simple linear regression foronline training. During the prediction stage, the output is directly fed to theinput of the reservoir for autonomous prediction. We employ our proposedreservoir for the modeling of the chaotic time series such as Mackey-Glass anddynamic time-series data, such as household building energy loads. Since onlythe last layer of a RC needs to be trained with linear regression, it is wellsuited for learning in real time on edge devices. Here we show that a skyrmionbased magnetic tunnel junction can potentially be used as a prototypical RC butany nanomagnetic magnetic tunnel junction with nonlinear magnetization behaviorcan implement such a RC. By comparing our spintronic physical RC approach withstate-of-the-art energy load forecasting algorithms, such as LSTMs and RNNs, weconclude that the proposed framework presents good performance in achievinghigh predictions accuracy, while also requiring low memory and energy both ofwhich are at a premium in hardware resource and power constrained edgeapplications. Further, the proposed approach is shown to require very smalltraining datasets and at the same time being at least 16X energy efficientcompared to the state-of-the-art sequence to sequence LSTM for accuratehousehold load predictions.", "output": "Spintronic Physical Reservoir for Autonomous Prediction and Long-Term Household Energy Load Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph collaborative filtering (GCF) is a popular technique for capturinghigh-order collaborative signals in recommendation systems. However, GCF'sbipartite adjacency matrix, which defines the neighbors being aggregated basedon user-item interactions, can be noisy for users/items with abundantinteractions and insufficient for users/items with scarce interactions.Additionally, the adjacency matrix ignores user-user and item-itemcorrelations, which can limit the scope of beneficial neighbors beingaggregated.In this work, we propose a new graph adjacency matrix that incorporatesuser-user and item-item correlations, as well as a properly designed user-iteminteraction matrix that balances the number of interactions across all users.To achieve this, we pre-train a graph-based recommendation method to obtainusers/items embeddings, and then enhance the user-item interaction matrix viatop-K sampling. We also augment the symmetric user-user and item-itemcorrelation components to the adjacency matrix. Our experiments demonstratethat the enhanced user-item interaction matrix with improved neighbors andlower density leads to significant benefits in graph-based recommendation.Moreover, we show that the inclusion of user-user and item-item correlationscan improve recommendations for users with both abundant and insufficientinteractions. The code is in url{", "output": "Graph Collaborative Signals Denoising and Augmentation for Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prediction of chemical shift in NMR using machine learning methods istypically done with the maximum amount of data available to achieve the bestresults. In some cases, such large amounts of data are not available, e.g. forheteronuclei. We demonstrate a novel machine learning model which is able toachieve good results with comparatively low amounts of data. We show this bypredicting 19F and 13C NMR chemical shifts of small molecules in specificsolvents.", "output": "NMR shift prediction from small data quantities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Decision-focused (DF) model-based reinforcement learning has recently beenintroduced as a powerful algorithm which can focus on learning the MDP dynamicswhich are most relevant for obtaining high rewards. While this approachincreases the performance of agents by focusing the learning towards optimizingfor the reward directly, it does so by learning less accurate dynamics (from aMLE standpoint), and may thus be brittle to changes in the reward function. Inthis work, we develop the robust decision-focused (RDF) algorithm whichleverages the non-identifiability of DF solutions to learn models whichmaximize expected returns while simultaneously learning models which are robustto changes in the reward function. We demonstrate on a variety of toy exampleand healthcare simulators that RDF significantly increases the robustness of DFto changes in the reward function, without decreasing the overall return theagent obtains.", "output": "Robust Decision-Focused Learning for Reward Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomalies are often indicators of malfunction or inefficiency in varioussystems such as manufacturing, healthcare, finance, surveillance, to name afew. While the literature is abundant in effective detection algorithms due tothis practical relevance, autonomous anomaly detection is rarely used inreal-world scenarios. Especially in high-stakes applications, ahuman-in-the-loop is often involved in processes beyond detection such asverification and troubleshooting. In this work, we introduce ALARM (forAnalyst-in-the-Loop Anomaly Reasoning and Management); an end-to-end frameworkthat supports the anomaly mining cycle comprehensively, from detection toaction. Besides unsupervised detection of emerging anomalies, it offers anomalyexplanations and an interactive GUI for human-in-the-loop processes -- visualexploration, sense-making, and ultimately action-taking via designing newdetection rules -- that help close ``the loop'' as the new rules complementrule-based supervised detection, typical of many deployed systems in practice.We demonstrate method's efficacy through a series of case studies with fraudanalysts from the financial industry.", "output": "From Explanation to Action: An End-to-End Human-in-the-loop Framework for Anomaly Reasoning and Management."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning algorithms are often used in environments which are notcaptured accurately even by the most carefully obtained training data, eitherdue to the possibility of `adversarial' test-time attacks, or on account of`natural' distribution shift. For test-time attacks, we introduce and analyze anovel robust reliability guarantee, which requires a learner to outputpredictions along with a reliability radius $eta$, with the meaning that itsprediction is guaranteed to be correct as long as the adversary has notperturbed the test point farther than a distance $eta$. We provide learnersthat are optimal in the sense that they always output the best possiblereliability radius on any test point, and we characterize the reliable region,i.e. the set of points where a given reliability radius is attainable. Weadditionally analyze reliable learners under distribution shift, where the testpoints may come from an arbitrary distribution Q different from the trainingdistribution P. For both cases, we bound the probability mass of the reliableregion for several interesting examples, for linear separators under nearlylog-concave and s-concave distributions, as well as for smooth boundaryclassifiers under smooth probability distributions.", "output": "Reliable Learning for Test-time Attacks and Distribution Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated machine learning (AutoML) methods improve upon existing models byoptimizing various aspects of their design. While present methods focus onhyperparameters and neural network topologies, other aspects of neural networkdesign can be optimized as well. To further the state of the art in AutoML,this dissertation introduces techniques for discovering more powerfulactivation functions and establishing more robust weight initialization forneural networks. These contributions improve performance, but also provide newperspectives on neural network optimization. First, the dissertationdemonstrates that discovering solutions specialized to specific architecturesand tasks gives better performance than reusing general approaches. Second, itshows that jointly optimizing different components of neural networks issynergistic, and results in better performance than optimizing individualcomponents alone. Third, it demonstrates that learned representations areeasier to optimize than hard-coded ones, creating further opportunities forAutoML. The dissertation thus makes concrete progress towards fully automaticmachine learning in the future.", "output": "Optimizing Neural Networks through Activation Function Discovery and Automatic Weight Initialization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The dynamics of neuron populations during diverse tasks often evolve onlow-dimensional manifolds. However, it remains challenging to discern thecontributions of geometry and dynamics for encoding relevant behaviouralvariables. Here, we introduce an unsupervised geometric deep learning frameworkfor representing non-linear dynamical systems based on statisticaldistributions of local phase portrait features. Our method provides robustgeometry-aware or geometry-agnostic representations for the unbiased comparisonof dynamics based on measured trajectories. We demonstrate that our statisticalrepresentation can generalise across neural network instances to discriminatecomputational mechanisms, obtain interpretable embeddings of neural dynamics ina primate reaching task with geometric correspondence to hand kinematics, anddevelop a decoding algorithm with state-of-the-art accuracy. Our resultshighlight the importance of using the intrinsic manifold structure overtemporal information to develop better decoding algorithms and assimilate dataacross experiments.", "output": "Interpretable statistical representations of neural population dynamics and geometry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce S$^2$VS, a video similarity learning approach withself-supervision. Self-Supervised Learning (SSL) is typically used to traindeep models on a proxy task so as to have strong transferability on targettasks after fine-tuning. Here, in contrast to prior work, SSL is used toperform video similarity learning and address multiple retrieval and detectiontasks at once with no use of labeled data. This is achieved by learning viainstance-discrimination with task-tailored augmentations and the widely usedInfoNCE loss together with an additional loss operating jointly onself-similarity and hard-negative similarity. We benchmark our method on taskswhere video relevance is defined with varying granularity, ranging from videocopies to videos depicting the same incident or event. We learn a singleuniversal model that achieves state-of-the-art performance on all tasks,surpassing previously proposed methods that use labeled data. The code andpretrained models are publicly available at:url{", "output": "Self-Supervised Video Similarity Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper demonstrates how to discover the whole causal graph from thesecond derivative of the log-likelihood in observational non-linear additiveGaussian noise models. Leveraging scalable machine learning approaches toapproximate the score function $nabla log p(mathbf{X})$, we extend the workof Rolland et al. (2022) that only recovers the topological order from thescore and requires an expensive pruning step removing spurious edges amongthose admitted by the ordering. Our analysis leads to DAS (acronym forDiscovery At Scale), a practical algorithm that reduces the complexity of thepruning by a factor proportional to the graph size. In practice, DAS achievescompetitive accuracy with current state-of-the-art while being over an order ofmagnitude faster. Overall, our approach enables principled and scalable causaldiscovery, significantly lowering the compute bar.", "output": "Scalable Causal Discovery with Score Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent developments in applications of artificial neural networks with over$n=10^{14}$ parameters make it extremely important to study the large $n$behaviour of such networks. Most works studying wide neural networks havefocused on the infinite width $n to +infty$ limit of such networks and haveshown that, at initialization, they correspond to Gaussian processes. In thiswork we will study their behavior for large, but finite $n$. Our maincontributions are the following:(1) The computation of the corrections to Gaussianity in terms of anasymptotic series in $n^{-frac{1}{2}}$. The coefficients in this expansion aredetermined by the statistics of parameter initialization and by the activationfunction.(2) Controlling the evolution of the outputs of finite width $n$ networks,during training, by computing deviations from the limiting infinite width case(in which the network evolves through a linear flow). This improves previousestimates and yields sharper decay rates for the (finite width) NTK in terms of$n$, valid during the entire training procedure. As a corollary, we also provethat, with arbitrarily high probability, the training of sufficiently wideneural networks converges to a global minimum of the corresponding quadraticloss function.(3) Estimating how the deviations from Gaussianity evolve with training interms of $n$. In particular, using a certain metric in the space of measures wefind that, along training, the resulting measure is within$n^{-frac{1}{2}}(log n)^{1+}$ of the time dependent Gaussian processcorresponding to the infinite width network (which is explicitly given byprecomposing the initial Gaussian process with the linear flow corresponding totraining in the infinite width limit).", "output": "Wide neural networks: From non-gaussian random fields at initialization to the NTK geometry of training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Neural Networks (DNNs) have become ubiquitous due to their performanceon prediction and classification problems. However, they face a variety ofthreats as their usage spreads. Model extraction attacks, which steal DNNs,endanger intellectual property, data privacy, and security. Previous researchhas shown that system-level side-channels can be used to leak the architectureof a victim DNN, exacerbating these risks. We propose two DNN architectureextraction techniques catering to various threat models. The first techniqueuses a malicious, dynamically linked version of PyTorch to expose a victim DNNarchitecture through the PyTorch profiler. The second, called EZClone, exploitsaggregate (rather than time-series) GPU profiles as a side-channel to predictDNN architecture, employing a simple approach and assuming little adversarycapability as compared to previous work. We investigate the effectiveness ofEZClone when minimizing the complexity of the attack, when applied to prunedmodels, and when applied across GPUs. We find that EZClone correctly predictsDNN architectures for the entire set of PyTorch vision architectures with 100%accuracy. No other work has shown this degree of architecture predictionaccuracy with the same adversarial constraints or using aggregate side-channelinformation. Prior work has shown that, once a DNN has been successfullycloned, further attacks such as model evasion or model inversion can beaccelerated significantly.", "output": "EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We are developing a virtual coaching system that helps patients adhere tobehavior change interventions (BCI). Our proposed system predicts whether apatient will perform the targeted behavior and uses counterfactual exampleswith feature control to guide personalizsation of BCI. We evaluated ourprediction model using simulated patient data with varying levels ofreceptivity to intervention.", "output": "Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Student opinions for a course are important to educators and administrators,regardless of the type of the course or the institution. Reading and manuallyanalyzing open-ended feedback becomes infeasible for massive volumes ofcomments at institution level or online forums. In this paper, we collected andpre-processed a large number of course reviews publicly available online. Weapplied machine learning techniques with the goal to gain insight into studentsentiments and topics. Specifically, we utilized current Natural LanguageProcessing (NLP) techniques, such as word embeddings and deep neural networks,and state-of-the-art BERT (Bidirectional Encoder Representations fromTransformers), RoBERTa (Robustly optimized BERT approach) and XLNet(Generalized Auto-regression Pre-training). We performed extensiveexperimentation to compare these techniques versus traditional approaches. Thiscomparative study demonstrates how to apply modern machine learning approachesfor sentiment polarity extraction and topic-based classification utilizingcourse feedback. For sentiment polarity, the top model was RoBERTa with 95.5%accuracy and 84.7% F1-macro, while for topic classification, an SVM (SupportVector Machine) was the top classifier with 79.8% accuracy and 80.6%F1-macro. We also provided an in-depth exploration of the effect of certainhyperparameters on the model performance and discussed our observations. Thesefindings can be used by institutions and course providers as a guide foranalyzing their own course feedback using NLP models towards self-evaluationand improvement.", "output": "Deep Learning for Opinion Mining and Topic Classification of Course Reviews."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum machine learning is a promising programming paradigm for theoptimization of quantum algorithms in the current era of noisy intermediatescale quantum (NISQ) computers. A fundamental challenge in quantum machinelearning is generalization, as the designer targets performance under testingconditions, while having access only to limited training data. Existinggeneralization analyses, while identifying important general trends and scalinglaws, cannot be used to assign reliable and informative \"error bars\" to thedecisions made by quantum models. In this article, we propose a generalmethodology that can reliably quantify the uncertainty of quantum models,irrespective of the amount of training data, of the number of shots, of theansatz, of the training algorithm, and of the presence of quantum hardwarenoise. The approach, which builds on probabilistic conformal prediction, turnsan arbitrary, possibly small, number of shots from a pre-trained quantum modelinto a set prediction, e.g., an interval, that provably contains the truetarget with any desired coverage level. Experimental results confirm thetheoretical calibration guarantees of the proposed framework, referred to asquantum conformal prediction.", "output": "Quantum Conformal Prediction for Reliable Uncertainty Quantification in Quantum Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in self-supervised learning have demonstrated thateffective visual representations can be learned from unlabeled images. This hasled to increased interest in applying self-supervised learning to the medicaldomain, where unlabeled images are abundant and labeled images are difficult toobtain. However, most self-supervised learning approaches are modeled as imagelevel discriminative or generative proxy tasks, which may not capture the finerlevel representations necessary for dense prediction tasks like multi-organsegmentation. In this paper, we propose a novel contrastive learning frameworkthat integrates Localized Region Contrast (LRC) to enhance existingself-supervised pre-training methods for medical image segmentation. Ourapproach involves identifying Super-pixels by Felzenszwalb's algorithm andperforming local contrastive learning using a novel contrastive sampling loss.Through extensive experiments on three multi-organ segmentation datasets, wedemonstrate that integrating LRC to an existing self-supervised method in alimited annotation setting significantly improves segmentation performance.Moreover, we show that LRC can also be applied to fully-supervised pre-trainingmethods to further boost performance.", "output": "Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We analyze the dynamics of finite width effects in wide but finite featurelearning neural networks. Unlike many prior analyses, our results, whileperturbative in width, are non-perturbative in the strength of featurelearning. Starting from a dynamical mean field theory (DMFT) description ofinfinite width deep neural network kernel and prediction dynamics, we provide acharacterization of the $mathcal{O}(1/sqrt{text{width}})$ fluctuations ofthe DMFT order parameters over random initialization of the network weights. Inthe lazy limit of network training, all kernels are random but static in timeand the prediction variance has a universal form. However, in the rich, featurelearning regime, the fluctuations of the kernels and predictions aredynamically coupled with variance that can be computed self-consistently. Intwo layer networks, we show how feature learning can dynamically reduce thevariance of the final NTK and final network predictions. We also show howinitialization variance can slow down online learning in wide but finitenetworks. In deeper networks, kernel variance can dramatically accumulatethrough subsequent layers at large feature learning strengths, but featurelearning continues to improve the SNR of the feature kernels. In discrete time,we demonstrate that large learning rate phenomena such as edge of stabilityeffects can be well captured by infinite width dynamics and that initializationvariance can decrease dynamically. For CNNs trained on CIFAR-10, we empiricallyfind significant corrections to both the bias and variance of network dynamicsdue to finite width.", "output": "Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Keyword spotting systems continuously process audio streams to detectkeywords. One of the most challenging tasks in designing such systems is toreduce False Alarm (FA) which happens when the system falsely registers akeyword despite the keyword not being uttered. In this paper, we propose asimple yet elegant solution to this problem that follows from the law of totalprobability. We show that existing deep keyword spotting mechanisms can beimproved by Successive Refinement, where the system first classifies whetherthe input audio is speech or not, followed by whether the input is keyword-likeor not, and finally classifies which keyword was uttered. We show acrossmultiple models with size ranging from 13K parameters to 2.41M parameters, thesuccessive refinement technique reduces FA by up to a factor of 8 on in-domainheld-out FA data, and up to a factor of 7 on out-of-domain (OOD) FA data.Further, our proposed approach is \"plug-and-play\" and can be applied to anydeep keyword spotting model.", "output": "To Wake-up or Not to Wake-up: Reducing Keyword False Alarm by Successive Refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a framework for the design of feedback controllers that combinesthe optimization-driven and model-free advantages of deep reinforcementlearning with the stability guarantees provided by using the Youla-Kuceraparameterization to define the search domain. Recent advances in behavioralsystems allow us to construct a data-driven internal model; this enables analternative realization of the Youla-Kucera parameterization based entirely oninput-output exploration data. Using a neural network to express aparameterized set of nonlinear stable operators enables seamless integrationwith standard deep learning libraries. We demonstrate the approach on arealistic simulation of a two-tank system.", "output": "A modular framework for stabilizing deep reinforcement learning control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scholars in the humanities rely heavily on ancient manuscripts to studyhistory, religion, and socio-political structures in the past. Many effortshave been devoted to digitizing these precious manuscripts using OCRtechnology, but most manuscripts were blemished over the centuries so that anOptical Character Recognition (OCR) program cannot be expected to capture fadedgraphs and stains on pages. This work presents a neural spelling correctionmodel built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisyoutput. This paper is divided into four sections: dataset, model architecture,training and analysis. First, we feature-engineered our raw Tibetan etextcorpus into two sets of structured data frames -- a set of paired toy data anda set of paired real data. Then, we implemented a Confidence Score mechanisminto the Transformer architecture to perform spelling correction tasks.According to the Loss and Character Error Rate, our Transformer + Confidencescore mechanism architecture proves to be superior to Transformer, LSTM-2-LSTMand GRU-2-GRU architectures. Finally, to examine the robustness of our model,we analyzed erroneous tokens, visualized Attention and Self-Attention heatmapsin our model.", "output": "Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised approaches for learning representations invariant to commontransformations are used quite often for object recognition. Learninginvariances makes models more robust and practical to use in real-worldscenarios. Since data transformations that do not change the intrinsicproperties of the object cause the majority of the complexity in recognitiontasks, models that are invariant to these transformations help reduce theamount of training data required. This further increases the model's efficiencyand simplifies training. In this paper, we investigate the generalization ofinvariant representations on out-of-distribution data and try to answer thequestion: Do model representations invariant to some transformations in aparticular seen domain also remain invariant in previously unseen domains?Through extensive experiments, we demonstrate that the invariant model learnsunstructured latent representations that are robust to distribution shifts,thus making invariance a desirable property for training inresource-constrained settings.", "output": "Domain Generalization In Robust Invariant Representation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Distribution shifts are problems where the distribution of data changesbetween training and testing, which can significantly degrade the performanceof a model deployed in the real world. Recent studies suggest that one reasonfor the degradation is a type of overfitting, and that proper regularizationcan mitigate the degradation, especially when using highly representativemodels such as neural networks. In this paper, we propose a new regularizationusing the supervised contrastive learning to prevent such overfitting and totrain models that do not degrade their performance under the distributionshifts. We extend the cosine similarity in contrastive loss to a more generalsimilarity measure and propose to use different parameters in the measure whencomparing a sample to a positive or negative example, which is analyticallyshown to act as a kind of margin in contrastive loss. Experiments on benchmarkdatasets that emulate distribution shifts, including subpopulation shift anddomain generalization, demonstrate the advantage of the proposed method overexisting regularization methods.", "output": "Supervised Contrastive Learning with Heterogeneous Similarity for Distribution Shifts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Believable proxies of human behavior can empower interactive applicationsranging from immersive environments to rehearsal spaces for interpersonalcommunication to prototyping tools. In this paper, we introduce generativeagents--computational software agents that simulate believable human behavior.Generative agents wake up, cook breakfast, and head to work; artists paint,while authors write; they form opinions, notice each other, and initiateconversations; they remember and reflect on days past as they plan the nextday. To enable generative agents, we describe an architecture that extends alarge language model to store a complete record of the agent's experiencesusing natural language, synthesize those memories over time into higher-levelreflections, and retrieve them dynamically to plan behavior. We instantiategenerative agents to populate an interactive sandbox environment inspired byThe Sims, where end users can interact with a small town of twenty five agentsusing natural language. In an evaluation, these generative agents producebelievable individual and emergent social behaviors: for example, starting withonly a single user-specified notion that one agent wants to throw a Valentine'sDay party, the agents autonomously spread invitations to the party over thenext two days, make new acquaintances, ask each other out on dates to theparty, and coordinate to show up for the party together at the right time. Wedemonstrate through ablation that the components of our agentarchitecture--observation, planning, and reflection--each contribute criticallyto the believability of agent behavior. By fusing large language models withcomputational, interactive agents, this work introduces architectural andinteraction patterns for enabling believable simulations of human behavior.", "output": "Generative Agents: Interactive Simulacra of Human Behavior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To leverage machine learning in any decision-making process, one must convertthe given knowledge (for example, natural language, unstructured text) intorepresentation vectors that can be understood and processed by machine learningmodel in their compatible language and data format. The frequently encountereddifficulty is, however, the given knowledge is not rich or reliable enough inthe first place. In such cases, one seeks to fuse side information from aseparate domain to mitigate the gap between good representation learning andthe scarce knowledge in the domain of interest. This approach is namedCross-Domain Knowledge Transfer. It is crucial to study the problem because ofthe commonality of scarce knowledge in many scenarios, from online healthcareplatform analyses to financial market risk quantification, leaving an obstaclein front of us benefiting from automated decision making. From the machinelearning perspective, the paradigm of semi-supervised learning takes advantageof large amount of data without ground truth and achieves impressive learningperformance improvement. It is adopted in this dissertation for cross-domainknowledge transfer. (to be continued)", "output": "Graph Enabled Cross-Domain Knowledge Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Linear probing (LP) (and $k$-NN) on the upstream dataset with labels (e.g.,ImageNet) and transfer learning (TL) to various downstream datasets arecommonly employed to evaluate the quality of visual representations learned viaself-supervised learning (SSL). Although existing SSL methods have shown goodperformances under those evaluation protocols, we observe that the performancesare very sensitive to the hyperparameters involved in LP and TL. We argue thatthis is an undesirable behavior since truly generic representations should beeasily adapted to any other visual recognition task, i.e., the learnedrepresentations should be robust to the settings of LP and TL hyperparameters.In this work, we try to figure out the cause of performance sensitivity byconducting extensive experiments with state-of-the-art SSL methods. First, wefind that input normalization for LP is crucial to eliminate performancevariations according to the hyperparameters. Specifically, batch normalizationbefore feeding inputs to a linear classifier considerably improves thestability of evaluation, and also resolves inconsistency of $k$-NN and LPmetrics. Second, for TL, we demonstrate that a weight decay parameter in SSLsignificantly affects the transferability of learned representations, whichcannot be identified by LP or $k$-NN evaluations on the upstream dataset. Webelieve that the findings of this study will be beneficial for the community bydrawing attention to the shortcomings in the current SSL evaluation schemes andunderscoring the need to reconsider them.", "output": "Rethinking Evaluation Protocols of Visual Representations Learned via Self-supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sequences are often not received in their entirety at once, but instead,received incrementally over time, element by element. Early predictionsyielding a higher benefit, one aims to classify a sequence as accurately aspossible, as soon as possible, without having to wait for the last element. Forthis early sequence classification, we introduce our novel classifier-inducedstopping. While previous methods depend on exploration during training to learnwhen to stop and classify, ours is a more direct, supervised approach. Ourclassifier-induced stopping achieves an average Pareto frontier AUC increase of11.8% over multiple experiments.", "output": "A Policy for Early Sequence Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of knowledge graph (KG) applications has led to a rising needfor entity alignment (EA) between heterogeneous KGs that are extracted fromvarious sources. Recently, graph neural networks (GNNs) have been widelyadopted in EA tasks due to GNNs' impressive ability to capture structureinformation. However, we have observed that the oversimplified settings of theexisting common EA datasets are distant from real-world scenarios, whichobstructs a full understanding of the advancements achieved by recent methods.This phenomenon makes us ponder: Do existing GNN-based EA methods really makegreat progress?In this paper, to study the performance of EA methods in realistic settings,we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGsand general KGs) which are different with regard to the scale and structure,and share fewer overlapping entities. First, we sweep the unreasonablesettings, and propose two new HHKG datasets that closely mimic real-world EAscenarios. Then, based on the proposed datasets, we conduct extensiveexperiments to evaluate previous representative EA methods, and revealinteresting findings about the progress of GNN-based EA methods. We find thatthe structural information becomes difficult to exploit but still valuable inaligning HHKGs. This phenomenon leads to inferior performance of existing EAmethods, especially GNN-based methods. Our findings shed light on the potentialproblems resulting from an impulsive application of GNN-based methods as apanacea for all EA datasets. Finally, we introduce a simple but effectivemethod: Simple-HHEA, which comprehensively utilizes entity name, structure, andtemporal information. Experiment results show Simple-HHEA outperforms previousmodels on HHKG datasets. The datasets and source code will be available at", "output": "Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic imaging addresses the recovery of a time-varying 2D or 3D object ateach time instant using its undersampled measurements. In particular, in thecase of dynamic tomography, only a single projection at a single view angle maybe available at a time, making the problem severely ill-posed. In this work, wepropose an approach, RED-PSM, which combines for the first time two powerfultechniques to address this challenging imaging problem. The first, arepartially separable models, which have been used to efficiently introduce alow-rank prior for the spatio-temporal object. The second is the recentRegularization by Denoising (RED), which provides a flexible framework toexploit the impressive performance of state-of-the-art image denoisingalgorithms, for various inverse problems. We propose a partially separableobjective with RED and an optimization scheme with variable splitting and ADMM,and prove convergence of our objective to a value corresponding to a stationarypoint satisfying the first order optimality conditions. Convergence isaccelerated by a particular projection-domain-based initialization. Wedemonstrate the performance and computational improvements of our proposedRED-PSM with a learned image denoiser by comparing it to a recentdeep-prior-based method TD-DIP.", "output": "RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "GPU-based HPC clusters are attracting more scientific application developersdue to their extensive parallelism and energy efficiency. In order to achieveportability among a variety of multi/many core architectures, a popular choicefor an application developer is to utilize directive-based parallel programmingmodels, such as OpenMP. However, even with OpenMP, the developer must choosefrom among many strategies for exploiting a GPU or a CPU. Recently, MachineLearning (ML) approaches have brought significant advances in the optimizationsof HPC applications. To this end, several ways have been proposed to representapplication characteristics for ML models. However, the available techniquesfail to capture features that are crucial for exposing parallelism. In thispaper, we introduce a new graph-based program representation for parallelapplications that extends the Abstract Syntax Tree to represent control anddata flow information. The originality of this work lies in the addition of newedges exploiting the implicit ordering and parent-child relationships in ASTs,as well as the introduction of edge weights to account for loop and conditioninformation. We evaluate our proposed representation by training a Graph NeuralNetwork (GNN) to predict the runtime of an OpenMP code region across CPUs andGPUs. Various transformations utilizing collapse and data transfer between theCPU and GPU are used to construct the dataset. The predicted runtime of themodel is used to determine which transformation provides the best performance.Results show that our approach is indeed effective and has normalized RMSE aslow as 0.004 to at most 0.01 in its runtime predictions.", "output": "ParaGraph: Weighted Graph Representation for Performance Optimization of HPC Kernels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) are becoming increasingly important components ofsoftware, and are considered the state-of-the-art solution for a number ofproblems, such as image recognition. However, DNNs are far from infallible, andincorrect behavior of DNNs can have disastrous real-world consequences. Thispaper addresses the problem of architecture-preserving V-polytope provablerepair of DNNs. A V-polytope defines a convex bounded polytope using its vertexrepresentation. V-polytope provable repair guarantees that the repaired DNNsatisfies the given specification on the infinite set of points in the givenV-polytope. An architecture-preserving repair only modifies the parameters ofthe DNN, without modifying its architecture. The repair has the flexibility tomodify multiple layers of the DNN, and runs in polynomial time. It supportsDNNs with activation functions that have some linear pieces, as well asfully-connected, convolutional, pooling and residual layers. To the best ourknowledge, this is the first provable repair approach that has all of thesefeatures. We implement our approach in a tool called APRNN. Using MNIST,ImageNet, and ACAS Xu DNNs, we show that it has better efficiency, scalability,and generalization compared to PRDNN and REASSURE, prior provable repairmethods that are not architecture preserving.", "output": "Architecture-Preserving Provable Repair of Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In order to serve better VR experiences to users, existing predictive methodsof Redirected Walking (RDW) exploit future information to reduce the number ofreset occurrences. However, such methods often impose a precondition duringdeployment, either in the virtual environment's layout or the user's walkingdirection, which constrains its universal applications. To tackle thischallenge, we propose a novel mechanism F-RDW that is twofold: (1) forecaststhe future information of a user in the virtual space without any assumptions,and (2) fuse this information while maneuvering existing RDW methods. Thebackbone of the first step is an LSTM-based model that ingests the user'sspatial and eye-tracking data to predict the user's future position in thevirtual space, and the following step feeds those predicted values intoexisting RDW methods (such as MPCRed, S2C, TAPF, and ARC) while respectingtheir internal mechanism in applicable ways.The results of our simulation testand user study demonstrate the significance of future information when usingRDW in small physical spaces or complex environments. We prove that theproposed mechanism significantly reduces the number of resets and increases thetraveled distance between resets, hence augmenting the redirection performanceof all RDW methods explored in this work.", "output": "F-RDW: Redirected Walking with Forecasting Future Position."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In graph neural networks (GNNs), both node features and labels are examplesof graph signals, a key notion in graph signal processing (GSP). While it iscommon in GSP to impose signal smoothness constraints in learning andestimation tasks, it is unclear how this can be done for discrete node labels.We bridge this gap by introducing the concept of distributional graph signals.In our framework, we work with the distributions of node labels instead oftheir values and propose notions of smoothness and non-uniformity of suchdistributional graph signals. We then propose a general regularization methodfor GNNs that allows us to encode distributional smoothness and non-uniformityof the model output in semi-supervised node classification tasks. Numericalexperiments demonstrate that our method can significantly improve theperformance of most base GNN models in different problem settings.", "output": "Distributional Signals for Node Classification in Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper describes our submission to Task 10 at SemEval 2023-ExplainableDetection of Online Sexism (EDOS), divided into three subtasks. The recent risein social media platforms has seen an increase in disproportionate levels ofsexism experienced by women on social media platforms. This has made detectingand explaining online sexist content more important than ever to make socialmedia safer and more accessible for women. Our approach consists ofexperimenting and finetuning BERT-based models and using a Majority Votingensemble model that outperforms individual baseline model scores. Our systemachieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319for Task C.", "output": "SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hierarchical reinforcement learning is a promising approach that usestemporal abstraction to solve complex long horizon problems. However,simultaneously learning a hierarchy of policies is unstable as it ischallenging to train higher-level policy when the lower-level primitive isnon-stationary. In this paper, we propose a novel hierarchical algorithm bygenerating a curriculum of achievable subgoals for evolving lower-levelprimitives using reinforcement learning and imitation learning. The lower levelprimitive periodically performs data relabeling on a handful of expertdemonstrations using our primitive informed parsing approach. We provideexpressions to bound the sub-optimality of our method and develop a practicalalgorithm for hierarchical reinforcement learning. Since our approach uses ahandful of expert demonstrations, it is suitable for most robotic controltasks. Experimental evaluation on complex maze navigation and roboticmanipulation environments show that inducing hierarchical curriculum learningsignificantly improves sample efficiency, and results in efficient goalconditioned policies for solving temporally extended tasks.", "output": "CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence (AI)-based models can help in diagnosing COVID-19from lung CT scans and X-ray images; however, these models require largeamounts of data for training and validation. Many researchers studiedGenerative Adversarial Networks (GANs) for producing synthetic lung CT scansand X-Ray images to improve the performance of AI-based models. It is not wellexplored how good GAN-based methods performed to generate reliable syntheticdata. This work analyzes 43 published studies that reported GANs for syntheticdata generation. Many of these studies suffered data bias, lack ofreproducibility, and lack of feedback from the radiologists or other domainexperts. A common issue in these studies is the unavailability of the sourcecode, hindering reproducibility. The included studies reported rescaling of theinput images to train the existing GANs architecture without providing clinicalinsights on how the rescaling was motivated. Finally, even though GAN-basedmethods have the potential for data augmentation and improving the training ofAI-based models, these methods fall short in terms of their use in clinicalpractice. This paper highlights research hotspots in countering the datascarcity problem, identifies various issues as well as potentials, and providesrecommendations to guide future research. These recommendations might be usefulto improve acceptability for the GAN-based approaches for data augmentation asGANs for data augmentation are increasingly becoming popular in the AI andmedical imaging research community.", "output": "Leveraging GANs for data scarcity of COVID-19: Beyond the hype."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inference centers need more data to have a more comprehensive and beneficiallearning model, and for this purpose, they need to collect data from dataproviders. On the other hand, data providers are cautious about deliveringtheir datasets to inference centers in terms of privacy considerations. In thispaper, by modifying the structure of the autoencoder, we present a method thatmanages the utility-privacy trade-off well. To be more precise, the data isfirst compressed using the encoder, then confidential and non-confidentialfeatures are separated and uncorrelated using the classifier. The confidentialfeature is appropriately combined with noise, and the non-confidential featureis enhanced, and at the end, data with the original data format is produced bythe decoder. The proposed architecture also allows data providers to set thelevel of privacy required for confidential features. The proposed method hasbeen examined for both image and categorical databases, and the results show asignificant performance improvement compared to previous methods.", "output": "Adjustable Privacy using Autoencoder-based Learning Structure."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Orchestrating a high-quality data preparation program is essential forsuccessful machine learning (ML), but it is known to be time and effortconsuming. Despite the impressive capabilities of large language models likeChatGPT in generating programs by interacting with users through naturallanguage prompts, there are still limitations. Specifically, a user mustprovide specific prompts to iteratively guide ChatGPT in improving datapreparation programs, which requires a certain level of expertise inprogramming, the dataset used and the ML task. Moreover, once a program hasbeen generated, it is non-trivial to revisit a previous version or make changesto the program without starting the process over again. In this paper, wepresent ChatPipe, a novel system designed to facilitate seamless interactionbetween users and ChatGPT. ChatPipe provides users with effectiverecommendation on next data preparation operations, and guides ChatGPT togenerate program for the operations. Also, ChatPipe enables users to easilyroll back to previous versions of the program, which facilitates more efficientexperimentation and testing. We have developed a web application for ChatPipeand prepared several real-world ML tasks from Kaggle. These tasks can showcasethe capabilities of ChatPipe and enable VLDB attendees to easily experimentwith our novel features to rapidly orchestrate a high-quality data preparationprogram.", "output": "ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has achieved impressive performance in many domains, such ascomputer vision and natural language processing, but its advantage overclassical shallow methods on tabular datasets remains questionable. It isespecially challenging to surpass the performance of tree-like ensembles, suchas XGBoost or Random Forests, on small-sized datasets (less than 1k samples).To tackle this challenge, we introduce HyperTab, a hypernetwork-based approachto solving small sample problems on tabular datasets. By combining theadvantages of Random Forests and neural networks, HyperTab generates anensemble of neural networks, where each target model is specialized to processa specific lower-dimensional view of the data. Since each view plays the roleof data augmentation, we virtually increase the number of training sampleswhile keeping the number of trainable parameters unchanged, which preventsmodel overfitting. We evaluated HyperTab on more than 40 tabular datasets of avarying number of samples and domains of origin, and compared its performancewith shallow and deep learning models representing the currentstate-of-the-art. We show that HyperTab consistently outranks other methods onsmall data (with a statistically significant difference) and scores comparableto them on larger datasets.We make a python package with the code available to download at", "output": "HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Responsible use of data is an indispensable part of any machine learning (ML)implementation. ML developers must carefully collect and curate their datasets,and document their provenance. They must also make sure to respect intellectualproperty rights, preserve individual privacy, and use data in an ethical way.Over the past few years, ML models have significantly increased in size andcomplexity. These models require a very large amount of data and computecapacity to train, to the extent that any defects in the training corpus cannotbe trivially remedied by retraining the model from scratch. Despitesophisticated controls on training data and a significant amount of effortdedicated to ensuring that training corpora are properly composed, the sheervolume of data required for the models makes it challenging to manually inspecteach datum comprising a training corpus. One potential fix for training corpusdata defects is model disgorgement -- the elimination of not just theimproperly used data, but also the effects of improperly used data on anycomponent of an ML model. Model disgorgement techniques can be used to addressa wide range of issues, such as reducing bias or toxicity, increasing fidelity,and ensuring responsible usage of intellectual property. In this paper, weintroduce a taxonomy of possible disgorgement methods that are applicable tomodern ML systems. In particular, we investigate the meaning of \"removing theeffects\" of data in the trained model in a way that does not require retrainingfrom scratch.", "output": "AI Model Disgorgement: Methods and Choices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has been highly successful in some applications. Nevertheless,its use for solving partial differential equations (PDEs) has only been ofrecent interest with current state-of-the-art machine learning libraries, e.g.,TensorFlow or PyTorch. Physics-informed neural networks (PINNs) are anattractive tool for solving partial differential equations based on sparse andnoisy data. Here extend PINNs to solve obstacle-related PDEs which present agreat computational challenge because they necessitate numerical methods thatcan yield an accurate approximation of the solution that lies above a givenobstacle. The performance of the proposed PINNs is demonstrated in multiplescenarios for linear and nonlinear PDEs subject to regular and irregularobstacles.", "output": "A physics-informed neural network framework for modeling obstacle-related equations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Variational autoencoder (VAE) architectures have the potential to developreduced-order models (ROMs) for chaotic fluid flows. We propose a method forlearning compact and near-orthogonal ROMs using a combination of a $beta$-VAEand a transformer, tested on numerical data from a two-dimensional viscous flowin both periodic and chaotic regimes. The $beta$-VAE is trained to learn acompact latent representation of the flow velocity, and the transformer istrained to predict the temporal dynamics in latent space. Using the $beta$-VAEto learn disentangled representations in latent-space, we obtain a moreinterpretable flow model with features that resemble those observed in theproper orthogonal decomposition, but with a more efficient representation.Using Poincar'e maps, the results show that our method can capture theunderlying dynamics of the flow outperforming other prediction models. Theproposed method has potential applications in other fields such as weatherforecasting, structural dynamics or biomedical engineering.", "output": "$\\beta$-Variational autoencoders and transformers for reduced-order modelling of fluid flows."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image segmentation is a fundamental task in the field of imaging and vision.Supervised deep learning for segmentation has achieved unparalleled successwhen sufficient training data with annotated labels are available. However,annotation is known to be expensive to obtain, especially for histopathologyimages where the target regions are usually with high morphology variations andirregular shapes. Thus, weakly supervised learning with sparse annotations ofpoints is promising to reduce the annotation workload. In this work, we proposea contrast-based variational model to generate segmentation results, whichserve as reliable complementary supervision to train a deep segmentation modelfor histopathology images. The proposed method considers the commoncharacteristics of target regions in histopathology images and can be trainedin an end-to-end manner. It can generate more regionally consistent andsmoother boundary segmentation, and is more robust to unlabeled `novel'regions. Experiments on two different histology datasets demonstrate itseffectiveness and efficiency in comparison to previous models.", "output": "Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing contrastive learning methods for anomalous sound detection refinethe audio representation of each audio sample by using the contrast between thesamples' augmentations (e.g., with time or frequency masking). However, theymight be biased by the augmented data, due to the lack of physical propertiesof machine sound, thereby limiting the detection performance. This paper usescontrastive learning to refine audio representations for each machine ID,rather than for each audio sample. The proposed two-stage method usescontrastive learning to pretrain the audio representation model byincorporating machine ID and a self-supervised ID classifier to fine-tune thelearnt model, while enhancing the relation between audio features from the sameID. Experiments show that our method outperforms the state-of-the-art methodsusing contrastive learning or self-supervised classification in overall anomalydetection performance and stability on DCASE 2020 Challenge Task2 dataset.", "output": "Anomalous Sound Detection using Audio Representation with Machine ID based Contrastive Learning Pretraining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of deep learning has witnessed significant progress, particularlyin computer vision (CV), natural language processing (NLP), and speech. The useof large-scale models trained on vast amounts of data holds immense promise forpractical applications, enhancing industrial productivity and facilitatingsocial development. With the increasing demands on computational capacity,though numerous studies have explored the efficient training, a comprehensivesummarization on acceleration techniques of training deep learning models isstill much anticipated. In this survey, we present a detailed review fortraining acceleration. We consider the fundamental update formulation and splitits basic components into five main perspectives: (1) data-centric: includingdataset regularization, data sampling, and data-centric curriculum learningtechniques, which can significantly reduce the computational complexity of thedata samples; (2) model-centric, including acceleration of basic modules,compression training, model initialization and model-centric curriculumlearning techniques, which focus on accelerating the training via reducing thecalculations on parameters; (3) optimization-centric, including the selectionof learning rate, the employment of large batchsize, the designs of efficientobjectives, and model average techniques, which pay attention to the trainingpolicy and improving the generality for the large-scale models; (4) budgetedtraining, including some distinctive acceleration methods on source-constrainedsituations; (5) system-centric, including some efficient open-sourcedistributed libraries/systems which provide adequate hardware support for theimplementation of acceleration algorithms. By presenting this comprehensivetaxonomy, our survey presents a comprehensive review to understand the generalmechanisms within each component and their joint interaction.", "output": "On Efficient Training of Large-Scale Deep Learning Models: A Literature Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many real-world data sets can be presented in the form of a matrix whoseentries correspond to the interaction between two entities of different natures(number of times a web user visits a web page, a student's grade in a subject,a patient's rating of a doctor, etc.). We assume in this paper that thementioned interaction is determined by unobservable latent variables describingeach entity. Our objective is to estimate the conditional expectation of thedata matrix given the unobservable variables. This is presented as a problem ofestimation of a bivariate function referred to as graphon. We study the casesof piecewise constant and H\"older-continuous graphons. We establish finitesample risk bounds for the least squares estimator and the exponentiallyweighted aggregate. These bounds highlight the dependence of the estimationerror on the size of the data set, the maximum intensity of the interactions,and the level of noise. As the analyzed least-squares estimator is intractable,we propose an adaptation of Lloyd's alternating minimization algorithm tocompute an approximation of the least-squares estimator. Finally, we presentnumerical experiments in order to illustrate the empirical performance of thegraphon estimator on synthetic data sets.", "output": "Graphon Estimation in bipartite graphs with observable edge labels and unobservable node labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classical map-based navigation methods are commonly used for robotnavigation, but they often struggle in crowded environments due to the FrozenRobot Problem (FRP). Deep reinforcement learning-based methods address the FRPproblem, however, suffer from the issues of generalization and scalability. Toovercome these challenges, we propose a method that uses Collision Probability(CP) to help the robot navigate safely through crowds. The inclusion of CP inthe observation space gives the robot a sense of the level of danger of themoving crowd. The robot will navigate through the crowd when it appears safebut will take a detour when the crowd is moving aggressively. By focusing onthe most dangerous obstacle, the robot will not be confused when the crowddensity is high, ensuring scalability of the model. Our approach was developedusing deep reinforcement learning (DRL) and trained using the Gazebo simulatorin a non cooperative crowd environment with obstacles moving at randomizedspeeds and directions. We then evaluated our model on four differentcrowd-behavior scenarios with varying densities of crowds. The results shownthat our method achieved a 100% success rate in all test settings. We comparedour approach with a current state-of-the-art DRLbased approach, and ourapproach has performed significantly better. Importantly, our method is highlygeneralizable and requires no fine-tuning after being trained once. We furtherdemonstrated the crowd navigation capability of our model in real-world tests.", "output": "Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current literature demonstrates that Large Language Models (LLMs) are greatfew-shot learners, and prompting significantly increases their performance on arange of downstream tasks in a few-shot learning setting. An attempt toautomate human-led prompting followed, with some progress achieved. Inparticular, subsequent work demonstrates automation can outperform fine-tuningin certain K-shot learning scenarios.In this paper, we revisit techniques for automated prompting on six differentdownstream tasks and a larger range of K-shot learning settings. We find thatautomated prompting does not consistently outperform simple manual prompts. Ourwork suggests that, in addition to fine-tuning, manual prompts should be usedas a baseline in this line of research.", "output": "Revisiting Automated Prompting: Are We Actually Doing Better?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The standard class-incremental continual learning setting assumes a set oftasks seen one after the other in a fixed and predefined order. This is notvery realistic in federated learning environments where each client worksindependently in an asynchronous manner getting data for the different tasks intime-frames and orders totally uncorrelated with the other ones. We introduce anovel federated learning setting (AFCL) where the continual learning ofmultiple tasks happens at each client with different orderings and inasynchronous time slots. We tackle this novel task using prototype-basedlearning, a representation loss, fractal pre-training, and a modifiedaggregation policy. Our approach, called FedSpace, effectively tackles thistask as shown by the results on the CIFAR-100 dataset using 3 differentfederated splits with 50, 100, and 500 clients, respectively. The code andfederated splits are available at ", "output": "Asynchronous Federated Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work we derive the performance achievable by a network of distributedagents that solve, adaptively and in the presence of communication constraints,a regression problem. Agents employ the recently proposed ACTC(adapt-compress-then-combine) diffusion strategy, where the signals exchangedlocally by neighboring agents are encoded with randomized differentialcompression operators. We provide a detailed characterization of themean-square estimation error, which is shown to comprise a term related to theerror that agents would achieve without communication constraints, plus a termarising from compression. The analysis reveals quantitative relationshipsbetween the compression loss and fundamental attributes of the distributedregression problem, in particular, the stochastic approximation error caused bythe gradient noise and the network topology (through the Perron eigenvector).We show that knowledge of such relationships is critical to allocate optimallythe communication resources across the agents, taking into account theirindividual attributes, such as the quality of their data or their degree ofcentrality in the network topology. We devise an optimized allocation strategywhere the parameters necessary for the optimization can be learned online bythe agents. Illustrative examples show that a significant performanceimprovement, as compared to a blind (i.e., uniform) resource allocation, can beachieved by optimizing the allocation by means of the providedmean-square-error formulas.", "output": "Compressed Regression over Adaptive Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Previous work has established that RNNs with an unbounded activation functionhave the capacity to count exactly. However, it has also been shown that RNNsare challenging to train effectively and generally do not learn exact countingbehaviour. In this paper, we focus on this problem by studying the simplestpossible RNN, a linear single-cell network. We conduct a theoretical analysisof linear RNNs and identify conditions for the models to exhibit exact countingbehaviour. We provide a formal proof that these conditions are necessary andsufficient. We also conduct an empirical analysis using tasks involving aDyck-1-like Balanced Bracket language under two different settings. We observethat linear RNNs generally do not meet the necessary and sufficient conditionsfor counting behaviour when trained with the standard approach. We investigatehow varying the length of training sequences and utilising different targetclasses impacts model behaviour during training and the ability of linear RNNmodels to effectively approximate the indicator conditions.", "output": "Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the growing concern about the security and privacy of smart gridsystems, cyberattacks on critical power grid components, such as stateestimation, have proven to be one of the top-priority cyber-related issues andhave received significant attention in recent years. However, cyberattackdetection in smart grids now faces new challenges, including privacypreservation and decentralized power zones with strategic data owners. Toaddress these technical bottlenecks, this paper proposes a novel FederatedLearning-based privacy-preserving and communication-efficient attack detectionframework, known as FedDiSC, that enables Discrimination between power Systemdisturbances and Cyberattacks. Specifically, we first propose a FederatedLearning approach to enable Supervisory Control and Data Acquisition subsystemsof decentralized power grid zones to collaboratively train an attack detectionmodel without sharing sensitive power related data. Secondly, we put forward arepresentation learning-based Deep Auto-Encoder network to accurately detectpower system and cybersecurity anomalies. Lastly, to adapt our proposedframework to the timeliness of real-world cyberattack detection in SGs, weleverage the use of a gradient privacy-preserving quantization scheme known asDP-SIGNSGD to improve its communication efficiency. Extensive simulations ofthe proposed framework on publicly available Industrial Control Systemsdatasets demonstrate that the proposed framework can achieve superior detectionaccuracy while preserving the privacy of sensitive power grid relatedinformation. Furthermore, we find that the gradient quantization schemeutilized improves communication efficiency by 40% when compared to atraditional federated learning approach without gradient quantization whichsuggests suitability in a real-world scenario.", "output": "FedDiSC: A Computation-efficient Federated Learning Framework for Power Systems Disturbance and Cyber Attack Discrimination."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nonsmooth composite optimization with orthogonality constraints has a broadspectrum of applications in statistical learning and data science. However,this problem is generally challenging to solve due to its non-convex andnon-smooth nature. Existing solutions are limited by one or more of thefollowing restrictions: (i) they are full gradient methods that require highcomputational costs in each iteration; (ii) they are not capable of solvinggeneral nonsmooth composite problems; (iii) they are infeasible methods and canonly achieve the feasibility of the solution at the limit point; (iv) they lackrigorous convergence guarantees; (v) they only obtain weak optimality ofcritical points. In this paper, we propose textit{textbf{OBCD}}, a new BlockCoordinate Descent method for solving general nonsmooth composite problemsunder Orthogonality constraints. textit{textbf{OBCD}} is a feasible methodwith low computation complexity footprints. In each iteration, our algorithmupdates $k$ rows of the solution matrix ($kgeq2$ is a parameter) to preservethe constraints. Then, it solves a small-sized nonsmooth composite optimizationproblem under orthogonality constraints either exactly or approximately. Wedemonstrate that any exact block-$k$ stationary point is always an approximateblock-$k$ stationary point, which is equivalent to the critical stationarypoint. We are particularly interested in the case where $k=2$ as the resultingsubproblem reduces to a one-dimensional nonconvex problem. We propose abreakpoint searching method and a fifth-order iterative method to solve thisproblem efficiently and effectively. We also propose two novel greedystrategies to find a good working set to further accelerate the convergence oftextit{textbf{OBCD}}. Finally, we have conducted extensive experiments onseveral tasks to demonstrate the superiority of our approach.", "output": "A Block Coordinate Descent Method for Nonsmooth Composite Optimization under Orthogonality Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a unique solution to tackle the often-competing goals of fairnessand utility in machine learning classification tasks. While fairness ensuresthat the model's predictions are unbiased and do not discriminate against anyparticular group, utility focuses on maximizing the accuracy of the model'spredictions. Our aim is to investigate the relationship between uncertainty andfairness. Our approach leverages this concept by employing Bayesian learning toestimate the uncertainty in sample predictions where the estimation isindependent of confounding effects related to the protected attribute. Throughempirical evidence, we show that samples with low classification uncertaintyare modeled more accurately and fairly than those with high uncertainty, whichmay have biased representations and higher prediction errors. To address thechallenge of balancing fairness and utility, we propose a novelfairness-utility objective that is defined based on uncertainty quantification.The weights in this objective are determined by the level of uncertainty,allowing us to optimize both fairness and utility simultaneously. Experimentson real-world datasets demonstrate the effectiveness of our approach. Ourresults show that our method outperforms state-of-the-art methods in terms ofthe fairness-utility tradeoff and this applies to both group and individualfairness metrics. This work presents a fresh perspective on the trade-offbetween accuracy and fairness in machine learning and highlights the potentialof using uncertainty as a means to achieve optimal fairness and utility.", "output": "Fairness through Aleatoric Uncertainty."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present a contraction-guided adaptive partitioningalgorithm for improving interval-valued robust reachable set estimates in anonlinear feedback loop with a neural network controller and disturbances.Based on an estimate of the contraction rate of over-approximated intervals,the algorithm chooses when and where to partition. Then, by leveraging adecoupling of the neural network verification step and reachabilitypartitioning layers, the algorithm can provide accuracy improvements for littlecomputational cost. This approach is applicable with any sufficiently accurateopen-loop interval-valued reachability estimation technique and any method forbounding the input-output behavior of a neural network. Using contraction-basedrobustness analysis, we provide guarantees of the algorithm's performance withmixed monotone reachability. Finally, we demonstrate the algorithm'sperformance through several numerical simulations and compare it with existingmethods in the literature. In particular, we report a sizable improvement inthe accuracy of reachable set estimation in a fraction of the runtime ascompared to state-of-the-art methods.", "output": "Contraction-Guided Adaptive Partitioning for Reachability Analysis of Neural Network Controlled Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quality-Diversity (QD) algorithms are designed to generate collections ofhigh-performing solutions while maximizing their diversity in a givendescriptor space. However, in the presence of unpredictable noise, the fitnessand descriptor of the same solution can differ significantly from oneevaluation to another, leading to uncertainty in the estimation of such values.Given the elitist nature of QD algorithms, they commonly end up with manydegenerate solutions in such noisy settings. In this work, we introduce ArchiveReproducibility Improvement Algorithm (ARIA); a plug-and-play approach thatimproves the reproducibility of the solutions present in an archive. We proposeit as a separate optimization module, relying on natural evolution strategies,that can be executed on top of any QD algorithm. Our module mutates solutionsto (1) optimize their probability of belonging to their niche, and (2) maximizetheir fitness. The performance of our method is evaluated on various tasks,including a classical optimization problem and two high-dimensional controltasks in simulated robotic environments. We show that our algorithm enhancesthe quality and descriptor space coverage of any given archive by at least 50%.", "output": "Don't Bet on Luck Alone: Enhancing Behavioral Reproducibility of Quality-Diversity Solutions in Uncertain Domains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the recent years, machine learning has made great advancements that havebeen at the root of many breakthroughs in different application domains.However, it is still an open issue how make them applicable to high-stakes orsafety-critical application domains, as they can often be brittle andunreliable. In this paper, we argue that requirements definition andsatisfaction can go a long way to make machine learning models even morefitting to the real world, especially in critical domains. To this end, wepresent two problems in which (i) requirements arise naturally, (ii) machinelearning models are or can be fruitfully deployed, and (iii) neglecting therequirements can have dramatic consequences. We show how the requirementsspecification can be fruitfully integrated into the standard machine learningdevelopment pipeline, proposing a novel pyramid development process in whichrequirements definition may impact all the subsequent phases in the pipeline,and viceversa.", "output": "Machine Learning with Requirements: a Manifesto."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The popularity of encryption mechanisms poses a great challenge to malicioustraffic detection. The reason is traditional detection techniques cannot workwithout the decryption of encrypted traffic. Currently, research on encryptedmalicious traffic detection without decryption has focused on featureextraction and the choice of machine learning or deep learning algorithms. Inthis paper, we first provide an in-depth analysis of traffic features andcompare different state-of-the-art traffic feature creation approaches, whileproposing a novel concept for encrypted traffic feature which is specificallydesigned for encrypted malicious traffic analysis. In addition, we propose aframework for encrypted malicious traffic detection. The framework is atwo-layer detection framework which consists of both deep learning andtraditional machine learning algorithms. Through comparative experiments, itoutperforms classical deep learning and traditional machine learningalgorithms, such as ResNet and Random Forest. Moreover, to provide sufficienttraining data for the deep learning model, we also curate a dataset composedentirely of public datasets. The composed dataset is more comprehensive thanusing any public dataset alone. Lastly, we discuss the future directions ofthis research.", "output": "Feature Mining for Encrypted Malicious Traffic Detection with Deep Learning and Other Machine Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Even though Bayesian neural networks offer a promising framework for modelinguncertainty, active learning and incorporating prior physical knowledge, fewapplications of them can be found in the context of interatomic force modeling.One of the main challenges in their application to learning interatomic forcesis the lack of suitable Monte Carlo Markov chain sampling algorithms for theposterior density, as the commonly used algorithms do not converge in apractical amount of time for many of the state-of-the-art architectures. As aresponse to this challenge, we introduce a new Monte Carlo Markov chainsampling algorithm in this paper which can circumvent the problems of theexisting sampling methods. In addition, we introduce a new stochastic neuralnetwork model based on the NequIP architecture and demonstrate that, whencombined with our novel sampling algorithm, we obtain predictions withstate-of-the-art accuracy as well as a good measure of uncertainty.", "output": "High Accuracy Uncertainty-Aware Interatomic Force Modeling with Equivariant Bayesian Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Single occupancy vehicles are the most attractive transportation alternativefor many commuters, leading to increased traffic congestion and air pollution.Advancements in information technologies create opportunities for smartsolutions that incentivize ridesharing and mode shift to higher occupancyvehicles (HOVs) to achieve the car lighter vision of cities. In this study, wepresent HumanLight, a novel decentralized adaptive traffic signal controlalgorithm designed to optimize people throughput at intersections. Our proposedcontroller is founded on reinforcement learning with the reward functionembedding the transportation-inspired concept of pressure at the person-level.By rewarding HOV commuters with travel time savings for their efforts to mergeinto a single ride, HumanLight achieves equitable allocation of green times.Apart from adopting FRAP, a state-of-the-art (SOTA) base model, HumanLightintroduces the concept of active vehicles, loosely defined as vehicles inproximity to the intersection within the action interval window. The proposedalgorithm showcases significant headroom and scalability in different networkconfigurations considering multimodal vehicle splits at various scenarios ofHOV adoption. Improvements in person delays and queues range from 15% to over55% compared to vehicle-level SOTA controllers. We quantify the impact ofincorporating active vehicles in the formulation of our RL model for differentnetwork structures. HumanLight also enables regulation of the aggressiveness ofthe HOV prioritization. The impact of parameter setting on the generated phaseprofile is investigated as a key component of acyclic signal controllersaffecting pedestrian waiting times. HumanLight's scalable, decentralized designcan reshape the resolution of traffic management to be more human-centric andempower policies that incentivize ridesharing and public transit systems.", "output": "HumanLight: Incentivizing Ridesharing via Human-centric Deep Reinforcement Learning in Traffic Signal Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapid evolvement of deepfake creation technologies is seriously threatingmedia information trustworthiness. The consequences impacting targetedindividuals and institutions can be dire. In this work, we study the evolutionsof deep learning architectures, particularly CNNs and Transformers. Weidentified eight promising deep learning architectures, designed and developedour deepfake detection models and conducted experiments over well-establisheddeepfake datasets. These datasets included the latest second and thirdgeneration deepfake datasets. We evaluated the effectiveness of our developedsingle model detectors in deepfake detection and cross datasets evaluations. Weachieved 88.74%, 99.53%, 97.68%, 99.73% and 92.02% accuracy and 99.95%, 100%,99.88%, 99.99% and 97.61% AUC, in the detection of FF++ 2020, Google DFD,Celeb-DF, Deeper Forensics and DFDC deepfakes, respectively. We also identifiedand showed the unique strengths of CNNs and Transformers models and analysedthe observed relationships among the different deepfake datasets, to aid futuredevelopments in this area.", "output": "Deepfake Detection with Deep Learning: Convolutional Neural Networks versus Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Single domain generalization aims to learn a model from a single trainingdomain (source domain) and apply it to multiple unseen test domains (targetdomains). Existing methods focus on expanding the distribution of the trainingdomain to cover the target domains, but without estimating the domain shiftbetween the source and target domains. In this paper, we propose a new learningparadigm, namely simulate-analyze-reduce, which first simulates the domainshift by building an auxiliary domain as the target domain, then learns toanalyze the causes of domain shift, and finally learns to reduce the domainshift for model adaptation. Under this paradigm, we propose a meta-causallearning method to learn meta-knowledge, that is, how to infer the causes ofdomain shift between the auxiliary and source domains during training. We usethe meta-knowledge to analyze the shift between the target and source domainsduring testing. Specifically, we perform multiple transformations on sourcedata to generate the auxiliary domain, perform counterfactual inference tolearn to discover the causal factors of the shift between the auxiliary andsource domains, and incorporate the inferred causality into factor-aware domainalignments. Extensive experiments on several benchmarks of image classificationshow the effectiveness of our method.", "output": "Meta-causal Learning for Single Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, contrastive learning approaches (e.g., CLIP (Radford et al., 2021))have received huge success in multimodal learning, where the model tries tominimize the distance between the representations of different views (e.g.,image and its caption) of the same data point while keeping the representationsof different data points away from each other. However, from a theoreticalperspective, it is unclear how contrastive learning can learn therepresentations from different views efficiently, especially when the data isnot isotropic. In this work, we analyze the training dynamics of a simplemultimodal contrastive learning model and show that contrastive pairs areimportant for the model to efficiently balance the learned representations. Inparticular, we show that the positive pairs will drive the model to align therepresentations at the cost of increasing the condition number, while thenegative pairs will reduce the condition number, keeping the learnedrepresentations balanced.", "output": "On the Importance of Contrastive Loss in Multimodal Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Structural health monitoring (SHM) tasks like damage detection are crucialfor decision-making regarding maintenance and deterioration. For example, crackdetection in SHM is crucial for bridge maintenance as crack progression canlead to structural instability. However, most AI/ML models in the literaturehave low latency and late inference time issues while performing in real-timeenvironments. This study aims to explore the integration of edge-AI in the SHMdomain for real-time bridge inspections. Based on edge-AI literature, itscapabilities will be valuable integration for a real-time decision supportsystem in SHM tasks such that real-time inferences can be performed on physicalsites. This study will utilize commercial edge-AI platforms, such as GoogleCoral Dev Board or Kneron KL520, to develop and analyze the effectiveness ofedge-AI devices. Thus, this study proposes an edge AI framework for thestructural health monitoring domain. An edge-AI-compatible deep learning modelis developed to validate the framework to perform real-time crackclassification. The effectiveness of this model will be evaluated based on itsaccuracy, the confusion matrix generated, and the inference time observed in areal-time setting.", "output": "Integrating Edge-AI in Structural Health Monitoring domain."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We explore the metric and preference learning problem in Hilbert spaces. Weobtain a novel representer theorem for the simultaneous task of metric andpreference learning. Our key observation is that the representer theorem can beformulated with respect to the norm induced by the inner product inherent inthe problem structure. Additionally, we demonstrate how our framework can beapplied to the task of metric learning from triplet comparisons and show thatit leads to a simple and self-contained representer theorem for this task. Inthe case of Reproducing Kernel Hilbert Spaces (RKHS), we demonstrate that thesolution to the learning problem can be expressed using kernel terms, akin toclassical representer theorems.", "output": "Representer Theorems for Metric and Preference Learning: A Geometric Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating synthetic data through generative models is gaining interest inthe ML community and beyond. In the past, synthetic data was often regarded asa means to private data release, but a surge of recent papers explore how itspotential reaches much further than this -- from creating more fair data todata augmentation, and from simulation to text generated by ChatGPT. In thisperspective we explore whether, and how, synthetic data may become a dominantforce in the machine learning world, promising a future where datasets can betailored to individual needs. Just as importantly, we discuss which fundamentalchallenges the community needs to overcome for wider relevance and applicationof synthetic data -- the most important of which is quantifying how much we cantrust any finding or prediction drawn from synthetic data.", "output": "Beyond Privacy: Navigating the Opportunities and Challenges of Synthetic Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As quantum chemical properties have a significant dependence on theirgeometries, graph neural networks (GNNs) using 3D geometric information haveachieved high prediction accuracy in many tasks. However, they often require 3Dgeometries obtained from high-level quantum mechanical calculations, which arepractically infeasible, limiting their applicability in real-world problems. Totackle this, we propose a method to accurately predict the properties withrelatively easy-to-obtain geometries (e.g., optimized geometries from themolecular force field). In this method, the input geometry, regarded as thecorrupted geometry of the correct one, gradually approaches the correct one asit passes through the stacked denoising layers. We investigated the performanceof the proposed method using 3D message-passing architectures for twoprediction tasks: molecular properties and chemical reaction property. Thereduction of positional errors through the denoising process contributed toperformance improvement by increasing the mutual information between thecorrect and corrupted geometries. Moreover, our analysis of the correlationbetween denoising power and predictive accuracy demonstrates the effectivenessof the denoising process.", "output": "Predicting quantum chemical property with easy-to-obtain geometry via positional denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We extend the provably convergent Full Gradient DQN algorithm for discountedreward Markov decision processes from Avrachenkov et al. (2021) to averagereward problems. We experimentally compare widely used RVI Q-Learning withrecently proposed Differential Q-Learning in the neural function approximationsetting with Full Gradient DQN and DQN. We also extend this to learn Whittleindices for Markovian restless multi-armed bandits. We observe a betterconvergence rate of the proposed Full Gradient variant across different tasks.", "output": "Full Gradient Deep Reinforcement Learning for Average-Reward Criterion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fairness in machine learning (ML) applications is an important practice fordevelopers in research and industry. In ML applications, unfairness istriggered due to bias in the data, curation process, erroneous assumptions, andimplicit bias rendered within the algorithmic development process. As MLapplications come into broader use developing fair ML applications is critical.Literature suggests multiple views on how fairness in ML is described from theusers perspective and students as future developers. In particular, MLdevelopers have not been the focus of research relating to perceived fairness.This paper reports on a pilot investigation of ML developers perception offairness. In describing the perception of fairness, the paper performs anexploratory pilot study to assess the attributes of this construct using asystematic focus group of developers. In the focus group, we asked participantsto discuss three questions- 1) What are the characteristics of fairness in ML?2) What factors influence developers belief about the fairness of ML? and 3)What practices and tools are utilized for fairness in ML development? Thefindings of this exploratory work from the focus group show that to assessfairness developers generally focus on the overall ML application design anddevelopment, i.e., business-specific requirements, data collection,pre-processing, in-processing, and post-processing. Thus, we conclude that theprocedural aspects of organizational justice theory can explain developersperception of fairness. The findings of this study can be utilized further toassist development teams in integrating fairness in the ML applicationdevelopment lifecycle. It will also motivate ML developers and organizations todevelop best practices for assessing the fairness of ML-based applications.", "output": "Assessing Perceived Fairness from Machine Learning Developer's Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, the U.S. Department of Energy (DOE), Office of Science, Biologicaland Environmental Research (BER), and Advanced Scientific Computing Research(ASCR) programs organized and held the Artificial Intelligence for Earth SystemPredictability (AI4ESP) workshop series. From this workshop, a criticalconclusion that the DOE BER and ASCR community came to is the requirement todevelop a new paradigm for Earth system predictability focused on enablingartificial intelligence (AI) across the field, lab, modeling, and analysisactivities, called ModEx. The BER's `Model-Experimentation', ModEx, is aniterative approach that enables process models to generate hypotheses. Thedeveloped hypotheses inform field and laboratory efforts to collect measurementand observation data, which are subsequently used to parameterize, drive, andtest model (e.g., process-based) predictions. A total of 17 technical sessionswere held in this AI4ESP workshop series. This paper discusses the topic of the`AI Architectures and Co-design' session and associated outcomes. The AIArchitectures and Co-design session included two invited talks, two plenarydiscussion panels, and three breakout rooms that covered specific topics,including: (1) DOE HPC Systems, (2) Cloud HPC Systems, and (3) Edge computingand Internet of Things (IoT). We also provide forward-looking ideas andperspectives on potential research in this co-design area that can be achievedby synergies with the other 16 session topics. These ideas include topics suchas: (1) reimagining co-design, (2) data acquisition to distribution, (3)heterogeneous HPC solutions for integration of AI/ML and other data analyticslike uncertainty quantification with earth system modeling and simulation, and(4) AI-enabled sensor integration into earth system measurements andobservations. Such perspectives are a distinguishing aspect of this paper.", "output": "Perspectives on AI Architectures and Co-design for Earth System Predictability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixed Integer Programming (MIP) is NP-hard, and yet modern solvers oftensolve large real-world problems within minutes. This success can partially beattributed to heuristics. Since their behavior is highly instance-dependent,relying on hard-coded rules derived from empirical testing on a largeheterogeneous corpora of benchmark instances might lead to sub-optimalperformance. In this work, we propose an online learning approach that adaptsthe application of heuristics towards the single instance at hand. We replacethe commonly used static heuristic handling with an adaptive frameworkexploiting past observations about the heuristic's behavior to make futuredecisions. In particular, we model the problem of controlling LargeNeighborhood Search and Diving - two broad and complex classes of heuristics -as a multi-armed bandit problem. Going beyond existing work in the literature,we control two different classes of heuristics simultaneously by a singlelearning agent. We verify our approach numerically and show consistent nodereductions over the MIPLIB 2017 Benchmark set. For harder instances that takeat least 1000 seconds to solve, we observe a speedup of 4%.", "output": "Online Learning for Scheduling MIP Heuristics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Replicability is essential in science as it allows us to validate and verifyresearch findings. Impagliazzo, Lei, Pitassi and Sorrell (`22) recentlyinitiated the study of replicability in machine learning. A learning algorithmis replicable if it typically produces the same output when applied on twoi.i.d. inputs using the same internal randomness. We study a variant ofreplicability that does not involve fixing the randomness. An algorithmsatisfies this form of replicability if it typically produces the same outputwhen applied on two i.i.d. inputs (without fixing the internal randomness).This variant is called global stability and was introduced by Bun, Livni andMoran (`20) in the context of differential privacy.Impagliazzo et al. showed how to boost any replicable algorithm so that itproduces the same output with probability arbitrarily close to 1. In contrast,we demonstrate that for numerous learning tasks, global stability can only beaccomplished weakly, where the same output is produced only with probabilitybounded away from 1. To overcome this limitation, we introduce the concept oflist replicability, which is equivalent to global stability. Moreover, we provethat list replicability can be boosted so that it is achieved with probabilityarbitrarily close to 1. We also describe basic relations between standardlearning-theoretic complexity measures and list replicable numbers. Our resultsin addition imply that, besides trivial cases, replicable algorithms (in thesense of Impagliazzo et al.) must be randomized.The proof of the impossibility result is based on a topological fixed-pointtheorem. For every algorithm, we are able to locate a \"hard input distribution\"by applying the Poincar'e-Miranda theorem in a related topological setting.The equivalence between global stability and list replicability is algorithmic.", "output": "Replicability and stability in learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Approaches for teaching learning agents via human demonstrations have beenwidely studied and successfully applied to multiple domains. However, themajority of imitation learning work utilizes only behavioral information fromthe demonstrator, i.e. which actions were taken, and ignores other usefulinformation. In particular, eye gaze information can give valuable insighttowards where the demonstrator is allocating visual attention, and holds thepotential to improve agent performance and generalization. In this work, wepropose Gaze Regularized Imitation Learning (GRIL), a novel context-aware,imitation learning architecture that learns concurrently from both humandemonstrations and eye gaze to solve tasks where visual attention providesimportant context. We apply GRIL to a visual navigation task, in which anunmanned quadrotor is trained to search for and navigate to a target vehicle ina photorealistic simulated environment. We show that GRIL outperforms severalstate-of-the-art gaze-based imitation learning algorithms, simultaneouslylearns to predict human visual attention, and generalizes to scenarios notpresent in the training data. Supplemental videos can be found at project and code at", "output": "Gaze Regularized Imitation Learning: Learning Continuous Control from Human Gaze."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many areas of science make extensive use of computer simulators thatimplicitly encode likelihood functions of complex systems. Classicalstatistical methods are poorly suited for these so-called likelihood-freeinference (LFI) settings, particularly outside asymptotic and low-dimensionalregimes. Although new machine learning methods, such as normalizing flows, haverevolutionized the sample efficiency and capacity of LFI methods, it remains anopen question whether they produce confidence sets with correct conditionalcoverage for small sample sizes. This paper unifies classical statistics withmodern machine learning to present (i) a practical procedure for the Neymanconstruction of confidence sets with finite-sample guarantees of nominalcoverage, and (ii) diagnostics that estimate conditional coverage over theentire parameter space. We refer to our framework as likelihood-freefrequentist inference (LF2I). Any method that defines a test statistic, likethe likelihood ratio, can leverage the LF2I machinery to create validconfidence sets and diagnostics without costly Monte Carlo samples at fixedparameter settings. We study the power of two test statistics (ACORE and BFF),which, respectively, maximize versus integrate an odds function over theparameter space. Our paper discusses the benefits and challenges of LF2I, witha breakdown of the sources of errors in LF2I confidence sets.", "output": "Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The classical development of neural networks has primarily focused onlearning mappings between finite dimensional Euclidean spaces or finite sets.We propose a generalization of neural networks to learn operators, termedneural operators, that map between infinite dimensional function spaces. Weformulate the neural operator as a composition of linear integral operators andnonlinear activation functions. We prove a universal approximation theorem forour proposed neural operator, showing that it can approximate any givennonlinear continuous operator. The proposed neural operators are alsodiscretization-invariant, i.e., they share the same model parameters amongdifferent discretization of the underlying function spaces. Furthermore, weintroduce four classes of efficient parameterization, viz., graph neuraloperators, multi-pole graph neural operators, low-rank neural operators, andFourier neural operators. An important application for neural operators islearning surrogate maps for the solution operators of partial differentialequations (PDEs). We consider standard PDEs such as the Burgers, Darcysubsurface flow, and the Navier-Stokes equations, and show that the proposedneural operators have superior performance compared to existing machinelearning based methodologies, while being several orders of magnitude fasterthan conventional PDE solvers.", "output": "Neural Operator: Learning Maps Between Function Spaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study entropy-regularized constrained Markov decision processes (CMDPs)under the soft-max parameterization, in which an agent aims to maximize theentropy-regularized value function while satisfying constraints on the expectedtotal utility. By leveraging the entropy regularization, our theoreticalanalysis shows that its Lagrangian dual function is smooth and the Lagrangianduality gap can be decomposed into the primal optimality gap and the constraintviolation. Furthermore, we propose an accelerated dual-descent method forentropy-regularized CMDPs. We prove that our method achieves the globalconvergence rate $widetilde{mathcal{O}}(1/T)$ for both the optimality gap andthe constraint violation for entropy-regularized CMDPs. A discussion about alinear convergence rate for CMDPs with a single constraint is also provided.", "output": "A Dual Approach to Constrained Markov Decision Processes with Entropy Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Performance debugging in production is a fundamental activity in modernservice-based systems. The diagnosis of performance issues is oftentime-consuming, since it requires thorough inspection of large volumes oftraces and performance indices. In this paper we present DeLag, a novelautomated search-based approach for diagnosing performance issues inservice-based systems. DeLag identifies subsets of requests that show, in thecombination of their Remote Procedure Call execution times, symptoms ofpotentially relevant performance issues. We call such symptoms LatencyDegradation Patterns. DeLag simultaneously searches for multiple latencydegradation patterns while optimizing precision, recall and latencydissimilarity. Experimentation on 700 datasets of requests generated from twomicroservice-based systems shows that our approach provides better and morestable effectiveness than three state-of-the-art approaches and general purposemachine learning clustering algorithms. DeLag is more effective than allbaseline techniques in at least one case study (with p $leq$ 0.05 andnon-negligible effect size). Moreover, DeLag outperforms in terms of efficiencythe second and the third most effective baseline techniques on the largestdatasets used in our evaluation (up to 22%).", "output": "DeLag: Using Multi-Objective Optimization to Enhance the Detection of Latency Degradation Patterns in Service-based Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study a class of generalized linear programs (GLP) in a large-scalesetting, which includes simple, possibly nonsmooth convex regularizer andsimple convex set constraints. By reformulating (GLP) as an equivalentconvex-concave min-max problem, we show that the linear structure in theproblem can be used to design an efficient, scalable first-order algorithm, towhich we give the name emph{Coordinate Linear Variance Reduction}(textsc{clvr}; pronounced \"clever\"). textsc{clvr} yields improved complexityresults for (GLP) that depend on the max row norm of the linear constraintmatrix in (GLP) rather than the spectral norm. When the regularization termsand constraints are separable, textsc{clvr} admits an efficient lazy updatestrategy that makes its complexity bounds scale with the number of nonzeroelements of the linear constraint matrix in (GLP) rather than the matrixdimensions. On the other hand, for the special case of linear programs, byexploiting sharpness, we propose a restart scheme for textsc{clvr} to obtainempirical linear convergence. Then we show that Distributionally RobustOptimization (DRO) problems with ambiguity sets based on both $f$-divergenceand Wasserstein metrics can be reformulated as (GLPs) by introducing sparselyconnected auxiliary variables. We complement our theoretical guarantees withnumerical experiments that verify our algorithm's practical effectiveness, interms of wall-clock time and number of data passes.", "output": "Coordinate Linear Variance Reduction for Generalized Linear Programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Detecting abnormal events in video is commonly framed as a one-classclassification task, where training videos contain only normal events, whiletest videos encompass both normal and abnormal events. In this scenario,anomaly detection is an open-set problem. However, some studies assimilateanomaly detection to action recognition. This is a closed-set scenario thatfails to test the capability of systems at detecting new anomaly types. To thisend, we propose UBnormal, a new supervised open-set benchmark composed ofmultiple virtual scenes for video anomaly detection. Unlike existing data sets,we introduce abnormal events annotated at the pixel level at training time, forthe first time enabling the use of fully-supervised learning methods forabnormal event detection. To preserve the typical open-set formulation, we makesure to include disjoint sets of anomaly types in our training and testcollections of videos. To our knowledge, UBnormal is the first video anomalydetection benchmark to allow a fair head-to-head comparison between one-classopen-set models and supervised closed-set models, as shown in our experiments.Moreover, we provide empirical evidence showing that UBnormal can enhance theperformance of a state-of-the-art anomaly detection framework on two prominentdata sets, Avenue and ShanghaiTech. Our benchmark is freely available at", "output": "UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A core component of the recent success of self-supervised learning iscropping data augmentation, which selects sub-regions of an image to be used aspositive views in the self-supervised loss. The underlying assumption is thatrandomly cropped and resized regions of a given image share information aboutthe objects of interest, which the learned representation will capture. Thisassumption is mostly satisfied in datasets such as ImageNet where there is alarge, centered object, which is highly likely to be present in random crops ofthe full image. However, in other datasets such as OpenImages or COCO, whichare more representative of real world uncurated data, there are typicallymultiple small objects in an image. In this work, we show that self-supervisedlearning based on the usual random cropping performs poorly on such datasets.We propose replacing one or both of the random crops with crops obtained froman object proposal algorithm. This encourages the model to learn both objectand scene level semantic representations. Using this approach, which we callobject-aware cropping, results in significant improvements over scene croppingon classification and object detection benchmarks. For example, on OpenImages,our approach achieves an improvement of 8.8% mAP over random scene-levelcropping using MoCo-v2 based pre-training. We also show significantimprovements on COCO and PASCAL-VOC object detection and segmentation tasksover the state-of-the-art self-supervised learning approaches. Our approach isefficient, simple and general, and can be used in most existing contrastive andnon-contrastive self-supervised learning frameworks.", "output": "Object-Aware Cropping for Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapid mutation of the influenza virus threatens public health.Reassortment among viruses with different hosts can lead to a fatal pandemic.However, it is difficult to detect the original host of the virus during orafter an outbreak as influenza viruses can circulate between different species.Therefore, early and rapid detection of the viral host would help reduce thefurther spread of the virus. We use various machine learning models withfeatures derived from the position-specific scoring matrix (PSSM) and featureslearned from word embedding and word encoding to infer the origin host ofviruses. The results show that the performance of the PSSM-based model reachesthe MCC around 95%, and the F1 around 96%. The MCC obtained using the modelwith word embedding is around 96%, and the F1 is around 97%.", "output": "Predicting Influenza A Viral Host Using PSSM and Word Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a combination of machine learning techniques to enableprompt evaluation of retired electric vehicle batteries as to either retainthose batteries for a second-life application and extend their operation beyondthe original and first intent or send them to recycle facilities. The proposedalgorithm generates features from available battery current and voltagemeasurements with simple statistics, selects and ranks the features usingcorrelation analysis, and employs Gaussian Process Regression enhanced withbagging. This approach is validated over publicly available aging datasets ofmore than 200 cells with slow and fast charging, with different cathodechemistries, and for diverse operating conditions. Promising results areobserved based on multiple training-test partitions, wherein the mean of RootMean Squared Percent Error and Mean Percent Error performance errors are foundto be less than 1.48% and 1.29%, respectively, in the worst-case scenarios.", "output": "Evaluating feasibility of batteries for second-life applications using machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The shipping industry is one of the strongest anthropogenic emitters of$text{NO}_text{x}$ -- substance harmful both to human health and theenvironment. The rapid growth of the industry causes societal pressure oncontrolling the emission levels produced by ships. All the methods currentlyused for ship emission monitoring are costly and require proximity to a ship,which makes global and continuous emission monitoring impossible. A promisingapproach is the application of remote sensing. Studies showed that some of the$text{NO}_text{2}$ plumes from individual ships can visually be distinguishedusing the TROPOspheric Monitoring Instrument on board the Copernicus Sentinel 5Precursor (TROPOMI/S5P). To deploy a remote sensing-based global emissionmonitoring system, an automated procedure for the estimation of$text{NO}_text{2}$ emissions from individual ships is needed. The extremelylow signal-to-noise ratio of the available data as well as the absence ofground truth makes the task very challenging. Here, we present a methodologyfor the automated segmentation of $text{NO}_text{2}$ plumes produced byseagoing ships using supervised machine learning on TROPOMI/S5P data. We showthat the proposed approach leads to a more than a 20% increase in the averageprecision score in comparison to the methods used in previous studies andresults in a high correlation of 0.834 with the theoretically derived shipemission proxy. This work is a crucial step toward the development of anautomated procedure for global ship emission monitoring using remote sensingdata.", "output": "Supervised segmentation of NO2 plumes from individual ships using TROPOMI satellite data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to separate signal from noise, and reason with cleanabstractions, is critical to intelligence. With this ability, humans canefficiently perform real world tasks without considering all possible nuisancefactors.How can artificial agents do the same? What kind of information canagents safely discard as noises?In this work, we categorize information out in the wild into four types basedon controllability and relation with reward, and formulate useful informationas that which is both controllable and reward-relevant. This frameworkclarifies the kinds information removed by various prior work on representationlearning in reinforcement learning (RL), and leads to our proposed approach oflearning a Denoised MDP that explicitly factors out certain noise distractors.Extensive experiments on variants of DeepMind Control Suite and RoboDeskdemonstrate superior performance of our denoised world model over using rawobservations alone, and over prior works, across policy optimization controltasks as well as the non-control task of joint position regression.", "output": "Denoised MDPs: Learning World Models Better Than the World Itself."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Percolation is an important topic in climate, physics, materials science,epidemiology, finance, and so on. Prediction of percolation thresholds withmachine learning methods remains challenging. In this paper, we build apowerful graph convolutional neural network to study the percolation in bothsupervised and unsupervised ways. From a supervised learning perspective, thegraph convolutional neural network simultaneously and correctly trains data ofdifferent lattice types, such as the square and triangular lattices. For theunsupervised perspective, combining the graph convolutional neural network andthe confusion method, the percolation threshold can be obtained by the \"W\"shaped performance. The finding of this work opens up the possibility ofbuilding a more general framework that can probe the percolation-relatedphenomenon.", "output": "Machine learning of percolation models using graph convolutional neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating the political leanings of social media users is a challenging andever more pressing problem given the increase in social media consumption. Weintroduce Retweet-BERT, a simple and scalable model to estimate the politicalleanings of Twitter users. Retweet-BERT leverages the retweet network structureand the language used in users' profile descriptions. Our assumptions stem frompatterns of networks and linguistics homophily among people who share similarideologies. Retweet-BERT demonstrates competitive performance against otherstate-of-the-art baselines, achieving 96%-97% macro-F1 on two recent Twitterdatasets (a COVID-19 dataset and a 2020 United States presidential electionsdataset). We also perform manual validation to validate the performance ofRetweet-BERT on users not in the training data. Finally, in a case study ofCOVID-19, we illustrate the presence of political echo chambers on Twitter andshow that it exists primarily among right-leaning users. Our code isopen-sourced and our data is publicly available.", "output": "Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A relational dataset is often analyzed by optimally assigning a label to eachelement through clustering or ordering. While similar characterizations of adataset would be achieved by both clustering and ordering methods, the formerhas been studied much more actively than the latter, particularly for the datarepresented as graphs. This study fills this gap by investigatingmethodological relationships between several clustering and ordering methods,focusing on spectral techniques. Furthermore, we evaluate the resultingperformance of the clustering and ordering methods. To this end, we propose ameasure called the label continuity error, which generically quantifies thedegree of consistency between a sequence and partition for a set of elements.Based on synthetic and real-world datasets, we evaluate the extents to which anordering method identifies a module structure and a clustering methodidentifies a banded structure.", "output": "Consistency between ordering and clustering methods for graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In online experimentation, appropriate metrics (e.g., purchase) providestrong evidence to support hypotheses and enhance the decision-making process.However, incomplete metrics are frequently occurred in the onlineexperimentation, making the available data to be much fewer than the plannedonline experiments (e.g., A/B testing). In this work, we introduce the conceptof dropout buyers and categorize users with incomplete metric values into twogroups: visitors and dropout buyers. For the analysis of incomplete metrics, wepropose a clustering-based imputation method using $k$-nearest neighbors. Ourproposed imputation method considers both the experiment-specific features andusers' activities along their shopping paths, allowing different imputationvalues for different users. To facilitate efficient imputation of large-scaledata sets in online experimentation, the proposed method uses a combination ofstratification and clustering. The performance of the proposed method iscompared to several conventional methods in both simulation studies and a realonline experiment at eBay.", "output": "Clustering-based Imputation for Dropout Buyers in Large-scale Online Experimentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning from Demonstration (LfD) approaches empower end-users to teachrobots novel tasks via demonstrations of the desired behaviors, democratizingaccess to robotics. However, current LfD frameworks are not capable of fastadaptation to heterogeneous human demonstrations nor the large-scale deploymentin ubiquitous robotics applications. In this paper, we propose a novel LfDframework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Ourapproach (1) leverages learned strategies to construct policy mixtures for fastadaptation to new demonstrations, allowing for quick end-user personalization,(2) distills common knowledge across demonstrations, achieving accurate taskinference; and (3) expands its model only when needed in lifelong deployments,maintaining a concise set of prototypical strategies that can approximate allbehaviors via policy mixtures. We empirically validate that FLAIR achievesadaptability (i.e., the robot adapts to heterogeneous, user-specific taskpreferences), efficiency (i.e., the robot achieves sample-efficientadaptation), and scalability (i.e., the model grows sublinearly with the numberof demonstrations while maintaining high performance). FLAIR surpassesbenchmarks across three control tasks with an average 57% improvement in policyreturns and an average 78% fewer episodes required for demonstration modelingusing policy mixtures. Finally, we demonstrate the success of FLAIR in a tabletennis task and find users rate FLAIR as having higher task (p&lt;.05) andpersonalization (p&lt;.05) performance.", "output": "Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are vulnerable to adversarial attacks. Most $L_{0}$-normbased white-box attacks craft perturbations by the gradient of models to theinput. Since the computation cost and memory limitation of calculating theHessian matrix, the application of Hessian or approximate Hessian in white-boxattacks is gradually shelved. In this work, we note that the sparsityrequirement on perturbations naturally lends itself to the usage of Hessianinformation. We study the attack performance and computation cost of the attackmethod based on the Hessian with a limited number of perturbation pixels.Specifically, we propose the Limited Pixel BFGS (LP-BFGS) attack method byincorporating the perturbation pixel selection strategy and the BFGS algorithm.Pixels with top-k attribution scores calculated by the Integrated Gradientmethod are regarded as optimization variables of the LP-BFGS attack.Experimental results across different networks and datasets demonstrate thatour approach has comparable attack ability with reasonable computation indifferent numbers of perturbation pixels compared with existing solutions.", "output": "LP-BFGS attack: An adversarial attack based on the Hessian with limited pixels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the increasing computational demand of Deep Neural Networks (DNNs),companies and organizations have begun to outsource the training process.However, the externally trained DNNs can potentially be backdoor attacked. Itis crucial to defend against such attacks, i.e., to postprocess a suspiciousmodel so that its backdoor behavior is mitigated while its normal predictionpower on clean inputs remain uncompromised. To remove the abnormal backdoorbehavior, existing methods mostly rely on additional labeled clean samples.However, such requirement may be unrealistic as the training data are oftenunavailable to end users. In this paper, we investigate the possibility ofcircumventing such barrier. We propose a novel defense method that does notrequire training labels. Through a carefully designed layer-wise weightre-initialization and knowledge distillation, our method can effectivelycleanse backdoor behaviors of a suspicious network with negligible compromisein its normal behavior. In experiments, we show that our method, trainedwithout labels, is on-par with state-of-the-art defense methods trained usinglabels. We also observe promising defense results even on out-of-distributiondata. This makes our method very practical. Code is available at:", "output": "Backdoor Cleansing with Unlabeled Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the current context where online platforms have been effectivelyweaponized in a variety of geo-political events and social issues, Internetmemes make fair content moderation at scale even more difficult. Existing workon meme classification and tracking has focused on black-box methods that donot explicitly consider the semantics of the memes or the context of theircreation. In this paper, we pursue a modular and explainable architecture forInternet meme understanding. We design and implement multimodal classificationmethods that perform example- and prototype-based reasoning over trainingcases, while leveraging both textual and visual SOTA models to represent theindividual cases. We study the relevance of our modular and explainable modelsin detecting harmful memes on two existing tasks: Hate Speech Detection andMisogyny Classification. We compare the performance between example- andprototype-based methods, and between text, vision, and multimodal models,across different categories of harmfulness (e.g., stereotype andobjectification). We devise a user-friendly interface that facilitates thecomparative analysis of examples retrieved by all of our models for any givenmeme, informing the community about the strengths and limitations of theseexplainable methods.", "output": "Multimodal and Explainable Internet Meme Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A long-standing goal of machine-learning-based protein engineering is toaccelerate the discovery of novel mutations that improve the function of aknown protein. We introduce a sampling framework for evolving proteins insilico that supports mixing and matching a variety of unsupervised models, suchas protein language models, and supervised models that predict protein functionfrom sequence. By composing these models, we aim to improve our ability toevaluate unseen mutations and constrain search to regions of sequence spacelikely to contain functional proteins. Our framework achieves this without anymodel fine-tuning or re-training by constructing a product of expertsdistribution directly in discrete protein space. Instead of resorting to bruteforce search or random sampling, which is typical of classic directedevolution, we introduce a fast MCMC sampler that uses gradients to proposepromising mutations. We conduct in silico directed evolution experiments onwide fitness landscapes and across a range of different pre-trainedunsupervised models, including a 650M parameter protein language model. Ourresults demonstrate an ability to efficiently discover variants with highevolutionary likelihood as well as estimated activity multiple mutations awayfrom a wild type protein, suggesting our sampler provides a practical andeffective new paradigm for machine-learning-based protein engineering.", "output": "Plug & Play Directed Evolution of Proteins with Gradient-based Discrete MCMC."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a crucial building block in vertical Federated Learning (vFL), SplitLearning (SL) has demonstrated its practice in the two-party model trainingcollaboration, where one party holds the features of data samples and anotherparty holds the corresponding labels. Such method is claimed to be privateconsidering the shared information is only the embedding vectors and gradientsinstead of private raw data and labels. However, some recent works have shownthat the private labels could be leaked by the gradients. These existing attackonly works under the classification setting where the private labels arediscrete. In this work, we step further to study the leakage in the scenario ofthe regression model, where the private labels are continuous numbers (insteadof discrete labels in classification). This makes previous attacks harder toinfer the continuous labels due to the unbounded output range. To address thelimitation, we propose a novel learning-based attack that integrates gradientinformation and extra learning regularization objectives in aspects of modeltraining properties, which can infer the labels under regression settingseffectively. The comprehensive experiments on various datasets and models havedemonstrated the effectiveness of our proposed attack. We hope our work canpave the way for future analyses that make the vFL framework more secure.", "output": "Label Inference Attack against Split Learning under Regression Setting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper reconsiders end-to-end learning approaches to the Optimal PowerFlow (OPF). Existing methods, which learn the input/output mapping of the OPF,suffer from scalability issues due to the high dimensionality of the outputspace. This paper first shows that the space of optimal solutions can besignificantly compressed using principal component analysis (PCA). It thenproposes Compact Learning, a new method that learns in a subspace of theprincipal components before translating the vectors into the original outputspace. This compression reduces the number of trainable parameterssubstantially, improving scalability and effectiveness. Compact Learning isevaluated on a variety of test cases from the PGLib with up to 30,000 buses.The paper also shows that the output of Compact Learning can be used towarm-start an exact AC solver to restore feasibility, while bringingsignificant speed-ups.", "output": "Compact Optimization Learning for AC Optimal Power Flow."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a modality-agnostic neural compression algorithm based on afunctional view of data and parameterised as an Implicit Neural Representation(INR). Bridging the gap between latent coding and sparsity, we obtain compactlatent representations non-linearly mapped to a soft gating mechanism. Thisallows the specialisation of a shared INR network to each data item throughsubnetwork selection. After obtaining a dataset of such latent representations,we directly optimise the rate/distortion trade-off in a modality-agnostic spaceusing neural compression. Variational Compression of Implicit NeuralRepresentations (VC-INR) shows improved performance given the samerepresentational capacity pre quantisation while also outperforming previousquantisation schemes used for other INR techniques. Our experiments demonstratestrong results over a large set of diverse modalities using the same algorithmwithout any modality-specific inductive biases. We show results on images,climate data, 3D shapes and scenes as well as audio and video, introducingVC-INR as the first INR-based method to outperform codecs as well-known anddiverse as JPEG 2000, MP3 and AVC/HEVC on their respective modalities.", "output": "Modality-Agnostic Variational Compression of Implicit Neural Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper reviews the state-of-the-art of language models architectures andstrategies for \"complex\" question-answering (QA, CQA, CPS) with a focus onhybridization. Large Language Models (LLM) are good at leveraging public dataon standard problems but once you want to tackle more specific complexquestions or problems (e.g. How does the concept of personal freedom varybetween different cultures ? What is the best mix of power generation methodsto reduce climate change ?) you may need specific architecture, knowledge,skills, methods, sensitive data protection, explainability, human approval andversatile feedback... Recent projects like ChatGPT and GALACTICA have allowednon-specialists to grasp the great potential as well as the equally stronglimitations of LLM in complex QA. In this paper, we start by reviewing requiredskills and evaluation techniques. We integrate findings from the robustcommunity edited research papers BIG, BLOOM and HELM which open source,benchmark and analyze limits and challenges of LLM in terms of tasks complexityand strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) asa baseline. We discuss some challenges associated with complex QA, includingdomain adaptation, decomposition and efficient multi-step QA, long form andnon-factoid QA, safety and multi-sensitivity data protection, multimodalsearch, hallucinations, explainability and truthfulness, temporal reasoning. Weanalyze current solutions and promising research trends, using elements suchas: hybrid LLM architectural patterns, training and prompting strategies,active human reinforcement learning supervised with AI, neuro-symbolic andstructured knowledge grounding, program synthesis, iterated decomposition andothers.", "output": "Complex QA and language models hybrid architectures, Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Starting from 2021, more demanding $text{NO}_text{x}$ emission restrictionswere introduced for ships operating in the North and Baltic Sea waters. Sinceall methods currently used for ship compliance monitoring are financially andtime demanding, it is important to prioritize the inspection of ships that havehigh chances of being non-compliant. The current state-of-the-art approach fora large-scale ship $text{NO}_text{2}$ estimation is a supervised machinelearning-based segmentation of ship plumes on TROPOMI/S5P images. However,challenging data annotation and insufficiently complex ship emission proxy usedfor the validation limit the applicability of the model for ship compliancemonitoring. In this study, we present a method for the automated selection ofpotentially non-compliant ships using a combination of machine learning modelson TROPOMI satellite data. It is based on a proposed regression modelpredicting the amount of $text{NO}_text{2}$ that is expected to be producedby a ship with certain properties operating in the given atmosphericconditions. The model does not require manual labeling and is validated withTROPOMI data directly. The differences between the predicted and actual amountof produced $text{NO}_text{2}$ are integrated over observations of the shipin time and are used as a measure of the inspection worthiness of a ship. Toassure the robustness of the results, we compare the obtained results with theresults of the previously developed segmentation-based method. Ships that arealso highly deviating in accordance with the segmentation method requirefurther attention. If no other explanations can be found by checking theTROPOMI data, the respective ships are advised to be the candidates forinspection.", "output": "Anomalous NO2 emitting ship detection with TROPOMI satellite data and machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We revisit in this paper the discrete-time linear quadratic regulator (LQR)problem from the perspective of receding-horizon policy gradient (RHPG), anewly developed model-free learning framework for control applications. Weprovide a fine-grained sample complexity analysis for RHPG to learn a controlpolicy that is both stabilizing and $epsilon$-close to the optimal LQRsolution, and our algorithm does not require knowing a stabilizing controlpolicy for initialization. Combined with the recent application of RHPG inlearning the Kalman filter, we demonstrate the general applicability of RHPG inlinear control and estimation with streamlined analyses.", "output": "Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the long runtime of Data Science (DS) pipelines, even smallprogramming mistakes can be very costly, if they are not detected statically.However, even basic static type checking of DS pipelines is difficult becausemost are written in Python. Static typing is available in Python only viaexternal linters. These require static type annotations for parameters orresults of functions, which many DS libraries do not provide. In this paper, weshow how the wealth of Python DS libraries can be used in a statically safe wayvia Safe-DS, a domain specific language (DSL) for DS. Safe-DS catchesconventional type errors plus errors related to range restrictions, datamanipulation, and call order of functions, going well beyond the abilities ofcurrent Python linters. Python libraries are integrated into Safe-DS via a stublanguage for specifying the interface of its declarations, and an API-Editorthat is able to extract type information from the code and documentation ofPython libraries, and automatically generate suitable stubs.Moreover, Safe-DS complements textual DS pipelines with a graphicalrepresentation that eases safe development by preventing syntax errors. Theseamless synchronization of textual and graphic view lets developers alwayschoose the one best suited for their skills and current task. We think thatSafe-DS can make DS development easier, faster, and more reliable,significantly reducing development costs.", "output": "Safe-DS: A Domain Specific Language to Make Data Science Safe."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the influence of different activation functions in the output layerof deep neural network models for soft and hard label prediction in thelearning with disagreement task. In this task, the goal is to quantify theamount of disagreement via predicting soft labels. To predict the soft labels,we use BERT-based preprocessors and encoders and vary the activation functionused in the output layer, while keeping other parameters constant. The softlabels are then used for the hard label prediction. The activation functionsconsidered are sigmoid as well as a step-function that is added to the modelpost-training and a sinusoidal activation function, which is introduced for thefirst time in this paper.", "output": "Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic Speech Recognition (ASR) in medical contexts has the potential tosave time, cut costs, increase report accuracy, and reduce physician burnout.However, the healthcare industry has been slower to adopt this technology, inpart due to the importance of avoiding medically-relevant transcriptionmistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASRmetric that penalizes clinically-relevant mistakes more than others. Wedemonstrate that this metric more closely aligns with clinician preferences onmedical sentences as compared to other metrics (WER, BLUE, METEOR, etc),sometimes by wide margins. We collect a benchmark of 18 clinician preferenceson 149 realistic medical sentences called the Clinician Transcript Preferencebenchmark (CTP), demonstrate that CBERTScore more closely matches whatclinicians prefer, and release the benchmark for the community to furtherdevelop clinically-aware ASR metrics.", "output": "Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the last few years, many works have tried to explain the predictions ofdeep learning models. Few methods, however, have been proposed to verify theaccuracy or faithfulness of these explanations. Recently, influence functions,which is a method that approximates the effect that leave-one-out training hason the loss function, has been shown to be fragile. The proposed reason fortheir fragility remains unclear. Although previous work suggests the use ofregularization to increase robustness, this does not hold in all cases. In thiswork, we seek to investigate the experiments performed in the prior work in aneffort to understand the underlying mechanisms of influence function fragility.First, we verify influence functions using procedures from the literature underconditions where the convexity assumptions of influence functions are met.Then, we relax these assumptions and study the effects of non-convexity byusing deeper models and more complex datasets. Here, we analyze the key metricsand procedures that are used to validate influence functions. Our resultsindicate that the validation procedures may cause the observed fragility.", "output": "Revisiting the Fragility of Influence Functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are widely used prediction algorithms whose performanceoften improves as the number of weights increases, leading toover-parametrization. We consider a two-layered neural network whose firstlayer is frozen while the last layer is trainable, known as the random featuremodel. We study over-parametrization in the context of a student-teacherframework by deriving a set of differential equations for the learningdynamics. For any finite ratio of hidden layer size and input dimension, thestudent cannot generalize perfectly, and we compute the non-zero asymptoticgeneralization error. Only when the student's hidden layer size isexponentially larger than the input dimension, an approach to perfectgeneralization is possible.", "output": "Online Learning for the Random Feature Model in the Student-Teacher Framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D object detection from visual sensors is a cornerstone capability ofrobotic systems. State-of-the-art methods focus on reasoning and decodingobject bounding boxes from multi-view camera input. In this work we gainintuition from the integral role of multi-view consistency in 3D sceneunderstanding and geometric learning. To this end, we introduce VEDet, a novel3D object detection framework that exploits 3D multi-view geometry to improvelocalization through viewpoint awareness and equivariance. VEDet leverages aquery-based transformer architecture and encodes the 3D scene by augmentingimage features with positional encodings from their 3D perspective geometry. Wedesign view-conditioned queries at the output level, which enables thegeneration of multiple virtual frames during training to learn viewpointequivariance by enforcing multi-view consistency. The multi-view geometryinjected at the input level as positional encodings and regularized at the losslevel provides rich geometric cues for 3D object detection, leading tostate-of-the-art performance on the nuScenes benchmark. The code and model aremade available at ", "output": "Viewpoint Equivariance for Multi-View 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the recent study of deep learning in scientific computation, thePhysics-Informed Neural Networks (PINNs) method has drawn widespread attentionfor solving Partial Differential Equations (PDEs). Compared to traditionalmethods, PINNs can efficiently handle high-dimensional problems, but theaccuracy is relatively low, especially for highly irregular problems. Inspiredby the idea of adaptive finite element methods and incremental learning, wepropose GAS, a Gaussian mixture distribution-based adaptive sampling method forPINNs. During the training procedure, GAS uses the current residual informationto generate a Gaussian mixture distribution for the sampling of additionalpoints, which are then trained together with historical data to speed up theconvergence of the loss and achieve higher accuracy. Several numericalsimulations on 2D and 10D problems show that GAS is a promising method thatachieves state-of-the-art accuracy among deep solvers, while being comparablewith traditional numerical solvers.", "output": "GAS: A Gaussian Mixture Distribution-Based Adaptive Sampling Method for PINNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The increasingly deeper neural networks hinder the democratization ofprivacy-enhancing distributed learning, such as federated learning (FL), toresource-constrained devices. To overcome this challenge, in this paper, weadvocate the integration of edge computing paradigm and parallel split learning(PSL), allowing multiple client devices to offload substantial trainingworkloads to an edge server via layer-wise model split. By observing thatexisting PSL schemes incur excessive training latency and large volume of datatransmissions, we propose an innovative PSL framework, namely, efficientparallel split learning (EPSL), to accelerate model training. To be specific,EPSL parallelizes client-side model training and reduces the dimension of localgradients for back propagation (BP) via last-layer gradient aggregation,leading to a significant reduction in server-side training and communicationlatency. Moreover, by considering the heterogeneous channel conditions andcomputing capabilities at client devices, we jointly optimize subchannelallocation, power control, and cut layer selection to minimize the per-roundlatency. Simulation results show that the proposed EPSL framework significantlydecreases the training latency needed to achieve a target accuracy comparedwith the state-of-the-art benchmarks, and the tailored resource management andlayer split strategy can considerably reduce latency than the counterpartwithout optimization.", "output": "Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ensemble forecast post-processing is a necessary step in producing accurateprobabilistic forecasts. Conventional post-processing methods operate byestimating the parameters of a parametric distribution, frequently on aper-location or per-lead-time basis. We propose a novel, neural network-basedmethod, which produces forecasts for all locations and lead times, jointly. Torelax the distributional assumption of many post-processing methods, ourapproach incorporates normalizing flows as flexible parametric distributionestimators. This enables us to model varying forecast distributions in amathematically exact way. We demonstrate the effectiveness of our method in thecontext of the EUPPBench benchmark, where we conduct temperature forecastpost-processing for stations in a sub-region of western Europe. We show thatour novel method exhibits state-of-the-art performance on the benchmark,outclassing our previous, well-performing entry. Additionally, by providing adetailed comparison of three variants of our novel post-processing method, weelucidate the reasons why our method outperforms per-lead-time-based approachesand approaches with distributional assumptions.", "output": "Ensemble weather forecast post-processing with a flexible probabilistic neural network approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In goal-reaching reinforcement learning (RL), the optimal value function hasa particular geometry, called quasimetric structure. This paper introducesQuasimetric Reinforcement Learning (QRL), a new RL method that utilizesquasimetric models to learn optimal value functions. Distinct from priorapproaches, the QRL objective is specifically designed for quasimetrics, andprovides strong theoretical recovery guarantees. Empirically, we conductthorough analyses on a discretized MountainCar environment, identifyingproperties of QRL and its advantages over alternatives. On offline and onlinegoal-reaching benchmarks, QRL also demonstrates improved sample efficiency andperformance, across both state-based and image-based observations.", "output": "Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The SUBNET neural network architecture has been developed to identifynonlinear state-space models from input-output data. To achieve this, itcombines the rolled-out nonlinear state-space equations and a state encoderfunction, both parameterised as neural networks The encoder function isintroduced to reconstruct the current state from past input-output data. Hence,it enables the forward simulation of the rolled-out state-space model. Whilethis approach has shown to provide high-accuracy and consistent modelestimation, its convergence can be significantly improved by efficientinitialization of the training process. This paper focuses on such aninitialisation of the subspace encoder approach using the Best LinearApproximation (BLA). Using the BLA provided state-space matrices and itsassociated reconstructability map, both the state-transition part of thenetwork and the encoder are initialized. The performance of the improvedinitialisation scheme is evaluated on a Wiener-Hammerstein simulation exampleand a benchmark dataset. The results show that for a weakly nonlinear system,the proposed initialisation based on the linear reconstructability map resultsin a faster convergence and a better model quality.", "output": "Initialization Approach for Nonlinear State-Space Identification via the Subspace Encoder Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most entropy measures depend on the spread of the probability distributionover the sample space X, and the maximum entropy achievable scalesproportionately with the sample space cardinality |X|. For a finite |X|, thisyields robust entropy measures which satisfy many important properties, such asinvariance to bijections, while the same is not true for continuous spaces(where |X|=infinity). Furthermore, since R and R^d (d in Z+) have the samecardinality (from Cantor's correspondence argument), cardinality-dependententropy measures cannot encode the data dimensionality. In this work, wequestion the role of cardinality and distribution spread in defining entropymeasures for continuous spaces, which can undergo multiple rounds oftransformations and distortions, e.g., in neural networks. We find that theaverage value of the local intrinsic dimension of a distribution, denoted asID-Entropy, can serve as a robust entropy measure for continuous spaces, whilecapturing the data dimensionality. We find that ID-Entropy satisfies manydesirable properties and can be extended to conditional entropy, joint entropyand mutual-information variants. ID-Entropy also yields new informationbottleneck principles and also links to causality. In the context of deeplearning, for feedforward architectures, we show, theoretically andempirically, that the ID-Entropy of a hidden layer directly controls thegeneralization gap for both classifiers and auto-encoders, when the targetfunction is Lipschitz continuous. Our work primarily shows that, for continuousspaces, taking a structural rather than a statistical approach yields entropymeasures which preserve intrinsic data dimensionality, while being relevant forstudying various architectures.", "output": "Local Intrinsic Dimensional Entropy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical data often exhibits long-tail distributions with heavy classimbalance, which naturally leads to difficulty in classifying the minorityclasses (i.e., boundary regions or rare objects). Recent work has significantlyimproved semi-supervised medical image segmentation in long-tailed scenarios byequipping them with unsupervised contrastive criteria. However, it remainsunclear how well they will perform in the labeled portion of data where classdistribution is also highly imbalanced. In this work, we present ACTION++, animproved contrastive learning framework with adaptive anatomical contrast forsemi-supervised medical segmentation. Specifically, we propose an adaptivesupervised contrastive loss, where we first compute the optimal locations ofclass centers uniformly distributed on the embedding space (i.e., off-line),and then perform online contrastive matching training by encouraging differentclass features to adaptively match these distinct and uniformly distributedclass centers. Moreover, we argue that blindly adopting a constant temperature$tau$ in the contrastive loss on long-tailed medical data is not optimal, andpropose to use a dynamic $tau$ via a simple cosine schedule to yield betterseparation between majority and minority classes. Empirically, we evaluateACTION++ on ACDC and LA benchmarks and show that it achieves state-of-the-artacross two semi-supervised settings. Theoretically, we analyze the performanceof adaptive anatomical contrast and confirm its superiority in labelefficiency.", "output": "ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The amount of sequencing data for SARS-CoV-2 is several orders of magnitudelarger than any virus. This will continue to grow geometrically for SARS-CoV-2,and other viruses, as many countries heavily finance genomic surveillanceefforts. Hence, we need methods for processing large amounts of sequence datato allow for effective yet timely decision-making. Such data will come fromheterogeneous sources: aligned, unaligned, or even unassembled raw nucleotideor amino acid sequencing reads pertaining to the whole genome or regions (e.g.,spike) of interest. In this work, we propose emph{ViralVectors}, a compactfeature vector generation from virome sequencing data that allows effectivedownstream analysis. Such generation is based on emph{minimizers}, a type oflightweight \"signature\" of a sequence, used traditionally in assembly and readmapping -- to our knowledge, the first use minimizers in this way. We validateour approach on different types of sequencing data: (a) 2.5M SARS-CoV-2 spikesequences (to show scalability); (b) 3K Coronaviridae spike sequences (to showrobustness to more genomic variability); and (c) 4K raw WGS reads sets takenfrom nasal-swab PCR tests (to show the ability to process unassembled reads).Our results show that ViralVectors outperforms current benchmarks in mostclassification and clustering tasks.", "output": "ViralVectors: Compact and Scalable Alignment-free Virome Feature Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unraveling the reasons behind the remarkable success and exceptionalgeneralization capabilities of deep neural networks presents a formidablechallenge. Recent insights from random matrix theory, specifically thoseconcerning the spectral analysis of weight matrices in deep neural networks,offer valuable clues to address this issue. A key finding indicates that thegeneralization performance of a neural network is associated with the degree ofheavy tails in the spectrum of its weight matrices. To capitalize on thisdiscovery, we introduce a novel regularization technique, termed Heavy-TailedRegularization, which explicitly promotes a more heavy-tailed spectrum in theweight matrix through regularization. Firstly, we employ the Weighted Alpha andStable Rank as penalty terms, both of which are differentiable, enabling thedirect calculation of their gradients. To circumvent over-regularization, weintroduce two variations of the penalty function. Then, adopting a Bayesianstatistics perspective and leveraging knowledge from random matrices, wedevelop two novel heavy-tailed regularization methods, utilizing Powerlawdistribution and Frechet distribution as priors for the global spectrum andmaximum eigenvalues, respectively. We empirically show that heavytailedregularization outperforms conventional regularization techniques in terms ofgeneralization performance.", "output": "Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a way to implement the \"right to be forgotten\" in machine learning,textit{machine unlearning} aims to completely remove the contributions andinformation of the samples to be deleted from a trained model without affectingthe contributions of other samples. Recently, many frameworks for machineunlearning have been proposed, and most of them focus on image and text data.To extend machine unlearning to graph data, textit{GraphEraser} has beenproposed. However, a critical issue is that textit{GraphEraser} isspecifically designed for the transductive graph setting, where the graph isstatic and attributes and edges of test nodes are visible during training. Itis unsuitable for the inductive setting, where the graph could be dynamic andthe test graph information is invisible in advance. Such inductive capabilityis essential for production machine learning systems with evolving graphs likesocial media and transaction networks. To fill this gap, we propose theunderline{{bf G}}underline{{bf U}}ided underline{{bf I}}nunderline{{bfD}}uctivunderline{{bf E}} Graph Unlearning framework (GUIDE). GUIDE consistsof three components: guided graph partitioning with fairness and balance,efficient subgraph repair, and similarity-based aggregation. Empirically, weevaluate our method on several inductive benchmarks and evolving transactiongraphs. Generally speaking, GUIDE can be efficiently implemented on theinductive graph learning tasks for its low graph partition cost, no matter oncomputation or structure information. The code will be available here:", "output": "Inductive Graph Unlearning."}]