[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cybersecurity has emerged as a critical challenge for the industry. With thelarge complexity of the security landscape, sophisticated and costly deeplearning models often fail to provide timely detection of cyber threats on edgedevices. Brain-inspired hyperdimensional computing (HDC) has been introduced asa promising solution to address this issue. However, existing HDC approachesuse static encoders and require very high dimensionality and hundreds oftraining iterations to achieve reasonable accuracy. This results in a seriousloss of learning efficiency and causes huge latency for detecting attacks. Inthis paper, we propose CyberHD, an innovative HDC learning framework thatidentifies and regenerates insignificant dimensions to capture complicatedpatterns of cyber threats with remarkably lower dimensionality. Additionally,the holographic distribution of patterns in high dimensional space providesCyberHD with notably high robustness against hardware errors.", "output": "Late Breaking Results: Scalable and Efficient Hyperdimensional Computing for Network Intrusion Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Meta-learning is a framework for learning learning algorithms throughrepeated interactions with an environment as opposed to designing them by hand.In recent years, this framework has established itself as a promising tool forbuilding models of human cognition. Yet, a coherent research program aroundmeta-learned models of cognition is still missing. The purpose of this articleis to synthesize previous work in this field and establish such a researchprogram. We rely on three key pillars to accomplish this goal. We first pointout that meta-learning can be used to construct Bayes-optimal learningalgorithms. This result not only implies that any behavioral phenomenon thatcan be explained by a Bayesian model can also be explained by a meta-learnedmodel but also allows us to draw strong connections to the rational analysis ofcognition. We then discuss several advantages of the meta-learning frameworkover traditional Bayesian methods. In particular, we argue that meta-learningcan be applied to situations where Bayesian inference is impossible and that itenables us to make rational models of cognition more realistic, either byincorporating limited computational resources or neuroscientific knowledge.Finally, we reexamine prior studies from psychology and neuroscience that haveapplied meta-learning and put them into the context of these new insights. Insummary, our work highlights that meta-learning considerably extends the scopeof rational analysis and thereby of cognitive theories more generally.", "output": "Meta-Learned Models of Cognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans excel at continually acquiring, consolidating, and retaininginformation from an ever-changing environment, whereas artificial neuralnetworks (ANNs) exhibit catastrophic forgetting. There are considerabledifferences in the complexity of synapses, the processing of information, andthe learning mechanisms in biological neural networks and their artificialcounterparts, which may explain the mismatch in performance. We consider abiologically plausible framework that constitutes separate populations ofexclusively excitatory and inhibitory neurons that adhere to Dale's principle,and the excitatory pyramidal neurons are augmented with dendritic-likestructures for context-dependent processing of stimuli. We then conduct acomprehensive study on the role and interactions of different mechanismsinspired by the brain, including sparse non-overlapping representations,Hebbian learning, synaptic consolidation, and replay of past activations thataccompanied the learning event. Our study suggests that the employing ofmultiple complementary mechanisms in a biologically plausible architecture,similar to the brain, may be effective in enabling continual learning in ANNs.", "output": "A Study of Biologically Plausible Neural Network: The Role and Interactions of Brain-Inspired Mechanisms in Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large decoder-only language models (LMs) can be largely improved in terms ofperplexity by retrieval (e.g., RETRO), but its impact on text generationquality and downstream task accuracy is unclear. Thus, it is still an openquestion: shall we pretrain large autoregressive LMs with retrieval? To answerit, we perform a comprehensive study on a scalable pre-trainedretrieval-augmented LM (i.e., RETRO) compared with standard GPT andretrieval-augmented GPT incorporated at fine-tuning or inference stages. Wefirst provide the recipe to reproduce RETRO up to 9.5B parameters whileretrieving a text corpus with 330B tokens. Based on that, we have the followingnovel findings: i) RETRO outperforms GPT on text generation with much lessdegeneration (i.e., repetition), moderately higher factual accuracy, andslightly lower toxicity with a nontoxic retrieval database. ii) On the LMEvaluation Harness benchmark, RETRO largely outperforms GPT onknowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore,we introduce a simple variant of the model, RETRO++, which largely improvesopen-domain QA results of original RETRO (e.g., EM score +8.6 on NaturalQuestion) and significantly outperforms retrieval-augmented GPT acrossdifferent model sizes. Our findings highlight the promising direction ofpretraining autoregressive LMs with retrieval as future foundation models. Werelease our implementation at: ", "output": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative foundation models are susceptible to implicit biases that canarise from extensive unsupervised training data. Such biases can producesuboptimal samples, skewed outcomes, and unfairness, with potentiallysignificant repercussions. Consequently, aligning these models with humanethics and preferences is an essential step toward ensuring their responsibleand effective deployment in real-world applications. Prior research hasprimarily employed Reinforcement Learning from Human Feedback (RLHF) as a meansof addressing this problem, wherein generative models are fine-tuned using RLalgorithms guided by a human-feedback-informed reward model. However, theinefficiencies and instabilities associated with RL algorithms frequentlypresent substantial obstacles to the successful alignment of generative models,necessitating the development of a more robust and streamlined approach. Tothis end, we introduce a new framework, Reward rAnked FineTuning (RAFT),designed to align generative models more effectively. Utilizing a reward modeland a sufficient number of samples, our approach selects the high-qualitysamples, discarding those that exhibit undesired behavior, and subsequentlyassembles a streaming dataset. This dataset serves as the basis for aligningthe generative model and can be employed under both offline and onlinesettings. Notably, the sample generation process within RAFT is gradient-free,rendering it compatible with black-box generators. Through extensiveexperiments, we demonstrate that our proposed algorithm exhibits strongperformance in the context of both large language models and diffusion models.", "output": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT (GPT) has become one of the most talked-about innovations in recentyears, with over 100 million users worldwide. However, there is still limitedknowledge about the sources of information GPT utilizes. As a result, wecarried out a study focusing on the sources of information within the field ofenvironmental science. In our study, we asked GPT to identify the ten mostsignificant subdisciplines within the field of environmental science. We thenasked it to compose a scientific review article on each subdiscipline,including 25 references. We proceeded to analyze these references, focusing onfactors such as the number of citations, publication date, and the journal inwhich the work was published. Our findings indicate that GPT tends to citehighly-cited publications in environmental science, with a median citationcount of 1184.5. It also exhibits a preference for older publications, with amedian publication year of 2010, and predominantly refers to well-respectedjournals in the field, with Nature being the most cited journal by GPT.Interestingly, our findings suggest that GPT seems to exclusively rely oncitation count data from Google Scholar for the works it cites, rather thanutilizing citation information from other scientific databases such as Web ofScience or Scopus. In conclusion, our study suggests that Google Scholarcitations play a significant role as a predictor for mentioning a study inGPT-generated content. This finding reinforces the dominance of Google Scholaramong scientific databases and perpetuates the Matthew Effect in science, wherethe rich get richer in terms of citations. With many scholars already utilizingGPT for literature review purposes, we can anticipate further disparities andan expanding gap between lesser-cited and highly-cited publications.", "output": "ChatGPT cites the most-cited articles and journals, relying solely on Google Scholar's citation counts. As a result, AI may amplify the Matthew Effect in environmental science."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large pre-trained models, also known as foundation models (FMs), are trainedin a task-agnostic manner on large-scale data and can be adapted to a widerange of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.Despite their successes in language and vision tasks, we have yet seen anattempt to develop foundation models for geospatial artificial intelligence(GeoAI). In this work, we explore the promises and challenges of developingmultimodal foundation models for GeoAI. We first investigate the potential ofmany existing FMs by testing their performances on seven tasks across multiplegeospatial subdomains including Geospatial Semantics, Health Geography, UrbanGeography, and Remote Sensing. Our results indicate that on several geospatialtasks that only involve text modality such as toponym recognition, locationdescription recognition, and US state-level/county-level dementia time seriesforecasting, these task-agnostic LLMs can outperform task-specificfully-supervised models in a zero-shot or few-shot learning setting. However,on other geospatial tasks, especially tasks that involve multiple datamodalities (e.g., POI-based urban function classification, street viewimage-based urban noise intensity classification, and remote sensing imagescene classification), existing foundation models still underperformtask-specific models. Based on these observations, we propose that one of themajor challenges of developing a FM for GeoAI is to address the multimodalitynature of geospatial tasks. After discussing the distinct challenges of eachgeospatial data modality, we suggest the possibility of a multimodal foundationmodel which can reason over various types of geospatial data through geospatialalignments. We conclude this paper by discussing the unique risks andchallenges to develop such a model for GeoAI.", "output": "On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel, physically-constrained and differentiable approach forthe generation of D-dimensional qudit states via spontaneous parametricdown-conversion (SPDC) in quantum optics. We circumvent any limitations imposedby the inherently stochastic nature of the physical process and incorporate aset of stochastic dynamical equations governing its evolution under the SPDCHamiltonian. We demonstrate the effectiveness of our model through the designof structured nonlinear photonic crystals (NLPCs) and shaped pump beams; andshow, theoretically and experimentally, how to generate maximally entangledstates in the spatial degree of freedom. The learning of NLPC structures offersa promising new avenue for shaping and controlling arbitrary quantum states andenables all-optical coherent control of the generated states. We believe thatthis approach can readily be extended from bulky crystals to thin Metasurfacesand potentially applied to other quantum systems sharing a similar Hamiltonianstructures, such as superfluids and superconductors.", "output": "Designing Nonlinear Photonic Crystals for High-Dimensional Quantum State Engineering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Out-of-distribution (OOD) detection aims to identify test examples that donot belong to the training distribution and are thus unlikely to be predictedreliably. Despite a plethora of existing works, most of them focused only onthe scenario where OOD examples come from semantic shift (e.g., unseencategories), ignoring other possible causes (e.g., covariate shift). In thispaper, we present a novel, unifying framework to study OOD detection in abroader scope. Instead of detecting OOD examples from a particular cause, wepropose to detect examples that a deployed machine learning model (e.g., animage classifier) is unable to predict correctly. That is, whether a testexample should be detected and rejected or not is ``model-specific''. We showthat this framework unifies the detection of OOD examples caused by semanticshift and covariate shift, and closely addresses the concern of applying amachine learning model to uncontrolled environments. We provide an extensiveanalysis that involves a variety of models (e.g., different architectures andtraining strategies), sources of OOD examples, and OOD detection approaches,and reveal several insights into improving and understanding OOD detection inuncontrolled environments.", "output": "Unified Out-Of-Distribution Detection: A Model-Specific Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integrating whole-slide images (WSIs) and bulk transcriptomics for predictingpatient survival can improve our understanding of patient prognosis. However,this multimodal task is particularly challenging due to the different nature ofthese data: WSIs represent a very high-dimensional spatial description of atumor, while bulk transcriptomics represent a global description of geneexpression levels within that tumor. In this context, our work aims to addresstwo key challenges: (1) how can we tokenize transcriptomics in a semanticallymeaningful and interpretable way?, and (2) how can we capture dense multimodalinteractions between these two modalities? Specifically, we propose to learnbiological pathway tokens from transcriptomics that can encode specificcellular functions. Together with histology patch tokens that encode thedifferent morphological patterns in the WSI, we argue that they formappropriate reasoning units for downstream interpretability analyses. Wepropose fusing both modalities using a memory-efficient multimodal Transformerthat can model interactions between pathway and histology patch tokens. Ourproposed model, SURVPATH, achieves state-of-the-art performance when evaluatedagainst both unimodal and multimodal baselines on five datasets from The CancerGenome Atlas. Our interpretability framework identifies key multimodalprognostic factors, and, as such, can provide valuable insights into theinteraction between genotype and phenotype, enabling a deeper understanding ofthe underlying biological mechanisms at play. We make our code public at:", "output": "Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic Graph Neural Networks (DGNNs) are becoming increasingly popular dueto their effectiveness in analyzing and predicting the evolution of complexinterconnected graph-based systems. However, hardware deployment of DGNNs stillremains a challenge. First, DGNNs do not fully utilize hardware resourcesbecause temporal data dependencies cause low hardware parallelism.Additionally, there is currently a lack of generic DGNN hardware acceleratorframeworks, and existing GNN accelerator frameworks have limited ability tohandle dynamic graphs with changing topologies and node features. To addressthe aforementioned challenges, in this paper, we propose DGNN-Booster, which isa novel Field-Programmable Gate Array (FPGA) accelerator framework forreal-time DGNN inference using High-Level Synthesis (HLS). It includes twodifferent FPGA accelerator designs with different dataflows that can supportthe most widely used DGNNs. We showcase the effectiveness of our designs byimplementing and evaluating two representative DGNN models on ZCU102 board andmeasuring the end-to-end performance. The experiment results demonstrate thatDGNN-Booster can achieve a speedup of up to 5.6x compared to the CPU baseline(6226R), 8.4x compared to the GPU baseline (A6000) and 2.1x compared to theFPGA baseline without applying optimizations proposed in this paper. Moreover,DGNN-Booster can achieve over 100x and over 1000x runtime energy efficiencythan the CPU and GPU baseline respectively. Our implementation code andon-board measurements are publicly available at", "output": "DGNN-Booster: A Generic FPGA Accelerator Framework For Dynamic Graph Neural Network Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although multi-task deep neural network (DNN) models have computation andstorage benefits over individual single-task DNN models, they can be furtheroptimized via model compression. Numerous structured pruning methods arealready developed that can readily achieve speedups in single-task models, butthe pruning of multi-task networks has not yet been extensively studied. Inthis work, we investigate the effectiveness of structured pruning on multi-taskmodels. We use an existing single-task filter pruning criterion and alsointroduce an MTL-based filter pruning criterion for estimating the filterimportance scores. We prune the model using an iterative pruning strategy withboth pruning methods. We show that, with careful hyper-parameter tuning,architectures obtained from different pruning methods do not have significantdifferences in their performances across tasks when the number of parameters issimilar. We also show that iterative structure pruning may not be the best wayto achieve a well-performing pruned model because, at extreme pruning levels,there is a high drop in performance across all tasks. But when the same modelsare randomly initialized and re-trained, they show better results.", "output": "Structured Pruning for Multi-Task Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robots operating in real-world environments must reason about possibleoutcomes of stochastic actions and make decisions based on partial observationsof the true world state. A major challenge for making accurate and robustaction predictions is the problem of confounding, which if left untreated canlead to prediction errors. The partially observable Markov decision process(POMDP) is a widely-used framework to model these stochastic andpartially-observable decision-making problems. However, due to a lack ofexplicit causal semantics, POMDP planning methods are prone to confounding biasand thus in the presence of unobserved confounders may produce underperformingpolicies. This paper presents a novel causally-informed extension of \"anytimeregularized determinized sparse partially observable tree\" (AR-DESPOT), amodern anytime online POMDP planner, using causal modelling and inference toeliminate errors caused by unmeasured confounder variables. We further proposea method to learn offline the partial parameterisation of the causal model forplanning, from ground truth model data. We evaluate our methods on a toyproblem with an unobserved confounder and show that the learned causal model ishighly accurate, while our planning method is more robust to confounding andproduces overall higher performing policies than AR-DESPOT.", "output": "CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a sampling-based strategy synthesis algorithm fornondeterministic hybrid systems with complex continuous dynamics under temporaland reachability constraints. We view the evolution of the hybrid system as atwo-player game, where the nondeterminism is an adversarial player whoseobjective is to prevent achieving temporal and reachability goals. The aim isto synthesize a winning strategy -- a reactive (robust) strategy thatguarantees the satisfaction of the goals under all possible moves of theadversarial player. The approach is based on growing a (search) game-tree inthe hybrid space by combining a sampling-based planning method with a novelbandit-based technique to select and improve on partial strategies. We provideconditions under which the algorithm is probabilistically complete, i.e., if awinning strategy exists, the algorithm will almost surely find it. The casestudies and benchmark results show that the algorithm is general andconsistently outperforms the state of the art.", "output": "Sampling-based Reactive Synthesis for Nondeterministic Hybrid Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unmanned Aerial Vehicles (UAVs), specifically drones equipped with remotesensing object detection technology, have rapidly gained a broad spectrum ofapplications and emerged as one of the primary research focuses in the field ofcomputer vision. Although UAV remote sensing systems have the ability to detectvarious objects, small-scale objects can be challenging to detect reliably dueto factors such as object size, image degradation, and real-time limitations.To tackle these issues, a real-time object detection algorithm (YOLO-Drone) isproposed and applied to two new UAV platforms as well as a specific lightsource (silicon-based golden LED). YOLO-Drone presents several novelties: 1)including a new backbone Darknet59; 2) a new complex feature aggregation moduleMSPP-FPN that incorporated one spatial pyramid pooling and three atrous spatialpyramid pooling modules; 3) and the use of Generalized Intersection over Union(GIoU) as the loss function. To evaluate performance, two benchmark datasets,UAVDT and VisDrone, along with one homemade dataset acquired at night undersilicon-based golden LEDs, are utilized. The experimental results show that, inboth UAVDT and VisDrone, the proposed YOLO-Drone outperforms state-of-the-art(SOTA) object detection methods by improving the mAP of 10.13% and 8.59%,respectively. With regards to UAVDT, the YOLO-Drone exhibits both highreal-time inference speed of 53 FPS and a maximum mAP of 34.04%. Notably,YOLO-Drone achieves high performance under the silicon-based golden LEDs, witha mAP of up to 87.71%, surpassing the performance of YOLO series under ordinarylight sources. To conclude, the proposed YOLO-Drone is a highly effectivesolution for object detection in UAV applications, particularly for nightdetection tasks where silicon-based golden light LED technology exhibitssignificant superiority.", "output": "YOLO-Drone:Airborne real-time detection of dense small objects from high-altitude perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We tackle the issue of generalized category discovery (GCD). GCD considersthe open-world problem of automatically clustering a partially labelleddataset, in which the unlabelled data contain instances from novel categoriesand also the labelled classes. In this paper, we address the GCD problemwithout a known category number in the unlabelled data. We propose a framework,named CiPR, to bootstrap the representation by exploiting Cross-instancePositive Relations for contrastive learning in the partially labelled datawhich are neglected in existing methods. First, to obtain reliablecross-instance relations to facilitate the representation learning, weintroduce a semi-supervised hierarchical clustering algorithm, named selectiveneighbor clustering (SNC), which can produce a clustering hierarchy directlyfrom the connected components in the graph constructed by selective neighbors.We also extend SNC to be capable of label assignment for the unlabelledinstances with the given class number. Moreover, we present a method toestimate the unknown class number using SNC with a joint reference scoreconsidering clustering indexes of both labelled and unlabelled data. Finally,we thoroughly evaluate our framework on public generic image recognitiondatasets and challenging fine-grained datasets, all establishing the newstate-of-the-art.", "output": "CiPR: An Efficient Framework with Cross-instance Positive Relations for Generalized Category Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sparse training is emerging as a promising avenue for reducing thecomputational cost of training neural networks. Several recent studies haveproposed pruning methods using learnable thresholds to efficiently explore thenon-uniform distribution of sparsity inherent within the models. In this paper,we propose Gradient Annealing (GA), where gradients of masked weights arescaled down in a non-linear manner. GA provides an elegant trade-off betweensparsity and accuracy without the need for additional sparsity-inducingregularization. We integrated GA with the latest learnable pruning methods tocreate an automated sparse training algorithm called AutoSparse, which achievesbetter accuracy and/or training/inference FLOPS reduction than existinglearnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K:AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS forResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperformssparse-to-sparse SotA method MEST (uniform sparsity) for 80% sparse ResNet50with similar accuracy, where MEST uses 12% more training FLOPS and 50% moreinference FLOPS.", "output": "AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prompt engineering and calibration make large language models excel atreasoning tasks, including multiple choice commonsense reasoning. From apractical perspective, we investigate and evaluate these strategies on smallerlanguage models. Through experiments on five commonsense reasoning benchmarks,we find that each strategy favors certain models, but their joint effects aremostly negative.", "output": "Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Depth Estimation has wide reaching applications in the field of Computervision such as target tracking, augmented reality, and self-driving cars. Thegoal of Monocular Depth Estimation is to predict the depth map, given a 2Dmonocular RGB image as input. The traditional depth estimation methods arebased on depth cues and used concepts like epipolar geometry. With theevolution of Convolutional Neural Networks, depth estimation has undergonetremendous strides. In this project, our aim is to explore possible extensionsto existing SoTA Deep Learning based Depth Estimation Models and to see whetherperformance metrics could be further improved. In a broader sense, we arelooking at the possibility of implementing Pose Estimation, Efficient Sub-PixelConvolution Interpolation, Semantic Segmentation Estimation techniques tofurther enhance our proposed architecture and to provide fine-grained and moreglobally coherent depth map predictions. We also plan to do away with cameraintrinsic parameters during training and apply weather augmentations to furthergeneralize our model.", "output": "Self-Supervised Learning based Depth Estimation from Monocular Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal heterogeneous information network (temporal HIN) embedding, aimingto represent various types of nodes of different timestamps into lowdimensional spaces while preserving structural and semantic information, is ofvital importance in diverse real-life tasks. Researchers have made greatefforts on temporal HIN embedding in Euclidean spaces and got some considerableachievements. However, there is always a fundamental conflict that manyreal-world networks show hierarchical property and power-law distribution, andare not isometric of Euclidean spaces. Recently, representation learning inhyperbolic spaces has been proved to be valid for data with hierarchical andpower-law structure. Inspired by this character, we propose a hyperbolicheterogeneous temporal network embedding (H2TNE) model for temporal HINs.Specifically, we leverage a temporally and heterogeneously double-constrainedrandom walk strategy to capture the structural and semantic information, andthen calculate the embedding by exploiting hyperbolic distance in proximitymeasurement. Experimental results show that our method has superior performanceon temporal link prediction and node classification compared with SOTA models.", "output": "H2TNE: Temporal Heterogeneous Information Network Embedding in Hyperbolic Spaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text simplification (TS) is the process of generating easy-to-understandsentences from a given sentence or piece of text. The aim of TS is to reduceboth the lexical (which refers to vocabulary complexity and meaning) andsyntactic (which refers to the sentence structure) complexity of a given textor sentence without the loss of meaning or nuance. In this paper, we presenttextsc{SimpLex}, a novel simplification architecture for generating simplifiedEnglish sentences. To generate a simplified sentence, the proposed architectureuses either word embeddings (i.e., Word2Vec) and perplexity, or sentencetransformers (i.e., BERT, RoBERTa, and GPT2) and cosine similarity. Thesolution is incorporated into a user-friendly and simple-to-use software. Weevaluate our system using two metrics, i.e., SARI, and Perplexity Decrease.Experimentally, we observe that the transformer models outperform the othermodels in terms of the SARI score. However, in terms of Perplexity, theWord-Embeddings-based models achieve the biggest decrease. Thus, the maincontributions of this paper are: (1) We propose a new Word Embedding andTransformer based algorithm for text simplification; (2) We designtextsc{SimpLex} -- a modular novel text simplification system -- that canprovide a baseline for further research; and (3) We perform an in-depthanalysis of our solution and compare our results with two state-of-the-artmodels, i.e., LightLS [19] and NTS-w2v [44]. We also make the code publiclyavailable online.", "output": "SimpLex: a lexical text simplification architecture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Convolutional Network (GCN) with the powerful capacity to exploregraph-structural data has gained noticeable success in recent years.Nonetheless, most of the existing GCN-based models suffer from the notoriousover-smoothing issue, owing to which shallow networks are extensively adopted.This may be problematic for complex graph datasets because a deeper GCN shouldbe beneficial to propagating information across remote neighbors. Recent workshave devoted effort to addressing over-smoothing problems, includingestablishing residual connection structure or fusing predictions frommulti-layer models. Because of the indistinguishable embeddings from deeplayers, it is reasonable to generate more reliable predictions beforeconducting the combination of outputs from various layers. In light of this, wepropose an Alternating Graph-regularized Neural Network (AGNN) composed ofGraph Convolutional Layer (GCL) and Graph Embedding Layer (GEL). GEL is derivedfrom the graph-regularized optimization containing Laplacian embedding term,which can alleviate the over-smoothing problem by periodic projection from thelow-order feature space onto the high-order space. With more distinguishablefeatures of distinct layers, an improved Adaboost strategy is utilized toaggregate outputs from each layer, which explores integrated embeddings ofmulti-hop neighbors. The proposed model is evaluated via a large number ofexperiments including performance comparison with some multi-layer ormulti-order graph neural networks, which reveals the superior performanceimprovement of AGNN compared with state-of-the-art models.", "output": "AGNN: Alternating Graph-Regularized Neural Networks to Alleviate Over-Smoothing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Long-term stability is a critical property for deep learning-baseddata-driven digital twins of the Earth system. Such data-driven digital twinsenable sub-seasonal and seasonal predictions of extreme environmental events,probabilistic forecasts, that require a large number of ensemble members, andcomputationally tractable high-resolution Earth system models where expensivecomponents of the models can be replaced with cheaper data-driven surrogates.Owing to computational cost, physics-based digital twins, though long-termstable, are intractable for real-time decision-making. Data-driven digitaltwins offer a cheaper alternative to them and can provide real-timepredictions. However, such digital twins can only provide short-term forecastsaccurately since they become unstable when time-integrated beyond 20 days.Currently, the cause of the instabilities is unknown, and the methods that areused to improve their stability horizons are ad-hoc and lack rigorous theory.In this paper, we reveal that the universal causal mechanism for theseinstabilities in any turbulent flow is due to textit{spectral bias} wherein,textit{any} deep learning architecture is biased to learn only the large-scaledynamics and ignores the small scales completely. We further elucidate howturbulence physics and the absence of convergence in deep learning-basedtime-integrators amplify this bias leading to unstable error propagation.Finally, using the quasigeostrophic flow and ECMWF Reanalysis data as testcases, we bridge the gap between deep learning theory and fundamental numericalanalysis to propose one mitigative solution to such instabilities. We developlong-term stable data-driven digital twins for the climate system anddemonstrate accurate short-term forecasts, and hundreds of years of long-termstable time-integration with accurate mean and variability.", "output": "Long-term instabilities of deep learning-based digital twins of the climate system: The cause and a solution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning-based recommender systems (DRSs) are increasingly and widelydeployed in the industry, which brings significant convenience to people'sdaily life in different ways. However, recommender systems are also shown tosuffer from multiple issues,e.g., the echo chamber and the Matthew effect, ofwhich the notation of \"fairness\" plays a core role.While many fairnessnotations and corresponding fairness testing approaches have been developed fortraditional deep classification models, they are essentially hardly applicableto DRSs. One major difficulty is that there still lacks a systematicunderstanding and mapping between the existing fairness notations and thediverse testing requirements for deep recommender systems, not to mentionfurther testing or debugging activities. To address the gap, we proposeFairRec, a unified framework that supports fairness testing of DRSs frommultiple customized perspectives, e.g., model utility, item diversity, itempopularity, etc. We also propose a novel, efficient search-based testingapproach to tackle the new challenge, i.e., double-ended discrete particleswarm optimization (DPSO) algorithm, to effectively search for hidden fairnessissues in the form of certain disadvantaged groups from a vast number ofcandidate groups. Given the testing report, by adopting a simple re-rankingmitigation strategy on these identified disadvantaged groups, we show that thefairness of DRSs can be significantly improved. We conducted extensiveexperiments on multiple industry-level DRSs adopted by leading companies. Theresults confirm that FairRec is effective and efficient in identifying thedeeply hidden fairness issues, e.g., achieving 95% testing accuracy with halfto 1/8 time.", "output": "FairRec: Fairness Testing for Deep Recommender Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes to reduce visibility data volume using abaseline-dependent lossy compression technique that preserves smearing at theedges of the field-of-view. We exploit the relation of the rank of a matrix andthe fact that a low-rank approximation can describe the raw visibility data asa sum of basic components where each basic component corresponds to a specificFourier component of the sky distribution. As such, the entire visibility datais represented as a collection of data matrices from baselines, instead of asingle tensor. The proposed methods are formulated as follows: provided a largedataset of the entire visibility data; the first algorithm, named $simple~SVD$projects the data into a regular sampling space of rank$-r$ data matrices. Inthis space, the data for all the baselines has the same rank, which makes thecompression factor equal across all baselines. The second algorithm, named$BDSVD$ projects the data into an irregular sampling space of rank$-r_{pq}$data matrices. The subscript $pq$ indicates that the rank of the data matrixvaries across baselines $pq$, which makes the compression factorbaseline-dependent. MeerKAT and the European Very Long Baseline InterferometryNetwork are used as reference telescopes to evaluate and compare theperformance of the proposed methods against traditional methods, such astraditional averaging and baseline-dependent averaging (BDA). For the samespatial resolution threshold, both $simple~SVD$ and $BDSVD$ show effectivecompression by two-orders of magnitude higher than traditional averaging andBDA. At the same space-saving rate, there is no decrease in spatial resolutionand there is a reduction in the noise variance in the data which improves theS/N to over $1.5$ dB at the edges of the field-of-view.", "output": "Lossy Compression of Large-Scale Radio Interferometric Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper discusses the results for the second edition of the MonocularDepth Estimation Challenge (MDEC). This edition was open to methods using anyform of supervision, including fully-supervised, self-supervised, multi-task orproxy depth. The challenge was based around the SYNS-Patches dataset, whichfeatures a wide diversity of environments with high-quality dense ground-truth.This includes complex natural environments, e.g. forests or fields, which aregreatly underrepresented in current benchmarks.The challenge received eight unique submissions that outperformed theprovided SotA baseline on any of the pointcloud- or image-based metrics. Thetop supervised submission improved relative F-Score by 27.62%, while the topself-supervised improved it by 16.61%. Supervised submissions generallyleveraged large collections of datasets to improve data diversity.Self-supervised submissions instead updated the network architecture andpretrained backbones. These results represent a significant progress in thefield, while highlighting avenues for future research, such as reducinginterpolation artifacts at depth boundaries, improving self-supervised indoorperformance and overall natural image accuracy.", "output": "The Second Monocular Depth Estimation Challenge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have witnessed an exponential increase in the demand for facevideo compression, and the success of artificial intelligence has expanded theboundaries beyond traditional hybrid video coding. Generative coding approacheshave been identified as promising alternatives with reasonable perceptualrate-distortion trade-offs, leveraging the statistical priors of face videos.However, the great diversity of distortion types in spatial and temporaldomains, ranging from the traditional hybrid coding frameworks to generativemodels, present grand challenges in compressed face video quality assessment(VQA). In this paper, we introduce the large-scale Compressed Face VideoQuality Assessment (CFVQA) database, which is the first attempt tosystematically understand the perceptual quality and diversified compressiondistortions in face videos. The database contains 3,240 compressed face videoclips in multiple compression levels, which are derived from 135 source videoswith diversified content using six representative video codecs, including twotraditional methods based on hybrid coding frameworks, two end-to-end methods,and two generative methods. In addition, a FAce VideO IntegeRity (FAVOR) indexfor face video compression was developed to measure the perceptual quality,considering the distinct content characteristics and temporal priors of theface videos. Experimental results exhibit its superior performance on theproposed CFVQA dataset. The benchmark is now made publicly available at:", "output": "Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces DroidBot-GPT, a tool that utilizes GPT-like largelanguage models (LLMs) to automate the interactions with Android mobileapplications. Given a natural language description of a desired task,DroidBot-GPT can automatically generate and execute actions that navigate theapp to complete the task. It works by translating the app GUI state informationand the available actions on the smartphone screen to natural language promptsand asking the LLM to make a choice of actions. Since the LLM is typicallytrained on a large amount of data including the how-to manuals of diversesoftware applications, it has the ability to make reasonable choices of actionsbased on the provided information. We evaluate DroidBot-GPT with a self-createddataset that contains 33 tasks collected from 17 Android applications spanning10 categories. It can successfully complete 39.39% of the tasks, and theaverage partial completion progress is about 66.76%. Given the fact that ourmethod is fully unsupervised (no modification required from both the app andthe LLM), we believe there is great potential to enhance automation performancewith better app development paradigms and/or custom model training.", "output": "DroidBot-GPT: GPT-powered UI Automation for Android."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning on knowledge graphs is a challenging task because it utilizesobserved information to predict the missing one. Specifically, answeringfirst-order logic formulas is of particular interest because of its clearsyntax and semantics. Recently, the query embedding method has been proposedwhich learns the embedding of a set of entities and treats logic operations asset operations. Though there has been much research following the samemethodology, it lacks a systematic inspection from the standpoint of logic. Inthis paper, we characterize the scope of queries investigated previously andprecisely identify the gap between it and the whole family of existentialformulas. Moreover, we develop a new dataset containing ten new formulas anddiscuss the new challenges coming simultaneously. Finally, we propose a newsearch algorithm from fuzzy logic theory which is capable of solving newformulas and outperforming the previous methods in existing formulas.", "output": "On Existential First Order Queries Inference on Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Entity alignment (EA) aims to find equivalent entities in different knowledgegraphs (KGs). State-of-the-art EA approaches generally use Graph NeuralNetworks (GNNs) to encode entities. However, most of them train the models andevaluate the results in a fullbatch fashion, which prohibits EA from beingscalable on largescale datasets. To enhance the usability of GNN-based EAmodels in real-world applications, we present SEA, a scalable entity alignmentsystem that enables to (i) train large-scale GNNs for EA, (ii) speed up thenormalization and the evaluation process, and (iii) report clear results forusers to estimate different models and parameter settings. SEA can be run on acomputer with merely one graphic card. Moreover, SEA encompasses sixstate-of-the-art EA models and provides access for users to quickly establishand evaluate their own models. Thus, SEA allows users to perform EA withoutbeing involved in tedious implementations, such as negative sampling andGPU-accelerated evaluation. With SEA, users can gain a clear view of the modelperformance. In the demonstration, we show that SEA is user-friendly and is ofhigh scalability even on computers with limited computational resources.", "output": "SEA: A Scalable Entity Alignment System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Structured reconstruction is a non-trivial dense prediction problem, whichextracts structural information (eg, building corners and edges) from a rasterimage, then reconstructs it to a 2D planar graph accordingly. Compared withcommon segmentation or detection problems, it significantly relays on thecapability that leveraging holistic geometric information for structuralreasoning. Current transformer-based approaches tackle this challenging problemin a two-stage manner, which detect corners in the first model and classify theproposed edges (corner-pairs) in the second model. However, they separatetwo-stage into different models and only share the backbone encoder. Unlike theexisting modeling strategies, we present an enhanced corner representationmethod: 1) It fuses knowledge between the corner detection and edge predictionby sharing feature in different granularity; 2) Corner candidates are proposedin four heatmap channels w.r.t its direction. Both qualitative and quantitativeevaluations demonstrate that our proposed method can better reconstructfine-grained structures, such as adjacent corners and tiny edges. Consequently,it outperforms the state-of-the-art model by +1.9%@F-1 on Corner and+3.0%@F-1 on Edge.", "output": "CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic and periodic recompiling of building databases with up-to-datehigh-resolution images has become a critical requirement for rapidly developingurban environments. However, the architecture of most existing approaches forchange extraction attempts to learn features related to changes but ignoresobjectives related to buildings. This inevitably leads to the generation ofsignificant pseudo-changes, due to factors such as seasonal changes in imagesand the inclination of building fac{c}ades. To alleviate the above-mentionedproblems, we developed a contrastive learning approach by validating historicalbuilding footprints against single up-to-date remotely sensed images. Thiscontrastive learning strategy allowed us to inject the semantics of buildingsinto a pipeline for the detection of changes, which is achieved by increasingthe distinguishability of features of buildings from those of non-buildings. Inaddition, to reduce the effects of inconsistencies between historical buildingpolygons and buildings in up-to-date images, we employed a deformableconvolutional neural network to learn offsets intuitively. In summary, weformulated a multi-branch building extraction method that identifies newlyconstructed and removed buildings, respectively. To validate our method, weconducted comparative experiments using the public Wuhan University buildingchange detection dataset and a more practical dataset named SI-BU that weestablished. Our method achieved F1 scores of 93.99% and 70.74% on the abovedatasets, respectively. Moreover, when the data of the public dataset weredivided in the same manner as in previous related studies, our method achievedan F1 score of 94.63%, which surpasses that of the state-of-the-art method.", "output": "BCE-Net: Reliable Building Footprints Change Extraction based on Historical Map and Up-to-Date Images using Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual reasoning is a long-term goal of vision research. In the last decade,several works have attempted to apply deep neural networks (DNNs) to the taskof learning visual relations from images, with modest results in terms of thegeneralization of the relations learned. In recent years, several innovationsin DNNs have been developed in order to enable learning abstract relation fromimages. In this work, we systematically evaluate a series of DNNs thatintegrate mechanism such as slot attention, recurrently guided attention, andexternal memory, in the simplest possible visual reasoning task: decidingwhether two objects are the same or different. We found that, although somemodels performed better than others in generalizing the same-different relationto specific types of images, no model was able to generalize this relationacross the board. We conclude that abstract visual reasoning remains largely anunresolved challenge for DNNs.", "output": "The role of object-centric representations, guided attention, and external memory on generalizing visual relations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper summarizes our contributions to the document-grounded dialog tasksat the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). Inboth iterations the task consists of three subtasks: first detect whether thecurrent turn is knowledge seeking, second select a relevant knowledge document,and third generate a response grounded on the selected document. For DSTC9 weproposed different approaches to make the selection task more efficient. Thebest method, Hierarchical Selection, actually improves the results compared tothe original baseline and gives a speedup of 24x. In the DSTC10 iteration ofthe task, the challenge was to adapt systems trained on written dialogs toperform well on noisy automatic speech recognition transcripts. Therefore, weproposed data augmentation techniques to increase the robustness of the modelsas well as methods to adapt the style of generated responses to fit well intothe proceeding dialog. Additionally, we proposed a noisy channel model thatallows for increasing the factuality of the generated responses. In addition tosummarizing our previous contributions, in this work, we also report on a fewsmall improvements and reconsider the automatic evaluation metrics for thegeneration task which have shown a low correlation to human judgments.", "output": "Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point clouds are widely regarded as one of the best dataset types for urbanmapping purposes. Hence, point cloud datasets are commonly investigated asbenchmark types for various urban interpretation methods. Yet, few researchershave addressed the use of point cloud benchmarks for fac{c}ade segmentation.Robust fac{c}ade segmentation is becoming a key factor in various applicationsranging from simulating autonomous driving functions to preserving culturalheritage. In this work, we present a method of enriching existing point clouddatasets with fac{c}ade-related classes that have been designed to facilitatefac{c}ade segmentation testing. We propose how to efficiently extend existingdatasets and comprehensively assess their potential for fac{c}adesegmentation. We use the method to create the TUM-FAc{C}ADE dataset, whichextends the capabilities of TUM-MLS-2016. Not only can TUM-FAc{C}ADEfacilitate the development of point-cloud-based fac{c}ade segmentation tasks,but our procedure can also be applied to enrich further datasets.", "output": "TUM-FA\\c{C}ADE: Reviewing and enriching point cloud benchmarks for fa\\c{c}ade segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Speech separation remains an important area of multi-speaker signalprocessing. Deep neural network (DNN) models have attained the best performanceon many speech separation benchmarks. Some of these models can take significanttime to train and have high memory requirements. Previous work has proposedshortening training examples to address these issues but the impact of this onmodel performance is not yet well understood. In this work, the impact ofapplying these training signal length (TSL) limits is analysed for two speechseparation models: SepFormer, a transformer model, and Conv-TasNet, aconvolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysedin terms of signal length distribution and its impact on training efficiency.It is demonstrated that, for specific distributions, applying specific TSLlimits results in better performance. This is shown to be mainly due torandomly sampling the start index of the waveforms resulting in more uniqueexamples for training. A SepFormer model trained using a TSL limit of 4.42s anddynamic mixing (DM) is shown to match the best-performing SepFormer modeltrained with DM and unlimited signal lengths. Furthermore, the 4.42s TSL limitresults in a 44% reduction in training time with WHAMR.", "output": "On Data Sampling Strategies for Training Neural Network Speech Separation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The car-following (CF) model is the core component for traffic simulationsand has been built-in in many production vehicles with Advanced DrivingAssistance Systems (ADAS). Research of CF behavior allows us to identify thesources of different macro phenomena induced by the basic process of pairwisevehicle interaction. The CF behavior and control model encompasses variousfields, such as traffic engineering, physics, cognitive science, machinelearning, and reinforcement learning. This paper provides a comprehensivesurvey highlighting differences, complementarities, and overlaps among variousCF models according to their underlying logic and principles. We reviewedrepresentative algorithms, ranging from the theory-based kinematic models,stimulus-response models, and cruise control models to data-driven BehaviorCloning (BC) and Imitation Learning (IL) and outlined their strengths andlimitations. This review categorizes CF models that are conceptualized invarying principles and summarize the vast literature with a holistic framework.", "output": "A Review on Longitudinal Car-Following Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A key challenge for a reinforcement learning (RL) agent is to incorporateexternal/expert1 advice in its learning. The desired goals of an algorithm thatcan shape the learning of an RL agent with external advice include (a)maintaining policy invariance; (b) accelerating the learning of the agent; and(c) learning from arbitrary advice [3]. To address this challenge this paperformulates the problem of incorporating external advice in RL as a multi-armedbandit called shaping-bandits. The reward of each arm of shaping banditscorresponds to the return obtained by following the expert or by following adefault RL algorithm learning on the true environment reward.We show thatdirectly applying existing bandit and shaping algorithms that do not reasonabout the non-stationary nature of the underlying returns can lead to poorresults. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES(LPIES) three different shaping algorithms built on different assumptions thatreason about the long-term consequences of following the expert policy or thedefault RL algorithm. Our experiments in four different settings show thatthese proposed algorithms achieve the above-mentioned goals whereas the otheralgorithms fail to do so.", "output": "Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "GPT-3 and several other language models (LMs) can effectively address variousnatural language processing (NLP) tasks, including machine translation and textsummarization. Recently, they have also been successfully employed in thebusiness process management (BPM) domain, e.g., for predictive processmonitoring and process extraction from text. This, however, typically requiresfine-tuning the employed LM, which, among others, necessitates large amounts ofsuitable training data. A possible solution to this problem is the use ofprompt engineering, which leverages pre-trained LMs without fine-tuning them.Recognizing this, we argue that prompt engineering can help bring thecapabilities of LMs to BPM research. We use this position paper to develop aresearch agenda for the use of prompt engineering for BPM research byidentifying the associated potentials and challenges.", "output": "Just Tell Me: Prompt Engineering in Business Process Management."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the last few years, we have not seen any major developments inmodel-free or model-based learning methods that would make one obsoleterelative to the other. In most cases, the used technique is heavily dependenton the use case scenario or other attributes, e.g. the environment. Bothapproaches have their own advantages, for example, sample efficiency orcomputational efficiency. However, when combining the two, the advantages ofeach can be combined and hence achieve better performance. The TD-MPC frameworkis an example of this approach. On the one hand, a world model in combinationwith model predictive control is used to get a good initial estimate of thevalue function. On the other hand, a Q function is used to provide a goodlong-term estimate. Similar to algorithms like MuZero a latent staterepresentation is used, where only task-relevant information is encoded toreduce the complexity. In this paper, we propose the use of a reconstructionfunction within the TD-MPC framework, so that the agent can reconstruct theoriginal observation given the internal state representation. This allows ouragent to have a more stable learning signal during training and also improvessample efficiency. Our proposed addition of another loss term leads to improvedperformance on both state- and image-based tasks from the DeepMind-Controlsuite.", "output": "Model Predictive Control with Self-supervised Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this technical report, we evaluated the performance of the ChatGPT andGPT-3 models for the task of vulnerability detection in code. Our evaluationwas conducted on our real-world dataset, using binary and multi-labelclassification tasks on CWE vulnerabilities. We decided to evaluate the modelbecause it has shown good performance on other code-based tasks, such assolving programming challenges and understanding code at a high level. However,we found that the ChatGPT model performed no better than a dummy classifier forboth binary and multi-label classification tasks for code vulnerabilitydetection.", "output": "Evaluation of ChatGPT Model for Vulnerability Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The localization of objects is a crucial task in various applications such asrobotics, virtual and augmented reality, and the transportation of goods inwarehouses. Recent advances in deep learning have enabled the localizationusing monocular visual cameras. While structure from motion (SfM) predicts theabsolute pose from a point cloud, absolute pose regression (APR) methods learna semantic understanding of the environment through neural networks. However,both fields face challenges caused by the environment such as motion blur,lighting changes, repetitive patterns, and feature-less structures. This studyaims to address these challenges by incorporating additional information andregularizing the absolute pose using relative pose regression (RPR) methods.The optical flow between consecutive images is computed using the Lucas-Kanadealgorithm, and the relative pose is predicted using an auxiliary smallrecurrent convolutional network. The fusion of absolute and relative poses is acomplex task due to the mismatch between the global and local coordinatesystems. State-of-the-art methods fusing absolute and relative poses use posegraph optimization (PGO) to regularize the absolute pose predictions usingrelative poses. In this work, we propose recurrent fusion networks to optimallyalign absolute and relative pose predictions to improve the absolute poseprediction. We evaluate eight different recurrent units and construct asimulation environment to pre-train the APR and RPR networks for bettergeneralized training. Additionally, we record a large database of differentscenarios in a challenging large-scale indoor environment that mimics awarehouse with transportation robots. We conduct hyperparameter searches andexperiments to show the effectiveness of our recurrent fusion method comparedto PGO.", "output": "Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models pre-trained on large self-supervised corpora, followed bytask-specific fine-tuning has become the dominant paradigm in NLP. Thesepre-training datasets often have a one-to-many structure--e.g. in dialoguethere are many valid responses for a given context. However, only some of theseresponses will be desirable in our downstream task. This raises the question ofhow we should train the model such that it can emulate the desirablebehaviours, but not the undesirable ones. Current approaches train in aone-to-one setup--only a single target response is given for a single dialoguecontext--leading to models only learning to predict the average response, whileignoring the full range of possible responses. Using text-based games as atestbed, our approach, PASA, uses discrete latent variables to capture therange of different behaviours represented in our larger pre-training dataset.We then use knowledge distillation to distil the posterior probabilitydistribution into a student model. This probability distribution is far richerthan learning from only the hard targets of the dataset, and thus allows thestudent model to benefit from the richer range of actions the teacher model haslearned. Results show up to 49% empirical improvement over the previousstate-of-the-art model on the Jericho Walkthroughs dataset.", "output": "Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of synthetically generating data that can closelyresemble human decisions made in the context of an interactive human-AI systemlike a computer game. We propose a novel algorithm that can generate synthetic,human-like, decision making data while starting from a very small set ofdecision making data collected from humans. Our proposed algorithm integratesthe concept of reward shaping with an imitation learning algorithm to generatethe synthetic data. We have validated our synthetic data generation techniqueby using the synthetically generated data as a surrogate for human interactiondata to solve three sequential decision making tasks of increasing complexitywithin a small computer game-like setup. Different empirical and statisticalanalyses of our results show that the synthetically generated data cansubstitute the human data and perform the game-playing tasks almostindistinguishably, with very low divergence, from a human performing the sametasks.", "output": "Synthetically Generating Human-like Data for Sequential Decision Making Tasks via Reward-Shaped Imitation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Combinatorial optimization problems (COPs) are an important research topic invarious fields. In recent times, there have been many attempts to solve COPsusing deep learning-based approaches. We propose a novel neural network modelthat solves COPs involving geometry based on self-attention and a new attentionmechanism. The proposed model is designed such that the model efficientlylearns point-to-point relationships in COPs involving geometry usingself-attention in the encoder. We propose efficient input and output sequenceordering methods that reduce ambiguities such that the model learns thesequences more regularly and effectively. Geometric COPs involve geometricrequirements that need to be satisfied. In the decoder, a new masking schemeusing domain knowledge is proposed to provide a high penalty when the geometricrequirement of the problem is not satisfied. The proposed neural net is aflexible framework that can be applied to various COPs involving geometry. Weconduct experiments to demonstrate the effectiveness of the proposed model forthree COPs involving geometry: Delaunay triangulation, convex hull, and theplanar Traveling Salesman problem. Our experimental results show that theproposed model exhibits competitive performance in finding approximatesolutions for solving these problems.", "output": "Learning Geometric Combinatorial Optimization Problems using Self-attention and Domain Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The traditional production paradigm of large batch production does not offerflexibility towards satisfying the requirements of individual customers. A newgeneration of smart factories is expected to support new multi-variety andsmall-batch customized production modes. For that, Artificial Intelligence (AI)is enabling higher value-added manufacturing by accelerating the integration ofmanufacturing and information communication technologies, including computing,communication, and control. The characteristics of a customized smart factoryare to include self-perception, operations optimization, dynamicreconfiguration, and intelligent decision-making. The AI technologies willallow manufacturing systems to perceive the environment, adapt to externalneeds, and extract the processed knowledge, including business models, such asintelligent production, networked collaboration, and extended service models.This paper focuses on the implementation of AI in customized manufacturing(CM). The architecture of an AI-driven customized smart factory is presented.Details of intelligent manufacturing devices, intelligent informationinteraction, and the construction of a flexible manufacturing line areshowcased. The state-of-the-art AI technologies of potential use in CM, i.e.,machine learning, multi-agent systems, Internet of Things, big data, andcloud-edge computing are surveyed. The AI-enabled technologies in a customizedsmart factory are validated with a case study of customized packaging. Theexperimental results have demonstrated that the AI-assisted CM offers thepossibility of higher production flexibility and efficiency. Challenges andsolutions related to AI in CM are also discussed.", "output": "Artificial Intelligence-Driven Customized Manufacturing Factory: Key Technologies, Applications, and Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, deep learning-based algorithms are widely adopted due to theadvantage of being able to establish anomaly detection models without or withminimal domain knowledge of the task. Instead, to train the artificial neuralnetwork more stable, it should be better to define the appropriate neuralnetwork structure or the loss function. For the training anomaly detectionmodel, the mean squared error (MSE) function is adopted widely. On the otherhand, the novel loss function, logarithmic mean squared error (LMSE), isproposed in this paper to train the neural network more stable. This studycovers a variety of comparisons from mathematical comparisons, visualization inthe differential domain for backpropagation, loss convergence in the trainingprocess, and anomaly detection performance. In an overall view, LMSE issuperior to the existing MSE function in terms of strongness of lossconvergence, anomaly detection performance. The LMSE function is expected to beapplicable for training not only the anomaly detection model but also thegeneral generative neural network.", "output": "Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Background: Many published machine learning studies are irreproducible.Issues with methodology and not properly accounting for variation introduced bythe algorithm themselves or their implementations are attributed as the maincontributors to the irreproducibility.Problem: There exist no theoreticalframework that relates experiment design choices to potential effects on theconclusions. Without such a framework, it is much harder for practitioners andresearchers to evaluate experiment results and describe the limitations ofexperiments. The lack of such a framework also makes it harder for independentresearchers to systematically attribute the causes of failed reproducibilityexperiments. Objective: The objective of this paper is to develop a frameworkthat enable applied data science practitioners and researchers to understandwhich experiment design choices can lead to false findings and how and by thishelp in analyzing the conclusions of reproducibility experiments. Method: Wehave compiled an extensive list of factors reported in the literature that canlead to machine learning studies being irreproducible. These factors areorganized and categorized in a reproducibility framework motivated by thestages of the scientific method. The factors are analyzed for how they canaffect the conclusions drawn from experiments. A model comparison study is usedas an example. Conclusion: We provide a framework that describes machinelearning methodology from experimental design decisions to the conclusionsinferred from them.", "output": "Sources of Irreproducibility in Machine Learning: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven, learning-based control methods offer the potential to improveoperations in complex systems, and model-free deep reinforcement learningrepresents a popular approach to data-driven control. However, existing classesof algorithms present a trade-off between two important deployment requirementsfor real-world control: (i) practical performance guarantees and (ii) dataefficiency. Off-policy algorithms make efficient use of data through samplereuse but lack theoretical guarantees, while on-policy algorithms guaranteeapproximate policy improvement throughout training but suffer from high samplecomplexity. In order to balance these competing goals, we develop a class ofGeneralized Policy Improvement algorithms that combines the policy improvementguarantees of on-policy methods with the efficiency of sample reuse. Wedemonstrate the benefits of this new class of algorithms through extensiveexperimental analysis on a variety of continuous control tasks from theDeepMind Control Suite.", "output": "Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a methodology for generating random multi-qubit stabilizer codesbased on solving a constraint satisfaction problem (CSP) on random bipartitegraphs. This framework allows us to enforce stabilizer commutation, $X/Z$balancing, finite rate, sparsity, and maximum-degree constraints simultaneouslyin a CSP that we can then solve numerically. Using a state-of-the-art CSPsolver, we obtain convincing evidence for the existence of a satisfiabilitythreshold. Furthermore, the extent of the satisfiable phase increases with thenumber of qubits. In that phase, finding sparse codes becomes an easy problem.Moreover, we observe that the sparse codes found in the satisfiable phasepractically achieve the channel capacity for erasure noise. Our results showthat intermediate-size finite-rate sparse quantum codes are easy to find, whilealso demonstrating a flexible methodology for generating good codes with customproperties. We therefore establish a complete and customizable pipeline forrandom quantum code discovery.", "output": "Finite-rate sparse quantum codes aplenty."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer-based pre-trained models like BERT have achieved great progresson Semantic Sentence Matching. Meanwhile, dependency prior knowledge has alsoshown general benefits in multiple NLP tasks. However, how to efficientlyintegrate dependency prior structure into pre-trained models to better modelcomplex semantic matching relations is still unsettled. In this paper, wepropose the textbf{D}ependency-Enhanced textbf{A}daptive textbf{F}usiontextbf{A}ttention (textbf{DAFA}), which explicitly introduces dependencystructure into pre-trained models and adaptively fuses it with semanticinformation. Specifically, textbf{emph{(i)}} DAFA first proposes astructure-sensitive paradigm to construct a dependency matrix for calibratingattention weights. It adopts an adaptive fusion module to integrate theobtained dependency information and the original semantic signals. Moreover,DAFA reconstructs the attention calculation flow and provides betterinterpretability. By applying it on BERT, our method achieves state-of-the-artor competitive performance on 10 public datasets, demonstrating the benefits ofadaptively fusing dependency structure in semantic matching task.", "output": "Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have demonstrated that natural language can be used to generateand edit 3D shapes. However, these methods generate shapes with limitedfidelity and diversity. We introduce CLIP-Sculptor, a method to address theseconstraints by producing high-fidelity and diverse 3D shapes without the needfor (text, shape) pairs during training. CLIP-Sculptor achieves this in amulti-resolution approach that first generates in a low-dimensional latentspace and then upscales to a higher resolution for improved shape fidelity. Forimproved shape diversity, we use a discrete latent space which is modeled usinga transformer conditioned on CLIP's image-text embedding space. We also presenta novel variant of classifier-free guidance, which improves theaccuracy-diversity trade-off. Finally, we perform extensive experimentsdemonstrating that CLIP-Sculptor outperforms state-of-the-art baselines. Thecode is available at ", "output": "CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In off-policy reinforcement learning, a behaviour policy performs exploratoryinteractions with the environment to obtain state-action-reward samples whichare then used to learn a target policy that optimises the expected return. Thisleads to a problem of off-policy evaluation, where one needs to evaluate thetarget policy from samples collected by the often unrelated behaviour policy.Importance sampling is a traditional statistical technique that is oftenapplied to off-policy evaluation. While importance sampling estimators areunbiased, their variance increases exponentially with the horizon of thedecision process due to computing the importance weight as a product of actionprobability ratios, yielding estimates with low accuracy for domains involvinglong-term planning. This paper proposes state-based importance sampling (SIS),which drops the action probability ratios of sub-trajectories with \"negligiblestates\" -- roughly speaking, those for which the chosen actions have no impacton the return estimate -- from the computation of the importance weight.Theoretical results demonstrate a smaller exponent for the variance upper boundas well as a lower mean squared error. To identify negligible states, twosearch algorithms are proposed, one based on covariance testing and one basedon state-action values. Using the formulation of SIS, we then analogouslyformulate state-based variants of weighted importance sampling, per-decisionimportance sampling, and incremental importance sampling based on thestate-action value identification algorithm. Moreover, we note that doublyrobust estimators may also benefit from SIS. Experiments in two gridworlddomains and one inventory management domain show that state-based methods yieldreduced variance and improved accuracy.", "output": "Low Variance Off-policy Evaluation with State-based Importance Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evolution of wireless communications into 6G and beyond is expected torely on new machine learning (ML)-based capabilities. These can enableproactive decisions and actions from wireless-network components to sustainquality-of-service (QoS) and user experience. Moreover, new use cases in thearea of vehicular and industrial communications will emerge. Specifically inthe area of vehicle communication, vehicle-to-everything (V2X) schemes willbenefit strongly from such advances. With this in mind, we have conducted adetailed measurement campaign that paves the way to a plethora of diverseML-based studies. The resulting datasets offer GPS-located wirelessmeasurements across diverse urban environments for both cellular (with twodifferent operators) and sidelink radio access technologies, thus enabling avariety of different studies towards V2X. The datasets are labeled and sampledwith a high time resolution. Furthermore, we make the data publicly availablewith all the necessary information to support the onboarding of newresearchers. We provide an initial analysis of the data showing some of thechallenges that ML needs to overcome and the features that ML can leverage, aswell as some hints at potential research studies.", "output": "Berlin V2X: A Machine Learning Dataset from Multiple Vehicles and Radio Access Technologies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose to integrate weapon system features (such as weapon systemmanufacturer, deployment time and location, storage time and location, etc.)into a parameterized Cox-Weibull [1] reliability model via a neural network,like DeepSurv [2], to improve predictive maintenance. In parallel, we developan alternative Bayesian model by parameterizing the Weibull parameters with aneural network and employing dropout methods such as Monte-Carlo (MC)-dropoutfor comparative purposes. Due to data collection procedures in weapon systemtesting we employ a novel interval-censored log-likelihood which incorporatesMonte-Carlo Markov Chain (MCMC) [3] sampling of the Weibull parameters duringgradient descent optimization. We compare classification metrics such asreceiver operator curve (ROC) area under the curve (AUC), precision-recall (PR)AUC, and F scores to show our model generally outperforms traditional powerfulmodels such as XGBoost and the current standard conditional Weibull probabilitydensity estimation model.", "output": "Bayesian Weapon System Reliability Modeling with Cox-Weibull Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) are often used for tasks involving the geometryof a given graph, such as molecular dynamics simulation. Although the distancematrix of a geometric graph contains complete geometric information, it hasbeen demonstrated that Message Passing Neural Networks (MPNNs) are insufficientfor learning this geometry. In this work, we expand on the families ofcounterexamples that MPNNs are unable to distinguish from their distancematrices, by constructing families of novel and symmetric geometric graphs. Wethen propose $k$-DisGNNs, which can effectively exploit the rich geometrycontained in the distance matrix. We demonstrate the high expressive power ofour models by proving the universality of $k$-DisGNNs for distinguishinggeometric graphs when $k geq 3$, and that some existing well-designedgeometric models can be unified by $k$-DisGNNs as special cases. Mostimportantly, we establish a connection between geometric deep learning andtraditional graph representation learning, showing that those highly expressiveGNN models originally designed for graph structure learning can also be appliedto geometric deep learning problems with impressive performance, and thatexisting complex, equivariant models are not the only solution. Experimentalresults verify our theory.", "output": "Is Distance Matrix Enough for Geometric Deep Learning?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Past work in natural language processing interpretability focused mainly onpopular classification tasks while largely overlooking generation settings,partly due to a lack of dedicated tools. In this work, we introduce Inseq, aPython library to democratize access to interpretability analyses of sequencegeneration models. Inseq enables intuitive and optimized extraction of models'internal information and feature importance scores for popular decoder-only andencoder-decoder Transformers architectures. We showcase its potential byadopting it to highlight gender biases in machine translation models and locatefactual knowledge inside GPT-2. Thanks to its extensible interface supportingcutting-edge techniques such as contrastive feature attribution, Inseq candrive future advances in explainable natural language generation, centralizinggood practices and enabling fair and reproducible model evaluations.", "output": "Inseq: An Interpretability Toolkit for Sequence Generation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rewrite systems [6, 10, 12] have been widely employing equality saturation[9], which is an optimisation methodology that uses a saturated e-graph torepresent all possible sequences of rewrite simultaneously, and then extractsthe optimal one. As such, optimal results can be achieved by avoiding thephase-ordering problem. However, we observe that when the e-graph is notsaturated, it cannot represent all possible rewrite opportunities and thereforethe phase-ordering problem is re-introduced during the construction phase ofthe e-graph. To address this problem, we propose MCTS-GEB, a domain-generalrewrite system that applies reinforcement learning (RL) to e-graphconstruction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3]to efficiently plan for the optimal e-graph construction, and therefore it caneffectively eliminate the phase-ordering problem at the construction phase andachieve better performance within a reasonable time. Evaluation in twodifferent domains shows MCTS-GEB can outperform the state-of-the-art rewritesystems by up to 49x, while the optimisation can generally take less than anhour, indicating MCTS-GEB is a promising building block for the futuregeneration of rewrite systems.", "output": "MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How will superhuman artificial intelligence (AI) affect human decisionmaking? And what will be the mechanisms behind this effect? We address thesequestions in a domain where AI already exceeds human performance, analyzingmore than 5.8 million move decisions made by professional Go players over thepast 71 years (1950-2021). To address the first question, we use a superhumanAI program to estimate the quality of human decisions across time, generating58 billion counterfactual game patterns and comparing the win rates of actualhuman decisions with those of counterfactual AI decisions. We find that humansbegan to make significantly better decisions following the advent of superhumanAI. We then examine human players' strategies across time and find that noveldecisions (i.e., previously unobserved moves) occurred more frequently andbecame associated with higher decision quality after the advent of superhumanAI. Our findings suggest that the development of superhuman AI programs mayhave prompted human players to break away from traditional strategies andinduced them to explore novel moves, which in turn may have improved theirdecision-making.", "output": "Superhuman Artificial Intelligence Can Improve Human Decision Making by Increasing Novelty."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial intelligence (AI) researchers have been developing and refininglarge language models (LLMs) that exhibit remarkable capabilities across avariety of domains and tasks, challenging our understanding of learning andcognition. The latest model developed by OpenAI, GPT-4, was trained using anunprecedented scale of compute and data. In this paper, we report on ourinvestigation of an early version of GPT-4, when it was still in activedevelopment by OpenAI. We contend that (this early version of) GPT-4 is part ofa new cohort of LLMs (along with ChatGPT and Google's PaLM for example) thatexhibit more general intelligence than previous AI models. We discuss therising capabilities and implications of these models. We demonstrate that,beyond its mastery of language, GPT-4 can solve novel and difficult tasks thatspan mathematics, coding, vision, medicine, law, psychology and more, withoutneeding any special prompting. Moreover, in all of these tasks, GPT-4'sperformance is strikingly close to human-level performance, and often vastlysurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4'scapabilities, we believe that it could reasonably be viewed as an early (yetstill incomplete) version of an artificial general intelligence (AGI) system.In our exploration of GPT-4, we put special emphasis on discovering itslimitations, and we discuss the challenges ahead for advancing towards deeperand more comprehensive versions of AGI, including the possible need forpursuing a new paradigm that moves beyond next-word prediction. We concludewith reflections on societal influences of the recent technological leap andfuture research directions.", "output": "Sparks of Artificial General Intelligence: Early experiments with GPT-4."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent text-to-image generation models like DreamBooth have made remarkableprogress in generating highly customized images of a target subject, byfine-tuning an ``expert model'' for a given subject from a few examples.However, this process is expensive, since a new expert model must be learnedfor each subject. In this paper, we present SuTI, a Subject-drivenText-to-Image generator that replaces subject-specific fine tuning withemph{in-context} learning. Given a few demonstrations of a new subject, SuTIcan instantly generate novel renditions of the subject in different scenes,without any subject-specific optimization. SuTI is powered by {emapprenticeship learning}, where a single apprentice model is learned from datagenerated by massive amount of subject-specific expert models. Specifically, wemine millions of image clusters from the Internet, each centered around aspecific visual subject. We adopt these clusters to train massive amount ofexpert models specialized on different subjects. The apprentice model SuTI thenlearns to mimic the behavior of these experts through the proposedapprenticeship learning algorithm. SuTI can generate high-quality andcustomized subject-specific images 20x faster than optimization-based SoTAmethods. On the challenging DreamBench and DreamBench-v2, our human evaluationshows that SuTI can significantly outperform existing approaches likeInstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen whileperforming on par with DreamBooth.", "output": "Subject-driven Text-to-Image Generation via Apprenticeship Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of artificial intelligence for science, it is consistently anessential challenge to face a limited amount of labeled data for real-worldproblems. The prevailing approach is to pretrain a powerful task-agnostic modelon a large unlabeled corpus but may struggle to transfer knowledge todownstream tasks. In this study, we propose InstructMol, a semi-supervisedlearning algorithm, to take better advantage of unlabeled examples. Itintroduces an instructor model to provide the confidence ratios as themeasurement of pseudo-labels' reliability. These confidence scores then guidethe target model to pay distinct attention to different data points, avoidingthe over-reliance on labeled data and the negative influence of incorrectpseudo-annotations. Comprehensive experiments show that InstructBiosubstantially improves the generalization ability of molecular models, in notonly molecular property predictions but also activity cliff estimations,demonstrating the superiority of the proposed method. Furthermore, our evidenceindicates that InstructBio can be equipped with cutting-edge pretrainingmethods and used to establish large-scale and task-specific pseudo-labeledmolecular datasets, which reduces the predictive errors and shortens thetraining process. Our work provides strong evidence that semi-supervisedlearning can be a promising tool to overcome the data scarcity limitation andadvance molecular representation learning.", "output": "InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The integration of renewable energy sources into the power grid is becomingincreasingly important as the world moves towards a more sustainable energyfuture in line with SDG 7. However, the intermittent nature of renewable energysources can make it challenging to manage the power grid and ensure a stablesupply of electricity, which is crucial for achieving SDG 9. In this paper, wepropose a deep learning-based approach for predicting energy demand in a smartpower grid, which can improve the integration of renewable energy sources byproviding accurate predictions of energy demand. Our approach aligns with SDG13 on climate action as it enables more efficient management of renewableenergy resources. We use long short-term memory networks, which are well-suitedfor time series data, to capture complex patterns and dependencies in energydemand data. The proposed approach is evaluated using four datasets ofhistorical short term energy demand data from different energy distributioncompanies including American Electric Power, Commonwealth Edison, Dayton Powerand Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposedmodel is also compared with three other state of the art forecasting algorithmsnamely, Facebook Prophet, Support Vector Regressor, and Random ForestRegressor. The experimental results show that the proposed REDf model canaccurately predict energy demand with a mean absolute error of 1.4%, indicatingits potential to enhance the stability and efficiency of the power grid andcontribute to achieving SDGs 7, 9, and 13. The proposed model also have thepotential to manage the integration of renewable energy sources in an effectivemanner.", "output": "Predicting Short Term Energy Demand in Smart Grid: A Deep Learning Approach for Integrating Renewable Energy Sources in Line with SDGs 7, 9, and 13."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The large-scale multiobjective optimization problem (LSMOP) is characterizedby simultaneously optimizing multiple conflicting objectives and involvinghundreds of decision variables. Many real-world applications in engineeringfields can be modeled as LSMOPs; simultaneously, engineering applicationsrequire insensitivity in performance. This requirement usually means that theresults from the algorithm runs should not only be good for every run in termsof performance but also that the performance of multiple runs should notfluctuate too much, i.e., the algorithm shows good insensitivity. Consideringthat substantial computational resources are requested for each run, it isessential to improve upon the performance of the large-scale multiobjectiveoptimization algorithm, as well as the insensitivity of the algorithm. However,existing large-scale multiobjective optimization algorithms solely focus onimproving the performance of the algorithms, leaving the insensitivitycharacteristics unattended. In this work, we propose an evolutionary algorithmfor solving LSMOPs based on Monte Carlo tree search, the so-called LMMOCTS,which aims to improve the performance and insensitivity for large-scalemultiobjective optimization problems. The proposed method samples the decisionvariables to construct new nodes on the Monte Carlo tree for optimization andevaluation. It selects nodes with good evaluation for further search to reducethe performance sensitivity caused by large-scale decision variables. Wecompare the proposed algorithm with several state-of-the-art designs ondifferent benchmark functions. We also propose two metrics to measure thesensitivity of the algorithm. The experimental results confirm theeffectiveness and performance insensitivity of the proposed design for solvinglarge-scale multiobjective optimization problems.", "output": "Improving Performance Insensitivity of Large-scale Multiobjective Optimization via Monte Carlo Tree Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Resource limitations make it hard to provide all students with one of themost effective educational interventions: personalized instruction.Reinforcement learning could be a key tool to reduce the development cost andimprove the effectiveness of intelligent tutoring software that aims to providethe right support, at the right time, to a student. Here we illustrate thatdeep reinforcement learning can be used to provide adaptive pedagogical supportto students learning about the concept of volume in a narrative storylinesoftware. Using explainable artificial intelligence tools, we extractedinterpretable insights about the pedagogical policy learned and demonstratedthat the resulting policy had similar performance in a different studentpopulation. Most importantly, in both studies, the reinforcement-learningnarrative system had the largest benefit for those students with the lowestinitial pretest scores, suggesting the opportunity for AI to adapt and providesupport for those most in need.", "output": "Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT has recently gathered attention from the general public and academiaas a tool that is able to generate plausible and human-sounding text answers tovarious questions. One potential use, or abuse, of ChatGPT is in answeringvarious questions or even generating whole essays and research papers in anacademic or classroom setting. While recent works have explored the use ofChatGPT in the context of humanities, business school, or medical school, thiswork explores how ChatGPT performs in the context of an introductory computerengineering course. This work assesses ChatGPT's aptitude in answering quizzes,homework, exam, and laboratory questions in an introductory-level computerengineering course. This work finds that ChatGPT can do well on questionsasking about generic concepts. However, predictably, as a text-only tool, itcannot handle questions with diagrams or figures, nor can it generate diagramsand figures. Further, also clearly, the tool cannot do hands-on labexperiments, breadboard assembly, etc., but can generate plausible answers tosome laboratory manual questions. One of the key observations presented in thiswork is that the ChatGPT tool could not be used to pass all components of thecourse. Nevertheless, it does well on quizzes and short-answer questions. Onthe other hand, plausible, human-sounding answers could confuse students whengenerating incorrect but still plausible answers.", "output": "Analyzing ChatGPT's Aptitude in an Introductory Computer Engineering Course."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatial control is a core capability in controllable image generation.Advancements in layout-guided image generation have shown promising results onin-distribution (ID) datasets with similar spatial configurations. However, itis unclear how these models perform when facing out-of-distribution (OOD)samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench,a diagnostic benchmark for layout-guided image generation that examines fourcategories of spatial control skills: number, position, size, and shape. Webenchmark two recent representative layout-guided image generation methods andobserve that the good ID layout control may not generalize well to arbitrarylayouts in the wild (e.g., objects at the boundary). Next, we proposeIterInpaint, a new baseline that generates foreground and background regions ina step-by-step manner via inpainting, demonstrating stronger generalizabilitythan existing models on OOD layouts in LayoutBench. We perform quantitative andqualitative evaluation and fine-grained analysis on the four LayoutBench skillsto pinpoint the weaknesses of existing models. Lastly, we show comprehensiveablation studies on IterInpaint, including training task ratio, crop&amp;paste vs.repaint, and generation order. Project website: ", "output": "Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This chapter provides an introduction to Presentation Attack Detection (PAD)in fingerprint biometrics, also coined anti-spoofing, describes earlydevelopments in this field, and briefly summarizes recent trends and openissues.", "output": "Introduction to Presentation Attack Detection in Fingerprint Biometrics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic neural networks can greatly reduce computation redundancy withoutcompromising accuracy by adapting their structures based on the input. In thispaper, we explore the robustness of dynamic neural networks againstenergy-oriented attacks targeted at reducing their efficiency. Specifically, weattack dynamic models with our novel algorithm GradMDM. GradMDM is a techniquethat adjusts the direction and the magnitude of the gradients to effectivelyfind a small perturbation for each input, that will activate more computationalunits of dynamic models during inference. We evaluate GradMDM on multipledatasets and dynamic models, where it outperforms previous energy-orientedattack techniques, significantly increasing computation complexity whilereducing the perceptibility of the perturbations.", "output": "GradMDM: Adversarial Attack on Dynamic Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans excel at continually acquiring, consolidating, and retaininginformation from an ever-changing environment, whereas artificial neuralnetworks (ANNs) exhibit catastrophic forgetting. There are considerabledifferences in the complexity of synapses, the processing of information, andthe learning mechanisms in biological neural networks and their artificialcounterparts, which may explain the mismatch in performance. We consider abiologically plausible framework that constitutes separate populations ofexclusively excitatory and inhibitory neurons that adhere to Dale's principle,and the excitatory pyramidal neurons are augmented with dendritic-likestructures for context-dependent processing of stimuli. We then conduct acomprehensive study on the role and interactions of different mechanismsinspired by the brain, including sparse non-overlapping representations,Hebbian learning, synaptic consolidation, and replay of past activations thataccompanied the learning event. Our study suggests that the employing ofmultiple complementary mechanisms in a biologically plausible architecture,similar to the brain, may be effective in enabling continual learning in ANNs.", "output": "A Study of Biologically Plausible Neural Network: The Role and Interactions of Brain-Inspired Mechanisms in Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative foundation models are susceptible to implicit biases that canarise from extensive unsupervised training data. Such biases can producesuboptimal samples, skewed outcomes, and unfairness, with potentiallysignificant repercussions. Consequently, aligning these models with humanethics and preferences is an essential step toward ensuring their responsibleand effective deployment in real-world applications. Prior research hasprimarily employed Reinforcement Learning from Human Feedback (RLHF) as a meansof addressing this problem, wherein generative models are fine-tuned using RLalgorithms guided by a human-feedback-informed reward model. However, theinefficiencies and instabilities associated with RL algorithms frequentlypresent substantial obstacles to the successful alignment of generative models,necessitating the development of a more robust and streamlined approach. Tothis end, we introduce a new framework, Reward rAnked FineTuning (RAFT),designed to align generative models more effectively. Utilizing a reward modeland a sufficient number of samples, our approach selects the high-qualitysamples, discarding those that exhibit undesired behavior, and subsequentlyassembles a streaming dataset. This dataset serves as the basis for aligningthe generative model and can be employed under both offline and onlinesettings. Notably, the sample generation process within RAFT is gradient-free,rendering it compatible with black-box generators. Through extensiveexperiments, we demonstrate that our proposed algorithm exhibits strongperformance in the context of both large language models and diffusion models.", "output": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point clouds offer comprehensive and precise data regarding the contour andconfiguration of objects. Employing such geometric and topological 3Dinformation of objects in class incremental learning can aid endlessapplication in 3D-computer vision. Well known 3D-point cloud class incrementallearning methods for addressing catastrophic forgetting generally entail theusage of previously encountered data, which can present difficulties insituations where there are restrictions on memory or when there are concernsabout the legality of the data. Towards this we pioneer to leverage exemplarfree class incremental learning on Point Clouds. In this paper we proposePointCLIMB: An exemplar Free Class Incremental Learning Benchmark. We focus ona pragmatic perspective to consider novel classes for class incrementallearning on 3D point clouds. We setup a benchmark for 3D Exemplar free classincremental learning. We investigate performance of various backbones on3D-Exemplar Free Class Incremental Learning framework. We demonstrate ourresults on ModelNet40 dataset.", "output": "PointCLIMB: An Exemplar-Free Point Cloud Class Incremental Benchmark."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern image inpainting systems, despite the significant progress, oftenstruggle with mask selection and holes filling. Based on Segment-Anything Model(SAM), we make the first attempt to the mask-free image inpainting and proposea new paradigm of ``clicking and filling'', which is named as Inpaint Anything(IA). The core idea behind IA is to combine the strengths of different modelsin order to build a very powerful and user-friendly pipeline for solvinginpainting-related problems. IA supports three main features: (i) RemoveAnything: users could click on an object and IA will remove it and smooth the``hole'' with the context; (ii) Fill Anything: after certain objects removal,users could provide text-based prompts to IA, and then it will fill the holewith the corresponding generative content via driving AIGC models like StableDiffusion; (iii) Replace Anything: with IA, users have another option to retainthe click-selected object and replace the remaining background with the newlygenerated scenes. We are also very willing to help everyone share and promotenew projects based on our Inpaint Anything (IA). Our codes are available at", "output": "Inpaint Anything: Segment Anything Meets Image Inpainting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large pre-trained models, also known as foundation models (FMs), are trainedin a task-agnostic manner on large-scale data and can be adapted to a widerange of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.Despite their successes in language and vision tasks, we have yet seen anattempt to develop foundation models for geospatial artificial intelligence(GeoAI). In this work, we explore the promises and challenges of developingmultimodal foundation models for GeoAI. We first investigate the potential ofmany existing FMs by testing their performances on seven tasks across multiplegeospatial subdomains including Geospatial Semantics, Health Geography, UrbanGeography, and Remote Sensing. Our results indicate that on several geospatialtasks that only involve text modality such as toponym recognition, locationdescription recognition, and US state-level/county-level dementia time seriesforecasting, these task-agnostic LLMs can outperform task-specificfully-supervised models in a zero-shot or few-shot learning setting. However,on other geospatial tasks, especially tasks that involve multiple datamodalities (e.g., POI-based urban function classification, street viewimage-based urban noise intensity classification, and remote sensing imagescene classification), existing foundation models still underperformtask-specific models. Based on these observations, we propose that one of themajor challenges of developing a FM for GeoAI is to address the multimodalitynature of geospatial tasks. After discussing the distinct challenges of eachgeospatial data modality, we suggest the possibility of a multimodal foundationmodel which can reason over various types of geospatial data through geospatialalignments. We conclude this paper by discussing the unique risks andchallenges to develop such a model for GeoAI.", "output": "On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Out-of-distribution (OOD) detection aims to identify test examples that donot belong to the training distribution and are thus unlikely to be predictedreliably. Despite a plethora of existing works, most of them focused only onthe scenario where OOD examples come from semantic shift (e.g., unseencategories), ignoring other possible causes (e.g., covariate shift). In thispaper, we present a novel, unifying framework to study OOD detection in abroader scope. Instead of detecting OOD examples from a particular cause, wepropose to detect examples that a deployed machine learning model (e.g., animage classifier) is unable to predict correctly. That is, whether a testexample should be detected and rejected or not is ``model-specific''. We showthat this framework unifies the detection of OOD examples caused by semanticshift and covariate shift, and closely addresses the concern of applying amachine learning model to uncontrolled environments. We provide an extensiveanalysis that involves a variety of models (e.g., different architectures andtraining strategies), sources of OOD examples, and OOD detection approaches,and reveal several insights into improving and understanding OOD detection inuncontrolled environments.", "output": "Unified Out-Of-Distribution Detection: A Model-Specific Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a method for adding sound-guided visual effects to specificregions of videos with a zero-shot setting. Animating the appearance of thevisual effect is challenging because each frame of the edited video should havevisual changes while maintaining temporal consistency. Moreover, existing videoediting solutions focus on temporal consistency across frames, ignoring thevisual style variations over time, e.g., thunderstorm, wave, fire crackling. Toovercome this limitation, we utilize temporal sound features for the dynamicstyle. Specifically, we guide denoising diffusion probabilistic models with anaudio latent representation in the audio-visual latent space. To the best ofour knowledge, our work is the first to explore sound-guided natural videoediting from various sound sources with sound-specialized properties, such asintensity, timbre, and volume. Additionally, we design optical flow-basedguidance to generate temporally consistent video frames, capturing thepixel-wise relationship between adjacent frames. Experimental results show thatour method outperforms existing video editing techniques, producing morerealistic visual effects that reflect the properties of sound. Please visit ourpage: ", "output": "Soundini: Sound-Guided Diffusion for Natural Video Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integrating whole-slide images (WSIs) and bulk transcriptomics for predictingpatient survival can improve our understanding of patient prognosis. However,this multimodal task is particularly challenging due to the different nature ofthese data: WSIs represent a very high-dimensional spatial description of atumor, while bulk transcriptomics represent a global description of geneexpression levels within that tumor. In this context, our work aims to addresstwo key challenges: (1) how can we tokenize transcriptomics in a semanticallymeaningful and interpretable way?, and (2) how can we capture dense multimodalinteractions between these two modalities? Specifically, we propose to learnbiological pathway tokens from transcriptomics that can encode specificcellular functions. Together with histology patch tokens that encode thedifferent morphological patterns in the WSI, we argue that they formappropriate reasoning units for downstream interpretability analyses. Wepropose fusing both modalities using a memory-efficient multimodal Transformerthat can model interactions between pathway and histology patch tokens. Ourproposed model, SURVPATH, achieves state-of-the-art performance when evaluatedagainst both unimodal and multimodal baselines on five datasets from The CancerGenome Atlas. Our interpretability framework identifies key multimodalprognostic factors, and, as such, can provide valuable insights into theinteraction between genotype and phenotype, enabling a deeper understanding ofthe underlying biological mechanisms at play. We make our code public at:", "output": "Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we tackle the problem of video alignment, the process ofmatching the frames of a pair of videos containing similar actions. The mainchallenge in video alignment is that accurate correspondence should beestablished despite the differences in the execution processes and appearancesbetween the two videos. We introduce an unsupervised method for alignment thatuses global and local features of the frames. In particular, we introduceeffective features for each video frame by means of three machine vision tools:person detection, pose estimation, and VGG network. Then the features areprocessed and combined to construct a multidimensional time series thatrepresent the video. The resulting time series are used to align videos of thesame actions using a novel version of dynamic time warping named DiagonalizedDynamic Time Warping(DDTW). The main advantage of our approach is that notraining is required, which makes it applicable for any new type of actionwithout any need to collect training samples for it. For evaluation, weconsidered video synchronization and phase classification tasks on the Pennaction dataset. Also, for an effective evaluation of the video synchronizationtask, we present a new metric called Enclosed Area Error(EAE). The results showthat our method outperforms previous state-of-the-art methods, such as TCC andother self-supervised and supervised methods.", "output": "Video alignment using unsupervised learning of local and global features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes a hybrid unsupervised/supervised learning method topretrain models applied in earth observation downstream tasks where only ahandful of labels denoting very general semantic concepts are available. Wecombine a contrastive approach to pretrain models with a pretext task topredict spatially coarse elevation maps which are commonly available worldwide.The intuition behind is that there is generally some correlation between theelevation and targets in many remote sensing tasks, allowing the model topre-learn useful representations. We assess the performance of our approach ona segmentation downstream task on labels gathering many possible subclasses(pixel level classification of farmlands vs. other) and an image binaryclassification task derived from the former, on a dataset on the north-east ofColombia. On both cases we pretrain our models with 39K unlabeled images, finetune the downstream task only with 80 labeled images and test it with 2944labeled images. Our experiments show that our methods, GLCNet+Elevation forsegmentation and SimCLR+Elevation for classification, outperform theircounterparts without the elevation pretext task in terms of accuracy andmacro-average F1, which supports the notion that including additionalinformation correlated to targets in downstream tasks can lead to improvedperformance.", "output": "A contrastive method based on elevation data for remote sensing with scarce and high level semantic labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new algorithm for selection of informative frames in videoaction recognition. Our approach is designed for aerial videos captured using amoving camera where human actors occupy a small spatial resolution of videoframes. Our algorithm utilizes the motion bias within aerial videos, whichenables the selection of motion-salient frames. We introduce the concept ofpatch mutual information (PMI) score to quantify the motion bias betweenadjacent frames, by measuring the similarity of patches. We use this score toassess the amount of discriminative motion information contained in one framerelative to another. We present an adaptive frame selection strategy usingshifted leaky ReLu and cumulative distribution function, which ensures that thesampled frames comprehensively cover all the essential segments with highmotion salience. Our approach can be integrated with any action recognitionmodel to enhance its accuracy. In practice, our method achieves a relativeimprovement of 2.2 - 13.8% in top-1 accuracy on UAV-Human, 6.8% on NEC Drone,and 9.0% on Diving48 datasets.", "output": "PMI Sampler: Patch similarity guided frame selection for Aerial Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in language-image models have led to the development ofhighly realistic images that can be generated from textual descriptions.However, the increased visual quality of these generated images poses apotential threat to the field of media forensics. This paper aims toinvestigate the level of challenge that language-image generation models poseto media forensics. To achieve this, we propose a new approach that leveragesthe DALL-E2 language-image model to automatically generate and splice maskedregions guided by a text prompt. To ensure the creation of realisticmanipulations, we have designed an annotation platform with human checking toverify reasonable text prompts. This approach has resulted in the creation of anew image dataset called AutoSplice, containing 5,894 manipulated and authenticimages. Specifically, we have generated a total of 3,621 images by locally orglobally manipulating real-world image-caption pairs, which we believe willprovide a valuable resource for developing generalized detection methods inthis area. The dataset is evaluated under two media forensic tasks: forgerydetection and localization. Our extensive experiments show that most mediaforensic models struggle to detect the AutoSplice dataset as an unseenmanipulation. However, when fine-tuned models are used, they exhibit improvedperformance in both tasks.", "output": "AutoSplice: A Text-prompt Manipulated Image Dataset for Media Forensics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-resolution satellite imagery is a key element for many Earth monitoringapplications. Satellites such as Sentinel-2 feature characteristics that arefavorable for super-resolution algorithms such as aliasing andband-misalignment. Unfortunately the lack of reliable high-resolution (HR)ground truth limits the application of deep learning methods to this task. Inthis work we propose L1BSR, a deep learning-based method for single-imagesuper-resolution and band alignment of Sentinel-2 L1B 10m bands. The method istrained with self-supervision directly on real L1B data by leveragingoverlapping areas in L1B images produced by adjacent CMOS detectors, thus notrequiring HR ground truth. Our self-supervised loss is designed to enforce thesuper-resolved output image to have all the bands correctly aligned. This isachieved via a novel cross-spectral registration network (CSR) which computesan optical flow between images of different spectral bands. The CSR network isalso trained with self-supervision using an Anchor-Consistency loss, which wealso introduce in this work. We demonstrate the performance of the proposedapproach on synthetic and real L1B data, where we show that it obtainscomparable results to supervised methods.", "output": "L1BSR: Exploiting Detector Overlap for Self-Supervised Single-Image Super-Resolution of Sentinel-2 L1B Imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this age of information, images are a critical medium for storing andtransmitting information. With the rapid growth of image data amount, visualcompression and visual data perception are two important research topicsattracting a lot attention. However, those two topics are rarely discussedtogether and follow separate research path. Due to the compact compresseddomain representation offered by learning-based image compression methods,there exists possibility to have one stream targeting both efficient datastorage and compression, and machine perception tasks. In this paper, wepropose a layered generative image compression model achieving high humanvision-oriented image reconstructed quality, even at extreme compressionratios. To obtain analysis efficiency and flexibility, a task-agnosticlearning-based compression model is proposed, which effectively supportsvarious compressed domain-based analytical tasks while reserves outstandingreconstructed perceptual quality, compared with traditional and learning-basedcodecs. In addition, joint optimization schedule is adopted to acquire bestbalance point among compression ratio, reconstructed image quality, anddownstream perception performance. Experimental results verify that ourproposed compressed domain-based multi-task analysis method can achievecomparable analysis results against the RGB image-based methods with up to99.6% bit rate saving (i.e., compared with taking original RGB image as theanalysis model input). The practical ability of our model is further justifiedfrom model size and information fidelity aspects.", "output": "Machine Perception-Driven Image Compression: A Layered Generative Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pretrained backbones with fine-tuning have been widely adopted in 2D visionand natural language processing tasks and demonstrated significant advantagesto task-specific networks. In this paper, we present a pretrained 3D backbone,named {SST}, which first outperforms all state-of-the-art methods indownstream 3D indoor scene understanding tasks. Our backbone network is basedon a 3D Swin transformer and carefully designed to efficiently conductself-attention on sparse voxels with linear memory complexity and capture theirregularity of point signals via generalized contextual relative positionalembedding. Based on this backbone design, we pretrained a large {SST} model ona synthetic Structed3D dataset that is 10 times larger than the ScanNet datasetand fine-tuned the pretrained model in various downstream real-world indoorscene understanding tasks. The results demonstrate that our model pretrained onthe synthetic dataset not only exhibits good generality in both downstreamsegmentation and detection on real 3D point datasets, but also surpasses thestate-of-the-art methods on downstream tasks after fine-tuning with +2.3 mIoUand +2.2 mIoU on S3DIS Area5 and 6-fold semantic segmentation, +2.1 mIoU onScanNet segmentation (val), +1.9 mAP@0.5 on ScanNet detection, +8.1 mAP@0.5 onS3DIS detection. Our method demonstrates the great potential of pretrained 3Dbackbones with fine-tuning for 3D understanding tasks. The code and models areavailable at  .", "output": "Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In most image retrieval systems, images include various high-level semantics,called tags or annotations. Virtually all the state-of-the-art image annotationmethods that handle imbalanced labeling are search-based techniques which aretime-consuming. In this paper, a novel coupled dictionary learning approach isproposed to learn a limited number of visual prototypes and their correspondingsemantics simultaneously. This approach leads to a real-time image annotationprocedure. Another contribution of this paper is that utilizes a marginalizedloss function instead of the squared loss function that is inappropriate forimage annotation with imbalanced labels. We have employed a marginalized lossfunction in our method to leverage a simple and effective method of prototypeupdating. Meanwhile, we have introduced ${ell}_1$ regularization on semanticprototypes to preserve the sparse and imbalanced nature of labels in learnedsemantic prototypes. Finally, comprehensive experimental results on variousdatasets demonstrate the efficiency of the proposed method for image annotationtasks in terms of accuracy and time. The reference implementation is publiclyavailable on ", "output": "Toward Real-Time Image Annotation Using Marginalized Coupled Dictionary Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) have been shown to be vulnerable to adversarialexamples. Moreover, the transferability of the adversarial examples hasreceived broad attention in recent years, which means that adversarial examplescrafted by a surrogate model can also attack unknown models. This phenomenongave birth to the transfer-based adversarial attacks, which aim to improve thetransferability of the generated adversarial examples. In this paper, wepropose to improve the transferability of adversarial examples in thetransfer-based attack via masking unimportant parameters (MUP). The key idea inMUP is to refine the pretrained surrogate models to boost the transfer-basedattack. Based on this idea, a Taylor expansion-based metric is used to evaluatethe parameter importance score and the unimportant parameters are masked duringthe generation of adversarial examples. This process is simple, yet can benaturally combined with various existing gradient-based optimizers forgenerating adversarial examples, thus further improving the transferability ofthe generated adversarial examples. Extensive experiments are conducted tovalidate the effectiveness of the proposed MUP-based methods.", "output": "Generating Adversarial Examples with Better Transferability via Masking Unimportant Parameters of Surrogate Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked autoencoders (MAE) have recently been introduced to 3D self-supervisedpretraining for point clouds due to their great success in NLP and computervision. Unlike MAEs used in the image domain, where the pretext task is torestore features at the masked pixels, such as colors, the existing 3D MAEworks reconstruct the missing geometry only, i.e, the location of the maskedpoints. In contrast to previous studies, we advocate that point locationrecovery is inessential and restoring intrinsic point features is muchsuperior. To this end, we propose to ignore point position reconstruction andrecover high-order features at masked points including surface normals andsurface variations, through a novel attention-based decoder which isindependent of the encoder design. We validate the effectiveness of our pretexttask and decoder design using different encoder structures for 3D training anddemonstrate the advantages of our pretrained networks on various point cloudanalysis tasks.", "output": "3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating a high-quality High Dynamic Range (HDR) image from dynamic sceneshas recently been extensively studied by exploiting Deep Neural Networks(DNNs). Most DNNs-based methods require a large amount of training data withground truth, requiring tedious and time-consuming work. Few-shot HDR imagingaims to generate satisfactory images with limited data. However, it isdifficult for modern DNNs to avoid overfitting when trained on only a fewimages. In this work, we propose a novel semi-supervised approach to realizefew-shot HDR imaging via two stages of training, called SSHDR. Unlikelyprevious methods, directly recovering content and removing ghostssimultaneously, which is hard to achieve optimum, we first generate content ofsaturated regions with a self-supervised mechanism and then address ghosts viaan iterative semi-supervised learning framework. Concretely, considering thatsaturated regions can be regarded as masking Low Dynamic Range (LDR) inputregions, we design a Saturated Mask AutoEncoder (SMAE) to learn a robustfeature representation and reconstruct a non-saturated HDR image. We alsopropose an adaptive pseudo-label selection strategy to pick high-quality HDRpseudo-labels in the second stage to avoid the effect of mislabeled samples.Experiments demonstrate that SSHDR outperforms state-of-the-art methodsquantitatively and qualitatively within and across different datasets,achieving appealing HDR visualization with few labeled samples.", "output": "SMAE: Few-shot Learning for HDR Deghosting with Saturation-Aware Masked Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of human stylization is to transfer full-body human photos to astyle specified by a single art character reference image. Although previouswork has succeeded in example-based stylization of faces and generic scenes,full-body human stylization is a more complex domain. This work addressesseveral unique challenges of stylizing full-body human images. We propose amethod for one-shot fine-tuning of a pose-guided human generator to preservethe \"content\" (garments, face, hair, pose) of the input photo and the \"style\"of the artistic reference. Since body shape deformation is an essentialcomponent of an art character's style, we incorporate a novel skeletondeformation module to reshape the pose of the input person and modify the DiOrpose-guided person generator to be more robust to the rescaled poses fallingoutside the distribution of the realistic poses that the generator isoriginally trained on. Several human studies verify the effectiveness of ourapproach.", "output": "One-Shot Stylization for Full-Body Human Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While having achieved great success in rich real-life applications, deepneural network (DNN) models have long been criticized for their vulnerabilityto adversarial attacks. Tremendous research efforts have been dedicated tomitigating the threats of adversarial attacks, but the essential trait ofadversarial examples is not yet clear, and most existing methods are yetvulnerable to hybrid attacks and suffer from counterattacks. In light of this,in this paper, we first reveal a gradient-based correlation between sensitivityanalysis-based DNN interpreters and the generation process of adversarialexamples, which indicates the Achilles's heel of adversarial attacks and shedslight on linking together the two long-standing challenges of DNN: fragilityand unexplainability. We then propose an interpreter-based ensemble frameworkcalled X-Ensemble for robust adversary defense. X-Ensemble adopts a noveldetection-rectification process and features in building multiple sub-detectorsand a rectifier upon various types of interpretation information toward targetclassifiers. Moreover, X-Ensemble employs the Random Forests (RF) model tocombine sub-detectors into an ensemble detector for adversarial hybrid attacksdefense. The non-differentiable property of RF further makes it a preciouschoice against the counterattack of adversaries. Extensive experiments undervarious types of state-of-the-art attacks and diverse attack scenariosdemonstrate the advantages of X-Ensemble to competitive baseline methods.", "output": "Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unmanned Aerial Vehicles (UAVs), specifically drones equipped with remotesensing object detection technology, have rapidly gained a broad spectrum ofapplications and emerged as one of the primary research focuses in the field ofcomputer vision. Although UAV remote sensing systems have the ability to detectvarious objects, small-scale objects can be challenging to detect reliably dueto factors such as object size, image degradation, and real-time limitations.To tackle these issues, a real-time object detection algorithm (YOLO-Drone) isproposed and applied to two new UAV platforms as well as a specific lightsource (silicon-based golden LED). YOLO-Drone presents several novelties: 1)including a new backbone Darknet59; 2) a new complex feature aggregation moduleMSPP-FPN that incorporated one spatial pyramid pooling and three atrous spatialpyramid pooling modules; 3) and the use of Generalized Intersection over Union(GIoU) as the loss function. To evaluate performance, two benchmark datasets,UAVDT and VisDrone, along with one homemade dataset acquired at night undersilicon-based golden LEDs, are utilized. The experimental results show that, inboth UAVDT and VisDrone, the proposed YOLO-Drone outperforms state-of-the-art(SOTA) object detection methods by improving the mAP of 10.13% and 8.59%,respectively. With regards to UAVDT, the YOLO-Drone exhibits both highreal-time inference speed of 53 FPS and a maximum mAP of 34.04%. Notably,YOLO-Drone achieves high performance under the silicon-based golden LEDs, witha mAP of up to 87.71%, surpassing the performance of YOLO series under ordinarylight sources. To conclude, the proposed YOLO-Drone is a highly effectivesolution for object detection in UAV applications, particularly for nightdetection tasks where silicon-based golden light LED technology exhibitssignificant superiority.", "output": "YOLO-Drone:Airborne real-time detection of dense small objects from high-altitude perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We tackle the issue of generalized category discovery (GCD). GCD considersthe open-world problem of automatically clustering a partially labelleddataset, in which the unlabelled data contain instances from novel categoriesand also the labelled classes. In this paper, we address the GCD problemwithout a known category number in the unlabelled data. We propose a framework,named CiPR, to bootstrap the representation by exploiting Cross-instancePositive Relations for contrastive learning in the partially labelled datawhich are neglected in existing methods. First, to obtain reliablecross-instance relations to facilitate the representation learning, weintroduce a semi-supervised hierarchical clustering algorithm, named selectiveneighbor clustering (SNC), which can produce a clustering hierarchy directlyfrom the connected components in the graph constructed by selective neighbors.We also extend SNC to be capable of label assignment for the unlabelledinstances with the given class number. Moreover, we present a method toestimate the unknown class number using SNC with a joint reference scoreconsidering clustering indexes of both labelled and unlabelled data. Finally,we thoroughly evaluate our framework on public generic image recognitiondatasets and challenging fine-grained datasets, all establishing the newstate-of-the-art.", "output": "CiPR: An Efficient Framework with Cross-instance Positive Relations for Generalized Category Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene Dynamic Recovery (SDR) by inverting distorted Rolling Shutter (RS)images to an undistorted high frame-rate Global Shutter (GS) video is aseverely ill-posed problem, particularly when prior knowledge aboutcamera/object motions is unavailable. Commonly used artificial assumptions onmotion linearity and data-specific characteristics, regarding the temporaldynamics information embedded in the RS scanlines, are prone to producingsub-optimal solutions in real-world scenarios. To address this challenge, wepropose an event-based RS2GS framework within a self-supervised learningparadigm that leverages the extremely high temporal resolution of event camerasto provide accurate inter/intra-frame information. % In this paper, we proposeto leverage the event camera to provide inter/intra-frame information as theemitted events have an extremely high temporal resolution and learn anevent-based RS2GS network within a self-supervised learning framework, wherereal-world events and RS images can be exploited to alleviate the performancedegradation caused by the domain gap between the synthesized and real data.Specifically, an Event-based Inter/intra-frame Compensator (E-IC) is proposedto predict the per-pixel dynamic between arbitrary time intervals, includingthe temporal transition and spatial translation. Exploring connections in termsof RS-RS, RS-GS, and GS-RS, we explicitly formulate mutual constraints with theproposed E-IC, resulting in supervisions without ground-truth GS images.Extensive evaluations over synthetic and real datasets demonstrate that theproposed method achieves state-of-the-art and shows remarkable performance forevent-based RS2GS inversion in real-world scenarios. The dataset and code areavailable at ", "output": "Self-Supervised Scene Dynamic Recovery from Rolling Shutter Images and Events."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) has been introduced to the healthcare domain as adecentralized learning paradigm that allows multiple parties to train a modelcollaboratively without privacy leakage. However, most previous studies haveassumed that every client holds an identical label set. In reality, medicalspecialists tend to annotate only diseases within their knowledge domain orinterest. This implies that label sets in each client can be different and evendisjoint. In this paper, we propose the framework FedLSM to solve the problemLabel Set Mismatch. FedLSM adopts different training strategies on data withdifferent uncertainty levels to efficiently utilize unlabeled or partiallylabeled data as well as class-wise adaptive aggregation in the classificationlayer to avoid inaccurate aggregation when clients have missing labels. Weevaluate FedLSM on two public real-world medical image datasets, includingchest x-ray (CXR) diagnosis with 112,120 CXR images and skin lesion diagnosiswith 10,015 dermoscopy images, and show that it significantly outperforms otherstate-of-the-art FL algorithms. Code will be made available upon acceptance.", "output": "Scale Federated Learning for Label Set Mismatch in Medical Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Animating an object in 3D often requires an articulated structure, e.g. akinematic chain or skeleton of the manipulated object with proper skinningweights, to obtain smooth movements and surface deformations. However, existingmodels that allow direct pose manipulations are either limited to specificobject categories or built with specialized equipment. To reduce the workneeded for creating animatable 3D models, we propose a novel reconstructionmethod that learns an animatable kinematic chain for any articulated object.Our method operates on monocular videos without prior knowledge of the object'sshape or underlying structure. Our approach is on par with state-of-the-art 3Dsurface reconstruction methods on various articulated object categories whileenabling direct pose manipulations by re-posing the learned kinematic chain.", "output": "CAMM: Building Category-Agnostic and Animatable 3D Models from Monocular Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In-context vision and language models like Flamingo support arbitrarilyinterleaved sequences of images and text as input. This format not only enablesfew-shot learning via interleaving independent supervised (image, text)examples, but also, more complex prompts involving interaction between images,e.g., \"What do image A and image B have in common?\" To support this interface,pretraining occurs over web corpora that similarly contain interleavedimages+text. To date, however, large-scale data of this form have not beenpublicly available.We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4corpus with images interleaved. We use a linear assignment algorithm to placeimages into longer bodies of text using CLIP features, a process that we showoutperforms alternatives. mmc4 spans everyday topics like cooking, travel,technology, etc. A manual inspection of a random sample of documents shows thata vast majority (90%) of images are topically relevant, and that linearassignment frequently selects individual sentences specifically well-alignedwith each image (78%). After filtering NSFW images, ads, etc., the corpuscontains 103M documents containing 585M images interleaved with 43B Englishtokens.", "output": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mapping Low Dynamic Range (LDR) images with different exposures to HighDynamic Range (HDR) remains nontrivial and challenging on dynamic scenes due toghosting caused by object motion or camera jitting. With the success of DeepNeural Networks (DNNs), several DNNs-based methods have been proposed toalleviate ghosting, they cannot generate approving results when motion andsaturation occur. To generate visually pleasing HDR images in various cases, wepropose a hybrid HDR deghosting network, called HyHDRNet, to learn thecomplicated relationship between reference and non-reference images. Theproposed HyHDRNet consists of a content alignment subnetwork and aTransformer-based fusion subnetwork. Specifically, to effectively avoidghosting from the source, the content alignment subnetwork uses patchaggregation and ghost attention to integrate similar content from othernon-reference images with patch level and suppress undesired components withpixel level. To achieve mutual guidance between patch-level and pixel-level, weleverage a gating module to sufficiently swap useful information both inghosted and saturated regions. Furthermore, to obtain a high-quality HDR image,the Transformer-based fusion subnetwork uses a Residual Deformable TransformerBlock (RDTB) to adaptively merge information for different exposed regions. Weexamined the proposed method on four widely used public HDR image deghostingdatasets. Experiments demonstrate that HyHDRNet outperforms state-of-the-artmethods both quantitatively and qualitatively, achieving appealing HDRvisualization with unified textures and colors.", "output": "A Unified HDR Imaging Method with Pixel and Patch Level."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reconstructing an image from noisy and incomplete measurements is a centraltask in several image processing applications. In recent years,state-of-the-art reconstruction methods have been developed based on recentadvances in deep learning. Especially for highly underdetermined problems,maintaining data consistency is a key goal. This can be achieved either byiterative network architectures or by a subsequent projection of the networkreconstruction. However, for such approaches to be used in safety-criticaldomains such as medical imaging, the network reconstruction should not onlyprovide the user with a reconstructed image, but also with some level ofconfidence in the reconstruction. In order to meet these two key requirements,this paper combines deep null-space networks with uncertainty quantification.Evaluation of the proposed method includes image reconstruction fromundersampled Radon measurements on a toy CT dataset and accelerated MRIreconstruction on the fastMRI dataset. This work is the first approach tosolving inverse problems that additionally models data-dependent uncertainty byestimating an input-dependent scale map, providing a robust assessment ofreconstruction quality.", "output": "Uncertainty-Aware Null Space Networks for Data-Consistent Image Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "CLIP (Contrastive Language-Image Pretraining) is well-developed foropen-vocabulary zero-shot image-level recognition, while its applications inpixel-level tasks are less investigated, where most efforts directly adopt CLIPfeatures without deliberative adaptations. In this work, we first demonstratethe necessity of image-pixel CLIP feature adaption, then provide Multi-ViewPrompt learning (MVP-SEG) as an effective solution to achieve image-pixeladaptation and to solve open-vocabulary semantic segmentation. Concretely,MVP-SEG deliberately learns multiple prompts trained by our OrthogonalConstraint Loss (OCLoss), by which each prompt is supervised to exploit CLIPfeature on different object parts, and collaborative segmentation masksgenerated by all prompts promote better segmentation. Moreover, MVP-SEGintroduces Global Prompt Refining (GPR) to further eliminate class-wisesegmentation noise. Experiments show that the multi-view prompts learned fromseen categories have strong generalization to unseen categories, and MVP-SEG+which combines the knowledge transfer stage significantly outperforms previousmethods on several benchmarks. Moreover, qualitative results justify thatMVP-SEG does lead to better focus on different local parts.", "output": "MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Depth Estimation has wide reaching applications in the field of Computervision such as target tracking, augmented reality, and self-driving cars. Thegoal of Monocular Depth Estimation is to predict the depth map, given a 2Dmonocular RGB image as input. The traditional depth estimation methods arebased on depth cues and used concepts like epipolar geometry. With theevolution of Convolutional Neural Networks, depth estimation has undergonetremendous strides. In this project, our aim is to explore possible extensionsto existing SoTA Deep Learning based Depth Estimation Models and to see whetherperformance metrics could be further improved. In a broader sense, we arelooking at the possibility of implementing Pose Estimation, Efficient Sub-PixelConvolution Interpolation, Semantic Segmentation Estimation techniques tofurther enhance our proposed architecture and to provide fine-grained and moreglobally coherent depth map predictions. We also plan to do away with cameraintrinsic parameters during training and apply weather augmentations to furthergeneralize our model.", "output": "Self-Supervised Learning based Depth Estimation from Monocular Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The limited ability of Convolutional Neural Networks to generalize to imagesfrom previously unseen domains is a major limitation, in particular, forsafety-critical clinical tasks such as dermoscopic skin cancer classification.In order to translate CNN-based applications into the clinic, it is essentialthat they are able to adapt to domain shifts. Such new conditions can arisethrough the use of different image acquisition systems or varying lightingconditions. In dermoscopy, shifts can also occur as a change in patient age oroccurence of rare lesion localizations (e.g. palms). These are not prominentlyrepresented in most training datasets and can therefore lead to a decrease inperformance. In order to verify the generalizability of classification modelsin real world clinical settings it is crucial to have access to data whichmimics such domain shifts. To our knowledge no dermoscopic image dataset existswhere such domain shifts are properly described and quantified. We thereforegrouped publicly available images from ISIC archive based on their metadata(e.g. acquisition location, lesion localization, patient age) to generatemeaningful domains. To verify that these domains are in fact distinct, we usedmultiple quantification measures to estimate the presence and intensity ofdomain shifts. Additionally, we analyzed the performance on these domains withand without an unsupervised domain adaptation technique. We observed that inmost of our grouped domains, domain shifts in fact exist. Based on our results,we believe these datasets to be helpful for testing the generalizationcapabilities of dermoscopic skin cancer classifiers.", "output": "Domain shifts in dermoscopic skin cancer datasets: Evaluation of essential limitations for clinical translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural radiance field (NeRF) has become a popular 3D representation methodfor human avatar reconstruction due to its high-quality rendering capabilities,e.g., regarding novel views and poses. However, previous methods for editingthe geometry and appearance of the avatar only allow for global editing throughbody shape parameters and 2D texture maps. In this paper, we propose a newapproach named textbf{U}nified textbf{V}olumetric textbf{A}vatar(textbf{UVA}) that enables local and independent editing of both geometry andtexture, while retaining the ability to render novel views and poses. UVAtransforms each observation point to a canonical space using a skinning motionfield and represents geometry and texture in separate neural fields. Each fieldis composed of a set of structured latent codes that are attached to anchornodes on a deformable mesh in canonical space and diffused into the entirespace via interpolation, allowing for local editing. To address spatialambiguity in code interpolation, we use a local signed height indicator. Wealso replace the view-dependent radiance color with a pose-dependent shadingfactor to better represent surface illumination in different poses. Experimentson multiple human avatars demonstrate that our UVA achieves competitive resultsin novel view synthesis and novel pose rendering while enabling local andindependent editing of geometry and appearance. The source code will bereleased.", "output": "UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose rendering, Geometry and Texture Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning new classes without forgetting is crucial for real-worldapplications for a classification model. Vision Transformers (ViT) recentlyachieve remarkable performance in Class Incremental Learning (CIL). Previousworks mainly focus on block design and model expansion for ViTs. However, inthis paper, we find that when the ViT is incrementally trained, the attentionlayers gradually lose concentration on local features. We call this interestingphenomenon as emph{Locality Degradation} in ViTs for CIL. Since the low-levellocal information is crucial to the transferability of the representation, itis beneficial to preserve the locality in attention layers. In this paper, weencourage the model to preserve more local information as the trainingprocedure goes on and devise a Locality-Preserved Attention (LPA) layer toemphasize the importance of local features. Specifically, we incorporate thelocal information directly into the vanilla attention and control the initialgradients of the vanilla attention by weighting it with a small initial value.Extensive experiments show that the representations facilitated by LPA capturemore low-level general information which is easier to transfer to follow-uptasks. The improved model gets consistently better performance on CIFAR100 andImageNet100.", "output": "Preserving Locality in Vision Transformers for Class Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study a real-world JPEG image restoration problem with biterrors on the encrypted bitstream. The bit errors bring unpredictable colorcasts and block shifts on decoded image contents, which cannot be resolved byexisting image restoration methods mainly relying on pre-defined degradationmodels in the pixel domain. To address these challenges, we propose a robustJPEG decoder, followed by a two-stage compensation and alignment framework torestore bitstream-corrupted JPEG images. Specifically, the robust JPEG decoderadopts an error-resilient mechanism to decode the corrupted JPEG bitstream. Thetwo-stage framework is composed of the self-compensation and alignment (SCA)stage and the guided-compensation and alignment (GCA) stage. The SCA adaptivelyperforms block-wise image color compensation and alignment based on theestimated color and block offsets via image content similarity. The GCAleverages the extracted low-resolution thumbnail from the JPEG header to guidefull-resolution pixel-wise image restoration in a coarse-to-fine manner. It isachieved by a coarse-guided pix2pix network and a refine-guided bi-directionalLaplacian pyramid fusion network. We conduct experiments on three benchmarkswith varying degrees of bit error rates. Experimental results and ablationstudies demonstrate the superiority of our proposed method. The code will bereleased at ", "output": "Bitstream-Corrupted JPEG Images are Restorable: Two-stage Compensation and Alignment Framework for Image Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we realize automatic visual recognition and directionestimation of pointing. We introduce the first neural pointing understandingmethod based on two key contributions. The first is the introduction of afirst-of-its-kind large-scale dataset for pointing recognition and directionestimation, which we refer to as the DP Dataset. DP Dataset consists of morethan 2 million frames of over 33 people pointing in various styles annotatedfor each frame with pointing timings and 3D directions. The second is DeePoint,a novel deep network model for joint recognition and 3D direction estimation ofpointing. DeePoint is a Transformer-based network which fully leverages thespatio-temporal coordination of the body parts, not just the hands. Throughextensive experiments, we demonstrate the accuracy and efficiency of DeePoint.We believe DP Dataset and DeePoint will serve as a sound foundation for visualhuman intention understanding.", "output": "DeePoint: Pointing Recognition and Direction Estimation From A Fixed View."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "File fragment classification (FFC) on small chunks of memory is essential inmemory forensics and Internet security. Existing methods mainly treat filefragments as 1d byte signals and utilize the captured inter-byte features forclassification, while the bit information within bytes, i.e., intra-byteinformation, is seldom considered. This is inherently inapt for classifyingvariable-length coding files whose symbols are represented as the variablenumber of bits. Conversely, we propose Byte2Image, a novel data augmentationtechnique, to introduce the neglected intra-byte information into filefragments and re-treat them as 2d gray-scale images, which allows us to captureboth inter-byte and intra-byte correlations simultaneously through powerfulconvolutional neural networks (CNNs). Specifically, to convert file fragmentsto 2d images, we employ a sliding byte window to expose the neglectedintra-byte information and stack their n-gram features row by row. We furtherpropose a byte sequence &amp; image fusion network as a classifier, which canjointly model the raw 1d byte sequence and the converted 2d image to performFFC. Experiments on FFT-75 dataset validate that our proposed method canachieve notable accuracy improvements over state-of-the-art methods in nearlyall scenarios. The code will be released at", "output": "A Byte Sequence is Worth an Image: CNN for File Fragment Classification Using Bit Shift and n-Gram Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient deep learning-based approaches have achieved remarkable performancein single image super-resolution. However, recent studies on efficientsuper-resolution have mainly focused on reducing the number of parameters andfloating-point operations through various network designs. Although thesemethods can decrease the number of parameters and floating-point operations,they may not necessarily reduce actual running time. To address this issue, wepropose a novel multi-stage lightweight network boosting method, which canenable lightweight networks to achieve outstanding performance. Specifically,we leverage enhanced high-resolution output as additional supervision toimprove the learning ability of lightweight student networks. Upon convergenceof the student network, we further simplify our network structure to a morelightweight level using reparameterization techniques and iterative networkpruning. Meanwhile, we adopt an effective lightweight network training strategythat combines multi-anchor distillation and progressive learning, enabling thelightweight network to achieve outstanding performance. Ultimately, ourproposed method achieves the fastest inference time among all participants inthe NTIRE 2023 efficient super-resolution challenge while maintainingcompetitive super-resolution performance. Additionally, extensive experimentsare conducted to demonstrate the effectiveness of the proposed components. Theresults show that our approach achieves comparable performance inrepresentative dataset DIV2K, both qualitatively and quantitatively, withfaster inference and fewer number of network parameters.", "output": "DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The exploitation of visible spectrum datasets has led deep networks to showremarkable success. However, real-world tasks include low-lighting conditionswhich arise performance bottlenecks for models trained on large-scale RGB imagedatasets. Thermal IR cameras are more robust against such conditions.Therefore, the usage of thermal imagery in real-world applications can beuseful. Unsupervised domain adaptation (UDA) allows transferring informationfrom a source domain to a fully unlabeled target domain. Despite substantialimprovements in UDA, the performance gap between UDA and its supervisedlearning counterpart remains significant. By picking a small number of targetsamples to annotate and using them in training, active domain adaptation triesto mitigate this gap with minimum annotation expense. We propose an activedomain adaptation method in order to examine the efficiency of combining thevisible spectrum and thermal imagery modalities. When the domain gap isconsiderably large as in the visible-to-thermal task, we may conclude that themethods without explicit domain alignment cannot achieve their full potential.To this end, we propose a spectral transfer guided active domain adaptationmethod to select the most informative unlabeled target samples while aligningsource and target domains. We used the large-scale visible spectrum datasetMS-COCO as the source domain and the thermal dataset FLIR ADAS as the targetdomain to present the results of our method. Extensive experimental evaluationdemonstrates that our proposed method outperforms the state-of-the-art activedomain adaptation methods. The code and models are publicly available.", "output": "Spectral Transfer Guided Active Domain Adaptation For Thermal Imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ultrasound is the primary modality to examine fetal growth during pregnancy,while the image quality could be affected by various factors. Qualityassessment is essential for controlling the quality of ultrasound images toguarantee both the perceptual and diagnostic values. Existing automatedapproaches often require heavy structural annotations and the predictions maynot necessarily be consistent with the assessment results by human experts.Furthermore, the overall quality of a scan and the correlation between thequality of frames should not be overlooked. In this work, we propose areinforcement learning framework powered by two hierarchical agents thatcollaboratively learn to perform both frame-level and video-level qualityassessments. It is equipped with a specially-designed reward mechanism thatconsiders temporal dependency among frame quality and only requires sparsebinary annotations to train. Experimental results on a challenging fetal braindataset verify that the proposed framework could perform dual-level qualityassessment and its predictions correlate well with the subjective assessmentresults.", "output": "Hierarchical Agent-based Reinforcement Learning Framework for Automated Quality Assessment of Fetal Ultrasound Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Low-light image enhancement (LLIE) investigates how to improve illuminationand produce normal-light images. The majority of existing methods improvelow-light images via a global and uniform manner, without taking into accountthe semantic information of different regions. Without semantic priors, anetwork may easily deviate from a region's original color. To address thisissue, we propose a novel semantic-aware knowledge-guided framework (SKF) thatcan assist a low-light enhancement model in learning rich and diverse priorsencapsulated in a semantic segmentation model. We concentrate on incorporatingsemantic knowledge from three key aspects: a semantic-aware embedding modulethat wisely integrates semantic priors in feature representation space, asemantic-guided color histogram loss that preserves color consistency ofvarious instances, and a semantic-guided adversarial loss that produces morenatural textures by semantic priors. Our SKF is appealing in acting as ageneral framework in LLIE task. Extensive experiments show that models equippedwith the SKF significantly outperform the baselines on multiple datasets andour SKF generalizes to different models and scenes well. The code is availableat Semantic-Aware-Low-Light-Image-Enhancement.", "output": "Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Indirect Time of Flight LiDARs can indirectly calculate the scene's depthfrom the phase shift angle between transmitted and received laser signals withamplitudes modulated at a predefined frequency. Unfortunately, this methodgenerates ambiguity in calculated depth when the phase shift angle valueexceeds $2pi$. Current state-of-the-art methods use raw samples generatedusing two distinct modulation frequencies to overcome this ambiguity problem.However, this comes at the cost of increasing laser components' stress andraising their temperature, which reduces their lifetime and increases powerconsumption. In our work, we study two different methods to recover the entiredepth range of the LiDAR using fewer raw data sample shots from a singlemodulation frequency with the support of sensor's gray scale output to reducethe laser components' stress and power consumption.", "output": "Near Field iToF LIDAR Depth Improvement from Limited Number of Shots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper discusses the results for the second edition of the MonocularDepth Estimation Challenge (MDEC). This edition was open to methods using anyform of supervision, including fully-supervised, self-supervised, multi-task orproxy depth. The challenge was based around the SYNS-Patches dataset, whichfeatures a wide diversity of environments with high-quality dense ground-truth.This includes complex natural environments, e.g. forests or fields, which aregreatly underrepresented in current benchmarks.The challenge received eight unique submissions that outperformed theprovided SotA baseline on any of the pointcloud- or image-based metrics. Thetop supervised submission improved relative F-Score by 27.62%, while the topself-supervised improved it by 16.61%. Supervised submissions generallyleveraged large collections of datasets to improve data diversity.Self-supervised submissions instead updated the network architecture andpretrained backbones. These results represent a significant progress in thefield, while highlighting avenues for future research, such as reducinginterpolation artifacts at depth boundaries, improving self-supervised indoorperformance and overall natural image accuracy.", "output": "The Second Monocular Depth Estimation Challenge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have witnessed an exponential increase in the demand for facevideo compression, and the success of artificial intelligence has expanded theboundaries beyond traditional hybrid video coding. Generative coding approacheshave been identified as promising alternatives with reasonable perceptualrate-distortion trade-offs, leveraging the statistical priors of face videos.However, the great diversity of distortion types in spatial and temporaldomains, ranging from the traditional hybrid coding frameworks to generativemodels, present grand challenges in compressed face video quality assessment(VQA). In this paper, we introduce the large-scale Compressed Face VideoQuality Assessment (CFVQA) database, which is the first attempt tosystematically understand the perceptual quality and diversified compressiondistortions in face videos. The database contains 3,240 compressed face videoclips in multiple compression levels, which are derived from 135 source videoswith diversified content using six representative video codecs, including twotraditional methods based on hybrid coding frameworks, two end-to-end methods,and two generative methods. In addition, a FAce VideO IntegeRity (FAVOR) indexfor face video compression was developed to measure the perceptual quality,considering the distinct content characteristics and temporal priors of theface videos. Experimental results exhibit its superior performance on theproposed CFVQA dataset. The benchmark is now made publicly available at:", "output": "Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating synthetic datasets for training face recognition models ischallenging because dataset generation entails more than creating high fidelityimages. It involves generating multiple images of same subjects under differentfactors (textit{e.g.}, variations in pose, illumination, expression, aging andocclusion) which follows the real image conditional distribution. Previousworks have studied the generation of synthetic datasets using GAN or 3D models.In this work, we approach the problem from the aspect of combining subjectappearance (ID) and external factor (style) conditions. These two conditionsprovide a direct way to control the inter-class and intra-class variations. Tothis end, we propose a Dual Condition Face Generator (DCFace) based on adiffusion model. Our novel Patch-wise style extractor and Time-step dependentID loss enables DCFace to consistently produce face images of the same subjectunder different styles with precise control. Face recognition models trained onsynthetic images from the proposed DCFace provide higher verificationaccuracies compared to previous works by $6.11%$ on average in $4$ out of $5$test datasets, LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code is available at", "output": "DCFace: Synthetic Face Generation with Dual Condition Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The highly structured energy landscape of the loss as a function ofparameters for deep neural networks makes it necessary to use sophisticatedoptimization strategies in order to discover (local) minima that guaranteereasonable performance. Overcoming less suitable local minima is an importantprerequisite and often momentum methods are employed to achieve this. As inother non local optimization procedures, this however creates the necessity tobalance between exploration and exploitation. In this work, we suggest an eventbased control mechanism for switching from exploration to exploitation based onreaching a predefined reduction of the loss function. As we give the momentummethod a port Hamiltonian interpretation, we apply the 'heavy ball withfriction' interpretation and trigger breaking (or friction) when achievingcertain goals. We benchmark our method against standard stochastic gradientdescent and provide experimental evidence for improved performance of deepneural networks when our strategy is applied.", "output": "Who breaks early, looses: goal oriented training of deep neural networks based on port Hamiltonian dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Structured reconstruction is a non-trivial dense prediction problem, whichextracts structural information (eg, building corners and edges) from a rasterimage, then reconstructs it to a 2D planar graph accordingly. Compared withcommon segmentation or detection problems, it significantly relays on thecapability that leveraging holistic geometric information for structuralreasoning. Current transformer-based approaches tackle this challenging problemin a two-stage manner, which detect corners in the first model and classify theproposed edges (corner-pairs) in the second model. However, they separatetwo-stage into different models and only share the backbone encoder. Unlike theexisting modeling strategies, we present an enhanced corner representationmethod: 1) It fuses knowledge between the corner detection and edge predictionby sharing feature in different granularity; 2) Corner candidates are proposedin four heatmap channels w.r.t its direction. Both qualitative and quantitativeevaluations demonstrate that our proposed method can better reconstructfine-grained structures, such as adjacent corners and tiny edges. Consequently,it outperforms the state-of-the-art model by +1.9%@F-1 on Corner and+3.0%@F-1 on Edge.", "output": "CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic and periodic recompiling of building databases with up-to-datehigh-resolution images has become a critical requirement for rapidly developingurban environments. However, the architecture of most existing approaches forchange extraction attempts to learn features related to changes but ignoresobjectives related to buildings. This inevitably leads to the generation ofsignificant pseudo-changes, due to factors such as seasonal changes in imagesand the inclination of building fac{c}ades. To alleviate the above-mentionedproblems, we developed a contrastive learning approach by validating historicalbuilding footprints against single up-to-date remotely sensed images. Thiscontrastive learning strategy allowed us to inject the semantics of buildingsinto a pipeline for the detection of changes, which is achieved by increasingthe distinguishability of features of buildings from those of non-buildings. Inaddition, to reduce the effects of inconsistencies between historical buildingpolygons and buildings in up-to-date images, we employed a deformableconvolutional neural network to learn offsets intuitively. In summary, weformulated a multi-branch building extraction method that identifies newlyconstructed and removed buildings, respectively. To validate our method, weconducted comparative experiments using the public Wuhan University buildingchange detection dataset and a more practical dataset named SI-BU that weestablished. Our method achieved F1 scores of 93.99% and 70.74% on the abovedatasets, respectively. Moreover, when the data of the public dataset weredivided in the same manner as in previous related studies, our method achievedan F1 score of 94.63%, which surpasses that of the state-of-the-art method.", "output": "BCE-Net: Reliable Building Footprints Change Extraction based on Historical Map and Up-to-Date Images using Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a DETR-based method for cross-domain weakly supervisedobject detection (CDWSOD), aiming at adapting the detector from source totarget domain through weak supervision. We think DETR has strong potential forCDWSOD due to an insight: the encoder and the decoder in DETR are both based onthe attention mechanism and are thus capable of aggregating semantics acrossthe entire image. The aggregation results, i.e., image-level predictions, cannaturally exploit the weak supervision for domain alignment. Such motivated, wepropose DETR with additional Global Aggregation (DETR-GA), a CDWSOD detectorthat simultaneously makes \"instance-level + image-level\" predictions andutilizes \"strong + weak\" supervisions. The key point of DETR-GA is very simple:for the encoder / decoder, we respectively add multiple class queries / aforeground query to aggregate the semantics into image-level predictions. Ourquery-based aggregation has two advantages. First, in the encoder, theweakly-supervised class queries are capable of roughly locating thecorresponding positions and excluding the distraction from non-relevantregions. Second, through our design, the object queries and the foregroundquery in the decoder share consensus on the class semantics, therefore makingthe strong and weak supervision mutually benefit each other for domainalignment. Extensive experiments on four popular cross-domain benchmarks showthat DETR-GA significantly improves CSWSOD and advances the states of the art(e.g., 29.0% --&gt; 79.4% mAP on PASCAL VOC --&gt; Clipart_all dataset).", "output": "DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion probabilistic models have been successful in generatinghigh-quality and diverse images. However, traditional models, whose input andoutput are high-resolution images, suffer from excessive memory requirements,making them less practical for edge devices. Previous approaches for generativeadversarial networks proposed a patch-based method that uses positionalencoding and global content information. Nevertheless, designing a patch-basedapproach for diffusion probabilistic models is non-trivial. In this paper, weresent a diffusion probabilistic model that generates images on apatch-by-patch basis. We propose two conditioning methods for a patch-basedgeneration. First, we propose position-wise conditioning using one-hotrepresentation to ensure patches are in proper positions. Second, we proposeGlobal Content Conditioning (GCC) to ensure patches have coherent content whenconcatenated together. We evaluate our model qualitatively and quantitativelyon CelebA and LSUN bedroom datasets and demonstrate a moderate trade-offbetween maximum memory consumption and generated image quality. Specifically,when an entire image is divided into 2 x 2 patches, our proposed approach canreduce the maximum memory consumption by half while maintaining comparableimage quality.", "output": "Memory Efficient Diffusion Probabilistic Models via Patch-based Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Delta Denoising Score (DDS), a novel scoring function fortext-based image editing that guides minimal modifications of an input imagetowards the content described in a target prompt. DDS leverages the richgenerative prior of text-to-image diffusion models and can be used as a lossterm in an optimization problem to steer an image towards a desired directiondictated by a text. DDS utilizes the Score Distillation Sampling (SDS)mechanism for the purpose of image editing. We show that using only SDS oftenproduces non-detailed and blurry outputs due to noisy gradients. To addressthis issue, DDS uses a prompt that matches the input image to identify andremove undesired erroneous directions of SDS. Our key premise is that SDSshould be zero when calculated on pairs of matched prompts and images, meaningthat if the score is non-zero, its gradients can be attributed to the erroneouscomponent of SDS. Our analysis demonstrates the competence of DDS for textbased image-to-image translation. We further show that DDS can be used to trainan effective zero-shot image translation model. Experimental results indicatethat DDS outperforms existing methods in terms of stability and quality,highlighting its potential for real-world applications in text-based imageediting.", "output": "Delta Denoising Score."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual reasoning is a long-term goal of vision research. In the last decade,several works have attempted to apply deep neural networks (DNNs) to the taskof learning visual relations from images, with modest results in terms of thegeneralization of the relations learned. In recent years, several innovationsin DNNs have been developed in order to enable learning abstract relation fromimages. In this work, we systematically evaluate a series of DNNs thatintegrate mechanism such as slot attention, recurrently guided attention, andexternal memory, in the simplest possible visual reasoning task: decidingwhether two objects are the same or different. We found that, although somemodels performed better than others in generalizing the same-different relationto specific types of images, no model was able to generalize this relationacross the board. We conclude that abstract visual reasoning remains largely anunresolved challenge for DNNs.", "output": "The role of object-centric representations, guided attention, and external memory on generalizing visual relations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Alzheimer's Disease (AD), which is the most common cause of dementia, is aprogressive disease preceded by Mild Cognitive Impairment (MCI). Earlydetection of the disease is crucial for making treatment decisions. However,most of the literature on computer-assisted detection of AD focuses onclassifying brain images into one of three major categories: healthy, MCI, andAD; or categorising MCI patients into one of (1) progressive: those whoprogress from MCI to AD at a future examination time during a given studyperiod, and (2) stable: those who stay as MCI and never progress to AD. Thismisses the opportunity to accurately identify the trajectory of progressive MCIpatients. In this paper, we revisit the brain image classification task for ADidentification and re-frame it as an ordinal classification task to predict howclose a patient is to the severe AD stage. To this end, we select progressiveMCI patients from the Alzheimer's Disease Neuroimaging Initiative (ADNI)dataset and construct an ordinal dataset with a prediction target thatindicates the time to progression to AD. We train a siamese network model topredict the time to onset of AD based on MRI brain images. We also propose aweighted variety of siamese networks and compare its performance to a baselinemodel. Our evaluations show that incorporating a weighting factor to siamesenetworks brings considerable performance gain at predicting how close inputbrain MRI images are to progressing to AD.", "output": "Weighted Siamese Network to Predict the Time to Onset of Alzheimer's Disease from MRI Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose SampleDepth, a Convolutional Neural Network (CNN), that is suitedfor an adaptive LiDAR. Typically,LiDAR sampling strategy is pre-defined,constant and independent of the observed scene. Instead of letting a LiDARsample the scene in this agnostic fashion, SampleDepth determines, adaptively,where it is best to sample the current frame.To do that, SampleDepth uses depthsamples from previous time steps to predict a sampling mask for the currentframe. Crucially, SampleDepth is trained to optimize the performance of a depthcompletion downstream task. SampleDepth is evaluated on two different depthcompletion networks and two LiDAR datasets, KITTI Depth Completion and thenewly introduced synthetic dataset, SHIFT. We show that SampleDepth iseffective and suitable for different depth completion downstream tasks.", "output": "Prior based Sampling for Adaptive LiDAR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-organ segmentation, which identifies and separates different organs inmedical images, is a fundamental task in medical image analysis. Recently, theimmense success of deep learning motivated its wide adoption in multi-organsegmentation tasks. However, due to expensive labor costs and expertise, theavailability of multi-organ annotations is usually limited and hence poses achallenge in obtaining sufficient training data for deep learning-basedmethods. In this paper, we aim to address this issue by combining off-the-shelfsingle-organ segmentation models to develop a multi-organ segmentation model onthe target dataset, which helps get rid of the dependence on annotated data formulti-organ segmentation. To this end, we propose a novel dual-stage methodthat consists of a Model Adaptation stage and a Model Ensemble stage. The firststage enhances the generalization of each off-the-shelf segmentation model onthe target domain, while the second stage distills and integrates knowledgefrom multiple adapted single-organ segmentation models. Extensive experimentson four abdomen datasets demonstrate that our proposed method can effectivelyleverage off-the-shelf single-organ segmentation models to obtain a tailoredmodel for multi-organ segmentation with high accuracy.", "output": "Tailored Multi-Organ Segmentation with Model Adaptation and Ensemble."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optical flow provides information on relative motion that is an importantcomponent in many computer vision pipelines. Neural networks provide highaccuracy optical flow, yet their complexity is often prohibitive forapplication at the edge or in robots, where efficiency and latency play crucialrole. To address this challenge, we build on the latest developments inevent-based vision and spiking neural networks. We propose a new networkarchitecture, inspired by Timelens, that improves the state-of-the-artself-supervised optical flow accuracy when operated both in spiking andnon-spiking mode. To implement a real-time pipeline with a physical eventcamera, we propose a methodology for principled model simplification based onactivity and latency analysis. We demonstrate high speed optical flowprediction with almost two orders of magnitude reduced complexity whilemaintaining the accuracy, opening the path for real-time deployments.", "output": "Neuromorphic Optical Flow and Real-time Implementation with Event Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point clouds are widely regarded as one of the best dataset types for urbanmapping purposes. Hence, point cloud datasets are commonly investigated asbenchmark types for various urban interpretation methods. Yet, few researchershave addressed the use of point cloud benchmarks for fac{c}ade segmentation.Robust fac{c}ade segmentation is becoming a key factor in various applicationsranging from simulating autonomous driving functions to preserving culturalheritage. In this work, we present a method of enriching existing point clouddatasets with fac{c}ade-related classes that have been designed to facilitatefac{c}ade segmentation testing. We propose how to efficiently extend existingdatasets and comprehensively assess their potential for fac{c}adesegmentation. We use the method to create the TUM-FAc{C}ADE dataset, whichextends the capabilities of TUM-MLS-2016. Not only can TUM-FAc{C}ADEfacilitate the development of point-cloud-based fac{c}ade segmentation tasks,but our procedure can also be applied to enrich further datasets.", "output": "TUM-FA\\c{C}ADE: Reviewing and enriching point cloud benchmarks for fa\\c{c}ade segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cancer is a highly heterogeneous condition that can occur almost anywhere inthe human body. 18F-fluorodeoxyglucose is an imaging modality commonly used todetect cancer due to its high sensitivity and clear visualisation of thepattern of metabolic activity. Nonetheless, as cancer is highly heterogeneous,it is challenging to train general-purpose discriminative cancer detectionmodels, with data availability and disease complexity often cited as a limitingfactor. Unsupervised anomaly detection models have been suggested as a putativesolution. These models learn a healthy representation of tissue and detectcancer by predicting deviations from the healthy norm, which requires modelscapable of accurately learning long-range interactions between organs and theirimaging patterns with high levels of expressivity. Such characteristics aresuitably satisfied by transformers, which have been shown to generatestate-of-the-art results in unsupervised anomaly detection by training onnormal data. This work expands upon such approaches by introducing multi-modalconditioning of the transformer via cross-attention i.e. supplying anatomicalreference from paired CT. Using 294 whole-body PET/CT samples, we show that ouranomaly detection method is robust and capable of achieving accurate cancerlocalization results even in cases where normal training data is unavailable.In addition, we show the efficacy of this approach on out-of-sample datashowcasing the generalizability of this approach with limited training data.Lastly, we propose to combine model uncertainty with a new kernel densityestimation approach, and show that it provides clinically and statisticallysignificant improvements when compared to the classic residual-based anomalymaps. Overall, a superior performance is demonstrated against leadingstate-of-the-art alternatives, drawing attention to the potential of theseapproaches.", "output": "Cross Attention Transformers for Multi-modal Unsupervised Whole-Body PET Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For visual estimation of optical flow, a crucial function for many visiontasks, unsupervised learning, using the supervision of view synthesis hasemerged as a promising alternative to supervised methods, since ground-truthflow is not readily available in many cases. However, unsupervised learning islikely to be unstable when pixel tracking is lost due to occlusion and motionblur, or the pixel matching is impaired due to variation in image content andspatial structure over time. In natural environments, dynamic occlusion orobject variation is a relatively slow temporal process spanning several frames.We, therefore, explore the optical flow estimation from multiple-framesequences of dynamic scenes, whereas most of the existing unsupervisedapproaches are based on temporal static models. We handle the unsupervisedoptical flow estimation with a temporal dynamic model by introducing aspatial-temporal dual recurrent block based on the predictive coding structure,which feeds the previous high-level motion prior to the current optical flowestimator. Assuming temporal smoothness of optical flow, we use motion priorsof the adjacent frames to provide more reliable supervision of the occludedregions. To grasp the essence of challenging scenes, we simulate variousscenarios across long sequences, including dynamic occlusion, contentvariation, and spatial variation, and adopt self-supervised distillation tomake the model understand the object's motion patterns in a prolonged dynamicenvironment. Experiments on KITTI 2012, KITTI 2015, Sintel Clean, and SintelFinal datasets demonstrate the effectiveness of our methods on unsupervisedoptical flow estimation. The proposal achieves state-of-the-art performancewith advantages in memory overhead.", "output": "Unsupervised Learning Optical Flow in Multi-frame Dynamic Environment Using Temporal Dynamic Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solar activity is one of the main drivers of variability in our solar systemand the key source of space weather phenomena that affect Earth and near Earthspace. The extensive record of high resolution extreme ultraviolet (EUV)observations from the Solar Dynamics Observatory (SDO) offers an unprecedented,very large dataset of solar images. In this work, we make use of thiscomprehensive dataset to investigate capabilities of current state-of-the-artgenerative models to accurately capture the data distribution behind theobserved solar activity states. Starting from StyleGAN-based methods, weuncover severe deficits of this model family in handling fine-scale details ofsolar images when training on high resolution samples, contrary to training onnatural face images. When switching to the diffusion based generative modelfamily, we observe strong improvements of fine-scale detail generation. For theGAN family, we are able to achieve similar improvements in fine-scalegeneration when turning to ProjectedGANs, which uses multi-scale discriminatorswith a pre-trained frozen feature extractor. We conduct ablation studies toclarify mechanisms responsible for proper fine-scale handling. Usingdistributed training on supercomputers, we are able to train generative modelsfor up to 1024x1024 resolution that produce high quality samplesindistinguishable to human experts, as suggested by the evaluation we conduct.We make all code, models and workflows used in this study publicly available aturl{", "output": "A Comparative Study on Generative Models for High Resolution Solar Observation Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, media reports have called out bias and racism in facerecognition technology. We review experimental results exploring severalspeculated causes for asymmetric cross-demographic performance. We consideraccuracy differences as represented by variations in non-mated (impostor) and /or mated (genuine) distributions for 1-to-1 face matching. Possible causesexplored include differences in skin tone, face size and shape, imbalance innumber of identities and images in the training data, and amount of facevisible in the test data (\"face pixels\"). We find that demographic differencesin face pixel information of the test images appear to most directly impact theresultant differences in face recognition accuracy.", "output": "Exploring Causes of Demographic Variations In Face Recognition Accuracy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent breakthroughs in natural language processing for model pretrainingon large quantities of data have opened the way for similar foundation modelsin computer vision. These models could greatly simplify the use of images inany system by producing all-purpose visual features, i.e., features that workacross image distributions and tasks without finetuning. This work shows thatexisting pretraining methods, especially self-supervised methods, can producesuch features if trained on enough curated data from diverse sources. Werevisit existing approaches and combine different techniques to scale ourpretraining in terms of data and model size. Most of the technicalcontributions aim at accelerating and stabilizing the training at scale. Interms of data, we propose an automatic pipeline to build a dedicated, diverse,and curated image dataset instead of uncurated data, as typically done in theself-supervised literature. In terms of models, we train a ViT model(Dosovitskiy et al., 2020) with 1B parameters and distill it into a series ofsmaller models that surpass the best available all-purpose features, OpenCLIP(Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.", "output": "DINOv2: Learning Robust Visual Features without Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding semantic scene segmentation of urban scenes captured from theUnmanned Aerial Vehicles (UAV) perspective plays a vital role in building aperception model for UAV. With the limitations of large-scale densely labeleddata, semantic scene segmentation for UAV views requires a broad understandingof an object from both its top and side views. Adapting from well-annotatedautonomous driving data to unlabeled UAV data is challenging due to thecross-view differences between the two data types. Our work proposes a novelCross-View Adaptation (CROVIA) approach to effectively adapt the knowledgelearned from on-road vehicle views to UAV views. First, a novel geometry-basedconstraint to cross-view adaptation is introduced based on the geometrycorrelation between views. Second, cross-view correlations from image space areeffectively transferred to segmentation space without any requirement of pairedon-road and UAV view data via a new Geometry-Constraint Cross-View (GeiCo)loss. Third, the multi-modal bijective networks are introduced to enforce theglobal structural modeling across views. Experimental results on new cross-viewadaptation benchmarks introduced in this work, i.e., SYNTHIA to UAVID and GTA5to UAVID, show the State-of-the-Art (SOTA) performance of our approach overprior adaptation methods", "output": "CROVIA: Seeing Drone Scenes from Car Perspective via Cross-View Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Event-based sensors have recently drawn increasing interest in roboticperception due to their lower latency, higher dynamic range, and lowerbandwidth requirements compared to standard CMOS-based imagers. Theseproperties make them ideal tools for real-time perception tasks in highlydynamic environments. In this work, we demonstrate an application where eventcameras excel: accurately estimating the impact location of fast-movingobjects. We introduce a lightweight event representation called Binary EventHistory Image (BEHI) to encode event data at low latency, as well as alearning-based approach that allows real-time inference of a confidence-enabledcontrol signal to the robot. To validate our approach, we present anexperimental catching system in which we catch fast-flying ping-pong balls. Weshow that the system is capable of achieving a success rate of 81% in catchingballs targeted at different locations, with a velocity of up to 13 m/s even oncompute-constrained embedded platforms such as the Nvidia Jetson NX.", "output": "EV-Catcher: High-Speed Object Catching Using Low-latency Event-based Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vegetation structure mapping is critical for understanding the global carboncycle and monitoring nature-based approaches to climate adaptation andmitigation. Repeat measurements of these data allow for the observation ofdeforestation or degradation of existing forests, natural forest regeneration,and the implementation of sustainable agricultural practices like agroforestry.Assessments of tree canopy height and crown projected area at a high spatialresolution are also important for monitoring carbon fluxes and assessingtree-based land uses, since forest structures can be highly spatiallyheterogeneous, especially in agroforestry systems. Very high resolutionsatellite imagery (less than one meter (1m) ground sample distance) makes itpossible to extract information at the tree level while allowing monitoring ata very large scale. This paper presents the first high-resolution canopy heightmap concurrently produced for multiple sub-national jurisdictions.Specifically, we produce canopy height maps for the states of California andS~{a}o Paolo, at sub-meter resolution, a significant improvement over the tenmeter (10m) resolution of previous Sentinel / GEDI based worldwide maps ofcanopy height. The maps are generated by applying a vision transformer tofeatures extracted from a self-supervised model in Maxar imagery from 2017 to2020, and are trained against aerial lidar and GEDI observations. We evaluatethe proposed maps with set-aside validation lidar data as well as by comparingwith other remotely sensed maps and field-collected data, and find our modelproduces an average Mean Absolute Error (MAE) within set-aside validation areasof 3.0 meters.", "output": "Sub-meter resolution canopy height maps using self-supervised learning and a vision transformer trained on Aerial and GEDI Lidar."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, pre-trained point cloud models have found extensive applications indownstream tasks like object classification. However, these tasks often require{full fine-tuning} of models and lead to storage-intensive procedures, thuslimiting the real applications of pre-trained models. Inspired by the greatsuccess of visual prompt tuning (VPT) in vision, we attempt to explore prompttuning, which serves as an efficient alternative to full fine-tuning forlarge-scale models, to point cloud pre-trained models to reduce storage costs.However, it is non-trivial to apply the traditional static VPT to point clouds,owing to the distribution diversity of point cloud data. For instance, thescanned point clouds exhibit various types of missing or noisy points. Toaddress this issue, we propose an Instance-aware Dynamic Prompt Tuning (IDPT)for point cloud pre-trained models, which utilizes a prompt module to perceivethe semantic prior features of each instance. This semantic prior facilitatesthe learning of unique prompts for each instance, thus enabling downstreamtasks to robustly adapt to pre-trained point cloud models. Notably, extensiveexperiments conducted on downstream tasks demonstrate that IDPT outperformsfull fine-tuning in most tasks with a mere 7% of the trainable parameters,thus significantly reducing the storage pressure. Code is available aturl{", "output": "Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pedestrian attribute recognition (PAR) has received increasing attentionbecause of its wide application in video surveillance and pedestrian analysis.Extracting robust feature representation is one of the key challenges in thistask. The existing methods mainly use the convolutional neural network (CNN) asthe backbone network to extract features. However, these methods mainly focuson small discriminative regions while ignoring the global perspective. Toovercome these limitations, we propose a pure transformer-based multi-task PARnetwork named PARFormer, which includes four modules. In the feature extractionmodule, we build a transformer-based strong baseline for feature extraction,which achieves competitive results on several PAR benchmarks compared with theexisting CNN-based baseline methods. In the feature processing module, wepropose an effective data augmentation strategy named batch random mask (BRM)block to reinforce the attentive feature learning of random patches.Furthermore, we propose a multi-attribute center loss (MACL) to enhance theinter-attribute discriminability in the feature representations. In theviewpoint perception module, we explore the impact of viewpoints on pedestrianattributes, and propose a multi-view contrastive loss (MCVL) that enables thenetwork to exploit the viewpoint information. In the attribute recognitionmodule, we alleviate the negative-positive imbalance problem to generate theattribute predictions. The above modules interact and jointly learn a highlydiscriminative feature space, and supervise the generation of the finalfeatures. Extensive experimental results show that the proposed PARFormernetwork performs well compared to the state-of-the-art methods on severalpublic datasets, including PETA, RAP, and PA100K. Code will be released at", "output": "PARFormer: Transformer-based Multi-Task Network for Pedestrian Attribute Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The University of California San Francisco Brain Metastases StereotacticRadiosurgery (UCSF-BMSR) dataset is a public, clinical, multimodal brain MRIdataset consisting of 560 brain MRIs from 412 patients with expert annotationsof 5136 brain metastases. Data consists of registered and skull stripped T1post-contrast, T1 pre-contrast, FLAIR and subtraction (T1 pre-contrast - T1post-contrast) images and voxelwise segmentations of enhancing brain metastasesin NifTI format. The dataset also includes patient demographics, surgicalstatus and primary cancer types. The UCSF-BSMR has been made publicly availablein the hopes that researchers will use these data to push the boundaries of AIapplications for brain metastases.", "output": "The University of California San Francisco, Brain Metastases Stereotactic Radiosurgery (UCSF-BMSR) MRI Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The localization of objects is a crucial task in various applications such asrobotics, virtual and augmented reality, and the transportation of goods inwarehouses. Recent advances in deep learning have enabled the localizationusing monocular visual cameras. While structure from motion (SfM) predicts theabsolute pose from a point cloud, absolute pose regression (APR) methods learna semantic understanding of the environment through neural networks. However,both fields face challenges caused by the environment such as motion blur,lighting changes, repetitive patterns, and feature-less structures. This studyaims to address these challenges by incorporating additional information andregularizing the absolute pose using relative pose regression (RPR) methods.The optical flow between consecutive images is computed using the Lucas-Kanadealgorithm, and the relative pose is predicted using an auxiliary smallrecurrent convolutional network. The fusion of absolute and relative poses is acomplex task due to the mismatch between the global and local coordinatesystems. State-of-the-art methods fusing absolute and relative poses use posegraph optimization (PGO) to regularize the absolute pose predictions usingrelative poses. In this work, we propose recurrent fusion networks to optimallyalign absolute and relative pose predictions to improve the absolute poseprediction. We evaluate eight different recurrent units and construct asimulation environment to pre-train the APR and RPR networks for bettergeneralized training. Additionally, we record a large database of differentscenarios in a challenging large-scale indoor environment that mimics awarehouse with transportation robots. We conduct hyperparameter searches andexperiments to show the effectiveness of our recurrent fusion method comparedto PGO.", "output": "Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Dynamic Mobile-Former(DMF), maximizes the capabilities ofdynamic convolution by harmonizing it with efficient operators.Our DynamicMobileFormer effectively utilizes the advantages of Dynamic MobileNet(MobileNet equipped with dynamic convolution) using global information fromlight-weight attention.A Transformer in Dynamic Mobile-Former only requires afew randomly initialized tokens to calculate global features, making itcomputationally efficient.And a bridge between Dynamic MobileNet andTransformer allows for bidirectional integration of local and globalfeatures.We also simplify the optimization process of vanilla dynamicconvolution by splitting the convolution kernel into an input-agnostic kerneland an input-dependent kernel.This allows for optimization in a wider kernelspace, resulting in enhanced capacity.By integrating lightweight attention andenhanced dynamic convolution, our Dynamic Mobile-Former achieves not only highefficiency, but also strong performance.We benchmark the Dynamic Mobile-Formeron a series of vision tasks, and showcase that it achieves impressiveperformance on image classification, COCO detection, and instanacesegmentation.For example, our DMF hits the top-1 accuracy of 79.4% onImageNet-1K, much higher than PVT-Tiny by 4.3% with only 1/4FLOPs.Additionally,our proposed DMF-S model performed well on challengingvision datasets such as COCO, achieving a 39.0% mAP,which is 1% higher thanthat of the Mobile-Former 508M model, despite using 3 GFLOPs lesscomputations.Code and models are available at ", "output": "Dynamic Mobile-Former: Strengthening Dynamic Convolution with Attention and Residual Connection in Kernel Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection has seen remarkable progress in recent years with theintroduction of Convolutional Neural Networks (CNN). Object detection is amulti-task learning problem where both the position of the objects in theimages as well as their classes needs to be correctly identified. The idea hereis to maximize the overlap between the ground-truth bounding boxes and thepredictions i.e. the Intersection over Union (IoU). In the scope of work seencurrently in this domain, IoU is approximated by using the Huber loss as aproxy but this indirect method does not leverage the IoU information and treatsthe bounding box as four independent, unrelated terms of regression. This isnot true for a bounding box where the four coordinates are highly correlatedand hold a semantic meaning when taken together. The direct optimization of theIoU is not possible due to its non-convex and non-differentiable nature. Inthis paper, we have formulated a novel loss namely, the Smooth IoU, whichdirectly optimizes the IoUs for the bounding boxes. This loss has beenevaluated on the Oxford IIIT Pets, Udacity self-driving car, PASCAL VOC, andVWFS Car Damage datasets and has shown performance gains over the standardHuber loss.", "output": "Directly Optimizing IoU for Bounding Box Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization (DG), aiming at models able to work on multiple unseendomains, is a must-have characteristic of general artificial intelligence. DGbased on single source domain training data is more challenging due to the lackof comparable information to help identify domain invariant features. In thispaper, it is determined that the domain invariant features could be containedin the single source domain training samples, then the task is to find properways to extract such domain invariant features from the single source domainsamples. An assumption is made that the domain invariant features are closelyrelated to the frequency. Then, a new method that learns through multiplefrequency domains is proposed. The key idea is, dividing the frequency domainof each original image into multiple subdomains, and learning features in thesubdomain by a designed two branches network. In this way, the model isenforced to learn features from more samples of the specifically limitedspectrum, which increases the possibility of obtaining the domain invariantfeatures that might have previously been defiladed by easily learned features.Extensive experimental investigation reveals that 1) frequency decompositioncan help the model learn features that are difficult to learn. 2) the proposedmethod outperforms the state-of-the-art methods of single-source domaingeneralization.", "output": "Frequency Decomposition to Tap the Potential of Single Domain for Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The strength of machine learning models stems from their ability to learncomplex function approximations from data; however, this strength also makestraining deep neural networks challenging. Notably, the complex models tend tomemorize the training data, which results in poor regularization performance ontest data. The regularization techniques such as L1, L2, dropout, etc. areproposed to reduce the overfitting effect; however, they bring in additionalhyperparameters tuning complexity. These methods also fall short when theinter-class similarity is high due to the underlying data distribution, leadingto a less accurate model. In this paper, we present a novel approach toregularize the models by leveraging the information-rich latent embeddings andtheir high intra-class correlation. We create phantom embeddings from a subsetof homogenous samples and use these phantom embeddings to decrease theinter-class similarity of instances in their latent embedding space. Theresulting models generalize better as a combination of their embedding andregularize them without requiring an expensive hyperparameter search. Weevaluate our method on two popular and challenging image classificationdatasets (CIFAR and FashionMNIST) and show how our approach outperforms thestandard baselines while displaying better training behavior.", "output": "Phantom Embeddings: Using Embedding Space for Model Regularization in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The standard non-invasive imaging technique used to assess the severity andextent of Coronary Artery Disease (CAD) is Coronary Computed TomographyAngiography (CCTA). However, manual grading of each patient's CCTA according tothe CAD-Reporting and Data System (CAD-RADS) scoring is time-consuming andoperator-dependent, especially in borderline cases. This work proposes a fullyautomated, and visually explainable, deep learning pipeline to be used as adecision support system for the CAD screening procedure. The pipeline performstwo classification tasks: firstly, identifying patients who require furtherclinical investigations and secondly, classifying patients into subgroups basedon the degree of stenosis, according to commonly used CAD-RADS thresholds. Thepipeline pre-processes multiplanar projections of the coronary arteries,extracted from the original CCTAs, and classifies them using a fine-tunedMulti-Axis Vision Transformer architecture. With the aim of emulating thecurrent clinical practice, the model is trained to assign a per-patient scoreby stacking the bi-dimensional longitudinal cross-sections of the three maincoronary arteries along channel dimension. Furthermore, it generates visuallyinterpretable maps to assess the reliability of the predictions. When run on adatabase of 1873 three-channel images of 253 patients collected at the MonzinoCardiology Center in Milan, the pipeline obtained an AUC of 0.87 and 0.93 forthe two classification tasks, respectively. According to our knowledge, this isthe first model trained to assign CAD-RADS scores learning solely from patientscores and not requiring finer imaging annotation steps that are not part ofthe clinical routine.", "output": "CAD-RADS scoring of coronary CT angiography with Multi-Axis Vision Transformer: a clinically-inspired deep learning pipeline."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the geometric properties of the functions learned by trainedConvNets in the preactivation space of their convolutional layers, byperforming an empirical study of hyperplane arrangements induced by aconvolutional layer. We introduce statistics over the weights of a trainednetwork to study local arrangements and relate them to the training dynamics.We observe that trained ConvNets show a significant statistical bias towardsregular hyperplane configurations. Furthermore, we find that layers showingbiased configurations are critical to validation performance for thearchitectures considered, trained on CIFAR10, CIFAR100 and ImageNet.", "output": "Hyperplane Arrangements of Trained ConvNets Are Biased."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces two new ensemble-based methods to reduce the data andcomputation costs of image classification. They can be used with any set ofclassifiers and do not require additional training. In the first approach, datausage is reduced by only analyzing a full-sized image if the model has lowconfidence in classifying a low-resolution pixelated version. When applied onthe best performing classifiers considered here, data usage is reduced by 61.2%on MNIST, 69.6% on KMNIST, 56.3% on FashionMNIST, 84.6% on SVHN, 40.6% onImageNet, and 27.6% on ImageNet-V2, all with a less than 5% reduction inaccuracy. However, for CIFAR-10, the pixelated data are not particularlyinformative, and the ensemble approach increases data usage while reducingaccuracy. In the second approach, compute costs are reduced by only using acomplex model if a simpler model has low confidence in its classification.Computation cost is reduced by 82.1% on MNIST, 47.6% on KMNIST, 72.3% onFashionMNIST, 86.9% on SVHN, 89.2% on ImageNet, and 81.5% on ImageNet-V2, allwith a less than 5% reduction in accuracy; for CIFAR-10 the correspondingimprovements are smaller at 13.5%. When cost is not an object, choosing theprojection from the most confident model for each observation increasesvalidation accuracy to 81.0% from 79.3% for ImageNet and to 69.4% from 67.5%for ImageNet-V2.", "output": "Problem-dependent attention and effort in neural networks with applications to image resolution and model selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image classifiers often rely overly on peripheral attributes that have astrong correlation with the target class (i.e., dataset bias) when makingpredictions. Due to the dataset bias, the model correctly classifies datasamples including bias attributes (i.e., bias-aligned samples) while failing tocorrectly predict those without bias attributes (i.e., bias-conflictingsamples). Recently, a myriad of studies focus on mitigating such dataset bias,the task of which is referred to as debiasing. However, our comprehensive studyindicates several issues need to be improved when conducting evaluation ofdebiasing in image classification. First, most of the previous studies do notspecify how they select their hyper-parameters and model checkpoints (i.e.,tuning criterion). Second, the debiasing studies until now evaluated theirproposed methods on datasets with excessively high bias-severities, showingdegraded performance on datasets with low bias severity. Third, the debiasingstudies do not share consistent experimental settings (e.g., datasets andneural networks) which need to be standardized for fair comparisons. Based onsuch issues, this paper 1) proposes an evaluation metric `Align-Conflict (AC)score' for the tuning criterion, 2) includes experimental settings with lowbias severity and shows that they are yet to be explored, and 3) unifies thestandardized experimental settings to promote fair comparisons betweendebiasing methods. We believe that our findings and lessons inspire futureresearchers in debiasing to further push state-of-the-art performances withfair comparisons.", "output": "Improving Evaluation of Debiasing in Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A growing body of work studies Blindspot Discovery Methods (\"BDM\"s): methodsthat use an image embedding to find semantically meaningful (i.e., united by ahuman-understandable concept) subsets of the data where an image classifierperforms significantly worse. Motivated by observed gaps in prior work, weintroduce a new framework for evaluating BDMs, SpotCheck, that uses syntheticimage datasets to train models with known blindspots and a new BDM, PlaneSpot,that uses a 2D image representation. We use SpotCheck to run controlledexperiments that identify factors that influence BDM performance (e.g., thenumber of blindspots in a model, or features used to define the blindspot) andshow that PlaneSpot is competitive with and in many cases outperforms existingBDMs. Importantly, we validate these findings by designing additionalexperiments that use real image data from MS-COCO, a large image benchmarkdataset. Our findings suggest several promising directions for future work onBDM design and evaluation. Overall, we hope that the methodology and analysespresented in this work will help facilitate a more rigorous science ofblindspot discovery.", "output": "Towards a More Rigorous Science of Blindspot Discovery in Image Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present NeuriCam, a novel deep learning-based system to achieve videocapture from low-power dual-mode IoT camera systems. Our idea is to design adual-mode camera system where the first mode is low-power (1.1 mW) but onlyoutputs grey-scale, low resolution, and noisy video and the second modeconsumes much higher power (100 mW) but outputs color and higher resolutionimages. To reduce total energy consumption, we heavily duty cycle the highpower mode to output an image only once every second. The data for this camerasystem is then wirelessly sent to a nearby plugged-in gateway, where we run ourreal-time neural network decoder to reconstruct a higher-resolution colorvideo. To achieve this, we introduce an attention feature filter mechanism thatassigns different weights to different features, based on the correlationbetween the feature map and the contents of the input frame at each spatiallocation. We design a wireless hardware prototype using off-the-shelf camerasand address practical issues including packet loss and perspective mismatch.Our evaluations show that our dual-camera approach reduces energy consumptionby 7x compared to existing systems. Further, our model achieves an averagegreyscale PSNR gain of 3.7 dB over prior single and dual-camera videosuper-resolution methods and 5.6 dB RGB gain over prior color propagationmethods. Open-source code: ", "output": "NeuriCam: Key-Frame Video Super-Resolution and Colorization for IoT Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Millimeter Wave (mmWave) Radar is gaining popularity as it can work inadverse environments like smoke, rain, snow, poor lighting, etc. Prior work hasexplored the possibility of reconstructing 3D skeletons or meshes from thenoisy and sparse mmWave Radar signals. However, it is unclear how accurately wecan reconstruct the 3D body from the mmWave signals across scenes and how itperforms compared with cameras, which are important aspects needed to beconsidered when either using mmWave radars alone or combining them withcameras. To answer these questions, an automatic 3D body annotation system isfirst designed and built up with multiple sensors to collect a large-scaledataset. The dataset consists of synchronized and calibrated mmWave radar pointclouds and RGB(D) images in different scenes and skeleton/mesh annotations forhumans in the scenes. With this dataset, we train state-of-the-art methods withinputs from different sensors and test them in various scenarios. The resultsdemonstrate that 1) despite the noise and sparsity of the generated pointclouds, the mmWave radar can achieve better reconstruction accuracy than theRGB camera but worse than the depth camera; 2) the reconstruction from themmWave radar is affected by adverse weather conditions moderately while theRGB(D) camera is severely affected. Further, analysis of the dataset and theresults shadow insights on improving the reconstruction from the mmWave radarand the combination of signals from different sensors.", "output": "mmBody Benchmark: 3D Body Reconstruction Dataset and Analysis for Millimeter Wave Radar."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-quality HDRIs(High Dynamic Range Images), typically HDR panoramas, areone of the most popular ways to create photorealistic lighting and 360-degreereflections of 3D scenes in graphics. Given the difficulty of capturing HDRIs,a versatile and controllable generative model is highly desired, where laymanusers can intuitively control the generation process. However, existingstate-of-the-art methods still struggle to synthesize high-quality panoramasfor complex scenes. In this work, we propose a zero-shot text-driven framework,Text2Light, to generate 4K+ resolution HDRIs without paired training data.Given a free-form text as the description of the scene, we synthesize thecorresponding HDRI with two dedicated steps: 1) text-driven panorama generationin low dynamic range(LDR) and low resolution, and 2) super-resolution inversetone mapping to scale up the LDR panorama both in resolution and dynamic range.Specifically, to achieve zero-shot text-driven panorama generation, we firstbuild dual codebooks as the discrete representation for diverse environmentaltextures. Then, driven by the pre-trained CLIP model, a text-conditioned globalsampler learns to sample holistic semantics from the global codebook accordingto the input text. Furthermore, a structure-aware local sampler learns tosynthesize LDR panoramas patch-by-patch, guided by holistic semantics. Toachieve super-resolution inverse tone mapping, we derive a continuousrepresentation of 360-degree imaging from the LDR panorama as a set ofstructured latent codes anchored to the sphere. This continuous representationenables a versatile module to upscale the resolution and dynamic rangesimultaneously. Extensive experiments demonstrate the superior capability ofText2Light in generating high-quality HDR panoramas. In addition, we show thefeasibility of our work in realistic rendering and immersive VR.", "output": "Text2Light: Zero-Shot Text-Driven HDR Panorama Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-supervised learning via teacher-student network can train a modeleffectively on a few labeled samples. It enables a student model to distillknowledge from the teacher's predictions of extra unlabeled data. However, suchknowledge flow is typically unidirectional, having the performance vulnerableto the quality of teacher model. In this paper, we seek to robust 3Dreconstruction of stereo endoscopic images by proposing a novel fashion ofbidirectional learning between two learners, each of which can play both rolesof teacher and student concurrently. Specifically, we introduce twoself-supervisions, i.e., Adaptive Cross Supervision (ACS) and Adaptive ParallelSupervision (APS), to learn a dual-branch convolutional neural network. The twobranches predict two different disparity probability distributions for the sameposition, and output their expectations as disparity values. The learnedknowledge flows across branches along two directions: a cross direction(disparity guides distribution in ACS) and a parallel direction (disparityguides disparity in APS). Moreover, each branch also learns confidences todynamically refine its provided supervisions. In ACS, the predicted disparityis softened into a unimodal distribution, and the lower the confidence, thesmoother the distribution. In APS, the incorrect predictions are suppressed bylowering the weights of those with low confidence. With the adaptivebidirectional learning, the two branches enjoy well-tuned supervisions, andeventually converge on a consistent and more accurate disparity estimation. Theextensive and comprehensive experimental results on four public datasetsdemonstrate our superior performance over other state-of-the-arts with arelative decrease of averaged disparity error by at least 9.76%.", "output": "Bidirectional Semi-supervised Dual-branch CNN for Robust 3D Reconstruction of Stereo Endoscopic Images via Adaptive Cross and Parallel Supervisions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a simple and novel method for generating 3D human motion fromcomplex natural language sentences, which describe different velocity,direction and composition of all kinds of actions. Different from existingmethods that use classical generative architecture, we apply the DenoisingDiffusion Probabilistic Model to this task, synthesizing diverse motion resultsunder the guidance of texts. The diffusion model converts white noise intostructured 3D motion by a Markov process with a series of denoising steps andis efficiently trained by optimizing a variational lower bound. To achieve thegoal of text-conditioned image synthesis, we use the classifier-free guidancestrategy to fuse text embedding into the model during training. Our experimentsdemonstrate that our model achieves competitive results on HumanML3D test setquantitatively and can generate more visually natural and diverse examples. Wealso show with experiments that our model is capable of zero-shot generation ofmotions for unseen text guidance.", "output": "Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have demonstrated that natural language can be used to generateand edit 3D shapes. However, these methods generate shapes with limitedfidelity and diversity. We introduce CLIP-Sculptor, a method to address theseconstraints by producing high-fidelity and diverse 3D shapes without the needfor (text, shape) pairs during training. CLIP-Sculptor achieves this in amulti-resolution approach that first generates in a low-dimensional latentspace and then upscales to a higher resolution for improved shape fidelity. Forimproved shape diversity, we use a discrete latent space which is modeled usinga transformer conditioned on CLIP's image-text embedding space. We also presenta novel variant of classifier-free guidance, which improves theaccuracy-diversity trade-off. Finally, we perform extensive experimentsdemonstrating that CLIP-Sculptor outperforms state-of-the-art baselines. Thecode is available at ", "output": "CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene flow estimation is a long-standing problem in computer vision, wherethe goal is to find the 3D motion of a scene from its consecutive observations.Recently, there have been efforts to compute the scene flow from 3D pointclouds. A common approach is to train a regression model that consumes sourceand target point clouds and outputs the per-point translation vector. Analternative is to learn point matches between the point clouds concurrentlywith regressing a refinement of the initial correspondence flow. In both cases,the learning task is very challenging since the flow regression is done in thefree 3D space, and a typical solution is to resort to a large annotatedsynthetic dataset. We introduce SCOOP, a new method for scene flow estimationthat can be learned on a small amount of data without employing ground-truthflow supervision. In contrast to previous work, we train a pure correspondencemodel focused on learning point feature representation and initialize the flowas the difference between a source point and its softly corresponding targetpoint. Then, in the run-time phase, we directly optimize a flow refinementcomponent with a self-supervised objective, which leads to a coherent andaccurate flow field between the point clouds. Experiments on widespreaddatasets demonstrate the performance gains achieved by our method compared toexisting leading techniques while using a fraction of the training data. Ourcode is publicly available at ", "output": "SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Novel view synthesis and 3D modeling using implicit neural fieldrepresentation are shown to be very effective for calibrated multi-viewcameras. Such representations are known to benefit from additional geometricand semantic supervision. Most existing methods that exploit additionalsupervision require dense pixel-wise labels or localized scene priors. Thesemethods cannot benefit from high-level vague scene priors provided in terms ofscenes' descriptions. In this work, we aim to leverage the geometric prior ofManhattan scenes to improve the implicit neural radiance field representations.More precisely, we assume that only the knowledge of the indoor scene (underinvestigation) being Manhattan is known -- with no additional informationwhatsoever -- with an unknown Manhattan coordinate frame. Such high-level prioris used to self-supervise the surface normals derived explicitly in theimplicit neural fields. Our modeling allows us to group the derived normals andexploit their orthogonality constraints for self-supervision. Our exhaustiveexperiments on datasets of diverse indoor scenes demonstrate the significantbenefit of the proposed method over the established baselines.", "output": "Neural Radiance Fields for Manhattan Scenes with Unknown Manhattan Frame."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel 3D morphable model for complete human heads based onhybrid neural fields. At the core of our model lies a neural parametricrepresentation that disentangles identity and expressions in disjoint latentspaces. To this end, we capture a person's identity in a canonical space as asigned distance field (SDF), and model facial expressions with a neuraldeformation field. In addition, our representation achieves high-fidelity localdetail by introducing an ensemble of local fields centered around facial anchorpoints. To facilitate generalization, we train our model on a newly-captureddataset of over 5200 head scans from 255 different identities using a customhigh-end 3D scanning setup. Our dataset significantly exceeds comparableexisting datasets, both with respect to quality and completeness of geometry,averaging around 3.5M mesh faces per scan. Finally, we demonstrate that ourapproach outperforms state-of-the-art methods in terms of fitting error andreconstruction quality.", "output": "Learning Neural Parametric Head Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training a Neural Radiance Field (NeRF) without pre-computed camera poses ischallenging. Recent advances in this direction demonstrate the possibility ofjointly optimising a NeRF and camera poses in forward-facing scenes. However,these methods still face difficulties during dramatic camera movement. Wetackle this challenging problem by incorporating undistorted monocular depthpriors. These priors are generated by correcting scale and shift parametersduring training, with which we are then able to constrain the relative posesbetween consecutive frames. This constraint is achieved using our proposednovel loss functions. Experiments on real-world indoor and outdoor scenes showthat our method can handle challenging camera trajectories and outperformsexisting methods in terms of novel view rendering quality and pose estimationaccuracy. Our project page is ", "output": "NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We learn a visual representation that captures information about the camerathat recorded a given photo. To do this, we train a multimodal embeddingbetween image patches and the EXIF metadata that cameras automatically insertinto image files. Our model represents this metadata by simply converting it totext and then processing it with a transformer. The features that we learnsignificantly outperform other self-supervised and supervised features ondownstream image forensics and calibration tasks. In particular, wesuccessfully localize spliced image regions \"zero shot\" by clustering thevisual embeddings for all of the patches within an image.", "output": "EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), afully-attentional model for general Satellite Image Time Series (SITS)processing based on the Vision Transformer (ViT). TSViT splits a SITS recordinto non-overlapping patches in space and time which are tokenized andsubsequently processed by a factorized temporo-spatial encoder. We argue, thatin contrast to natural images, a temporal-then-spatial factorization is moreintuitive for SITS processing and present experimental evidence for this claim.Additionally, we enhance the model's discriminative power by introducing twonovel mechanisms for acquisition-time-specific temporal positional encodingsand multiple learnable class tokens. The effect of all novel design choices isevaluated through an extensive ablation study. Our proposed architectureachieves state-of-the-art performance, surpassing previous approaches by asignificant margin in three publicly available SITS semantic segmentation andclassification datasets. All model, training and evaluation codes are madepublicly available to facilitate further research.", "output": "ViTs for SITS: Vision Transformers for Satellite Image Time Series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a generative document-specific approach to character analysis andrecognition in text lines. Our main idea is to build on unsupervisedmulti-object segmentation methods and in particular those that reconstructimages based on a limited amount of visual elements, called sprites. Taking asinput a set of text lines with similar font or handwriting, our approach canlearn a large number of different characters and leverage line-levelannotations when available. Our contribution is twofold. First, we provide thefirst adaptation and evaluation of a deep unsupervised multi-objectsegmentation approach for text line analysis. Since these methods have mainlybeen evaluated on synthetic data in a completely unsupervised setting,demonstrating that they can be adapted and quantitatively evaluated on realimages of text and that they can be trained using weak supervision aresignificant progresses. Second, we show the potential of our method for newapplications, more specifically in the field of paleography, which studies thehistory and variations of handwriting, and for cipher analysis. We demonstrateour approach on three very different datasets: a printed volume of theGoogle1000 dataset, the Copiale cipher and historical handwritten charters fromthe 12th and early 13th century.", "output": "The Learnable Typewriter: A Generative Approach to Text Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Effective BEV object detection on infrastructure can greatly improve trafficscenes understanding and vehicle-toinfrastructure (V2I) cooperative perception.However, cameras installed on infrastructure have various postures, andprevious BEV detection methods rely on accurate calibration, which is difficultfor practical applications due to inevitable natural factors (e.g., wind andsnow). In this paper, we propose a Calibration-free BEV Representation (CBR)network, which achieves 3D detection based on BEV representation withoutcalibration parameters and additional depth supervision. Specifically, weutilize two multi-layer perceptrons for decoupling the features fromperspective view to front view and birdeye view under boxes-induced foregroundsupervision. Then, a cross-view feature fusion module matches features fromorthogonal views according to similarity and conducts BEV feature enhancementwith front view features. Experimental results on DAIR-V2X demonstrate that CBRachieves acceptable performance without any camera parameters and is naturallynot affected by calibration noises. We hope CBR can serve as a baseline forfuture research addressing practical challenges of infrastructure perception.", "output": "Calibration-free BEV Representation for Infrastructure Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present our advanced solutions to the two sub-challenges ofAffective Behavior Analysis in the wild (ABAW) 2023: the Emotional ReactionIntensity (ERI) Estimation Challenge and Expression (Expr) ClassificationChallenge. ABAW 2023 aims to tackle the challenge of affective behavioranalysis in natural contexts, with the ultimate goal of creating intelligentmachines and robots that possess the ability to comprehend human emotions,feelings, and behaviors. For the Expression Classification Challenge, wepropose a streamlined approach that handles the challenges of classificationeffectively. However, our main contribution lies in our use of diverse modelsand tools to extract multimodal features such as audio and video cues from theHume-Reaction dataset. By studying, analyzing, and combining these features, wesignificantly enhance the model's accuracy for sentiment prediction in amultimodal context. Furthermore, our method achieves outstanding results on theEmotional Reaction Intensity (ERI) Estimation Challenge, surpassing thebaseline method by an impressive 84% increase, as measured by the PearsonCoefficient, on the validation dataset.", "output": "Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos with Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While it is well known that population differences from genetics, sex, race,and environmental factors contribute to disease, AI studies in medicine havelargely focused on locoregional patient cohorts with less diverse data sources.Such limitation stems from barriers to large-scale data share and ethicalconcerns over data privacy. Federated learning (FL) is one potential pathwayfor AI development that enables learning across hospitals without data share.In this study, we show the results of various FL strategies on one of thelargest and most diverse COVID-19 chest CT datasets: 21 participating hospitalsacross five continents that comprise &gt;10,000 patients with &gt;1 million images.We also propose an FL strategy that leverages synthetically generated data toovercome class and size imbalances. We also describe the sources of dataheterogeneity in the context of FL, and show how even among the correctlylabeled populations, disparities can arise due to these biases.", "output": "AI Models Close to your Chest: Robust Federated Learning Strategies for Multi-site CT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical images usually suffer from image degradation in clinical practice,leading to decreased performance of deep learning-based models. To resolve thisproblem, most previous works have focused on filtering out degradation-causinglow-quality images while ignoring their potential value for models. Througheffectively learning and leveraging the knowledge of degradations, models canbetter resist their adverse effects and avoid misdiagnosis. In this paper, weraise the problem of image quality-aware diagnosis, which aims to takeadvantage of low-quality images and image quality labels to achieve a moreaccurate and robust diagnosis. However, the diversity of degradations andsuperficially unrelated targets between image quality assessment and diseasediagnosis makes it still quite challenging to effectively leverage qualitylabels to assist diagnosis. Thus, to tackle these issues, we propose a novelmeta-knowledge co-embedding network, consisting of two subnets: Task Net andMeta Learner. Task Net constructs an explicit quality information utilizationmechanism to enhance diagnosis via knowledge co-embedding features, while MetaLearner ensures the effectiveness and constrains the semantics of thesefeatures via meta-learning and joint-encoding masking. Superior performance onfive datasets with four widely-used medical imaging modalities demonstrates theeffectiveness and generalizability of our method.", "output": "Image Quality-aware Diagnosis via Meta-knowledge Co-embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the realm of machine learning, the study of anomaly detection andlocalization within image data has gained substantial traction, particularlyfor practical applications such as industrial defect detection. While themajority of existing methods predominantly use Convolutional Neural Networks(CNN) as their primary network architecture, we introduce a novel approachbased on the Transformer backbone network. Our method employs a two-stageincremental learning strategy. During the first stage, we train a MaskedAutoencoder (MAE) model solely on normal images. In the subsequent stage, weapply pixel-level data augmentation techniques to generate corrupted normalimages and their corresponding pixel labels. This process allows the model tolearn how to repair corrupted regions and classify the status of each pixel.Ultimately, the model generates a pixel reconstruction error matrix and a pixelanomaly probability matrix. These matrices are then combined to produce ananomaly scoring matrix that effectively detects abnormal regions. Whenbenchmarked against several state-of-the-art CNN-based methods, our approachexhibits superior performance on the MVTec AD dataset, achieving an impressive97.6% AUC.", "output": "ISSTAD: Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent text-to-image generation models like DreamBooth have made remarkableprogress in generating highly customized images of a target subject, byfine-tuning an ``expert model'' for a given subject from a few examples.However, this process is expensive, since a new expert model must be learnedfor each subject. In this paper, we present SuTI, a Subject-drivenText-to-Image generator that replaces subject-specific fine tuning withemph{in-context} learning. Given a few demonstrations of a new subject, SuTIcan instantly generate novel renditions of the subject in different scenes,without any subject-specific optimization. SuTI is powered by {emapprenticeship learning}, where a single apprentice model is learned from datagenerated by massive amount of subject-specific expert models. Specifically, wemine millions of image clusters from the Internet, each centered around aspecific visual subject. We adopt these clusters to train massive amount ofexpert models specialized on different subjects. The apprentice model SuTI thenlearns to mimic the behavior of these experts through the proposedapprenticeship learning algorithm. SuTI can generate high-quality andcustomized subject-specific images 20x faster than optimization-based SoTAmethods. On the challenging DreamBench and DreamBench-v2, our human evaluationshows that SuTI can significantly outperform existing approaches likeInstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen whileperforming on par with DreamBooth.", "output": "Subject-driven Text-to-Image Generation via Apprenticeship Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) aims to learn a single global model that enables thecentral server to help the model training in local clients without accessingtheir local data. The key challenge of FL is the heterogeneity of local data indifferent clients, such as heterogeneous label distribution and feature shift,which could lead to significant performance degradation of the learned models.Although many studies have been proposed to address the heterogeneous labeldistribution problem, few studies attempt to explore the feature shift issue.To address this issue, we propose a simple yet effective algorithm, namelytextbf{p}ersonalized textbf{Fed}erated learning with textbf{L}ocaltextbf{A}ttention (pFedLA), by incorporating the attention mechanism intopersonalized models of clients while keeping the attention blocksclient-specific. Specifically, two modules are proposed in pFedLA, i.e., thepersonalized single attention module and the personalized hybrid attentionmodule. In addition, the proposed pFedLA method is quite flexible and generalas it can be incorporated into any FL method to improve their performancewithout introducing additional communication costs. Extensive experimentsdemonstrate that the proposed pFedLA method can boost the performance ofstate-of-the-art FL methods on different tasks such as image classification andobject detection tasks.", "output": "Personalized Federated Learning with Local Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Let us rethink the real-world scenarios that require human motion predictiontechniques, such as human-robot collaboration. Current works simplify the taskof predicting human motions into a one-off process of forecasting a shortfuture sequence (usually no longer than 1 second) based on a historicalobserved one. However, such simplification may fail to meet practical needs dueto the neglect of the fact that motion prediction in real applications is notan isolated ``observe then predict'' unit, but a consecutive process composedof many rounds of such unit, semi-overlapped along the entire sequence. As timegoes on, the predicted part of previous round has its corresponding groundtruth observable in the new round, but their deviation in-between is neitherexploited nor able to be captured by existing isolated learning fashion. Inthis paper, we propose DeFeeNet, a simple yet effective network that can beadded on existing one-off prediction models to realize deviation perception andfeedback when applied to consecutive motion prediction task. At each predictionround, the deviation generated by previous unit is first encoded by ourDeFeeNet, and then incorporated into the existing predictor to enable adeviation-aware prediction manner, which, for the first time, allows forinformation transmit across adjacent prediction units. We design two versionsof DeFeeNet as MLP-based and GRU-based, respectively. On Human3.6M and morecomplicated BABEL, experimental results indicate that our proposed networkimproves consecutive human motion prediction performance regardless of thebasic model.", "output": "DeFeeNet: Consecutive 3D Human Motion Prediction with Deviation Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual simultaneous localization and mapping (SLAM) systems face challengesin detecting loop closure under the circumstance of large viewpoint changes. Inthis paper, we present an object-based loop closure detection method based onthe spatial layout and semanic consistency of the 3D scene graph. Firstly, wepropose an object-level data association approach based on the semanticinformation from semantic labels, intersection over union (IoU), object color,and object embedding. Subsequently, multi-view bundle adjustment with theassociated objects is utilized to jointly optimize the poses of objects andcameras. We represent the refined objects as a 3D spatial graph with semanticsand topology. Then, we propose a graph matching approach to selectcorrespondence objects based on the structure layout and semantic propertysimilarity of vertices' neighbors. Finally, we jointly optimize cameratrajectories and object poses in an object-level pose graph optimization, whichresults in a globally consistent map. Experimental results demonstrate that ourproposed data association approach can construct more accurate 3D semanticmaps, and our loop closure method is more robust than point-based andobject-based methods in circumstances with large viewpoint changes.", "output": "Loop Closure Detection Based on Object-level Spatial Layout and Semantic Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present DreamPose, a diffusion-based method for generating animatedfashion videos from still images. Given an image and a sequence of human bodyposes, our method synthesizes a video containing both human and fabric motion.To achieve this, we transform a pretrained text-to-image model (StableDiffusion) into a pose-and-image guided video synthesis model, using a novelfinetuning strategy, a set of architectural changes to support the addedconditioning signals, and techniques to encourage temporal consistency. Wefine-tune on a collection of fashion videos from the UBC Fashion dataset. Weevaluate our method on a variety of clothing styles and poses, and demonstratethat our method produces state-of-the-art results on fashion video animation.Video results are available on our project page.", "output": "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exemplar-based image colorization aims to colorize a target grayscale imagebased on a color reference image, and the key is to establish accuratepixel-level semantic correspondence between these two images. Previous methodssearch for correspondence across the entire reference image, and this type ofglobal matching is easy to get mismatch. We summarize the difficulties in twoaspects: (1) When the reference image only contains a part of objects relatedto target image, improper correspondence will be established in unrelatedregions. (2) It is prone to get mismatch in regions where the shape or textureof the object is easily confused. To overcome these issues, we propose SPColor,a semantic prior guided exemplar-based image colorization framework. Differentfrom previous methods, SPColor first coarsely classifies pixels of thereference and target images to several pseudo-classes under the guidance ofsemantic prior, then the correspondences are only established locally betweenthe pixels in the same class via the newly designed semantic prior guidedcorrespondence network. In this way, improper correspondence between differentsemantic classes is explicitly excluded, and the mismatch is obviouslyalleviated. Besides, to better reserve the color from reference, a similaritymasked perceptual loss is designed. Noting that the carefully designed SPColorutilizes the semantic prior provided by an unsupervised segmentation model,which is free for additional manual semantic annotations. Experimentsdemonstrate that our model outperforms recent state-of-the-art methods bothquantitatively and qualitatively on public dataset.", "output": "SPColor: Semantic Prior Guided Exemplar-based Image Colorization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent applications of deep convolutional neural networks in medical imagingraise concerns about their interpretability. While most explainable deeplearning applications use post hoc methods (such as GradCAM) to generatefeature attribution maps, there is a new type of case-based reasoning models,namely ProtoPNet and its variants, which identify prototypes during trainingand compare input image patches with those prototypes. We propose the firstmedical prototype network (MProtoNet) to extend ProtoPNet to brain tumorclassification with 3D multi-parametric magnetic resonance imaging (mpMRI)data. To address different requirements between 2D natural images and 3D mpMRIsespecially in terms of localizing attention regions, a new attention modulewith soft masking and online-CAM loss is introduced. Soft masking helps sharpenattention maps, while online-CAM loss directly utilizes image-level labels whentraining the attention module. MProtoNet achieves statistically significantimprovements in interpretability metrics of both correctness and localizationcoherence (with a best activation precision of $0.713pm0.058$) withouthuman-annotated labels during training, when compared with GradCAM and severalProtoPNet variants. The source code is available at", "output": "MProtoNet: A Case-Based Interpretable Model for Brain Tumor Classification with 3D Multi-parametric Magnetic Resonance Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The advances in automatic sign language translation (SLT) to spoken languageshave been mostly benchmarked with datasets of limited size and restricteddomains. Our work advances the state of the art by providing the first baselineresults on How2Sign, a large and broad dataset.We train a Transformer over I3D video features, using the reduced BLEU as areference metric for validation, instead of the widely used BLEU score. Wereport a result of 8.03 on the BLEU score, and publish the first open-sourceimplementation of its kind to promote further advances.", "output": "Sign Language Translation from Instructional Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Flatlandia, a novel problem for visual localization of an imagefrom object detections composed of two specific tasks: i) Coarse MapLocalization: localizing a single image observing a set of objects in respectto a 2D map of object landmarks; ii) Fine-grained 3DoF Localization: estimatinglatitude, longitude, and orientation of the image within a 2D map. Solutionsfor these new tasks exploit the wide availability of open urban maps annotatedwith GPS locations of common objects (eg via surveying or crowd-sourced). Suchmaps are also more storage-friendly than standard large-scale 3D models oftenused in visual localization while additionally being privacy-preserving. Asexisting datasets are unsuited for the proposed problem, we provide theFlatlandia dataset, designed for 3DoF visual localization in multiple urbansettings and based on crowd-sourced data from five European cities. We use theFlatlandia dataset to validate the complexity of the proposed tasks.", "output": "You are here! Finding position and orientation on a 2D map from a single image: The Flatlandia localization problem and dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Document-based Visual Question Answering examines the document understandingof document images in conditions of natural language questions. We proposed anew document-based VQA dataset, PDF-VQA, to comprehensively examine thedocument understanding from various aspects, including document elementrecognition, document layout structural understanding as well as contextualunderstanding and key information extraction. Our PDF-VQA dataset extends thecurrent scale of document understanding that limits on the single document pageto the new scale that asks questions over the full document of multiple pages.We also propose a new graph-based VQA model that explicitly integrates thespatial and hierarchically structural relationships between different documentelements to boost the document structural understanding. The performances arecompared with several baselines over different question types andtasksfootnote{The full dataset will be released after paper acceptance.", "output": "PDF-VQA: A New Dataset for Real-World VQA on PDF Documents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatial control is a core capability in controllable image generation.Advancements in layout-guided image generation have shown promising results onin-distribution (ID) datasets with similar spatial configurations. However, itis unclear how these models perform when facing out-of-distribution (OOD)samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench,a diagnostic benchmark for layout-guided image generation that examines fourcategories of spatial control skills: number, position, size, and shape. Webenchmark two recent representative layout-guided image generation methods andobserve that the good ID layout control may not generalize well to arbitrarylayouts in the wild (e.g., objects at the boundary). Next, we proposeIterInpaint, a new baseline that generates foreground and background regions ina step-by-step manner via inpainting, demonstrating stronger generalizabilitythan existing models on OOD layouts in LayoutBench. We perform quantitative andqualitative evaluation and fine-grained analysis on the four LayoutBench skillsto pinpoint the weaknesses of existing models. Lastly, we show comprehensiveablation studies on IterInpaint, including training task ratio, crop&amp;paste vs.repaint, and generation order. Project website: ", "output": "Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic neural networks can greatly reduce computation redundancy withoutcompromising accuracy by adapting their structures based on the input. In thispaper, we explore the robustness of dynamic neural networks againstenergy-oriented attacks targeted at reducing their efficiency. Specifically, weattack dynamic models with our novel algorithm GradMDM. GradMDM is a techniquethat adjusts the direction and the magnitude of the gradients to effectivelyfind a small perturbation for each input, that will activate more computationalunits of dynamic models during inference. We evaluate GradMDM on multipledatasets and dynamic models, where it outperforms previous energy-orientedattack techniques, significantly increasing computation complexity whilereducing the perceptibility of the perturbations.", "output": "GradMDM: Adversarial Attack on Dynamic Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Meta-learning is a framework for learning learning algorithms throughrepeated interactions with an environment as opposed to designing them by hand.In recent years, this framework has established itself as a promising tool forbuilding models of human cognition. Yet, a coherent research program aroundmeta-learned models of cognition is still missing. The purpose of this articleis to synthesize previous work in this field and establish such a researchprogram. We rely on three key pillars to accomplish this goal. We first pointout that meta-learning can be used to construct Bayes-optimal learningalgorithms. This result not only implies that any behavioral phenomenon thatcan be explained by a Bayesian model can also be explained by a meta-learnedmodel but also allows us to draw strong connections to the rational analysis ofcognition. We then discuss several advantages of the meta-learning frameworkover traditional Bayesian methods. In particular, we argue that meta-learningcan be applied to situations where Bayesian inference is impossible and that itenables us to make rational models of cognition more realistic, either byincorporating limited computational resources or neuroscientific knowledge.Finally, we reexamine prior studies from psychology and neuroscience that haveapplied meta-learning and put them into the context of these new insights. Insummary, our work highlights that meta-learning considerably extends the scopeof rational analysis and thereby of cognitive theories more generally.", "output": "Meta-Learned Models of Cognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Coronaviruses are membrane-enveloped, non-segmented positive-strand RNAviruses belonging to the Coronaviridae family. Various animal species, mainlymammalian and avian, are severely infected by various coronaviruses, causingserious concerns like the recent pandemic (COVID-19). Therefore, building adeeper understanding of these viruses is essential to devise prevention andmitigation mechanisms. In the Coronavirus genome, an essential structuralregion is the spike region, and it's responsible for attaching the virus to thehost cell membrane. Therefore, the usage of only the spike protein, instead ofthe full genome, provides most of the essential information for performinganalyses such as host classification. In this paper, we propose a novel methodfor predicting the host specificity of coronaviruses by analyzing spike proteinsequences from different viral subgenera and species. Our method involves usingthe Poisson correction distance to generate a distance matrix, followed byusing a radial basis function (RBF) kernel and kernel principal componentanalysis (PCA) to generate a low-dimensional embedding. Finally, we applyclassification algorithms to the low-dimensional embedding to generate theresulting predictions of the host specificity of coronaviruses. We providetheoretical proofs for the non-negativity, symmetry, and triangle inequalityproperties of the Poisson correction distance metric, which are importantproperties in a machine-learning setting. By encoding the spike proteinstructure and sequences using this comprehensive approach, we aim to uncoverhidden patterns in the biological sequences to make accurate predictions abouthost specificity. Finally, our classification results illustrate that ourmethod can achieve higher predictive accuracy and improve performance overexisting baselines.", "output": "PCD2Vec: A Poisson Correction Distance-Based Approach for Viral Host Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper considers the problem of testing the maximum in-degree of theBayes net underlying an unknown probability distribution $P$ over ${0,1}^n$,given sample access to $P$. We show that the sample complexity of the problemis $tilde{Theta}(2^{n/2}/varepsilon^2)$. Our algorithm relies on atesting-by-learning framework, previously used to obtain sample-optimaltesters; in order to apply this framework, we develop new algorithms for``near-proper'' learning of Bayes nets, and high-probability learning under$chi^2$ divergence, which are of independent interest.", "output": "Near-Optimal Degree Testing for Bayes Nets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans excel at continually acquiring, consolidating, and retaininginformation from an ever-changing environment, whereas artificial neuralnetworks (ANNs) exhibit catastrophic forgetting. There are considerabledifferences in the complexity of synapses, the processing of information, andthe learning mechanisms in biological neural networks and their artificialcounterparts, which may explain the mismatch in performance. We consider abiologically plausible framework that constitutes separate populations ofexclusively excitatory and inhibitory neurons that adhere to Dale's principle,and the excitatory pyramidal neurons are augmented with dendritic-likestructures for context-dependent processing of stimuli. We then conduct acomprehensive study on the role and interactions of different mechanismsinspired by the brain, including sparse non-overlapping representations,Hebbian learning, synaptic consolidation, and replay of past activations thataccompanied the learning event. Our study suggests that the employing ofmultiple complementary mechanisms in a biologically plausible architecture,similar to the brain, may be effective in enabling continual learning in ANNs.", "output": "A Study of Biologically Plausible Neural Network: The Role and Interactions of Brain-Inspired Mechanisms in Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop an end-to-end workflow for the training and implementation ofco-designed neural networks (NNs) for efficient field-programmable gate array(FPGA) and application-specific integrated circuit (ASIC) hardware. Ourapproach leverages Hessian-aware quantization (HAWQ) of NNs, the Quantized OpenNeural Network Exchange (QONNX) intermediate representation, and the hls4mltool flow for transpiling NNs into FPGA and ASIC firmware. This makes efficientNN implementations in hardware accessible to nonexperts, in a singleopen-sourced workflow that can be deployed for real-time machine learningapplications in a wide range of scientific and industrial settings. Wedemonstrate the workflow in a particle physics application involving triggerdecisions that must operate at the 40 MHz collision rate of the CERN LargeHadron Collider (LHC). Given the high collision rate, all data processing mustbe implemented on custom ASIC and FPGA hardware within a strict area andlatency. Based on these constraints, we implement an optimized mixed-precisionNN classifier for high-momentum particle jets in simulated LHC proton-protoncollisions.", "output": "End-to-end codesign of Hessian-aware quantized neural networks for FPGAs and ASICs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large decoder-only language models (LMs) can be largely improved in terms ofperplexity by retrieval (e.g., RETRO), but its impact on text generationquality and downstream task accuracy is unclear. Thus, it is still an openquestion: shall we pretrain large autoregressive LMs with retrieval? To answerit, we perform a comprehensive study on a scalable pre-trainedretrieval-augmented LM (i.e., RETRO) compared with standard GPT andretrieval-augmented GPT incorporated at fine-tuning or inference stages. Wefirst provide the recipe to reproduce RETRO up to 9.5B parameters whileretrieving a text corpus with 330B tokens. Based on that, we have the followingnovel findings: i) RETRO outperforms GPT on text generation with much lessdegeneration (i.e., repetition), moderately higher factual accuracy, andslightly lower toxicity with a nontoxic retrieval database. ii) On the LMEvaluation Harness benchmark, RETRO largely outperforms GPT onknowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore,we introduce a simple variant of the model, RETRO++, which largely improvesopen-domain QA results of original RETRO (e.g., EM score +8.6 on NaturalQuestion) and significantly outperforms retrieval-augmented GPT acrossdifferent model sizes. Our findings highlight the promising direction ofpretraining autoregressive LMs with retrieval as future foundation models. Werelease our implementation at: ", "output": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative foundation models are susceptible to implicit biases that canarise from extensive unsupervised training data. Such biases can producesuboptimal samples, skewed outcomes, and unfairness, with potentiallysignificant repercussions. Consequently, aligning these models with humanethics and preferences is an essential step toward ensuring their responsibleand effective deployment in real-world applications. Prior research hasprimarily employed Reinforcement Learning from Human Feedback (RLHF) as a meansof addressing this problem, wherein generative models are fine-tuned using RLalgorithms guided by a human-feedback-informed reward model. However, theinefficiencies and instabilities associated with RL algorithms frequentlypresent substantial obstacles to the successful alignment of generative models,necessitating the development of a more robust and streamlined approach. Tothis end, we introduce a new framework, Reward rAnked FineTuning (RAFT),designed to align generative models more effectively. Utilizing a reward modeland a sufficient number of samples, our approach selects the high-qualitysamples, discarding those that exhibit undesired behavior, and subsequentlyassembles a streaming dataset. This dataset serves as the basis for aligningthe generative model and can be employed under both offline and onlinesettings. Notably, the sample generation process within RAFT is gradient-free,rendering it compatible with black-box generators. Through extensiveexperiments, we demonstrate that our proposed algorithm exhibits strongperformance in the context of both large language models and diffusion models.", "output": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning algorithms, both in their classical and quantum versions,heavily rely on optimization algorithms based on gradients, such as gradientdescent and alike. The overall performance is dependent on the appearance oflocal minima and barren plateaus, which slow-down calculations and lead tonon-optimal solutions. In practice, this results in dramatic computational andenergy costs for AI applications. In this paper we introduce a generic strategyto accelerate and improve the overall performance of such methods, allowing toalleviate the effect of barren plateaus and local minima. Our method is basedon coordinate transformations, somehow similar to variational rotations, addingextra directions in parameter space that depend on the cost function itself,and which allow to explore the configuration landscape more efficiently. Thevalidity of our method is benchmarked by boosting a number of quantum machinelearning algorithms, getting a very significant improvement in theirperformance.", "output": "Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online recognition of gestures is critical for intuitive human-robotinteraction (HRI) and further push collaborative robotics into the market,making robots accessible to more people. The problem is that it is difficult toachieve accurate gesture recognition in real unstructured environments, oftenusing distorted and incomplete multisensory data. This paper introduces an HRIframework to classify large vocabularies of interwoven static gestures (SGs)and dynamic gestures (DGs) captured with wearable sensors. DG features areobtained by applying data dimensionality reduction to raw data from sensors(resampling with cubic interpolation and principal component analysis).Experimental tests were conducted using the UC2017 hand gesture dataset withsamples from eight different subjects. The classification models show anaccuracy of 95.6% for a library of 24 SGs with a random forest and 99.3% for 10DGs using artificial neural networks. These results compare equally orfavorably with different commonly used classifiers. Long short-term memory deepnetworks achieved similar performance in online frame-by-frame classificationusing raw incomplete data, performing better in terms of accuracy than staticmodels with specially crafted features, but worse in training and inferencetime. The recognized gestures are used to teleoperate a robot in acollaborative process that consists in preparing a breakfast meal.", "output": "Online Recognition of Incomplete Gesture Data to Interface Collaborative Robots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of learning conditional distributions of the form $p(G |hat G)$, where $G$ and $hat G$ are two 3D graphs, using continuousnormalizing flows. We derive a semi-equivariance condition on the flow whichensures that conditional invariance to rigid motions holds. We demonstrate theeffectiveness of the technique in the molecular setting of receptor-awareligand generation.", "output": "Semi-Equivariant Conditional Normalizing Flows."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a distributionally robust approach to regret optimalcontrol of discrete-time linear dynamical systems with quadratic costs subjectto stochastic additive disturbance on the state process. The underlyingprobability distribution of the disturbance process is unknown, but assumed tolie in a given ball of distributions defined in terms of the type-2 Wassersteindistance. In this framework, strictly causal linear disturbance feedbackcontrollers are designed to minimize the worst-case expected regret. The regretincurred by a controller is defined as the difference between the cost itincurs in response to a realization of the disturbance process and the costincurred by the optimal noncausal controller which has perfect knowledge of thedisturbance process realization at the outset. Building on a well-establishedduality theory for optimal transport problems, we show how to equivalentlyreformulate this minimax regret optimal control problem as a tractablesemidefinite program. The equivalent dual reformulation also allows us tocharacterize a worst-case distribution achieving the worst-case expected regretin relation to the distribution at the center of the Wasserstein ball.", "output": "A Distributionally Robust Approach to Regret Optimal Control using the Wasserstein Distance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the first $varepsilon$-differentially private, computationallyefficient algorithm that estimates the means of product distributions over${0,1}^d$ accurately in total-variation distance, whilst attaining theoptimal sample complexity to within polylogarithmic factors. The prior work hadeither solved this problem efficiently and optimally under weaker notions ofprivacy, or had solved it optimally while having exponential running times.", "output": "A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The decision tree ensembles use a single data feature at each node forsplitting the data. However, splitting in this manner may fail to capture thegeometric properties of the data. Thus, oblique decision trees generate theoblique hyperplane for splitting the data at each non-leaf node. Obliquedecision trees capture the geometric properties of the data and hence, showbetter generalization. The performance of the oblique decision trees depends onthe way oblique hyperplanes are generate and the data used for the generationof those hyperplanes. Recently, multiple classifiers have been used in aheterogeneous random forest (RaF) classifier, however, it fails to generate thetrees of proper depth. Moreover, double RaF studies highlighted that largertrees can be generated via bootstrapping the data at each non-leaf node andsplitting the original data instead of the bootstrapped data recently. Thestudy of heterogeneous RaF lacks the generation of larger trees while as thedouble RaF based model fails to take over the geometric characteristics of thedata. To address these shortcomings, we propose heterogeneous oblique doubleRaF. The proposed model employs several linear classifiers at each non-leafnode on the bootstrapped data and splits the original data based on the optimallinear classifier. The optimal hyperplane corresponds to the models based onthe optimized impurity criterion. The experimental analysis indicates that theperformance of the introduced heterogeneous double random forest iscomparatively better than the baseline models. To demonstrate the effectivenessof the proposed heterogeneous double random forest, we used it for thediagnosis of Schizophrenia disease. The proposed model predicted the diseasemore accurately compared to the baseline models.", "output": "Heterogeneous Oblique Double Random Forest."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Edge computing solutions that enable the extraction of high level informationfrom a variety of sensors is in increasingly high demand. This is due to theincreasing number of smart devices that require sensory processing for theirapplication on the edge. To tackle this problem, we present a smart visionsensor System on Chip (Soc), featuring an event-based camera and a low powerasynchronous spiking Convolutional Neuronal Network (sCNN) computingarchitecture embedded on a single chip. By combining both sensor and processingon a single die, we can lower unit production costs significantly. Moreover,the simple end-to-end nature of the SoC facilitates small stand-aloneapplications as well as functioning as an edge node in a larger systems. Theevent-driven nature of the vision sensor delivers high-speed signals in asparse data stream. This is reflected in the processing pipeline, focuses onoptimising highly sparse computation and minimising latency for 9 sCNN layersto $3.36mu s$. Overall, this results in an extremely low-latency visualprocessing pipeline deployed on a small form factor with a low energy budgetand sensor cost. We present the asynchronous architecture, the individualblocks, the sCNN processing principle and benchmark against other sCNN capableprocessors.", "output": "Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a novel Token-and-Duration Transducer (TDT)architecture for sequence-to-sequence tasks. TDT extends conventionalRNN-Transducer architectures by jointly predicting both a token and itsduration, i.e. the number of input frames covered by the emitted token. This isachieved by using a joint network with two outputs which are independentlynormalized to generate distributions over tokens and durations. Duringinference, TDT models can skip input frames guided by the predicted durationoutput, which makes them significantly faster than conventional Transducerswhich process the encoder output frame by frame. TDT models achieve both betteraccuracy and significantly faster inference than conventional Transducers ondifferent sequence transduction tasks. TDT models for Speech Recognitionachieve better accuracy and up to 2.82X faster inference than RNN-Transducers.TDT models for Speech Translation achieve an absolute gain of over 1 BLEU onthe MUST-C test compared with conventional Transducers, and its inference is2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDTmodels improve the intent accuracy up to over 1% (absolute) over conventionalTransducers, while running up to 1.28X faster.", "output": "Efficient Sequence Transduction by Jointly Predicting Tokens and Durations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel approach for black-box VI that bypasses the difficultiesof stochastic gradient ascent, including the task of selecting step-sizes. Ourapproach involves using a sequence of sample average approximation (SAA)problems. SAA approximates the solution of stochastic optimization problems bytransforming them into deterministic ones. We use quasi-Newton methods and linesearch to solve each deterministic optimization problem and present a heuristicpolicy to automate hyperparameter selection. Our experiments show that ourmethod simplifies the VI problem and achieves faster performance than existingmethods.", "output": "Sample Average Approximation for Black-Box VI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many collective systems exist in nature far from equilibrium, ranging fromcellular sheets up to flocks of birds. These systems reflect a form of activematter, whereby individual material components have internal energy. Underspecific parameter regimes, these active systems undergo phase transitionswhereby small fluctuations of single components can lead to global changes tothe rheology of the system. Simulations and methods from statistical physicsare typically used to understand and predict these phase transitions forreal-world observations. In this work, we demonstrate that simulation-basedinference can be used to robustly infer active matter parameters from systemobservations. Moreover, we demonstrate that a small number (from one to three)snapshots of the system can be used for parameter inference and that thisgraph-informed approach outperforms typical metrics such as the averagevelocity or mean square displacement of the system. Our work highlights thathigh-level system information is contained within the relational structure of acollective system and that this can be exploited to better couple models todata.", "output": "Graph-informed simulation-based inference for models of active matter."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study actively labeling streaming data, where an active learner is facedwith a stream of data points and must carefully choose which of these points tolabel via an expensive experiment. Such problems frequently arise inapplications such as healthcare and astronomy. We first study a setting whenthe data's inputs belong to one of $K$ discrete distributions and formalizethis problem via a loss that captures the labeling cost and the predictionerror. When the labeling cost is $B$, our algorithm, which chooses to label apoint if the uncertainty is larger than a time and cost dependent threshold,achieves a worst-case upper bound of $O(B^{frac{1}{3}} K^{frac{1}{3}}T^{frac{2}{3}})$ on the loss after $T$ rounds. We also provide a more nuancedupper bound which demonstrates that the algorithm can adapt to the arrivalpattern, and achieves better performance when the arrival pattern is morefavorable. We complement both upper bounds with matching lower bounds. We nextstudy this problem when the inputs belong to a continuous domain and the outputof the experiment is a smooth function with bounded RKHS norm. After $T$ roundsin $d$ dimensions, we show that the loss is bounded by $O(B^{frac{1}{d+3}}T^{frac{d+2}{d+3}})$ in an RKHS with a squared exponential kernel and by$O(B^{frac{1}{2d+3}} T^{frac{2d+2}{2d+3}})$ in an RKHS with a Mat'ernkernel. Our empirical evaluation demonstrates that our method outperforms otherbaselines in several synthetic experiments and two real experiments in medicineand astronomy.", "output": "Active Cost-aware Labeling of Streaming Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ragnarock is a virtual reality (VR) rhythm game in which you play a Vikingcaptain competing in a longship race. With two hammers, the task is to crushthe incoming runes in sync with epic Viking music. The runes are defined by abeat map which the player can manually create. The creation of beat maps takeshours. This work aims to automate the process of beat map creation, also knownas the task of learning to choreograph. The assignment is broken down intothree parts: determining the timing of the beats (action placement),determining where in space the runes connected with the chosen beats should beplaced (action selection) and web-application creation. For the first task ofaction placement, extraction of predominant local pulse (PLP) information frommusic recordings is used. This approach allows to learn where and how manybeats are supposed to be placed. For the second task of action selection,Recurrent Neural Networks (RNN) are used, specifically Gated recurrent unit(GRU) to learn sequences of beats, and their patterns to be able to recreatethose rules and receive completely new levels. Then the last task was to builda solution for non-technical players, the task was to combine the results ofthe first and the second parts into a web application for easy use. For thistask the frontend was built using JavaScript and React and the backend - pythonand FastAPI.", "output": "Level generation for rhythm VR games."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel, physically-constrained and differentiable approach forthe generation of D-dimensional qudit states via spontaneous parametricdown-conversion (SPDC) in quantum optics. We circumvent any limitations imposedby the inherently stochastic nature of the physical process and incorporate aset of stochastic dynamical equations governing its evolution under the SPDCHamiltonian. We demonstrate the effectiveness of our model through the designof structured nonlinear photonic crystals (NLPCs) and shaped pump beams; andshow, theoretically and experimentally, how to generate maximally entangledstates in the spatial degree of freedom. The learning of NLPC structures offersa promising new avenue for shaping and controlling arbitrary quantum states andenables all-optical coherent control of the generated states. We believe thatthis approach can readily be extended from bulky crystals to thin Metasurfacesand potentially applied to other quantum systems sharing a similar Hamiltonianstructures, such as superfluids and superconductors.", "output": "Designing Nonlinear Photonic Crystals for High-Dimensional Quantum State Engineering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Out-of-distribution (OOD) detection aims to identify test examples that donot belong to the training distribution and are thus unlikely to be predictedreliably. Despite a plethora of existing works, most of them focused only onthe scenario where OOD examples come from semantic shift (e.g., unseencategories), ignoring other possible causes (e.g., covariate shift). In thispaper, we present a novel, unifying framework to study OOD detection in abroader scope. Instead of detecting OOD examples from a particular cause, wepropose to detect examples that a deployed machine learning model (e.g., animage classifier) is unable to predict correctly. That is, whether a testexample should be detected and rejected or not is ``model-specific''. We showthat this framework unifies the detection of OOD examples caused by semanticshift and covariate shift, and closely addresses the concern of applying amachine learning model to uncontrolled environments. We provide an extensiveanalysis that involves a variety of models (e.g., different architectures andtraining strategies), sources of OOD examples, and OOD detection approaches,and reveal several insights into improving and understanding OOD detection inuncontrolled environments.", "output": "Unified Out-Of-Distribution Detection: A Model-Specific Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLM) are a new class of computation engines,\"programmed\" via prompt engineering. We are still learning how to best\"program\" these LLMs to help developers. We start with the intuition thatdevelopers tend to consciously and unconsciously have a collection of semanticsfacts in mind when working on coding tasks. Mostly these are shallow, simplefacts arising from a quick read. For a function, examples of facts mightinclude parameter and local variable names, return expressions, simple pre- andpost-conditions, and basic control and data flow, etc.One might assume that the powerful multi-layer architecture oftransformer-style LLMs makes them inherently capable of doing this simple levelof \"code analysis\" and extracting such information, implicitly, whileprocessing code: but are they, really? If they aren't, could explicitly addingthis information help? Our goal here is to investigate this question, using thecode summarization task and evaluate whether automatically augmenting an LLM'sprompt with semantic facts explicitly, actually helps.Prior work shows that LLM performance on code summarization benefits fromfew-shot samples drawn either from the same-project or from examples found viainformation retrieval methods (such as BM25). While summarization performancehas steadily increased since the early days, there is still room forimprovement: LLM performance on code summarization still lags its performanceon natural-language tasks like translation and text summarization.We find that adding semantic facts actually does help! This approach improvesperformance in several different settings suggested by prior work, includingfor two different Large Language Models. In most cases, improvement nears orexceeds 2 BLEU; for the PHP language in the challenging CodeSearchNet dataset,this augmentation actually yields performance surpassing 30 BLEU.", "output": "Improving Few-Shot Prompts with Relevant Static Analysis Products."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This technical report studies the problem of ranking from pairwisecomparisons in the classical Bradley-Terry-Luce (BTL) model, with a focus onscore estimation. For general graphs, we show that, with sufficiently manysamples, maximum likelihood estimation (MLE) achieves an entrywise estimationerror matching the Cram'er-Rao lower bound, which can be stated in terms ofeffective resistances; the key to our analysis is a connection betweenstatistical estimation and iterative optimization by preconditioned gradientdescent. We are also particularly interested in graphs with locality, whereonly nearby items can be connected by edges; our analysis identifies conditionsunder which locality does not hurt, i.e. comparing the scores between a pair ofitems that are far apart in the graph is nearly as easy as comparing a pair ofnearby items. We further explore divide-and-conquer algorithms that canprovably achieve similar guarantees even in the regime with the sparsestsamples, while enjoying certain computational advantages. Numerical resultsvalidate our theory and confirm the efficacy of the proposed algorithms.", "output": "Ranking from Pairwise Comparisons in General Graphs and Graphs with Locality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic Graph Neural Networks (DGNNs) are becoming increasingly popular dueto their effectiveness in analyzing and predicting the evolution of complexinterconnected graph-based systems. However, hardware deployment of DGNNs stillremains a challenge. First, DGNNs do not fully utilize hardware resourcesbecause temporal data dependencies cause low hardware parallelism.Additionally, there is currently a lack of generic DGNN hardware acceleratorframeworks, and existing GNN accelerator frameworks have limited ability tohandle dynamic graphs with changing topologies and node features. To addressthe aforementioned challenges, in this paper, we propose DGNN-Booster, which isa novel Field-Programmable Gate Array (FPGA) accelerator framework forreal-time DGNN inference using High-Level Synthesis (HLS). It includes twodifferent FPGA accelerator designs with different dataflows that can supportthe most widely used DGNNs. We showcase the effectiveness of our designs byimplementing and evaluating two representative DGNN models on ZCU102 board andmeasuring the end-to-end performance. The experiment results demonstrate thatDGNN-Booster can achieve a speedup of up to 5.6x compared to the CPU baseline(6226R), 8.4x compared to the GPU baseline (A6000) and 2.1x compared to theFPGA baseline without applying optimizations proposed in this paper. Moreover,DGNN-Booster can achieve over 100x and over 1000x runtime energy efficiencythan the CPU and GPU baseline respectively. Our implementation code andon-board measurements are publicly available at", "output": "DGNN-Booster: A Generic FPGA Accelerator Framework For Dynamic Graph Neural Network Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a simple non-linear embedding adaptation layer, which isfine-tuned on top of fixed pre-trained features for one-shot tasks, improvingsignificantly transductive entropy-based inference for low-shot regimes. Ournorm-induced transformation could be understood as a re-parametrization of thefeature space to disentangle the representations of different classes in a taskspecific manner. It focuses on the relevant feature dimensions while hinderingthe effects of non-relevant dimensions that may cause overfitting in a one-shotsetting. We also provide an interpretation of our proposed featuretransformation in the basic case of few-shot inference with K-means clustering.Furthermore, we give an interesting bound-optimization link between K-means andentropy minimization. This emphasizes why our feature transformation is usefulin the context of entropy minimization. We report comprehensive experiments,which show consistent improvements over a variety of one-shot benchmarks,outperforming recent state-of-the-art methods.", "output": "Task Adaptive Feature Transformation for One-Shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In data-driven stochastic optimization, model parameters of the underlyingdistribution need to be estimated from data in addition to the optimizationtask. Recent literature suggests the integration of the estimation andoptimization processes, by selecting model parameters that lead to the bestempirical objective performance. Such an integrated approach can be readilyshown to outperform simple ``estimate then optimize\" when the model ismisspecified. In this paper, we argue that when the model class is rich enoughto cover the ground truth, the performance ordering between the two approachesis reversed for nonlinear problems in a strong sense. Simple ``estimate thenoptimize\" outperforms the integrated approach in terms of stochastic dominanceof the asymptotic optimality gap, i,e, the mean, all other moments, and theentire asymptotic distribution of the optimality gap is always better.Analogous results also hold under constrained settings and when contextualfeatures are available. We also provide experimental findings to support ourtheory.", "output": "Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose to use various artificial neural network (ANN)structures for modeling and compensation of intra- and inter-subcarrier fibernonlinear interference in digital subcarrier multiplexing (DSCM) opticaltransmission systems. We perform nonlinear channel equalization by employingdifferent ANN cores including convolutional neural networks (CNN) and longshort-term memory (LSTM) layers. We start to compensate the fiber nonlinearitydistortion in DSCM systems by a fully connected network across all subcarriers.In subsequent steps, and borrowing from fiber nonlinearity analysis, wegradually upgrade the designs towards modular structures with betterperformance-complexity advantages. Our study shows that putting proper macrostructures in design of ANN nonlinear equalizers in DSCM systems can be crucialfor practical solutions in future generations of coherent optical transceivers.", "output": "Neural Network Architectures for Optical Channel Nonlinear Compensation in Digital Subcarrier Multiplexing Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although multi-task deep neural network (DNN) models have computation andstorage benefits over individual single-task DNN models, they can be furtheroptimized via model compression. Numerous structured pruning methods arealready developed that can readily achieve speedups in single-task models, butthe pruning of multi-task networks has not yet been extensively studied. Inthis work, we investigate the effectiveness of structured pruning on multi-taskmodels. We use an existing single-task filter pruning criterion and alsointroduce an MTL-based filter pruning criterion for estimating the filterimportance scores. We prune the model using an iterative pruning strategy withboth pruning methods. We show that, with careful hyper-parameter tuning,architectures obtained from different pruning methods do not have significantdifferences in their performances across tasks when the number of parameters issimilar. We also show that iterative structure pruning may not be the best wayto achieve a well-performing pruned model because, at extreme pruning levels,there is a high drop in performance across all tasks. But when the same modelsare randomly initialized and re-trained, they show better results.", "output": "Structured Pruning for Multi-Task Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we tackle the problem of video alignment, the process ofmatching the frames of a pair of videos containing similar actions. The mainchallenge in video alignment is that accurate correspondence should beestablished despite the differences in the execution processes and appearancesbetween the two videos. We introduce an unsupervised method for alignment thatuses global and local features of the frames. In particular, we introduceeffective features for each video frame by means of three machine vision tools:person detection, pose estimation, and VGG network. Then the features areprocessed and combined to construct a multidimensional time series thatrepresent the video. The resulting time series are used to align videos of thesame actions using a novel version of dynamic time warping named DiagonalizedDynamic Time Warping(DDTW). The main advantage of our approach is that notraining is required, which makes it applicable for any new type of actionwithout any need to collect training samples for it. For evaluation, weconsidered video synchronization and phase classification tasks on the Pennaction dataset. Also, for an effective evaluation of the video synchronizationtask, we present a new metric called Enclosed Area Error(EAE). The results showthat our method outperforms previous state-of-the-art methods, such as TCC andother self-supervised and supervised methods.", "output": "Video alignment using unsupervised learning of local and global features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recommender systems are increasingly successful in recommending personalizedcontent to users. However, these systems often capitalize on popular content.There is also a continuous evolution of user interests that need to becaptured, but there is no direct way to systematically explore users'interests. This also tends to affect the overall quality of the recommendationpipeline as training data is generated from the candidates presented to theuser. In this paper, we present a framework for exploration in large-scalerecommender systems to address these challenges. It consists of three parts,first the user-creator exploration which focuses on identifying the bestcreators that users are interested in, second the online exploration frameworkand third a feed composition mechanism that balances explore and exploit toensure optimal prevalence of exploratory videos. Our methodology can be easilyintegrated into an existing large-scale recommender system with minimalmodifications. We also analyze the value of exploration by defining relevantmetrics around user-creator connections and understanding how this helps theoverall recommendation pipeline with strong online gains in creator andecosystem value. In contrast to the regression on user engagement metricsgenerally seen while exploring, our method is able to achieve significantimprovements of 3.50% in strong creator connections and 0.85% increase in novelcreator connections. Moreover, our work has been deployed in production onFacebook Watch, a popular video discovery and sharing platform serving billionsof users.", "output": "PIE: Personalized Interest Exploration for Large-Scale Recommender Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robots operating in real-world environments must reason about possibleoutcomes of stochastic actions and make decisions based on partial observationsof the true world state. A major challenge for making accurate and robustaction predictions is the problem of confounding, which if left untreated canlead to prediction errors. The partially observable Markov decision process(POMDP) is a widely-used framework to model these stochastic andpartially-observable decision-making problems. However, due to a lack ofexplicit causal semantics, POMDP planning methods are prone to confounding biasand thus in the presence of unobserved confounders may produce underperformingpolicies. This paper presents a novel causally-informed extension of \"anytimeregularized determinized sparse partially observable tree\" (AR-DESPOT), amodern anytime online POMDP planner, using causal modelling and inference toeliminate errors caused by unmeasured confounder variables. We further proposea method to learn offline the partial parameterisation of the causal model forplanning, from ground truth model data. We evaluate our methods on a toyproblem with an unobserved confounder and show that the learned causal model ishighly accurate, while our planning method is more robust to confounding andproduces overall higher performing policies than AR-DESPOT.", "output": "CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vaccine hesitancy continues to be a main challenge for public healthofficials during the COVID-19 pandemic. As this hesitancy undermines vaccinecampaigns, many researchers have sought to identify its root causes, findingthat the increasing volume of anti-vaccine misinformation on social mediaplatforms is a key element of this problem. We explored Twitter as a source ofmisleading content with the goal of extracting overlapping cultural andpolitical beliefs that motivate the spread of vaccine misinformation. To dothis, we have collected a data set of vaccine-related Tweets and annotated themwith the help of a team of annotators with a background in communications andjournalism. Ultimately we hope this can lead to effective and targeted publichealth communication strategies for reaching individuals with anti-vaccinebeliefs. Moreover, this information helps with developing Machine Learningmodels to automatically detect vaccine misinformation posts and combat theirnegative impacts. In this paper, we present Vax-Culture, a novel TwitterCOVID-19 dataset consisting of 6373 vaccine-related tweets accompanied by anextensive set of human-provided annotations including vaccine-hesitancy stance,indication of any misinformation in tweets, the entities criticized andsupported in each tweet and the communicated message of each tweet. Moreover,we define five baseline tasks including four classification and one sequencegeneration tasks, and report the results of a set of recent transformer-basedmodels for them. The dataset and code are publicly available at", "output": "Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large pre-trained language models are widely used in the community. Thesemodels are usually trained on unmoderated and unfiltered data from open sourceslike the Internet. Due to this, biases that we see in platforms online whichare a reflection of those in society are in turn captured and learned by thesemodels. These models are deployed in applications that affect millions ofpeople and their inherent biases are harmful to the targeted social groups. Inthis work, we study the general trend in bias reduction as newer pre-trainedmodels are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT)are chosen and evaluated against two bias benchmarks, StereoSet andCrowS-Pairs. They are compared to the baseline of BERT using the associatedmetrics. We explore whether as advancements are made and newer, faster, lightermodels are released: are they being developed responsibly such that theirinherent social biases have been reduced compared to their older counterparts?The results are compiled and we find that all the models under study do exhibitbiases but have generally improved as compared to BERT.", "output": "Evaluation of Social Biases in Recent Large Pre-Trained Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervision methods learn representations by solving pretext tasks thatdo not require human-generated labels, alleviating the need for time-consumingannotations. These methods have been applied in computer vision, naturallanguage processing, environmental sound analysis, and recently in musicinformation retrieval, e.g. for pitch estimation. Particularly in the contextof music, there are few insights about the fragility of these models regardingdifferent distributions of data, and how they could be mitigated. In thispaper, we explore these questions by dissecting a self-supervised model forpitch estimation adapted for tempo estimation via rigorous experimentation withsynthetic data. Specifically, we study the relationship between the inputrepresentation and data distribution for self-supervised tempo estimation.", "output": "Tempo vs. Pitch: understanding self-supervised tempo estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As language models scale up, it becomes increasingly expensive to verifyresearch ideas because conclusions on small models do not trivially transfer tolarge ones. A possible solution is to establish a generic system that directlypredicts some metrics for large models solely based on the results andhyperparameters from small models. Existing methods based on scaling lawsrequire hyperparameter search on the largest models, which is impractical withlimited resources. We address this issue by presenting our discoveriesindicating that Maximal Update parametrization (muP) enables accurate fittingof scaling laws for hyperparameters close to common loss basins, without anysearch. Thus, different models can be directly compared on large scales withloss prediction even before the training starts. We propose a new paradigm as afirst step towards reliable academic research for any model scale without heavycomputation. Code will be publicly available shortly.", "output": "Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Performative prediction is a framework for learning models that influence thedata they intend to predict. We focus on finding classifiers that areperformatively stable, i.e. optimal for the data distribution they induce.Standard convergence results for finding a performatively stable classifierwith the method of repeated risk minimization assume that the data distributionis Lipschitz continuous to the model's parameters. Under this assumption, theloss must be strongly convex and smooth in these parameters; otherwise, themethod will diverge for some problems. In this work, we instead assume that thedata distribution is Lipschitz continuous with respect to the model'spredictions, a more natural assumption for performative systems. As a result,we are able to significantly relax the assumptions on the loss function. Inparticular, we do not need to assume convexity with respect to the model'sparameters. As an illustration, we introduce a resampling procedure that modelsrealistic distribution shifts and show that it satisfies our assumptions. Wesupport our theory by showing that one can learn performatively stableclassifiers with neural networks making predictions about real data that shiftaccording to our proposed procedure.", "output": "Performative Prediction with Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning algorithms are increasingly used to make or supportdecisions in a wide range of settings. With such expansive use there is alsogrowing concern about the fairness of such methods. Prior literature onalgorithmic fairness has extensively addressed risks and in many casespresented approaches to manage some of them. However, most studies have focusedon fairness issues that arise from actions taken by a (single) focaldecision-maker or agent. In contrast, most real-world systems have many agentsthat work collectively as part of a larger ecosystem. For example, in a lendingscenario, there are multiple lenders who evaluate loans for applicants, alongwith policymakers and other institutions whose decisions also affect outcomes.Thus, the broader impact of any lending decision of a single decision makerwill likely depend on the actions of multiple different agents in theecosystem. This paper develops formalisms for firm versus systemic fairness,and calls for a greater focus in the algorithmic fairness literature onecosystem-wide fairness - or more simply systemic fairness - in real-worldcontexts.", "output": "Systemic Fairness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In most image retrieval systems, images include various high-level semantics,called tags or annotations. Virtually all the state-of-the-art image annotationmethods that handle imbalanced labeling are search-based techniques which aretime-consuming. In this paper, a novel coupled dictionary learning approach isproposed to learn a limited number of visual prototypes and their correspondingsemantics simultaneously. This approach leads to a real-time image annotationprocedure. Another contribution of this paper is that utilizes a marginalizedloss function instead of the squared loss function that is inappropriate forimage annotation with imbalanced labels. We have employed a marginalized lossfunction in our method to leverage a simple and effective method of prototypeupdating. Meanwhile, we have introduced ${ell}_1$ regularization on semanticprototypes to preserve the sparse and imbalanced nature of labels in learnedsemantic prototypes. Finally, comprehensive experimental results on variousdatasets demonstrate the efficiency of the proposed method for image annotationtasks in terms of accuracy and time. The reference implementation is publiclyavailable on ", "output": "Toward Real-Time Image Annotation Using Marginalized Coupled Dictionary Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) have been shown to be vulnerable to adversarialexamples. Moreover, the transferability of the adversarial examples hasreceived broad attention in recent years, which means that adversarial examplescrafted by a surrogate model can also attack unknown models. This phenomenongave birth to the transfer-based adversarial attacks, which aim to improve thetransferability of the generated adversarial examples. In this paper, wepropose to improve the transferability of adversarial examples in thetransfer-based attack via masking unimportant parameters (MUP). The key idea inMUP is to refine the pretrained surrogate models to boost the transfer-basedattack. Based on this idea, a Taylor expansion-based metric is used to evaluatethe parameter importance score and the unimportant parameters are masked duringthe generation of adversarial examples. This process is simple, yet can benaturally combined with various existing gradient-based optimizers forgenerating adversarial examples, thus further improving the transferability ofthe generated adversarial examples. Extensive experiments are conducted tovalidate the effectiveness of the proposed MUP-based methods.", "output": "Generating Adversarial Examples with Better Transferability via Masking Unimportant Parameters of Surrogate Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While having achieved great success in rich real-life applications, deepneural network (DNN) models have long been criticized for their vulnerabilityto adversarial attacks. Tremendous research efforts have been dedicated tomitigating the threats of adversarial attacks, but the essential trait ofadversarial examples is not yet clear, and most existing methods are yetvulnerable to hybrid attacks and suffer from counterattacks. In light of this,in this paper, we first reveal a gradient-based correlation between sensitivityanalysis-based DNN interpreters and the generation process of adversarialexamples, which indicates the Achilles's heel of adversarial attacks and shedslight on linking together the two long-standing challenges of DNN: fragilityand unexplainability. We then propose an interpreter-based ensemble frameworkcalled X-Ensemble for robust adversary defense. X-Ensemble adopts a noveldetection-rectification process and features in building multiple sub-detectorsand a rectifier upon various types of interpretation information toward targetclassifiers. Moreover, X-Ensemble employs the Random Forests (RF) model tocombine sub-detectors into an ensemble detector for adversarial hybrid attacksdefense. The non-differentiable property of RF further makes it a preciouschoice against the counterattack of adversaries. Extensive experiments undervarious types of state-of-the-art attacks and diverse attack scenariosdemonstrate the advantages of X-Ensemble to competitive baseline methods.", "output": "Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) has been introduced to the healthcare domain as adecentralized learning paradigm that allows multiple parties to train a modelcollaboratively without privacy leakage. However, most previous studies haveassumed that every client holds an identical label set. In reality, medicalspecialists tend to annotate only diseases within their knowledge domain orinterest. This implies that label sets in each client can be different and evendisjoint. In this paper, we propose the framework FedLSM to solve the problemLabel Set Mismatch. FedLSM adopts different training strategies on data withdifferent uncertainty levels to efficiently utilize unlabeled or partiallylabeled data as well as class-wise adaptive aggregation in the classificationlayer to avoid inaccurate aggregation when clients have missing labels. Weevaluate FedLSM on two public real-world medical image datasets, includingchest x-ray (CXR) diagnosis with 112,120 CXR images and skin lesion diagnosiswith 10,015 dermoscopy images, and show that it significantly outperforms otherstate-of-the-art FL algorithms. Code will be made available upon acceptance.", "output": "Scale Federated Learning for Label Set Mismatch in Medical Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The abstract outlines the problem of toxic comments on social mediaplatforms, where individuals use disrespectful, abusive, and unreasonablelanguage that can drive users away from discussions. This behavior is referredto as anti-social behavior, which occurs during online debates, comments, andfights. The comments containing explicit language can be classified intovarious categories, such as toxic, severe toxic, obscene, threat, insult, andidentity hate. This behavior leads to online harassment and cyberbullying,which forces individuals to stop expressing their opinions and ideas. Toprotect users from offensive language, companies have started flagging commentsand blocking users. The abstract proposes to create a classifier using anLstm-cnn model that can differentiate between toxic and non-toxic comments withhigh accuracy. The classifier can help organizations examine the toxicity ofthe comment section better.", "output": "Classification of social media Toxic comments using Machine learning models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sparse training is emerging as a promising avenue for reducing thecomputational cost of training neural networks. Several recent studies haveproposed pruning methods using learnable thresholds to efficiently explore thenon-uniform distribution of sparsity inherent within the models. In this paper,we propose Gradient Annealing (GA), where gradients of masked weights arescaled down in a non-linear manner. GA provides an elegant trade-off betweensparsity and accuracy without the need for additional sparsity-inducingregularization. We integrated GA with the latest learnable pruning methods tocreate an automated sparse training algorithm called AutoSparse, which achievesbetter accuracy and/or training/inference FLOPS reduction than existinglearnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K:AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS forResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperformssparse-to-sparse SotA method MEST (uniform sparsity) for 80% sparse ResNet50with similar accuracy, where MEST uses 12% more training FLOPS and 50% moreinference FLOPS.", "output": "AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In cross-device Federated Learning (FL) environments, scaling synchronous FLmethods is challenging as stragglers hinder the training process. Moreover, theavailability of each client to join the training is highly variable over timedue to system heterogeneities and intermittent connectivity. Recentasynchronous FL methods (e.g., FedBuff) have been proposed to overcome theseissues by allowing slower users to continue their work on local training basedon stale models and to contribute to aggregation when ready. However, we showempirically that this method can lead to a substantial drop in trainingaccuracy as well as a slower convergence rate. The primary reason is thatfast-speed devices contribute to many more rounds of aggregation while othersjoin more intermittently or not at all, and with stale model updates. Toovercome this barrier, we propose TimelyFL, a heterogeneity-aware asynchronousFL framework with adaptive partial training. During the training, TimelyFLadjusts the local training workload based on the real-time resourcecapabilities of each client, aiming to allow more available clients to join inthe global update without staleness. We demonstrate the performance benefits ofTimelyFL by conducting extensive experiments on various datasets (e.g.,CIFAR-10, Google Speech, and Reddit) and models (e.g., ResNet20, VGG11, andALBERT). In comparison with the state-of-the-art (i.e., FedBuff), ourevaluations reveal that TimelyFL improves participation rate by 21.13%,harvests 1.28x - 2.89x more efficiency on convergence rate, and provides a6.25% increment on test accuracy.", "output": "TimelyFL: Heterogeneity-aware Asynchronous Federated Learning with Adaptive Partial Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hypertension is a medical condition characterized by high blood pressure, andclassifying it into its various stages is crucial to managing the disease. Inthis project, a novel method is proposed for classifying stages of hypertensionusing Photoplethysmography (PPG) signals and deep learning models, namelyAvgPool_VGG-16. The PPG signal is a non-invasive method of measuring bloodpressure through the use of light sensors that measure the changes in bloodvolume in the microvasculature of tissues. PPG images from the publiclyavailable blood pressure classification dataset were used to train the model.Multiclass classification for various PPG stages were done. The results showthe proposed method achieves high accuracy in classifying hypertension stages,demonstrating the potential of PPG signals and deep learning models inhypertension diagnosis and management.", "output": "PPG Signals for Hypertension Diagnosis: A Novel Method using Deep Learning Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding the COVID-19 vaccine hesitancy, such as who and why, is verycrucial since a large-scale vaccine adoption remains as one of the mostefficient methods of controlling the pandemic. Such an understanding alsoprovides insights into designing successful vaccination campaigns for futurepandemics. Unfortunately, there are many factors involving in deciding whetherto take the vaccine, especially from the cultural point of view. To obtainthese goals, we design a novel culture-aware machine learning (ML) model, basedon our new data collection, for predicting vaccination willingness. We furtheranalyze the most important features which contribute to the ML model'spredictions using advanced AI explainers such as the Probabilistic GraphicalModel (PGM) and Shapley Additive Explanations (SHAP). These analyses reveal thekey factors that most likely impact the vaccine adoption decisions. Ourfindings show that Hispanic and African American are most likely impacted bycultural characteristics such as religions and ethnic affiliation, whereas thevaccine trust and approval influence the Asian communities the most. Ourresults also show that cultural characteristics, rumors, and politicalaffiliation are associated with increased vaccine rejection.", "output": "Cultural-aware Machine Learning based Analysis of COVID-19 Vaccine Hesitancy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Soft-thresholding has been widely used in neural networks. Its basic networkstructure is a two-layer convolution neural network with soft-thresholding. Dueto the network's nature of nonlinearity and nonconvexity, the training processheavily depends on an appropriate initialization of network parameters,resulting in the difficulty of obtaining a globally optimal solution. Toaddress this issue, a convex dual network is designed here. We theoreticallyanalyze the network convexity and numerically confirm that the strong dualityholds. This conclusion is further verified in the linear fitting and denoisingexperiments. This work provides a new way to convexify soft-thresholding neuralnetworks.", "output": "Convex Dual Theory Analysis of Two-Layer Convolutional Neural Networks with Soft-Thresholding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Depth Estimation has wide reaching applications in the field of Computervision such as target tracking, augmented reality, and self-driving cars. Thegoal of Monocular Depth Estimation is to predict the depth map, given a 2Dmonocular RGB image as input. The traditional depth estimation methods arebased on depth cues and used concepts like epipolar geometry. With theevolution of Convolutional Neural Networks, depth estimation has undergonetremendous strides. In this project, our aim is to explore possible extensionsto existing SoTA Deep Learning based Depth Estimation Models and to see whetherperformance metrics could be further improved. In a broader sense, we arelooking at the possibility of implementing Pose Estimation, Efficient Sub-PixelConvolution Interpolation, Semantic Segmentation Estimation techniques tofurther enhance our proposed architecture and to provide fine-grained and moreglobally coherent depth map predictions. We also plan to do away with cameraintrinsic parameters during training and apply weather augmentations to furthergeneralize our model.", "output": "Self-Supervised Learning based Depth Estimation from Monocular Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning new classes without forgetting is crucial for real-worldapplications for a classification model. Vision Transformers (ViT) recentlyachieve remarkable performance in Class Incremental Learning (CIL). Previousworks mainly focus on block design and model expansion for ViTs. However, inthis paper, we find that when the ViT is incrementally trained, the attentionlayers gradually lose concentration on local features. We call this interestingphenomenon as emph{Locality Degradation} in ViTs for CIL. Since the low-levellocal information is crucial to the transferability of the representation, itis beneficial to preserve the locality in attention layers. In this paper, weencourage the model to preserve more local information as the trainingprocedure goes on and devise a Locality-Preserved Attention (LPA) layer toemphasize the importance of local features. Specifically, we incorporate thelocal information directly into the vanilla attention and control the initialgradients of the vanilla attention by weighting it with a small initial value.Extensive experiments show that the representations facilitated by LPA capturemore low-level general information which is easier to transfer to follow-uptasks. The improved model gets consistently better performance on CIFAR100 andImageNet100.", "output": "Preserving Locality in Vision Transformers for Class Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven prediction of fluid flow and temperature distribution in marineand aerospace engineering has received extensive research and demonstrated itspotential in real-time prediction recently. However, usually large amounts ofhigh-fidelity data are required to describe and accurately predict the complexphysical information, while in reality, only limited high-fidelity data isavailable due to the high experiment/computational cost. Therefore, this workproposes a novel multi-fidelity learning method based on the Fourier NeuralOperator by jointing abundant low-fidelity data and limited high-fidelity dataunder transfer learning paradigm. First, as a resolution-invariant operator,the Fourier Neural Operator is first and gainfully applied to integratemulti-fidelity data directly, which can utilize the scarce high-fidelity dataand abundant low-fidelity data simultaneously. Then, the transfer learningframework is developed for the current task by extracting the rich low-fidelitydata knowledge to assist high-fidelity modeling training, to further improvedata-driven prediction accuracy. Finally, three typical fluid and temperatureprediction problems are chosen to validate the accuracy of the proposedmulti-fidelity model. The results demonstrate that our proposed method has higheffectiveness when compared with other high-fidelity models, and has the highmodeling accuracy of 99% for all the selected physical field problems.Significantly, the proposed multi-fidelity learning method has the potential ofa simple structure with high precision, which can provide a reference for theconstruction of the subsequent model.", "output": "Multi-fidelity prediction of fluid flow and temperature field based on transfer learning using Fourier Neural Operator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, communication engineers put strong emphasis on artificialneural network (ANN)-based algorithms with the aim of increasing theflexibility and autonomy of the system and its components. In this context,unsupervised training is of special interest as it enables adaptation withoutthe overhead of transmitting pilot symbols. In this work, we present a novelANN-based, unsupervised equalizer and its trainable field programmable gatearray (FPGA) implementation. We demonstrate that our custom loss functionallows the ANN to adapt for varying channel conditions, approaching theperformance of a supervised baseline. Furthermore, as a first step towards apractical communication system, we design an efficient FPGA implementation ofour proposed algorithm, which achieves a throughput in the order of Gbit/s,outperforming a high-performance GPU by a large margin.", "output": "Unsupervised ANN-Based Equalizer and Its Trainable FPGA Implementation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Convolutional Network (GCN) with the powerful capacity to exploregraph-structural data has gained noticeable success in recent years.Nonetheless, most of the existing GCN-based models suffer from the notoriousover-smoothing issue, owing to which shallow networks are extensively adopted.This may be problematic for complex graph datasets because a deeper GCN shouldbe beneficial to propagating information across remote neighbors. Recent workshave devoted effort to addressing over-smoothing problems, includingestablishing residual connection structure or fusing predictions frommulti-layer models. Because of the indistinguishable embeddings from deeplayers, it is reasonable to generate more reliable predictions beforeconducting the combination of outputs from various layers. In light of this, wepropose an Alternating Graph-regularized Neural Network (AGNN) composed ofGraph Convolutional Layer (GCL) and Graph Embedding Layer (GEL). GEL is derivedfrom the graph-regularized optimization containing Laplacian embedding term,which can alleviate the over-smoothing problem by periodic projection from thelow-order feature space onto the high-order space. With more distinguishablefeatures of distinct layers, an improved Adaboost strategy is utilized toaggregate outputs from each layer, which explores integrated embeddings ofmulti-hop neighbors. The proposed model is evaluated via a large number ofexperiments including performance comparison with some multi-layer ormulti-order graph neural networks, which reveals the superior performanceimprovement of AGNN compared with state-of-the-art models.", "output": "AGNN: Alternating Graph-Regularized Neural Networks to Alleviate Over-Smoothing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning, and representation learning in particular, has thepotential to facilitate drug discovery by screening billions of compounds. Forexample, a successful approach is representing the molecules as a graph andutilizing graph neural networks (GNN). Yet, these approaches still requireexperimental measurements of thousands of compounds to construct a propertraining set. While in some domains it is easier to acquire experimental data,in others it might be more limited. For example, it is easier to test thecompounds on bacteria than perform in-vivo experiments. Thus, a key question ishow to utilize information from a large available dataset together with a smallsubset of compounds where both domains are measured to predict compounds'effect on the second, experimentally less available domain. Current transferlearning approaches for drug discovery, including training of pre-trainedmodules or meta-learning, have limited success. In this work, we develop anovel method, named Symbiotic Message Passing Neural Network (SMPNN), formerging graph-neural-network models from different domains. Using routing newmessage passing lanes between them, our approach resolves some of the potentialconflicts between the different domains, and implicit constraints induced bythe larger datasets. By collecting public data and performing additionalhigh-throughput experiments, we demonstrate the advantage of our approach bypredicting anti-fungal activity from anti-bacterial activity. We compare ourmethod to the standard transfer learning approach and show that SMPNN providedbetter and less variable performances. Our approach is general and can be usedto facilitate information transfer between any two domains such as differentorganisms, different organelles, or different environments.", "output": "Symbiotic Message Passing Model for Transfer Learning between Anti-Fungal and Anti-Bacterial Domains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient deep learning-based approaches have achieved remarkable performancein single image super-resolution. However, recent studies on efficientsuper-resolution have mainly focused on reducing the number of parameters andfloating-point operations through various network designs. Although thesemethods can decrease the number of parameters and floating-point operations,they may not necessarily reduce actual running time. To address this issue, wepropose a novel multi-stage lightweight network boosting method, which canenable lightweight networks to achieve outstanding performance. Specifically,we leverage enhanced high-resolution output as additional supervision toimprove the learning ability of lightweight student networks. Upon convergenceof the student network, we further simplify our network structure to a morelightweight level using reparameterization techniques and iterative networkpruning. Meanwhile, we adopt an effective lightweight network training strategythat combines multi-anchor distillation and progressive learning, enabling thelightweight network to achieve outstanding performance. Ultimately, ourproposed method achieves the fastest inference time among all participants inthe NTIRE 2023 efficient super-resolution challenge while maintainingcompetitive super-resolution performance. Additionally, extensive experimentsare conducted to demonstrate the effectiveness of the proposed components. Theresults show that our approach achieves comparable performance inrepresentative dataset DIV2K, both qualitatively and quantitatively, withfaster inference and fewer number of network parameters.", "output": "DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Irregularly measured time series are common in many of the applied settingsin which time series modelling is a key statistical tool, including medicine.This provides challenges in model choice, often necessitating imputation orsimilar strategies. Continuous time autoregressive recurrent neural networks(CTRNNs) are a deep learning model that account for irregular observationsthrough incorporating continuous evolution of the hidden states betweenobservations. This is achieved using a neural ordinary differential equation(ODE) or neural flow layer. In this manuscript, we give an overview of thesemodels, including the varying architectures that have been proposed to accountfor issues such as ongoing medical interventions. Further, we demonstrate theapplication of these models to probabilistic forecasting of blood glucose in acritical care setting using electronic medical record and simulated data. Theexperiments confirm that addition of a neural ODE or neural flow layergenerally improves the performance of autoregressive recurrent neural networksin the irregular measurement setting. However, several CTRNN architecture areoutperformed by an autoregressive gradient boosted tree model (Catboost), withonly a long short-term memory (LSTM) and neural ODE based architecture(ODE-LSTM) achieving comparable performance on probabilistic forecastingmetrics such as the continuous ranked probability score (ODE-LSTM:0.118$pm$0.001; Catboost: 0.118$pm$0.001), ignorance score (0.152$pm$0.008;0.149$pm$0.002) and interval score (175$pm$1; 176$pm$1).", "output": "Continuous time recurrent neural networks: overview and application to forecasting blood glucose in the intensive care unit."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Long-term stability is a critical property for deep learning-baseddata-driven digital twins of the Earth system. Such data-driven digital twinsenable sub-seasonal and seasonal predictions of extreme environmental events,probabilistic forecasts, that require a large number of ensemble members, andcomputationally tractable high-resolution Earth system models where expensivecomponents of the models can be replaced with cheaper data-driven surrogates.Owing to computational cost, physics-based digital twins, though long-termstable, are intractable for real-time decision-making. Data-driven digitaltwins offer a cheaper alternative to them and can provide real-timepredictions. However, such digital twins can only provide short-term forecastsaccurately since they become unstable when time-integrated beyond 20 days.Currently, the cause of the instabilities is unknown, and the methods that areused to improve their stability horizons are ad-hoc and lack rigorous theory.In this paper, we reveal that the universal causal mechanism for theseinstabilities in any turbulent flow is due to textit{spectral bias} wherein,textit{any} deep learning architecture is biased to learn only the large-scaledynamics and ignores the small scales completely. We further elucidate howturbulence physics and the absence of convergence in deep learning-basedtime-integrators amplify this bias leading to unstable error propagation.Finally, using the quasigeostrophic flow and ECMWF Reanalysis data as testcases, we bridge the gap between deep learning theory and fundamental numericalanalysis to propose one mitigative solution to such instabilities. We developlong-term stable data-driven digital twins for the climate system anddemonstrate accurate short-term forecasts, and hundreds of years of long-termstable time-integration with accurate mean and variability.", "output": "Long-term instabilities of deep learning-based digital twins of the climate system: The cause and a solution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The exploitation of visible spectrum datasets has led deep networks to showremarkable success. However, real-world tasks include low-lighting conditionswhich arise performance bottlenecks for models trained on large-scale RGB imagedatasets. Thermal IR cameras are more robust against such conditions.Therefore, the usage of thermal imagery in real-world applications can beuseful. Unsupervised domain adaptation (UDA) allows transferring informationfrom a source domain to a fully unlabeled target domain. Despite substantialimprovements in UDA, the performance gap between UDA and its supervisedlearning counterpart remains significant. By picking a small number of targetsamples to annotate and using them in training, active domain adaptation triesto mitigate this gap with minimum annotation expense. We propose an activedomain adaptation method in order to examine the efficiency of combining thevisible spectrum and thermal imagery modalities. When the domain gap isconsiderably large as in the visible-to-thermal task, we may conclude that themethods without explicit domain alignment cannot achieve their full potential.To this end, we propose a spectral transfer guided active domain adaptationmethod to select the most informative unlabeled target samples while aligningsource and target domains. We used the large-scale visible spectrum datasetMS-COCO as the source domain and the thermal dataset FLIR ADAS as the targetdomain to present the results of our method. Extensive experimental evaluationdemonstrates that our proposed method outperforms the state-of-the-art activedomain adaptation methods. The code and models are publicly available.", "output": "Spectral Transfer Guided Active Domain Adaptation For Thermal Imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ultrasound is the primary modality to examine fetal growth during pregnancy,while the image quality could be affected by various factors. Qualityassessment is essential for controlling the quality of ultrasound images toguarantee both the perceptual and diagnostic values. Existing automatedapproaches often require heavy structural annotations and the predictions maynot necessarily be consistent with the assessment results by human experts.Furthermore, the overall quality of a scan and the correlation between thequality of frames should not be overlooked. In this work, we propose areinforcement learning framework powered by two hierarchical agents thatcollaboratively learn to perform both frame-level and video-level qualityassessments. It is equipped with a specially-designed reward mechanism thatconsiders temporal dependency among frame quality and only requires sparsebinary annotations to train. Experimental results on a challenging fetal braindataset verify that the proposed framework could perform dual-level qualityassessment and its predictions correlate well with the subjective assessmentresults.", "output": "Hierarchical Agent-based Reinforcement Learning Framework for Automated Quality Assessment of Fetal Ultrasound Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "PAC-Bayes learning is an established framework to assess the generalisationability of learning algorithm during the training phase. However, it remainschallenging to know whether PAC-Bayes is useful to understand, before training,why the output of well-known algorithms generalise well. We positively answerthis question by expanding the emph{Wasserstein PAC-Bayes} framework, brieflyintroduced in cite{amit2022ipm}. We provide new generalisation boundsexploiting geometric assumptions on the loss function. Using our framework, weprove, before any training, that the output of an algorithm fromcitet{lambert2022variational} has a strong asymptotic generalisation ability.More precisely, we show that it is possible to incorporate optimisation resultswithin a generalisation framework, building a bridge between PAC-Bayes andoptimisation algorithms.", "output": "Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes to reduce visibility data volume using abaseline-dependent lossy compression technique that preserves smearing at theedges of the field-of-view. We exploit the relation of the rank of a matrix andthe fact that a low-rank approximation can describe the raw visibility data asa sum of basic components where each basic component corresponds to a specificFourier component of the sky distribution. As such, the entire visibility datais represented as a collection of data matrices from baselines, instead of asingle tensor. The proposed methods are formulated as follows: provided a largedataset of the entire visibility data; the first algorithm, named $simple~SVD$projects the data into a regular sampling space of rank$-r$ data matrices. Inthis space, the data for all the baselines has the same rank, which makes thecompression factor equal across all baselines. The second algorithm, named$BDSVD$ projects the data into an irregular sampling space of rank$-r_{pq}$data matrices. The subscript $pq$ indicates that the rank of the data matrixvaries across baselines $pq$, which makes the compression factorbaseline-dependent. MeerKAT and the European Very Long Baseline InterferometryNetwork are used as reference telescopes to evaluate and compare theperformance of the proposed methods against traditional methods, such astraditional averaging and baseline-dependent averaging (BDA). For the samespatial resolution threshold, both $simple~SVD$ and $BDSVD$ show effectivecompression by two-orders of magnitude higher than traditional averaging andBDA. At the same space-saving rate, there is no decrease in spatial resolutionand there is a reduction in the noise variance in the data which improves theS/N to over $1.5$ dB at the edges of the field-of-view.", "output": "Lossy Compression of Large-Scale Radio Interferometric Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have witnessed an exponential increase in the demand for facevideo compression, and the success of artificial intelligence has expanded theboundaries beyond traditional hybrid video coding. Generative coding approacheshave been identified as promising alternatives with reasonable perceptualrate-distortion trade-offs, leveraging the statistical priors of face videos.However, the great diversity of distortion types in spatial and temporaldomains, ranging from the traditional hybrid coding frameworks to generativemodels, present grand challenges in compressed face video quality assessment(VQA). In this paper, we introduce the large-scale Compressed Face VideoQuality Assessment (CFVQA) database, which is the first attempt tosystematically understand the perceptual quality and diversified compressiondistortions in face videos. The database contains 3,240 compressed face videoclips in multiple compression levels, which are derived from 135 source videoswith diversified content using six representative video codecs, including twotraditional methods based on hybrid coding frameworks, two end-to-end methods,and two generative methods. In addition, a FAce VideO IntegeRity (FAVOR) indexfor face video compression was developed to measure the perceptual quality,considering the distinct content characteristics and temporal priors of theface videos. Experimental results exhibit its superior performance on theproposed CFVQA dataset. The benchmark is now made publicly available at:", "output": "Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning on knowledge graphs is a challenging task because it utilizesobserved information to predict the missing one. Specifically, answeringfirst-order logic formulas is of particular interest because of its clearsyntax and semantics. Recently, the query embedding method has been proposedwhich learns the embedding of a set of entities and treats logic operations asset operations. Though there has been much research following the samemethodology, it lacks a systematic inspection from the standpoint of logic. Inthis paper, we characterize the scope of queries investigated previously andprecisely identify the gap between it and the whole family of existentialformulas. Moreover, we develop a new dataset containing ten new formulas anddiscuss the new challenges coming simultaneously. Finally, we propose a newsearch algorithm from fuzzy logic theory which is capable of solving newformulas and outperforming the previous methods in existing formulas.", "output": "On Existential First Order Queries Inference on Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The highly structured energy landscape of the loss as a function ofparameters for deep neural networks makes it necessary to use sophisticatedoptimization strategies in order to discover (local) minima that guaranteereasonable performance. Overcoming less suitable local minima is an importantprerequisite and often momentum methods are employed to achieve this. As inother non local optimization procedures, this however creates the necessity tobalance between exploration and exploitation. In this work, we suggest an eventbased control mechanism for switching from exploration to exploitation based onreaching a predefined reduction of the loss function. As we give the momentummethod a port Hamiltonian interpretation, we apply the 'heavy ball withfriction' interpretation and trigger breaking (or friction) when achievingcertain goals. We benchmark our method against standard stochastic gradientdescent and provide experimental evidence for improved performance of deepneural networks when our strategy is applied.", "output": "Who breaks early, looses: goal oriented training of deep neural networks based on port Hamiltonian dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The transportation sector accounts for about 25% of global greenhouse gasemissions. Therefore, an improvement of energy efficiency in the traffic sectoris crucial to reducing the carbon footprint. Efficiency is typically measuredin terms of energy use per traveled distance, e.g. liters of fuel perkilometer. Leading factors that impact the energy efficiency are the type ofvehicle, environment, driver behavior, and weather conditions. These varyingfactors introduce uncertainty in estimating the vehicles' energy efficiency. Wepropose in this paper an ensemble learning approach based on deep neuralnetworks (ENN) that is designed to reduce the predictive uncertainty and tooutput measures of such uncertainty. We evaluated it using the publiclyavailable Vehicle Energy Dataset (VED) and compared it with several baselinesper vehicle and energy type. The results showed a high predictive performanceand they allowed to output a measure of predictive uncertainty.", "output": "Uncertainty-Aware Vehicle Energy Efficiency Prediction using an Ensemble of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion probabilistic models have been successful in generatinghigh-quality and diverse images. However, traditional models, whose input andoutput are high-resolution images, suffer from excessive memory requirements,making them less practical for edge devices. Previous approaches for generativeadversarial networks proposed a patch-based method that uses positionalencoding and global content information. Nevertheless, designing a patch-basedapproach for diffusion probabilistic models is non-trivial. In this paper, weresent a diffusion probabilistic model that generates images on apatch-by-patch basis. We propose two conditioning methods for a patch-basedgeneration. First, we propose position-wise conditioning using one-hotrepresentation to ensure patches are in proper positions. Second, we proposeGlobal Content Conditioning (GCC) to ensure patches have coherent content whenconcatenated together. We evaluate our model qualitatively and quantitativelyon CelebA and LSUN bedroom datasets and demonstrate a moderate trade-offbetween maximum memory consumption and generated image quality. Specifically,when an entire image is divided into 2 x 2 patches, our proposed approach canreduce the maximum memory consumption by half while maintaining comparableimage quality.", "output": "Memory Efficient Diffusion Probabilistic Models via Patch-based Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Delta Denoising Score (DDS), a novel scoring function fortext-based image editing that guides minimal modifications of an input imagetowards the content described in a target prompt. DDS leverages the richgenerative prior of text-to-image diffusion models and can be used as a lossterm in an optimization problem to steer an image towards a desired directiondictated by a text. DDS utilizes the Score Distillation Sampling (SDS)mechanism for the purpose of image editing. We show that using only SDS oftenproduces non-detailed and blurry outputs due to noisy gradients. To addressthis issue, DDS uses a prompt that matches the input image to identify andremove undesired erroneous directions of SDS. Our key premise is that SDSshould be zero when calculated on pairs of matched prompts and images, meaningthat if the score is non-zero, its gradients can be attributed to the erroneouscomponent of SDS. Our analysis demonstrates the competence of DDS for textbased image-to-image translation. We further show that DDS can be used to trainan effective zero-shot image translation model. Experimental results indicatethat DDS outperforms existing methods in terms of stability and quality,highlighting its potential for real-world applications in text-based imageediting.", "output": "Delta Denoising Score."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Alzheimer's Disease (AD), which is the most common cause of dementia, is aprogressive disease preceded by Mild Cognitive Impairment (MCI). Earlydetection of the disease is crucial for making treatment decisions. However,most of the literature on computer-assisted detection of AD focuses onclassifying brain images into one of three major categories: healthy, MCI, andAD; or categorising MCI patients into one of (1) progressive: those whoprogress from MCI to AD at a future examination time during a given studyperiod, and (2) stable: those who stay as MCI and never progress to AD. Thismisses the opportunity to accurately identify the trajectory of progressive MCIpatients. In this paper, we revisit the brain image classification task for ADidentification and re-frame it as an ordinal classification task to predict howclose a patient is to the severe AD stage. To this end, we select progressiveMCI patients from the Alzheimer's Disease Neuroimaging Initiative (ADNI)dataset and construct an ordinal dataset with a prediction target thatindicates the time to progression to AD. We train a siamese network model topredict the time to onset of AD based on MRI brain images. We also propose aweighted variety of siamese networks and compare its performance to a baselinemodel. Our evaluations show that incorporating a weighting factor to siamesenetworks brings considerable performance gain at predicting how close inputbrain MRI images are to progressing to AD.", "output": "Weighted Siamese Network to Predict the Time to Onset of Alzheimer's Disease from MRI Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper summarizes our contributions to the document-grounded dialog tasksat the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). Inboth iterations the task consists of three subtasks: first detect whether thecurrent turn is knowledge seeking, second select a relevant knowledge document,and third generate a response grounded on the selected document. For DSTC9 weproposed different approaches to make the selection task more efficient. Thebest method, Hierarchical Selection, actually improves the results compared tothe original baseline and gives a speedup of 24x. In the DSTC10 iteration ofthe task, the challenge was to adapt systems trained on written dialogs toperform well on noisy automatic speech recognition transcripts. Therefore, weproposed data augmentation techniques to increase the robustness of the modelsas well as methods to adapt the style of generated responses to fit well intothe proceeding dialog. Additionally, we proposed a noisy channel model thatallows for increasing the factuality of the generated responses. In addition tosummarizing our previous contributions, in this work, we also report on a fewsmall improvements and reconsider the automatic evaluation metrics for thegeneration task which have shown a low correlation to human judgments.", "output": "Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainability in yield prediction helps us fully explore the potential ofmachine learning models that are already able to achieve high accuracy for avariety of yield prediction scenarios. The data included for the prediction ofyields are intricate and the models are often difficult to understand. However,understanding the models can be simplified by using natural groupings of theinput features. Grouping can be achieved, for example, by the time the featuresare captured or by the sensor used to do so. The state-of-the-art forinterpreting machine learning models is currently defined by the game-theoreticapproach of Shapley values. To handle groups of features, the calculatedShapley values are typically added together, ignoring the theoreticallimitations of this approach. We explain the concept of Shapley values directlycomputed for predefined groups of features and introduce an algorithm tocompute them efficiently on tree structures. We provide a blueprint fordesigning swarm plots that combine many local explanations for globalunderstanding. Extensive evaluation of two different yield prediction problemsshows the worth of our approach and demonstrates how we can enable a betterunderstanding of yield prediction models in the future, ultimately leading tomutual enrichment of research and application.", "output": "Grouping Shapley Value Feature Importances of Random Forests for explainable Yield Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "By formulating data samples' formation as a Markov denoising process,diffusion models achieve state-of-the-art performances in a collection oftasks. Recently, many variants of diffusion models have been proposed to enablecontrolled sample generation. Most of these existing methods either formulatethe controlling information as an input (i.e.,: conditional representation) forthe noise approximator, or introduce a pre-trained classifier in the test-phaseto guide the Langevin dynamic towards the conditional goal. However, the formerline of methods only work when the controlling information can be formulated asconditional representations, while the latter requires the pre-trained guidanceclassifier to be differentiable. In this paper, we propose a novel frameworknamed RGDM (Reward-Guided Diffusion Model) that guides the training-phase ofdiffusion models via reinforcement learning (RL). The proposed trainingframework bridges the objective of weighted log-likelihood and maximum entropyRL, which enables calculating policy gradients via samples from a pay-offdistribution proportional to exponential scaled rewards, rather than frompolicies themselves. Such a framework alleviates the high gradient variancesand enables diffusion models to explore for highly rewarded samples in thereverse process. Experiments on 3D shape and molecule generation tasks showsignificant improvements over existing conditional diffusion models.", "output": "Towards Controllable Diffusion Models via Reward-Guided Exploration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current machine learning models produce outstanding results in many areasbut, at the same time, suffer from shortcut learning and spurious correlations.To address such flaws, the explanatory interactive machine learning (XIL)framework has been proposed to revise a model by employing user feedback on amodel's explanation. This work sheds light on the explanations used within thisframework. In particular, we investigate simultaneous model revision throughmultiple explanation methods. To this end, we identified that textit{oneexplanation does not fit XIL} and propose considering multiple ones whenrevising models via XIL.", "output": "One Explanation Does Not Fit XIL."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point clouds are widely regarded as one of the best dataset types for urbanmapping purposes. Hence, point cloud datasets are commonly investigated asbenchmark types for various urban interpretation methods. Yet, few researchershave addressed the use of point cloud benchmarks for fac{c}ade segmentation.Robust fac{c}ade segmentation is becoming a key factor in various applicationsranging from simulating autonomous driving functions to preserving culturalheritage. In this work, we present a method of enriching existing point clouddatasets with fac{c}ade-related classes that have been designed to facilitatefac{c}ade segmentation testing. We propose how to efficiently extend existingdatasets and comprehensively assess their potential for fac{c}adesegmentation. We use the method to create the TUM-FAc{C}ADE dataset, whichextends the capabilities of TUM-MLS-2016. Not only can TUM-FAc{C}ADEfacilitate the development of point-cloud-based fac{c}ade segmentation tasks,but our procedure can also be applied to enrich further datasets.", "output": "TUM-FA\\c{C}ADE: Reviewing and enriching point cloud benchmarks for fa\\c{c}ade segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Speech separation remains an important area of multi-speaker signalprocessing. Deep neural network (DNN) models have attained the best performanceon many speech separation benchmarks. Some of these models can take significanttime to train and have high memory requirements. Previous work has proposedshortening training examples to address these issues but the impact of this onmodel performance is not yet well understood. In this work, the impact ofapplying these training signal length (TSL) limits is analysed for two speechseparation models: SepFormer, a transformer model, and Conv-TasNet, aconvolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysedin terms of signal length distribution and its impact on training efficiency.It is demonstrated that, for specific distributions, applying specific TSLlimits results in better performance. This is shown to be mainly due torandomly sampling the start index of the waveforms resulting in more uniqueexamples for training. A SepFormer model trained using a TSL limit of 4.42s anddynamic mixing (DM) is shown to match the best-performing SepFormer modeltrained with DM and unlimited signal lengths. Furthermore, the 4.42s TSL limitresults in a 44% reduction in training time with WHAMR.", "output": "On Data Sampling Strategies for Training Neural Network Speech Separation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cancer is a highly heterogeneous condition that can occur almost anywhere inthe human body. 18F-fluorodeoxyglucose is an imaging modality commonly used todetect cancer due to its high sensitivity and clear visualisation of thepattern of metabolic activity. Nonetheless, as cancer is highly heterogeneous,it is challenging to train general-purpose discriminative cancer detectionmodels, with data availability and disease complexity often cited as a limitingfactor. Unsupervised anomaly detection models have been suggested as a putativesolution. These models learn a healthy representation of tissue and detectcancer by predicting deviations from the healthy norm, which requires modelscapable of accurately learning long-range interactions between organs and theirimaging patterns with high levels of expressivity. Such characteristics aresuitably satisfied by transformers, which have been shown to generatestate-of-the-art results in unsupervised anomaly detection by training onnormal data. This work expands upon such approaches by introducing multi-modalconditioning of the transformer via cross-attention i.e. supplying anatomicalreference from paired CT. Using 294 whole-body PET/CT samples, we show that ouranomaly detection method is robust and capable of achieving accurate cancerlocalization results even in cases where normal training data is unavailable.In addition, we show the efficacy of this approach on out-of-sample datashowcasing the generalizability of this approach with limited training data.Lastly, we propose to combine model uncertainty with a new kernel densityestimation approach, and show that it provides clinically and statisticallysignificant improvements when compared to the classic residual-based anomalymaps. Overall, a superior performance is demonstrated against leadingstate-of-the-art alternatives, drawing attention to the potential of theseapproaches.", "output": "Cross Attention Transformers for Multi-modal Unsupervised Whole-Body PET Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the increasing penetration of renewable power sources such as wind andsolar, accurate short-term, nowcasting renewable power prediction is becomingincreasingly important. This paper investigates the multi-modal (MM) learningand end-to-end (E2E) learning for nowcasting renewable power as an intermediateto energy management systems. MM combines features from all-sky imagery andmeteorological sensor data as two modalities to predict renewable powergeneration that otherwise could not be combined effectively. The combined,predicted values are then input to a differentiable optimal power flow (OPF)formulation simulating the energy management. For the first time, MM iscombined with E2E training of the model that minimises the expected totalsystem cost. The case study tests the proposed methodology on the real sky andmeteorological data from the Netherlands. In our study, the proposed MM-E2Emodel reduced system cost by 30% compared to uni-modal baselines.", "output": "End-to-End Learning with Multiple Modalities for System-Optimised Renewables Nowcasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Subgraph-enhanced graph neural networks (SGNN) can increase the expressivepower of the standard message-passing framework. This model family representseach graph as a collection of subgraphs, generally extracted by random samplingor with hand-crafted heuristics. Our key observation is that by selecting\"meaningful\" subgraphs, besides improving the expressivity of a GNN, it is alsopossible to obtain interpretable results. For this purpose, we introduce anovel framework that jointly predicts the class of the graph and a set ofexplanatory sparse subgraphs, which can be analyzed to understand the decisionprocess of the classifier. We compare the performance of our framework againststandard subgraph extraction policies, like random node/edge deletionstrategies. The subgraphs produced by our framework allow to achieve comparableperformance in terms of accuracy, with the additional benefit of providingexplanations.", "output": "Combining Stochastic Explainers and Subgraph Neural Networks can Increase Expressivity and Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A key challenge for a reinforcement learning (RL) agent is to incorporateexternal/expert1 advice in its learning. The desired goals of an algorithm thatcan shape the learning of an RL agent with external advice include (a)maintaining policy invariance; (b) accelerating the learning of the agent; and(c) learning from arbitrary advice [3]. To address this challenge this paperformulates the problem of incorporating external advice in RL as a multi-armedbandit called shaping-bandits. The reward of each arm of shaping banditscorresponds to the return obtained by following the expert or by following adefault RL algorithm learning on the true environment reward.We show thatdirectly applying existing bandit and shaping algorithms that do not reasonabout the non-stationary nature of the underlying returns can lead to poorresults. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES(LPIES) three different shaping algorithms built on different assumptions thatreason about the long-term consequences of following the expert policy or thedefault RL algorithm. Our experiments in four different settings show thatthese proposed algorithms achieve the above-mentioned goals whereas the otheralgorithms fail to do so.", "output": "Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solar activity is one of the main drivers of variability in our solar systemand the key source of space weather phenomena that affect Earth and near Earthspace. The extensive record of high resolution extreme ultraviolet (EUV)observations from the Solar Dynamics Observatory (SDO) offers an unprecedented,very large dataset of solar images. In this work, we make use of thiscomprehensive dataset to investigate capabilities of current state-of-the-artgenerative models to accurately capture the data distribution behind theobserved solar activity states. Starting from StyleGAN-based methods, weuncover severe deficits of this model family in handling fine-scale details ofsolar images when training on high resolution samples, contrary to training onnatural face images. When switching to the diffusion based generative modelfamily, we observe strong improvements of fine-scale detail generation. For theGAN family, we are able to achieve similar improvements in fine-scalegeneration when turning to ProjectedGANs, which uses multi-scale discriminatorswith a pre-trained frozen feature extractor. We conduct ablation studies toclarify mechanisms responsible for proper fine-scale handling. Usingdistributed training on supercomputers, we are able to train generative modelsfor up to 1024x1024 resolution that produce high quality samplesindistinguishable to human experts, as suggested by the evaluation we conduct.We make all code, models and workflows used in this study publicly available aturl{", "output": "A Comparative Study on Generative Models for High Resolution Solar Observation Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "GPT-3 and several other language models (LMs) can effectively address variousnatural language processing (NLP) tasks, including machine translation and textsummarization. Recently, they have also been successfully employed in thebusiness process management (BPM) domain, e.g., for predictive processmonitoring and process extraction from text. This, however, typically requiresfine-tuning the employed LM, which, among others, necessitates large amounts ofsuitable training data. A possible solution to this problem is the use ofprompt engineering, which leverages pre-trained LMs without fine-tuning them.Recognizing this, we argue that prompt engineering can help bring thecapabilities of LMs to BPM research. We use this position paper to develop aresearch agenda for the use of prompt engineering for BPM research byidentifying the associated potentials and challenges.", "output": "Just Tell Me: Prompt Engineering in Business Process Management."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Complex networked systems in fields such as physics, biology, and socialsciences often involve interactions that extend beyond simple pairwise ones.Hypergraphs serve as powerful modeling tools for describing and analyzing theintricate behaviors of systems with multi-body interactions. Herein, weinvestigate a discrete-time nonlinear averaging dynamics with three-bodyinteractions: an underlying hypergraph, comprising triples as hyperedges,delineates the structure of these interactions, while the vertices update theirstates through a weighted, state-dependent average of neighboring pairs'states. This dynamics captures reinforcing group effects, such as peerpressure, and exhibits higher-order dynamical effects resulting from a complexinterplay between initial states, hypergraph topology, and nonlinearity of theupdate. Differently from linear averaging dynamics on graphs with two-bodyinteractions, this model does not converge to the average of the initial statesbut rather induces a shift. By assuming random initial states and by makingsome regularity and density assumptions on the hypergraph, we prove that thedynamics converges to a multiplicatively-shifted average of the initial states,with high probability. We further characterize the shift as a function of twoparameters describing the initial state and interaction strength, as well asthe convergence time as a function of the hypergraph structure.", "output": "On the convergence of nonlinear averaging dynamics with three-body interactions on hypergraphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Compact user representations (such as embeddings) form the backbone ofpersonalization services. In this work, we present a new theoretical frameworkto measure re-identification risk in such user representations. Our framework,based on hypothesis testing, formally bounds the probability that an attackermay be able to obtain the identity of a user from their representation. As anapplication, we show how our framework is general enough to model importantreal-world applications such as the Chrome's Topics API for interest-basedadvertising. We complement our theoretical bounds by showing provably goodattack algorithms for re-identification that we use to estimate there-identification risk in the Topics API. We believe this work provides arigorous and interpretable notion of re-identification risk and a framework tomeasure it that can be used to inform real-world applications.", "output": "Measuring Re-identification Risk."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vegetation structure mapping is critical for understanding the global carboncycle and monitoring nature-based approaches to climate adaptation andmitigation. Repeat measurements of these data allow for the observation ofdeforestation or degradation of existing forests, natural forest regeneration,and the implementation of sustainable agricultural practices like agroforestry.Assessments of tree canopy height and crown projected area at a high spatialresolution are also important for monitoring carbon fluxes and assessingtree-based land uses, since forest structures can be highly spatiallyheterogeneous, especially in agroforestry systems. Very high resolutionsatellite imagery (less than one meter (1m) ground sample distance) makes itpossible to extract information at the tree level while allowing monitoring ata very large scale. This paper presents the first high-resolution canopy heightmap concurrently produced for multiple sub-national jurisdictions.Specifically, we produce canopy height maps for the states of California andS~{a}o Paolo, at sub-meter resolution, a significant improvement over the tenmeter (10m) resolution of previous Sentinel / GEDI based worldwide maps ofcanopy height. The maps are generated by applying a vision transformer tofeatures extracted from a self-supervised model in Maxar imagery from 2017 to2020, and are trained against aerial lidar and GEDI observations. We evaluatethe proposed maps with set-aside validation lidar data as well as by comparingwith other remotely sensed maps and field-collected data, and find our modelproduces an average Mean Absolute Error (MAE) within set-aside validation areasof 3.0 meters.", "output": "Sub-meter resolution canopy height maps using self-supervised learning and a vision transformer trained on Aerial and GEDI Lidar."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the last few years, we have not seen any major developments inmodel-free or model-based learning methods that would make one obsoleterelative to the other. In most cases, the used technique is heavily dependenton the use case scenario or other attributes, e.g. the environment. Bothapproaches have their own advantages, for example, sample efficiency orcomputational efficiency. However, when combining the two, the advantages ofeach can be combined and hence achieve better performance. The TD-MPC frameworkis an example of this approach. On the one hand, a world model in combinationwith model predictive control is used to get a good initial estimate of thevalue function. On the other hand, a Q function is used to provide a goodlong-term estimate. Similar to algorithms like MuZero a latent staterepresentation is used, where only task-relevant information is encoded toreduce the complexity. In this paper, we propose the use of a reconstructionfunction within the TD-MPC framework, so that the agent can reconstruct theoriginal observation given the internal state representation. This allows ouragent to have a more stable learning signal during training and also improvessample efficiency. Our proposed addition of another loss term leads to improvedperformance on both state- and image-based tasks from the DeepMind-Controlsuite.", "output": "Model Predictive Control with Self-supervised Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the development of the Internet of Things (IoT), network intrusiondetection is becoming more complex and extensive. It is essential toinvestigate an intelligent, automated, and robust network intrusion detectionmethod. Graph neural networks based network intrusion detection methods havebeen proposed. However, it still needs further studies because the graphconstruction method of the existing methods does not fully adapt to thecharacteristics of the practical network intrusion datasets. To address theabove issue, this paper proposes a graph neural network algorithm based onbehavior similarity (BS-GAT) using graph attention network. First, a novelgraph construction method is developed using the behavior similarity byanalyzing the characteristics of the practical datasets. The data flows aretreated as nodes in the graph, and the behavior rules of nodes are used asedges in the graph, constructing a graph with a relatively uniform number ofneighbors for each node. Then, the edge behavior relationship weights areincorporated into the graph attention network to utilize the relationshipbetween data flows and the structure information of the graph, which is used toimprove the performance of the network intrusion detection. Finally,experiments are conducted based on the latest datasets to evaluate theperformance of the proposed behavior similarity based graph attention networkfor the network intrusion detection. The results show that the proposed methodis effective and has superior performance comparing to existing solutions.", "output": "BS-GAT Behavior Similarity Based Graph Attention Network for Network Intrusion Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article measures how sparsity can make neural networks more robust tomembership inference attacks. The obtained empirical results show that sparsityimproves the privacy of the network, while preserving comparable performanceson the task at hand. This empirical study completes and extends existingliterature.", "output": "Sparsity in neural networks can increase their privacy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Design exploration is an important step in the engineering design process.This involves the search for design/s that meet the specified design criteriaand accomplishes the predefined objective/s. In recent years, machinelearning-based approaches have been widely used in engineering design problems.This paper showcases Artificial Neural Network (ANN) architecture applied to anengineering design problem to explore and identify improved design solutions.The case problem of this study is the design of flexible disc elements used indisc couplings. We are required to improve the design of the disc elements bylowering the mass and stress without lowering the torque transmission andmisalignment capability. To accomplish this objective, we employ ANN coupledwith genetic algorithm in the design exploration step to identify designs thatmeet the specified criteria (torque and misalignment) while having minimum massand stress. The results are comparable to the optimized results obtained fromthe traditional response surface method. This can have huge advantage when weare evaluating conceptual designs against multiple conflicting requirements.", "output": "Machine Learning-Based Multi-Objective Design Exploration Of Flexible Disc Elements."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We provide a novel Neural Network architecture that can: i) output R-matrixfor a given quantum integrable spin chain, ii) search for an integrableHamiltonian and the corresponding R-matrix under assumptions of certainsymmetries or other restrictions, iii) explore the space of Hamiltonians aroundalready learned models and reconstruct the family of integrable spin chainswhich they belong to. The neural network training is done by minimizing lossfunctions encoding Yang-Baxter equation, regularity and other model-specificrestrictions such as hermiticity. Holomorphy is implemented via the choice ofactivation functions. We demonstrate the work of our Neural Network on thetwo-dimensional spin chains of difference form. In particular, we reconstructthe R-matrices for all 14 classes. We also demonstrate its utility as antextit{Explorer}, scanning a certain subspace of Hamiltonians and identifyingintegrable classes after clusterisation. The last strategy can be used infuture to carve out the map of integrable spin chains in higher dimensions andin more general settings where no analytical methods are available.", "output": "The R-mAtrIx Net."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection has seen remarkable progress in recent years with theintroduction of Convolutional Neural Networks (CNN). Object detection is amulti-task learning problem where both the position of the objects in theimages as well as their classes needs to be correctly identified. The idea hereis to maximize the overlap between the ground-truth bounding boxes and thepredictions i.e. the Intersection over Union (IoU). In the scope of work seencurrently in this domain, IoU is approximated by using the Huber loss as aproxy but this indirect method does not leverage the IoU information and treatsthe bounding box as four independent, unrelated terms of regression. This isnot true for a bounding box where the four coordinates are highly correlatedand hold a semantic meaning when taken together. The direct optimization of theIoU is not possible due to its non-convex and non-differentiable nature. Inthis paper, we have formulated a novel loss namely, the Smooth IoU, whichdirectly optimizes the IoUs for the bounding boxes. This loss has beenevaluated on the Oxford IIIT Pets, Udacity self-driving car, PASCAL VOC, andVWFS Car Damage datasets and has shown performance gains over the standardHuber loss.", "output": "Directly Optimizing IoU for Bounding Box Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models pre-trained on large self-supervised corpora, followed bytask-specific fine-tuning has become the dominant paradigm in NLP. Thesepre-training datasets often have a one-to-many structure--e.g. in dialoguethere are many valid responses for a given context. However, only some of theseresponses will be desirable in our downstream task. This raises the question ofhow we should train the model such that it can emulate the desirablebehaviours, but not the undesirable ones. Current approaches train in aone-to-one setup--only a single target response is given for a single dialoguecontext--leading to models only learning to predict the average response, whileignoring the full range of possible responses. Using text-based games as atestbed, our approach, PASA, uses discrete latent variables to capture therange of different behaviours represented in our larger pre-training dataset.We then use knowledge distillation to distil the posterior probabilitydistribution into a student model. This probability distribution is far richerthan learning from only the hard targets of the dataset, and thus allows thestudent model to benefit from the richer range of actions the teacher model haslearned. Results show up to 49% empirical improvement over the previousstate-of-the-art model on the Jericho Walkthroughs dataset.", "output": "Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The standard non-invasive imaging technique used to assess the severity andextent of Coronary Artery Disease (CAD) is Coronary Computed TomographyAngiography (CCTA). However, manual grading of each patient's CCTA according tothe CAD-Reporting and Data System (CAD-RADS) scoring is time-consuming andoperator-dependent, especially in borderline cases. This work proposes a fullyautomated, and visually explainable, deep learning pipeline to be used as adecision support system for the CAD screening procedure. The pipeline performstwo classification tasks: firstly, identifying patients who require furtherclinical investigations and secondly, classifying patients into subgroups basedon the degree of stenosis, according to commonly used CAD-RADS thresholds. Thepipeline pre-processes multiplanar projections of the coronary arteries,extracted from the original CCTAs, and classifies them using a fine-tunedMulti-Axis Vision Transformer architecture. With the aim of emulating thecurrent clinical practice, the model is trained to assign a per-patient scoreby stacking the bi-dimensional longitudinal cross-sections of the three maincoronary arteries along channel dimension. Furthermore, it generates visuallyinterpretable maps to assess the reliability of the predictions. When run on adatabase of 1873 three-channel images of 253 patients collected at the MonzinoCardiology Center in Milan, the pipeline obtained an AUC of 0.87 and 0.93 forthe two classification tasks, respectively. According to our knowledge, this isthe first model trained to assign CAD-RADS scores learning solely from patientscores and not requiring finer imaging annotation steps that are not part ofthe clinical routine.", "output": "CAD-RADS scoring of coronary CT angiography with Multi-Axis Vision Transformer: a clinically-inspired deep learning pipeline."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies reward-agnostic exploration in reinforcement learning (RL)-- a scenario where the learner is unware of the reward functions during theexploration stage -- and designs an algorithm that improves over the state ofthe art. More precisely, consider a finite-horizon non-stationary Markovdecision process with $S$ states, $A$ actions, and horizon length $H$, andsuppose that there are no more than a polynomial number of given rewardfunctions of interest. By collecting an order of begin{align*}frac{SAH^3}{varepsilon^2} text{ sample episodes (up to log factor)}end{align*} without guidance of the reward information, our algorithm is ableto find $varepsilon$-optimal policies for all these reward functions, providedthat $varepsilon$ is sufficiently small. This forms the first reward-agnosticexploration scheme in this context that achieves provable minimax optimality.Furthermore, once the sample size exceeds $frac{S^2AH^3}{varepsilon^2}$episodes (up to log factor), our algorithm is able to yield $varepsilon$accuracy for arbitrarily many reward functions (even when they areadversarially designed), a task commonly dubbed as ``reward-free exploration.''The novelty of our algorithm design draws on insights from offline RL: theexploration scheme attempts to maximize a critical reward-agnostic quantitythat dictates the performance of offline RL, while the policy learning paradigmleverages ideas from sample-optimal offline RL paradigms.", "output": "Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of synthetically generating data that can closelyresemble human decisions made in the context of an interactive human-AI systemlike a computer game. We propose a novel algorithm that can generate synthetic,human-like, decision making data while starting from a very small set ofdecision making data collected from humans. Our proposed algorithm integratesthe concept of reward shaping with an imitation learning algorithm to generatethe synthetic data. We have validated our synthetic data generation techniqueby using the synthetically generated data as a surrogate for human interactiondata to solve three sequential decision making tasks of increasing complexitywithin a small computer game-like setup. Different empirical and statisticalanalyses of our results show that the synthetically generated data cansubstitute the human data and perform the game-playing tasks almostindistinguishably, with very low divergence, from a human performing the sametasks.", "output": "Synthetically Generating Human-like Data for Sequential Decision Making Tasks via Reward-Shaped Imitation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-entropy is a widely used loss function in applications. It coincideswith the logistic loss applied to the outputs of a neural network, when thesoftmax is used. But, what guarantees can we rely on when using cross-entropyas a surrogate loss? We present a theoretical analysis of a broad family oflosses, comp-sum losses, that includes cross-entropy (or logistic loss),generalized cross-entropy, the mean absolute error and other losscross-entropy-like functions. We give the first $H$-consistency bounds forthese loss functions. These are non-asymptotic guarantees that upper bound thezero-one loss estimation error in terms of the estimation error of a surrogateloss, for the specific hypothesis set $H$ used. We further show that our boundsare tight. These bounds depend on quantities called minimizability gaps, whichonly depend on the loss function and the hypothesis set. To make them moreexplicit, we give a specific analysis of these gaps for comp-sum losses. Wealso introduce a new family of loss functions, smooth adversarial comp-sumlosses, derived from their comp-sum counterparts by adding in a related smoothterm. We show that these loss functions are beneficial in the adversarialsetting by proving that they admit $H$-consistency bounds. This leads to newadversarial robustness algorithms that consist of minimizing a regularizedsmooth adversarial comp-sum loss. While our main purpose is a theoreticalanalysis, we also present an extensive empirical analysis comparing comp-sumlosses. We further report the results of a series of experiments demonstratingthat our adversarial robustness algorithms outperform the currentstate-of-the-art, while also achieving a superior non-adversarial accuracy.", "output": "Cross-Entropy Loss Functions: Theoretical Analysis and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the geometric properties of the functions learned by trainedConvNets in the preactivation space of their convolutional layers, byperforming an empirical study of hyperplane arrangements induced by aconvolutional layer. We introduce statistics over the weights of a trainednetwork to study local arrangements and relate them to the training dynamics.We observe that trained ConvNets show a significant statistical bias towardsregular hyperplane configurations. Furthermore, we find that layers showingbiased configurations are critical to validation performance for thearchitectures considered, trained on CIFAR10, CIFAR100 and ImageNet.", "output": "Hyperplane Arrangements of Trained ConvNets Are Biased."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most signal processing and statistical applications heavily rely on specificdata distribution models. The Gaussian distributions, although being the mostcommon choice, are inadequate in most real world scenarios as they fail toaccount for data coming from heavy-tailed populations or contaminated byoutliers. Such problems call for the use of Robust Statistics. The robustmodels and estimators are usually based on elliptical populations, making thelatter ubiquitous in all methods of robust statistics. To determine whethersuch tools are applicable in any specific case, goodness-of-fit (GoF) tests areused to verify the ellipticity hypothesis. Ellipticity GoF tests are usuallyhard to analyze and often their statistical power is not particularly strong.In this work, assuming the true covariance matrix is unknown we design andrigorously analyze a robust GoF test consistent against all alternatives toellipticity on the unit sphere. The proposed test is based on Tyler's estimatorand is formulated in terms of easily computable statistics of the data. For itsrigorous analysis, we develop a novel framework based on the exchangeablerandom variables calculus introduced by de Finetti. Our findings are supportedby numerical simulations comparing them to other popular GoF tests anddemonstrating the significantly higher statistical power of the suggestedtechnique.", "output": "A Robust Test for Elliptical Symmetry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion-based generative models have demonstrated a capacity forperceptually impressive synthesis, but can they also be great likelihood-basedmodels? We answer this in the affirmative, and introduce a family ofdiffusion-based generative models that obtain state-of-the-art likelihoods onstandard image density estimation benchmarks. Unlike other diffusion-basedmodels, our method allows for efficient optimization of the noise schedulejointly with the rest of the model. We show that the variational lower bound(VLB) simplifies to a remarkably short expression in terms of thesignal-to-noise ratio of the diffused data, thereby improving our theoreticalunderstanding of this model class. Using this insight, we prove an equivalencebetween several models proposed in the literature. In addition, we show thatthe continuous-time VLB is invariant to the noise schedule, except for thesignal-to-noise ratio at its endpoints. This enables us to learn a noiseschedule that minimizes the variance of the resulting VLB estimator, leading tofaster optimization. Combining these advances with architectural improvements,we obtain state-of-the-art likelihoods on image density estimation benchmarks,outperforming autoregressive models that have dominated these benchmarks formany years, with often significantly faster optimization. In addition, we showhow to use the model as part of a bits-back compression scheme, and demonstratelossless compression rates close to the theoretical optimum. Code is availableat  .", "output": "Variational Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we investigate a two-layer fully connected neural network ofthe form $f(X)=frac{1}{sqrt{d_1}}boldsymbol{a}^top sigmaleft(WXright)$,where $Xinmathbb{R}^{d_0times n}$ is a deterministic data matrix,$Winmathbb{R}^{d_1times d_0}$ and $boldsymbol{a}inmathbb{R}^{d_1}$ arerandom Gaussian weights, and $sigma$ is a nonlinear activation function. Westudy the limiting spectral distributions of two empirical kernel matricesassociated with $f(X)$: the empirical conjugate kernel (CK) and neural tangentkernel (NTK), beyond the linear-width regime ($d_1asymp n$). We focus on the$textit{ultra-wide regime}$, where the width $d_1$ of the first layer is muchlarger than the sample size $n$. Under appropriate assumptions on $X$ and$sigma$, a deformed semicircle law emerges as $d_1/ntoinfty$ and$ntoinfty$. We first prove this limiting law for generalized samplecovariance matrices with some dependency. To specify it for our neural networkmodel, we provide a nonlinear Hanson-Wright inequality that is suitable forneural networks with random weights and Lipschitz activation functions. We alsodemonstrate non-asymptotic concentrations of the empirical CK and NTK aroundtheir limiting kernels in the spectral norm, along with lower bounds on theirsmallest eigenvalues. As an application, we show that random feature regressioninduced by the empirical kernel achieves the same asymptotic performance as itslimiting kernel regression under the ultra-wide regime. This allows us tocalculate the asymptotic training and test errors for random feature regressionusing the corresponding kernel regression.", "output": "Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The increasing complexity of today's software requires the contribution ofthousands of developers. This complex collaboration structure makes developersmore likely to introduce defect-prone changes that lead to software faults.Determining when these defect-prone changes are introduced has provenchallenging, and using traditional machine learning (ML) methods to make thesedeterminations seems to have reached a plateau. In this work, we buildcontribution graphs consisting of developers and source files to capture thenuanced complexity of changes required to build software. By leveraging thesecontribution graphs, our research shows the potential of using graph-based MLto improve Just-In-Time (JIT) defect prediction. We hypothesize that featuresextracted from the contribution graphs may be better predictors of defect-pronechanges than intrinsic features derived from software characteristics. Wecorroborate our hypothesis using graph-based ML for classifying edges thatrepresent defect-prone changes. This new framing of the JIT defect predictionproblem leads to remarkably better results. We test our approach on 14open-source projects and show that our best model can predict whether or not acode change will lead to a defect with an F1 score as high as 77.55% and aMatthews correlation coefficient (MCC) as high as 53.16%. This represents a152% higher F1 score and a 3% higher MCC over the state-of-the-art JIT defectprediction. We describe limitations, open challenges, and how this method canbe used for operational JIT defect prediction.", "output": "Graph-Based Machine Learning Improves Just-in-Time Defect Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art deep neural networks (DNNs) have been proven to bevulnerable to adversarial manipulation and backdoor attacks. Backdoored modelsdeviate from expected behavior on inputs with predefined triggers whileretaining performance on clean data. Recent works focus on software simulationof backdoor injection during the inference phase by modifying network weights,which we find often unrealistic in practice due to restrictions in hardware.In contrast, in this work for the first time, we present an end-to-endbackdoor injection attack realized on actual hardware on a classifier modelusing Rowhammer as the fault injection method. To this end, we firstinvestigate the viability of backdoor injection attacks in real-lifedeployments of DNNs on hardware and address such practical issues in hardwareimplementation from a novel optimization perspective. We are motivated by thefact that vulnerable memory locations are very rare, device-specific, andsparsely distributed. Consequently, we propose a novel network trainingalgorithm based on constrained optimization to achieve a realistic backdoorinjection attack in hardware. By modifying parameters uniformly across theconvolutional and fully-connected layers as well as optimizing the triggerpattern together, we achieve state-of-the-art attack performance with fewer bitflips. For instance, our method on a hardware-deployed ResNet-20 model trainedon CIFAR-10 achieves over 89% test accuracy and 92% attack success rate byflipping only 10 out of 2.2 million bits.", "output": "Don't Knock! Rowhammer at the Backdoor of DNN Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Considering a probability distribution over parameters is known as anefficient strategy to learn a neural network with non-differentiable activationfunctions. We study the expectation of a probabilistic neural network as apredictor by itself, focusing on the aggregation of binary activated neuralnetworks with normal distributions over real-valued weights. Our work leveragesa recent analysis derived from the PAC-Bayesian framework that derives tightgeneralization bounds and learning procedures for the expected output value ofsuch an aggregation, which is given by an analytical expression. While thecombinatorial nature of the latter has been circumvented by approximations inprevious works, we show that the exact computation remains tractable for deepbut narrow neural networks, thanks to a dynamic programming approach. Thisleads us to a peculiar bound minimization learning algorithm for binaryactivated neural networks, where the forward pass propagates probabilities overrepresentations instead of activation values. A stochastic counterpart thatscales to wide architectures is proposed.", "output": "PAC-Bayesian Learning of Aggregated Binary Activated Neural Networks with Probabilities over Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose physics-informed neural operators (PINO) that usesavailable data and/or physics constraints to learn the solution operator of afamily of parametric Partial Differential Equation (PDE). This hybrid approachallows PINO to overcome the limitations of purely data-driven and physics-basedmethods. For instance, data-driven methods fail to learn when data is oflimited quantity and/or quality, and physics-based approaches fail to optimizeon challenging PDE constraints. By combining both data and PDE constraints,PINO overcomes all these challenges. Additionally, a unique property that PINOenjoys over other hybrid learning methods is its ability to incorporate dataand PDE constraints at different resolutions. This allows us to combinecoarse-resolution data, which is inexpensive to obtain from numerical solvers,with higher resolution PDE constraints, and the resulting PINO has nodegradation in accuracy even on high-resolution test instances. Thisdiscretization-invariance property in PINO is due to neural-operator frameworkwhich learns mappings between function spaces and allows evaluation atdifferent resolutions without the need for re-training. Moreover, PINO succeedsin the purely physics setting, where no data is available, while otherapproaches such as the Physics-Informed Neural Network (PINN) fail due tooptimization challenges, e.g. in multi-scale dynamic systems such as Kolmogorovflows. This is because PINO learns the solution operator by optimizing PDEconstraints on multiple instances while PINN optimizes PDE constraints of asingle PDE instance. Further, in PINO, we incorporate the Fourier neuraloperator (FNO) architecture which achieves orders-of-magnitude speedup overnumerical solvers and also allows us to compute explicit gradients on functionspaces efficiently.", "output": "Physics-Informed Neural Operator for Learning Partial Differential Equations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A variety of theoretically-sound policy gradient algorithms exist for theon-policy setting due to the policy gradient theorem, which provides asimplified form for the gradient. The off-policy setting, however, has beenless clear due to the existence of multiple objectives and the lack of anexplicit off-policy policy gradient theorem. In this work, we unify theseobjectives into one off-policy objective, and provide a policy gradient theoremfor this unified objective. The derivation involves emphatic weightings andinterest functions. We show multiple strategies to approximate the gradients,in an algorithm called Actor Critic with Emphatic weightings (ACE). We prove ina counterexample that previous (semi-gradient) off-policy actor-criticmethods--particularly Off-Policy Actor-Critic (OffPAC) and Deterministic PolicyGradient (DPG)--converge to the wrong solution whereas ACE finds the optimalsolution. We also highlight why these semi-gradient approaches can stillperform well in practice, suggesting strategies for variance reduction in ACE.We empirically study several variants of ACE on two classic controlenvironments and an image-based environment designed to illustrate thetradeoffs made by each gradient approximation. We find that by approximatingthe emphatic weightings directly, ACE performs as well as or better than OffPACin all settings tested.", "output": "Off-Policy Actor-Critic with Emphatic Weightings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces two new ensemble-based methods to reduce the data andcomputation costs of image classification. They can be used with any set ofclassifiers and do not require additional training. In the first approach, datausage is reduced by only analyzing a full-sized image if the model has lowconfidence in classifying a low-resolution pixelated version. When applied onthe best performing classifiers considered here, data usage is reduced by 61.2%on MNIST, 69.6% on KMNIST, 56.3% on FashionMNIST, 84.6% on SVHN, 40.6% onImageNet, and 27.6% on ImageNet-V2, all with a less than 5% reduction inaccuracy. However, for CIFAR-10, the pixelated data are not particularlyinformative, and the ensemble approach increases data usage while reducingaccuracy. In the second approach, compute costs are reduced by only using acomplex model if a simpler model has low confidence in its classification.Computation cost is reduced by 82.1% on MNIST, 47.6% on KMNIST, 72.3% onFashionMNIST, 86.9% on SVHN, 89.2% on ImageNet, and 81.5% on ImageNet-V2, allwith a less than 5% reduction in accuracy; for CIFAR-10 the correspondingimprovements are smaller at 13.5%. When cost is not an object, choosing theprojection from the most confident model for each observation increasesvalidation accuracy to 81.0% from 79.3% for ImageNet and to 69.4% from 67.5%for ImageNet-V2.", "output": "Problem-dependent attention and effort in neural networks with applications to image resolution and model selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, deep learning-based algorithms are widely adopted due to theadvantage of being able to establish anomaly detection models without or withminimal domain knowledge of the task. Instead, to train the artificial neuralnetwork more stable, it should be better to define the appropriate neuralnetwork structure or the loss function. For the training anomaly detectionmodel, the mean squared error (MSE) function is adopted widely. On the otherhand, the novel loss function, logarithmic mean squared error (LMSE), isproposed in this paper to train the neural network more stable. This studycovers a variety of comparisons from mathematical comparisons, visualization inthe differential domain for backpropagation, loss convergence in the trainingprocess, and anomaly detection performance. In an overall view, LMSE issuperior to the existing MSE function in terms of strongness of lossconvergence, anomaly detection performance. The LMSE function is expected to beapplicable for training not only the anomaly detection model but also thegeneral generative neural network.", "output": "Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce the notion of a reproducible algorithm in the context oflearning. A reproducible learning algorithm is resilient to variations in itssamples -- with high probability, it returns the exact same output when run ontwo samples from the same underlying distribution. We begin by unpacking thedefinition, clarifying how randomness is instrumental in balancing accuracy andreproducibility. We initiate a theory of reproducible algorithms, showing howreproducibility implies desirable properties such as data reuse and efficienttestability. Despite the exceedingly strong demand of reproducibility, thereare efficient reproducible algorithms for several fundamental problems instatistics and learning. First, we show that any statistical query algorithmcan be made reproducible with a modest increase in sample complexity, and weuse this to construct reproducible algorithms for finding approximateheavy-hitters and medians. Using these ideas, we give the first reproduciblealgorithm for learning halfspaces via a reproducible weak learner and areproducible boosting algorithm. Finally, we initiate the study of lower boundsand inherent tradeoffs for reproducible algorithms, giving nearly tight samplecomplexity upper and lower bounds for reproducible versus nonreproducible SQalgorithms.", "output": "Reproducibility in Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model Predictive Control (MPC) has become a popular framework in embeddedcontrol for high-performance autonomous systems. However, to achieve goodcontrol performance using MPC, an accurate dynamics model is key. To maintainreal-time operation, the dynamics models used on embedded systems have beenlimited to simple first-principle models, which substantially limits theirrepresentative power. In contrast to such simple models, machine learningapproaches, specifically neural networks, have been shown to accurately modeleven complex dynamic effects, but their large computational complexity hinderedcombination with fast real-time iteration loops. With this work, we presentReal-time Neural MPC, a framework to efficiently integrate large, complexneural network architectures as dynamics models within a model-predictivecontrol pipeline. Our experiments, performed in simulation and the real worldonboard a highly agile quadrotor platform, demonstrate the capabilities of thedescribed system to run learned models with, previously infeasible, largemodeling capacity using gradient-based online optimization MPC. Compared toprior implementations of neural networks in online optimization MPC we canleverage models of over 4000 times larger parametric capacity in a 50Hzreal-time window on an embedded platform. Further, we show the feasibility ofour framework on real-world problems by reducing the positional tracking errorby up to 82% when compared to state-of-the-art MPC approaches without neuralnetwork dynamics.", "output": "Real-time Neural-MPC: Deep Learning Model Predictive Control for Quadrotors and Agile Robotic Platforms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Background: Many published machine learning studies are irreproducible.Issues with methodology and not properly accounting for variation introduced bythe algorithm themselves or their implementations are attributed as the maincontributors to the irreproducibility.Problem: There exist no theoreticalframework that relates experiment design choices to potential effects on theconclusions. Without such a framework, it is much harder for practitioners andresearchers to evaluate experiment results and describe the limitations ofexperiments. The lack of such a framework also makes it harder for independentresearchers to systematically attribute the causes of failed reproducibilityexperiments. Objective: The objective of this paper is to develop a frameworkthat enable applied data science practitioners and researchers to understandwhich experiment design choices can lead to false findings and how and by thishelp in analyzing the conclusions of reproducibility experiments. Method: Wehave compiled an extensive list of factors reported in the literature that canlead to machine learning studies being irreproducible. These factors areorganized and categorized in a reproducibility framework motivated by thestages of the scientific method. The factors are analyzed for how they canaffect the conclusions drawn from experiments. A model comparison study is usedas an example. Conclusion: We provide a framework that describes machinelearning methodology from experimental design decisions to the conclusionsinferred from them.", "output": "Sources of Irreproducibility in Machine Learning: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop an algorithm for parameter-free stochastic convex optimization(SCO) whose rate of convergence is only a double-logarithmic factor larger thanthe optimal rate for the corresponding known-parameter setting. In contrast,the best previously known rates for parameter-free SCO are based on onlineparameter-free regret bounds, which contain unavoidable excess logarithmicterms compared to their known-parameter counterparts. Our algorithm isconceptually simple, has high-probability guarantees, and is also partiallyadaptive to unknown gradient norms, smoothness, and strong convexity. At theheart of our results is a novel parameter-free certificate for SGD step sizechoice, and a time-uniform concentration result that assumes no a-priori boundson SGD iterates.", "output": "Making SGD Parameter-Free."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Is overparameterization a privacy liability? In this work, we study theeffect that the number of parameters has on a classifier's vulnerability tomembership inference attacks. We first demonstrate how the number of parametersof a model can induce a privacy--utility trade-off: increasing the number ofparameters generally improves generalization performance at the expense oflower privacy. However, remarkably, we then show that if coupled with properregularization, increasing the number of parameters of a model can actuallysimultaneously increase both its privacy and performance, thereby eliminatingthe privacy--utility trade-off. Theoretically, we demonstrate this curiousphenomenon for logistic regression with ridge regularization in a bi-levelfeature ensemble setting. Pursuant to our theoretical exploration, we develop anovel leave-one-out analysis tool to precisely characterize the vulnerabilityof a linear classifier to the optimal membership inference attack. Weempirically exhibit this \"blessing of dimensionality\" for neural networks on avariety of tasks using early stopping as the regularizer.", "output": "A Blessing of Dimensionality in Membership Inference through Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop two compression based stochastic gradient algorithms to solve aclass of non-smooth strongly convex-strongly concave saddle-point problems in adecentralized setting (without a central server). Our first algorithm is aRestart-based Decentralized Proximal Stochastic Gradient method withCompression (C-RDPSG) for general stochastic settings. We provide rigoroustheoretical guarantees of C-RDPSG with gradient computation complexity andcommunication complexity of order $mathcal{O}( (1+delta)^4frac{1}{L^2}{kappa_f^2}kappa_g^2 frac{1}{epsilon} )$, to achieve an$epsilon$-accurate saddle-point solution, where $delta$ denotes thecompression factor, $kappa_f$ and $kappa_g$ denote respectively the conditionnumbers of objective function and communication graph, and $L$ denotes thesmoothness parameter of the smooth part of the objective function. Next, wepresent a Decentralized Proximal Stochastic Variance Reduced Gradient algorithmwith Compression (C-DPSVRG) for finite sum setting which exhibits gradientcomputation complexity and communication complexity of order $mathcal{O}left((1+delta) max {kappa_f^2, sqrt{delta}kappa^2_fkappa_g,kappa_g }logleft(frac{1}{epsilon}right) right)$. Extensive numerical experimentsshow competitive performance of the proposed algorithms and provide support tothe theoretical results obtained.", "output": "Stochastic Gradient Methods with Compressed Communication for Decentralized Saddle Point Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven, learning-based control methods offer the potential to improveoperations in complex systems, and model-free deep reinforcement learningrepresents a popular approach to data-driven control. However, existing classesof algorithms present a trade-off between two important deployment requirementsfor real-world control: (i) practical performance guarantees and (ii) dataefficiency. Off-policy algorithms make efficient use of data through samplereuse but lack theoretical guarantees, while on-policy algorithms guaranteeapproximate policy improvement throughout training but suffer from high samplecomplexity. In order to balance these competing goals, we develop a class ofGeneralized Policy Improvement algorithms that combines the policy improvementguarantees of on-policy methods with the efficiency of sample reuse. Wedemonstrate the benefits of this new class of algorithms through extensiveexperimental analysis on a variety of continuous control tasks from theDeepMind Control Suite.", "output": "Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In deterministic optimization, it is typically assumed that all problemparameters are fixed and known. In practice, however, some parameters may be apriori unknown but can be estimated from historical data. A typicalpredict-then-optimize approach separates predictions and optimization into twostages. Recently, end-to-end predict-then-optimize has become an attractivealternative. In this work, we present the PyEPO package, a PyTorchbasedend-to-end predict-then-optimize library in Python. To the best of ourknowledge, PyEPO (pronounced like pineapple with a silent \"n\") is the firstsuch generic tool for linear and integer programming with predicted objectivefunction coefficients. It provides four base algorithms: a convex surrogateloss function from the seminal work of Elmachtoub and Grigas [16], adifferentiable black-box solver approach of Pogancic et al. [35], and twodifferentiable perturbation-based methods from Berthet et al. [6]. PyEPOprovides a simple interface for the definition of new optimization problems,the implementation of state-of-the-art predict-then-optimize trainingalgorithms, the use of custom neural network architectures, and the comparisonof end-to-end approaches with the two-stage approach. PyEPO enables us toconduct a comprehensive set of experiments comparing a number of end-to-end andtwo-stage approaches along axes such as prediction accuracy, decision quality,and running time on problems such as Shortest Path, Multiple Knapsack, and theTraveling Salesperson Problem. We discuss some empirical insights from theseexperiments, which could guide future research. PyEPO and its documentation areavailable at ", "output": "PyEPO: A PyTorch-based End-to-End Predict-then-Optimize Library for Linear and Integer Programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A growing body of work studies Blindspot Discovery Methods (\"BDM\"s): methodsthat use an image embedding to find semantically meaningful (i.e., united by ahuman-understandable concept) subsets of the data where an image classifierperforms significantly worse. Motivated by observed gaps in prior work, weintroduce a new framework for evaluating BDMs, SpotCheck, that uses syntheticimage datasets to train models with known blindspots and a new BDM, PlaneSpot,that uses a 2D image representation. We use SpotCheck to run controlledexperiments that identify factors that influence BDM performance (e.g., thenumber of blindspots in a model, or features used to define the blindspot) andshow that PlaneSpot is competitive with and in many cases outperforms existingBDMs. Importantly, we validate these findings by designing additionalexperiments that use real image data from MS-COCO, a large image benchmarkdataset. Our findings suggest several promising directions for future work onBDM design and evaluation. Overall, we hope that the methodology and analysespresented in this work will help facilitate a more rigorous science ofblindspot discovery.", "output": "Towards a More Rigorous Science of Blindspot Discovery in Image Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work studies the threats of adversarial attack on multivariateprobabilistic forecasting models and viable defense mechanisms. Our studiesdiscover a new attack pattern that negatively impact the forecasting of atarget time series via making strategic, sparse (imperceptible) modificationsto the past observations of a small number of other time series. To mitigatethe impact of such attack, we have developed two defense strategies. First, weextend a previously developed randomized smoothing technique in classificationto multivariate forecasting scenarios. Second, we develop an adversarialtraining algorithm that learns to create adversarial examples and at the sametime optimizes the forecasting model to improve its robustness against suchadversarial simulation. Extensive experiments on real-world datasets confirmthat our attack schemes are powerful and our defense algorithms are moreeffective compared with baseline defense mechanisms.", "output": "Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Normalizing Flows (NF) are powerful likelihood-based generative models thatare able to trade off between expressivity and tractability to model complexdensities. A now well established research avenue leverages optimal transport(OT) and looks for Monge maps, i.e. models with minimal effort between thesource and target distributions. This paper introduces a method based onBrenier's polar factorization theorem to transform any trained NF into a moreOT-efficient version without changing the final density. We do so by learning arearrangement of the source (Gaussian) distribution that minimizes the OT costbetween the source and the final density. We further constrain the path leadingto the estimated Monge map to lie on a geodesic in the space ofvolume-preserving diffeomorphisms thanks to Euler's equations. The proposedmethod leads to smooth flows with reduced OT cost for several existing modelswithout affecting the model performance.", "output": "Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Regularized optimal transport (OT) is now increasingly used as a loss or as amatching layer in neural networks. Entropy-regularized OT can be computed usingthe Sinkhorn algorithm but it leads to fully-dense transportation plans,meaning that all sources are (fractionally) matched with all targets. Toaddress this issue, several works have investigated quadratic regularizationinstead. This regularization preserves sparsity and leads to unconstrained andsmooth (semi) dual objectives, that can be solved with off-the-shelf gradientmethods. Unfortunately, quadratic regularization does not give direct controlover the cardinality (number of nonzeros) of the transportation plan. Wepropose in this paper a new approach for OT with explicit cardinalityconstraints on the transportation plan. Our work is motivated by an applicationto sparse mixture of experts, where OT can be used to match input tokens suchas image patches with expert models such as neural networks. Cardinalityconstraints ensure that at most $k$ tokens are matched with an expert, which iscrucial for computational performance reasons. Despite the nonconvexity ofcardinality constraints, we show that the corresponding (semi) dual problemsare tractable and can be solved with first-order gradient methods. Our methodcan be thought as a middle ground between unregularized OT (recovered in thelimit case $k=1$) and quadratically-regularized OT (recovered when $k$ is largeenough). The smoothness of the objectives increases as $k$ increases, givingrise to a trade-off between convergence speed and sparsity of the optimal plan.", "output": "Sparsity-Constrained Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal graph is an abstraction for modeling dynamic systems that consist ofevolving interaction elements. In this paper, we aim to solve an important yetneglected problem -- how to learn information from high-order neighbors intemporal graphs? -- to enhance the informativeness and discriminativeness forthe learned node representations. We argue that when learning high-orderinformation from temporal graphs, we encounter two challenges, i.e.,computational inefficiency and over-smoothing, that cannot be solved byconventional techniques applied on static graphs. To remedy these deficiencies,we propose a temporal propagation-based graph neural network, namely TPGNN. Tobe specific, the model consists of two distinct components, i.e., propagatorand node-wise encoder. The propagator is leveraged to propagate messages fromthe anchor node to its temporal neighbors within $k$-hop, and thensimultaneously update the state of neighborhoods, which enables efficientcomputation, especially for a deep model. In addition, to preventover-smoothing, the model compels the messages from $n$-hop neighbors to updatethe $n$-hop memory vector preserved on the anchor. The node-wise encoder adoptstransformer architecture to learn node representations by explicitly learningthe importance of memory vectors preserved on the node itself, that is,implicitly modeling the importance of messages from neighbors at differentlayers, thus mitigating the over-smoothing. Since the encoding process will notquery temporal neighbors, we can dramatically save time consumption ininference. Extensive experiments on temporal link prediction and nodeclassification demonstrate the superiority of TPGNN over state-of-the-artbaselines in efficiency and robustness.", "output": "TPGNN: Learning High-order Information in Dynamic Graphs via Temporal Propagation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic graph neural network (DGNN) is becoming increasingly popular becauseof its widespread use in capturing dynamic features in the real world. Avariety of dynamic graph neural networks designed from algorithmic perspectiveshave succeeded in incorporating temporal information into graph processing.Despite the promising algorithmic performance, deploying DGNNs on hardwarepresents additional challenges due to the model complexity, diversity, and thenature of the time dependency. Meanwhile, the differences between DGNNs andstatic graph neural networks make hardware-related optimizations for staticgraph neural networks unsuitable for DGNNs. In this paper, we select eightprevailing DGNNs with different characteristics and profile them on both CPUand GPU. The profiling results are summarized and analyzed, providing in-depthinsights into the bottlenecks of DGNNs on hardware and identifying potentialoptimization opportunities for future DGNN acceleration. Followed by acomprehensive survey, we provide a detailed analysis of DGNN performancebottlenecks on hardware, including temporal data dependency, workloadimbalance, data movement, and GPU warm-up. We suggest several optimizationsfrom both software and hardware perspectives. This paper is the first toprovide an in-depth analysis of the hardware performance of DGNN Code isavailable at ", "output": "Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weather and climate simulations produce petabytes of high-resolution datathat are later analyzed by researchers in order to understand climate change orsevere weather. We propose a new method of compressing this multidimensionalweather and climate data: a coordinate-based neural network is trained tooverfit the data, and the resulting parameters are taken as a compactrepresentation of the original grid-based data. While compression ratios rangefrom 300x to more than 3,000x, our method outperforms the state-of-the-artcompressor SZ3 in terms of weighted RMSE, MAE. It can faithfully preserveimportant large scale atmosphere structures and does not introduce artifacts.When using the resulting neural network as a 790x compressed dataloader totrain the WeatherBench forecasting model, its RMSE increases by less than 2%.The three orders of magnitude compression democratizes access tohigh-resolution climate data and enables numerous new research directions.", "output": "Compressing multidimensional weather and climate data into neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncertainty quantification is a central challenge in reliable and trustworthymachine learning. Naive measures such as last-layer scores are well-known toyield overconfident estimates in the context of overparametrized neuralnetworks. Several methods, ranging from temperature scaling to differentBayesian treatments of neural networks, have been proposed to mitigateoverconfidence, most often supported by the numerical observation that theyyield better calibrated uncertainty measures. In this work, we provide a sharpcomparison between popular uncertainty measures for binary classification in amathematically tractable model for overparametrized neural networks: the randomfeatures model. We discuss a trade-off between classification accuracy andcalibration, unveiling a double descent like behavior in the calibration curveof optimally regularized estimators as a function of overparametrization. Thisis in contrast with the empirical Bayes method, which we show to be wellcalibrated in our setting despite the higher generalization error andoverparametrization.", "output": "A study of uncertainty quantification in overparametrized high-dimensional models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Performance monitoring of machine learning (ML)-based risk prediction modelsin healthcare is complicated by the issue of confounding medical interventions(CMI): when an algorithm predicts a patient to be at high risk for an adverseevent, clinicians are more likely to administer prophylactic treatment andalter the very target that the algorithm aims to predict. A simple approach isto ignore CMI and monitor only the untreated patients, whose outcomes remainunaltered. In general, ignoring CMI may inflate Type I error because (i)untreated patients disproportionally represent those with low predicted riskand (ii) evolution in both the model and clinician trust in the model caninduce complex dependencies that violate standard assumptions. Nevertheless, weshow that valid inference is still possible if one monitors conditionalperformance and if either conditional exchangeability or time-constantselection bias hold. Specifically, we develop a new score-based cumulative sum(CUSUM) monitoring procedure with dynamic control limits. Through simulations,we demonstrate the benefits of combining model updating with monitoring andinvestigate how over-trust in a prediction model may delay detection ofperformance deterioration. Finally, we illustrate how these monitoring methodscan be used to detect calibration decay of an ML-based risk calculator forpostoperative nausea and vomiting during the COVID-19 pandemic.", "output": "Monitoring machine learning (ML)-based risk prediction algorithms in the presence of confounding medical interventions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Algorithmic fairness plays an increasingly critical role in machine learningresearch. Several group fairness notions and algorithms have been proposed.However, the fairness guarantee of existing fair classification methods mainlydepends on specific data distributional assumptions, often requiring largesample sizes, and fairness could be violated when there is a modest number ofsamples, which is often the case in practice. In this paper, we propose FaiREE,a fair classification algorithm that can satisfy group fairness constraintswith finite-sample and distribution-free theoretical guarantees. FaiREE can beadapted to satisfy various group fairness notions (e.g., Equality ofOpportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimalaccuracy. These theoretical guarantees are further supported by experiments onboth synthetic and real data. FaiREE is shown to have favorable performanceover state-of-the-art algorithms.", "output": "FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In off-policy reinforcement learning, a behaviour policy performs exploratoryinteractions with the environment to obtain state-action-reward samples whichare then used to learn a target policy that optimises the expected return. Thisleads to a problem of off-policy evaluation, where one needs to evaluate thetarget policy from samples collected by the often unrelated behaviour policy.Importance sampling is a traditional statistical technique that is oftenapplied to off-policy evaluation. While importance sampling estimators areunbiased, their variance increases exponentially with the horizon of thedecision process due to computing the importance weight as a product of actionprobability ratios, yielding estimates with low accuracy for domains involvinglong-term planning. This paper proposes state-based importance sampling (SIS),which drops the action probability ratios of sub-trajectories with \"negligiblestates\" -- roughly speaking, those for which the chosen actions have no impacton the return estimate -- from the computation of the importance weight.Theoretical results demonstrate a smaller exponent for the variance upper boundas well as a lower mean squared error. To identify negligible states, twosearch algorithms are proposed, one based on covariance testing and one basedon state-action values. Using the formulation of SIS, we then analogouslyformulate state-based variants of weighted importance sampling, per-decisionimportance sampling, and incremental importance sampling based on thestate-action value identification algorithm. Moreover, we note that doublyrobust estimators may also benefit from SIS. Experiments in two gridworlddomains and one inventory management domain show that state-based methods yieldreduced variance and improved accuracy.", "output": "Low Variance Off-policy Evaluation with State-based Importance Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current work in named entity recognition (NER) uses either cross entropy (CE)or conditional random fields (CRF) as the objective/loss functions to optimizethe underlying NER model. Both of these traditional objective functions for theNER problem generally produce adequate performance when the data distributionis balanced and there are sufficient annotated training examples. But since NERis inherently an imbalanced tagging problem, the model performance under thelow-resource settings could suffer using these standard objective functions.Based on recent advances in area under the ROC curve (AUC) maximization, wepropose to optimize the NER model by maximizing the AUC score. We give evidencethat by simply combining two binary-classifiers that maximize the AUC score,significant performance improvement over traditional loss functions is achievedunder low-resource NER settings. We also conduct extensive experiments todemonstrate the advantages of our method under the low-resource andhighly-imbalanced data distribution settings. To the best of our knowledge,this is the first work that brings AUC maximization to the NER setting.Furthermore, we show that our method is agnostic to different types of NERembeddings, models and domains. The code to replicate this work will beprovided upon request.", "output": "AUC Maximization for Low-Resource Named Entity Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evolution of wireless communications into 6G and beyond is expected torely on new machine learning (ML)-based capabilities. These can enableproactive decisions and actions from wireless-network components to sustainquality-of-service (QoS) and user experience. Moreover, new use cases in thearea of vehicular and industrial communications will emerge. Specifically inthe area of vehicle communication, vehicle-to-everything (V2X) schemes willbenefit strongly from such advances. With this in mind, we have conducted adetailed measurement campaign that paves the way to a plethora of diverseML-based studies. The resulting datasets offer GPS-located wirelessmeasurements across diverse urban environments for both cellular (with twodifferent operators) and sidelink radio access technologies, thus enabling avariety of different studies towards V2X. The datasets are labeled and sampledwith a high time resolution. Furthermore, we make the data publicly availablewith all the necessary information to support the onboarding of newresearchers. We provide an initial analysis of the data showing some of thechallenges that ML needs to overcome and the features that ML can leverage, aswell as some hints at potential research studies.", "output": "Berlin V2X: A Machine Learning Dataset from Multiple Vehicles and Radio Access Technologies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Influenced mixed moving average fields are a versatile modeling class forspatio-temporal data. However, their predictive distribution is not generallyaccessible. Under this modeling assumption, we define a novel theory-guidedmachine learning approach that employs a generalized Bayesian algorithm to makepredictions. We employ a Lipschitz predictor, for example, a linear model or afeed-forward neural network, and determine a randomized estimator by minimizinga novel PAC Bayesian bound for data serially correlated along a spatial andtemporal dimension. Performing causal future predictions is a highlight of ourmethodology as its potential application to data with short and long-rangedependence. We conclude by showing the performance of the learning methodologyin an example with linear predictors and simulated spatio-temporal data from anSTOU process.", "output": "Mixed moving average field guided learning for spatio-temporal data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose to integrate weapon system features (such as weapon systemmanufacturer, deployment time and location, storage time and location, etc.)into a parameterized Cox-Weibull [1] reliability model via a neural network,like DeepSurv [2], to improve predictive maintenance. In parallel, we developan alternative Bayesian model by parameterizing the Weibull parameters with aneural network and employing dropout methods such as Monte-Carlo (MC)-dropoutfor comparative purposes. Due to data collection procedures in weapon systemtesting we employ a novel interval-censored log-likelihood which incorporatesMonte-Carlo Markov Chain (MCMC) [3] sampling of the Weibull parameters duringgradient descent optimization. We compare classification metrics such asreceiver operator curve (ROC) area under the curve (AUC), precision-recall (PR)AUC, and F scores to show our model generally outperforms traditional powerfulmodels such as XGBoost and the current standard conditional Weibull probabilitydensity estimation model.", "output": "Bayesian Weapon System Reliability Modeling with Cox-Weibull Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), afully-attentional model for general Satellite Image Time Series (SITS)processing based on the Vision Transformer (ViT). TSViT splits a SITS recordinto non-overlapping patches in space and time which are tokenized andsubsequently processed by a factorized temporo-spatial encoder. We argue, thatin contrast to natural images, a temporal-then-spatial factorization is moreintuitive for SITS processing and present experimental evidence for this claim.Additionally, we enhance the model's discriminative power by introducing twonovel mechanisms for acquisition-time-specific temporal positional encodingsand multiple learnable class tokens. The effect of all novel design choices isevaluated through an extensive ablation study. Our proposed architectureachieves state-of-the-art performance, surpassing previous approaches by asignificant margin in three publicly available SITS semantic segmentation andclassification datasets. All model, training and evaluation codes are madepublicly available to facilitate further research.", "output": "ViTs for SITS: Vision Transformers for Satellite Image Time Series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study automated intrusion response and formulate the interaction betweenan attacker and a defender as an optimal stopping game where attack and defensestrategies evolve through reinforcement learning and self-play. Thegame-theoretic modeling enables us to find defender strategies that areeffective against a dynamic attacker, i.e. an attacker that adapts its strategyin response to the defender strategy. Further, the optimal stopping formulationallows us to prove that optimal strategies have threshold properties. To obtainnear-optimal defender strategies, we develop Threshold Fictitious Self-Play(T-FP), a fictitious self-play algorithm that learns Nash equilibria throughstochastic approximation. We show that T-FP outperforms a state-of-the-artalgorithm for our use case. The experimental part of this investigationincludes two systems: a simulation system where defender strategies areincrementally learned and an emulation system where statistics are collectedthat drive simulation runs and where learned strategies are evaluated. We arguethat this approach can produce effective defender strategies for a practical ITinfrastructure.", "output": "Learning Near-Optimal Intrusion Responses Against Dynamic Attackers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) are often used for tasks involving the geometryof a given graph, such as molecular dynamics simulation. Although the distancematrix of a geometric graph contains complete geometric information, it hasbeen demonstrated that Message Passing Neural Networks (MPNNs) are insufficientfor learning this geometry. In this work, we expand on the families ofcounterexamples that MPNNs are unable to distinguish from their distancematrices, by constructing families of novel and symmetric geometric graphs. Wethen propose $k$-DisGNNs, which can effectively exploit the rich geometrycontained in the distance matrix. We demonstrate the high expressive power ofour models by proving the universality of $k$-DisGNNs for distinguishinggeometric graphs when $k geq 3$, and that some existing well-designedgeometric models can be unified by $k$-DisGNNs as special cases. Mostimportantly, we establish a connection between geometric deep learning andtraditional graph representation learning, showing that those highly expressiveGNN models originally designed for graph structure learning can also be appliedto geometric deep learning problems with impressive performance, and thatexisting complex, equivariant models are not the only solution. Experimentalresults verify our theory.", "output": "Is Distance Matrix Enough for Geometric Deep Learning?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Teachers and students are increasingly relying on online learning resourcesto supplement the ones provided in school. This increase in the breadth anddepth of available resources is a great thing for students, but only providedthey are able to find answers to their queries. Question-answering andinformation retrieval systems have benefited from public datasets to train andevaluate their algorithms, but most of these datasets have been in English textwritten by and for adults. We introduce a new public French question-answeringdataset collected from Alloprof, a Quebec-based primary and high-school helpwebsite, containing 29 349 questions and their explanations in a variety ofschool subjects from 10 368 students, with more than half of the explanationscontaining links to other questions or some of the 2 596 reference pages on thewebsite. We also present a case study of this dataset in an informationretrieval task. This dataset was collected on the Alloprof public forum, withall questions verified for their appropriateness and the explanations verifiedboth for their appropriateness and their relevance to the question. To predictrelevant documents, architectures using pre-trained BERT models were fine-tunedand evaluated. This dataset will allow researchers to developquestion-answering, information retrieval and other algorithms specifically forthe French speaking education context. Furthermore, the range of languageproficiency, images, mathematical symbols and spelling mistakes willnecessitate algorithms based on a multimodal comprehension. The case study wepresent as a baseline shows an approach that relies on recent techniquesprovides an acceptable performance level, but more work is necessary before itcan reliably be used and trusted in a production setting.", "output": "Alloprof: a new French question-answer education dataset and its use in an information retrieval case study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recommendation systems are a core feature of social media companies withtheir uses including recommending organic and promoted contents. Many modernrecommendation systems are split into multiple stages - candidate generationand heavy ranking - to balance computational cost against recommendationquality. We focus on the candidate generation phase of a large-scale adsrecommendation problem in this paper, and present a machine learning firstheterogeneous re-architecture of this stage which we term TwERC. We show that asystem that combines a real-time light ranker with sourcing strategies capableof capturing additional information provides validated gains. We present twostrategies. The first strategy uses a notion of similarity in the interactiongraph, while the second strategy caches previous scores from the ranking stage.The graph based strategy achieves a 4.08% revenue gain and the rankscore basedstrategy achieves a 1.38% gain. These two strategies have biases thatcomplement both the light ranker and one another. Finally, we describe a set ofmetrics that we believe are valuable as a means of understanding the complexproduct trade offs inherent in industrial candidate generation systems.", "output": "TwERC: High Performance Ensembled Candidate Generation for Ads Recommendation at Twitter."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Past work in natural language processing interpretability focused mainly onpopular classification tasks while largely overlooking generation settings,partly due to a lack of dedicated tools. In this work, we introduce Inseq, aPython library to democratize access to interpretability analyses of sequencegeneration models. Inseq enables intuitive and optimized extraction of models'internal information and feature importance scores for popular decoder-only andencoder-decoder Transformers architectures. We showcase its potential byadopting it to highlight gender biases in machine translation models and locatefactual knowledge inside GPT-2. Thanks to its extensible interface supportingcutting-edge techniques such as contrastive feature attribution, Inseq candrive future advances in explainable natural language generation, centralizinggood practices and enabling fair and reproducible model evaluations.", "output": "Inseq: An Interpretability Toolkit for Sequence Generation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatial objects often come with textual information, such as Points ofInterest (POIs) with their descriptions, which are referred to as geo-textualdata. To retrieve such data, spatial keyword queries that take into accountboth spatial proximity and textual relevance have been extensively studied.Existing indexes designed for spatial keyword queries are mostly built based onthe geo-textual data without considering the distribution of queries alreadyreceived. However, previous studies have shown that utilizing the known querydistribution can improve the index structure for future query processing. Inthis paper, we propose WISK, a learned index for spatial keyword queries, whichself-adapts for optimizing querying costs given a query workload. One keychallenge is how to utilize both structured spatial attributes and unstructuredtextual information during learning the index. We first divide the data objectsinto partitions, aiming to minimize the processing costs of the given queryworkload. We prove the NP-hardness of the partitioning problem and propose amachine learning model to find the optimal partitions. Then, to achieve morepruning power, we build a hierarchical structure based on the generatedpartitions in a bottom-up manner with a reinforcement learning-based approach.We conduct extensive experiments on real-world datasets and query workloadswith various distributions, and the results show that WISK outperforms allcompetitors, achieving up to 8x speedup in querying time with comparablestorage overhead.", "output": "WISK: A Workload-aware Learned Index for Spatial Keyword Queries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In a federated learning (FL) system, malicious participants can easily embedbackdoors into the aggregated model while maintaining the model's performanceon the main task. To this end, various defenses, including training stageaggregation-based defenses and post-training mitigation defenses, have beenproposed recently. While these defenses obtain reasonable performance againstexisting backdoor attacks, which are mainly heuristics based, we show that theyare insufficient in the face of more advanced attacks. In particular, wepropose a general reinforcement learning-based backdoor attack framework wherethe attacker first trains a (non-myopic) attack policy using a simulator builtupon its local data and common knowledge on the FL system, which is thenapplied during actual FL training. Our attack framework is both adaptive andflexible and achieves strong attack performance and durability even understate-of-the-art defenses.", "output": "Learning to Backdoor Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rewrite systems [6, 10, 12] have been widely employing equality saturation[9], which is an optimisation methodology that uses a saturated e-graph torepresent all possible sequences of rewrite simultaneously, and then extractsthe optimal one. As such, optimal results can be achieved by avoiding thephase-ordering problem. However, we observe that when the e-graph is notsaturated, it cannot represent all possible rewrite opportunities and thereforethe phase-ordering problem is re-introduced during the construction phase ofthe e-graph. To address this problem, we propose MCTS-GEB, a domain-generalrewrite system that applies reinforcement learning (RL) to e-graphconstruction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3]to efficiently plan for the optimal e-graph construction, and therefore it caneffectively eliminate the phase-ordering problem at the construction phase andachieve better performance within a reasonable time. Evaluation in twodifferent domains shows MCTS-GEB can outperform the state-of-the-art rewritesystems by up to 49x, while the optimisation can generally take less than anhour, indicating MCTS-GEB is a promising building block for the futuregeneration of rewrite systems.", "output": "MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of finding the optimal value of n in the n-steptemporal difference (TD) learning algorithm. We find the optimal n by resortingto a model-free optimization technique involving a one-simulation simultaneousperturbation stochastic approximation (SPSA) based procedure that we adopt tothe discrete optimization setting by using a random projection approach. Weprove the convergence of our proposed algorithm, SDPSA, using a differentialinclusions approach and show that it finds the optimal value of n in n-step TD.Through experiments, we show that the optimal value of n is achieved with SDPSAfor arbitrary initial values.", "output": "n-Step Temporal Difference Learning with Optimal n."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many natural language processing tasks benefit from long inputs, butprocessing long documents with Transformers is expensive -- not only due toquadratic attention complexity but also from applying feedforward andprojection layers to every token. However, not all tokens are equallyimportant, especially for longer documents. We propose CoLT5, a long-inputTransformer model that builds on this intuition by employing conditionalcomputation, devoting more resources to important tokens in both feedforwardand attention layers. We show that CoLT5 achieves stronger performance thanLongT5 with much faster training and inference, achieving SOTA on thelong-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractablymake use of extremely long inputs, showing strong gains up to 64k input length.", "output": "CoLT5: Faster Long-Range Transformers with Conditional Computation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While it is well known that population differences from genetics, sex, race,and environmental factors contribute to disease, AI studies in medicine havelargely focused on locoregional patient cohorts with less diverse data sources.Such limitation stems from barriers to large-scale data share and ethicalconcerns over data privacy. Federated learning (FL) is one potential pathwayfor AI development that enables learning across hospitals without data share.In this study, we show the results of various FL strategies on one of thelargest and most diverse COVID-19 chest CT datasets: 21 participating hospitalsacross five continents that comprise &gt;10,000 patients with &gt;1 million images.We also propose an FL strategy that leverages synthetically generated data toovercome class and size imbalances. We also describe the sources of dataheterogeneity in the context of FL, and show how even among the correctlylabeled populations, disparities can arise due to these biases.", "output": "AI Models Close to your Chest: Robust Federated Learning Strategies for Multi-site CT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep ensemble is a simple and straightforward approach for approximatingBayesian inference and has been successfully applied to many classificationtasks. This study aims to comprehensively investigate this approach in themulti-output regression task to predict the aerodynamic performance of amissile configuration. By scrutinizing the effect of the number of neuralnetworks used in the ensemble, an obvious trend toward underconfidence inestimated uncertainty is observed. In this context, we propose the deepensemble framework that applies the post-hoc calibration method, and itsimproved uncertainty quantification performance is demonstrated. It is comparedwith Gaussian process regression, the most prevalent model for uncertaintyquantification in engineering, and is proven to have superior performance interms of regression accuracy, reliability of estimated uncertainty, andtraining efficiency. Finally, the impact of the suggested framework on theresults of Bayesian optimization is examined, showing that whether or not thedeep ensemble is calibrated can result in completely different explorationcharacteristics. This framework can be seamlessly applied and extended to anyregression task, as no special assumptions have been made for the specificproblem used in this study.", "output": "Towards Reliable Uncertainty Quantification via Deep Ensembles in Multi-output Regression Task."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the realm of machine learning, the study of anomaly detection andlocalization within image data has gained substantial traction, particularlyfor practical applications such as industrial defect detection. While themajority of existing methods predominantly use Convolutional Neural Networks(CNN) as their primary network architecture, we introduce a novel approachbased on the Transformer backbone network. Our method employs a two-stageincremental learning strategy. During the first stage, we train a MaskedAutoencoder (MAE) model solely on normal images. In the subsequent stage, weapply pixel-level data augmentation techniques to generate corrupted normalimages and their corresponding pixel labels. This process allows the model tolearn how to repair corrupted regions and classify the status of each pixel.Ultimately, the model generates a pixel reconstruction error matrix and a pixelanomaly probability matrix. These matrices are then combined to produce ananomaly scoring matrix that effectively detects abnormal regions. Whenbenchmarked against several state-of-the-art CNN-based methods, our approachexhibits superior performance on the MVTec AD dataset, achieving an impressive97.6% AUC.", "output": "ISSTAD: Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) aims to learn a single global model that enables thecentral server to help the model training in local clients without accessingtheir local data. The key challenge of FL is the heterogeneity of local data indifferent clients, such as heterogeneous label distribution and feature shift,which could lead to significant performance degradation of the learned models.Although many studies have been proposed to address the heterogeneous labeldistribution problem, few studies attempt to explore the feature shift issue.To address this issue, we propose a simple yet effective algorithm, namelytextbf{p}ersonalized textbf{Fed}erated learning with textbf{L}ocaltextbf{A}ttention (pFedLA), by incorporating the attention mechanism intopersonalized models of clients while keeping the attention blocksclient-specific. Specifically, two modules are proposed in pFedLA, i.e., thepersonalized single attention module and the personalized hybrid attentionmodule. In addition, the proposed pFedLA method is quite flexible and generalas it can be incorporated into any FL method to improve their performancewithout introducing additional communication costs. Extensive experimentsdemonstrate that the proposed pFedLA method can boost the performance ofstate-of-the-art FL methods on different tasks such as image classification andobject detection tasks.", "output": "Personalized Federated Learning with Local Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of artificial intelligence for science, it is consistently anessential challenge to face a limited amount of labeled data for real-worldproblems. The prevailing approach is to pretrain a powerful task-agnostic modelon a large unlabeled corpus but may struggle to transfer knowledge todownstream tasks. In this study, we propose InstructMol, a semi-supervisedlearning algorithm, to take better advantage of unlabeled examples. Itintroduces an instructor model to provide the confidence ratios as themeasurement of pseudo-labels' reliability. These confidence scores then guidethe target model to pay distinct attention to different data points, avoidingthe over-reliance on labeled data and the negative influence of incorrectpseudo-annotations. Comprehensive experiments show that InstructBiosubstantially improves the generalization ability of molecular models, in notonly molecular property predictions but also activity cliff estimations,demonstrating the superiority of the proposed method. Furthermore, our evidenceindicates that InstructBio can be equipped with cutting-edge pretrainingmethods and used to establish large-scale and task-specific pseudo-labeledmolecular datasets, which reduces the predictive errors and shortens thetraining process. Our work provides strong evidence that semi-supervisedlearning can be a promising tool to overcome the data scarcity limitation andadvance molecular representation learning.", "output": "InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The integration of renewable energy sources into the power grid is becomingincreasingly important as the world moves towards a more sustainable energyfuture in line with SDG 7. However, the intermittent nature of renewable energysources can make it challenging to manage the power grid and ensure a stablesupply of electricity, which is crucial for achieving SDG 9. In this paper, wepropose a deep learning-based approach for predicting energy demand in a smartpower grid, which can improve the integration of renewable energy sources byproviding accurate predictions of energy demand. Our approach aligns with SDG13 on climate action as it enables more efficient management of renewableenergy resources. We use long short-term memory networks, which are well-suitedfor time series data, to capture complex patterns and dependencies in energydemand data. The proposed approach is evaluated using four datasets ofhistorical short term energy demand data from different energy distributioncompanies including American Electric Power, Commonwealth Edison, Dayton Powerand Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposedmodel is also compared with three other state of the art forecasting algorithmsnamely, Facebook Prophet, Support Vector Regressor, and Random ForestRegressor. The experimental results show that the proposed REDf model canaccurately predict energy demand with a mean absolute error of 1.4%, indicatingits potential to enhance the stability and efficiency of the power grid andcontribute to achieving SDGs 7, 9, and 13. The proposed model also have thepotential to manage the integration of renewable energy sources in an effectivemanner.", "output": "Predicting Short Term Energy Demand in Smart Grid: A Deep Learning Approach for Integrating Renewable Energy Sources in Line with SDGs 7, 9, and 13."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the past few years, more and more AI applications have been applied toedge devices. However, models trained by data scientists with machine learningframeworks, such as PyTorch or TensorFlow, can not be seamlessly executed onedge. In this paper, we develop an end-to-end code generator parsing apre-trained model to C source libraries for the backend using MicroTVM, amachine learning compiler framework extension addressing inference on baremetal devices. An analysis shows that specific compute-intensive operators canbe easily offloaded to the dedicated accelerator with a Universal ModularAccelerator (UMA) interface, while others are processed in the CPU cores. Byusing the automatically generated ahead-of-time C runtime, we conduct a handgesture recognition experiment on an ARM Cortex M4F core.", "output": "Deploying Machine Learning Models to Ahead-of-Time Runtime on Edge Using MicroTVM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent data regulations necessitate machine unlearning (MU): The removal ofthe effect of specific examples from the model. While exact unlearning ispossible by conducting a model retraining with the remaining data from scratch,its computational cost has led to the development of approximate but efficientunlearning schemes. Beyond data-centric MU solutions, we advance MU through anovel model-based viewpoint: sparsification via weight pruning. Our results inboth theory and practice indicate that model sparsity can boost themulti-criteria unlearning performance of an approximate unlearner, closing theapproximation gap, while continuing to be efficient. With this insight, wedevelop two new sparsity-aware unlearning meta-schemes, termed `prune first,then unlearn' and `sparsity-aware unlearning'. Extensive experiments show thatour findings and proposals consistently benefit MU in various scenarios,including class-wise data scrubbing, random data scrubbing, and backdoor dataforgetting. One highlight is the 77% unlearning efficacy gain of fine-tuning(one of the simplest approximate unlearning methods) in the proposedsparsity-aware unlearning paradigm. Codes are available at", "output": "Model Sparsification Can Simplify Machine Unlearning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The most relevant problems in discounted reinforcement learning involveestimating the mean of a function under the stationary distribution of a Markovreward process, such as the expected return in policy evaluation, or the policygradient in policy optimization. In practice, these estimates are producedthrough a finite-horizon episodic sampling, which neglects the mixingproperties of the Markov process. It is mostly unclear how this mismatchbetween the practical and the ideal setting affects the estimation, and theliterature lacks a formal study on the pitfalls of episodic sampling, and howto do it optimally. In this paper, we present a minimax lower bound on thediscounted mean estimation problem that explicitly connects the estimationerror with the mixing properties of the Markov process and the discount factor.Then, we provide a statistical analysis on a set of notable estimators and thecorresponding sampling procedures, which includes the finite-horizon estimatorsoften used in practice. Crucially, we show that estimating the mean by directlysampling from the discounted kernel of the Markov process brings compellingstatistical properties w.r.t. the alternative estimators, as it matches thelower bound without requiring a careful tuning of the episode horizon.", "output": "A Tale of Sampling and Estimation in Discounted Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Physics-informed neural networks (PINNs) have proven a suitable mathematicalscaffold for solving inverse ordinary (ODE) and partial differential equations(PDE). Typical inverse PINNs are formulated as soft-constrained multi-objectiveoptimization problems with several hyperparameters. In this work, wedemonstrate that inverse PINNs can be framed in terms of maximum-likelihoodestimators (MLE) to allow explicit error propagation from interpolation to thephysical model space through Taylor expansion, without the need ofhyperparameter tuning. We explore its application to high-dimensional coupledODEs constrained by differential algebraic equations that are common intransient chemical and biological kinetics. Furthermore, we show thatsingular-value decomposition (SVD) of the ODE coupling matrices (reactionstoichiometry matrix) provides reduced uncorrelated subspaces in which PINNssolutions can be represented and over which residuals can be projected.Finally, SVD bases serve as preconditioners for the inversion of covariancematrices in this hyperparameter-free robust application of MLE to``kinetics-informed neural networks''.", "output": "Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent applications of deep convolutional neural networks in medical imagingraise concerns about their interpretability. While most explainable deeplearning applications use post hoc methods (such as GradCAM) to generatefeature attribution maps, there is a new type of case-based reasoning models,namely ProtoPNet and its variants, which identify prototypes during trainingand compare input image patches with those prototypes. We propose the firstmedical prototype network (MProtoNet) to extend ProtoPNet to brain tumorclassification with 3D multi-parametric magnetic resonance imaging (mpMRI)data. To address different requirements between 2D natural images and 3D mpMRIsespecially in terms of localizing attention regions, a new attention modulewith soft masking and online-CAM loss is introduced. Soft masking helps sharpenattention maps, while online-CAM loss directly utilizes image-level labels whentraining the attention module. MProtoNet achieves statistically significantimprovements in interpretability metrics of both correctness and localizationcoherence (with a best activation precision of $0.713pm0.058$) withouthuman-annotated labels during training, when compared with GradCAM and severalProtoPNet variants. The source code is available at", "output": "MProtoNet: A Case-Based Interpretable Model for Brain Tumor Classification with 3D Multi-parametric Magnetic Resonance Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It has been reported that clustering-based topic models, which clusterhigh-quality sentence embeddings with an appropriate word selection method, cangenerate better topics than generative probabilistic topic models. However,these approaches suffer from the inability to select appropriate parameters andincomplete models that overlook the quantitative relation between words withtopics and topics with text. To solve these issues, we propose graph to topic(G2T), a simple but effective framework for topic modelling. The framework iscomposed of four modules. First, document representation is acquired usingpretrained language models. Second, a semantic graph is constructed accordingto the similarity between document representations. Third, communities indocument semantic graphs are identified, and the relationship between topicsand documents is quantified accordingly. Fourth, the word--topic distributionis computed based on a variant of TFIDF. Automatic evaluation suggests that G2Tachieved state-of-the-art performance on both English and Chinese documentswith different lengths. Human judgements demonstrate that G2T can producetopics with better interpretability and coverage than baselines. In addition,G2T can not only determine the topic number automatically but also give theprobabilistic distribution of words in topics and topics in documents. Finally,G2T is publicly available, and the distillation experiments provide instructionon how it works.", "output": "G2T: A Simple but Effective Framework for Topic Modeling based on Pretrained Language Model and Community Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatial control is a core capability in controllable image generation.Advancements in layout-guided image generation have shown promising results onin-distribution (ID) datasets with similar spatial configurations. However, itis unclear how these models perform when facing out-of-distribution (OOD)samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench,a diagnostic benchmark for layout-guided image generation that examines fourcategories of spatial control skills: number, position, size, and shape. Webenchmark two recent representative layout-guided image generation methods andobserve that the good ID layout control may not generalize well to arbitrarylayouts in the wild (e.g., objects at the boundary). Next, we proposeIterInpaint, a new baseline that generates foreground and background regions ina step-by-step manner via inpainting, demonstrating stronger generalizabilitythan existing models on OOD layouts in LayoutBench. We perform quantitative andqualitative evaluation and fine-grained analysis on the four LayoutBench skillsto pinpoint the weaknesses of existing models. Lastly, we show comprehensiveablation studies on IterInpaint, including training task ratio, crop&amp;paste vs.repaint, and generation order. Project website: ", "output": "Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation."}]