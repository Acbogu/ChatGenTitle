[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The degradation of sewer pipes poses significant economical, environmentaland health concerns. The maintenance of such assets requires structured plansto perform inspections, which are more efficient when structural andenvironmental features are considered along with the results of previousinspection reports. The development of such plans requires degradation modelsthat can be based on statistical and machine learning methods. This workproposes a methodology to assess their suitability to plan inspectionsconsidering three dimensions: accuracy metrics, ability to produce long-termdegradation curves and explainability. Results suggest that although ensemblemodels yield the highest accuracy, they are unable to infer the long-termdegradation of the pipes, whereas the Logistic Regression offers a slightlyless accurate model that is able to produce consistent degradation curves witha high explainability. A use case is presented to demonstrate this methodologyand the efficiency of model-based planning compared to the current inspectionplan.", "output": "Assessment of the suitability of degradation models for the planning of CCTV inspections of sewer pipes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In imperative programming, the Domain-Driven Design methodology helps incoping with the complexity of software development by materializing in code theinvariants of a domain of interest. Code is cleaner and more secure because anyimplicit assumption is removed in favor of invariants, thus enabling a failfast mindset and the immediate reporting of unexpected conditions. This articleintroduces a notion of template for Answer Set Programming that, in addition tothe don't repeat yourself principle, enforces locality of some predicates bymeans of a simple naming convention. Local predicates are mapped to the usualglobal namespace adopted by mainstream engines, using universally uniqueidentifiers to avoid name clashes. This way, local predicates can be used toenforce invariants on the expected outcome of a template in a possibly emptycontext of application, independently by other rules that can be added to sucha context. Template applications transpiled this way can be processed bymainstream engines and safely shared with other knowledge designers, even whenthey have zero knowledge of templates.", "output": "Rethinking Answer Set Programming Templates."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT is a generative pretrained transformer language model created usingartificial intelligence implemented as chatbot which can provide very detailedresponses to a wide variety of questions. As a very contemporary phenomenon,this tool has a wide variety of potential use cases that have yet to beexplored. With the significant extent of information on a broad assortment ofpotential topics, ChatGPT could add value to many information security usescases both from an efficiency perspective as well as to offer another source ofsecurity information that could be used to assist with securing Internetaccessible assets of organizations. One information security practice thatcould benefit from ChatGPT is the reconnaissance phase of penetration testing.This research uses a case study methodology to explore and investigate the usesof ChatGPT in obtaining valuable reconnaissance data. ChatGPT is able toprovide many types of intel regarding targeted properties which includesInternet Protocol (IP) address ranges, domain names, network topology, vendortechnologies, SSL/TLS ciphers, ports &amp; services, and operating systems used bythe target. The reconnaissance information can then be used during the planningphase of a penetration test to determine the tactics, tools, and techniques toguide the later phases of the penetration test in order to discover potentialrisks such as unpatched software components and security misconfigurationrelated issues. The study provides insights into how artificial intelligencelanguage models can be used in cybersecurity and contributes to the advancementof penetration testing techniques.Keywords: ChatGPT, Penetration Testing, Reconnaissance", "output": "Maximizing Penetration Testing Success with Effective Reconnaissance Techniques using ChatGPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The book attempts to introduce a gentle introduction to the field of FacialMicro Expressions Recognition (FMER) using Color and Depth images, with the aidof MATLAB programming environment. FMER is a subset of image processing and itis a multidisciplinary topic to analysis. So, it requires familiarity withother topics of Artifactual Intelligence (AI) such as machine learning, digitalimage processing, psychology and more. So, it is a great opportunity to write abook which covers all of these topics for beginner to professional readers inthe field of AI and even without having background of AI. Our goal is toprovide a standalone introduction in the field of MFER analysis in the form oftheorical descriptions for readers with no background in image processing withreproducible Matlab practical examples. Also, we describe any basic definitionsfor FMER analysis and MATLAB library which is used in the text, that helpsfinal reader to apply the experiments in the real-world applications. Webelieve that this book is suitable for students, researchers, and professionalsalike, who need to develop practical skills, along with a basic understandingof the field. We expect that, after reading this book, the reader feelscomfortable with different key stages such as color and depth image processing,color and depth image representation, classification, machine learning, facialmicro-expressions recognition, feature extraction and dimensionality reduction.The book attempts to introduce a gentle introduction to the field of FacialMicro Expressions Recognition (FMER) using Color and Depth images, with the aidof MATLAB programming environment.", "output": "Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach (Second Edition, 2023)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal logic can be used to formally specify autonomous agent goals, butsynthesizing planners that guarantee goal satisfaction can be computationallyprohibitive. This paper shows how to turn goals specified using a subset offinite trace Linear Temporal Logic (LTL) into a behavior tree (BT) thatguarantees that successful traces satisfy the LTL goal. Useful LTL formulas forachievement goals can be derived using achievement-oriented task missiongrammars, leading to missions made up of tasks combined using LTL operators.Constructing BTs from LTL formulas leads to a relaxed behavior synthesisproblem in which a wide range of planners can implement the action nodes in theBT. Importantly, any successful trace induced by the planners satisfies thecorresponding LTL formula. The usefulness of the approach is demonstrated intwo ways: a) exploring the alignment between two planners and LTL goals, and b)solving a sequential key-door problem for a Fetch robot.", "output": "Designing Behavior Trees from Goal-Oriented LTLf Formulas."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs), such as GPT-4, have demonstrated remarkablecapabilities across a wide range of tasks, including health applications. Inthis paper, we study how LLMs can be used to scale biomedical knowledgecuration. We find that while LLMs already possess decent competency instructuring biomedical text, by distillation into a task-specific student modelthrough self-supervised learning, substantial gains can be attained overout-of-box LLMs, with additional advantages such as cost, efficiency, andwhite-box model access.We conduct a case study on adverse drug event (ADE) extraction, which is animportant area for improving care. On standard ADE extraction evaluation, aGPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervisedstate-of-the-art models without using any labeled data. Despite being over1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 byover 6 absolute points in F1 and GPT-4 by over 5 absolute points.Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT)and ADE extraction architecture shed light on best practice for biomedicalknowledge extraction. Similar gains were attained by distillation for otherstandard biomedical knowledge extraction tasks such as gene-diseaseassociations and protected health information, further illustrating the promiseof this approach.", "output": "Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The computation necessary for training Transformer-based language models hasskyrocketed in recent years. This trend has motivated research on efficienttraining algorithms designed to improve training, validation, and downstreamperformance faster than standard training. In this work, we revisit threecategories of such algorithms: dynamic architectures (layer stacking, layerdropping), batch selection (selective backprop, RHO loss), and efficientoptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixedcomputation budget using such methods, we find that their training, validation,and downstream gains vanish compared to a baseline with a fully-decayedlearning rate. We define an evaluation protocol that enables computation to bedone on arbitrary machines by mapping all computation time to a referencemachine which we call reference system time. We discuss the limitations of ourproposed protocol and release our code to encourage rigorous research inefficient training procedures: ", "output": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We advance the Cohn-Umans framework for developing fast matrix multiplicationalgorithms. We introduce, analyze, and search for a new subclass of stronguniquely solvable puzzles (SUSP), which we call simplifiable SUSPs. We showthat these puzzles are efficiently verifiable, which remains an open questionfor general SUSPs. We also show that individual simplifiable SUSPs can achievethe same strength of bounds on the matrix multiplication exponent $omega$ thatinfinite families of SUSPs can. We report on the construction, by computersearch, of larger SUSPs than previously known for small width. This, combinedwith our tighter analysis, strengthens the upper bound on the matrixmultiplication exponent from $2.66$ to $2.505$ obtainable via thiscomputational approach, and nears the results of the handcrafted constructionsof Cohn et al.", "output": "Efficiently-Verifiable Strong Uniquely Solvable Puzzles and Matrix Multiplication."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated classifiers (ACs), often built via supervised machine learning(SML), can categorize large, statistically powerful samples of data rangingfrom text to images and video, and have become widely popular measurementdevices in communication science and related fields. Despite this popularity,even highly accurate classifiers make errors that cause misclassification biasand misleading results in downstream analyses-unless such analyses account forthese errors. As we show in a systematic literature review of SML applications,communication scholars largely ignore misclassification bias. In principle,existing statistical methods can use \"gold standard\" validation data, such asthat created by human annotators, to correct misclassification bias and produceconsistent estimates. We introduce and test such methods, including a newmethod we design and implement in the R package misclassificationmodels, viaMonte Carlo simulations designed to reveal each method's limitations, which wealso release. Based on our results, we recommend our new error correctionmethod as it is versatile and efficient. In sum, automated classifiers, eventhose below common accuracy standards or making systematic misclassifications,can be useful for measurement with careful study design and appropriate errorcorrection methods.", "output": "Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models are susceptible to adversarial samples in white andblack-box environments. Although previous studies have shown high attacksuccess rates, coupling DNN models with interpretation models could offer asense of security when a human expert is involved, who can identify whether agiven sample is benign or malicious. However, in white-box environments,interpretable deep learning systems (IDLSes) have been shown to be vulnerableto malicious manipulations. In black-box settings, as access to the componentsof IDLSes is limited, it becomes more challenging for the adversary to fool thesystem. In this work, we propose a Query-efficient Score-based black-box attackagainst IDLSes, QuScore, which requires no knowledge of the target model andits coupled interpretation model. QuScore is based on transfer-based andscore-based methods by employing an effective microbial genetic algorithm. Ourmethod is designed to reduce the number of queries necessary to carry outsuccessful attacks, resulting in a more efficient process. By continuouslyrefining the adversarial samples created based on feedback scores from theIDLS, our approach effectively navigates the search space to identifyperturbations that can fool the system. We evaluate the attack's effectivenesson four CNN models (Inception, ResNet, VGG, DenseNet) and two interpretationmodels (CAM, Grad), using both ImageNet and CIFAR datasets. Our results showthat the proposed approach is query-efficient with a high attack success ratethat can reach between 95% and 100% and transferability with an average successrate of 69% in the ImageNet and CIFAR datasets. Our attack method generatesadversarial examples with attribution maps that resemble benign samples. Wehave also demonstrated that our attack is resilient against variouspreprocessing defense techniques and can easily be transferred to different DNNmodels.", "output": "Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Objective: The artificial pancreas (AP) has shown promising potential inachieving closed-loop glucose control for individuals with type 1 diabetesmellitus (T1DM). However, designing an effective control policy for the APremains challenging due to the complex physiological processes, delayed insulinresponse, and inaccurate glucose measurements. While model predictive control(MPC) offers safety and stability through the dynamic model and safetyconstraints, it lacks individualization and is adversely affected byunannounced meals. Conversely, deep reinforcement learning (DRL) providespersonalized and adaptive strategies but faces challenges with distributionshifts and substantial data requirements. Methods: We propose a hybrid controlpolicy for the artificial pancreas (HyCPAP) to address the above challenges.HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging thestrengths of both policies while compensating for their respective limitations.To facilitate faster deployment of AP systems in real-world settings, wefurther incorporate meta-learning techniques into HyCPAP, leveraging previousexperience and patient-shared knowledge to enable fast adaptation to newpatients with limited available data. Results: We conduct extensive experimentsusing the FDA-accepted UVA/Padova T1DM simulator across three scenarios. Ourapproaches achieve the highest percentage of time spent in the desiredeuglycemic range and the lowest occurrences of hypoglycemia. Conclusion: Theresults clearly demonstrate the superiority of our methods for closed-loopglucose management in individuals with T1DM. Significance: The study presentsnovel control policies for AP systems, affirming the great potential ofproposed methods for efficient closed-loop glucose control.", "output": "Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integrating deep learning with clinical expertise holds great potential foraddressing healthcare challenges and empowering medical professionals withimproved diagnostic tools. However, the need for annotated medical images isoften an obstacle to leveraging the full power of machine learning models. Ourresearch demonstrates that by combining synthetic images, generated usingdiffusion models, with real images, we can enhance nonalcoholic fatty liverdisease (NAFLD) classification performance. We evaluate the quality of thesynthetic images by comparing two metrics: Inception Score (IS) and Fr'{e}chetInception Distance (FID), computed on diffusion-generated images and generativeadversarial networks (GANs)-generated images. Our results show superiorperformance for the diffusion-generated images, with a maximum IS score of$1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ comparedto $99.53$ for GANs. Utilizing a partially frozen CNN backbone (EfficientNetv1), our synthetic augmentation method achieves a maximum image-level ROC AUCof $0.904$ on a NAFLD prediction task.", "output": "Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Beliefs and values are increasingly being incorporated into our AI systemsthrough alignment processes, such as carefully curating data collectionprinciples or regularizing the loss function used for training. However, themeta-alignment problem is that these human beliefs are diverse and not alignedacross populations; furthermore, the implicit strength of each belief may notbe well calibrated even among humans, especially when trying to generalizeacross contexts. Specifically, in high regret situations, we observe thatcontextual counterfactuals and recourse costs are particularly important inupdating a decision maker's beliefs and the strengths to which such beliefs areheld. Therefore, we argue that including counterfactuals is key to an accuratecalibration of beliefs during alignment. To do this, we first segment beliefdiversity into two categories: subjectivity (across individuals within apopulation) and epistemic uncertainty (within an individual across differentcontexts). By leveraging our notion of epistemic uncertainty, we introduce `thebelief calibration cycle' framework to more holistically calibrate thisdiversity of beliefs with context-driven counterfactual reasoning by using amulti-objective optimization. We empirically apply our framework for finding aPareto frontier of clustered optimal belief strengths that generalize acrossdifferent contexts, demonstrating its efficacy on a toy dataset for creditdecisions.", "output": "Leveraging Contextual Counterfactuals Toward Belief Calibration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Drug discovery is adapting to novel technologies such as data science,informatics, and artificial intelligence (AI) to accelerate effective treatmentdevelopment while reducing costs and animal experiments. AI is transformingdrug discovery, as indicated by increasing interest from investors, industrialand academic scientists, and legislators. Successful drug discovery requiresoptimizing properties related to pharmacodynamics, pharmacokinetics, andclinical outcomes. This review discusses the use of AI in the three pillars ofdrug discovery: diseases, targets, and therapeutic modalities, with a focus onsmall molecule drugs. AI technologies, such as generative chemistry, machinelearning, and multi-property optimization, have enabled several compounds toenter clinical trials. The scientific community must carefully vet knowninformation to address the reproducibility crisis. The full potential of AI indrug discovery can only be realized with sufficient ground truth andappropriate human intervention at later pipeline stages.", "output": "Artificial Intelligence for Drug Discovery: Are We There Yet?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inverse reinforcement learning (IRL) algorithms often rely on (forward)reinforcement learning or planning over a given time horizon to compute anapproximately optimal policy for a hypothesized reward function and then matchthis policy with expert demonstrations. The time horizon plays a critical rolein determining both the accuracy of reward estimate and the computationalefficiency of IRL algorithms. Interestingly, an effective time horizon shorterthan the ground-truth value often produces better results faster. This workformally analyzes this phenomenon and provides an explanation: the time horizoncontrols the complexity of an induced policy class and mitigates overfittingwith limited data. This analysis leads to a principled choice of the effectivehorizon for IRL. It also prompts us to reexamine the classic IRL formulation:it is more natural to learn jointly the reward and the effective horizontogether rather than the reward alone with a given horizon. Our experimentalresults confirm the theoretical analysis.", "output": "On the Effective Horizon of Inverse Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prescriptive process monitoring methods seek to optimize the performance ofbusiness processes by triggering interventions at runtime, thereby increasingthe probability of positive case outcomes. These interventions are triggeredaccording to an intervention policy. Reinforcement learning has been putforward as an approach to learning intervention policies through trial anderror. Existing approaches in this space assume that the number of resourcesavailable to perform interventions in a process is unlimited, an unrealisticassumption in practice. This paper argues that, in the presence of resourceconstraints, a key dilemma in the field of prescriptive process monitoring isto trigger interventions based not only on predictions of their necessity,timeliness, or effect but also on the uncertainty of these predictions and thelevel of resource utilization. Indeed, committing scarce resources to anintervention when the necessity or effects of this intervention are highlyuncertain may intuitively lead to suboptimal intervention effects. Accordingly,the paper proposes a reinforcement learning approach for prescriptive processmonitoring that leverages conformal prediction techniques to consider theuncertainty of the predictions upon which an intervention decision is based. Anevaluation using real-life datasets demonstrates that explicitly modelinguncertainty using conformal predictions helps reinforcement learning agentsconverge towards policies with higher net intervention gain", "output": "Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ship orientation angle prediction (SOAP) with optical remote sensing imagesis an important image processing task, which often relies on deep convolutionalneural networks (CNNs) to make accurate predictions. This paper proposes anovel framework to reduce the model sizes and computational costs of SOAPmodels without harming prediction accuracy. First, a new SOAP model calledMobile-SOAP is designed based on MobileNetV2, achieving state-of-the-artprediction accuracy. Four tiny SOAP models are also created by replacing theconvolutional blocks in Mobile-SOAP with four small-scale networks,respectively. Then, to transfer knowledge from Mobile-SOAP to four lightweightmodels, we propose a novel knowledge distillation (KD) framework termed SOAP-KDconsisting of a novel feature-based guidance loss and an optimized syntheticsamples-based knowledge transfer mechanism. Lastly, extensive experiments onthe FGSC-23 dataset confirm the superiority of Mobile-SOAP over existing modelsand also demonstrate the effectiveness of SOAP-KD in improving the predictionperformance of four specially designed tiny models. Notably, by using SOAP-KD,the test mean absolute error of the ShuffleNetV2x1.0-based model is only 8%higher than that of Mobile-SOAP, but its number of parameters andmultiply-accumulate operations (MACs) are respectively 61.6% and 60.8% less.", "output": "Regression-Oriented Knowledge Distillation for Lightweight Ship Orientation Angle Prediction with Optical Remote Sensing Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Retinal vessel segmentation is generally grounded in image-based datasetscollected with bench-top devices. The static images naturally lose the dynamiccharacteristics of retina fluctuation, resulting in diminished datasetrichness, and the usage of bench-top devices further restricts datasetscalability due to its limited accessibility. Considering these limitations, weintroduce the first video-based retinal dataset by employing handheld devicesfor data acquisition. The dataset comprises 635 smartphone-based fundus videoscollected from four different clinics, involving 415 patients from 50 to 75years old. It delivers comprehensive and precise annotations of retinalstructures in both spatial and temporal dimensions, aiming to advance thelandscape of vasculature segmentation. Specifically, the dataset provides threelevels of spatial annotations: binary vessel masks for overall retinalstructure delineation, general vein-artery masks for distinguishing the veinand artery, and fine-grained vein-artery masks for further characterizing thegranularities of each artery and vein. In addition, the dataset offers temporalannotations that capture the vessel pulsation characteristics, assisting indetecting ocular diseases that require fine-grained recognition of hemodynamicfluctuation. In application, our dataset exhibits a significant domain shiftwith respect to data captured by bench-top devices, thus posing greatchallenges to existing methods. In the experiments, we provide evaluationmetrics and benchmark results on our dataset, reflecting both the potential andchallenges it offers for vessel segmentation tasks. We hope this challengingdataset would significantly contribute to the development of eye diseasediagnosis and early prevention.", "output": "RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, the no-box adversarial attack, in which the attacker lacks accessto the model's architecture, weights, and training data, become the mostpractical and challenging attack setup. However, there is an unawareness of thepotential and flexibility inherent in the surrogate model selection process onno-box setting. Inspired by the burgeoning interest in utilizing foundationalmodels to address downstream tasks, this paper adopts an innovative idea that1) recasting adversarial attack as a downstream task. Specifically, image noisegeneration to meet the emerging trend and 2) introducing foundational models assurrogate models. Harnessing the concept of non-robust features, we elaborateon two guiding principles for surrogate model selection to explain why thefoundational model is an optimal choice for this role. However, paradoxically,we observe that these foundational models underperform. Analyzing thisunexpected behavior within the feature space, we attribute the lacklusterperformance of foundational models (e.g., CLIP) to their significantrepresentational capacity and, conversely, their lack of discriminativeprowess. To mitigate this issue, we propose the use of a margin-based lossstrategy for the fine-tuning of foundational models on target images. Theexperimental results verify that our approach, which employs the basic FastGradient Sign Method (FGSM) attack algorithm, outstrips the performance ofother, more convoluted algorithms. We conclude by advocating for the researchcommunity to consider surrogate models as crucial determinants in theeffectiveness of adversarial attacks in no-box settings. The implications ofour work bear relevance for improving the efficacy of such adversarial attacksand the overall robustness of AI systems.", "output": "Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Software vulnerabilities leading to various detriments such as crashes, dataloss, and security breaches, significantly hinder the quality, affecting themarket adoption of software applications and systems. Although traditionalmethods such as automated software testing, fault localization, and repair havebeen intensively studied, static analysis tools are most commonly used and havean inherent false positives rate, posing a solid challenge to developerproductivity. Large Language Models (LLMs) offer a promising solution to thesepersistent issues. Among these, FalconLLM has shown substantial potential inidentifying intricate patterns and complex vulnerabilities, hence crucial insoftware vulnerability detection. In this paper, for the first time, FalconLLMis being fine-tuned for cybersecurity applications, thus introducingSecureFalcon, an innovative model architecture built upon FalconLLM.SecureFalcon is trained to differentiate between vulnerable and non-vulnerableC code samples. We build a new training dataset, FormAI, constructed thanks toGenerative Artificial Intelligence (AI) and formal verification to evaluate itsperformance. SecureFalcon achieved an impressive 94% accuracy rate in detectingsoftware vulnerabilities, emphasizing its significant potential to redefinesoftware vulnerability detection methods in cybersecurity.", "output": "SecureFalcon: The Next Cyber Reasoning System for Cyber Security."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, the novel Image Transformation Sequence Retrieval (ITSR) taskis presented, in which a model must retrieve the sequence of transformationsbetween two given images that act as source and target, respectively. Givencertain characteristics of the challenge such as the multiplicity of a correctsequence or the correlation between consecutive steps of the process, wepropose a solution to ITSR using a general model-based Reinforcement Learningsuch as Monte Carlo Tree Search (MCTS), which is combined with a deep neuralnetwork. Our experiments provide a benchmark in both synthetic and realdomains, where the proposed approach is compared with supervised training. Theresults report that a model trained with MCTS is able to outperform itssupervised counterpart in both the simplest and the most complex cases. Ourwork draws interesting conclusions about the nature of ITSR and its associatedchallenges.", "output": "Image Transformation Sequence Retrieval with General Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present DeepIPCv2, an autonomous driving model that perceives theenvironment using a LiDAR sensor for more robust drivability, especially whendriving under poor illumination conditions. DeepIPCv2 takes a set of LiDARpoint clouds for its main perception input. As point clouds are not affected byillumination changes, they can provide a clear observation of the surroundingsno matter what the condition is. This results in a better scene understandingand stable features provided by the perception module to support the controllermodule in estimating navigational control properly. To evaluate itsperformance, we conduct several tests by deploying the model to predict a setof driving records and perform real automated driving under three differentconditions. We also conduct ablation and comparative studies with some recentmodels to justify its performance. Based on the experimental results, DeepIPCv2shows a robust performance by achieving the best drivability in all conditions.Codes are available at ", "output": "DeepIPCv2: LiDAR-powered Robust Environmental Perception and Navigational Control for Autonomous Vehicle."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The presence of coherent noise in seismic data leads to errors anduncertainties, and as such it is paramount to suppress noise as early andefficiently as possible. Self-supervised denoising circumvents the commonrequirement of deep learning procedures of having noisy-clean training pairs.However, self-supervised coherent noise suppression methods require extensiveknowledge of the noise statistics. We propose the use of explainable artificialintelligence approaches to see inside the black box that is the denoisingnetwork and use the gained knowledge to replace the need for any priorknowledge of the noise itself. This is achieved in practice by leveragingbias-free networks and the direct linear link between input and output providedby the associated Jacobian matrix; we show that a simple averaging of theJacobian contributions over a number of randomly selected input pixels,provides an indication of the most effective mask to suppress noise present inthe data. The proposed method therefore becomes a fully automated denoisingprocedure requiring no clean training labels or prior knowledge. Realisticsynthetic examples with noise signals of varying complexities, ranging fromsimple time-correlated noise to complex pseudo rig noise propagating at thevelocity of the ocean, are used to validate the proposed approach. Itsautomated nature is highlighted further by an application to two fielddatasets. Without any substantial pre-processing or any knowledge of theacquisition environment, the automatically identified blind-masks are shown toperform well in suppressing both trace-wise noise in common shot gathers fromthe Volve marine dataset and colored noise in post stack seismic images from aland seismic survey.", "output": "Explainable Artificial Intelligence driven mask design for self-supervised seismic denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, ubiquitous semantic Metaverse has been studied torevolutionize immersive cyber-virtual experiences for augmented reality (AR)and virtual reality (VR) users, which leverages advanced semantic understandingand representation to enable seamless, context-aware interactions withinmixed-reality environments. This survey focuses on the intelligence andspatio-temporal characteristics of four fundamental system components inubiquitous semantic Metaverse, i.e., artificial intelligence (AI),spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),and semantic-enhanced digital twin (SDT). We thoroughly survey therepresentative techniques of the four fundamental system components that enableintelligent, personalized, and context-aware interactions with typical usecases of the ubiquitous semantic Metaverse, such as remote education, work andcollaboration, entertainment and socialization, healthcare, and e-commercemarketing. Furthermore, we outline the opportunities for constructing thefuture ubiquitous semantic Metaverse, including scalability andinteroperability, privacy and security, performance measurement andstandardization, as well as ethical considerations and responsible AI.Addressing those challenges is important for creating a robust, secure, andethically sound system environment that offers engaging immersive experiencesfor the users and AR/VR applications.", "output": "Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge Graph Embedding (KGE) models are used to learn continuousrepresentations of entities and relations. A key task in the literature ispredicting missing links between entities. However, Knowledge Graphs are notjust sets of links but also have semantics underlying their structure.Semantics is crucial in several downstream tasks, such as query answering orreasoning. We introduce the subgraph inference task, where a model has togenerate likely and semantically valid subgraphs. We propose IntelliGraphs, aset of five new Knowledge Graph datasets. The IntelliGraphs datasets containsubgraphs with semantics expressed in logical rules for evaluating subgraphinference. We also present the dataset generator that produced the syntheticdatasets. We designed four novel baseline models, which include three modelsbased on traditional KGEs. We evaluate their expressiveness and show that thesemodels cannot capture the semantics. We believe this benchmark will encouragethe development of machine learning models that emphasize semanticunderstanding.", "output": "IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the video prediction task by putting forth a novel model thatcombines (i) our recently proposed hierarchical residual vector quantizedvariational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN(ST-PixelCNN). We refer to this approach as a sequential hierarchical residuallearning vector quantized variational autoencoder (S-HR-VQVAE). By leveragingthe intrinsic capabilities of HR-VQVAE at modeling still images with aparsimonious representation, combined with the ST-PixelCNN's ability athandling spatiotemporal information, S-HR-VQVAE can better deal with chiefchallenges in video prediction. These include learning spatiotemporalinformation, handling high dimensional data, combating blurry prediction, andimplicit modeling of physical characteristics. Extensive experimental resultson the KTH Human Action and Moving-MNIST tasks demonstrate that our modelcompares favorably against top video prediction techniques both in quantitativeand qualitative evaluations despite a much smaller model size. Finally, weboost S-HR-VQVAE by proposing a novel training method to jointly estimate theHR-VQVAE and ST-PixelCNN parameters.", "output": "S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A wide variety of generative models for graphs have been proposed. They areused in drug discovery, road networks, neural architecture search, and programsynthesis. Generating graphs has theoretical challenges, such as isomorphicrepresentations -- evaluating how well a generative model performs isdifficult. Which model to choose depending on the application domain?We extensively study kernel-based metrics on distributions of graphinvariants and manifold-based and kernel-based metrics in graph embeddingspace. Manifold-based metrics outperform kernel-based metrics in embeddingspace. We use these metrics to compare GraphRNN and GRAN, two well-knowngenerative models for graphs, and unveil the influence of node orderings. Itshows the superiority of GRAN over GraphRNN - further, our proposed adaptationof GraphRNN with a depth-first search ordering is effective for small-sizedgraphs.A guideline on good practices regarding dataset selection and node featureinitialization is provided. Our work is accompanied by open-source code andreproducible experiments.", "output": "GRAN is superior to GraphRNN: node orderings, kernel- and graph embeddings-based metrics for graph generators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series forecasting has received wide interest from existing research dueto its broad applications and inherent challenging. The research challenge liesin identifying effective patterns in historical series and applying them tofuture forecasting. Advanced models based on point-wise connected MLP andTransformer architectures have strong fitting power, but their secondarycomputational complexity limits practicality. Additionally, those structuresinherently disrupt the temporal order, reducing the information utilization andmaking the forecasting process uninterpretable. To solve these problems, thispaper proposes a forecasting model, MPR-Net. It first adaptively decomposesmulti-scale historical series patterns using convolution operation, thenconstructs a pattern extension forecasting method based on the prior knowledgeof pattern reproduction, and finally reconstructs future patterns into futureseries using deconvolution operation. By leveraging the temporal dependenciespresent in the time series, MPR-Net not only achieves linear time complexity,but also makes the forecasting process interpretable. By carrying outsufficient experiments on more than ten real data sets of both short and longterm forecasting tasks, MPR-Net achieves the state of the art forecastingperformance, as well as good generalization and robustness performance.", "output": "MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The integrated development of city clusters has given rise to an increasingdemand for intercity travel. Intercity ride-pooling service exhibitsconsiderable potential in upgrading traditional intercity bus services byimplementing demand-responsive enhancements. Nevertheless, its onlineoperations suffer the inherent complexities due to the coupling of vehicleresource allocation among cities and pooled-ride vehicle routing. To tacklethese challenges, this study proposes a two-level framework designed tofacilitate online fleet management. Specifically, a novel multi-agent feudalreinforcement learning model is proposed at the upper level of the framework tocooperatively assign idle vehicles to different intercity lines, while thelower level updates the routes of vehicles using an adaptive large neighborhoodsearch heuristic. Numerical studies based on the realistic dataset of Xiamenand its surrounding cities in China show that the proposed frameworkeffectively mitigates the supply and demand imbalances, and achievessignificant improvement in both the average daily system profit and orderfulfillment ratio.", "output": "Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling Services: A Multi-Agent Hierarchical Reinforcement Learning Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we present a layered approach for multi-agent control problem,decomposed into three stages, each building upon the results of the previousone. First, a high-level plan for a coarse abstraction of the system iscomputed, relying on parametric timed automata augmented with stopwatches asthey allow to efficiently model simplified dynamics of such systems. In thesecond stage, the high-level plan, based on SMT-formulation, mainly handles thecombinatorial aspects of the problem, provides a more dynamically accuratesolution. These stages are collectively referred to as the SWA-SMT solver. Theyare correct by construction but lack a crucial feature: they cannot be executedin real time. To overcome this, we use SWA-SMT solutions as the initialtraining dataset for our last stage, which aims at obtaining a neural networkcontrol policy. We use reinforcement learning to train the policy, and showthat the initial dataset is crucial for the overall success of the method.", "output": "Layered controller synthesis for dynamic multi-agent systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the last decade, there has been a vast increase in eating disorderdiagnoses and eating disorder-attributed deaths, reaching their zenith duringthe Covid-19 pandemic. This immense growth derived in part from the stressorsof the pandemic but also from increased exposure to social media, which is rifewith content that promotes eating disorders. Such content can induce eatingdisorders in viewers. This study aimed to create a multimodal deep learningmodel capable of determining whether a given social media post promotes eatingdisorders based on a combination of visual and textual data. A labeled datasetof Tweets was collected from Twitter, upon which twelve deep learning modelswere trained and tested. Based on model performance, the most effective deeplearning model was the multimodal fusion of the RoBERTa natural languageprocessing model and the MaxViT image classification model, attaining accuracyand F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusionmodel, deployed to classify an unlabeled dataset of posts from the social mediasites Tumblr and Reddit, generated similar classifications as previous researchstudies that did not employ artificial intelligence, showing that artificialintelligence can develop insights congruent to those of researchers.Additionally, the model was used to conduct a time-series analysis of yetunseen Tweets from eight Twitter hashtags, uncovering that the relativeabundance of pro-eating disorder content has decreased drastically. However,since approximately 2018, pro-eating disorder content has either stopped itsdecline or risen once more in ampleness.", "output": "A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Larger language models, such as GPT-3, have shown to be excellent in manytasks. However, we demonstrate that out-of-ordinary questions can throw themodel off guard. This work focuses on finding answers to negated complementaryquestions in commonsense scenarios. We illustrate how such questions adverselyaffect the model responses. We propose a model-agnostic methodology to improvethe performance in negated complementary scenarios. Our method outperformsfew-shot generation from GPT-3 (by more than 11 points) and, more importantly,highlights the significance of studying the response of large language modelsin negated complementary questions. The code, data, and experiments areavailable under: ", "output": "Negated Complementary Commonsense using Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of Tiny Machine Learning (TinyML) has made substantial advancementsin democratizing machine learning on low-footprint devices, such asmicrocontrollers. The prevalence of these miniature devices raises the questionof whether aggregating their knowledge can benefit TinyML applications.Federated meta-learning is a promising answer to this question, as it addressesthe scarcity of labeled data and heterogeneous data distribution across devicesin the real world. However, deploying TinyML hardware faces unique resourceconstraints, making existing methods impractical due to energy, privacy, andcommunication limitations. We introduce TinyMetaFed, a model-agnosticmeta-learning framework suitable for TinyML. TinyMetaFed facilitatescollaborative training of a neural network initialization that can be quicklyfine-tuned on new devices. It offers communication savings and privacyprotection through partial local reconstruction and Top-P% selectivecommunication, computational efficiency via online learning, and robustness toclient heterogeneity through few-shot learning. The evaluations on three TinyMLuse cases demonstrate that TinyMetaFed can significantly reduce energyconsumption and communication overhead, accelerate convergence, and stabilizethe training process.", "output": "TinyMetaFed: Efficient Federated Meta-Learning for TinyML."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern data-driven science, reproducibility and reusability are keychallenges. Scientists are well skilled in the process from data topublication. Although some publication channels require source code and data tobe made accessible, rerunning and verifying experiments is usually hard due toa lack of standards. Therefore, reusing existing scientific data processingcode from state-of-the-art research is hard as well. This is why we introduceCLAIMED, which has a proven track record in scientific research for addressingthe repeatability and reusability issues in modern data-driven science. CLAIMEDis a framework to build reusable operators and scalable scientific workflows bysupporting the scientist to draw from previous work by re-composing workflowsfrom existing libraries of coarse-grained scientific operators. Althoughvarious implementations exist, CLAIMED is programming language, scientificlibrary, and execution environment agnostic.", "output": "CLAIMED -- the open source framework for building coarse-grained operators for accelerated discovery in science."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization (DG) is about learning models that generalize well tonew domains that are related to, but different from, the training domain(s). Itis a fundamental problem in machine learning and has attracted much attentionin recent years. A large number of approaches have been proposed. Differentapproaches are motivated from different perspectives, making it difficult togain an overall understanding of the area. In this paper, we propose a causalframework for domain generalization and present an understanding of common DGapproaches in the framework. Our work sheds new lights on the followingquestions: (1) What are the key ideas behind each DG method? (2) Why is itexpected to improve generalization to new domains theoretically? (3) How aredifferent DG methods related to each other and what are relative advantages andlimitations? By providing a unified perspective on DG, we hope to helpresearchers better understand the underlying principles and develop moreeffective approaches for this critical problem in machine learning.", "output": "A Causal Framework to Unify Common Domain Generalization Approaches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes exploiting the common sense knowledge learned by largelanguage models to perform zero-shot reasoning about crimes given textualdescriptions of surveillance videos. We show that when video is (manually)converted to high quality textual descriptions, large language models arecapable of detecting and classifying crimes with state-of-the-art performanceusing only zero-shot reasoning. However, existing automated video-to-textapproaches are unable to generate video descriptions of sufficient quality tosupport reasoning (garbage video descriptions into the large language model,garbage out).", "output": "Garbage in, garbage out: Zero-shot detection of crime using Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate 3D sensing of suturing thread is a challenging problem in automatedsurgical suturing because of the high state-space complexity, thinness anddeformability of the thread, and possibility of occlusion by the grippers andtissue. In this work we present a method for tracking surgical thread in 3Dwhich is robust to occlusions and complex thread configurations, and apply itto autonomously perform the surgical suture \"tail-shortening\" task: pullingthread through tissue until a desired \"tail\" length remains exposed. The methodutilizes a learned 2D surgical thread detection network to segment suturingthread in RGB images. It then identifies the thread path in 2D and reconstructsthe thread in 3D as a NURBS spline by triangulating the detections from twostereo cameras. Once a 3D thread model is initialized, the method tracks thethread across subsequent frames. Experiments suggest the method achieves a 1.33pixel average reprojection error on challenging single-frame 3D threadreconstructions, and an 0.84 pixel average reprojection error on two trackingsequences. On the tail-shortening task, it accomplishes a 90% success rateacross 20 trials. Supplemental materials are available at .", "output": "Self-Supervised Learning for Interactive Perception of Surgical Thread for Autonomous Suture Tail-Shortening."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present a novel approach for improving the quality andconsistency of generated outputs from large-scale pre-trained language models(LLMs). Self-consistency has emerged as an effective approach for prompts withfixed answers, selecting the answer with the highest number of votes. In thispaper, we introduce a generalized framework for self-consistency that extendsits applicability beyond problems that have fixed-answer answers. Throughextensive simulations, we demonstrate that our approach consistently recoversthe optimal or near-optimal generation from a set of candidates. We alsopropose lightweight parameter-free similarity functions that show significantand consistent improvements across code generation, autoformalization, andsummarization tasks, even without access to token log probabilities. Our methodincurs minimal computational overhead, requiring no auxiliary reranker modelsor modifications to the existing model.", "output": "Self-consistency for open-ended generations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Strategy of intelligent cognitive control systems based on quantum and softcomputing presented. Quantum self-organization knowledge base synergetic effectextracted from intelligent fuzzy controllers imperfect knowledge basesdescribed. That technology improved of robustness of intelligent cognitivecontrol systems in hazard control situations described with the cognitiveneuro-interface and different types of robot cooperation. Examples demonstratedthe introduction of quantum fuzzy inference gate design as preparedprogrammable algorithmic solution for board embedded control systems. Thepossibility of neuro-interface application based on cognitive helmet withquantum fuzzy controller for driving of the vehicle is shown.", "output": "Unconventional Cognitive Intelligent Robotic Control: Quantum Soft Computing Approach in Human Being Emotion Estimation -- QCOptKB Toolkit Application."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The generations of large language models are commonly controlled throughprompting techniques, where a user's query to the model is prefixed with aprompt that aims to guide the model's behaviour on the query. The prompts usedby companies to guide their models are often treated as secrets, to be hiddenfrom the user making the query. They have even been treated as commodities tobe bought and sold. However, there has been anecdotal evidence showing that theprompts can be extracted by a user even when they are kept secret. In thispaper, we present a framework for systematically measuring the success ofprompt extraction attacks. In experiments with multiple sources of prompts andmultiple underlying language models, we find that simple text-based attacks canin fact reveal prompts with high probability.", "output": "Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing evaluation metrics for natural language generation (NLG) tasks facethe challenges on generalization ability and interpretability. Specifically,most of the well-performed metrics are required to train on evaluation datasetsof specific NLG tasks and evaluation dimensions, which may cause over-fittingto task-specific datasets. Furthermore, existing metrics only provide anevaluation score for each dimension without revealing the evidence to interprethow this score is obtained. To deal with these challenges, we propose a simpleyet effective metric called DecompEval. This metric formulates NLG evaluationas an instruction-style question answering task and utilizes instruction-tunedpre-trained language models (PLMs) without training on evaluation datasets,aiming to enhance the generalization ability. To make the evaluation processmore interpretable, we decompose our devised instruction-style question aboutthe quality of generated texts into the subquestions that measure the qualityof each sentence. The subquestions with their answers generated by PLMs arethen recomposed as evidence to obtain the evaluation result. Experimentalresults show that DecompEval achieves state-of-the-art performance in untrainedmetrics for evaluating text summarization and dialogue generation, which alsoexhibits strong dimension-level / task-level generalization ability andinterpretability.", "output": "DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A robot deployed in a home over long stretches of time faces a true lifelonglearning problem. As it seeks to provide assistance to its users, the robotshould leverage any accumulated experience to improve its own knowledge tobecome a more proficient assistant. We formalize this setting with a novellifelong learning problem formulation in the context of learning for task andmotion planning (TAMP). Exploiting the modularity of TAMP systems, we develop agenerative mixture model that produces candidate continuous parameters for aplanner. Whereas most existing lifelong learning approaches determine a priorihow data is shared across task models, our approach learns shared andnon-shared models and determines which to use online during planning based onauxiliary tasks that serve as a proxy for each model's understanding of astate. Our method exhibits substantial improvements in planning success onsimulated 2D domains and on several problems from the BEHAVIOR benchmark.", "output": "Embodied Lifelong Learning for Task and Motion Planning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of continual learning in the domain of reinforcement learning,often called non-stationary reinforcement learning, has been identified as animportant challenge to the application of reinforcement learning. We prove aworst-case complexity result, which we believe captures this challenge:Modifying the probabilities or the reward of a single state-action pair in areinforcement learning problem requires an amount of time almost as large asthe number of states in order to keep the value function up to date, unless thestrong exponential time hypothesis (SETH) is false; SETH is a widely acceptedstrengthening of the P $neq$ NP conjecture. Recall that the number of statesin current applications of reinforcement learning is typically astronomical. Incontrast, we show that just $textit{adding}$ a new state-action pair isconsiderably easier to implement.", "output": "The complexity of non-stationary reinforcement learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Before deploying a language model (LM) within a given domain, it is importantto measure its tendency to generate factually incorrect information in thatdomain. Existing factual generation evaluation methods focus on facts sampledfrom the LM itself, and thus do not control the set of evaluated facts andmight under-represent rare and unlikely facts. We propose FACTOR: FactualAssessment via Corpus TransfORmation, a scalable approach for evaluating LMfactuality. FACTOR automatically transforms a factual corpus of interest into abenchmark evaluating an LM's propensity to generate true facts from the corpusvs. similar but incorrect statements. We use our framework to create twobenchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scoresincrease with model size and improve when the LM is augmented with retrieval;(ii) benchmark score correlates with perplexity, but the two metrics do notalways agree on model ranking; and (iii) when perplexity and benchmark scoredisagree, the latter better reflects factuality in open-ended generation, asmeasured by human annotators. We make our data and code publicly available in", "output": "Generating Benchmarks for Factuality Evaluation of Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpreting the inner workings of deep learning models is crucial forestablishing trust and ensuring model safety. Concept-based explanations haveemerged as a superior approach that is more interpretable than featureattribution estimates such as pixel saliency. However, defining the conceptsfor the interpretability analysis biases the explanations by the user'sexpectations on the concepts. To address this, we propose a novel post-hocunsupervised method that automatically uncovers the concepts learned by deepmodels during training. By decomposing the latent space of a layer in singularvectors and refining them by unsupervised clustering, we uncover conceptvectors aligned with directions of high variance that are relevant to the modelprediction, and that point to semantically distinct concepts. Our extensiveexperiments reveal that the majority of our concepts are readily understandableto humans, exhibit coherency, and bear relevance to the task at hand. Moreover,we showcase the practical utility of our method in dataset exploration, whereour concept vectors successfully identify outlier training samples affected byvarious confounding factors. This novel exploration technique has remarkableversatility to data types and model architectures and it will facilitate theidentification of biases and the discovery of sources of error within trainingdata.", "output": "Uncovering Unique Concept Vectors through Latent Space Decomposition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge Graphs (KG) provide us with a structured, flexible, transparent,cross-system, and collaborative way of organizing our knowledge and data acrossvarious domains in society and industrial as well as scientific disciplines.KGs surpass any other form of representation in terms of effectiveness.However, Knowledge Graph Engineering (KGE) requires in-depth experiences ofgraph structures, web technologies, existing models and vocabularies, rulesets, logic, as well as best practices. It also demands a significant amount ofwork. Considering the advancements in large language models (LLMs) and theirinterfaces and applications in recent years, we have conducted comprehensiveexperiments with ChatGPT to explore its potential in supporting KGE. In thispaper, we present a selection of these experiments and their results todemonstrate how ChatGPT can assist us in the development and management of KGs.", "output": "LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Persons with visual impairments (PwVI) have difficulties understanding andnavigating spaces around them. Current wayfinding technologies either focussolely on navigation or provide limited communication about the environment.Motivated by recent advances in visual-language grounding and semanticnavigation, we propose DRAGON, a guiding robot powered by a dialogue system andthe ability to associate the environment with natural language. Byunderstanding the commands from the user, DRAGON is able to guide the user tothe desired landmarks on the map, describe the environment, and answerquestions from visual observations. Through effective utilization of dialogue,the robot can ground the user's free-form descriptions to landmarks in theenvironment, and give the user semantic information through spoken language. Weconduct a user study with blindfolded participants in an everyday indoorenvironment. Our results demonstrate that DRAGON is able to communicate withthe user smoothly, provide a good guiding experience, and connect users withtheir surrounding environment in an intuitive manner.", "output": "DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL)can enhance model adaptation by leveraging more sensitive and distributed datawhile preserving data privacy. However, few studies have focused on thismethod. Therefore, we conduct the first comprehensive empirical study toevaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). Wedemonstrate that FDAPT can maintain competitive downstream task performance tothe centralized baseline in both IID and non-IID situations. Furthermore, wepropose a novel algorithm, Frozen Federated Domain-adaptive Pre-training(FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average andexhibits similar downstream task performance to standard FDAPT, with generalperformance fluctuations remaining less than 1%. Finally, through a criticalevaluation of our work, we identify promising future research directions forthis new research area.", "output": "FDAPT: Federated Domain-adaptive Pre-training for Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable Artificial Intelligence (XAI) has received widespread interest inrecent years, and two of the most popular types of explanations are featureattributions, and counterfactual explanations. These classes of approaches havebeen largely studied independently and the few attempts at reconciling themhave been primarily empirical. This work establishes a clear theoreticalconnection between game-theoretic feature attributions, focusing on but notlimited to SHAP, and counterfactuals explanations. After motivating operativechanges to Shapley values based feature attributions and counterfactualexplanations, we prove that, under conditions, they are in fact equivalent. Wethen extend the equivalency result to game-theoretic solution concepts beyondShapley values. Moreover, through the analysis of the conditions of suchequivalence, we shed light on the limitations of naively using counterfactualexplanations to provide feature importances. Experiments on three datasetsquantitatively show the difference in explanations at every stage of theconnection between the two approaches and corroborate the theoretical findings.", "output": "On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose the In-context Autoencoder (ICAE) for context compression in alarge language model (LLM). The ICAE has two modules: a learnable encoderadapted with LoRA from an LLM for compressing a long context into a limitednumber of memory slots, and a fixed decoder which is the target LLM that cancondition on the memory slots for various purposes. We first pretrain the ICAEusing both autoencoding and language modeling objectives on massive text data,enabling it to generate memory slots that accurately and comprehensivelyrepresent the original context. Then, we fine-tune the pretrained ICAE on asmall amount of instruct data to enhance its interaction with various promptsfor producing desirable responses. Our experimental results demonstrate thatthe ICAE learned with our proposed pretraining and fine-tuning paradigm caneffectively produce memory slots with $4times$ context compression, which canbe well conditioned on by the target LLM to respond to various prompts. Thepromising results demonstrate significant implications of the ICAE for itsnovel approach to the long context problem and its potential to reducecomputation and memory overheads for LLM inference in practice, suggestingfurther research effort in context management for an LLM. Our code and datawill be released shortly.", "output": "In-context Autoencoder for Context Compression in a Large Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on three large-scale datasets(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Ourcode/models are released at ", "output": "Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Personalization has emerged as a prominent aspect within the field ofgenerative AI, enabling the synthesis of individuals in diverse contexts andstyles, while retaining high-fidelity to their identities. However, the processof personalization presents inherent challenges in terms of time and memoryrequirements. Fine-tuning each personalized model needs considerable GPU timeinvestment, and storing a personalized model per subject can be demanding interms of storage capacity. To overcome these challenges, we proposeHyperDreamBooth-a hypernetwork capable of efficiently generating a small set ofpersonalized weights from a single image of a person. By composing theseweights into the diffusion model, coupled with fast finetuning, HyperDreamBoothcan generate a person's face in various contexts and styles, with high subjectdetails while also preserving the model's crucial knowledge of diverse stylesand semantic modifications. Our method achieves personalization on faces inroughly 20 seconds, 25x faster than DreamBooth and 125x faster than TextualInversion, using as few as one reference image, with the same quality and stylediversity as DreamBooth. Also our method yields a model that is 10000x smallerthan a normal DreamBooth model. Project page: ", "output": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Regulation of Multi-Agent Systems (MAS) and Declarative ElectronicInstitutions (DEIs) was a multidisciplinary research topic of the past decadeinvolving (Physical and Software) Agents and Law since the beginning, butrecently evolved towards News-claimed Robot Lawyer since 2016. One of thesefirst proposals of restricting the behaviour of Software Agentswas ElectronicInstitutions.However, with the recent reformulation of Artificial NeuralNetworks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legalissues regarding the use of DL has raised concerns in the ArtificialIntelligence (AI) Community. Now that the Regulation of MAS is almost correctlyaddressed, we propose the Regulation of Artificial Neural Networks asAgent-based Training of a special type of regulated Artificial Neural Networkthat we call Institutional Neural Network (INN).The main purpose of this paperis to bring attention to Artificial Teaching (AT) and to give a tentativeanswer showing a proof-of-concept implementation of Regulated Deep Learning(RDL). This paper introduces the former concept and provide sI, a languagepreviously used to model declaratively and extend Electronic Institutions, as ameans to regulate the execution of Artificial Neural Networks and theirinteractions with Artificial Teachers (ATs)", "output": "Declarative Mechanism Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many coordination problems, independently reasoning humans are able todiscover mutually compatible policies. In contrast, independently trainedself-play policies are often mutually incompatible. Zero-shot coordination(ZSC) has recently been proposed as a new frontier in multi-agent reinforcementlearning to address this fundamental issue. Prior work approaches the ZSCproblem by assuming players can agree on a shared learning algorithm but not onlabels for actions and observations, and proposes other-play as an optimalsolution. However, until now, this \"label-free\" problem has only beeninformally defined. We formalize this setting as the label-free coordination(LFC) problem by defining the label-free coordination game. We show thatother-play is not an optimal solution to the LFC problem as it fails toconsistently break ties between incompatible maximizers of the other-playobjective. We introduce an extension of the algorithm, other-play withtie-breaking, and prove that it is optimal in the LFC problem and anequilibrium in the LFC game. Since arbitrary tie-breaking is precisely what theZSC setting aims to prevent, we conclude that the LFC problem does not reflectthe aims of ZSC. To address this, we introduce an alternative informaloperationalization of ZSC as a starting point for future work.", "output": "A New Formalism, Method and Open Issues for Zero-Shot Coordination."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Navigating a large-scaled robot in unknown and cluttered height-constrainedenvironments is challenging. Not only is a fast and reliable planning algorithmrequired to go around obstacles, the robot should also be able to change itsintrinsic dimension by crouching in order to travel underneathheight-constrained regions. There are few mobile robots that are capable ofhandling such a challenge, and bipedal robots provide a solution. However, asbipedal robots have nonlinear and hybrid dynamics, trajectory planning whileensuring dynamic feasibility and safety on these robots is challenging. Thispaper presents an end-to-end autonomous navigation framework which leveragesthree layers of planners and a variable walking height controller to enablebipedal robots to safely explore height-constrained environments. Avertically-actuated Spring-Loaded Inverted Pendulum (vSLIP) model is introducedto capture the robot's coupled dynamics of planar walking and vertical walkingheight. This reduced-order model is utilized to optimize for long-term andshort-term safe trajectory plans. A variable walking height controller isleveraged to enable the bipedal robot to maintain stable periodic walking gaitswhile following the planned trajectory. The entire framework is tested andexperimentally validated using a bipedal robot Cassie. This demonstratesreliable autonomy to drive the robot to safely avoid obstacles while walking tothe goal location in various kinds of height-constrained clutteredenvironments.", "output": "Autonomous Navigation of Underactuated Bipedal Robots in Height-Constrained Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The capability of Deep Neural Networks (DNNs) to recognize objects inorientations outside the distribution of the training data is not wellunderstood. We present evidence that DNNs are capable of generalizing toobjects in novel orientations by disseminating orientation-invariance obtainedfrom familiar objects seen from many viewpoints. This capability strengthenswhen training the DNN with an increasing number of familiar objects, but onlyin orientations that involve 2D rotations of familiar orientations. We showthat this dissemination is achieved via neurons tuned to common featuresbetween familiar and unfamiliar objects. These results implicate brain-likeneural mechanisms for generalization.", "output": "Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning is a process which can update decision rules, based on pastexperience, such that future performance improves. Traditionally, machinelearning is often evaluated under the assumption that the future will beidentical to the past in distribution or change adversarially. But theseassumptions can be either too optimistic or pessimistic for many problems inthe real world. Real world scenarios evolve over multiple spatiotemporal scaleswith partially predictable dynamics. Here we reformulate the learning problemto one that centers around this idea of dynamic futures that are partiallylearnable. We conjecture that certain sequences of tasks are notretrospectively learnable (in which the data distribution is fixed), but areprospectively learnable (in which distributions may be dynamic), suggestingthat prospective learning is more difficult in kind than retrospectivelearning. We argue that prospective learning more accurately characterizes manyreal world problems that (1) currently stymie existing artificial intelligencesolutions and/or (2) lack adequate explanations for how natural intelligencessolve them. Thus, studying prospective learning will lead to deeper insightsand solutions to currently vexing challenges in both natural and artificialintelligences.", "output": "Prospective Learning: Principled Extrapolation to the Future."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deepfake detection methods based on convolutional neural networks (CNN) havedemonstrated high accuracy. textcolor{black}{However, these methods oftensuffer from decreased performance when faced with unknown forgery methods andcommon transformations such as resizing and blurring, resulting in deviationsbetween training and testing domains.} This phenomenon, known as overfitting,poses a significant challenge. To address this issue, we propose a novel blockshuffling regularization method. Firstly, our approach involves dividing theimages into blocks and applying both intra-block and inter-block shufflingtechniques. This process indirectly achieves weight-sharing across differentdimensions. Secondly, we introduce an adversarial loss algorithm to mitigatethe overfitting problem induced by the shuffling noise. Finally, we restore thespatial layout of the blocks to capture the semantic associations among them.Extensive experiments validate the effectiveness of our proposed method, whichsurpasses existing approaches in forgery face detection. Notably, our methodexhibits excellent generalization capabilities, demonstrating robustnessagainst cross-dataset evaluations and common image transformations. Especiallyour method can be easily integrated with various CNN models. Source code isavailable athref{", "output": "Block shuffling learning for Deepfake Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In social choice theory, anonymity (all agents being treated equally) andneutrality (all alternatives being treated equally) are widely regarded as``minimal demands'' and ``uncontroversial'' axioms of equity and fairness.However, the ANR impossibility -- there is no voting rule that satisfiesanonymity, neutrality, and resolvability (always choosing one winner) -- holdseven in the simple setting of two alternatives and two agents. How to designvoting rules that optimally satisfy anonymity, neutrality, and resolvabilityremains an open question.We address the optimal design question for a wide range of preferences anddecisions that include ranked lists and committees. Our conceptual contributionis a novel and strong notion of most equitable refinements that optimallypreserves anonymity and neutrality for any irresolute rule that satisfies thetwo axioms. Our technical contributions are twofold. First, we characterize theconditions for the ANR impossibility to hold under general settings, especiallywhen the number of agents is large. Second, we propose themost-favorable-permutation (MFP) tie-breaking to compute a most equitablerefinement and design a polynomial-time algorithm to compute MFP when agents'preferences are full rankings.", "output": "Most Equitable Voting Rules."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Drawing from memory the face of a friend you have not seen in years is adifficult task. However, if you happen to cross paths, you would easilyrecognize each other. The biological memory is equipped with an impressivecompression algorithm that can store the essential, and then infer the detailsto match perception. The Willshaw Memory is a simple abstract model forcortical computations which implements mechanisms of biological memories. Usingour recently proposed sparse coding prescription for visual patterns, thismodel can store and retrieve an impressive amount of real-world data in afault-tolerant manner. In this paper, we extend the capabilities of the basicAssociative Memory Model by using a Multiple-Modality framework. In thissetting, the memory stores several modalities (e.g., visual, or textual) ofeach pattern simultaneously. After training, the memory can be used to infermissing modalities when just a subset is perceived. Using a simpleencoder-memory-decoder architecture, and a newly proposed iterative retrievalalgorithm for the Willshaw Model, we perform experiments on the MNIST dataset.By storing both the images and labels as modalities, a single Memory can beused not only to retrieve and complete patterns but also to classify andgenerate new ones. We further discuss how this model could be used for otherlearning tasks, thus serving as a biologically-inspired framework for learning.", "output": "Classification and Generation of real-world data with an Associative Memory Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continuous monitoring of trained ML models to determine when theirpredictions should and should not be trusted is essential for their safedeployment. Such a framework ought to be high-performing, explainable, post-hocand actionable. We propose TRUST-LAPSE, a \"mistrust\" scoring framework forcontinuous model monitoring. We assess the trustworthiness of each inputsample's model prediction using a sequence of latent-space embeddings.Specifically, (a) our latent-space mistrust score estimates mistrust usingdistance metrics (Mahalanobis distance) and similarity metrics (cosinesimilarity) in the latent-space and (b) our sequential mistrust scoredetermines deviations in correlations over the sequence of past inputrepresentations in a non-parametric, sliding-window based algorithm foractionable continuous monitoring. We evaluate TRUST-LAPSE via two downstreamtasks: (1) distributionally shifted input detection, and (2) data driftdetection. We evaluate across diverse domains - audio and vision using publicdatasets and further benchmark our approach on challenging, real-worldelectroencephalograms (EEG) datasets for seizure detection. Our latent-spacemistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision),73.9 (audio), and 77.1 (clinical EEGs), outperforming baselines by over 10points. We expose critical failures in popular baselines that remaininsensitive to input semantic content, rendering them unfit for real-worldmodel monitoring. We show that our sequential mistrust scores achieve highdrift detection rates; over 90% of the streams show &lt; 20% error for alldomains. Through extensive qualitative and quantitative evaluations, we showthat our mistrust scores are more robust and provide explainability for easyadoption into practice.", "output": "TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has emerged as a promising alternative tocreate supervisory signals to real-world problems, avoiding the extensive costof manual labeling. SSL is particularly attractive for unsupervised tasks suchas anomaly detection (AD), where labeled anomalies are rare or oftennonexistent. A large catalog of augmentation functions has been used forSSL-based AD (SSAD) on image data, and recent works have reported that the typeof augmentation has a significant impact on accuracy. Motivated by those, thiswork sets out to put image-based SSAD under a larger lens and investigate therole of data augmentation in SSAD. Through extensive experiments on 3 differentdetector models and across 420 AD tasks, we provide comprehensive numerical andvisual evidences that the alignment between data augmentation andanomaly-generating mechanism is the key to the success of SSAD, and in the lackthereof, SSL may even impair accuracy. To the best of our knowledge, this isthe first meta-analysis on the role of data augmentation in SSAD.", "output": "Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the adaption of soft actor-critic (SAC) from continuous action spaceto discrete action space. We revisit vanilla SAC and provide an in-depthunderstanding of its Q value underestimation and performance instability issueswhen applied to discrete settings. We thereby propose entropy-penalty anddouble average Q-learning with Q-clip to address these issues. Extensiveexperiments on typical benchmarks with discrete action space, including Atarigames and a large-scale MOBA game, show the efficacy of our proposed method.Our code is at:", "output": "Revisiting Discrete Soft Actor-Critic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graph (KG) reasoning is an important problem for knowledge graphs.In this paper, we propose a novel and principled framework called textbf{RulE}(stands for {Rul}e {E}mbedding) to effectively leverage logical rules toenhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulElearns rule embeddings from existing triplets and first-order {rules} byjointly representing textbf{entities}, textbf{relations} and textbf{logicalrules} in a unified embedding space. Based on the learned rule embeddings, aconfidence score can be calculated for each rule, reflecting its consistencywith the observed triplets. This allows us to perform logical rule inference ina soft way, thus alleviating the brittleness of logic. On the other hand, RulEinjects prior logical rule information into the embedding space, enriching andregularizing the entity/relation embeddings. This makes KGE alone performbetter too. RulE is conceptually simple and empirically effective. We conductextensive experiments to verify each component of RulE. Results on multiplebenchmarks reveal that our model outperforms the majority of existingembedding-based and rule-based approaches.", "output": "RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We attack the state-of-the-art Go-playing AI system KataGo by trainingadversarial policies against it, achieving a &gt;97% win rate against KataGorunning at superhuman settings. Our adversaries do not win by playing Go well.Instead, they trick KataGo into making serious blunders. Our attack transferszero-shot to other superhuman Go-playing AIs, and is comprehensible to theextent that human experts can implement it without algorithmic assistance toconsistently beat superhuman AIs. The core vulnerability uncovered by ourattack persists even in KataGo agents adversarially trained to defend againstour attack. Our results demonstrate that even superhuman AI systems may harborsurprising failure modes. Example games are available ", "output": "Adversarial Policies Beat Superhuman Go AIs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning long-horizon tasks such as navigation has presented difficultchallenges for successfully applying reinforcement learning to robotics. Fromanother perspective, under known environments, sampling-based planning canrobustly find collision-free paths in environments without learning. In thiswork, we propose Control Transformer that models return-conditioned sequencesfrom low-level policies guided by a sampling-based Probabilistic Roadmap (PRM)planner. We demonstrate that our framework can solve long-horizon navigationtasks using only local information. We evaluate our approach onpartially-observed maze navigation with MuJoCo robots, including Ant, Point,and Humanoid. We show that Control Transformer can successfully navigatethrough mazes and transfer to unknown environments. Additionally, we apply ourmethod to a differential drive robot (Turtlebot3) and show zero-shot sim2realtransfer under noisy observations.", "output": "Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "World models power some of the most efficient reinforcement learningalgorithms. In this work, we showcase that they can be harnessed for continuallearning - a situation when the agent faces changing environments. World modelstypically employ a replay buffer for training, which can be naturally extendedto continual learning. We systematically study how different selectiveexperience replay methods affect performance, forgetting, and transfer. We alsoprovide recommendations regarding various modeling options for using worldmodels. The best set of choices is called Continual-Dreamer, it istask-agnostic and utilizes the world model for continual exploration.Continual-Dreamer is sample efficient and outperforms state-of-the-arttask-agnostic continual reinforcement learning methods on Minigrid and Minihackbenchmarks.", "output": "The Effectiveness of World Models for Continual Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated synthesis of provably correct controllers for cyber-physicalsystems is crucial for deployment in safety-critical scenarios. However, hybridfeatures and stochastic or unknown behaviours make this problem challenging. Wepropose a method for synthesising controllers for Markov jump linear systems(MJLSs), a class of discrete-time models for cyber-physical systems, so thatthey certifiably satisfy probabilistic computation tree logic (PCTL) formulae.An MJLS consists of a finite set of stochastic linear dynamics and discretejumps between these dynamics that are governed by a Markov decision process(MDP). We consider the cases where the transition probabilities of this MDP areeither known up to an interval or completely unknown. Our approach is based ona finite-state abstraction that captures both the discrete (mode-jumping) andcontinuous (stochastic linear) behaviour of the MJLS. We formalise thisabstraction as an interval MDP (iMDP) for which we compute intervals oftransition probabilities using sampling techniques from the so-called 'scenarioapproach', resulting in a probabilistically sound approximation. We apply ourmethod to multiple realistic benchmark problems, in particular, a temperaturecontrol and an aerial vehicle delivery problem.", "output": "Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer has been considered the dominating neural architecture in NLP andCV, mostly under supervised settings. Recently, a similar surge of usingTransformers has appeared in the domain of reinforcement learning (RL), but itis faced with unique design choices and challenges brought by the nature of RL.However, the evolution of Transformers in RL has not yet been well unraveled.In this paper, we seek to systematically review motivations and progress onusing Transformers in RL, provide a taxonomy on existing works, discuss eachsub-field, and summarize future prospects.", "output": "A Survey on Transformers in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many practical fluid dynamics experiments, measuring variables such asvelocity and pressure is possible only at a limited number of sensor locations,textcolor{black}{for a few two-dimensional planes, or for a small 3D domain inthe flow}. However, knowledge of the full fields is necessary to understand thedynamics of many flows. Deep learning reconstruction of full flow fields fromsparse measurements has recently garnered significant research interest, as away of overcoming this limitation. This task is referred to as the flowreconstruction (FR) task. In the present study, we propose a convolutionalautoencoder based neural network model, dubbed FR3D, which enables FR to becarried out for three-dimensional flows around extruded 3D objects withdifferent cross-sections. An innovative mapping approach, whereby multiplefluid domains are mapped to an annulus, enables FR3D to generalize itsperformance to objects not encountered during training. We conclusivelydemonstrate this generalization capability using a dataset composed of 80training and 20 testing geometries, all randomly generated. We show that theFR3D model reconstructs pressure and velocity components with a few percentagepoints of error. Additionally, using these predictions, we accurately estimatethe Q-criterion fields as well lift and drag forces on the geometries.", "output": "FR3D: Three-dimensional Flow Reconstruction and Force Estimation for Unsteady Flows Around Extruded Bluff Bodies via Conformal Mapping Aided Convolutional Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an end-to-end deep learning approach to rectify fisheye images andsimultaneously calibrate camera intrinsic and distortion parameters. Our methodconsists of two parts: a Quick Image Rectification Module developed with aPix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with aCNN architecture. Our Quick Rectification Network performs robust rectificationwith good resolution, making it suitable for constant calibration incamera-based surveillance equipment. To achieve high-quality calibration, weuse the straightened output from the Quick Rectification Module as aguidance-like semantic feature map for the Calibration Module to learn thegeometric relationship between the straightened feature and the distortedfeature. We train and validate our method with a large synthesized datasetlabeled with well-simulated parameters applied to a perspective image dataset.Our solution has achieved robust performance in high-resolution with asignificant PSNR value of 22.343.", "output": "FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trust evaluation assesses trust relationships between entities andfacilitates decision-making. Machine Learning (ML) shows great potential fortrust evaluation owing to its learning capabilities. In recent years, GraphNeural Networks (GNNs), as a new ML paradigm, have demonstrated superiority indealing with graph data. This has motivated researchers to explore their use intrust evaluation, as trust relationships among entities can be modeled as agraph. However, current trust evaluation methods that employ GNNs fail to fullysatisfy the dynamic nature of trust, overlook the adverse effects of attacks ontrust evaluation, and cannot provide convincing explanations on evaluationresults. To address these problems, we propose TrustGuard, a GNN-based accuratetrust evaluation model that supports trust dynamicity, is robust againsttypical attacks, and provides explanations through visualization. Specifically,TrustGuard is designed with a layered architecture that contains a snapshotinput layer, a spatial aggregation layer, a temporal aggregation layer, and aprediction layer. Among them, the spatial aggregation layer adopts a defensemechanism to robustly aggregate local trust, and the temporal aggregation layerapplies an attention mechanism for effective learning of temporal patterns.Extensive experiments on two real-world datasets show that TrustGuardoutperforms state-of-the-art GNN-based trust evaluation models with respect totrust prediction across single-timeslot and multi-timeslot, even in thepresence of attacks. In addition, TrustGuard can explain its evaluation resultsby visualizing both spatial and temporal views.", "output": "TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models(LLMS) have shown excellent text generationcapabilities,capable of generating fluent responses for many downstream tasks.However,applying large language models to real-world critical tasks remainschallenging due to their susceptibility to hallucinations and inability todirectly use external knowledge. To address the above challenges,this paperproposes PatternGPT, a pattern-driven text generation framework for largelanguage models. First,the framework utilizes the extraction capabilities oflarge language models to generate rich and diverse patterns and later draws onthe idea of federated learning. Using multiple agents to achieve sharing toobtain more diverse patterns. Finally, it searches for high-quality patternsusing judgment criteria and optimization algorithms and uses the searchedpatterns to guide the model for generation. This framework has the advantagesof generating diversified patterns, protecting data privacy,combining externalknowledge, and improving the quality of generation, which provides an effectivemethod to optimize the text generation capability of large language models,andmake it better applied to the field of intelligent dialogue and contentgeneration.", "output": "PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The dynamic nature of driving environments and the presence of diverse roadusers pose significant challenges for decision-making in autonomous driving.Deep reinforcement learning (DRL) has emerged as a popular approach to tacklethis problem. However, the application of existing DRL solutions is mainlyconfined to simulated environments due to safety concerns, impeding theirdeployment in real-world. To overcome this limitation, this paper introduces anovel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics(DRLSL) that combines the strengths of DRL (learning from experience) andsymbolic first-order logics (knowledge-driven reasoning) to enable safelearning in real-time interactions of autonomous driving within realenvironments. This innovative approach provides a means to learn autonomousdriving policies by actively engaging with the physical environment whileensuring safety. We have implemented the DRLSL framework in autonomous drivingusing the highD dataset and demonstrated that our method successfully avoidsunsafe actions during both the training and testing phases. Furthermore, ourresults indicate that DRLSL achieves faster convergence during training andexhibits better generalizability to new driving scenarios compared totraditional DRL methods.", "output": "Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) are gaining increasing popularity in bothacademia and industry, owing to their unprecedented performance in variousapplications. As LLMs continue to play a vital role in both research and dailyuse, their evaluation becomes increasingly critical, not only at the tasklevel, but also at the society level for better understanding of theirpotential risks. Over the past years, significant efforts have been made toexamine LLMs from various perspectives. This paper presents a comprehensivereview of these evaluation methods for LLMs, focusing on three key dimensions:what to evaluate, where to evaluate, and how to evaluate. Firstly, we providean overview from the perspective of evaluation tasks, encompassing generalnatural language processing tasks, reasoning, medical usage, ethics,educations, natural and social sciences, agent applications, and other areas.Secondly, we answer the `where' and `how' questions by diving into theevaluation methods and benchmarks, which serve as crucial components inassessing performance of LLMs. Then, we summarize the success and failure casesof LLMs in different tasks. Finally, we shed light on several future challengesthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights toresearchers in the realm of LLMs evaluation, thereby aiding the development ofmore proficient LLMs. Our key point is that evaluation should be treated as anessential discipline to better assist the development of LLMs. We consistentlymaintain the related open-source materials at:", "output": "A Survey on Evaluation of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To realize the potential benefits and mitigate potential risks of AI, it isnecessary to develop a framework of governance that conforms to ethics andfundamental human values. Although several organizations have issued guidelinesand ethical frameworks for trustworthy AI, without a mediating governancestructure, these ethical principles will not translate into practice. In thispaper, we propose a multilevel governance approach that involves three groupsof interdependent stakeholders: governments, corporations, and citizens. Weexamine their interrelationships through dimensions of trust, such ascompetence, integrity, and benevolence. The levels of governance combined withthe dimensions of trust in AI provide practical insights that can be used tofurther enhance user experiences and inform public policy related to AI.", "output": "A multilevel framework for AI governance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supply chain operations traditionally involve a variety of complex decisionmaking problems. Over the last few decades, supply chains greatly benefitedfrom advances in computation, which allowed the transition from manualprocessing to automation and cost-effective optimization. Nonetheless, businessoperators still need to spend substantial efforts in explaining andinterpreting the optimization outcomes to stakeholders. Motivated by the recentadvances in Large Language Models (LLMs), we study how this disruptivetechnology can help bridge the gap between supply chain automation and humancomprehension and trust thereof. We design OptiGuide -- a framework thataccepts as input queries in plain text, and outputs insights about theunderlying optimization outcomes. Our framework does not forgo thestate-of-the-art combinatorial optimization technology, but rather leverages itto quantitatively answer what-if scenarios (e.g., how would the cost change ifwe used supplier B instead of supplier A for a given demand?). Importantly, ourdesign does not require sending proprietary data over to LLMs, which can be aprivacy concern in some circumstances. We demonstrate the effectiveness of ourframework on a real server placement scenario within Microsoft's cloud supplychain. Along the way, we develop a general evaluation benchmark, which can beused to evaluate the accuracy of the LLM output in other scenarios.", "output": "Large Language Models for Supply Chain Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Radiology reporting is a crucial part of the communication betweenradiologists and other medical professionals, but it can be time-consuming anderror-prone. One approach to alleviate this is structured reporting, whichsaves time and enables a more accurate evaluation than free-text reports.However, there is limited research on automating structured reporting, and nopublic benchmark is available for evaluating and comparing different methods.To close this gap, we introduce Rad-ReStruct, a new benchmark dataset thatprovides fine-grained, hierarchically ordered annotations in the form ofstructured reports for X-Ray images. We model the structured reporting task ashierarchical visual question answering (VQA) and propose hi-VQA, a novel methodthat considers prior context in the form of previously asked questions andanswers for populating a structured radiology report. Our experiments show thathi-VQA achieves competitive performance to the state-of-the-art on the medicalVQA benchmark VQARad while performing best among methods withoutdomain-specific vision-language pretraining and provides a strong baseline onRad-ReStruct. Our work represents a significant step towards the automatedpopulation of structured radiology reports and provides a valuable firstbenchmark for future research in this area. We will make all annotations andour code for annotation generation, model evaluation, and training publiclyavailable upon acceptance. Our dataset and code is available at", "output": "Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated radiology report generation aims to generate radiology reports thatcontain rich, fine-grained descriptions of radiology imaging. Compared withimage captioning in the natural image domain, medical images are very similarto each other, with only minor differences in the occurrence of diseases. Giventhe importance of these minor differences in the radiology report, it iscrucial to encourage the model to focus more on the subtle regions of diseaseoccurrence. Secondly, the problem of visual and textual data biases is serious.Not only do normal cases make up the majority of the dataset, but sentencesdescribing areas with pathological changes also constitute only a small part ofthe paragraph. Lastly, generating medical image reports involves the challengeof long text generation, which requires more expertise and empirical trainingin medical knowledge. As a result, the difficulty of generating such reports isincreased. To address these challenges, we propose a disease-oriented retrievalframework that utilizes similar reports as prior knowledge references. Wedesign a factual consistency captioning generator to generate more accurate andfactually consistent disease descriptions. Our framework can find most similarreports for a given disease from the CXR database by retrieving adisease-oriented mask consisting of the position and morphologicalcharacteristics. By referencing the disease-oriented similar report and thevisual features, the factual consistency model can generate a more accurateradiology report.", "output": "Reading Radiology Imaging Like The Radiologist."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, a denoising Cycle-GAN (Cycle Consistent Generative AdversarialNetwork) is implemented to yield high-field, high resolution, highsignal-to-noise ratio (SNR) Magnetic Resonance Imaging (MRI) images fromsimulated low-field, low resolution, low SNR MRI images. Resampling andadditive Rician noise were used to simulate low-field MRI. Images were utilizedto train a Denoising Autoencoder (DAE) and a Cycle-GAN, with paired andunpaired cases. Both networks were evaluated using SSIM and PSNR image qualitymetrics. This work demonstrates the use of a generative deep learning modelthat can outperform classical DAEs to improve low-field MRI images and does notrequire image pairs.", "output": "Denoising Simulated Low-Field MRI (70mT) using Denoising Autoencoders (DAE) and Cycle-Consistent Generative Adversarial Networks (Cycle-GAN)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the last few years, neural image compression has gained wide attentionfrom research and industry, yielding promising end-to-end deep neural codecsoutperforming their conventional counterparts in rate-distortion performance.Despite significant advancement, current methods, including attention-basedtransform coding, still need to be improved in reducing the coding rate whilepreserving the reconstruction fidelity, especially in non-homogeneous texturedimage areas. Those models also require more parameters and a higher decodingtime. To tackle the above challenges, we propose ConvNeXt-ChARM, an efficientConvNeXt-based transform coding framework, paired with a compute-efficientchannel-wise auto-regressive prior to capturing both global and local contextsfrom the hyper and quantized latent representations. The proposed architecturecan be optimized end-to-end to fully exploit the context information andextract compact latent representation while reconstructing higher-qualityimages. Experimental results on four widely-used datasets showed thatConvNeXt-ChARM brings consistent and significant BD-rate (PSNR) reductionsestimated on average to 5.24% and 1.22% over the versatile video coding (VVC)reference encoder (VTM-18.0) and the state-of-the-art learned image compressionmethod SwinT-ChARM, respectively. Moreover, we provide model scaling studies toverify the computational efficiency of our approach and conduct severalobjective and subjective analyses to bring to the fore the performance gapbetween the next generation ConvNet, namely ConvNeXt, and Swin Transformer.", "output": "ConvNeXt-ChARM: ConvNeXt-based Transform for Efficient Neural Image Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In X-ray Computed Tomography (CT), projections from many angles are acquiredand used for 3D reconstruction. To make CT suitable for in-line qualitycontrol, reducing the number of angles while maintaining reconstruction qualityis necessary. Sparse-angle tomography is a popular approach for obtaining 3Dreconstructions from limited data. To optimize its performance, one can adaptscan angles sequentially to select the most informative angles for each scannedobject. Mathematically, this corresponds to solving and optimal experimentaldesign (OED) problem. OED problems are high-dimensional, non-convex, bi-leveloptimization problems that cannot be solved online, i.e., during the scan. Toaddress these challenges, we pose the OED problem as a partially observableMarkov decision process in a Bayesian framework, and solve it through deepreinforcement learning. The approach learns efficient non-greedy policies tosolve a given class of OED problems through extensive offline training ratherthan solving a given OED problem directly via numerical optimization. As such,the trained policy can successfully find the most informative scan anglesonline. We use a policy training method based on the Actor-Critic approach andevaluate its performance on 2D tomography with synthetic data.", "output": "Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to its superior efficiency in utilizing annotations and addressinggigapixel-sized images, multiple instance learning (MIL) has shown greatpromise as a framework for whole slide image (WSI) classification in digitalpathology diagnosis. However, existing methods tend to focus on advancedaggregators with different structures, often overlooking the intrinsic featuresof H&amp;E pathological slides. To address this limitation, we introduced twopathological priors: nuclear heterogeneity of diseased cells and spatialcorrelation of pathological tiles. Leveraging the former, we proposed a dataaugmentation method that utilizes stain separation during extractor trainingvia a contrastive learning strategy to obtain instance-level representations.We then described the spatial relationships between the tiles using anadjacency matrix. By integrating these two views, we designed a multi-instanceframework for analyzing H&amp;E-stained tissue images based on pathologicalinductive bias, encompassing feature extraction, filtering, and aggregation.Extensive experiments on the Camelyon16 breast dataset and TCGA-NSCLC Lungdataset demonstrate that our proposed framework can effectively handle tasksrelated to cancer detection and differentiation of subtypes, outperformingstate-of-the-art medical image classification methods based on MIL. The codewill be released later.", "output": "The Whole Pathological Slide Classification via Weakly Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the stunning ability to generate high-quality images by recenttext-to-image models, current approaches often struggle to effectively composeobjects with different attributes and relationships into a complex and coherentscene. We propose T2I-CompBench, a comprehensive benchmark for open-worldcompositional text-to-image generation, consisting of 6,000 compositional textprompts from 3 categories (attribute binding, object relationships, and complexcompositions) and 6 sub-categories (color binding, shape binding, texturebinding, spatial relationships, non-spatial relationships, and complexcompositions). We further propose several evaluation metrics specificallydesigned to evaluate compositional text-to-image generation. We introduce a newapproach, Generative mOdel fine-tuning with Reward-driven Sample selection(GORS), to boost the compositional text-to-image generation abilities ofpretrained text-to-image models. Extensive experiments and evaluations areconducted to benchmark previous methods on T2I-CompBench, and to validate theeffectiveness of our proposed evaluation metrics and GORS approach. Projectpage is available at ", "output": "T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Audio-Visual Event Localization (AVEL) is the task of temporally localizingand classifying emph{audio-visual events}, i.e., events simultaneously visibleand audible in a video. In this paper, we solve AVEL in a weakly-supervisedsetting, where only video-level event labels (their presence/absence, but nottheir locations in time) are available as supervision for training. Our idea isto use a base model to estimate labels on the training data at a finer temporalresolution than at the video level and re-train the model with these labels.I.e., we determine the subset of labels for each emph{slice} of frames in atraining video by (i) replacing the frames outside the slice with those from asecond video having no overlap in video-level labels, and (ii) feeding thissynthetic video into the base model to extract labels for just the slice inquestion. To handle the out-of-distribution nature of our synthetic videos, wepropose an auxiliary objective for the base model that induces more reliablepredictions of the localized event labels as desired. Our three-stage pipelineoutperforms several existing AVEL methods with no architectural changes andimproves performance on a related weakly-supervised task as well.", "output": "Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a deep learning-based method to segment theskeletal structures in the micro-CT images of Dutch-Belted rabbit fetuses whichcan assist in the assessment of drug-induced skeletal abnormalities as arequired study in developmental and reproductive toxicology (DART). Ourstrategy leverages sub-optimal segmentation labels of 22 skull bones from 26micro-CT volumes and maps them to 250 unlabeled volumes on which a deepCNN-based segmentation model is trained. In the experiments, our model was ableto achieve an average Dice Similarity Coefficient (DSC) of 0.89 across allbones on the testing set, and 14 out of the 26 skull bones reached average DSC&gt;0.93. Our next steps are segmenting the whole body followed by developing amodel to classify abnormalities.", "output": "Deep learning-based Segmentation of Rabbit fetal skull with limited and sub-optimal annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The book attempts to introduce a gentle introduction to the field of FacialMicro Expressions Recognition (FMER) using Color and Depth images, with the aidof MATLAB programming environment. FMER is a subset of image processing and itis a multidisciplinary topic to analysis. So, it requires familiarity withother topics of Artifactual Intelligence (AI) such as machine learning, digitalimage processing, psychology and more. So, it is a great opportunity to write abook which covers all of these topics for beginner to professional readers inthe field of AI and even without having background of AI. Our goal is toprovide a standalone introduction in the field of MFER analysis in the form oftheorical descriptions for readers with no background in image processing withreproducible Matlab practical examples. Also, we describe any basic definitionsfor FMER analysis and MATLAB library which is used in the text, that helpsfinal reader to apply the experiments in the real-world applications. Webelieve that this book is suitable for students, researchers, and professionalsalike, who need to develop practical skills, along with a basic understandingof the field. We expect that, after reading this book, the reader feelscomfortable with different key stages such as color and depth image processing,color and depth image representation, classification, machine learning, facialmicro-expressions recognition, feature extraction and dimensionality reduction.The book attempts to introduce a gentle introduction to the field of FacialMicro Expressions Recognition (FMER) using Color and Depth images, with the aidof MATLAB programming environment.", "output": "Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach (Second Edition, 2023)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic and accurate segmentation of colon polyps is essential for earlydiagnosis of colorectal cancer. Advanced deep learning models have shownpromising results in polyp segmentation. However, they still have limitationsin representing multi-scale features and generalization capability. To addressthese issues, this paper introduces RaBiT, an encoder-decoder model thatincorporates a lightweight Transformer-based architecture in the encoder tomodel multiple-level global semantic relationships. The decoder consists ofseveral bidirectional feature pyramid layers with reverse attention modules tobetter fuse feature maps at various levels and incrementally refine polypboundaries. We also propose ideas to lighten the reverse attention module andmake it more suitable for multi-class segmentation. Extensive experiments onseveral benchmark datasets show that our method outperforms existing methodsacross all datasets while maintaining low computational complexity. Moreover,our method demonstrates high generalization capability in cross-datasetexperiments, even when the training and test sets have differentcharacteristics.", "output": "RaBiT: An Efficient Transformer using Bidirectional Feature Pyramid Network with Reverse Attention for Colon Polyp Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video frame interpolation is an increasingly important research task withseveral key industrial applications in the video coding, broadcast andproduction sectors. Recently, transformers have been introduced to the fieldresulting in substantial performance gains. However, this comes at a cost ofgreatly increased memory usage, training and inference time. In this paper, anovel method integrating a transformer encoder and convolutional features isproposed. This network reduces the memory burden by close to 50% and runs up tofour times faster during inference time compared to existing transformer-basedinterpolation methods. A dual-encoder architecture is introduced which combinesthe strength of convolutions in modelling local correlations with those of thetransformer for long-range dependencies. Quantitative evaluations are conductedon various benchmarks with complex motion to showcase the robustness of theproposed method, achieving competitive performance compared to state-of-the-artinterpolation networks.", "output": "Efficient Convolution and Transformer-Based Network for Video Frame Interpolation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding the spread of images across the web helps us understand thereuse of scientific visualizations and their relationship with the public. The\"Flatten the Curve\" graphic was heavily used during the COVID-19 pandemic toconvey a complex concept in a simple form. It displays two curves comparing theimpact on case loads for medical facilities if the populace either adopts orfails to adopt protective measures during a pandemic. We use five variants ofthe \"Flatten the Curve\" image as a case study for viewing the spread of animage online. To evaluate its spread, we leverage three information channels:reverse image search engines, social media, and web archives. Reverse imagesearches give us a current view into image reuse. Social media helps usunderstand a variant's popularity over time. Web archives help us see when itwas preserved, highlighting a view of popularity for future researchers. Ourcase study leverages document URLs can be used as a proxy for images whenstudying the spread of images online.", "output": "Discovering Image Usage Online: A Case Study With \"Flatten the Curve''."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autism Spectrum Disorder (ASD) has been emerging as a growing public healththreat. Early diagnosis of ASD is crucial for timely, effective interventionand treatment. However, conventional diagnosis methods based on communicationsand behavioral patterns are unreliable for children younger than 2 years ofage. Given evidences of neurodevelopmental abnormalities in ASD infants, weresort to a novel deep learning-based method to extract key features from theinherently scarce, class-imbalanced, and heterogeneous structural MR images forearly autism diagnosis. Specifically, we propose a Siamese verificationframework to extend the scarce data, and an unsupervised compressor toalleviate data imbalance by extracting key features. We also proposed weightconstraints to cope with sample heterogeneity by giving different samplesdifferent voting weights during validation, and we used Path Signature tounravel meaningful developmental features from the two-time point datalongitudinally. Extensive experiments have shown that our method performed wellunder practical scenarios, transcending existing machine learning methods.", "output": "Early Autism Diagnosis based on Path Signature and Siamese Unsupervised Feature Compressor."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present a novel Single-class target-specific Adversarialattack called SingleADV. The goal of SingleADV is to generate a universalperturbation that deceives the target model into confusing a specific categoryof objects with a target category while ensuring highly relevant and accurateinterpretations. The universal perturbation is stochastically and iterativelyoptimized by minimizing the adversarial loss that is designed to consider boththe classifier and interpreter costs in targeted and non-targeted categories.In this optimization framework, ruled by the first- and second-momentestimations, the desired loss surface promotes high confidence andinterpretation score of adversarial samples. By avoiding unintendedmisclassification of samples from other categories, SingleADV enables moreeffective targeted attacks on interpretable deep learning systems in bothwhite-box and black-box scenarios. To evaluate the effectiveness of SingleADV,we conduct experiments using four different model architectures (ResNet-50,VGG-16, DenseNet-169, and Inception-V3) coupled with three interpretationmodels (CAM, Grad, and MASK). Through extensive empirical evaluation, wedemonstrate that SingleADV effectively deceives the target deep learning modelsand their associated interpreters under various conditions and settings. Ourexperimental results show that the performance of SingleADV is effective, withan average fooling ratio of 0.74 and an adversarial confidence level of 0.78 ingenerating deceptive adversarial samples. Furthermore, we discuss severalcountermeasures against SingleADV, including a transfer-based learning approachand existing preprocessing defenses.", "output": "Single-Class Target-Specific Attack against Interpretable Deep Learning Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models are susceptible to adversarial samples in white andblack-box environments. Although previous studies have shown high attacksuccess rates, coupling DNN models with interpretation models could offer asense of security when a human expert is involved, who can identify whether agiven sample is benign or malicious. However, in white-box environments,interpretable deep learning systems (IDLSes) have been shown to be vulnerableto malicious manipulations. In black-box settings, as access to the componentsof IDLSes is limited, it becomes more challenging for the adversary to fool thesystem. In this work, we propose a Query-efficient Score-based black-box attackagainst IDLSes, QuScore, which requires no knowledge of the target model andits coupled interpretation model. QuScore is based on transfer-based andscore-based methods by employing an effective microbial genetic algorithm. Ourmethod is designed to reduce the number of queries necessary to carry outsuccessful attacks, resulting in a more efficient process. By continuouslyrefining the adversarial samples created based on feedback scores from theIDLS, our approach effectively navigates the search space to identifyperturbations that can fool the system. We evaluate the attack's effectivenesson four CNN models (Inception, ResNet, VGG, DenseNet) and two interpretationmodels (CAM, Grad), using both ImageNet and CIFAR datasets. Our results showthat the proposed approach is query-efficient with a high attack success ratethat can reach between 95% and 100% and transferability with an average successrate of 69% in the ImageNet and CIFAR datasets. Our attack method generatesadversarial examples with attribution maps that resemble benign samples. Wehave also demonstrated that our attack is resilient against variouspreprocessing defense techniques and can easily be transferred to different DNNmodels.", "output": "Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional neural networks (CNNs) have demonstrated remarkable success invision-related tasks. However, their susceptibility to failing when inputsdeviate from the training distribution is well-documented. Recent studiessuggest that CNNs exhibit a bias toward texture instead of object shape inimage classification tasks, and that background information may affectpredictions. This paper investigates the ability of CNNs to adapt to differentcolor distributions in an image while maintaining context and background. Theresults of our experiments on modified MNIST and FashionMNIST data demonstratethat changes in color can substantially affect classification accuracy. Thepaper explores the effects of various regularization techniques ongeneralization error across datasets and proposes a minor architecturalmodification utilizing the dropout regularization in a novel way that enhancesmodel reliance on color-invariant intensity-based features for improvedclassification accuracy. Overall, this work contributes to ongoing efforts tounderstand the limitations and challenges of CNNs in image classification tasksand offers potential solutions to enhance their performance.", "output": "On the ability of CNNs to extract color invariant intensity based features for image classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous driving on water surfaces plays an essential role in executinghazardous and time-consuming missions, such as maritime surveillance, survivorsrescue, environmental monitoring, hydrography mapping and waste cleaning. Thiswork presents WaterScenes, the first multi-task 4D radar-camera fusion datasetfor autonomous driving on water surfaces. Equipped with a 4D radar and amonocular camera, our Unmanned Surface Vehicle (USV) proffers all-weathersolutions for discerning object-related information, including color, shape,texture, range, velocity, azimuth, and elevation. Focusing on typical staticand dynamic objects on water surfaces, we label the camera images and radarpoint clouds at pixel-level and point-level, respectively. In addition to basicperception tasks, such as object detection, instance segmentation and semanticsegmentation, we also provide annotations for free-space segmentation andwaterline segmentation. Leveraging the multi-task and multi-modal data, weconduct numerous experiments on the single modality of radar and camera, aswell as the fused modalities. Results demonstrate that 4D radar-camera fusioncan considerably enhance the robustness of perception on water surfaces,especially in adverse lighting and weather conditions. WaterScenes dataset ispublic on ", "output": "WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset and Benchmark for Autonomous Driving on Water Surfaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integrating deep learning with clinical expertise holds great potential foraddressing healthcare challenges and empowering medical professionals withimproved diagnostic tools. However, the need for annotated medical images isoften an obstacle to leveraging the full power of machine learning models. Ourresearch demonstrates that by combining synthetic images, generated usingdiffusion models, with real images, we can enhance nonalcoholic fatty liverdisease (NAFLD) classification performance. We evaluate the quality of thesynthetic images by comparing two metrics: Inception Score (IS) and Fr'{e}chetInception Distance (FID), computed on diffusion-generated images and generativeadversarial networks (GANs)-generated images. Our results show superiorperformance for the diffusion-generated images, with a maximum IS score of$1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ comparedto $99.53$ for GANs. Utilizing a partially frozen CNN backbone (EfficientNetv1), our synthetic augmentation method achieves a maximum image-level ROC AUCof $0.904$ on a NAFLD prediction task.", "output": "Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale pre-trained vision-language models allow for the zero-shottext-based generation of 3D avatars. The previous state-of-the-art methodutilized CLIP to supervise neural implicit models that reconstructed a humanbody mesh. However, this approach has two limitations. Firstly, the lack ofavatar-specific models can cause facial distortion and unrealistic clothing inthe generated avatars. Secondly, CLIP only provides optimization direction forthe overall appearance, resulting in less impressive results. To address theselimitations, we propose AvatarFusion, the first framework to use a latentdiffusion model to provide pixel-level guidance for generating human-realisticavatars while simultaneously segmenting clothing from the avatar's body.AvatarFusion includes the first clothing-decoupled neural implicit avatar modelthat employs a novel Dual Volume Rendering strategy to render the decoupledskin and clothing sub-models in one space. We also introduce a noveloptimization method, called Pixel-Semantics Difference-Sampling (PS-DS), whichsemantically separates the generation of body and clothes, and generates avariety of clothing styles. Moreover, we establish the first benchmark forzero-shot text-to-avatar generation. Our experimental results demonstrate thatour framework outperforms previous approaches, with significant improvementsobserved in all metrics. Additionally, since our model is clothing-decoupled,we can exchange the clothes of avatars. Code will be available on Github.", "output": "AvatarFusion: Zero-shot Generation of Clothing-Decoupled 3D Avatars Using 2D Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Egocentric action recognition is gaining significant attention in the fieldof human action recognition. In this paper, we address data scarcity issue inegocentric action recognition from a compositional generalization perspective.To tackle this problem, we propose a free-form composition network (FFCN) thatcan simultaneously learn disentangled verb, preposition, and nounrepresentations, and then use them to compose new samples in the feature spacefor rare classes of action videos. First, we use a graph to capture thespatial-temporal relations among different hand/object instances in each actionvideo. We thus decompose each action into a set of verb and prepositionspatial-temporal representations using the edge features in the graph. Thetemporal decomposition extracts verb and preposition representations fromdifferent video frames, while the spatial decomposition adaptively learns verband preposition representations from action-related instances in each frame.With these spatial-temporal representations of verbs and prepositions, we cancompose new samples for those rare classes in a free-form manner, which is notrestricted to a rigid form of a verb and a noun. The proposed FFCN can directlygenerate new training data samples for rare classes, hence significantlyimprove action recognition performance. We evaluated our method on threepopular egocentric action recognition datasets, Something-Something V2, H2O,and EPIC-KITCHENS-100, and the experimental results demonstrate theeffectiveness of the proposed method for handling data scarcity problems,including long-tailed and few-shot egocentric action recognition.", "output": "Free-Form Composition Networks for Egocentric Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study introduces a novel and efficient least squares based method forrectangle fitting, using a continuous fitness function that approximates a unitsquare accurately. The proposed method is compared with the existing method inthe literature using both simulated data and real data. The real data isderived from aerial photogrammetry point clouds of a rectangular building. Thesimulated tests show that the proposed method performs better than thereference method, reducing the root-mean-square error by about 93% and 14% forclean datasets and noisy point clouds, respectively. The proposed method alsoimproves the fitting of the real dataset by about 81%, achieving centimetrelevel accuracy. Furthermore, the test results show that the proposed methodconverges in fewer than 10 iterations.", "output": "Optimised Least Squares Approach for Accurate Rectangle Fitting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing person re-identification (re-ID) research mainly focuses onpedestrian identity matching across cameras in adjacent areas. However, inreality, it is inevitable to face the problem of pedestrian identity matchingacross long-distance scenes. The cross-camera pedestrian samples collected fromlong-distance scenes often have no positive samples. It is extremelychallenging to use cross-camera negative samples to achieve cross-regionpedestrian identity matching. Therefore, a novel domain-adaptive person re-IDmethod that focuses on cross-camera consistent discriminative feature learningunder the supervision of unpaired samples is proposed. This method mainlyincludes category synergy co-promotion module (CSCM) and cross-cameraconsistent feature learning module (CCFLM). In CSCM, a task-specific featurerecombination (FRT) mechanism is proposed. This mechanism first groups featuresaccording to their contributions to specific tasks. Then an interactivepromotion learning (IPL) scheme between feature groups is developed andembedded in this mechanism to enhance feature discriminability. Since thecontrol parameters of the specific task model are reduced after division bytask, the generalization ability of the model is improved. In CCFLM,instance-level feature distribution alignment and cross-camera identityconsistent learning methods are constructed. Therefore, the supervised modeltraining is achieved under the style supervision of the target domain byexchanging styles between source-domain samples and target-domain samples, andthe challenges caused by the lack of cross-camera paired samples are solved byutilizing cross-camera similar samples. In experiments, three challengingdatasets are used as target domains, and the effectiveness of the proposedmethod is demonstrated through four experimental settings.", "output": "Domain-adaptive Person Re-identification without Cross-camera Paired Samples."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate a framework for binary image denoising via restrictedBoltzmann machines (RBMs) that introduces a denoising objective in quadraticunconstrained binary optimization (QUBO) form and is well-suited for quantumannealing. The denoising objective is attained by balancing the distributionlearned by a trained RBM with a penalty term for derivations from the noisyimage. We derive the statistically optimal choice of the penalty parameterassuming the target distribution has been well-approximated, and furthersuggest an empirically supported modification to make the method robust to thatidealistic assumption. We also show under additional assumptions that thedenoised images attained by our method are, in expectation, strictly closer tothe noise-free images than the noisy images are. While we frame the model as animage denoising model, it can be applied to any binary data. As the QUBOformulation is well-suited for implementation on quantum annealers, we test themodel on a D-Wave Advantage machine, and also test on data too large forcurrent quantum annealers by approximating QUBO solutions through classicalheuristics.", "output": "An Image-Denoising Framework Fit for Quantum Annealing via QUBO and Restricted Boltzmann Machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lung cancer is the leading cause of cancer death and early diagnosis isassociated with a positive prognosis. Chest X-ray (CXR) provides an inexpensiveimaging mode for lung cancer diagnosis. Suspicious nodules are difficult todistinguish from vascular and bone structures using CXR. Computer vision haspreviously been proposed to assist human radiologists in this task, however,leading studies use down-sampled images and computationally expensive methodswith unproven generalization. Instead, this study localizes lung nodules usingefficient encoder-decoder neural networks that process full resolution imagesto avoid any signal loss resulting from down-sampling. Encoder-decoder networksare trained and tested using the JSRT lung nodule dataset. The networks areused to localize lung nodules from an independent external CXR dataset.Sensitivity and false positive rates are measured using an automated frameworkto eliminate any observer subjectivity. These experiments allow for thedetermination of the optimal network depth, image resolution and pre-processingpipeline for generalized lung nodule localization. We find that nodulelocalization is influenced by subtlety, with more subtle nodules being detectedin earlier training epochs. Therefore, we propose a novel self-ensemble modelfrom three consecutive epochs centered on the validation optimum. This ensembleachieved a sensitivity of 85% in 10-fold internal testing with false positivesof 8 per image. A sensitivity of 81% is achieved at a false positive rate of 6following morphological false positive reduction. This result is comparable tomore computationally complex systems based on linear and spatial filtering, butwith a sub-second inference time that is faster than other methods. Theproposed algorithm achieved excellent generalization results against anexternal dataset with sensitivity of 77% at a false positive rate of 7.6.", "output": "Full-resolution Lung Nodule Segmentation from Chest X-ray Images using Residual Encoder-Decoder Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks have proven to be vulnerable to adversarial attacks inthe form of adding specific perturbations on images to make wrong outputs.Designing stronger adversarial attack methods can help more reliably evaluatethe robustness of DNN models. To release the harbor burden and improve theattack performance, auto machine learning (AutoML) has recently emerged as onesuccessful technique to help automatically find the near-optimal adversarialattack strategy. However, existing works about AutoML for adversarial attacksonly focus on $L_{infty}$-norm-based perturbations. In fact, semanticperturbations attract increasing attention due to their naturalnesses andphysical realizability. To bridge the gap between AutoML and semanticadversarial attacks, we propose a novel method called multi-objectiveevolutionary search of variable-length composite semantic perturbations(MES-VCSP). Specifically, we construct the mathematical model ofvariable-length composite semantic perturbations, which provides fivegradient-based semantic attack methods. The same type of perturbation in anattack sequence is allowed to be performed multiple times. Besides, weintroduce the multi-objective evolutionary search consisting of NSGA-II andneighborhood search to find near-optimal variable-length attack sequences.Experimental results on CIFAR10 and ImageNet datasets show that compared withexisting methods, MES-VCSP can obtain adversarial examples with a higher attacksuccess rate, more naturalness, and less time cost.", "output": "Multi-objective Evolutionary Search of Variable-length Composite Semantic Perturbations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ship orientation angle prediction (SOAP) with optical remote sensing imagesis an important image processing task, which often relies on deep convolutionalneural networks (CNNs) to make accurate predictions. This paper proposes anovel framework to reduce the model sizes and computational costs of SOAPmodels without harming prediction accuracy. First, a new SOAP model calledMobile-SOAP is designed based on MobileNetV2, achieving state-of-the-artprediction accuracy. Four tiny SOAP models are also created by replacing theconvolutional blocks in Mobile-SOAP with four small-scale networks,respectively. Then, to transfer knowledge from Mobile-SOAP to four lightweightmodels, we propose a novel knowledge distillation (KD) framework termed SOAP-KDconsisting of a novel feature-based guidance loss and an optimized syntheticsamples-based knowledge transfer mechanism. Lastly, extensive experiments onthe FGSC-23 dataset confirm the superiority of Mobile-SOAP over existing modelsand also demonstrate the effectiveness of SOAP-KD in improving the predictionperformance of four specially designed tiny models. Notably, by using SOAP-KD,the test mean absolute error of the ShuffleNetV2x1.0-based model is only 8%higher than that of Mobile-SOAP, but its number of parameters andmultiply-accumulate operations (MACs) are respectively 61.6% and 60.8% less.", "output": "Regression-Oriented Knowledge Distillation for Lightweight Ship Orientation Angle Prediction with Optical Remote Sensing Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this technical report, we present our findings from a study conducted onthe EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for ActionRecognition. Our research focuses on the innovative application of adifferentiable logic loss in the training to leverage the co-occurrencerelations between verb and noun, as well as the pre-trained Large LanguageModels (LLMs) to generate the logic rules for the adaptation to unseen actionlabels. Specifically, the model's predictions are treated as the truthassignment of a co-occurrence logic formula to compute the logic loss, whichmeasures the consistency between the predictions and the logic constraints. Byusing the verb-noun co-occurrence matrix generated from the dataset, we observea moderate improvement in model performance compared to our baseline framework.To further enhance the model's adaptability to novel action labels, weexperiment with rules generated using GPT-3.5, which leads to a slight decreasein performance. These findings shed light on the potential and challenges ofincorporating differentiable logic and LLMs for knowledge extraction inunsupervised domain adaptation for action recognition. Our final submission(entitled `NS-LLM') achieved the first place in terms of top-1 actionrecognition accuracy.", "output": "A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Retinal vessel segmentation is generally grounded in image-based datasetscollected with bench-top devices. The static images naturally lose the dynamiccharacteristics of retina fluctuation, resulting in diminished datasetrichness, and the usage of bench-top devices further restricts datasetscalability due to its limited accessibility. Considering these limitations, weintroduce the first video-based retinal dataset by employing handheld devicesfor data acquisition. The dataset comprises 635 smartphone-based fundus videoscollected from four different clinics, involving 415 patients from 50 to 75years old. It delivers comprehensive and precise annotations of retinalstructures in both spatial and temporal dimensions, aiming to advance thelandscape of vasculature segmentation. Specifically, the dataset provides threelevels of spatial annotations: binary vessel masks for overall retinalstructure delineation, general vein-artery masks for distinguishing the veinand artery, and fine-grained vein-artery masks for further characterizing thegranularities of each artery and vein. In addition, the dataset offers temporalannotations that capture the vessel pulsation characteristics, assisting indetecting ocular diseases that require fine-grained recognition of hemodynamicfluctuation. In application, our dataset exhibits a significant domain shiftwith respect to data captured by bench-top devices, thus posing greatchallenges to existing methods. In the experiments, we provide evaluationmetrics and benchmark results on our dataset, reflecting both the potential andchallenges it offers for vessel segmentation tasks. We hope this challengingdataset would significantly contribute to the development of eye diseasediagnosis and early prevention.", "output": "RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a fresh perspective on shot noise corrupted images and noiseremoval. By viewing image formation as the sequential accumulation of photonson a detector grid, we show that a network trained to predict where the nextphoton could arrive is in fact solving the minimum mean square error (MMSE)denoising task. This new perspective allows us to make three contributions: Wepresent a new strategy for self-supervised denoising, We present a new methodfor sampling from the posterior of possible solutions by iteratively samplingand adding small numbers of photons to the image. We derive a full generativemodel by starting this process from an empty canvas. We evaluate our methodquantitatively and qualitatively on 4 new fluorescence microscopy datasets,which will be made available to the community. We find that it outperformssupervised, self-supervised and unsupervised baselines or performs on-par.", "output": "Image Denoising and the Generative Accumulation of Photons."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training Artificial Intelligence (AI) models on three-dimensional image datapresents unique challenges compared to the two-dimensional case: Firstly, thecomputational resources are significantly higher, and secondly, theavailability of large pretraining datasets is often limited, impeding trainingsuccess. In this study, we propose a simple approach of adapting 2D networkswith an intermediate feature representation for processing 3D volumes. Ourmethod involves sequentially applying these networks to slices of a 3D volumefrom all orientations. Subsequently, a feature reduction module combines theextracted slice features into a single representation, which is then used forclassification. We evaluate our approach on medical classification benchmarksand a real-world clinical dataset, demonstrating comparable results to existingmethods. Furthermore, by employing attention pooling as a feature reductionmodule we obtain weighted importance values for each slice during the forwardpass. We show that slices deemed important by our approach allow the inspectionof the basis of a model's prediction.", "output": "Explainable 2D Vision Models for 3D Medical Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-agent multi-lidar sensor fusion between connected vehicles forcooperative perception has recently been recognized as the best technique forminimizing the blind zone of individual vehicular perception systems andfurther enhancing the overall safety of autonomous driving systems. Thistechnique relies heavily on the reliability and availability ofvehicle-to-everything (V2X) communication. In practical sensor fusionapplication scenarios, the non-line-of-sight (NLOS) issue causes blind zonesfor not only the perception system but also V2X direct communication. Tocounteract underlying communication issues, we introduce an abstract perceptionmatrix matching method for quick sensor fusion matching procedures andmobility-height hybrid relay determination procedures, proactively improvingthe efficiency and performance of V2X communication to serve the upper layerapplication fusion requirements. To demonstrate the effectiveness of oursolution, we design a new simulation framework to consider autonomous driving,sensor fusion and V2X communication in general, paving the way for end-to-endperformance evaluation and further solution derivation.", "output": "NLOS Dies Twice: Challenges and Solutions of V2X for Cooperative Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deception detection is an interdisciplinary field attracting researchers frompsychology, criminology, computer science, and economics. We propose amultimodal approach combining deep learning and discriminative models forautomated deception detection. Using video modalities, we employ convolutionalend-to-end learning to analyze gaze, head pose, and facial expressions,achieving promising results compared to state-of-the-art methods. Due tolimited training data, we also utilize discriminative models for deceptiondetection. Although sequence-to-class approaches are explored, discriminativemodels outperform them due to data scarcity. Our approach is evaluated on fivedatasets, including a new Rolling-Dice Experiment motivated by economicfactors. Results indicate that facial expressions outperform gaze and headpose, and combining modalities with feature selection enhances detectionperformance. Differences in expressed features across datasets emphasize theimportance of scenario-specific training data and the influence of context ondeceptive behavior. Cross-dataset experiments reinforce these findings. Despitethe challenges posed by low-stake datasets, including the Rolling-DiceExperiment, deception detection performance exceeds chance levels. Our proposedmultimodal approach and comprehensive evaluation shed light on the potential ofautomating deception detection from video modalities, opening avenues forfuture research.", "output": "Automated Deception Detection from Videos: Using End-to-End Learning Based High-Level Features and Classification Approaches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, the novel Image Transformation Sequence Retrieval (ITSR) taskis presented, in which a model must retrieve the sequence of transformationsbetween two given images that act as source and target, respectively. Givencertain characteristics of the challenge such as the multiplicity of a correctsequence or the correlation between consecutive steps of the process, wepropose a solution to ITSR using a general model-based Reinforcement Learningsuch as Monte Carlo Tree Search (MCTS), which is combined with a deep neuralnetwork. Our experiments provide a benchmark in both synthetic and realdomains, where the proposed approach is compared with supervised training. Theresults report that a model trained with MCTS is able to outperform itssupervised counterpart in both the simplest and the most complex cases. Ourwork draws interesting conclusions about the nature of ITSR and its associatedchallenges.", "output": "Image Transformation Sequence Retrieval with General Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present DeepIPCv2, an autonomous driving model that perceives theenvironment using a LiDAR sensor for more robust drivability, especially whendriving under poor illumination conditions. DeepIPCv2 takes a set of LiDARpoint clouds for its main perception input. As point clouds are not affected byillumination changes, they can provide a clear observation of the surroundingsno matter what the condition is. This results in a better scene understandingand stable features provided by the perception module to support the controllermodule in estimating navigational control properly. To evaluate itsperformance, we conduct several tests by deploying the model to predict a setof driving records and perform real automated driving under three differentconditions. We also conduct ablation and comparative studies with some recentmodels to justify its performance. Based on the experimental results, DeepIPCv2shows a robust performance by achieving the best drivability in all conditions.Codes are available at ", "output": "DeepIPCv2: LiDAR-powered Robust Environmental Perception and Navigational Control for Autonomous Vehicle."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Blockchain (BC) and Computer Vision (CV) are the two emerging fields with thepotential to transform various sectors.The ability of BC can help in offeringdecentralized and secure data storage, while CV allows machines to learn andunderstand visual data. This integration of the two technologies holds massivepromise for developing innovative applications that can provide solutions tothe challenges in various sectors such as supply chain management, healthcare,smart cities, and defense. This review explores a comprehensive analysis of theintegration of BC and CV by examining their combination and potentialapplications. It also provides a detailed analysis of the fundamental conceptsof both technologies, highlighting their strengths and limitations. This paperalso explores current research efforts that make use of the benefits offered bythis combination. The effort includes how BC can be used as an added layer ofsecurity in CV systems and also ensure data integrity, enabling decentralizedimage and video analytics using BC. The challenges and open issues associatedwith this integration are also identified, and appropriate potential futuredirections are also proposed.", "output": "A Comprehensive Analysis of Blockchain Applications for Securing Computer Vision Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The automatic classification of 3D medical data is memory-intensive. Also,variations in the number of slices between samples is common. Naive solutionssuch as subsampling can solve these problems, but at the cost of potentiallyeliminating relevant diagnosis information. Transformers have shown promisingperformance for sequential data analysis. However, their application forlong-sequences is data, computationally, and memory demanding. In this paper,we propose an end-to-end Transformer-based framework that allows to classifyvolumetric data of variable length in an efficient fashion. Particularly, byrandomizing the input slice-wise resolution during training, we enhance thecapacity of the learnable positional embedding assigned to each volume slice.Consequently, the accumulated positional information in each positionalembedding can be generalized to the neighbouring slices, even for highresolution volumes at the test time. By doing so, the model will be more robustto variable volume length and amenable to different computational budgets. Weevaluated the proposed approach in retinal OCT volume classification andachieved 21.96% average improvement in balanced accuracy on a 9-classdiagnostic task, compared to state-of-the-art video transformers. Our findingsshow that varying the slice-wise resolution of the input during trainingresults in more informative volume representation as compared to training withfixed number of slices per volume. Our code is available at:", "output": "Transformer-based end-to-end classification of variable-length volumetric data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks face many problems in the field of hyperspectral imageclassification, lack of effective utilization of spatial spectral information,gradient disappearance and overfitting as the model depth increases. In orderto accelerate the deployment of the model on edge devices with strict latencyrequirements and limited computing power, we introduce a lightweight modelbased on the improved 3D-Densenet model and designs DGCNet. It improves thedisadvantage of group convolution. Referring to the idea of dynamic network,dynamic group convolution(DGC) is designed on 3d convolution kernel. DGCintroduces small feature selectors for each grouping to dynamically decidewhich part of the input channel to connect based on the activations of allinput channels. Multiple groups can capture different and complementary visualand semantic features of input images, allowing convolution neural network(CNN)to learn rich features. 3D convolution extracts high-dimensional and redundanthyperspectral data, and there is also a lot of redundant information betweenconvolution kernels. DGC module allows 3D-Densenet to select channelinformation with richer semantic features and discard inactive regions. The3D-CNN passing through the DGC module can be regarded as a pruned network. DGCnot only allows 3D-CNN to complete sufficient feature extraction, but alsotakes into account the requirements of speed and calculation amount. Theinference speed and accuracy have been improved, with outstanding performanceon the IN, Pavia and KSC datasets, ahead of the mainstream hyperspectral imageclassification methods.", "output": "DGCNet: An Efficient 3D-Densenet based on Dynamic Group Convolution for Hyperspectral Remote Sensing Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the realm of Tiny AI, we introduce \"You Only Look at Interested Cells\"(YOLIC), an efficient method for object localization and classification on edgedevices. Seamlessly blending the strengths of semantic segmentation and objectdetection, YOLIC offers superior computational efficiency and precision. Byadopting Cells of Interest for classification instead of individual pixels,YOLIC encapsulates relevant information, reduces computational load, andenables rough object shape inference. Importantly, the need for bounding boxregression is obviated, as YOLIC capitalizes on the predetermined cellconfiguration that provides information about potential object location, size,and shape. To tackle the issue of single-label classification limitations, amulti-label classification approach is applied to each cell, effectivelyrecognizing overlapping or closely situated objects. This paper presentsextensive experiments on multiple datasets, demonstrating that YOLIC achievesdetection performance comparable to the state-of-the-art YOLO algorithms whilesurpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU. All resourcesrelated to this study, including datasets, cell designer, image annotationtool, and source code, have been made publicly available on our project websiteat ", "output": "YOLIC: An Efficient Method for Object Localization and Classification on Edge Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the video prediction task by putting forth a novel model thatcombines (i) our recently proposed hierarchical residual vector quantizedvariational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN(ST-PixelCNN). We refer to this approach as a sequential hierarchical residuallearning vector quantized variational autoencoder (S-HR-VQVAE). By leveragingthe intrinsic capabilities of HR-VQVAE at modeling still images with aparsimonious representation, combined with the ST-PixelCNN's ability athandling spatiotemporal information, S-HR-VQVAE can better deal with chiefchallenges in video prediction. These include learning spatiotemporalinformation, handling high dimensional data, combating blurry prediction, andimplicit modeling of physical characteristics. Extensive experimental resultson the KTH Human Action and Moving-MNIST tasks demonstrate that our modelcompares favorably against top video prediction techniques both in quantitativeand qualitative evaluations despite a much smaller model size. Finally, weboost S-HR-VQVAE by proposing a novel training method to jointly estimate theHR-VQVAE and ST-PixelCNN parameters.", "output": "S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies a reconstruction-based approach for weakly-supervisedanimal detection from aerial images in marine environments. Such an approachleverages an anomaly detection framework that computes metrics directly on theinput space, enhancing interpretability and anomaly localization compared tofeature embedding methods. Building upon the success of Vector-QuantizedVariational Autoencoders in anomaly detection on computer vision datasets, weadapt them to the marine animal detection domain and address the challenge ofhandling noisy data. To evaluate our approach, we compare it with existingmethods in the context of marine animal detection from aerial image data.Experiments conducted on two dedicated datasets demonstrate the superiorperformance of the proposed method over recent studies in the literature. Ourframework offers improved interpretability and localization of anomalies,providing valuable insights for monitoring marine ecosystems and mitigating theimpact of human activities on marine animals.", "output": "Weakly supervised marine animal detection from remote sensing images using vector-quantized variational autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection in remote sensing is a crucial computer vision task that hasseen significant advancements with deep learning techniques. However, mostexisting works in this area focus on the use of generic object detection and donot leverage the potential of multimodal data fusion. In this paper, we presenta comparison of methods for multimodal object detection in remote sensing,survey available multimodal datasets suitable for evaluation, and discussfuture directions.", "output": "Multimodal Object Detection in Remote Sensing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human Pose Estimation is a thoroughly researched problem; however, mostdatasets focus on the side and front-view scenarios. We address the limitationby proposing a novel approach that tackles the challenges posed by extremeviewpoints and poses. We introduce a new method for synthetic data generation -RePoGen, RarE POses GENerator - with comprehensive control over pose and viewto augment the COCO dataset. Experiments on a new dataset of real images showthat adding RePoGen data to the COCO surpasses previous attempts to top-viewpose estimation and significantly improves performance on the bottom-viewdataset. Through an extensive ablation study on both the top and bottom viewdata, we elucidate the contributions of methodological choices and demonstrateimproved performance. The code and the datasets are available on the projectwebsite.", "output": "Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gait Recognition is a computer vision task aiming to identify people by theirwalking patterns. Existing methods show impressive results on individualdatasets but lack the ability to generalize to unseen scenarios. UnsupervisedDomain Adaptation (UDA) tries to adapt a model, pre-trained in a supervisedmanner on a source domain, to an unlabelled target domain. UDA for GaitRecognition is still in its infancy and existing works proposed solutions tolimited scenarios. In this paper, we reveal a fundamental phenomenon inadaptation of gait recognition models, in which the target domain is biased topose-based features rather than identity features, causing a significantperformance drop in the identification task. We suggest Gait Orientation-basedmethod for Unsupervised Domain Adaptation (GOUDA) to reduce this bias. To thisend, we present a novel Triplet Selection algorithm with a curriculum learningframework, aiming to adapt the embedding space by pushing away samples ofsimilar poses and bringing closer samples of different poses. We provideextensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP,GREW, and Gait3D, and on three backbones, GaitSet, GaitPart, and GaitGL,showing the superiority of our proposed method over prior works.", "output": "Watch Your Pose: Unsupervised Domain Adaption with Pose based Triplet Selection for Gait Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Meta-learning has recently been an emerging data-efficient learning techniquefor various medical imaging operations and has helped advance contemporary deeplearning models. Furthermore, meta-learning enhances the knowledgegeneralization of the imaging tasks by learning both shared and discriminativeweights for various configurations of imaging tasks. However, existingmeta-learning models attempt to learn a single set of weight initializations ofa neural network that might be restrictive for multimodal data. This work aimsto develop a multimodal meta-learning model for image reconstruction, whichaugments meta-learning with evolutionary capabilities to encompass diverseacquisition settings of multimodal data. Our proposed model called KM-MAML(Kernel Modulation-based Multimodal Meta-Learning), has hypernetworks thatevolve to generate mode-specific weights. These weights provide themode-specific inductive bias for multiple modes by re-calibrating each kernelof the base network for image reconstruction via a low-rank kernel modulationoperation. We incorporate gradient-based meta-learning (GBML) in the contextualspace to update the weights of the hypernetworks for different modes. Thehypernetworks and the reconstruction network in the GBML setting providediscriminative mode-specific features and low-level image features,respectively. Experiments on multi-contrast MRI reconstruction show that ourmodel, (i) exhibits superior reconstruction performance over joint training,other meta-learning methods, and context-specific MRI reconstruction methods,and (ii) better adaptation capabilities with improvement margins of 0.5 dB inPSNR and 0.01 in SSIM. Besides, a representation analysis with U-Net shows thatkernel modulation infuses 80% of mode-specific representation changes in thehigh-resolution layers. Our source code is available at", "output": "Generalizing Supervised Deep Learning MRI Reconstruction to Multiple and Unseen Contrasts using Meta-Learning Hypernetworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a novel algorithm for crack localisation and detectionbased on visual and tactile analysis via fibre-optics. A finger-shaped sensorbased on fibre-optics is employed for the data acquisition to collect data forthe analysis and the experiments. To detect the possible locations of cracks acamera is used to scan an environment while running an object detectionalgorithm. Once the crack is detected, a fully-connected graph is created froma skeletonised version of the crack. A minimum spanning tree is then employedfor calculating the shortest path to explore the crack which is then used todevelop the motion planner for the robotic manipulator. The motion plannerdivides the crack into multiple nodes which are then explored individually.Then, the manipulator starts the exploration and performs the tactile dataclassification to confirm if there is indeed a crack in that location or just afalse positive from the vision algorithm. If a crack is detected, also thelength, width, orientation and number of branches are calculated. This isrepeated until all the nodes of the crack are explored.In order to validate the complete algorithm, various experiments areperformed: comparison of exploration of cracks through full scan and motionplanning algorithm, implementation of frequency-based features for crackclassification and geometry analysis using a combination of vision and tactiledata. From the results of the experiments, it is shown that the proposedalgorithm is able to detect cracks and improve the results obtained from visionto correctly classify cracks and their geometry with minimal cost thanks to themotion planning algorithm.", "output": "Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-language foundation models such as CLIP have shown impressivezero-shot performance on many tasks and datasets, especially thanks to theirfree-text inputs. However, they struggle to handle some downstream tasks, suchas fine-grained attribute detection and localization. In this paper, we proposea multitask fine-tuning strategy based on a positive/negative promptformulation to further leverage the capacities of the vision-languagefoundation models. Using the CLIP architecture as baseline, we show strongimprovements on bird fine-grained attribute detection and localization tasks,while also increasing the classification performance on the CUB200-2011dataset. We provide source code for reproducibility purposes: it is availableat ", "output": "Leveraging Vision-Language Foundation Models for Fine-Grained Downstream Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes exploiting the common sense knowledge learned by largelanguage models to perform zero-shot reasoning about crimes given textualdescriptions of surveillance videos. We show that when video is (manually)converted to high quality textual descriptions, large language models arecapable of detecting and classifying crimes with state-of-the-art performanceusing only zero-shot reasoning. However, existing automated video-to-textapproaches are unable to generate video descriptions of sufficient quality tosupport reasoning (garbage video descriptions into the large language model,garbage out).", "output": "Garbage in, garbage out: Zero-shot detection of crime using Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lane detection plays a pivotal role in the field of autonomous vehicles andadvanced driving assistant systems (ADAS). Over the years, numerous algorithmshave emerged, spanning from rudimentary image processing techniques tosophisticated deep neural networks. The performance of deep learning-basedmodels is highly dependent on the quality of their training data. Consequently,these models often experience a decline in performance when confronted withchallenging scenarios such as extreme lighting conditions, partially visiblelane markings, and sparse lane markings like Botts' dots. To address this, wepresent an end-to-end lane detection and classification system based on deeplearning methodologies. In our study, we introduce a unique datasetmeticulously curated to encompass scenarios that pose significant challengesfor state-of-the-art (SOTA) models. Through fine-tuning selected models, we aimto achieve enhanced localization accuracy. Moreover, we propose a CNN-basedclassification branch, seamlessly integrated with the detector, facilitatingthe identification of distinct lane types. This architecture enables informedlane-changing decisions and empowers more resilient ADAS capabilities. We alsoinvestigate the effect of using mixed precision training and testing ondifferent models and batch sizes. Experimental evaluations conducted on thewidely-used TuSimple dataset, Caltech lane dataset, and our LVLane datasetdemonstrate the effectiveness of our model in accurately detecting andclassifying lanes amidst challenging scenarios. Our method achievesstate-of-the-art classification results on the TuSimple dataset. The code ofthe work will be published upon the acceptance of the paper.", "output": "LVLane: Deep Learning for Lane Detection and Classification in Challenging Conditions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Noise injection is a fundamental tool for data augmentation, and yet there isno widely accepted procedure to incorporate it with learning frameworks. Thisstudy analyzes the effects of adding or applying different noise models ofvarying magnitudes to Convolutional Neural Network (CNN) architectures. Noisemodels that are distributed with different density functions are given commonmagnitude levels via Structural Similarity (SSIM) metric in order to create anappropriate ground for comparison. The basic results are conforming with themost of the common notions in machine learning, and also introduce some novelheuristics and recommendations on noise injection. The new approaches willprovide better understanding on optimal learning procedures for imageclassification.", "output": "Data Augmentation in Training CNNs: Injecting Noise to Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpreting the inner workings of deep learning models is crucial forestablishing trust and ensuring model safety. Concept-based explanations haveemerged as a superior approach that is more interpretable than featureattribution estimates such as pixel saliency. However, defining the conceptsfor the interpretability analysis biases the explanations by the user'sexpectations on the concepts. To address this, we propose a novel post-hocunsupervised method that automatically uncovers the concepts learned by deepmodels during training. By decomposing the latent space of a layer in singularvectors and refining them by unsupervised clustering, we uncover conceptvectors aligned with directions of high variance that are relevant to the modelprediction, and that point to semantically distinct concepts. Our extensiveexperiments reveal that the majority of our concepts are readily understandableto humans, exhibit coherency, and bear relevance to the task at hand. Moreover,we showcase the practical utility of our method in dataset exploration, whereour concept vectors successfully identify outlier training samples affected byvarious confounding factors. This novel exploration technique has remarkableversatility to data types and model architectures and it will facilitate theidentification of biases and the discovery of sources of error within trainingdata.", "output": "Uncovering Unique Concept Vectors through Latent Space Decomposition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image (T2I) personalization allows users to guide the creative imagegeneration process by combining their own visual concepts in natural languageprompts. Recently, encoder-based techniques have emerged as a new effectiveapproach for T2I personalization, reducing the need for multiple images andlong training times. However, most existing encoders are limited to asingle-class domain, which hinders their ability to handle diverse concepts. Inthis work, we propose a domain-agnostic method that does not require anyspecialized dataset or prior information about the personalized concepts. Weintroduce a novel contrastive-based regularization technique to maintain highfidelity to the target concept characteristics while keeping the predictedembeddings close to editable regions of the latent space, by pushing thepredicted tokens toward their nearest existing CLIP tokens. Our experimentalresults demonstrate the effectiveness of our approach and show how the learnedtokens are more semantic than tokens predicted by unregularized models. Thisleads to a better representation that achieves state-of-the-art performancewhile being more flexible than previous methods.", "output": "Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modular vision-language models (Vision-LLMs) align pretrained image encoderswith (pretrained) large language models (LLMs), representing a computationallymuch more efficient alternative to end-to-end training of large vision-languagemodels from scratch, which is prohibitively expensive for most. Vision-LLMsinstead post-hoc condition LLMs to `understand' the output of an image encoder.With the abundance of readily available high-quality English image-text data aswell as monolingual English LLMs, the research focus has been on English-onlyVision-LLMs. Multilingual vision-language models are still predominantlyobtained via expensive end-to-end pretraining, resulting in comparativelysmaller models, trained on limited multilingual image data supplemented withtext-only multilingual corpora. In this work, we present mBLIP, the firstmultilingual Vision-LLM, which we obtain in a computationally efficient manner-- on consumer hardware using only a few million training examples -- byleveraging a pretrained multilingual LLM. To this end, we textit{re-align} animage encoder previously tuned to an English LLM to a new, multilingual LLM --for this, we leverage multilingual data from a mix of vision-and-languagetasks, which we obtain by machine-translating high-quality English data to 95languages. On the IGLUE benchmark, mBLIP yields results competitive withstate-of-the-art models. Moreover, in image captioning on XM3600, mBLIP(zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared tothese very large multilingual vision-language models trained from scratch, weobtain mBLIP by training orders of magnitude fewer parameters on magnitudesless data. We release our model and code aturl{", "output": "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating videos for visual storytelling can be a tedious and complexprocess that typically requires either live-action filming or graphicsanimation rendering. To bypass these challenges, our key idea is to utilize theabundance of existing video clips and synthesize a coherent storytelling videoby customizing their appearances. We achieve this by developing a frameworkcomprised of two functional modules: (i) Motion Structure Retrieval, whichprovides video candidates with desired scene or motion context described byquery texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generatesplot-aligned videos under the guidance of motion structure and text prompts.For the first module, we leverage an off-the-shelf video retrieval system andextract video depths as motion structure. For the second module, we propose acontrollable video generation model that offers flexible controls overstructure and characters. The videos are synthesized by following thestructural guidance and appearance instruction. To ensure visual consistencyacross clips, we propose an effective concept personalization approach, whichallows the specification of the desired character identities through textprompts. Extensive experiments demonstrate that our approach exhibitssignificant advantages over various existing baselines.", "output": "Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable Artificial Intelligence (XAI) has received widespread interest inrecent years, and two of the most popular types of explanations are featureattributions, and counterfactual explanations. These classes of approaches havebeen largely studied independently and the few attempts at reconciling themhave been primarily empirical. This work establishes a clear theoreticalconnection between game-theoretic feature attributions, focusing on but notlimited to SHAP, and counterfactuals explanations. After motivating operativechanges to Shapley values based feature attributions and counterfactualexplanations, we prove that, under conditions, they are in fact equivalent. Wethen extend the equivalency result to game-theoretic solution concepts beyondShapley values. Moreover, through the analysis of the conditions of suchequivalence, we shed light on the limitations of naively using counterfactualexplanations to provide feature importances. Experiments on three datasetsquantitatively show the difference in explanations at every stage of theconnection between the two approaches and corroborate the theoretical findings.", "output": "On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces InternVid, a large-scale video-centric multimodaldataset that enables learning powerful and transferable video-textrepresentations for multimodal understanding and generation. The InternViddataset contains over 7 million videos lasting nearly 760K hours, yielding 234Mvideo clips accompanied by detailed descriptions of total 4.1B words. Our corecontribution is to develop a scalable approach to autonomously build ahigh-quality video-text dataset with large language models (LLM), therebyshowcasing its efficacy in learning video-language representation at scale.Specifically, we utilize a multi-scale approach to generate video-relateddescriptions. Furthermore, we introduce ViCLIP, a video-text representationlearning model based on ViT-L. Learned on InternVid via contrastive learning,this model demonstrates leading zero-shot action recognition and competitivevideo retrieval performance. Beyond basic video understanding tasks likerecognition and retrieval, our dataset and model have broad applications. Theyare particularly beneficial for generating interleaved video-text data forlearning a video-centric dialogue system, advancing video-to-text andtext-to-video generation research. These proposed resources provide a tool forresearchers and practitioners interested in multimodal video understanding andgeneration.", "output": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on three large-scale datasets(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Ourcode/models are released at ", "output": "Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prompt learning has emerged as an efficient alternative for fine-tuningfoundational models, such as CLIP, for various downstream tasks. Conventionallytrained using the task-specific objective, i.e., cross-entropy loss, promptstend to overfit downstream data distributions and find it challenging tocapture task-agnostic general features from the frozen CLIP. This leads to theloss of the model's original generalization capability. To address this issue,our work introduces a self-regularization framework for prompting calledPromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides theprompts to optimize for both task-specific and task-agnostic generalrepresentations using a three-pronged approach by: (a) regulating {prompted}representations via mutual agreement maximization with the frozen model, (b)regulating with self-ensemble of prompts over the training trajectory to encodetheir complementary strengths, and (c) regulating with textual diversity tomitigate sample diversity imbalance with the visual branch. To the best of ourknowledge, this is the first regularization framework for prompt learning thatavoids overfitting by jointly attending to pre-trained model features, thetraining trajectory during prompting, and the textual diversity. PromptSRCexplicitly steers the prompts to learn a representation space that maximizesperformance on downstream tasks without compromising CLIP generalization. Weperform extensive experiments on 4 benchmarks where PromptSRC overall performsfavorably well compared to the existing methods. Our code and pre-trainedmodels are publicly available at: ", "output": "Self-regulating Prompts: Foundational Model Adaptation without Forgetting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Personalization has emerged as a prominent aspect within the field ofgenerative AI, enabling the synthesis of individuals in diverse contexts andstyles, while retaining high-fidelity to their identities. However, the processof personalization presents inherent challenges in terms of time and memoryrequirements. Fine-tuning each personalized model needs considerable GPU timeinvestment, and storing a personalized model per subject can be demanding interms of storage capacity. To overcome these challenges, we proposeHyperDreamBooth-a hypernetwork capable of efficiently generating a small set ofpersonalized weights from a single image of a person. By composing theseweights into the diffusion model, coupled with fast finetuning, HyperDreamBoothcan generate a person's face in various contexts and styles, with high subjectdetails while also preserving the model's crucial knowledge of diverse stylesand semantic modifications. Our method achieves personalization on faces inroughly 20 seconds, 25x faster than DreamBooth and 125x faster than TextualInversion, using as few as one reference image, with the same quality and stylediversity as DreamBooth. Also our method yields a model that is 10000x smallerthan a normal DreamBooth model. Project page: ", "output": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Along with the development of modern smart cities, human-centric videoanalysis has been encountering the challenge of analyzing diverse and complexevents in real scenes. A complex event relates to dense crowds, anomalousindividuals, or collective behaviors. However, limited by the scale andcoverage of existing video datasets, few human analysis approaches havereported their performances on such complex events. To this end, we present anew large-scale dataset with comprehensive annotations, named Human-in-Eventsor HiEve (Human-centric video analysis in complex Events), for theunderstanding of human motions, poses, and actions in a variety of realisticevents, especially in crowd &amp; complex events. It contains a record number ofposes (&gt;1M), the largest number of action instances (&gt;56k) under complexevents, as well as one of the largest numbers of trajectories lasting forlonger time (with an average trajectory length of &gt;480 frames). Based on itsdiverse annotation, we present two simple baselines for action recognition andpose estimation, respectively. They leverage cross-label information duringtraining to enhance the feature learning in corresponding visual tasks.Experiments show that they could boost the performance of existing actionrecognition and pose estimation pipelines. More importantly, they prove thewidely ranged annotations in HiEve can improve various video tasks.Furthermore, we conduct extensive experiments to benchmark recent videoanalysis approaches together with our baseline methods, demonstrating HiEve isa challenging dataset for human-centric video analysis. We expect that thedataset will advance the development of cutting-edge techniques inhuman-centric analysis and the understanding of complex events. The dataset isavailable at <a href=\" http URL</a>", "output": "Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The capability of Deep Neural Networks (DNNs) to recognize objects inorientations outside the distribution of the training data is not wellunderstood. We present evidence that DNNs are capable of generalizing toobjects in novel orientations by disseminating orientation-invariance obtainedfrom familiar objects seen from many viewpoints. This capability strengthenswhen training the DNN with an increasing number of familiar objects, but onlyin orientations that involve 2D rotations of familiar orientations. We showthat this dissemination is achieved via neurons tuned to common featuresbetween familiar and unfamiliar objects. These results implicate brain-likeneural mechanisms for generalization.", "output": "Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatically generating a report from a patient's Chest X-Rays (CXRs) is apromising solution to reducing clinical workload and improving patient care.However, current CXR report generators -- which are predominantlyencoder-to-decoder models -- lack the diagnostic accuracy to be deployed in aclinical setting. To improve CXR report generation, we investigate warmstarting the encoder and decoder with recent open-source computer vision andnatural language processing checkpoints, such as the Vision Transformer (ViT)and PubMedBERT. To this end, each checkpoint is evaluated on the MIMIC-CXR andIU X-Ray datasets. Our experimental investigation demonstrates that theConvolutional vision Transformer (CvT) ImageNet-21K and the DistilledGenerative Pre-trained Transformer 2 (DistilGPT2) checkpoints are best for warmstarting the encoder and decoder, respectively. Compared to thestate-of-the-art ($mathcal{M}^2$ Transformer Progressive), CvT2DistilGPT2attained an improvement of 8.3% for CE F-1, 1.8% for BLEU-4, 1.6% forROUGE-L, and 1.0% for METEOR. The reports generated by CvT2DistilGPT2 have ahigher similarity to radiologist reports than previous approaches. Thisindicates that leveraging warm starting improves CXR report generation. Codeand checkpoints for CvT2DistilGPT2 are available at", "output": "Improving Chest X-Ray Report Generation by Leveraging Warm Starting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deepfake detection methods based on convolutional neural networks (CNN) havedemonstrated high accuracy. textcolor{black}{However, these methods oftensuffer from decreased performance when faced with unknown forgery methods andcommon transformations such as resizing and blurring, resulting in deviationsbetween training and testing domains.} This phenomenon, known as overfitting,poses a significant challenge. To address this issue, we propose a novel blockshuffling regularization method. Firstly, our approach involves dividing theimages into blocks and applying both intra-block and inter-block shufflingtechniques. This process indirectly achieves weight-sharing across differentdimensions. Secondly, we introduce an adversarial loss algorithm to mitigatethe overfitting problem induced by the shuffling noise. Finally, we restore thespatial layout of the blocks to capture the semantic associations among them.Extensive experiments validate the effectiveness of our proposed method, whichsurpasses existing approaches in forgery face detection. Notably, our methodexhibits excellent generalization capabilities, demonstrating robustnessagainst cross-dataset evaluations and common image transformations. Especiallyour method can be easily integrated with various CNN models. Source code isavailable athref{", "output": "Block shuffling learning for Deepfake Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-based grasp estimation is an essential part of robotic manipulationtasks in the real world. Existing planar grasp estimation algorithms have beendemonstrated to work well in relatively simple scenes. But when it comes tocomplex scenes, such as cluttered scenes with messy backgrounds and movingobjects, the algorithms from previous works are prone to generate inaccurateand unstable grasping contact points. In this work, we first study the existingplanar grasp estimation algorithms and analyze the related challenges incomplex scenes. Secondly, we design a Pixel-wise Efficient Grasp GenerationNetwork (PEGG-Net) to tackle the problem of grasping in complex scenes.PEGG-Net can achieve improved state-of-the-art performance on the Cornelldataset (98.9%) and second-best performance on the Jacquard dataset (93.8%),outperforming other existing algorithms without the introduction of complexstructures. Thirdly, PEGG-Net could operate in a closed-loop manner for addedrobustness in dynamic environments using position-based visual servoing (PBVS).Finally, we conduct real-world experiments on static, dynamic, and clutteredobjects in different complex scenes. The results show that our proposed networkachieves a high success rate in grasping irregular objects, household objects,and workshop tools. To benefit the community, our trained model andsupplementary materials are available at ", "output": "PEGG-Net: Pixel-Wise Efficient Grasp Generation in Complex Scenes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Zero-shot learning (ZSL) aims to identify unseen classes with zero samplesduring training. Broadly speaking, present ZSL methods usually adoptclass-level semantic labels and compare them with instance-level semanticpredictions to infer unseen classes. However, we find that such existing modelsmostly produce imbalanced semantic predictions, i.e. these models could performprecisely for some semantics, but may not for others. To address the drawback,we aim to introduce an imbalanced learning framework into ZSL. However, we findthat imbalanced ZSL has two unique challenges: (1) Its imbalanced predictionsare highly correlated with the value of semantic labels rather than the numberof samples as typically considered in the traditional imbalanced learning; (2)Different semantics follow quite different error distributions between classes.To mitigate these issues, we first formalize ZSL as an imbalanced regressionproblem which offers empirical evidences to interpret how semantic labels leadto imbalanced semantic predictions. We then propose a re-weighted loss termedRe-balanced Mean-Squared Error (ReMSE), which tracks the mean and variance oferror distributions, thus ensuring rebalanced learning across classes. As amajor contribution, we conduct a series of analyses showing that ReMSE istheoretically well established. Extensive experiments demonstrate that theproposed method effectively alleviates the imbalance in semantic prediction andoutperforms many state-of-the-art ZSL methods. Our code is available at", "output": "Rebalanced Zero-shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although Deep Neural Networks (DNNs) have been widely applied in variousreal-world scenarios, they are vulnerable to adversarial examples. The currentadversarial attacks in computer vision can be divided into digital attacks andphysical attacks according to their different attack forms. Compared withdigital attacks, which generate perturbations in the digital pixels, physicalattacks are more practical in the real world. Owing to the serious securityproblem caused by physically adversarial examples, many works have beenproposed to evaluate the physically adversarial robustness of DNNs in the pastyears. In this paper, we summarize a survey versus the current physicallyadversarial attacks and physically adversarial defenses in computer vision. Toestablish a taxonomy, we organize the current physical attacks from attacktasks, attack forms, and attack methods, respectively. Thus, readers can have asystematic knowledge of this topic from different aspects. For the physicaldefenses, we establish the taxonomy from pre-processing, in-processing, andpost-processing for the DNN models to achieve full coverage of the adversarialdefenses. Based on the above survey, we finally discuss the challenges of thisresearch field and further outlook on the future direction.", "output": "Visually Adversarial Attacks and Defenses in the Physical World: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Flow-based methods have demonstrated promising results in addressing theill-posed nature of super-resolution (SR) by learning the distribution ofhigh-resolution (HR) images with the normalizing flow. However, these methodscan only perform a predefined fixed-scale SR, limiting their potential inreal-world applications. Meanwhile, arbitrary-scale SR has gained moreattention and achieved great progress. Nonetheless, previous arbitrary-scale SRmethods ignore the ill-posed problem and train the model with per-pixel L1loss, leading to blurry SR outputs. In this work, we propose \"Local ImplicitNormalizing Flow\" (LINF) as a unified solution to the above problems. LINFmodels the distribution of texture details under different scaling factors withnormalizing flow. Thus, LINF can generate photo-realistic HR images with richtexture details in arbitrary scale factors. We evaluate LINF with extensiveexperiments and show that LINF achieves the state-of-the-art perceptual qualitycompared with prior arbitrary-scale SR methods.", "output": "Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate and efficient classification of different types of cancer iscritical for early detection and effective treatment. In this paper, we presentthe results of our experiments using the EfficientNet algorithm forclassification of brain tumor, breast cancer mammography, chest cancer, andskin cancer. We used publicly available datasets and preprocessed the images toensure consistency and comparability. Our experiments show that theEfficientNet algorithm achieved high accuracy, precision, recall, and F1 scoreson each of the cancer datasets, outperforming other state-of-the-art algorithmsin the literature. We also discuss the strengths and weaknesses of theEfficientNet algorithm and its potential applications in clinical practice. Ourresults suggest that the EfficientNet algorithm is well-suited forclassification of different types of cancer and can be used to improve theaccuracy and efficiency of cancer diagnosis.", "output": "EfficientNet Algorithm for Classification of Different Types of Cancer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an end-to-end deep learning approach to rectify fisheye images andsimultaneously calibrate camera intrinsic and distortion parameters. Our methodconsists of two parts: a Quick Image Rectification Module developed with aPix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with aCNN architecture. Our Quick Rectification Network performs robust rectificationwith good resolution, making it suitable for constant calibration incamera-based surveillance equipment. To achieve high-quality calibration, weuse the straightened output from the Quick Rectification Module as aguidance-like semantic feature map for the Calibration Module to learn thegeometric relationship between the straightened feature and the distortedfeature. We train and validate our method with a large synthesized datasetlabeled with well-simulated parameters applied to a perspective image dataset.Our solution has achieved robust performance in high-resolution with asignificant PSNR value of 22.343.", "output": "FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatically determining whether a text and a corresponding image aresemantically aligned is a significant challenge for vision-language models,with applications in generative text-to-image and image-to-text tasks. In thiswork, we study methods for automatic text-image alignment evaluation. We firstintroduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasetsfrom both text-to-image and image-to-text generation tasks, with humanjudgements for whether a given text-image pair is semantically aligned. We thendescribe two automatic methods to determine alignment: the first involving apipeline based on question generation and visual question answering models, andthe second employing an end-to-end classification approach by finetuningmultimodal pretrained models. Both methods surpass prior approaches in varioustext-image alignment tasks, with significant improvements in challenging casesthat involve complex composition or unnatural images. Finally, we demonstratehow our approaches can localize specific misalignments between an image and agiven text, and how they can be used to automatically re-rank candidates intext-to-image generation.", "output": "What You See is What You Read? Improving Text-Image Alignment Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Driver distraction causes a significant number of traffic accidents everyyear, resulting in economic losses and casualties. Currently, the level ofautomation in commercial vehicles is far from completely unmanned, and driversstill play an important role in operating and controlling the vehicle.Therefore, driver distraction behavior detection is crucial for road safety. Atpresent, driver distraction detection primarily relies on traditionalconvolutional neural networks (CNN) and supervised learning methods. However,there are still challenges such as the high cost of labeled datasets, limitedability to capture high-level semantic information, and weak generalizationperformance. In order to solve these problems, this paper proposes a newself-supervised learning method based on masked image modeling for driverdistraction behavior detection. Firstly, a self-supervised learning frameworkfor masked image modeling (MIM) is introduced to solve the serious human andmaterial consumption issues caused by dataset labeling. Secondly, the SwinTransformer is employed as an encoder. Performance is enhanced by reconfiguringthe Swin Transformer block and adjusting the distribution of the number ofwindow multi-head self-attention (W-MSA) and shifted window multi-headself-attention (SW-MSA) detection heads across all stages, which leads to modelmore lightening. Finally, various data augmentation strategies are used alongwith the best random masking strategy to strengthen the model's recognition andgeneralization ability. Test results on a large-scale driver distractionbehavior dataset show that the self-supervised learning method proposed in thispaper achieves an accuracy of 99.60%, approximating the excellent performanceof advanced supervised learning methods. Our code is publicly available atgithub.com/Rocky1salady-killer/SL-DDBD.", "output": "A Novel Driver Distraction Behavior Detection Method Based on Self-supervised Learning with Masked Image Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked autoencoder (MAE) has attracted unprecedented attention and achievesremarkable performance in many vision tasks. It reconstructs random maskedimage patches (known as proxy task) during pretraining and learns meaningfulsemantic representations that can be transferred to downstream tasks. However,MAE has not been thoroughly explored in ultrasound imaging. In this work, weinvestigate the potential of MAE for ultrasound image recognition. Motivated bythe unique property of ultrasound imaging in high noise-to-signal ratio, wepropose a novel deblurring MAE approach that incorporates deblurring into theproxy task during pretraining. The addition of deblurring facilitates thepretraining to better recover the subtle details presented in the ultrasoundimages, thus improving the performance of the downstream classification task.Our experimental results demonstrate the effectiveness of our deblurring MAE,achieving state-of-the-art performance in ultrasound image classification.Overall, our work highlights the potential of MAE for ultrasound imagerecognition and presents a novel approach that incorporates deblurring tofurther improve its effectiveness.", "output": "Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The perception module of self-driving vehicles relies on a multi-sensorsystem to understand its environment. Recent advancements in deep learning haveled to the rapid development of approaches that integrate multi-sensorymeasurements to enhance perception capabilities. This paper surveys the latestdeep learning integration techniques applied to the perception module inautonomous driving systems, categorizing integration approaches based on \"what,how, and when to integrate\". A new taxonomy of integration is proposed, basedon three dimensions: multi-view, multi-modality, and multi-frame. Theintegration operations and their pros and cons are summarized, providing newinsights into the properties of an \"ideal\" data integration approach that canalleviate the limitations of existing methods. After reviewing hundreds ofrelevant papers, this survey concludes with a discussion of the key features ofan optimal data integration approach.", "output": "A survey on deep learning approaches for data integration in autonomous driving system."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling newcapabilities of perceiving object descriptions (e.g., bounding boxes) andgrounding text to the visual world. Specifically, we represent referexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', whereobject descriptions are sequences of location tokens. Together with multimodalcorpora, we construct large-scale data of grounded image-text pairs (calledGrIT) to train the model. In addition to the existing capabilities of MLLMs(e.g., perceiving general modalities, following instructions, and performingin-context learning), Kosmos-2 integrates the grounding capability intodownstream applications. We evaluate Kosmos-2 on a wide range of tasks,including (i) multimodal grounding, such as referring expression comprehension,and phrase grounding, (ii) multimodal referring, such as referring expressiongeneration, (iii) perception-language tasks, and (iv) language understandingand generation. This work lays out the foundation for the development ofEmbodiment AI and sheds light on the big convergence of language, multimodalperception, action, and world modeling, which is a key step toward artificialgeneral intelligence. Code and pretrained models are available at", "output": "Kosmos-2: Grounding Multimodal Large Language Models to the World."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Facial wound segmentation plays a crucial role in preoperative planning andoptimizing patient outcomes in various medical applications. In this paper, wepropose an efficient approach for automating 3D facial wound segmentation usinga two-stream graph convolutional network. Our method leverages the Cir3D-FaIRdataset and addresses the challenge of data imbalance through extensiveexperimentation with different loss functions. To achieve accuratesegmentation, we conducted thorough experiments and selected a high-performingmodel from the trained models. The selected model demonstrates exceptionalsegmentation performance for complex 3D facial wounds. Furthermore, based onthe segmentation model, we propose an improved approach for extracting 3Dfacial wound fillers and compare it to the results of the previous study. Ourmethod achieved a remarkable accuracy of 0.9999986% on the test suite,surpassing the performance of the previous method. From this result, we use 3Dprinting technology to illustrate the shape of the wound filling. The outcomesof this study have significant implications for physicians involved inpreoperative planning and intervention design. By automating facial woundsegmentation and improving the accuracy of wound-filling extraction, ourapproach can assist in carefully assessing and optimizing interventions,leading to enhanced patient outcomes. Additionally, it contributes to advancingfacial reconstruction techniques by utilizing machine learning and 3Dbioprinting for printing skin tissue implants. Our source code is available aturl{", "output": "Advancing Wound Filling Extraction on 3D Faces: Auto-Segmentation and Wound Face Regeneration Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We demonstrate two novel editing procedures in the context of fantasy cardart. Palette transfer applies a specified reference palette to a given card.For fantasy art, the desired change in palette can be very large, leading tohuge changes in the \"look\" of the art. We demonstrate that a pipeline of vectorquantization; matching; and \"vector dequantization\" (using a diffusion model)produces successful extreme palette transfers. Segment control allows an artistto move one or more image segments, and to optionally specify the desired colorof the result. The combination of these two types of edit yields valuableworkflows, including: move a segment, then recolor; recolor, then force somesegments to take a prescribed color. We demonstrate our methods on thechallenging Yu-Gi-Oh card art dataset.", "output": "Applying a Color Palette with Local Control using Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Street-view imagery provides us with novel experiences to explore differentplaces remotely. Carefully calibrated street-view images (e.g. Google StreetView) can be used for different downstream tasks, e.g. navigation, map featuresextraction. As personal high-quality cameras have become much more affordableand portable, an enormous amount of crowdsourced street-view images areuploaded to the internet, but commonly with missing or noisy sensorinformation. To prepare this hidden treasure for \"ready-to-use\" status,determining missing location information and camera orientation angles are twoequally important tasks. Recent methods have achieved high performance ongeo-localization of street-view images by cross-view matching with a pool ofgeo-referenced satellite imagery. However, most of the existing works focusmore on geo-localization than estimating the image orientation. In this work,we re-state the importance of finding fine-grained orientation for street-viewimages, formally define the problem and provide a set of evaluation metrics toassess the quality of the orientation estimation. We propose two methods toimprove the granularity of the orientation estimation, achieving 82.4% and72.3% accuracy for images with estimated angle errors below 2 degrees for CVUSAand CVACT datasets, corresponding to 34.9% and 28.2% absolute improvementcompared to previous works. Integrating fine-grained orientation estimation intraining also improves the performance on geo-localization, giving top 1 recall95.5%/85.5% and 86.8%/80.4% for orientation known/unknown tests on the twodatasets.", "output": "Beyond Geo-localization: Fine-grained Orientation of Street-view Images by Cross-view Matching with Satellite Imagery with Supplementary Materials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Blocks2World, a novel method for 3D scene rendering and editingthat leverages a two-step process: convex decomposition of images andconditioned synthesis. Our technique begins by extracting 3D parallelepipedsfrom various objects in a given scene using convex decomposition, thusobtaining a primitive representation of the scene. These primitives are thenutilized to generate paired data through simple ray-traced depth maps. The nextstage involves training a conditioned model that learns to generate images fromthe 2D-rendered convex primitives. This step establishes a direct mappingbetween the 3D model and its 2D representation, effectively learning thetransition from a 3D model to an image. Once the model is fully trained, itoffers remarkable control over the synthesis of novel and edited scenes. Thisis achieved by manipulating the primitives at test time, including translatingor adding them, thereby enabling a highly customizable scene rendering process.Our method provides a fresh perspective on 3D scene rendering and editing,offering control and flexibility. It opens up new avenues for research andapplications in the field, including authoring and data augmentation.", "output": "Blocks2World: Controlling Realistic Scenes with Editable Primitives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We describe a method to parse a complex, cluttered indoor scene intoprimitives which offer a parsimonious abstraction of scene structure. Ourprimitives are simple convexes. Our method uses a learned regression procedureto parse a scene into a fixed number of convexes from RGBD input, and canoptionally accept segmentations to improve the decomposition. The result isthen polished with a descent method which adjusts the convexes to produce avery good fit, and greedily removes superfluous primitives. Because the entirescene is parsed, we can evaluate using traditional depth, normal, andsegmentation error metrics. Our evaluation procedure demonstrates that theerror from our primitive representation is comparable to that of predictingdepth from a single image.", "output": "Convex Decomposition of Indoor Scenes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the context of label-efficient learning on video data, the distillationmethod and the structural design of the teacher-student architecture have asignificant impact on knowledge distillation. However, the relationship betweenthese factors has been overlooked in previous research. To address this gap, wepropose a new weakly supervised learning framework for knowledge distillationin video classification that is designed to improve the efficiency and accuracyof the student model. Our approach leverages the concept of substage-basedlearning to distill knowledge based on the combination of student substages andthe correlation of corresponding substages. We also employ the progressivecascade training method to address the accuracy loss caused by the largecapacity gap between the teacher and the student. Additionally, we propose apseudo-label optimization strategy to improve the initial data label. Tooptimize the loss functions of different distillation substages during thetraining process, we introduce a new loss method based on feature distribution.We conduct extensive experiments on both real and simulated data sets,demonstrating that our proposed approach outperforms existing distillationmethods in terms of knowledge distillation for video classification tasks. Ourproposed substage-based distillation approach has the potential to informfuture research on label-efficient learning for video data.", "output": "The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Radiology reporting is a crucial part of the communication betweenradiologists and other medical professionals, but it can be time-consuming anderror-prone. One approach to alleviate this is structured reporting, whichsaves time and enables a more accurate evaluation than free-text reports.However, there is limited research on automating structured reporting, and nopublic benchmark is available for evaluating and comparing different methods.To close this gap, we introduce Rad-ReStruct, a new benchmark dataset thatprovides fine-grained, hierarchically ordered annotations in the form ofstructured reports for X-Ray images. We model the structured reporting task ashierarchical visual question answering (VQA) and propose hi-VQA, a novel methodthat considers prior context in the form of previously asked questions andanswers for populating a structured radiology report. Our experiments show thathi-VQA achieves competitive performance to the state-of-the-art on the medicalVQA benchmark VQARad while performing best among methods withoutdomain-specific vision-language pretraining and provides a strong baseline onRad-ReStruct. Our work represents a significant step towards the automatedpopulation of structured radiology reports and provides a valuable firstbenchmark for future research in this area. We will make all annotations andour code for annotation generation, model evaluation, and training publiclyavailable upon acceptance. Our dataset and code is available at", "output": "Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce PIGEON, a multi-task end-to-end system for planet-scale imagegeolocalization that achieves state-of-the-art performance on both externalbenchmarks and in human evaluation. Our work incorporates semantic geocellcreation with label smoothing, conducts pretraining of a vision transformer onimages with geographic information, and refines location predictions withProtoNets across a candidate set of geocells. The contributions of PIGEON arethree-fold: first, we design a semantic geocells creation and splittingalgorithm based on open-source data which can be adapted to any geospatialdataset. Second, we show the effectiveness of intra-geocell refinement and theapplicability of unsupervised clustering and ProtNets to the task. Finally, wemake our pre-trained CLIP transformer model, StreetCLIP, publicly available foruse in adjacent domains with applications to fighting climate change and urbanand rural scene understanding.", "output": "PIGEON: Predicting Image Geolocations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated radiology report generation aims to generate radiology reports thatcontain rich, fine-grained descriptions of radiology imaging. Compared withimage captioning in the natural image domain, medical images are very similarto each other, with only minor differences in the occurrence of diseases. Giventhe importance of these minor differences in the radiology report, it iscrucial to encourage the model to focus more on the subtle regions of diseaseoccurrence. Secondly, the problem of visual and textual data biases is serious.Not only do normal cases make up the majority of the dataset, but sentencesdescribing areas with pathological changes also constitute only a small part ofthe paragraph. Lastly, generating medical image reports involves the challengeof long text generation, which requires more expertise and empirical trainingin medical knowledge. As a result, the difficulty of generating such reports isincreased. To address these challenges, we propose a disease-oriented retrievalframework that utilizes similar reports as prior knowledge references. Wedesign a factual consistency captioning generator to generate more accurate andfactually consistent disease descriptions. Our framework can find most similarreports for a given disease from the CXR database by retrieving adisease-oriented mask consisting of the position and morphologicalcharacteristics. By referencing the disease-oriented similar report and thevisual features, the factual consistency model can generate a more accurateradiology report.", "output": "Reading Radiology Imaging Like The Radiologist."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose a novel approach called Operational SupportEstimator Networks (OSENs) for the support estimation task. Support Estimation(SE) is defined as finding the locations of non-zero elements in a sparsesignal. By its very nature, the mapping between the measurement and sparsesignal is a non-linear operation. Traditional support estimators rely oncomputationally expensive iterative signal recovery techniques to achieve suchnon-linearity. Contrary to the convolution layers, the proposed OSEN approachconsists of operational layers that can learn such complex non-linearitieswithout the need for deep networks. In this way, the performance of thenon-iterative support estimation is greatly improved. Moreover, the operationallayers comprise so-called generative textit{super neurons} with non-localkernels. The kernel location for each neuron/feature map is optimized jointlyfor the SE task during the training. We evaluate the OSENs in three differentapplications: i. support estimation from Compressive Sensing (CS) measurements,ii. representation-based classification, and iii. learning-aided CSreconstruction where the output of OSENs is used as prior knowledge to the CSalgorithm for an enhanced reconstruction. Experimental results show that theproposed approach achieves computational efficiency and outperforms competingmethods, especially at low measurement rates by a significant margin. Thesoftware implementation is publicly shared at", "output": "Operational Support Estimator Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of incomplete utterance rewriting has recently gotten muchattention. Previous models struggled to extract information from the dialoguecontext, as evidenced by the low restoration scores. To address this issue, wepropose a novel sequence tagging-based model, which is more adept at extractinginformation from context. Meanwhile, we introduce speaker-aware embedding tomodel speaker variation. Experiments on multiple public datasets show that ourmodel achieves optimal results on all nine restoration scores while havingother metric scores comparable to previous state-of-the-art models.Furthermore, benefitting from the model's simplicity, our approach outperformsmost previous models on inference speed.", "output": "Incomplete Utterance Rewriting as Sequential Greedy Tagging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The degradation of sewer pipes poses significant economical, environmentaland health concerns. The maintenance of such assets requires structured plansto perform inspections, which are more efficient when structural andenvironmental features are considered along with the results of previousinspection reports. The development of such plans requires degradation modelsthat can be based on statistical and machine learning methods. This workproposes a methodology to assess their suitability to plan inspectionsconsidering three dimensions: accuracy metrics, ability to produce long-termdegradation curves and explainability. Results suggest that although ensemblemodels yield the highest accuracy, they are unable to infer the long-termdegradation of the pipes, whereas the Logistic Regression offers a slightlyless accurate model that is able to produce consistent degradation curves witha high explainability. A use case is presented to demonstrate this methodologyand the efficiency of model-based planning compared to the current inspectionplan.", "output": "Assessment of the suitability of degradation models for the planning of CCTV inspections of sewer pipes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In X-ray Computed Tomography (CT), projections from many angles are acquiredand used for 3D reconstruction. To make CT suitable for in-line qualitycontrol, reducing the number of angles while maintaining reconstruction qualityis necessary. Sparse-angle tomography is a popular approach for obtaining 3Dreconstructions from limited data. To optimize its performance, one can adaptscan angles sequentially to select the most informative angles for each scannedobject. Mathematically, this corresponds to solving and optimal experimentaldesign (OED) problem. OED problems are high-dimensional, non-convex, bi-leveloptimization problems that cannot be solved online, i.e., during the scan. Toaddress these challenges, we pose the OED problem as a partially observableMarkov decision process in a Bayesian framework, and solve it through deepreinforcement learning. The approach learns efficient non-greedy policies tosolve a given class of OED problems through extensive offline training ratherthan solving a given OED problem directly via numerical optimization. As such,the trained policy can successfully find the most informative scan anglesonline. We use a policy training method based on the Actor-Critic approach andevaluate its performance on 2D tomography with synthetic data.", "output": "Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Physically informed neural networks (PINNs) are a promising emerging methodfor solving differential equations. As in many other deep learning approaches,the choice of PINN design and training protocol requires careful craftsmanship.Here, we suggest a comprehensive theoretical framework that sheds light on thisimportant problem. Leveraging an equivalence between infinitelyover-parameterized neural networks and Gaussian process regression (GPR), wederive an integro-differential equation that governs PINN prediction in thelarge data-set limit -- the Neurally-Informed Equation (NIE). This equationaugments the original one by a kernel term reflecting architecture choices andallows quantifying implicit bias induced by the network via a spectraldecomposition of the source term in the original differential equation.", "output": "Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Photoplethysmography (PPG) signals, typically acquired from wearable devices,hold significant potential for continuous fitness-health monitoring. Inparticular, heart conditions that manifest in rare and subtle deviating heartpatterns may be interesting. However, robust and reliable anomaly detectionwithin these data remains a challenge due to the scarcity of labeled data andhigh inter-subject variability. This paper introduces a two-stage frameworkleveraging representation learning and personalization to improve anomalydetection performance in PPG data. The proposed framework first employsrepresentation learning to transform the original PPG signals into a morediscriminative and compact representation. We then apply three differentunsupervised anomaly detection methods for movement detection and biometricidentification. We validate our approach using two different datasets in bothgeneralized and personalized scenarios. The results show that representationlearning significantly improves anomaly detection performance while reducingthe high inter-subject variability. Personalized models further enhance anomalydetection performance, underscoring the role of personalization in PPG-basedfitness-health monitoring systems. The results from biometric identificationshow that it's easier to distinguish a new user from one intended authorizeduser than from a group of users. Overall, this study provides evidence of theeffectiveness of representation learning and personalization for anomalydetection in PPG data.", "output": "Personalized Anomaly Detection in PPG Data using Representation Learning and Biometric Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Audio-Visual Event Localization (AVEL) is the task of temporally localizingand classifying emph{audio-visual events}, i.e., events simultaneously visibleand audible in a video. In this paper, we solve AVEL in a weakly-supervisedsetting, where only video-level event labels (their presence/absence, but nottheir locations in time) are available as supervision for training. Our idea isto use a base model to estimate labels on the training data at a finer temporalresolution than at the video level and re-train the model with these labels.I.e., we determine the subset of labels for each emph{slice} of frames in atraining video by (i) replacing the frames outside the slice with those from asecond video having no overlap in video-level labels, and (ii) feeding thissynthetic video into the base model to extract labels for just the slice inquestion. To handle the out-of-distribution nature of our synthetic videos, wepropose an auxiliary objective for the base model that induces more reliablepredictions of the localized event labels as desired. Our three-stage pipelineoutperforms several existing AVEL methods with no architectural changes andimproves performance on a related weakly-supervised task as well.", "output": "Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding how the dynamics in biological and artificial neural networksimplement the computations required for a task is a salient open question inmachine learning and neuroscience. In particular, computations requiringcomplex memory storage and retrieval pose a significant challenge for thesenetworks to implement or learn. Recently, a family of models described byneural ordinary differential equations (nODEs) has emerged as powerfuldynamical neural network models capable of capturing complex dynamics. Here, weextend nODEs by endowing them with adaptive timescales using gatinginteractions. We refer to these as gated neural ODEs (gnODEs). Using a taskthat requires memory of continuous quantities, we demonstrate the inductivebias of the gnODEs to learn (approximate) continuous attractors. We furthershow how reduced-dimensional gnODEs retain their modeling power while greatlyimproving interpretability, even allowing explicit visualization of thestructure of learned attractors. We introduce a novel measure of expressivitywhich probes the capacity of a neural network to generate complex trajectories.Using this measure, we explore how the phase-space dimension of the nODEs andthe complexity of the function modeling the flow field contribute toexpressivity. We see that a more complex function for modeling the flow fieldallows a lower-dimensional nODE to capture a given target dynamics. Finally, wedemonstrate the benefit of gating in nODEs on several real-world tasks.", "output": "Trainability, Expressivity and Interpretability in Gated Neural ODEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bayesian network (BN) structure discovery algorithms typically either makeassumptions about the sparsity of the true underlying network, or are limitedby computational constraints to networks with a small number of variables.While these sparsity assumptions can take various forms, frequently theassumptions focus on an upper bound for the maximum in-degree of the underlyinggraph $nabla_G$. Theorem 2 in Duttweiler et. al. (2023) demonstrates that thelargest eigenvalue of the normalized inverse covariance matrix ($Omega$) of alinear BN is a lower bound for $nabla_G$. Building on this result, this paperprovides the asymptotic properties of, and a debiasing procedure for, thesample eigenvalues of $Omega$, leading to a hypothesis test that may be usedto determine if the BN has max in-degree greater than 1. A linear BN structurediscovery workflow is suggested in which the investigator uses this hypothesistest to aid in selecting an appropriate structure discovery algorithm. Thehypothesis test performance is evaluated through simulations and the workflowis demonstrated on data from a human psoriasis study.", "output": "Testing Sparsity Assumptions in Bayesian Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph learning methods, such as Graph Neural Networks (GNNs) based on graphconvolutions, are highly successful in solving real-world learning problemsinvolving graph-structured data. However, graph learning methods exposesensitive user information and interactions not only through their modelparameters but also through their model predictions. Consequently, standardDifferential Privacy (DP) techniques that merely offer model weight privacy areinadequate. This is especially the case for node predictions that leverageneighboring node attributes directly via graph convolutions that createadditional risks of privacy leakage. To address this problem, we introduceGraph Differential Privacy (GDP), a new formal DP framework tailored to graphlearning settings that ensures both provably private model parameters andpredictions. Furthermore, since there may be different privacy requirements forthe node attributes and graph structure, we introduce a novel notion of relaxednode-level data adjacency. This relaxation can be used for establishingguarantees for different degrees of graph topology privacy while maintainingnode attribute privacy. Importantly, this relaxation reveals a useful trade-offbetween utility and topology privacy for graph learning methods. In addition,our analysis of GDP reveals that existing DP-GNNs fail to exploit thistrade-off due to the complex interplay between graph topology and attributedata in standard graph convolution designs. To mitigate this problem, weintroduce the Differentially Private Decoupled Graph Convolution (DPDGC) model,which benefits from decoupled graph convolution while providing GDP guarantees.Extensive experiments on seven node classification benchmarking datasetsdemonstrate the superior privacy-utility trade-off of DPDGC over existingDP-GNNs based on standard graph convolution design.", "output": "Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy-based models are a simple yet powerful class of probabilistic models,but their widespread adoption has been limited by the computational burden oftraining them. We propose a novel loss function called Energy Discrepancy (ED)which does not rely on the computation of scores or expensive Markov chainMonte Carlo. We show that ED approaches the explicit score matching andnegative log-likelihood loss under different limits, effectively interpolatingbetween both. Consequently, minimum ED estimation overcomes the problem ofnearsightedness encountered in score-based estimation methods, while alsoenjoying theoretical guarantees. Through numerical experiments, we demonstratethat ED learns low-dimensional data distributions faster and more accuratelythan explicit score matching or contrastive divergence. For high-dimensionalimage data, we describe how the manifold hypothesis puts limitations on ourapproach and demonstrate the effectiveness of energy discrepancy by trainingthe energy-based model as a prior of a variational decoder model.", "output": "Energy Discrepancies: A Score-Independent Loss for Energy-Based Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When measuring rare processes at Belle II, a huge luminosity is required,which means a large number of simulations are necessary to determine signalefficiencies and background contributions. However, this process demands highcomputation costs while most of the simulated data, in particular in case ofbackground, are discarded by the event selection. Thus, filters using graphneural networks are introduced at an early stage to save the resources for thedetector simulation and reconstruction of events discarded at analysis level.In our work, we improved the performance of the filters using graph attentionand investigated statistical methods including sampling and reweighting to dealwith the biases introduced by the filtering.", "output": "Improved selective background Monte Carlo simulation at Belle II with graph attention networks and weighted events."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The computation necessary for training Transformer-based language models hasskyrocketed in recent years. This trend has motivated research on efficienttraining algorithms designed to improve training, validation, and downstreamperformance faster than standard training. In this work, we revisit threecategories of such algorithms: dynamic architectures (layer stacking, layerdropping), batch selection (selective backprop, RHO loss), and efficientoptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixedcomputation budget using such methods, we find that their training, validation,and downstream gains vanish compared to a baseline with a fully-decayedlearning rate. We define an evaluation protocol that enables computation to bedone on arbitrary machines by mapping all computation time to a referencemachine which we call reference system time. We discuss the limitations of ourproposed protocol and release our code to encourage rigorous research inefficient training procedures: ", "output": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study sensor/agent data collection and collaboration policies forparameter estimation, accounting for resource constraints and correlationbetween observations collected by distinct sensors/agents. Specifically, weconsider a group of sensors/agents each samples from different variables of amultivariate Gaussian distribution and has different estimation objectives, andwe formulate a sensor/agent's data collection and collaboration policy designproblem as a Fisher information maximization (or Cramer-Rao bound minimization)problem. When the knowledge of correlation between variables is available, weanalytically identify two particular scenarios: (1) where the knowledge of thecorrelation between samples cannot be leveraged for collaborative estimationpurposes and (2) where the optimal data collection policy involves investingscarce resources to collaboratively sample and transfer information that is notof immediate interest and whose statistics are already known, with the solegoal of increasing the confidence on the estimate of the parameter of interest.When the knowledge of certain correlation is unavailable but collaboration maystill be worthwhile, we propose novel ways to apply multi-armed banditalgorithms to learn the optimal data collection and collaboration policy in ourdistributed parameter estimation problem and demonstrate that the proposedalgorithms, DOUBLE-F, DOUBLE-Z, UCB-F, UCB-Z, are effective throughsimulations.", "output": "On Collaboration in Distributed Parameter Estimation with Resource Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a numerical methodology for finding the closed-loopNash equilibrium of stochastic delay differential games through deep learning.These games are prevalent in finance and economics where multi-agentinteraction and delayed effects are often desired features in a model, but areintroduced at the expense of increased dimensionality of the problem. Thisincreased dimensionality is especially significant as that arising from thenumber of players is coupled with the potential infinite dimensionality causedby the delay. Our approach involves parameterizing the controls of each playerusing distinct recurrent neural networks. These recurrent neural network-basedcontrols are then trained using a modified version of Brown's fictitious play,incorporating deep learning techniques. To evaluate the effectiveness of ourmethodology, we test it on finance-related problems with known solutions.Furthermore, we also develop new problems and derive their analytical Nashequilibrium solutions, which serve as additional benchmarks for assessing theperformance of our proposed deep learning approach.", "output": "Stochastic Delay Differential Games: Financial Modeling and Machine Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Obtaining rigorous statistical guarantees for generalization underdistribution shift remains an open and active research area. We study a settingwe call combinatorial distribution shift, where (a) under the test- andtraining-distributions, the labels $z$ are determined by pairs of features$(x,y)$, (b) the training distribution has coverage of certain marginaldistributions over $x$ and $y$ separately, but (c) the test distributioninvolves examples from a product distribution over $(x,y)$ that is {not}covered by the training distribution. Focusing on the special case where thelabels are given by bilinear embeddings into a Hilbert space $H$: $mathbb{E}[zmid x,y ]=langle f_{star}(x),g_{star}(y)rangle_{{H}}$, we aim toextrapolate to a test distribution domain that is $not$ covered in training,i.e., achieving bilinear combinatorial extrapolation.Our setting generalizes a special case of matrix completion frommissing-not-at-random data, for which all existing results require theground-truth matrices to be either exactly low-rank, or to exhibit very sharpspectral cutoffs. In this work, we develop a series of theoretical results thatenable bilinear combinatorial extrapolation under gradual spectral decay asobserved in typical high-dimensional data, including novel algorithms,generalization guarantees, and linear-algebraic results. A key tool is a novelperturbation bound for the rank-$k$ singular value decomposition approximationsbetween two matrices that depends on the relative spectral gap rather than theabsolute spectral gap, a result that may be of broader independent interest.", "output": "Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated classifiers (ACs), often built via supervised machine learning(SML), can categorize large, statistically powerful samples of data rangingfrom text to images and video, and have become widely popular measurementdevices in communication science and related fields. Despite this popularity,even highly accurate classifiers make errors that cause misclassification biasand misleading results in downstream analyses-unless such analyses account forthese errors. As we show in a systematic literature review of SML applications,communication scholars largely ignore misclassification bias. In principle,existing statistical methods can use \"gold standard\" validation data, such asthat created by human annotators, to correct misclassification bias and produceconsistent estimates. We introduce and test such methods, including a newmethod we design and implement in the R package misclassificationmodels, viaMonte Carlo simulations designed to reveal each method's limitations, which wealso release. Based on our results, we recommend our new error correctionmethod as it is versatile and efficient. In sum, automated classifiers, eventhose below common accuracy standards or making systematic misclassifications,can be useful for measurement with careful study design and appropriate errorcorrection methods.", "output": "Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models are susceptible to adversarial samples in white andblack-box environments. Although previous studies have shown high attacksuccess rates, coupling DNN models with interpretation models could offer asense of security when a human expert is involved, who can identify whether agiven sample is benign or malicious. However, in white-box environments,interpretable deep learning systems (IDLSes) have been shown to be vulnerableto malicious manipulations. In black-box settings, as access to the componentsof IDLSes is limited, it becomes more challenging for the adversary to fool thesystem. In this work, we propose a Query-efficient Score-based black-box attackagainst IDLSes, QuScore, which requires no knowledge of the target model andits coupled interpretation model. QuScore is based on transfer-based andscore-based methods by employing an effective microbial genetic algorithm. Ourmethod is designed to reduce the number of queries necessary to carry outsuccessful attacks, resulting in a more efficient process. By continuouslyrefining the adversarial samples created based on feedback scores from theIDLS, our approach effectively navigates the search space to identifyperturbations that can fool the system. We evaluate the attack's effectivenesson four CNN models (Inception, ResNet, VGG, DenseNet) and two interpretationmodels (CAM, Grad), using both ImageNet and CIFAR datasets. Our results showthat the proposed approach is query-efficient with a high attack success ratethat can reach between 95% and 100% and transferability with an average successrate of 69% in the ImageNet and CIFAR datasets. Our attack method generatesadversarial examples with attribution maps that resemble benign samples. Wehave also demonstrated that our attack is resilient against variouspreprocessing defense techniques and can easily be transferred to different DNNmodels.", "output": "Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Objective: The artificial pancreas (AP) has shown promising potential inachieving closed-loop glucose control for individuals with type 1 diabetesmellitus (T1DM). However, designing an effective control policy for the APremains challenging due to the complex physiological processes, delayed insulinresponse, and inaccurate glucose measurements. While model predictive control(MPC) offers safety and stability through the dynamic model and safetyconstraints, it lacks individualization and is adversely affected byunannounced meals. Conversely, deep reinforcement learning (DRL) providespersonalized and adaptive strategies but faces challenges with distributionshifts and substantial data requirements. Methods: We propose a hybrid controlpolicy for the artificial pancreas (HyCPAP) to address the above challenges.HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging thestrengths of both policies while compensating for their respective limitations.To facilitate faster deployment of AP systems in real-world settings, wefurther incorporate meta-learning techniques into HyCPAP, leveraging previousexperience and patient-shared knowledge to enable fast adaptation to newpatients with limited available data. Results: We conduct extensive experimentsusing the FDA-accepted UVA/Padova T1DM simulator across three scenarios. Ourapproaches achieve the highest percentage of time spent in the desiredeuglycemic range and the lowest occurrences of hypoglycemia. Conclusion: Theresults clearly demonstrate the superiority of our methods for closed-loopglucose management in individuals with T1DM. Significance: The study presentsnovel control policies for AP systems, affirming the great potential ofproposed methods for efficient closed-loop glucose control.", "output": "Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Beliefs and values are increasingly being incorporated into our AI systemsthrough alignment processes, such as carefully curating data collectionprinciples or regularizing the loss function used for training. However, themeta-alignment problem is that these human beliefs are diverse and not alignedacross populations; furthermore, the implicit strength of each belief may notbe well calibrated even among humans, especially when trying to generalizeacross contexts. Specifically, in high regret situations, we observe thatcontextual counterfactuals and recourse costs are particularly important inupdating a decision maker's beliefs and the strengths to which such beliefs areheld. Therefore, we argue that including counterfactuals is key to an accuratecalibration of beliefs during alignment. To do this, we first segment beliefdiversity into two categories: subjectivity (across individuals within apopulation) and epistemic uncertainty (within an individual across differentcontexts). By leveraging our notion of epistemic uncertainty, we introduce `thebelief calibration cycle' framework to more holistically calibrate thisdiversity of beliefs with context-driven counterfactual reasoning by using amulti-objective optimization. We empirically apply our framework for finding aPareto frontier of clustered optimal belief strengths that generalize acrossdifferent contexts, demonstrating its efficacy on a toy dataset for creditdecisions.", "output": "Leveraging Contextual Counterfactuals Toward Belief Calibration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine Learning (ML) systems, particularly when deployed in high-stakesdomains, are deeply consequential. They can exacerbate existing inequities,create new modes of discrimination, and reify outdated social constructs.Accordingly, the social context (i.e. organisations, teams, cultures) in whichML systems are developed is a site of active research for the field of AIethics, and intervention for policymakers. This paper focuses on one aspect ofsocial context that is often overlooked: interactions between practitioners andthe tools they rely on, and the role these interactions play in shaping MLpractices and the development of ML systems. In particular, through anempirical study of questions asked on the Stack Exchange forums, the use ofinteractive computing platforms (e.g. Jupyter Notebook and Google Colab) in MLpractices is explored. I find that interactive computing platforms are used ina host of learning and coordination practices, which constitutes aninfrastructural relationship between interactive computing platforms and MLpractitioners. I describe how ML practices are co-evolving alongside thedevelopment of interactive computing platforms, and highlight how this risksmaking invisible aspects of the ML life cycle that AI ethics researchers' havedemonstrated to be particularly salient for the societal impact of deployed MLsystems.", "output": "Machine Learning practices and infrastructures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Drug discovery is adapting to novel technologies such as data science,informatics, and artificial intelligence (AI) to accelerate effective treatmentdevelopment while reducing costs and animal experiments. AI is transformingdrug discovery, as indicated by increasing interest from investors, industrialand academic scientists, and legislators. Successful drug discovery requiresoptimizing properties related to pharmacodynamics, pharmacokinetics, andclinical outcomes. This review discusses the use of AI in the three pillars ofdrug discovery: diseases, targets, and therapeutic modalities, with a focus onsmall molecule drugs. AI technologies, such as generative chemistry, machinelearning, and multi-property optimization, have enabled several compounds toenter clinical trials. The scientific community must carefully vet knowninformation to address the reproducibility crisis. The full potential of AI indrug discovery can only be realized with sufficient ground truth andappropriate human intervention at later pipeline stages.", "output": "Artificial Intelligence for Drug Discovery: Are We There Yet?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has proven effective in solving variousproblems by generating internal supervisory signals. Unsupervised anomalydetection, which faces the high cost of obtaining true labels, is an area thatcan greatly benefit from SSL. However, recent literature suggests that tuningthe hyperparameters (HP) of data augmentation functions is crucial to thesuccess of SSL-based anomaly detection (SSAD), yet a systematic method fordoing so remains unknown. In this work, we propose DSV (Discordance andSeparability Validation), an unsupervised validation loss to selecthigh-performing detection models with effective augmentation HPs. DSV capturesthe alignment between an augmentation function and the anomaly-generatingmechanism with surrogate losses, which approximate the discordance andseparability of test data, respectively. As a result, the evaluation via DSVleads to selecting an effective SSAD model exhibiting better alignment, whichresults in high detection accuracy. We theoretically derive the degree ofapproximation conducted by the surrogate losses and empirically show that DSVoutperforms a wide range of baselines on 21 real-world tasks.", "output": "DSV: An Alignment Validation Loss for Self-supervised Outlier Model Selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently Chen and Poor initiated the study of learning mixtures of lineardynamical systems. While linear dynamical systems already have wide-rangingapplications in modeling time-series data, using mixture models can lead to abetter fit or even a richer understanding of underlying subpopulationsrepresented in the data. In this work we give a new approach to learningmixtures of linear dynamical systems that is based on tensor decompositions. Asa result, our algorithm succeeds without strong separation conditions on thecomponents, and can be used to compete with the Bayes optimal clustering of thetrajectories. Moreover our algorithm works in the challengingpartially-observed setting. Our starting point is the simple but powerfulobservation that the classic Ho-Kalman algorithm is a close relative of moderntensor decomposition methods for learning latent variable models. This gives usa playbook for how to extend it to work with more complicated generativemodels.", "output": "Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study addressed the complex task of sentiment analysis on a dataset of119,988 original tweets from Weibo using a Convolutional Neural Network (CNN),offering a new approach to Natural Language Processing (NLP). The data, sourcedfrom Baidu's PaddlePaddle AI platform, were meticulously preprocessed,tokenized, and categorized based on sentiment labels. A CNN-based model wasutilized, leveraging word embeddings for feature extraction, and trained toperform sentiment classification. The model achieved a macro-average F1-scoreof approximately 0.73 on the test set, showing balanced performance acrosspositive, neutral, and negative sentiments. The findings underscore theeffectiveness of CNNs for sentiment analysis tasks, with implications forpractical applications in social media analysis, market research, and policystudies. The complete experimental content and code have been made publiclyavailable on the Kaggle data platform for further research and development.Future work may involve exploring different architectures, such as RecurrentNeural Networks (RNN) or transformers, or using more complex pre-trained modelslike BERT, to further improve the model's ability to understand linguisticnuances and context.", "output": "Convolutional Neural Networks for Sentiment Analysis on Weibo Data: A Natural Language Processing Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inverse reinforcement learning (IRL) algorithms often rely on (forward)reinforcement learning or planning over a given time horizon to compute anapproximately optimal policy for a hypothesized reward function and then matchthis policy with expert demonstrations. The time horizon plays a critical rolein determining both the accuracy of reward estimate and the computationalefficiency of IRL algorithms. Interestingly, an effective time horizon shorterthan the ground-truth value often produces better results faster. This workformally analyzes this phenomenon and provides an explanation: the time horizoncontrols the complexity of an induced policy class and mitigates overfittingwith limited data. This analysis leads to a principled choice of the effectivehorizon for IRL. It also prompts us to reexamine the classic IRL formulation:it is more natural to learn jointly the reward and the effective horizontogether rather than the reward alone with a given horizon. Our experimentalresults confirm the theoretical analysis.", "output": "On the Effective Horizon of Inverse Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lung cancer is the leading cause of cancer death and early diagnosis isassociated with a positive prognosis. Chest X-ray (CXR) provides an inexpensiveimaging mode for lung cancer diagnosis. Suspicious nodules are difficult todistinguish from vascular and bone structures using CXR. Computer vision haspreviously been proposed to assist human radiologists in this task, however,leading studies use down-sampled images and computationally expensive methodswith unproven generalization. Instead, this study localizes lung nodules usingefficient encoder-decoder neural networks that process full resolution imagesto avoid any signal loss resulting from down-sampling. Encoder-decoder networksare trained and tested using the JSRT lung nodule dataset. The networks areused to localize lung nodules from an independent external CXR dataset.Sensitivity and false positive rates are measured using an automated frameworkto eliminate any observer subjectivity. These experiments allow for thedetermination of the optimal network depth, image resolution and pre-processingpipeline for generalized lung nodule localization. We find that nodulelocalization is influenced by subtlety, with more subtle nodules being detectedin earlier training epochs. Therefore, we propose a novel self-ensemble modelfrom three consecutive epochs centered on the validation optimum. This ensembleachieved a sensitivity of 85% in 10-fold internal testing with false positivesof 8 per image. A sensitivity of 81% is achieved at a false positive rate of 6following morphological false positive reduction. This result is comparable tomore computationally complex systems based on linear and spatial filtering, butwith a sub-second inference time that is faster than other methods. Theproposed algorithm achieved excellent generalization results against anexternal dataset with sensitivity of 77% at a false positive rate of 7.6.", "output": "Full-resolution Lung Nodule Segmentation from Chest X-ray Images using Residual Encoder-Decoder Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores the expressive power of deep neural networks for adiverse range of activation functions. An activation function set $mathscr{A}$is defined to encompass the majority of commonly used activation functions,such as $mathtt{ReLU}$, $mathtt{LeakyReLU}$, $mathtt{ReLU}^2$,$mathtt{ELU}$, $mathtt{SELU}$, $mathtt{Softplus}$, $mathtt{GELU}$,$mathtt{SiLU}$, $mathtt{Swish}$, $mathtt{Mish}$, $mathtt{Sigmoid}$,$mathtt{Tanh}$, $mathtt{Arctan}$, $mathtt{Softsign}$, $mathtt{dSiLU}$, and$mathtt{SRS}$. We demonstrate that for any activation function $varrhoinmathscr{A}$, a $mathtt{ReLU}$ network of width $N$ and depth $L$ can beapproximated to arbitrary precision by a $varrho$-activated network of width$6N$ and depth $2L$ on any bounded set. This finding enables the extension ofmost approximation results achieved with $mathtt{ReLU}$ networks to a widevariety of other activation functions, at the cost of slightly largerconstants.", "output": "Deep Network Approximation: Beyond ReLU to Diverse Activation Functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Detection of Volatile Organic Compounds (VOCs) from the breath is becoming aviable route for the early detection of diseases non-invasively. This paperpresents a sensor array with three metal oxide electrodes that can use machinelearning methods to identify four distinct VOCs in a mixture. The metal oxidesensor array was subjected to various VOC concentrations, including ethanol,acetone, toluene and chloroform. The dataset obtained from individual gases andtheir mixtures were analyzed using multiple machine learning algorithms, suchas Random Forest (RF), K-Nearest Neighbor (KNN), Decision Tree, LinearRegression, Logistic Regression, Naive Bayes, Linear Discriminant Analysis,Artificial Neural Network, and Support Vector Machine. KNN and RF have shownmore than 99% accuracy in classifying different varying chemicals in the gasmixtures. In regression analysis, KNN has delivered the best results with R2value of more than 0.99 and LOD of 0.012, 0.015, 0.014 and 0.025 PPM forpredicting the concentrations of varying chemicals Acetone, Toluene, Ethanol,and Chloroform, respectively in complex mixtures. Therefore, it is demonstratedthat the array utilizing the provided algorithms can classify and predict theconcentrations of the four gases simultaneously for disease diagnosis andtreatment monitoring.", "output": "Metal Oxide-based Gas Sensor Array for the VOCs Analysis in Complex Mixtures using Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prescriptive process monitoring methods seek to optimize the performance ofbusiness processes by triggering interventions at runtime, thereby increasingthe probability of positive case outcomes. These interventions are triggeredaccording to an intervention policy. Reinforcement learning has been putforward as an approach to learning intervention policies through trial anderror. Existing approaches in this space assume that the number of resourcesavailable to perform interventions in a process is unlimited, an unrealisticassumption in practice. This paper argues that, in the presence of resourceconstraints, a key dilemma in the field of prescriptive process monitoring isto trigger interventions based not only on predictions of their necessity,timeliness, or effect but also on the uncertainty of these predictions and thelevel of resource utilization. Indeed, committing scarce resources to anintervention when the necessity or effects of this intervention are highlyuncertain may intuitively lead to suboptimal intervention effects. Accordingly,the paper proposes a reinforcement learning approach for prescriptive processmonitoring that leverages conformal prediction techniques to consider theuncertainty of the predictions upon which an intervention decision is based. Anevaluation using real-life datasets demonstrates that explicitly modelinguncertainty using conformal predictions helps reinforcement learning agentsconverge towards policies with higher net intervention gain", "output": "Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has been widely used in many fields, but the model trainingprocess usually consumes massive computational resources and time. Therefore,designing an efficient neural network training method with a provableconvergence guarantee is a fundamental and important research question. In thispaper, we present a static half-space report data structure that consists of afully connected two-layer neural network for shifted ReLU activation to enableactivated neuron identification in sublinear time via geometric search. We alsoprove that our algorithm can converge in $O(M^2/epsilon^2)$ time with networksize quadratic in the coefficient norm upper bound $M$ and error term$epsilon$.", "output": "Efficient SGD Neural Network Training via Sublinear Activated Neuron Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For prediction of clustered time-to-event data, we propose a new deep neuralnetwork based gamma frailty model (DNN-FM). An advantage of the proposed modelis that the joint maximization of the new h-likelihood provides maximumlikelihood estimators for fixed parameters and best unbiased predictors forrandom frailties. Thus, the proposed DNN-FM is trained by using a negativeprofiled h-likelihood as a loss function, constructed by profiling out thenon-parametric baseline hazard. Experimental studies show that the proposedmethod enhances the prediction performance of the existing methods. A real dataanalysis shows that the inclusion of subject-specific frailties helps toimprove prediction of the DNN based Cox model (DNN-Cox).", "output": "Deep Neural Networks for Semiparametric Frailty Models via H-likelihood."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, the no-box adversarial attack, in which the attacker lacks accessto the model's architecture, weights, and training data, become the mostpractical and challenging attack setup. However, there is an unawareness of thepotential and flexibility inherent in the surrogate model selection process onno-box setting. Inspired by the burgeoning interest in utilizing foundationalmodels to address downstream tasks, this paper adopts an innovative idea that1) recasting adversarial attack as a downstream task. Specifically, image noisegeneration to meet the emerging trend and 2) introducing foundational models assurrogate models. Harnessing the concept of non-robust features, we elaborateon two guiding principles for surrogate model selection to explain why thefoundational model is an optimal choice for this role. However, paradoxically,we observe that these foundational models underperform. Analyzing thisunexpected behavior within the feature space, we attribute the lacklusterperformance of foundational models (e.g., CLIP) to their significantrepresentational capacity and, conversely, their lack of discriminativeprowess. To mitigate this issue, we propose the use of a margin-based lossstrategy for the fine-tuning of foundational models on target images. Theexperimental results verify that our approach, which employs the basic FastGradient Sign Method (FGSM) attack algorithm, outstrips the performance ofother, more convoluted algorithms. We conclude by advocating for the researchcommunity to consider surrogate models as crucial determinants in theeffectiveness of adversarial attacks in no-box settings. The implications ofour work bear relevance for improving the efficacy of such adversarial attacksand the overall robustness of AI systems.", "output": "Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The performance of data fusion and tracking algorithms often depends onparameters that not only describe the sensor system, but can also betask-specific. While for the sensor system tuning these variables istime-consuming and mostly requires expert knowledge, intrinsic parameters oftargets under track can even be completely unobservable until the system isdeployed. With state-of-the-art sensor systems growing more and more complex,the number of parameters naturally increases, necessitating the automaticoptimization of the model variables. In this paper, the parameters of aninteracting multiple model (IMM) filter are optimized solely usingmeasurements, thus without necessity for any ground-truth data. The resultingmethod is evaluated through an ablation study on simulated data, where thetrained model manages to match the performance of a filter parametrized withground-truth values.", "output": "Learning IMM Filter Parameters from Measurements using Gradient Descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we consider online distributed learning problems. Onlinedistributed learning refers to the process of training learning models ondistributed data sources. In our setting a set of agents need to cooperativelytrain a learning model from streaming data. Differently from federatedlearning, the proposed approach does not rely on a central server but only onpeer-to-peer communications among the agents. This approach is often used inscenarios where data cannot be moved to a centralized location due to privacy,security, or cost reasons. In order to overcome the absence of a centralserver, we propose a distributed algorithm that relies on a quantized,finite-time coordination protocol to aggregate the locally trained models.Furthermore, our algorithm allows for the use of stochastic gradients duringlocal training. Stochastic gradients are computed using a randomly sampledsubset of the local training data, which makes the proposed algorithm moreefficient and scalable than traditional gradient descent. In our paper, weanalyze the performance of the proposed algorithm in terms of the mean distancefrom the online solution. Finally, we present numerical results for a logisticregression task.", "output": "Online Distributed Learning with Quantized Finite-Time Coordination."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work investigates the application of quantum machine learning techniquesfor classical and quantum communication across different qubit channel models.By employing parameterized quantum circuits and a flexible channel noise model,we develop a machine learning framework to generate quantum channel codes andevaluate their effectiveness. We explore classical, entanglement-assisted, andquantum communication scenarios within our framework. Applying it to variousquantum channel models as proof of concept, we demonstrate strong performancein each case. Our results highlight the potential of quantum machine learningin advancing research on quantum communication systems, enabling a betterunderstanding of capacity bounds under modulation constraints, variouscommunication settings, and diverse channel models.", "output": "Quantum Autoencoders for Learning Quantum Channel Codes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation (KD) has shown great potential for transferringknowledge from a complex teacher model to a simple student model in which theheavy learning task can be accomplished efficiently and without losing too muchprediction accuracy. Recently, many attempts have been made by applying the KDmechanism to the graph representation learning models such as graph neuralnetworks (GNNs) to accelerate the model's inference speed via student models.However, many existing KD-based GNNs utilize MLP as a universal approximator inthe student model to imitate the teacher model's process without consideringthe graph knowledge from the teacher model. In this work, we provide a KD-basedframework on multi-scaled GNNs, known as graph framelet, and prove that byadequately utilizing the graph knowledge in a multi-scaled manner provided bygraph framelet decomposition, the student model is capable of adapting bothhomophilic and heterophilic graphs and has the potential of alleviating theover-squashing issue with a simple yet effectively graph surgery. Furthermore,we show how the graph knowledge supplied by the teacher is learned and digestedby the student model via both algebra and geometry. Comprehensive experimentsshow that our proposed model can generate learning accuracy identical to oreven surpass the teacher model while maintaining the high speed of inference.", "output": "Frameless Graph Knowledge Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Decentralized learning algorithms are an essential tool for designingmulti-agent systems, as they enable agents to autonomously learn from theirexperience and past interactions. In this work, we propose a theoretical andalgorithmic framework for real-time identification of the learning dynamicsthat govern agent behavior using a short burst of a single system trajectory.Our method identifies agent dynamics through polynomial regression, where wecompensate for limited data by incorporating side-information constraints thatcapture fundamental assumptions or expectations about agent behavior. Theseconstraints are enforced computationally using sum-of-squares optimization,leading to a hierarchy of increasingly better approximations of the true agentdynamics. Extensive experiments demonstrated that our approach, using only 5samples from a short run of a single trajectory, accurately recovers the truedynamics across various benchmarks, including equilibrium selection andprediction of chaotic systems up to 10 Lyapunov times. These findings suggestthat our approach has significant potential to support effective policy anddecision-making in strategic multi-agent systems.", "output": "Discovering How Agents Learn Using Few Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The fat-shattering dimension characterizes the uniform convergence propertyof real-valued functions. The state-of-the-art upper bounds feature amultiplicative squared logarithmic factor on the sample complexity, leaving anopen gap with the existing lower bound. We provide an improved uniformconvergence bound that closes this gap.", "output": "An Improved Uniform Convergence Bound with Fat-Shattering Dimension."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting the behavior of real-time traffic (e.g., VoIP) in mobilityscenarios could help the operators to better plan their network infrastructuresand to optimize the allocation of resources. Accordingly, in this work theauthors propose a forecasting analysis of crucial QoS/QoE descriptors (some ofwhich neglected in the technical literature) of VoIP traffic in a real mobileenvironment. The problem is formulated in terms of a multivariate time seriesanalysis. Such a formalization allows to discover and model the temporalrelationships among various descriptors and to forecast their behaviors forfuture periods. Techniques such as Vector Autoregressive models and machinelearning (deep-based and tree-based) approaches are employed and compared interms of performance and time complexity, by reframing the multivariate timeseries problem into a supervised learning one. Moreover, a series of auxiliaryanalyses (stationarity, orthogonal impulse responses, etc.) are performed todiscover the analytical structure of the time series and to provide deepinsights about their relationships. The whole theoretical analysis has anexperimental counterpart since a set of trials across a real-world LTE-Advancedenvironment has been performed to collect, post-process and analyze about600,000 voice packets, organized per flow and differentiated per codec.", "output": "Multivariate Time Series characterization and forecasting of VoIP traffic in real mobile networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Heading towards navigational autonomy in unmanned surface vehicles (USVs) inthe maritime sector can fundamentally lead towards safer waters as well asreduced operating costs, while also providing a range of exciting newcapabilities for oceanic research, exploration and monitoring. However,achieving such a goal is challenging. USV control systems must, safely andreliably, be able to adhere to the international regulations for preventingcollisions at sea (COLREGs) in encounters with other vessels as they navigateto a given waypoint while being affected by realistic weather conditions,either during the day or at night. To deal with the multitude of possiblescenarios, it is critical to have a virtual environment that is able toreplicate the realistic operating conditions USVs will encounter, before theycan be implemented in the real world. Such \"digital twins\" form the foundationsupon which Deep Reinforcement Learning (DRL) and Computer Vision (CV)algorithms can be used to develop and guide USV control systems. In this paperwe describe the novel development of a COLREG-compliant DRL-based collisionavoidant navigational system with CV-based awareness in a realistic oceansimulation environment. The performance of the trained autonomous Agentsresulting from this approach is evaluated in several successful navigations toset waypoints in both open sea and coastal encounters with other vessels. Abinary executable version of the simulator with trained agents is available at", "output": "Aeolus Ocean -- A simulation environment for the autonomous COLREG-compliant navigation of Unmanned Surface Vehicles using Deep Reinforcement Learning and Maritime Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ageing detection and failure prediction are essential in many Internet ofThings (IoT) deployments, which operate huge quantities of embedded devicesunattended in the field for years. In this paper, we present a large-scaleempirical analysis of natural SRAM wear-out using 154 boards from ageneral-purpose testbed. Starting from SRAM initialization bias, which eachnode can easily collect at startup, we apply various metrics for featureextraction and experiment with common machine learning methods to predict theage of operation for this node. Our findings indicate that even though ageingimpacts are subtle, our indicators can well estimate usage times with an $R^2$score of 0.77 and a mean error of 24% using regressors, and with an F1 scoreabove 0.6 for classifiers applying a six-months resolution.", "output": "Ageing Analysis of Embedded SRAM on a Large-Scale Testbed Using Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge Graph Embedding (KGE) models are used to learn continuousrepresentations of entities and relations. A key task in the literature ispredicting missing links between entities. However, Knowledge Graphs are notjust sets of links but also have semantics underlying their structure.Semantics is crucial in several downstream tasks, such as query answering orreasoning. We introduce the subgraph inference task, where a model has togenerate likely and semantically valid subgraphs. We propose IntelliGraphs, aset of five new Knowledge Graph datasets. The IntelliGraphs datasets containsubgraphs with semantics expressed in logical rules for evaluating subgraphinference. We also present the dataset generator that produced the syntheticdatasets. We designed four novel baseline models, which include three modelsbased on traditional KGEs. We evaluate their expressiveness and show that thesemodels cannot capture the semantics. We believe this benchmark will encouragethe development of machine learning models that emphasize semanticunderstanding.", "output": "IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the video prediction task by putting forth a novel model thatcombines (i) our recently proposed hierarchical residual vector quantizedvariational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN(ST-PixelCNN). We refer to this approach as a sequential hierarchical residuallearning vector quantized variational autoencoder (S-HR-VQVAE). By leveragingthe intrinsic capabilities of HR-VQVAE at modeling still images with aparsimonious representation, combined with the ST-PixelCNN's ability athandling spatiotemporal information, S-HR-VQVAE can better deal with chiefchallenges in video prediction. These include learning spatiotemporalinformation, handling high dimensional data, combating blurry prediction, andimplicit modeling of physical characteristics. Extensive experimental resultson the KTH Human Action and Moving-MNIST tasks demonstrate that our modelcompares favorably against top video prediction techniques both in quantitativeand qualitative evaluations despite a much smaller model size. Finally, weboost S-HR-VQVAE by proposing a novel training method to jointly estimate theHR-VQVAE and ST-PixelCNN parameters.", "output": "S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A wide variety of generative models for graphs have been proposed. They areused in drug discovery, road networks, neural architecture search, and programsynthesis. Generating graphs has theoretical challenges, such as isomorphicrepresentations -- evaluating how well a generative model performs isdifficult. Which model to choose depending on the application domain?We extensively study kernel-based metrics on distributions of graphinvariants and manifold-based and kernel-based metrics in graph embeddingspace. Manifold-based metrics outperform kernel-based metrics in embeddingspace. We use these metrics to compare GraphRNN and GRAN, two well-knowngenerative models for graphs, and unveil the influence of node orderings. Itshows the superiority of GRAN over GraphRNN - further, our proposed adaptationof GraphRNN with a depth-first search ordering is effective for small-sizedgraphs.A guideline on good practices regarding dataset selection and node featureinitialization is provided. Our work is accompanied by open-source code andreproducible experiments.", "output": "GRAN is superior to GraphRNN: node orderings, kernel- and graph embeddings-based metrics for graph generators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A wide variety of natural language tasks are currently being addressed withlarge-scale language models (LLMs). These models are usually trained with avery large amount of unsupervised text data and adapted to perform a downstreamnatural language task using methods like fine-tuning, calibration or in-contextlearning. In this work, we propose an approach to adapt the prior classdistribution to perform text classification tasks without the need for labelledsamples and only few in-domain sample queries. The proposed approach treats theLLM as a black box, adding a stage where the model posteriors are calibrated tothe task. Results show that these methods outperform the un-adapted model fordifferent number of training shots in the prompt and a previous approach werecalibration is performed without using any adaptation data.", "output": "Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dialog policies, which determine a system's action based on the current stateat each dialog turn, are crucial to the success of the dialog. In recent years,reinforcement learning (RL) has emerged as a promising option for dialog policylearning (DPL). In RL-based DPL, dialog policies are updated according torewards. The manual construction of fine-grained rewards, such asstate-action-based ones, to effectively guide the dialog policy is challengingin multi-domain task-oriented dialog scenarios with numerous state-action paircombinations. One way to estimate rewards from collected data is to train thereward estimator and dialog policy simultaneously using adversarial learning(AL). Although this method has demonstrated superior performanceexperimentally, it is fraught with the inherent problems of AL, such as modecollapse. This paper first identifies the role of AL in DPL through detailedanalyses of the objective functions of dialog policy and reward estimator.Next, based on these analyses, we propose a method that eliminates AL fromreward estimation and DPL while retaining its advantages. We evaluate ourmethod using MultiWOZ, a multi-domain task-oriented dialog corpus.", "output": "Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study parallel algorithms for the correlation clusteringproblem, where every pair of two different entities is labeled with similar ordissimilar. The goal is to partition the entities into clusters to minimize thenumber of disagreements with the labels. Currently, all efficient parallelalgorithms have an approximation ratio of at least 3. In comparison with the$1.994+epsilon$ ratio achieved by polynomial-time sequential algorithms[CLN22], a significant gap exists.We propose the first poly-logarithmic depth parallel algorithm that achievesa better approximation ratio than 3. Specifically, our algorithm computes a$(2.4+epsilon)$-approximate solution and uses $tilde{O}(m^{1.5})$ work.Additionally, it can be translated into a $tilde{O}(m^{1.5})$-time sequentialalgorithm and a poly-logarithmic rounds sublinear-memory MPC algorithm with$tilde{O}(m^{1.5})$ total memory.Our approach is inspired by Awerbuch, Khandekar, and Rao's [AKR12]length-constrained multi-commodity flow algorithm, where we develop anefficient parallel algorithm to solve a truncated correlation clustering linearprogram of Charikar, Guruswami, and Wirth [CGW05]. Then we show the solution ofthe truncated linear program can be rounded with a factor of at most 2.4 lossby using the framework of [CMSY15]. Such a rounding framework can then beimplemented using parallel pivot-based approaches.", "output": "Breaking 3-Factor Approximation for Correlation Clustering in Polylogarithmic Rounds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series forecasting has received wide interest from existing research dueto its broad applications and inherent challenging. The research challenge liesin identifying effective patterns in historical series and applying them tofuture forecasting. Advanced models based on point-wise connected MLP andTransformer architectures have strong fitting power, but their secondarycomputational complexity limits practicality. Additionally, those structuresinherently disrupt the temporal order, reducing the information utilization andmaking the forecasting process uninterpretable. To solve these problems, thispaper proposes a forecasting model, MPR-Net. It first adaptively decomposesmulti-scale historical series patterns using convolution operation, thenconstructs a pattern extension forecasting method based on the prior knowledgeof pattern reproduction, and finally reconstructs future patterns into futureseries using deconvolution operation. By leveraging the temporal dependenciespresent in the time series, MPR-Net not only achieves linear time complexity,but also makes the forecasting process interpretable. By carrying outsufficient experiments on more than ten real data sets of both short and longterm forecasting tasks, MPR-Net achieves the state of the art forecastingperformance, as well as good generalization and robustness performance.", "output": "MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The integrated development of city clusters has given rise to an increasingdemand for intercity travel. Intercity ride-pooling service exhibitsconsiderable potential in upgrading traditional intercity bus services byimplementing demand-responsive enhancements. Nevertheless, its onlineoperations suffer the inherent complexities due to the coupling of vehicleresource allocation among cities and pooled-ride vehicle routing. To tacklethese challenges, this study proposes a two-level framework designed tofacilitate online fleet management. Specifically, a novel multi-agent feudalreinforcement learning model is proposed at the upper level of the framework tocooperatively assign idle vehicles to different intercity lines, while thelower level updates the routes of vehicles using an adaptive large neighborhoodsearch heuristic. Numerical studies based on the realistic dataset of Xiamenand its surrounding cities in China show that the proposed frameworkeffectively mitigates the supply and demand imbalances, and achievessignificant improvement in both the average daily system profit and orderfulfillment ratio.", "output": "Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling Services: A Multi-Agent Hierarchical Reinforcement Learning Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The learning of Gaussian Mixture Models (also referred to simply as GMMs)plays an important role in machine learning. Known for their expressiveness andinterpretability, Gaussian mixture models have a wide range of applications,from statistics, computer vision to distributional reinforcement learning.However, as of today, few known algorithms can fit or learn these models, someof which include Expectation-Maximization algorithms and Sliced WassersteinDistance. Even fewer algorithms are compatible with gradient descent, thecommon learning process for neural networks.In this paper, we derive a closed formula of two GMMs in the univariate,one-dimensional case, then propose a distance function called Sliced Cram'er2-distance for learning general multivariate GMMs. Our approach has severaladvantages over many previous methods. First, it has a closed-form expressionfor the univariate case and is easy to compute and implement using commonmachine learning libraries (e.g., PyTorch and TensorFlow). Second, it iscompatible with gradient descent, which enables us to integrate GMMs withneural networks seamlessly. Third, it can fit a GMM not only to a set of datapoints, but also to another GMM directly, without sampling from the targetmodel. And fourth, it has some theoretical guarantees like global gradientboundedness and unbiased sampling gradient. These features are especiallyuseful for distributional reinforcement learning and Deep Q Networks, where thegoal is to learn a distribution over future rewards. We will also construct aGaussian Mixture Distributional Deep Q Network as a toy example to demonstrateits effectiveness. Compared with previous models, this model is parameterefficient in terms of representing a distribution and possesses betterinterpretability.", "output": "Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We initiate an empirical investigation into differentially private graphneural networks on population graphs from the medical domain by examiningprivacy-utility trade-offs at different privacy levels on both real-world andsynthetic datasets and performing auditing through membership inferenceattacks. Our findings highlight the potential and the challenges of thisspecific DP application area. Moreover, we find evidence that the underlyinggraph structure constitutes a potential factor for larger performance gaps byshowing a correlation between the degree of graph homophily and the accuracy ofthe trained model.", "output": "Privacy-Utility Trade-offs in Neural Networks for Medical Population Graphs: Insights from Differential Privacy and Graph Structure."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Meta-learning has recently been an emerging data-efficient learning techniquefor various medical imaging operations and has helped advance contemporary deeplearning models. Furthermore, meta-learning enhances the knowledgegeneralization of the imaging tasks by learning both shared and discriminativeweights for various configurations of imaging tasks. However, existingmeta-learning models attempt to learn a single set of weight initializations ofa neural network that might be restrictive for multimodal data. This work aimsto develop a multimodal meta-learning model for image reconstruction, whichaugments meta-learning with evolutionary capabilities to encompass diverseacquisition settings of multimodal data. Our proposed model called KM-MAML(Kernel Modulation-based Multimodal Meta-Learning), has hypernetworks thatevolve to generate mode-specific weights. These weights provide themode-specific inductive bias for multiple modes by re-calibrating each kernelof the base network for image reconstruction via a low-rank kernel modulationoperation. We incorporate gradient-based meta-learning (GBML) in the contextualspace to update the weights of the hypernetworks for different modes. Thehypernetworks and the reconstruction network in the GBML setting providediscriminative mode-specific features and low-level image features,respectively. Experiments on multi-contrast MRI reconstruction show that ourmodel, (i) exhibits superior reconstruction performance over joint training,other meta-learning methods, and context-specific MRI reconstruction methods,and (ii) better adaptation capabilities with improvement margins of 0.5 dB inPSNR and 0.01 in SSIM. Besides, a representation analysis with U-Net shows thatkernel modulation infuses 80% of mode-specific representation changes in thehigh-resolution layers. Our source code is available at", "output": "Generalizing Supervised Deep Learning MRI Reconstruction to Multiple and Unseen Contrasts using Meta-Learning Hypernetworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the last decade, there has been a vast increase in eating disorderdiagnoses and eating disorder-attributed deaths, reaching their zenith duringthe Covid-19 pandemic. This immense growth derived in part from the stressorsof the pandemic but also from increased exposure to social media, which is rifewith content that promotes eating disorders. Such content can induce eatingdisorders in viewers. This study aimed to create a multimodal deep learningmodel capable of determining whether a given social media post promotes eatingdisorders based on a combination of visual and textual data. A labeled datasetof Tweets was collected from Twitter, upon which twelve deep learning modelswere trained and tested. Based on model performance, the most effective deeplearning model was the multimodal fusion of the RoBERTa natural languageprocessing model and the MaxViT image classification model, attaining accuracyand F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusionmodel, deployed to classify an unlabeled dataset of posts from the social mediasites Tumblr and Reddit, generated similar classifications as previous researchstudies that did not employ artificial intelligence, showing that artificialintelligence can develop insights congruent to those of researchers.Additionally, the model was used to conduct a time-series analysis of yetunseen Tweets from eight Twitter hashtags, uncovering that the relativeabundance of pro-eating disorder content has decreased drastically. However,since approximately 2018, pro-eating disorder content has either stopped itsdecline or risen once more in ampleness.", "output": "A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a novel algorithm for crack localisation and detectionbased on visual and tactile analysis via fibre-optics. A finger-shaped sensorbased on fibre-optics is employed for the data acquisition to collect data forthe analysis and the experiments. To detect the possible locations of cracks acamera is used to scan an environment while running an object detectionalgorithm. Once the crack is detected, a fully-connected graph is created froma skeletonised version of the crack. A minimum spanning tree is then employedfor calculating the shortest path to explore the crack which is then used todevelop the motion planner for the robotic manipulator. The motion plannerdivides the crack into multiple nodes which are then explored individually.Then, the manipulator starts the exploration and performs the tactile dataclassification to confirm if there is indeed a crack in that location or just afalse positive from the vision algorithm. If a crack is detected, also thelength, width, orientation and number of branches are calculated. This isrepeated until all the nodes of the crack are explored.In order to validate the complete algorithm, various experiments areperformed: comparison of exploration of cracks through full scan and motionplanning algorithm, implementation of frequency-based features for crackclassification and geometry analysis using a combination of vision and tactiledata. From the results of the experiments, it is shown that the proposedalgorithm is able to detect cracks and improve the results obtained from visionto correctly classify cracks and their geometry with minimal cost thanks to themotion planning algorithm.", "output": "Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we address the challenge of using energy-based models toproduce high-quality, label-specific data in complex structured datasets, suchas population genetics, RNA or protein sequences data. Traditional trainingmethods encounter difficulties due to inefficient Markov chain Monte Carlomixing, which affects the diversity of synthetic data and increases generationtimes. To address these issues, we use a novel training algorithm that exploitsnon-equilibrium effects. This approach, applied on the Restricted BoltzmannMachine, improves the model's ability to correctly classify samples andgenerate high-quality synthetic data in only a few sampling steps. Theeffectiveness of this method is demonstrated by its successful application tofour different types of data: handwritten digits, mutations of human genomesclassified by continental origin, functionally characterized sequences of anenzyme protein family, and homologous RNA sequences from specific taxonomies.", "output": "Fast and Functional Structured Data Generators Rooted in Out-of-Equilibrium Physics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A data-driven parametric model order reduction (MOR) method using a deepartificial neural network is proposed. The present network, which is theleast-squares hierarchical variational autoencoder (LSH-VAE), is capable ofperforming nonlinear MOR for the parametric interpolation of a nonlineardynamic system with a significant number of degrees of freedom. LSH-VAEexploits two major changes to the existing networks: a hierarchical deepstructure and a hybrid weighted, probabilistic loss function. The enhancementsresult in a significantly improved accuracy and stability compared against theconventional nonlinear MOR methods, autoencoder, and variational autoencoder.Upon LSH-VAE, a parametric MOR framework is presented based on the sphericallylinear interpolation of the latent manifold. The present framework is validatedand evaluated on three nonlinear and multiphysics dynamic systems. First, thepresent framework is evaluated on the fluid-structure interaction benchmarkproblem to assess its efficiency and accuracy. Then, a highly nonlinearaeroelastic phenomenon, limit cycle oscillation, is analyzed. Finally, thepresent framework is applied to a three-dimensional fluid flow to demonstrateits capability of efficiently analyzing a significantly large number of degreesof freedom. The performance of LSH-VAE is emphasized by comparing its resultsagainst that of the widely used nonlinear MOR methods, convolutionalautoencoder, and $beta$-VAE. The present framework exhibits a significantlyenhanced accuracy to the conventional methods while still exhibiting a largespeed-up factor.", "output": "Data-driven Nonlinear Parametric Model Order Reduction Framework using Deep Hierarchical Variational Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we investigate the use of the learned digital back-propagation(LDBP) for equalizing dual-polarization fiber-optic transmission indispersion-managed (DM) links. LDBP is a deep neural network that optimizes theparameters of DBP using the stochastic gradient descent. We evaluate DBP andLDBP in a simulated WDM dual-polarization fiber transmission system operatingat the bitrate of 256 Gbit/s per channel, with a dispersion map designed for a2016 km link with 15% residual dispersion. Our results show that insingle-channel transmission, LDBP achieves an effective signal-to-noise ratioimprovement of 6.3 dB and 2.5 dB, respectively, over linear equalization andDBP. In WDM transmission, the corresponding $Q$-factor gains are 1.1 dB and 0.4dB, respectively. Additionally, we conduct a complexity analysis, which revealsthat a frequency-domain implementation of LDBP and DBP is more favorable interms of complexity than the time-domain implementation. These findingsdemonstrate the effectiveness of LDBP in mitigating the nonlinear effects in DMfiber-optic transmission systems.", "output": "Equalization in Dispersion-Managed Systems Using Learned Digital Back-Propagation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of Tiny Machine Learning (TinyML) has made substantial advancementsin democratizing machine learning on low-footprint devices, such asmicrocontrollers. The prevalence of these miniature devices raises the questionof whether aggregating their knowledge can benefit TinyML applications.Federated meta-learning is a promising answer to this question, as it addressesthe scarcity of labeled data and heterogeneous data distribution across devicesin the real world. However, deploying TinyML hardware faces unique resourceconstraints, making existing methods impractical due to energy, privacy, andcommunication limitations. We introduce TinyMetaFed, a model-agnosticmeta-learning framework suitable for TinyML. TinyMetaFed facilitatescollaborative training of a neural network initialization that can be quicklyfine-tuned on new devices. It offers communication savings and privacyprotection through partial local reconstruction and Top-P% selectivecommunication, computational efficiency via online learning, and robustness toclient heterogeneity through few-shot learning. The evaluations on three TinyMLuse cases demonstrate that TinyMetaFed can significantly reduce energyconsumption and communication overhead, accelerate convergence, and stabilizethe training process.", "output": "TinyMetaFed: Efficient Federated Meta-Learning for TinyML."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern data-driven science, reproducibility and reusability are keychallenges. Scientists are well skilled in the process from data topublication. Although some publication channels require source code and data tobe made accessible, rerunning and verifying experiments is usually hard due toa lack of standards. Therefore, reusing existing scientific data processingcode from state-of-the-art research is hard as well. This is why we introduceCLAIMED, which has a proven track record in scientific research for addressingthe repeatability and reusability issues in modern data-driven science. CLAIMEDis a framework to build reusable operators and scalable scientific workflows bysupporting the scientist to draw from previous work by re-composing workflowsfrom existing libraries of coarse-grained scientific operators. Althoughvarious implementations exist, CLAIMED is programming language, scientificlibrary, and execution environment agnostic.", "output": "CLAIMED -- the open source framework for building coarse-grained operators for accelerated discovery in science."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization (DG) is about learning models that generalize well tonew domains that are related to, but different from, the training domain(s). Itis a fundamental problem in machine learning and has attracted much attentionin recent years. A large number of approaches have been proposed. Differentapproaches are motivated from different perspectives, making it difficult togain an overall understanding of the area. In this paper, we propose a causalframework for domain generalization and present an understanding of common DGapproaches in the framework. Our work sheds new lights on the followingquestions: (1) What are the key ideas behind each DG method? (2) Why is itexpected to improve generalization to new domains theoretically? (3) How aredifferent DG methods related to each other and what are relative advantages andlimitations? By providing a unified perspective on DG, we hope to helpresearchers better understand the underlying principles and develop moreeffective approaches for this critical problem in machine learning.", "output": "A Causal Framework to Unify Common Domain Generalization Approaches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In their seminal 1990 paper, Wasserman and Kadane establish an upper boundfor the Bayes' posterior probability of a measurable set $A$, when the priorlies in a class of probability measures $mathcal{P}$ and the likelihood isprecise. They also give a sufficient condition for such upper bound to holdwith equality. In this paper, we introduce a generalization of their result byadditionally addressing uncertainty related to the likelihood. We give an upperbound for the posterior probability when both the prior and the likelihoodbelong to a set of probabilities. Furthermore, we give a sufficient conditionfor this upper bound to become an equality. This result is interesting on itsown, and has the potential of being applied to various fields of engineering(e.g. model predictive control), machine learning, and artificial intelligence.", "output": "A Novel Bayes' Theorem for Upper Probabilities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article introduces a new method to improve the dependability ofmillimeter-wave (mmWave) and terahertz (THz) network services in dynamicoutdoor environments. In these settings, line-of-sight (LoS) connections areeasily interrupted by moving obstacles like humans and vehicles. The proposedapproach, coined as Radar-aided Dynamic blockage Recognition (RaDaR), leveragesradar measurements and federated learning (FL) to train a dual-output neuralnetwork (NN) model capable of simultaneously predicting blockage status andtime. This enables determining the optimal point for proactive handover (PHO)or beam switching, thereby reducing the latency introduced by 5G new radioprocedures and ensuring high quality of experience (QoE). The framework employsradar sensors to monitor and track objects movement, generating range-angle andrange-velocity maps that are useful for scene analysis and predictions.Moreover, FL provides additional benefits such as privacy protection,scalability, and knowledge sharing. The framework is assessed using anextensive real-world dataset comprising mmWave channel information and radardata. The evaluation results show that RaDaR substantially enhances networkreliability, achieving an average success rate of 94% for PHO compared toexisting reactive HO procedures that lack proactive blockage prediction.Additionally, RaDaR maintains a superior QoE by ensuring sustained highthroughput levels and minimising PHO latency.", "output": "Enhancing Reliability in Federated mmWave Networks: A Practical and Scalable Solution using Radar-Aided Dynamic Blockage Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building on the success of PC-JeDi we introduce PC-Droid, a substantiallyimproved diffusion model for the generation of jet particle clouds. Byleveraging a new diffusion formulation, studying more recent integrationsolvers, and training on all jet types simultaneously, we are able to achievestate-of-the-art performance for all types of jets across all evaluationmetrics. We study the trade-off between generation speed and quality bycomparing two attention based architectures, as well as the potential ofconsistency distillation to reduce the number of diffusion steps. Both thefaster architecture and consistency models demonstrate performance surpassingmany competing models, with generation time up to two orders of magnitudefaster than PC-JeDi.", "output": "PC-Droid: Faster diffusion and improved quality for particle cloud generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Regression algorithms are regularly used for improving the accuracy ofsatellite precipitation products. In this context, ground-based measurementsare the dependent variable and the satellite data are the predictor variables,together with topography factors. Alongside this, it is increasingly recognisedin many fields that combinations of algorithms through ensemble learning canlead to substantial predictive performance improvements. Still, a sufficientnumber of ensemble learners for improving the accuracy of satelliteprecipitation products and their large-scale comparison are currently missingfrom the literature. In this work, we fill this specific gap by proposing 11new ensemble learners in the field and by extensively comparing them for theentire contiguous United States and for a 15-year period. We use monthly datafrom the PERSIANN (Precipitation Estimation from Remotely Sensed Informationusing Artificial Neural Networks) and IMERG (Integrated Multi-satellitERetrievals for GPM) gridded datasets. We also use gauge-measured precipitationdata from the Global Historical Climatology Network monthly database, version 2(GHCNm). The ensemble learners combine the predictions by six regressionalgorithms (base learners), namely the multivariate adaptive regression splines(MARS), multivariate adaptive polynomial splines (poly-MARS), random forests(RF), gradient boosting machines (GBM), extreme gradient boosting (XGBoost) andBayesian regularized neural networks (BRNN), and each of them is based on adifferent combiner. The combiners include the equal-weight combiner, the mediancombiner, two best learners and seven variants of a sophisticated stackingmethod. The latter stacks a regression algorithm on the top of the baselearners to combine their independent predictions...", "output": "Ensemble learning for blending gridded satellite and gauge-measured precipitation data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper addresses the efficient management of Mobile Access Points (MAPs),which are Unmanned Aerial Vehicles (UAV), in 5G networks. We propose atwo-level hierarchical architecture, which dynamically reconfigures the networkwhile considering Integrated Access-Backhaul (IAB) constraints. The high-layerdecision process determines the number of MAPs through consensus, and wedevelop a joint optimization process to account for co-dependence in networkself-management. In the low-layer, MAPs manage their placement using adouble-attention based Deep Reinforcement Learning (DRL) model that encouragescooperation without retraining. To improve generalization and reducecomplexity, we propose a federated mechanism for training and sharing oneplacement model for every MAP in the low-layer. Additionally, we jointlyoptimize the placement and backhaul connectivity of MAPs using amulti-objective reward function, considering the impact of varying MAPplacement on wireless backhaul connectivity.", "output": "Federated Multi-Agent Deep Reinforcement Learning for Dynamic and Flexible 3D Operation of 5G Multi-MAP Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we consider the network latency estimation, which has been animportant metric for network performance. However, a large scale of networklatency estimation requires a lot of computing time. Therefore, we propose anew method that is much faster and maintains high accuracy. The data structureof network nodes can form a matrix, and the tensor model can be formed byintroducing the time dimension. Thus, the entire problem can be be summarizedas a tensor completion problem. The main idea of our method is improving thetensor leverage sampling strategy and introduce tensor QR decomposition intotensor completion. To achieve faster tensor leverage sampling, we replacetensor singular decomposition (t-SVD) with tensor CSVD-QR to appoximate t-SVD.To achieve faster completion for incomplete tensor, we use the tensor$L_{2,1}$-norm rather than traditional tensor nuclear norm. Furthermore, weintroduce tensor QR decomposition into alternating direction method ofmultipliers (ADMM) framework. Numerical experiments witness that our method isfaster than state-of-art algorithms with satisfactory accuracy.", "output": "Tensor Completion via Leverage Sampling and Tensor QR Decomposition for Network Latency Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Noise injection is a fundamental tool for data augmentation, and yet there isno widely accepted procedure to incorporate it with learning frameworks. Thisstudy analyzes the effects of adding or applying different noise models ofvarying magnitudes to Convolutional Neural Network (CNN) architectures. Noisemodels that are distributed with different density functions are given commonmagnitude levels via Structural Similarity (SSIM) metric in order to create anappropriate ground for comparison. The basic results are conforming with themost of the common notions in machine learning, and also introduce some novelheuristics and recommendations on noise injection. The new approaches willprovide better understanding on optimal learning procedures for imageclassification.", "output": "Data Augmentation in Training CNNs: Injecting Noise to Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present a novel approach for improving the quality andconsistency of generated outputs from large-scale pre-trained language models(LLMs). Self-consistency has emerged as an effective approach for prompts withfixed answers, selecting the answer with the highest number of votes. In thispaper, we introduce a generalized framework for self-consistency that extendsits applicability beyond problems that have fixed-answer answers. Throughextensive simulations, we demonstrate that our approach consistently recoversthe optimal or near-optimal generation from a set of candidates. We alsopropose lightweight parameter-free similarity functions that show significantand consistent improvements across code generation, autoformalization, andsummarization tasks, even without access to token log probabilities. Our methodincurs minimal computational overhead, requiring no auxiliary reranker modelsor modifications to the existing model.", "output": "Self-consistency for open-ended generations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Global change is predicted to induce shifts in anuran acoustic behavior,which can be studied through passive acoustic monitoring (PAM). Understandingchanges in calling behavior requires the identification of anuran species,which is challenging due to the particular characteristics of neotropicalsoundscapes. In this paper, we introduce a large-scale multi-species dataset ofanuran amphibians calls recorded by PAM, that comprises 27 hours of expertannotations for 42 different species from two Brazilian biomes. We provide openaccess to the dataset, including the raw recordings, experimental setup code,and a benchmark with a baseline model of the fine-grained categorizationproblem. Additionally, we highlight the challenges of the dataset to encouragemachine learning researchers to solve the problem of anuran call identificationtowards conservation policy. All our experiments and resources can be found onour GitHub repository ", "output": "AnuraSet: A dataset for benchmarking Neotropical anuran calls identification in passive acoustic monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A robot deployed in a home over long stretches of time faces a true lifelonglearning problem. As it seeks to provide assistance to its users, the robotshould leverage any accumulated experience to improve its own knowledge tobecome a more proficient assistant. We formalize this setting with a novellifelong learning problem formulation in the context of learning for task andmotion planning (TAMP). Exploiting the modularity of TAMP systems, we develop agenerative mixture model that produces candidate continuous parameters for aplanner. Whereas most existing lifelong learning approaches determine a priorihow data is shared across task models, our approach learns shared andnon-shared models and determines which to use online during planning based onauxiliary tasks that serve as a proxy for each model's understanding of astate. Our method exhibits substantial improvements in planning success onsimulated 2D domains and on several problems from the BEHAVIOR benchmark.", "output": "Embodied Lifelong Learning for Task and Motion Planning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Local authorities in England, such as Leicestershire County Council (LCC),provide Early Help services that can be offered at any point in a youngperson's life when they experience difficulties that cannot be supported byuniversal services alone, such as schools. This paper investigates theutilisation of machine learning (ML) to assist experts in identifying familiesthat may need to be referred for Early Help assessment and support. LCCprovided an anonymised dataset comprising 14360 records of young people underthe age of 18. The dataset was pre-processed, machine learning models werebuild, and experiments were conducted to validate and test the performance ofthe models. Bias mitigation techniques were applied to improve the fairness ofthese models. During testing, while the models demonstrated the capability toidentify young people requiring intervention or early help, they also produceda significant number of false positives, especially when constructed withimbalanced data, incorrectly identifying individuals who most likely did notneed an Early Help referral. This paper empirically explores the suitability ofdata-driven ML models for identifying young people who may require Early Helpservices and discusses their appropriateness and limitations for this task.", "output": "Identifying Early Help Referrals For Local Authorities With Machine Learning And Bias Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of continual learning in the domain of reinforcement learning,often called non-stationary reinforcement learning, has been identified as animportant challenge to the application of reinforcement learning. We prove aworst-case complexity result, which we believe captures this challenge:Modifying the probabilities or the reward of a single state-action pair in areinforcement learning problem requires an amount of time almost as large asthe number of states in order to keep the value function up to date, unless thestrong exponential time hypothesis (SETH) is false; SETH is a widely acceptedstrengthening of the P $neq$ NP conjecture. Recall that the number of statesin current applications of reinforcement learning is typically astronomical. Incontrast, we show that just $textit{adding}$ a new state-action pair isconsiderably easier to implement.", "output": "The complexity of non-stationary reinforcement learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Delays and asynchrony are inevitable in large-scale machine-learning problemswhere communication plays a key role. As such, several works have extensivelyanalyzed stochastic optimization with delayed gradients. However, as far as weare aware, no analogous theory is available for min-max optimization, a topicthat has gained recent popularity due to applications in adversarialrobustness, game theory, and reinforcement learning. Motivated by this gap, weexamine the performance of standard min-max optimization algorithms withdelayed gradient updates. First, we show (empirically) that even small delayscan cause prominent algorithms like Extra-gradient (texttt{EG}) to diverge onsimple instances for which texttt{EG} guarantees convergence in the absence ofdelays. Our empirical study thus suggests the need for a careful analysis ofdelayed versions of min-max optimization algorithms. Accordingly, undersuitable technical assumptions, we prove that Gradient Descent-Ascent(texttt{GDA}) and texttt{EG} with delayed updates continue to guaranteeconvergence to saddle points for convex-concave and strongly convex-stronglyconcave settings. Our complexity bounds reveal, in a transparent manner, theslow-down in convergence caused by delays.", "output": "Min-Max Optimization under Delays."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Feature learning, i.e. extracting meaningful representations of data, isquintessential to the practical success of neural networks trained withgradient descent, yet it is notoriously difficult to explain how and why itoccurs. Recent theoretical studies have shown that shallow neural networksoptimized on a single task with gradient-based methods can learn meaningfulfeatures, extending our understanding beyond the neural tangent kernel orrandom feature regime in which negligible feature learning occurs. But inpractice, neural networks are increasingly often trained on {em many} taskssimultaneously with differing loss functions, and these prior analyses do notgeneralize to such settings. In the multi-task learning setting, a variety ofstudies have shown effective feature learning by simple linear models. However,multi-task learning via {em nonlinear} models, arguably the most commonlearning paradigm in practice, remains largely mysterious. In this work, wepresent the first results proving feature learning occurs in a multi-tasksetting with a nonlinear model. We show that when the tasks are binaryclassification problems with labels depending on only $r$ directions within theambient $dgg r$-dimensional input space, executing a simple gradient-basedmultitask learning algorithm on a two-layer ReLU neural network learns theground-truth $r$ directions. In particular, any downstream task on the $r$ground-truth coordinates can be solved by learning a linear classifier withsample and neuron complexity independent of the ambient dimension $d$, while arandom feature model requires exponential complexity in $d$ for such aguarantee.", "output": "Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpreting the inner workings of deep learning models is crucial forestablishing trust and ensuring model safety. Concept-based explanations haveemerged as a superior approach that is more interpretable than featureattribution estimates such as pixel saliency. However, defining the conceptsfor the interpretability analysis biases the explanations by the user'sexpectations on the concepts. To address this, we propose a novel post-hocunsupervised method that automatically uncovers the concepts learned by deepmodels during training. By decomposing the latent space of a layer in singularvectors and refining them by unsupervised clustering, we uncover conceptvectors aligned with directions of high variance that are relevant to the modelprediction, and that point to semantically distinct concepts. Our extensiveexperiments reveal that the majority of our concepts are readily understandableto humans, exhibit coherency, and bear relevance to the task at hand. Moreover,we showcase the practical utility of our method in dataset exploration, whereour concept vectors successfully identify outlier training samples affected byvarious confounding factors. This novel exploration technique has remarkableversatility to data types and model architectures and it will facilitate theidentification of biases and the discovery of sources of error within trainingdata.", "output": "Uncovering Unique Concept Vectors through Latent Space Decomposition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stochastic Gradient Descent (SGD) is one of the simplest and most popularalgorithms in modern statistical and machine learning due to its computationaland memory efficiency. Various averaging schemes have been proposed toaccelerate the convergence of SGD in different settings. In this paper, weexplore a general averaging scheme for SGD. Specifically, we establish theasymptotic normality of a broad range of weighted averaged SGD solutions andprovide asymptotically valid online inference approaches. Furthermore, wepropose an adaptive averaging scheme that exhibits both optimal statisticalrate and favorable non-asymptotic convergence, drawing insights from theoptimal weight for the linear model in terms of non-asymptotic mean squarederror (MSE).", "output": "Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Persons with visual impairments (PwVI) have difficulties understanding andnavigating spaces around them. Current wayfinding technologies either focussolely on navigation or provide limited communication about the environment.Motivated by recent advances in visual-language grounding and semanticnavigation, we propose DRAGON, a guiding robot powered by a dialogue system andthe ability to associate the environment with natural language. Byunderstanding the commands from the user, DRAGON is able to guide the user tothe desired landmarks on the map, describe the environment, and answerquestions from visual observations. Through effective utilization of dialogue,the robot can ground the user's free-form descriptions to landmarks in theenvironment, and give the user semantic information through spoken language. Weconduct a user study with blindfolded participants in an everyday indoorenvironment. Our results demonstrate that DRAGON is able to communicate withthe user smoothly, provide a good guiding experience, and connect users withtheir surrounding environment in an intuitive manner.", "output": "DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image (T2I) personalization allows users to guide the creative imagegeneration process by combining their own visual concepts in natural languageprompts. Recently, encoder-based techniques have emerged as a new effectiveapproach for T2I personalization, reducing the need for multiple images andlong training times. However, most existing encoders are limited to asingle-class domain, which hinders their ability to handle diverse concepts. Inthis work, we propose a domain-agnostic method that does not require anyspecialized dataset or prior information about the personalized concepts. Weintroduce a novel contrastive-based regularization technique to maintain highfidelity to the target concept characteristics while keeping the predictedembeddings close to editable regions of the latent space, by pushing thepredicted tokens toward their nearest existing CLIP tokens. Our experimentalresults demonstrate the effectiveness of our approach and show how the learnedtokens are more semantic than tokens predicted by unregularized models. Thisleads to a better representation that achieves state-of-the-art performancewhile being more flexible than previous methods.", "output": "Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL)can enhance model adaptation by leveraging more sensitive and distributed datawhile preserving data privacy. However, few studies have focused on thismethod. Therefore, we conduct the first comprehensive empirical study toevaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). Wedemonstrate that FDAPT can maintain competitive downstream task performance tothe centralized baseline in both IID and non-IID situations. Furthermore, wepropose a novel algorithm, Frozen Federated Domain-adaptive Pre-training(FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average andexhibits similar downstream task performance to standard FDAPT, with generalperformance fluctuations remaining less than 1%. Finally, through a criticalevaluation of our work, we identify promising future research directions forthis new research area.", "output": "FDAPT: Federated Domain-adaptive Pre-training for Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable Artificial Intelligence (XAI) has received widespread interest inrecent years, and two of the most popular types of explanations are featureattributions, and counterfactual explanations. These classes of approaches havebeen largely studied independently and the few attempts at reconciling themhave been primarily empirical. This work establishes a clear theoreticalconnection between game-theoretic feature attributions, focusing on but notlimited to SHAP, and counterfactuals explanations. After motivating operativechanges to Shapley values based feature attributions and counterfactualexplanations, we prove that, under conditions, they are in fact equivalent. Wethen extend the equivalency result to game-theoretic solution concepts beyondShapley values. Moreover, through the analysis of the conditions of suchequivalence, we shed light on the limitations of naively using counterfactualexplanations to provide feature importances. Experiments on three datasetsquantitatively show the difference in explanations at every stage of theconnection between the two approaches and corroborate the theoretical findings.", "output": "On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose the In-context Autoencoder (ICAE) for context compression in alarge language model (LLM). The ICAE has two modules: a learnable encoderadapted with LoRA from an LLM for compressing a long context into a limitednumber of memory slots, and a fixed decoder which is the target LLM that cancondition on the memory slots for various purposes. We first pretrain the ICAEusing both autoencoding and language modeling objectives on massive text data,enabling it to generate memory slots that accurately and comprehensivelyrepresent the original context. Then, we fine-tune the pretrained ICAE on asmall amount of instruct data to enhance its interaction with various promptsfor producing desirable responses. Our experimental results demonstrate thatthe ICAE learned with our proposed pretraining and fine-tuning paradigm caneffectively produce memory slots with $4times$ context compression, which canbe well conditioned on by the target LLM to respond to various prompts. Thepromising results demonstrate significant implications of the ICAE for itsnovel approach to the long context problem and its potential to reducecomputation and memory overheads for LLM inference in practice, suggestingfurther research effort in context management for an LLM. Our code and datawill be released shortly.", "output": "In-context Autoencoder for Context Compression in a Large Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Personalization has emerged as a prominent aspect within the field ofgenerative AI, enabling the synthesis of individuals in diverse contexts andstyles, while retaining high-fidelity to their identities. However, the processof personalization presents inherent challenges in terms of time and memoryrequirements. Fine-tuning each personalized model needs considerable GPU timeinvestment, and storing a personalized model per subject can be demanding interms of storage capacity. To overcome these challenges, we proposeHyperDreamBooth-a hypernetwork capable of efficiently generating a small set ofpersonalized weights from a single image of a person. By composing theseweights into the diffusion model, coupled with fast finetuning, HyperDreamBoothcan generate a person's face in various contexts and styles, with high subjectdetails while also preserving the model's crucial knowledge of diverse stylesand semantic modifications. Our method achieves personalization on faces inroughly 20 seconds, 25x faster than DreamBooth and 125x faster than TextualInversion, using as few as one reference image, with the same quality and stylediversity as DreamBooth. Also our method yields a model that is 10000x smallerthan a normal DreamBooth model. Project page: ", "output": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Agents that can learn to imitate given video observation -- emph{withoutdirect access to state or action information} are more applicable to learningin the natural world. However, formulating a reinforcement learning (RL) agentthat facilitates this goal remains a significant challenge. We approach thischallenge using contrastive training to learn a reward function comparing anagent's behaviour with a single demonstration. We use a Siamese recurrentneural network architecture to learn rewards in space and time between motionclips while training an RL policy to minimize this distance. Throughexperimentation, we also find that the inclusion of multi-task data andadditional image encoding losses improve the temporal consistency of thelearned rewards and, as a result, significantly improves policy learning. Wedemonstrate our approach on simulated humanoid, dog, and raptor agents in 2Dand a quadruped and a humanoid in 3D. We show that our method outperformscurrent state-of-the-art techniques in these environments and can learn toimitate from a single video demonstration.", "output": "Towards Learning to Imitate from a Single Video Demonstration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Regulation of Multi-Agent Systems (MAS) and Declarative ElectronicInstitutions (DEIs) was a multidisciplinary research topic of the past decadeinvolving (Physical and Software) Agents and Law since the beginning, butrecently evolved towards News-claimed Robot Lawyer since 2016. One of thesefirst proposals of restricting the behaviour of Software Agentswas ElectronicInstitutions.However, with the recent reformulation of Artificial NeuralNetworks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legalissues regarding the use of DL has raised concerns in the ArtificialIntelligence (AI) Community. Now that the Regulation of MAS is almost correctlyaddressed, we propose the Regulation of Artificial Neural Networks asAgent-based Training of a special type of regulated Artificial Neural Networkthat we call Institutional Neural Network (INN).The main purpose of this paperis to bring attention to Artificial Teaching (AT) and to give a tentativeanswer showing a proof-of-concept implementation of Regulated Deep Learning(RDL). This paper introduces the former concept and provide sI, a languagepreviously used to model declaratively and extend Electronic Institutions, as ameans to regulate the execution of Artificial Neural Networks and theirinteractions with Artificial Teachers (ATs)", "output": "Declarative Mechanism Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is common to use networks to encode the architecture of interactionsbetween entities in complex systems in the physical, biological, social, andinformation sciences. To study the large-scale behavior of complex systems, itis useful to examine mesoscale structures in networks as building blocks thatinfluence such behavior. We present a new approach for describing low-rankmesoscale structures in networks, and we illustrate our approach using severalsynthetic network models and empirical friendship, collaboration, andprotein--protein interaction (PPI) networks. We find that these networkspossess a relatively small number of `latent motifs' that together cansuccessfully approximate most subgraphs of a network at a fixed mesoscale. Weuse an algorithm for `network dictionary learning' (NDL), which combines anetwork-sampling method and nonnegative matrix factorization, to learn thelatent motifs of a given network. The ability to encode a network using a setof latent motifs has a wide variety of applications to network-analysis tasks,such as comparison, denoising, and edge inference. Additionally, using a newnetwork denoising and reconstruction (NDR) algorithm, we demonstrate how todenoise a corrupted network by using only the latent motifs that one learnsdirectly from the corrupted network.", "output": "Learning low-rank latent mesoscale structures in networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many coordination problems, independently reasoning humans are able todiscover mutually compatible policies. In contrast, independently trainedself-play policies are often mutually incompatible. Zero-shot coordination(ZSC) has recently been proposed as a new frontier in multi-agent reinforcementlearning to address this fundamental issue. Prior work approaches the ZSCproblem by assuming players can agree on a shared learning algorithm but not onlabels for actions and observations, and proposes other-play as an optimalsolution. However, until now, this \"label-free\" problem has only beeninformally defined. We formalize this setting as the label-free coordination(LFC) problem by defining the label-free coordination game. We show thatother-play is not an optimal solution to the LFC problem as it fails toconsistently break ties between incompatible maximizers of the other-playobjective. We introduce an extension of the algorithm, other-play withtie-breaking, and prove that it is optimal in the LFC problem and anequilibrium in the LFC game. Since arbitrary tie-breaking is precisely what theZSC setting aims to prevent, we conclude that the LFC problem does not reflectthe aims of ZSC. To address this, we introduce an alternative informaloperationalization of ZSC as a starting point for future work.", "output": "A New Formalism, Method and Open Issues for Zero-Shot Coordination."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a balanced coarsening scheme for multilevel hypergraphpartitioning. In addition, an initial partitioning algorithm is designed toimprove the quality of k-way hypergraph partitioning. By assigning vertexweights through the LPT algorithm, we generate a prior hypergraph under arelaxed balance constraint. With the prior hypergraph, we have defined theWasserstein discrepancy to coordinate the optimal transport of coarseningprocess. And the optimal transport matrix is solved by Sinkhorn algorithm. Ourcoarsening scheme fully takes into account the minimization of connectivitymetric (objective function). For the initial partitioning stage, we define anormalized cut function induced by Fiedler vector, which is theoreticallyproved to be a concave function. Thereby, a three-point algorithm is designedto find the best cut under the balance constraint.", "output": "Balanced Coarsening for Multilevel Hypergraph Partitioning via Wasserstein Discrepancy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The capability of Deep Neural Networks (DNNs) to recognize objects inorientations outside the distribution of the training data is not wellunderstood. We present evidence that DNNs are capable of generalizing toobjects in novel orientations by disseminating orientation-invariance obtainedfrom familiar objects seen from many viewpoints. This capability strengthenswhen training the DNN with an increasing number of familiar objects, but onlyin orientations that involve 2D rotations of familiar orientations. We showthat this dissemination is achieved via neurons tuned to common featuresbetween familiar and unfamiliar objects. These results implicate brain-likeneural mechanisms for generalization.", "output": "Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Projecting climate change is a generalization problem: we extrapolate therecent past using physical models across past, present, and future climates.Current climate models require representations of processes that occur atscales smaller than model grid size, which have been the main source of modelprojection uncertainty. Recent machine learning (ML) algorithms hold promise toimprove such process representations, but tend to extrapolate poorly to climateregimes they were not trained on. To get the best of the physical andstatistical worlds, we propose a new framework -- termed \"climate-invariant\" ML-- incorporating knowledge of climate processes into ML algorithms, and showthat it can maintain high accuracy across a wide range of climate andgeographic conditions in three distinct atmospheric models. Our results suggestthat explicitly incorporating physical knowledge into data-driven models ofEarth system processes can improve their consistency, data efficiency, andgeneralizability across climate regimes.", "output": "Climate-Invariant Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We revisit the method of mixture technique, also known as the Laplace method,to study the concentration phenomenon in generic exponential families.Combining the properties of Bregman divergence associated with log-partitionfunction of the family with the method of mixtures for super-martingales, weestablish a generic bound controlling the Bregman divergence between theparameter of the family and a finite sample estimate of the parameter. Ourbound is time-uniform and makes appear a quantity extending the classicalinformation gain to exponential families, which we call the Bregman informationgain. For the practitioner, we instantiate this novel bound to severalclassical families, e.g., Gaussian, Bernoulli, Exponential, Weibull, Pareto,Poisson and Chi-square yielding explicit forms of the confidence sets and theBregman information gain. We further numerically compare the resultingconfidence bounds to state-of-the-art alternatives for time-uniformconcentration and show that this novel method yields competitive results.Finally, we highlight the benefit of our concentration bounds on someillustrative applications.", "output": "Bregman Deviations of Generic Exponential Families."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning is a process which can update decision rules, based on pastexperience, such that future performance improves. Traditionally, machinelearning is often evaluated under the assumption that the future will beidentical to the past in distribution or change adversarially. But theseassumptions can be either too optimistic or pessimistic for many problems inthe real world. Real world scenarios evolve over multiple spatiotemporal scaleswith partially predictable dynamics. Here we reformulate the learning problemto one that centers around this idea of dynamic futures that are partiallylearnable. We conjecture that certain sequences of tasks are notretrospectively learnable (in which the data distribution is fixed), but areprospectively learnable (in which distributions may be dynamic), suggestingthat prospective learning is more difficult in kind than retrospectivelearning. We argue that prospective learning more accurately characterizes manyreal world problems that (1) currently stymie existing artificial intelligencesolutions and/or (2) lack adequate explanations for how natural intelligencessolve them. Thus, studying prospective learning will lead to deeper insightsand solutions to currently vexing challenges in both natural and artificialintelligences.", "output": "Prospective Learning: Principled Extrapolation to the Future."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider stochastic optimization problems where data is drawn from aMarkov chain. Existing methods for this setting crucially rely on knowing themixing time of the chain, which in real-world applications is usually unknown.We propose the first optimization method that does not require the knowledge ofthe mixing time, yet obtains the optimal asymptotic convergence rate whenapplied to convex problems. We further show that our approach can be extendedto: (i) finding stationary points in non-convex optimization with Markoviandata, and (ii) obtaining better dependence on the mixing time in temporaldifference (TD) learning; in both cases, our method is completely oblivious tothe mixing time. Our method relies on a novel combination of multi-level MonteCarlo (MLMC) gradient estimation together with an adaptive learning method.", "output": "Adapting to Mixing Time in Stochastic Optimization with Markovian Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bayesian policy reuse (BPR) is a general policy transfer framework forselecting a source policy from an offline library by inferring the task beliefbased on some observation signals and a trained observation model. In thispaper, we propose an improved BPR method to achieve more efficient policytransfer in deep reinforcement learning (DRL). First, most BPR algorithms usethe episodic return as the observation signal that contains limited informationand cannot be obtained until the end of an episode. Instead, we employ thestate transition sample, which is informative and instantaneous, as theobservation signal for faster and more accurate task inference. Second, BPRalgorithms usually require numerous samples to estimate the probabilitydistribution of the tabular-based observation model, which may be expensive andeven infeasible to learn and maintain, especially when using the statetransition sample as the signal. Hence, we propose a scalable observation modelbased on fitting state transition functions of source tasks from only a smallnumber of samples, which can generalize to any signals observed in the targettask. Moreover, we extend the offline-mode BPR to the continual learningsetting by expanding the scalable observation model in a plug-and-play fashion,which can avoid negative transfer when faced with new unknown tasks.Experimental results show that our method can consistently facilitate fasterand more efficient policy transfer.", "output": "Efficient Bayesian Policy Reuse with a Scalable Observation Model in Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Deep Neural Networks (DNNs) are the backbone of today's artificialintelligence due to their ability to make accurate predictions when beingtrained on huge datasets. With advancing technologies, such as the Internet ofThings, interpreting large quantities of data generated by sensors is becomingan increasingly important task. However, in many applications not only thepredictive performance but also the energy consumption of deep learning modelsis of major interest. This paper investigates the efficient deployment of deeplearning models on resource-constrained microcontroller architectures vianetwork compression. We present a methodology for the systematic exploration ofdifferent DNN pruning, quantization, and deployment strategies, targetingdifferent ARM Cortex-M based low-power systems. The exploration allows toanalyze trade-offs between key metrics such as accuracy, memory consumption,execution time, and power consumption. We discuss experimental results on threedifferent DNN architectures and show that we can compress them to below 10% oftheir original parameter count before their predictive quality decreases. Thisalso allows us to deploy and evaluate them on Cortex-M based microcontrollers.", "output": "Energy-efficient Deployment of Deep Learning Applications on Cortex-M based Microcontrollers using Deep Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of Out-of-Distribution (OOD) detection, that is,detecting whether a learning algorithm's output can be trusted at inferencetime. While a number of tests for OOD detection have been proposed in priorwork, a formal framework for studying this problem is lacking. We propose adefinition for the notion of OOD that includes both the input distribution andthe learning algorithm, which provides insights for the construction ofpowerful tests for OOD detection. We propose a multiple hypothesis testinginspired procedure to systematically combine any number of different statisticsfrom the learning algorithm using conformal p-values. We further provide strongguarantees on the probability of incorrectly classifying an in-distributionsample as OOD. In our experiments, we find that threshold-based tests proposedin prior work perform well in specific settings, but not uniformly well acrossdifferent types of OOD instances. In contrast, our proposed method thatcombines multiple statistics performs uniformly well across different datasetsand neural networks.", "output": "Multiple Testing Framework for Out-of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Drawing from memory the face of a friend you have not seen in years is adifficult task. However, if you happen to cross paths, you would easilyrecognize each other. The biological memory is equipped with an impressivecompression algorithm that can store the essential, and then infer the detailsto match perception. The Willshaw Memory is a simple abstract model forcortical computations which implements mechanisms of biological memories. Usingour recently proposed sparse coding prescription for visual patterns, thismodel can store and retrieve an impressive amount of real-world data in afault-tolerant manner. In this paper, we extend the capabilities of the basicAssociative Memory Model by using a Multiple-Modality framework. In thissetting, the memory stores several modalities (e.g., visual, or textual) ofeach pattern simultaneously. After training, the memory can be used to infermissing modalities when just a subset is perceived. Using a simpleencoder-memory-decoder architecture, and a newly proposed iterative retrievalalgorithm for the Willshaw Model, we perform experiments on the MNIST dataset.By storing both the images and labels as modalities, a single Memory can beused not only to retrieve and complete patterns but also to classify andgenerate new ones. We further discuss how this model could be used for otherlearning tasks, thus serving as a biologically-inspired framework for learning.", "output": "Classification and Generation of real-world data with an Associative Memory Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continuous monitoring of trained ML models to determine when theirpredictions should and should not be trusted is essential for their safedeployment. Such a framework ought to be high-performing, explainable, post-hocand actionable. We propose TRUST-LAPSE, a \"mistrust\" scoring framework forcontinuous model monitoring. We assess the trustworthiness of each inputsample's model prediction using a sequence of latent-space embeddings.Specifically, (a) our latent-space mistrust score estimates mistrust usingdistance metrics (Mahalanobis distance) and similarity metrics (cosinesimilarity) in the latent-space and (b) our sequential mistrust scoredetermines deviations in correlations over the sequence of past inputrepresentations in a non-parametric, sliding-window based algorithm foractionable continuous monitoring. We evaluate TRUST-LAPSE via two downstreamtasks: (1) distributionally shifted input detection, and (2) data driftdetection. We evaluate across diverse domains - audio and vision using publicdatasets and further benchmark our approach on challenging, real-worldelectroencephalograms (EEG) datasets for seizure detection. Our latent-spacemistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision),73.9 (audio), and 77.1 (clinical EEGs), outperforming baselines by over 10points. We expose critical failures in popular baselines that remaininsensitive to input semantic content, rendering them unfit for real-worldmodel monitoring. We show that our sequential mistrust scores achieve highdrift detection rates; over 90% of the streams show &lt; 20% error for alldomains. Through extensive qualitative and quantitative evaluations, we showthat our mistrust scores are more robust and provide explainability for easyadoption into practice.", "output": "TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has emerged as a promising alternative tocreate supervisory signals to real-world problems, avoiding the extensive costof manual labeling. SSL is particularly attractive for unsupervised tasks suchas anomaly detection (AD), where labeled anomalies are rare or oftennonexistent. A large catalog of augmentation functions has been used forSSL-based AD (SSAD) on image data, and recent works have reported that the typeof augmentation has a significant impact on accuracy. Motivated by those, thiswork sets out to put image-based SSAD under a larger lens and investigate therole of data augmentation in SSAD. Through extensive experiments on 3 differentdetector models and across 420 AD tasks, we provide comprehensive numerical andvisual evidences that the alignment between data augmentation andanomaly-generating mechanism is the key to the success of SSAD, and in the lackthereof, SSL may even impair accuracy. To the best of our knowledge, this isthe first meta-analysis on the role of data augmentation in SSAD.", "output": "Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The notion of local intrinsic dimensionality (LID) is an importantadvancement in data dimensionality analysis, with applications in data mining,machine learning and similarity search problems. Existing distance-based LIDestimators were designed for tabular datasets encompassing data pointsrepresented as vectors in a Euclidean space. After discussing their limitationsfor graph-structured data considering graph embeddings and graph distances, wepropose NC-LID, a novel LID-related measure for quantifying the discriminatorypower of the shortest-path distance with respect to natural communities ofnodes as their intrinsic localities. It is shown how this measure can be usedto design LID-aware graph embedding algorithms by formulating two LID-elasticvariants of node2vec with personalized hyperparameters that are adjustedaccording to NC-LID values. Our empirical analysis of NC-LID on a large numberof real-world graphs shows that this measure is able to point to nodes withhigh link reconstruction errors in node2vec embeddings better than nodecentrality metrics. The experimental evaluation also shows that the proposedLID-elastic node2vec extensions improve node2vec by better preserving graphstructure in generated embeddings.", "output": "Local Intrinsic Dimensionality Measures for Graphs, with Applications to Graph Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the multi-user detection (MUD) problem in uplink grant-freenon-orthogonal multiple access (NOMA), where the access point has to identifythe total number and correct identity of the active Internet of Things (IoT)devices and decode their transmitted data. We assume that IoT devices usecomplex spreading sequences and transmit information in a random-access mannerfollowing the burst-sparsity model, where some IoT devices transmit their datain multiple adjacent time slots with a high probability, while others transmitonly once during a frame. Exploiting the temporal correlation, we propose anattention-based bidirectional long short-term memory (BiLSTM) network to solvethe MUD problem. The BiLSTM network creates a pattern of the device activationhistory using forward and reverse pass LSTMs, whereas the attention mechanismprovides essential context to the device activation points. By doing so, ahierarchical pathway is followed for detecting active devices in a grant-freescenario. Then, by utilising the complex spreading sequences, blind datadetection for the estimated active devices is performed. The proposed frameworkdoes not require prior knowledge of device sparsity levels and channels forperforming MUD. The results show that the proposed network achieves betterperformance compared to existing benchmark schemes.", "output": "Joint User and Data Detection in Grant-Free NOMA with Attention-based BiLSTM Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the adaption of soft actor-critic (SAC) from continuous action spaceto discrete action space. We revisit vanilla SAC and provide an in-depthunderstanding of its Q value underestimation and performance instability issueswhen applied to discrete settings. We thereby propose entropy-penalty anddouble average Q-learning with Q-clip to address these issues. Extensiveexperiments on typical benchmarks with discrete action space, including Atarigames and a large-scale MOBA game, show the efficacy of our proposed method.Our code is at:", "output": "Revisiting Discrete Soft Actor-Critic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a goodness-of-fit measure for probability densities modelingobservations with varying dimensionality, such as text documents of differinglengths or variable-length sequences. The proposed measure is an instance ofthe kernel Stein discrepancy (KSD), which has been used to constructgoodness-of-fit tests for unnormalized densities. The KSD is defined by itsStein operator: current operators used in testing apply to fixed-dimensionalspaces. As our main contribution, we extend the KSD to the variable-dimensionsetting by identifying appropriate Stein operators, and propose a novel KSDgoodness-of-fit test. As with the previous variants, the proposed KSD does notrequire the density to be normalized, allowing the evaluation of a large classof models. Our test is shown to perform well in practice on discrete sequentialdata benchmarks.", "output": "A kernel Stein test of goodness of fit for sequential models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graph (KG) reasoning is an important problem for knowledge graphs.In this paper, we propose a novel and principled framework called textbf{RulE}(stands for {Rul}e {E}mbedding) to effectively leverage logical rules toenhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulElearns rule embeddings from existing triplets and first-order {rules} byjointly representing textbf{entities}, textbf{relations} and textbf{logicalrules} in a unified embedding space. Based on the learned rule embeddings, aconfidence score can be calculated for each rule, reflecting its consistencywith the observed triplets. This allows us to perform logical rule inference ina soft way, thus alleviating the brittleness of logic. On the other hand, RulEinjects prior logical rule information into the embedding space, enriching andregularizing the entity/relation embeddings. This makes KGE alone performbetter too. RulE is conceptually simple and empirically effective. We conductextensive experiments to verify each component of RulE. Results on multiplebenchmarks reveal that our model outperforms the majority of existingembedding-based and rule-based approaches.", "output": "RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a novel Framelet Graph approach based on p-LaplacianGNN. The proposed two models, named p-Laplacian undecimated framelet graphconvolution (pL-UFG) and generalized p-Laplacian undecimated framelet graphconvolution (pL-fUFG) inherit the nature of p-Laplacian with the expressivepower of multi-resolution decomposition of graph signals. The empirical studyhighlights the excellent performance of the pL-UFG and pL-fUFG in differentgraph learning tasks including node classification and signal denoising.", "output": "Generalized Laplacian Regularized Framelet Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We attack the state-of-the-art Go-playing AI system KataGo by trainingadversarial policies against it, achieving a &gt;97% win rate against KataGorunning at superhuman settings. Our adversaries do not win by playing Go well.Instead, they trick KataGo into making serious blunders. Our attack transferszero-shot to other superhuman Go-playing AIs, and is comprehensible to theextent that human experts can implement it without algorithmic assistance toconsistently beat superhuman AIs. The core vulnerability uncovered by ourattack persists even in KataGo agents adversarially trained to defend againstour attack. Our results demonstrate that even superhuman AI systems may harborsurprising failure modes. Example games are available ", "output": "Adversarial Policies Beat Superhuman Go AIs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Simulations of biophysical systems are fundamental for studying physiologicalmechanisms and developing human machine interfaces. Whilst advanced numericalmethods, such as finite element models, can excel in this task, they areextremely computationally expensive to use when generating a large number ofsimulations or simulating dynamic events with continuously changing structuralparameters. We propose an architecture that uses a conditional generative modelto interpolate between the numerical model states, dramatically lowering themodeling time while maintaining a high generation accuracy. As a demonstrationof this concept, we present BioMime, a hybrid-structured generative model thatenables an accurate, ultra-fast, and arbitrarily high temporal-resolutionsimulation of a specific biophysical system during dynamic changes. Thismethodology has wide applications in physiological and clinical research aswell as in supporting data augmentation strategies for signal analysis,representing a computationally efficient and highly accurate model forbiophysical simulations.", "output": "Human Biophysics as Network Weights: Conditional Generative Models for Dynamic Simulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning long-horizon tasks such as navigation has presented difficultchallenges for successfully applying reinforcement learning to robotics. Fromanother perspective, under known environments, sampling-based planning canrobustly find collision-free paths in environments without learning. In thiswork, we propose Control Transformer that models return-conditioned sequencesfrom low-level policies guided by a sampling-based Probabilistic Roadmap (PRM)planner. We demonstrate that our framework can solve long-horizon navigationtasks using only local information. We evaluate our approach onpartially-observed maze navigation with MuJoCo robots, including Ant, Point,and Humanoid. We show that Control Transformer can successfully navigatethrough mazes and transfer to unknown environments. Additionally, we apply ourmethod to a differential drive robot (Turtlebot3) and show zero-shot sim2realtransfer under noisy observations.", "output": "Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "World models power some of the most efficient reinforcement learningalgorithms. In this work, we showcase that they can be harnessed for continuallearning - a situation when the agent faces changing environments. World modelstypically employ a replay buffer for training, which can be naturally extendedto continual learning. We systematically study how different selectiveexperience replay methods affect performance, forgetting, and transfer. We alsoprovide recommendations regarding various modeling options for using worldmodels. The best set of choices is called Continual-Dreamer, it istask-agnostic and utilizes the world model for continual exploration.Continual-Dreamer is sample efficient and outperforms state-of-the-arttask-agnostic continual reinforcement learning methods on Minigrid and Minihackbenchmarks.", "output": "The Effectiveness of World Models for Continual Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer has been considered the dominating neural architecture in NLP andCV, mostly under supervised settings. Recently, a similar surge of usingTransformers has appeared in the domain of reinforcement learning (RL), but itis faced with unique design choices and challenges brought by the nature of RL.However, the evolution of Transformers in RL has not yet been well unraveled.In this paper, we seek to systematically review motivations and progress onusing Transformers in RL, provide a taxonomy on existing works, discuss eachsub-field, and summarize future prospects.", "output": "A Survey on Transformers in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the vast data-collecting capabilities of current and future high-energycollider experiments, there is an increasing demand for computationallyefficient simulations. Generative machine learning models enable fast eventgeneration, yet so far these approaches are largely constrained to fixed datastructures and rigid detector geometries. In this paper, we introduce EPiC-GAN- equivariant point cloud generative adversarial network - which can producepoint clouds of variable multiplicity. This flexible framework is based on deepsets and is well suited for simulating sprays of particles called jets. Thegenerator and discriminator utilize multiple EPiC layers with an interpretableglobal latent vector. Crucially, the EPiC layers do not rely on pairwiseinformation sharing between particles, which leads to a significant speed-upover graph- and transformer-based approaches with more complex relationdiagrams. We demonstrate that EPiC-GAN scales well to large particlemultiplicities and achieves high generation fidelity on benchmark jetgeneration tasks.", "output": "EPiC-GAN: Equivariant Point Cloud Generation for Particle Jets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bayesian model comparison (BMC) offers a principled approach for assessingthe relative merits of competing computational models and propagatinguncertainty into model selection decisions. However, BMC is often intractablefor the popular class of hierarchical models due to their high-dimensionalnested parameter structure. To address this intractability, we propose a deeplearning method for performing BMC on any set of hierarchical models which canbe instantiated as probabilistic programs. Since our method enables amortizedinference, it allows efficient re-estimation of posterior model probabilitiesand fast performance validation prior to any real-data application. In a seriesof extensive validation studies, we benchmark the performance of our methodagainst the state-of-the-art bridge sampling method and demonstrate excellentamortized inference across all BMC settings. We then showcase our method bycomparing four hierarchical evidence accumulation models that have previouslybeen deemed intractable for BMC due to partly implicit likelihoods. In thisapplication, we corroborate evidence for the recently proposed L'evy flightmodel of decision-making and show how transfer learning can be leveraged toenhance training efficiency. We provide reproducible code for all analyses andan open-source implementation of our method.", "output": "A Deep Learning Method for Comparing Bayesian Hierarchical Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative adversarial networks (GANs) learn a target probabilitydistribution by optimizing a generator and a discriminator with minimaxobjectives. This paper addresses the question of whether such optimizationactually provides the generator with gradients that make its distribution closeto the target distribution. We derive metrizable conditions, sufficientconditions for the discriminator to serve as the distance between thedistributions by connecting the GAN formulation with the concept of slicedoptimal transport. Furthermore, by leveraging these theoretical results, wepropose a novel GAN training scheme, called slicing adversarial network (SAN).With only simple modifications, a broad class of existing GANs can be convertedto SANs. Experiments on synthetic and image datasets support our theoreticalresults and the SAN's effectiveness as compared to usual GANs. Furthermore, wealso apply SAN to StyleGAN-XL, which leads to state-of-the-art FID scoreamongst GANs for class conditional generation on ImageNet 256$times$256.", "output": "SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many industrial applications, obtaining labeled observations is notstraightforward as it often requires the intervention of human experts or theuse of expensive testing equipment. In these circumstances, active learning canbe highly beneficial in suggesting the most informative data points to be usedwhen fitting a model. Reducing the number of observations needed for modeldevelopment alleviates both the computational burden required for training andthe operational expenses related to labeling. Online active learning, inparticular, is useful in high-volume production processes where the decisionabout the acquisition of the label for a data point needs to be taken within anextremely short time frame. However, despite the recent efforts to developonline active learning strategies, the behavior of these methods in thepresence of outliers has not been thoroughly examined. In this work, weinvestigate the performance of online active linear regression in contaminateddata streams. Our study shows that the currently available query strategies areprone to sample outliers, whose inclusion in the training set eventuallydegrades the predictive performance of the models. To address this issue, wepropose a solution that bounds the search area of a conditional D-optimalalgorithm and uses a robust estimator. Our approach strikes a balance betweenexploring unseen regions of the input space and protecting against outliers.Through numerical simulations, we show that the proposed method is effective inimproving the performance of online active learning in the presence ofoutliers, thus expanding the potential applications of this powerful tool.", "output": "Robust online active learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continuous normalizing flows (CNFs) are an attractive generative modelingtechnique, but they have been held back by limitations in theirsimulation-based maximum likelihood training. We introduce the generalizedconditional flow matching (CFM) technique, a family of simulation-free trainingobjectives for CNFs. CFM features a stable regression objective like that usedto train the stochastic flow in diffusion models but enjoys the efficientinference of deterministic flow models. In contrast to both diffusion modelsand prior CNF training algorithms, CFM does not require the source distributionto be Gaussian or require evaluation of its density. A variant of our objectiveis optimal transport CFM (OT-CFM), which creates simpler flows that are morestable to train and lead to faster inference, as evaluated in our experiments.Furthermore, OT-CFM is the first method to compute dynamic OT in asimulation-free way. Training CNFs with CFM improves results on a variety ofconditional and unconditional generation tasks, such as inferring single celldynamics, unsupervised image translation, and Schr\"odinger bridge inference.", "output": "Improving and generalizing flow-based generative models with minibatch optimal transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The modeling of time-varying graph signals as stationary time-vertexstochastic processes permits the inference of missing signal values byefficiently employing the correlation patterns of the process across differentgraph nodes and time instants. In this study, we propose an algorithm forcomputing graph autoregressive moving average (graph ARMA) processes based onlearning the joint time-vertex power spectral density of the process from itsincomplete realizations for the task of signal interpolation. Our solutionrelies on first roughly estimating the joint spectrum of the process frompartially observed realizations and then refining this estimate by projectingit onto the spectrum manifold of the graph ARMA process through convexrelaxations. The initially missing signal values are then estimated based onthe learnt model. Experimental results show that the proposed approach achieveshigh accuracy in time-vertex signal estimation problems.", "output": "Learning Graph ARMA Processes from Time-Vertex Spectra."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate and efficient classification of different types of cancer iscritical for early detection and effective treatment. In this paper, we presentthe results of our experiments using the EfficientNet algorithm forclassification of brain tumor, breast cancer mammography, chest cancer, andskin cancer. We used publicly available datasets and preprocessed the images toensure consistency and comparability. Our experiments show that theEfficientNet algorithm achieved high accuracy, precision, recall, and F1 scoreson each of the cancer datasets, outperforming other state-of-the-art algorithmsin the literature. We also discuss the strengths and weaknesses of theEfficientNet algorithm and its potential applications in clinical practice. Ourresults suggest that the EfficientNet algorithm is well-suited forclassification of different types of cancer and can be used to improve theaccuracy and efficiency of cancer diagnosis.", "output": "EfficientNet Algorithm for Classification of Different Types of Cancer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the convergence of message passing graph neural networks on randomgraph models to their continuous counterpart as the number of nodes tends toinfinity. Until now, this convergence was only known for architectures withaggregation functions in the form of normalized means, or, equivalently, of anapplication of classical operators like the adjacency matrix or the graphLaplacian. We extend such results to a large class of aggregation functions,that encompasses all classically used message passing graph neural networks,such as attention-based message passing, max convolutional message passing or(degree-normalized) convolutional message passing. Under mild assumptions, wegive non-asymptotic bounds with high probability to quantify this convergence.Our main result is based on the McDiarmid inequality. Interestingly, thisresult does not apply to the case where the aggregation is a coordinate-wisemaximum. We treat this case separately and obtain a different convergence rate.", "output": "Convergence of Message Passing Graph Neural Networks with Generic Aggregation On Large Random Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Synthetic data has been hailed as the silver bullet for privacy preservingdata analysis. If a record is not real, then how could it violate a person'sprivacy? In addition, deep-learning based generative models are employedsuccessfully to approximate complex high-dimensional distributions from dataand draw realistic samples from this learned distribution. It is oftenoverlooked though that generative models are prone to memorising many detailsof individual training records and often generate synthetic data that tooclosely resembles the underlying sensitive training data, hence violatingstrong privacy regulations as, e.g., encountered in health care. Differentialprivacy is the well-known state-of-the-art framework for guaranteeingprotection of sensitive individuals' data, allowing aggregate statistics andeven machine learning models to be released publicly without compromisingprivacy. The training mechanisms however often add too much noise during thetraining process, and thus severely compromise the utility of these privatemodels. Even worse, the tight privacy budgets do not allow for many trainingepochs so that model quality cannot be properly controlled in practice. In thispaper we explore an alternative approach for privately generating data thatmakes direct use of the inherent stochasticity in generative models, e.g.,variational autoencoders. The main idea is to appropriately constrain thecontinuity modulus of the deep models instead of adding another noise mechanismon top. For this approach, we derive mathematically rigorous privacy guaranteesand illustrate its effectiveness with practical experiments.", "output": "Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The coupling of deep reinforcement learning to numerical flow controlproblems has recently received a considerable attention, leading togroundbreaking results and opening new perspectives for the domain. Due to theusually high computational cost of fluid dynamics solvers, the use of parallelenvironments during the learning process represents an essential ingredient toattain efficient control in a reasonable time. Yet, most of the deepreinforcement learning literature for flow control relies on on-policyalgorithms, for which the massively parallel transition collection may breaktheoretical assumptions and lead to suboptimal control models. To overcome thisissue, we propose a parallelism pattern relying on partial-trajectory buffersterminated by a return bootstrapping step, allowing a flexible use of parallelenvironments while preserving the on-policiness of the updates. This approachis illustrated on a CPU-intensive continuous flow control problem from theliterature.", "output": "Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have made remarkable advancements in the fieldof natural language processing. However, their increasing size poses challengesin terms of computational cost. On the other hand, Small Language Models (SLMs)are known for their efficiency, but they often struggle with limited capacityand training data, especially in specific domains. In this paper, we introducea novel method aimed at improving SLMs in the medical domain using LLM-basedgenerative data augmentation. The objective of our approach is to develop moreefficient and capable models that are specifically tailored for specializedapplications. Through experiments conducted on the PubMedQA dataset, wedemonstrate the effectiveness of LLMs in refining and diversifying existingquestion-answer pairs. This refinement process leads to improved performance ina significantly smaller model after fine-tuning. Notably, our best SLM, withunder 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQAdataset. Our code and generated data are publicly available to facilitatefurther explorations.", "output": "Improving Small Language Models on PubMedQA via Generative Data Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a comprehensive theoretical analysis of the graphp-Laplacian regularized framelet network (pL-UFG) to establish a solidunderstanding of its properties. We conduct a convergence analysis on pL-UFG,addressing the gap in the understanding of its asymptotic behaviors. Further byinvestigating the generalized Dirichlet energy of pL-UFG, we demonstrate thatthe Dirichlet energy remains non-zero throughout convergence, ensuring theavoidance of over-smoothing issues. Additionally, we elucidate the energydynamic perspective, highlighting the synergistic relationship between theimplicit layer in pL-UFG and graph framelets. This synergy enhances the model'sadaptability to both homophilic and heterophilic data. Notably, we reveal thatpL-UFG can be interpreted as a generalized non-linear diffusion process,thereby bridging the gap between pL-UFG and differential equations on thegraph. Importantly, these multifaceted analyses lead to unified conclusionsthat offer novel insights for understanding and implementing pL-UFG, as well asother graph neural network (GNN) models. Finally, based on our dynamicanalysis, we propose two novel pL-UFG models with manually controlled energydynamics. We demonstrate empirically and theoretically that our proposed modelsnot only inherit the advantages of pL-UFG but also significantly reducecomputational costs for training on large-scale graph datasets.", "output": "Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Personalized federated learning (PFL) jointly trains a variety of localmodels through balancing between knowledge sharing across clients and modelpersonalization per client. This paper addresses PFL via explicit disentanglinglatent representations into two parts to capture the shared knowledge andclient-specific personalization, which leads to more reliable and effectivePFL. The disentanglement is achieved by a novel Federated Dual VariationalAutoencoder (FedDVA), which employs two encoders to infer the two types ofrepresentations. FedDVA can produce a better understanding of the trade-offbetween global knowledge sharing and local personalization in PFL. Moreover, itcan be integrated with existing FL methods and turn them into personalizedmodels for heterogeneous downstream tasks. Extensive experiments validate theadvantages caused by disentanglement and show that models trained withdisentangled representations substantially outperform those vanilla methods.", "output": "Personalization Disentanglement for Federated Learning: An explainable perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-language models (LLMs) such as GPT-4 caught the interest of manyscientists. Recent studies suggested that these models could be useful inchemistry and materials science. To explore these possibilities, we organized ahackathon.This article chronicles the projects built as part of this hackathon.Participants employed LLMs for various applications, including predictingproperties of molecules and materials, designing novel interfaces for tools,extracting knowledge from unstructured data, and developing new educationalapplications.The diverse topics and the fact that working prototypes could be generated inless than two days highlight that LLMs will profoundly impact the future of ourfields. The rich collection of ideas and projects also indicates that theapplications of LLMs are not limited to materials science and chemistry butoffer potential benefits to a wide range of scientific disciplines.", "output": "14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study property prediction for crystal materials. A crystal structureconsists of a minimal unit cell that is repeated infinitely in 3D space. How toaccurately represent such repetitive structures in machine learning modelsremains unresolved. Current methods construct graphs by establishing edges onlybetween nearby nodes, thereby failing to faithfully capture infinite repeatingpatterns and distant interatomic interactions. In this work, we propose severalinnovations to overcome these limitations. First, we propose to modelphysics-principled interatomic potentials directly instead of only usingdistances as in many existing methods. These potentials include the Coulombpotential, London dispersion potential, and Pauli repulsion potential. Second,we model the complete set of potentials among all atoms, instead of onlybetween nearby atoms as in existing methods. This is enabled by ourapproximations of infinite potential summations with provable error bounds. Wefurther develop efficient algorithms to compute the approximations. Finally, wepropose to incorporate our computations of complete interatomic potentials intomessage passing neural networks for representation learning. We performexperiments on the JARVIS and Materials Project benchmarks for evaluation.Results show that the use of interatomic potentials and complete interatomicpotentials leads to consistent performance improvements with reasonablecomputational costs. Our code is publicly available as part of the AIRS library(", "output": "Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The perception module of self-driving vehicles relies on a multi-sensorsystem to understand its environment. Recent advancements in deep learning haveled to the rapid development of approaches that integrate multi-sensorymeasurements to enhance perception capabilities. This paper surveys the latestdeep learning integration techniques applied to the perception module inautonomous driving systems, categorizing integration approaches based on \"what,how, and when to integrate\". A new taxonomy of integration is proposed, basedon three dimensions: multi-view, multi-modality, and multi-frame. Theintegration operations and their pros and cons are summarized, providing newinsights into the properties of an \"ideal\" data integration approach that canalleviate the limitations of existing methods. After reviewing hundreds ofrelevant papers, this survey concludes with a discussion of the key features ofan optimal data integration approach.", "output": "A survey on deep learning approaches for data integration in autonomous driving system."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trust evaluation assesses trust relationships between entities andfacilitates decision-making. Machine Learning (ML) shows great potential fortrust evaluation owing to its learning capabilities. In recent years, GraphNeural Networks (GNNs), as a new ML paradigm, have demonstrated superiority indealing with graph data. This has motivated researchers to explore their use intrust evaluation, as trust relationships among entities can be modeled as agraph. However, current trust evaluation methods that employ GNNs fail to fullysatisfy the dynamic nature of trust, overlook the adverse effects of attacks ontrust evaluation, and cannot provide convincing explanations on evaluationresults. To address these problems, we propose TrustGuard, a GNN-based accuratetrust evaluation model that supports trust dynamicity, is robust againsttypical attacks, and provides explanations through visualization. Specifically,TrustGuard is designed with a layered architecture that contains a snapshotinput layer, a spatial aggregation layer, a temporal aggregation layer, and aprediction layer. Among them, the spatial aggregation layer adopts a defensemechanism to robustly aggregate local trust, and the temporal aggregation layerapplies an attention mechanism for effective learning of temporal patterns.Extensive experiments on two real-world datasets show that TrustGuardoutperforms state-of-the-art GNN-based trust evaluation models with respect totrust prediction across single-timeslot and multi-timeslot, even in thepresence of attacks. In addition, TrustGuard can explain its evaluation resultsby visualizing both spatial and temporal views.", "output": "TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lately, differential privacy (DP) has been introduced in cooperativemultiagent reinforcement learning (CMARL) to safeguard the agents' privacyagainst adversarial inference during knowledge sharing. Nevertheless, we arguethat the noise introduced by DP mechanisms may inadvertently give rise to anovel poisoning threat, specifically in the context of private knowledgesharing during CMARL, which remains unexplored in the literature. To addressthis shortcoming, we present an adaptive, privacy-exploiting, andevasion-resilient localized poisoning attack (PeLPA) that capitalizes on theinherent DP-noise to circumvent anomaly detection systems and hinder theoptimal convergence of the CMARL model. We rigorously evaluate our proposedPeLPA attack in diverse environments, encompassing both non-adversarial andmultiple-adversarial contexts. Our findings reveal that, in a medium-scaleenvironment, the PeLPA attack with attacker ratios of 20% and 40% can lead toan increase in average steps to goal by 50.69% and 64.41%, respectively.Furthermore, under similar conditions, PeLPA can result in a 1.4x and 1.6xcomputational time increase in optimal reward attainment and a 1.18x and 1.38xslower convergence for attacker ratios of 20% and 40%, respectively.", "output": "Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Under the framework of network-based neurodegeneration, brain functionalconnectome (FC)-based Graph Neural Networks (GNN) have emerged as a valuabletool for the diagnosis and prognosis of neurodegenerative diseases such asAlzheimer's disease (AD). However, these models are tailored for brain FC at asingle time point instead of characterizing FC trajectory. Discerning how FCevolves with disease progression, particularly at the predementia stages suchas cognitively normal individuals with amyloid deposition or individuals withmild cognitive impairment (MCI), is crucial for delineating disease spreadingpatterns and developing effective strategies to slow down or even halt diseaseadvancement. In this work, we proposed the first interpretable framework forbrain FC trajectory embedding with application to neurodegenerative diseasediagnosis and prognosis, namely Brain Tokenized Graph Transformer (BrainTokenGT). It consists of two modules: 1) Graph Invariant and Variant Embedding(GIVE) for generation of node and spatio-temporal edge embeddings, which weretokenized for downstream processing; 2) Brain Informed Graph TransformerReadout (BIGTR) which augments previous tokens with trainable type identifiersand non-trainable node identifiers and feeds them into a standard transformerencoder to readout. We conducted extensive experiments on two publiclongitudinal fMRI datasets of the AD continuum for three tasks, includingdifferentiating MCI from controls, predicting dementia conversion in MCI, andclassification of amyloid positive or negative cognitively normal individuals.Based on brain FC trajectory, the proposed Brain TokenGT approach outperformedall the other benchmark models and at the same time provided excellentinterpretability. The code is available at", "output": "Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The dynamic nature of driving environments and the presence of diverse roadusers pose significant challenges for decision-making in autonomous driving.Deep reinforcement learning (DRL) has emerged as a popular approach to tacklethis problem. However, the application of existing DRL solutions is mainlyconfined to simulated environments due to safety concerns, impeding theirdeployment in real-world. To overcome this limitation, this paper introduces anovel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics(DRLSL) that combines the strengths of DRL (learning from experience) andsymbolic first-order logics (knowledge-driven reasoning) to enable safelearning in real-time interactions of autonomous driving within realenvironments. This innovative approach provides a means to learn autonomousdriving policies by actively engaging with the physical environment whileensuring safety. We have implemented the DRLSL framework in autonomous drivingusing the highD dataset and demonstrated that our method successfully avoidsunsafe actions during both the training and testing phases. Furthermore, ourresults indicate that DRLSL achieves faster convergence during training andexhibits better generalizability to new driving scenarios compared totraditional DRL methods.", "output": "Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a class of stochastic smooth convex optimization problems underrather general assumptions on the noise in the stochastic gradient observation.As opposed to the classical problem setting in which the variance of noise isassumed to be uniformly bounded, herein we assume that the variance ofstochastic gradients is related to the \"sub-optimality\" of the approximatesolutions delivered by the algorithm. Such problems naturally arise in avariety of applications, in particular, in the well-known generalized linearregression problem in statistics. However, to the best of our knowledge, noneof the existing stochastic approximation algorithms for solving this class ofproblems attain optimality in terms of the dependence on accuracy, problemparameters, and mini-batch size.We discuss two non-Euclidean accelerated stochastic approximationroutines--stochastic accelerated gradient descent (SAGD) and stochasticgradient extrapolation (SGE)--which carry a particular duality relationship. Weshow that both SAGD and SGE, under appropriate conditions, achieve the optimalconvergence rate, attaining the optimal iteration and sample complexitiessimultaneously. However, corresponding assumptions for the SGE algorithm aremore general; they allow, for instance, for efficient application of the SGE tostatistical estimation problems under heavy tail noises and discontinuous scorefunctions. We also discuss the application of the SGE to problems satisfyingquadratic growth conditions, and show how it can be used to recover sparsesolutions. Finally, we report on some simulation experiments to illustratenumerical performance of our proposed algorithms in high-dimensional settings.", "output": "Accelerated stochastic approximation with state-dependent noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncertainty sampling is a prevalent active learning algorithm that queriessequentially the annotations of data samples which the current prediction modelis uncertain about. However, the usage of uncertainty sampling has been largelyheuristic: (i) There is no consensus on the proper definition of \"uncertainty\"for a specific task under a specific loss; (ii) There is no theoreticalguarantee that prescribes a standard protocol to implement the algorithm, forexample, how to handle the sequentially arrived annotated data under theframework of optimization algorithms such as stochastic gradient descent. Inthis work, we systematically examine uncertainty sampling algorithms under bothstream-based and pool-based active learning. We propose a notion of equivalentloss which depends on the used uncertainty measure and the original lossfunction and establish that an uncertainty sampling algorithm essentiallyoptimizes against such an equivalent loss. The perspective verifies theproperness of existing uncertainty measures from two aspects: surrogateproperty and loss convexity. Furthermore, we propose a new notion for designinguncertainty measures called textit{loss as uncertainty}. The idea is to usethe conditional expected loss given the features as the uncertainty measure.Such an uncertainty measure has nice analytical properties and generality tocover both classification and regression problems, which enable us to providethe first generalization bound for uncertainty sampling algorithms under bothstream-based and pool-based settings, in the full generality of the underlyingmodel and problem. Lastly, we establish connections between certain variants ofthe uncertainty sampling algorithms with risk-sensitive objectives anddistributional robustness, which can partly explain the advantage ofuncertainty sampling algorithms when the sample size is small.", "output": "Understanding Uncertainty Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The real-time crash likelihood prediction model is an essential component ofthe proactive traffic safety management system. Over the years, numerousstudies have attempted to construct a crash likelihood prediction model inorder to enhance traffic safety, but mostly on freeways. In the majority of theexisting studies, researchers have primarily employed a deep learning-basedframework to identify crash potential. Lately, Transformer has emerged as apotential deep neural network that fundamentally operates throughattention-based mechanisms. Transformer has several functional benefits overextant deep learning models such as Long Short-Term Memory (LSTM), ConvolutionNeural Network (CNN), etc. Firstly, Transformer can readily handle long-termdependencies in a data sequence. Secondly, Transformers can parallelly processall elements in a data sequence during training. Finally, a Transformer doesnot have the vanishing gradient issue. Realizing the immense possibility ofTransformers, this paper proposes inTersection-Transformer (inTformer), atime-embedded attention-based Transformer model that can effectively predictintersection crash likelihood in real-time. The proposed model was evaluatedusing connected vehicle data extracted from INRIX and Center for AdvancedTransportation Technology (CATT) Lab's Signal Analytics Platform. The data wasparallelly formatted and stacked at different timesteps to develop nineinTformer models. The best inTformer model achieved a sensitivity of 73%. Thismodel was also compared to earlier studies on crash likelihood prediction atintersections and with several established deep learning models trained on thesame connected vehicle dataset. In every scenario, this inTformer outperformedthe benchmark models confirming the viability of the proposed inTformerarchitecture.", "output": "inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supply chain operations traditionally involve a variety of complex decisionmaking problems. Over the last few decades, supply chains greatly benefitedfrom advances in computation, which allowed the transition from manualprocessing to automation and cost-effective optimization. Nonetheless, businessoperators still need to spend substantial efforts in explaining andinterpreting the optimization outcomes to stakeholders. Motivated by the recentadvances in Large Language Models (LLMs), we study how this disruptivetechnology can help bridge the gap between supply chain automation and humancomprehension and trust thereof. We design OptiGuide -- a framework thataccepts as input queries in plain text, and outputs insights about theunderlying optimization outcomes. Our framework does not forgo thestate-of-the-art combinatorial optimization technology, but rather leverages itto quantitatively answer what-if scenarios (e.g., how would the cost change ifwe used supplier B instead of supplier A for a given demand?). Importantly, ourdesign does not require sending proprietary data over to LLMs, which can be aprivacy concern in some circumstances. We demonstrate the effectiveness of ourframework on a real server placement scenario within Microsoft's cloud supplychain. Along the way, we develop a general evaluation benchmark, which can beused to evaluate the accuracy of the LLM output in other scenarios.", "output": "Large Language Models for Supply Chain Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce PIGEON, a multi-task end-to-end system for planet-scale imagegeolocalization that achieves state-of-the-art performance on both externalbenchmarks and in human evaluation. Our work incorporates semantic geocellcreation with label smoothing, conducts pretraining of a vision transformer onimages with geographic information, and refines location predictions withProtoNets across a candidate set of geocells. The contributions of PIGEON arethree-fold: first, we design a semantic geocells creation and splittingalgorithm based on open-source data which can be adapted to any geospatialdataset. Second, we show the effectiveness of intra-geocell refinement and theapplicability of unsupervised clustering and ProtNets to the task. Finally, wemake our pre-trained CLIP transformer model, StreetCLIP, publicly available foruse in adjacent domains with applications to fighting climate change and urbanand rural scene understanding.", "output": "PIGEON: Predicting Image Geolocations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned toempower various areas as a bridge between physical objects and the digitalworld. Through virtualization and simulation techniques, multiple functions canbe achieved by leveraging computing resources. In this process, Mobile CloudComputing (MCC) and Mobile Edge Computing (MEC) have become two of the keyfactors to achieve real-time feedback. However, current works only considerededge servers or cloud servers in the DT system models. Besides, The modelsignore the DT with not only one data resource. In this paper, we propose a newDT system model considering a heterogeneous MEC/MCC environment. Each DT in themodel is maintained in one of the servers via multiple data collection devices.The offloading decision-making problem is also considered and a new offloadingscheme is proposed based on Distributed Deep Learning (DDL). Simulation resultsdemonstrate that our proposed algorithm can effectively and efficientlydecrease the system's average latency and energy consumption. Significantimprovement is achieved compared with the baselines under the dynamicenvironment of DTs.", "output": "Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud Computing Environment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building energy prediction and management has become increasingly importantin recent decades, driven by the growth of Internet of Things (IoT) devices andthe availability of more energy data. However, energy data is often collectedfrom multiple sources and can be incomplete or inconsistent, which can hinderaccurate predictions and management of energy systems and limit the usefulnessof the data for decision-making and research. To address this issue, paststudies have focused on imputing missing gaps in energy data, including randomand continuous gaps. One of the main challenges in this area is the lack ofvalidation on a benchmark dataset with various building and meter types, makingit difficult to accurately evaluate the performance of different imputationmethods. Another challenge is the lack of application of state-of-the-artimputation methods for missing gaps in energy data. Contemporaryimage-inpainting methods, such as Partial Convolution (PConv), have been widelyused in the computer vision domain and have demonstrated their effectiveness indealing with complex missing patterns. To study whether energy data imputationcan benefit from the image-based deep learning method, this study comparedPConv, Convolutional neural networks (CNNs), and weekly persistence methodusing one of the biggest publicly available whole building energy datasets,consisting of 1479 power meters worldwide, as the benchmark. The results showthat, compared to the CNN with the raw time series (1D-CNN) and the weeklypersistence method, neural network models with reshaped energy data with twodimensions reduced the Mean Squared Error (MSE) by 10% to 30%. The advanceddeep learning method, Partial convolution (PConv), has further reduced the MSEby 20-30% than 2D-CNN and stands out among all models.", "output": "Filling time-series gaps using image techniques: Multidimensional context autoencoder approach for building energy data imputation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work establishes provably faster convergence rates for gradient descentvia a computer-assisted analysis technique. Our theory allows nonconstantstepsize policies with frequent long steps potentially violating descent byanalyzing the overall effect of many iterations at once rather than the typicalone-iteration inductions used in most first-order method analyses. We show thatlong steps, which may increase the objective value in the short term, lead toprovably faster convergence in the long term. A conjecture towards proving afaster $O(1/Tlog T)$ rate for gradient descent is also motivated along withsimple numerical validation.", "output": "Provably Faster Gradient Descent via Long Steps."}]