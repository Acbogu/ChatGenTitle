[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans can collaborate and complete tasks based on visual signals andinstruction from the environment. Training such a robot is difficult especiallydue to the understanding of the instruction and the complicated environment.Previous instruction-following agents are biased to English-centric corpus,making it unrealizable to be applied to users that use multiple languages oreven low-resource languages. Nevertheless, the instruction-following agents arepre-trained in a mode that assumes the user can observe the environment, whichlimits its accessibility. In this work, we're trying to generalize the successof instruction-following agents to non-English languages with little corpusresources, and improve its intractability and accessibility. We introduce UVLN(Universal Vision-Language Navigation), a novel machine-translationinstructional augmented framework for cross-lingual vision-language navigation,with a novel composition of state-of-the-art large language model (GPT3) withthe image caption model (BLIP). We first collect a multilanguagevision-language navigation dataset via machine translation. Then we extend thestandard VLN training objectives to a multilingual setting via a cross-linguallanguage encoder. The alignment between different languages is captured througha shared vision and action context via a cross-modal transformer, which encodesthe inputs of language instruction, visual observation, and action decisionsequences. To improve the intractability, we connect our agent with the largelanguage model that informs the situation and current state to the user andalso explains the action decisions. Experiments over Room Across Room Datasetprove the effectiveness of our approach. And the qualitative results show thepromising intractability and accessibility of our instruction-following agent.", "output": "Accessible Instruction-Following Agent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine unlearning (MU) is a field that is gaining increasing attention dueto the need to remove or modify predictions made by machine learning (ML)models. While training models have become more efficient and accurate, theimportance of unlearning previously learned information has become increasinglysignificant in fields such as privacy, security, and fairness. This paperpresents a comprehensive survey of MU, covering current state-of-the-arttechniques and approaches, including data deletion, perturbation, and modelupdates. In addition, commonly used metrics and datasets are also presented.The paper also highlights the challenges that need to be addressed, includingattack sophistication, standardization, transferability, interpretability,training data, and resource constraints. The contributions of this paperinclude discussions about the potential benefits of MU and its futuredirections in Natural Language Processing, Computer vision, and RecommenderSystems. Additionally, the paper emphasizes the need for researchers andpractitioners to continue exploring and refining unlearning techniques toensure that ML models can adapt to changing circumstances while maintaininguser trust. The importance of unlearning is further highlighted in makingArtificial Intelligence (AI) more trustworthy and transparent, especially withthe increasing importance of AI in various domains that involve large amountsof personal user data", "output": "Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We observe that the mapping between an image's representation in one model toits representation in another can be learned surprisingly well with just alinear layer, even across diverse models. Building on this observation, wepropose $textit{text-to-concept}$, where features from a fixed pretrainedmodel are aligned linearly to the CLIP space, so that text embeddings fromCLIP's text encoder become directly comparable to the aligned features. Withtext-to-concept, we convert fixed off-the-shelf vision encoders to surprisinglystrong zero-shot classifiers for free, with accuracy at times even surpassingthat of CLIP, despite being much smaller models and trained on a small fractionof the data compared to CLIP. We show other immediate use-cases oftext-to-concept, like building concept bottleneck models with no conceptsupervision, diagnosing distribution shifts in terms of human concepts, andretrieving images satisfying a set of text-based constraints. Lastly, wedemonstrate the feasibility of $textit{concept-to-text}$, where vectors in amodel's feature space are decoded by first aligning to the CLIP before beingfed to a GPT-based generative model. Our work suggests existing deep models,with presumably diverse architectures and training, represent input samplesrelatively similarly, and a two-way communication across model representationspaces and to humans (through language) is viable.", "output": "Text-To-Concept (and Back) via Cross-Model Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised knowledge-graph completion (KGC) relies on estimating ascoring model over (entity, relation, entity)-tuples, for example, by embeddingan initial knowledge graph. Prediction quality can be improved by calibratingthe scoring model, typically by adjusting the prediction thresholds usingmanually annotated examples. In this paper, we attempt for the first timecold-start calibration for KGC, where no annotated examples exist initially forcalibration, and only a limited number of tuples can be selected forannotation. Our new method ACTC finds good per-relation thresholds efficientlybased on a limited set of annotated tuples. Additionally to a few annotatedtuples, ACTC also leverages unlabeled tuples by estimating their correctnesswith Logistic Regression or Gaussian Process classifiers. We also experimentwith different methods for selecting candidate tuples for annotation:density-based and random selection. Experiments with five scoring models and anoracle annotator show an improvement of 7% points when using ACTC in thechallenging setting with an annotation budget of only 10 tuples, and an averageimprovement of 4% points over different budgets.", "output": "ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adaptive learning is an area of educational technology that consists indelivering personalized learning experiences to address the unique needs ofeach learner. An important subfield of adaptive learning is learning pathpersonalization: it aims at designing systems that recommend sequences ofeducational activities to maximize students' learning outcomes. Many machinelearning approaches have already demonstrated significant results in a varietyof contexts related to learning path personalization. However, most of themwere designed for very specific settings and are not very reusable. This isaccentuated by the fact that they often rely on non-scalable models, which areunable to integrate new elements after being trained on a specific set ofeducational resources. In this paper, we introduce a flexible and scalableapproach towards the problem of learning path personalization, which weformalize as a reinforcement learning problem. Our model is a sequentialrecommender system based on a graph neural network, which we evaluate on apopulation of simulated learners. Our results demonstrate that it can learn tomake good recommendations in the small-data regime.", "output": "Towards Scalable Adaptive Learning with Graph Neural Networks and Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text embeddings are useful features for several NLP applications, such assentence similarity, text clustering, and semantic search. In this paper, wepresent a Low-rank Adaptation with a Contrastive objective on top of 8-bitSiamese-BLOOM, a multilingual large language model optimized to producesemantically meaningful word embeddings. The innovation is threefold. First, wecast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalableadapter (LoRA) and 8-bit Adam optimizer for sentence similarity classification.Third, we apply a Siamese architecture on BLOOM model with a contrastiveobjective to ease the multi-lingual labeled data scarcity. The experimentresults show the quality of learned embeddings from LACoS-BLOOM is proportionalto the number of model parameters and the amount of unlabeled training data.With the parameter efficient fine-tuning design, we are able to run BLOOM 7.1billion parameters end-to-end on a single GPU machine with 32GB memory.Compared to previous solution Sentence-BERT, we achieve significant improvementon both English and multi-lingual STS tasks.", "output": "LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Outside-knowledge visual question answering is a challenging task thatrequires both the acquisition and the use of open-ended real-world knowledge.Some existing solutions draw external knowledge into the cross-modality spacewhich overlooks the much vaster textual knowledge in natural-language space,while others transform the image into a text that further fuses with thetextual knowledge into the natural-language space and completely abandons theuse of visual features. In this paper, we are inspired to constrain thecross-modality space into the same space of natural-language space which makesthe visual features preserved directly, and the model still benefits from thevast knowledge in natural-language space. To this end, we propose a novelframework consisting of a multimodal encoder, a textual encoder and an answerdecoder. Such structure allows us to introduce more types of knowledgeincluding explicit and implicit multimodal and textual knowledge. Extensiveexperiments validate the superiority of the proposed method which outperformsthe state-of-the-art by 6.17% accuracy. We also conduct comprehensive ablationsof each component, and systematically study the roles of varying types ofknowledge. Codes and knowledge data can be found at", "output": "Combo of Thinking and Observing for Outside-Knowledge VQA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diabetes is a global health priority, especially in low- and-middle-incomecountries, where over 50% of premature deaths are attributed to high bloodglucose. Several studies have demonstrated the feasibility of using CommunityHealth Worker (CHW) programs to provide affordable and culturally tailoredsolutions for early detection and management of diabetes. Yet, scalable modelsto design and implement CHW programs while accounting for screening,management, and patient enrollment decisions have not been proposed. Weintroduce an optimization framework to determine personalized CHW visits thatmaximize glycemic control at a community-level. Our framework explicitly modelsthe trade-off between screening new patients and providing management visits toindividuals who are already enrolled in treatment. We account for patients'motivational states, which affect their decisions to enroll or drop out oftreatment and, therefore, the effectiveness of the intervention. We incorporatethese decisions by modeling patients as utility-maximizing agents within abi-level provider problem that we solve using approximate dynamic programming.By estimating patients' health and motivational states, our model builds visitplans that account for patients' tradeoffs when deciding to enroll intreatment, leading to reduced dropout rates and improved resource allocation.We apply our approach to generate CHW visit plans using operational data from asocial enterprise serving low-income neighborhoods in urban areas of India.Through extensive simulation experiments, we find that our framework requiresup to 73.4% less capacity than the best naive policy to achieve the sameperformance in terms of glycemic control. Our experiments also show that oursolution algorithm can improve upon naive policies by up to 124.5% using thesame CHW capacity.", "output": "Planning a Community Approach to Diabetes Care in Low- and Middle-Income Countries Using Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In Islam, readers must apply a set of pronunciation rules called Tajweedrules to recite the Quran in the same way that the angel Jibrael taught theProphet, Muhammad. The traditional process of learning the correct applicationof these rules requires a human who must have a license and great experience todetect mispronunciation. Due to the increasing number of Muslims around theworld, the number of Tajweed teachers is not enough nowadays for dailyrecitation practice for every Muslim. Therefore, lots of work has been done forautomatic Tajweed rules' mispronunciation detection to help readers reciteQuran correctly in an easier way and shorter time than traditional learningways. All previous works have three common problems. First, most of themfocused on machine learning algorithms only. Second, they used private datasetswith no benchmark to compare with. Third, they did not take into considerationthe sequence of input data optimally, although the speech signal is timeseries. To overcome these problems, we proposed a solution that consists ofMel-Frequency Cepstral Coefficient (MFCC) features with Long Short-Term Memory(LSTM) neural networks which use the time series, to detect mispronunciation inTajweed rules. In addition, our experiments were performed on a public dataset,the QDAT dataset, which contains more than 1500 voices of the correct andincorrect recitation of three Tajweed rules (Separate stretching , Tight Noon ,and Hide ). To the best of our knowledge, the QDAT dataset has not been used byany research paper yet. We compared the performance of the proposed LSTM modelwith traditional machine learning algorithms used in SoTA. The LSTM model withtime series showed clear superiority over traditional machine learning. Theaccuracy achieved by LSTM on the QDAT dataset was 96%, 95%, and 96% for thethree rules (Separate stretching, Tight Noon, and Hide), respectively.", "output": "Mispronunciation Detection of Basic Quranic Recitation Rules using Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As IoT devices are becoming widely deployed, there exist many threats toIoT-based systems due to their inherent vulnerabilities. One effective approachto improving IoT security is to deploy IoT honeypot systems, which can collectattack information and reveal the methods and strategies used by attackers.However, building high-interaction IoT honeypots is challenging due to theheterogeneity of IoT devices. Vulnerabilities in IoT devices typically dependon specific device types or firmware versions, which encourages attackers toperform pre-attack checks to gather device information before launchingattacks. Moreover, conventional honeypots are easily detected because theirreplying logic differs from that of the IoT devices they try to mimic. Toaddress these problems, we develop an adaptive high-interaction honeypot forIoT devices, called HoneyIoT. We first build a real device based attack tracecollection system to learn how attackers interact with IoT devices. We thenmodel the attack behavior through markov decision process and leveragereinforcement learning techniques to learn the best responses to engageattackers based on the attack trace. We also use differential analysistechniques to mutate response values in some fields to generate high-fidelityresponses. HoneyIoT has been deployed on the public Internet. Experimentalresults show that HoneyIoT can effectively bypass the pre-attack checks andmislead the attackers into uploading malware. Furthermore, HoneyIoT is covertagainst widely used reconnaissance and honeypot detection tools.", "output": "HoneyIoT: Adaptive High-Interaction Honeypot for IoT Devices Through Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of mini-batches of data in training artificial neural networks isnowadays very common. Despite its broad usage, theories explainingquantitatively how large or small the optimal mini-batch size should be aremissing. This work presents a systematic attempt at understanding the role ofthe mini-batch size in training two-layer neural networks. Working in theteacher-student scenario, with a sparse teacher, and focusing on tasks ofdifferent complexity, we quantify the effects of changing the mini-batch size$m$. We find that often the generalization performances of the student stronglydepend on $m$ and may undergo sharp phase transitions at a critical value$m_c$, such that for $m&lt;m_c$ the training process fails, while for $m&gt;m_c$ thestudent learns perfectly or generalizes very well the teacher. Phasetransitions are induced by collective phenomena firstly discovered instatistical mechanics and later observed in many fields of science. Finding aphase transition varying the mini-batch size raises several important questionson the role of a hyperparameter which have been somehow overlooked until now.", "output": "Phase transitions in the mini-batch size for sparse and dense neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rapid progress in Multi-Agent Path Finding (MAPF), researchers havestudied how MAPF algorithms can be deployed to coordinate hundreds of robots inlarge automated warehouses. While most works try to improve the throughput ofsuch warehouses by developing better MAPF algorithms, we focus on improving thethroughput by optimizing the warehouse layout. We show that, even withstate-of-the-art MAPF algorithms, commonly used human-designed layouts can leadto congestion for warehouses with large numbers of robots and thus have limitedscalability. We extend existing automatic scenario generation methods tooptimize warehouse layouts. Results show that our optimized warehouse layouts(1) reduce traffic congestion and thus improve throughput, (2) improve thescalability of the automated warehouses by doubling the number of robots insome cases, and (3) are capable of generating layouts with user-specifieddiversity measures. We include the source code at:url{", "output": "Multi-Robot Coordination and Layout Design for Automated Warehousing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised video representation learning aimed at maximizing similaritybetween different temporal segments of one video, in order to enforce featurepersistence over time. This leads to loss of pertinent information related totemporal relationships, rendering actions such as `enter' and `leave' to beindistinguishable. To mitigate this limitation, we propose Latent TimeNavigation (LTN), a time-parameterized contrastive learning strategy that isstreamlined to capture fine-grained motions. Specifically, we maximize therepresentation similarity between different video segments from one video,while maintaining their representations time-aware along a subspace of thelatent representation code including an orthogonal basis to represent temporalchanges. Our extensive experimental analysis suggests that learning videorepresentations by LTN consistently improves performance of actionclassification in fine-grained and human-oriented tasks (e.g., on ToyotaSmarthome dataset). In addition, we demonstrate that our proposed model, whenpre-trained on Kinetics-400, generalizes well onto the unseen real world videobenchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance inaction recognition.", "output": "Self-Supervised Video Representation Learning via Latent Time Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study multi-agent reinforcement learning in the setting of episodic Markovdecision processes, where multiple agents cooperate via communication through acentral server. We propose a provably efficient algorithm based on valueiteration that enable asynchronous communication while ensuring the advantageof cooperation with low communication overhead. With linear functionapproximation, we prove that our algorithm enjoys an$tilde{mathcal{O}}(d^{3/2}H^2sqrt{K})$ regret with$tilde{mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the featuredimension, $H$ is the horizon length, $M$ is the total number of agents, and$K$ is the total number of episodes. We also provide a lower bound showing thata minimal $Omega(dM)$ communication complexity is required to improve theperformance through collaboration.", "output": "Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Technological advances in the context of digital transformation are the basisfor rapid developments in the field of artificial intelligence (AI). AlthoughAI is not a new topic in computer science (CS), recent developments are havingan immense impact on everyday life and society. In consequence, everyone needscompetencies to be able to adequately and competently analyze, discuss and helpshape the impact, opportunities, and limits of artificial intelligence on theirpersonal lives and our society. As a result, an increasing number of CScurricula are being extended to include the topic of AI. However, in order tointegrate AI into existing CS curricula, what students can and should learn inthe context of AI needs to be clarified. This has proven to be particularlydifficult, considering that so far CS education research on central conceptsand principles of AI lacks sufficient elaboration. Therefore, in this paper, wepresent a curriculum of learning objectives that addresses digital literacy andthe societal perspective in particular. The learning objectives can be used tocomprehensively design curricula, but also allow for analyzing currentcurricula and teaching materials and provide insights into the central conceptsand corresponding competencies of AI.", "output": "What Students Can Learn About Artificial Intelligence -- Recommendations for K-12 Computing Education."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs), such as ChatGPT, demonstrate a strongunderstanding of human natural language and have been explored and applied invarious fields, including reasoning, creative writing, code generation,translation, and information retrieval. By adopting LLM as the reasoning core,we propose Autonomous GIS, an AI-powered geographic information system (GIS)that leverages the LLM's general abilities in natural language understanding,reasoning and coding for addressing spatial problems with automatic spatialdata collection, analysis and visualization. We envision that autonomous GISwill need to achieve five autonomous goals including self-generating,self-organizing, self-verifying, self-executing, and self-growing. We introducethe design principles of autonomous GIS to achieve these five autonomous goalsfrom the aspects of information sufficiency, LLM ability, and agentarchitecture. We developed a prototype system called LLM-Geo using GPT-4 API ina Python environment, demonstrating what an autonomous GIS looks like and howit delivers expected results without human intervention using two case studies.For both case studies, LLM-Geo successfully returned accurate results,including aggregated numbers, graphs, and maps, significantly reducing manualoperation time. Although still lacking several important modules such aslogging and code testing, LLM-Geo demonstrates a potential path towardsnext-generation AI-powered GIS. We advocate for the GIScience community todedicate more effort to the research and development of autonomous GIS, makingspatial analysis easier, faster, and more accessible to a broader audience.", "output": "Autonomous GIS: the next-generation AI-powered GIS."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prognostics and health management (PHM) technology plays a critical role inindustrial production and equipment maintenance by identifying and predictingpossible equipment failures and damages, thereby allowing necessary maintenancemeasures to be taken to enhance equipment service life and reliability whilereducing production costs and downtime. In recent years, PHM technology basedon artificial intelligence (AI) has made remarkable achievements in the contextof the industrial IoT and big data, and it is widely used in variousindustries, such as railway, energy, and aviation, for condition monitoring,fault prediction, and health management. The emergence of large-scalefoundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry ofAI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolvedfrom a research paradigm of single-modal, single-task, and limited-data to amulti-modal, multi-task, massive data, and super-large model paradigm. ChatGPTrepresents a landmark achievement in this research paradigm, offering hope forgeneral artificial intelligence due to its highly intelligent natural languageunderstanding ability. However, the PHM field lacks a consensus on how torespond to this significant change in the AI field, and a systematic review androadmap is required to elucidate future development directions. To fill thisgap, this paper systematically expounds on the key components and latestdevelopments of LSF-Models. Then, we systematically answered how to build theLSF-Model applicable to PHM tasks and outlined the challenges and futuredevelopment roadmaps for this research paradigm.", "output": "ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "People who are visually impaired face a lot of difficulties while studying.One of the major causes to this is lack of available text in Bharti Braillescript. In this paper, we have suggested a scheme to convert text in majorIndian languages into Bharti Braille. The system uses a hybrid approach whereat first the text in Indian language is given to a rule based system and incase if there is any ambiguity then it is resolved by applying a LSTM basedmodel. The developed model has also been tested and found to have produced nearaccurate results.", "output": "A Model for Translation of Text from Indian Languages to Bharti Braille Characters."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Task planning is an important component of traditional robotics systemsenabling robots to compose fine grained skills to perform more complex tasks.Recent work building systems for translating natural language to executableactions for task completion in simulated embodied agents is focused on directlypredicting low level action sequences that would be expected to be directlyexecutable by a physical robot. In this work, we instead focus on predicting ahigher level plan representation for one such embodied task completion dataset- TEACh, under the assumption that techniques for high-level plan predictionfrom natural language are expected to be more transferable to physical robotsystems. We demonstrate that better plans can be predicted using multimodalcontext, and that plan prediction and plan execution modules are likelydependent on each other and hence it may not be ideal to fully decouple them.Further, we benchmark execution of oracle plans to quantify the scope forimprovement in plan prediction models.", "output": "Multimodal Contextualized Plan Prediction for Embodied Task Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale pre-trained language models have shown outstanding performance ina variety of NLP tasks. However, they are also known to be significantlybrittle against specifically crafted adversarial examples, leading toincreasing interest in probing the adversarial robustness of NLP systems. Weintroduce RSMI, a novel two-stage framework that combines randomized smoothing(RS) with masked inference (MI) to improve the adversarial robustness of NLPsystems. RS transforms a classifier into a smoothed classifier to obtain robustrepresentations, whereas MI forces a model to exploit the surrounding contextof a masked token in an input sequence. RSMI improves adversarial robustness by2 to 3 times over existing state-of-the-art methods on benchmark datasets. Wealso perform in-depth qualitative analysis to validate the effectiveness of thedifferent stages of RSMI and probe the impact of its components throughextensive ablations. By empirically proving the stability of RSMI, we put itforward as a practical method to robustly train large-scale NLP models. Ourcode and datasets are available at ", "output": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in Natural Language Processing (NLP) has led to theproliferation of large pretrained language models. These models have been shownto yield good performance, using in-context learning, even on unseen tasks andlanguages. They have also been exposed as commercial APIs as a form oflanguage-model-as-a-service, with great adoption. However, their performance onAfrican languages is largely unknown. We present a preliminary analysis ofcommercial large language models on two tasks (machine translation and textclassification) across eight African languages, spanning different languagefamilies and geographical areas. Our results suggest that commercial languagemodels produce below-par performance on African languages. We also find thatthey perform better on text classification than machine translation. Ingeneral, our findings present a call-to-action to ensure African languages arewell represented in commercial large language models, given their growingpopularity.", "output": "How Good are Commercial Large Language Models on African Languages?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spectral clustering is one of the most popular clustering algorithms that hasstood the test of time. It is simple to describe, can be implemented usingstandard linear algebra, and often finds better clusters than traditionalclustering algorithms like $k$-means and $k$-centers. The foundationalalgorithm for two-way spectral clustering, by Shi and Malik, creates ageometric graph from data and finds a spectral cut of the graph.In modern machine learning, many data sets are modeled as a large number ofpoints drawn from a probability density function. Little is known about whenspectral clustering works in this setting -- and when it doesn't. Pastresearchers justified spectral clustering by appealing to the graph Cheegerinequality (which states that the spectral cut of a graph approximates the``Normalized Cut''), but this justification is known to break down on largedata sets.We provide theoretically-informed intuition about spectral clustering onlarge data sets drawn from probability densities, by proving when a continuousform of spectral clustering considered by past researchers (the unweightedspectral cut of a probability density) finds good clusters of the underlyingdensity itself. Our work suggests that Shi-Malik spectral clustering works wellon data drawn from mixtures of Laplace distributions, and works poorly on datadrawn from certain other densities, such as a density we call the `square-roottrough'.Our core theorem proves that weighted spectral cuts have low weightedisoperimetry for all probability densities. Our key tool is a new Cheeger-Buserinequality for all probability densities, including discontinuous ones.", "output": "Spectral Clustering on Large Datasets: When Does it Work? Theory from Continuous Clustering and Density Cheeger-Buser."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With a fast developing pace of geographic applications, automatable andintelligent models are essential to be designed to handle the large volume ofinformation. However, few researchers focus on geographic natural languageprocessing, and there has never been a benchmark to build a unified standard.In this work, we propose a GeoGraphic Language Understanding Evaluationbenchmark, named GeoGLUE. We collect data from open-released geographicresources and introduce six natural language understanding tasks, includinggeographic textual similarity on recall, geographic textual similarity onrerank, geographic elements tagging, geographic composition analysis,geographic where what cut, and geographic entity alignment. We also pro videevaluation experiments and analysis of general baselines, indicating theeffectiveness and significance of the GeoGLUE benchmark.", "output": "GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lifelong learning (LL) is an important ability for NLP models to learn newtasks continuously. Architecture-based approaches are reported to be effectiveimplementations for LL models. However, it is non-trivial to extend previousapproaches to domain incremental LL scenarios since they either require accessto task identities in the testing phase or cannot handle samples from unseentasks. In this paper, we propose textbf{Diana}: aunderline{d}ynamunderline{i}c underline{a}rchitecture-basedlifelounderline{n}g leunderline{a}rning model that tries to learn a sequenceof tasks with a prompt-enhanced language model. Four types of hierarchicallyorganized prompts are used in Diana to capture knowledge from differentgranularities. Specifically, we dedicate task-level prompts to capturetask-specific knowledge to retain high LL performances and maintaininstance-level prompts to learn knowledge shared across input samples toimprove the model's generalization performance. Moreover, we dedicate separateprompts to explicitly model unseen tasks and introduce a set of prompt keyvectors to facilitate knowledge sharing between tasks. Extensive experimentsdemonstrate that Diana outperforms state-of-the-art LL models, especially inhandling unseen tasks. We release the code and data aturl{", "output": "Domain Incremental Lifelong Learning in an Open World."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world data often have an open long-tailed distribution, and building aunified QA model supporting various tasks is vital for practical QAapplications. However, it is non-trivial to extend previous QA approaches sincethey either require access to seen tasks of adequate samples or do notexplicitly model samples from unseen tasks. In this paper, we define OpenLong-Tailed QA (OLTQA) as learning from long-tailed distributed data andoptimizing performance over seen and unseen QA tasks. We propose an OLTQA modelthat encourages knowledge sharing between head, tail and unseen tasks, andexplicitly mines knowledge from a large pre-trained language model (LM).Specifically, we organize our model through a pool of fine-grained componentsand dynamically combine these components for an input to facilitate knowledgesharing. A retrieve-then-rerank frame is further introduced to selectin-context examples, which guild the LM to generate text that express knowledgefor QA tasks. Moreover, a two-stage training approach is introduced topre-train the framework by knowledge distillation (KD) from the LM and thenjointly train the frame and a QA model through an adaptive mutual KD method. Ona large-scale OLTQA dataset we curate from 43 existing QA datasets, our modelconsistently outperforms the state-of-the-art. We release the code and data aturl{", "output": "Long-Tailed Question Answering in an Open World."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recommendation foundation model utilizes large language models (LLM) forrecommendation by converting recommendation tasks into natural language tasks.It enables generative recommendation which directly generates the item(s) torecommend rather than calculating a ranking score for each and every candidateitem in traditional recommendation models, simplifying the recommendationpipeline from multi-stage filtering to single-stage filtering. To avoidgenerating excessively long text when deciding which item(s) to recommend,creating LLM-compatible item IDs is essential for recommendation foundationmodels. In this study, we systematically examine the item indexing problem forrecommendation foundation models, using P5 as the representative backbone modeland replicating its results with various indexing methods. To emphasize theimportance of item indexing, we first discuss the issues of several trivialitem indexing methods, such as independent indexing, title indexing, and randomindexing. We then propose four simple yet effective solutions, includingsequential indexing, collaborative indexing, semantic (content-based) indexing,and hybrid indexing. Our reproducibility study of P5 highlights the significantinfluence of item indexing methods on the model performance, and our results onreal-world datasets validate the effectiveness of our proposed solutions.", "output": "How to Index Item IDs for Recommendation Foundation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Entity alignment is the task of identifying corresponding entities acrossdifferent knowledge graphs (KGs). Although recent embedding-based entityalignment methods have shown significant advancements, they still struggle tofully utilize KG structural information. In this paper, we introduce FGWEA, anunsupervised entity alignment framework that leverages the FusedGromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison ofentity semantics and KG structures within a joint optimization framework. Toaddress the computational challenges associated with optimizing FGW, we devisea three-stage progressive optimization algorithm. It starts with a basicsemantic embedding matching, proceeds to approximate cross-KG structural andrelational similarity matching based on iterative updates of high-confidenceentity links, and ultimately culminates in a global structural comparisonbetween KGs. We perform extensive experiments on four entity alignment datasetscovering 14 distinct KGs across five languages. Without any supervision orhyper-parameter tuning, FGWEA surpasses 21 competitive baselines, includingcutting-edge supervised entity alignment methods. Our code is available at", "output": "A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the findings of SemEval-2023 Task 2 on Fine-grained MultilingualNamed Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the taskfocused on methods to identify complex fine-grained named entities (likeWRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual andmultilingual scenarios, as well as noisy settings. The task used the MultiCoNERV2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, andUkrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. Itattracted 842 submissions from 47 teams, and 34 teams submitted system papers.Results showed that complex entity types such as media titles and product nameswere the most challenging. Methods fusing external knowledge into transformermodels achieved the best performance, and the largest gains were on theCreative Work and Group classes, which are still challenging even with externalknowledge. Some fine-grained classes proved to be more challenging than others,such as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy datahas a significant impact on model performance, with an average drop of 10% onthe noisy subset. The task highlights the need for future research on improvingNER robustness on noisy data containing complex entities.", "output": "SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spectral-temporal graph neural network is a promising abstraction underlyingmost time series forecasting models that are based on graph neural networks(GNNs). However, more is needed to know about the underpinnings of this branchof methods. In this paper, we establish a theoretical framework that unravelsthe expressive power of spectral-temporal GNNs. Our results show that linearspectral-temporal GNNs are universal under mild assumptions, and theirexpressive power is bounded by our extended first-order Weisfeiler-Lemanalgorithm on discrete-time dynamic graphs. To make our findings useful inpractice on valid instantiations, we discuss related constraints in detail andoutline a theoretical blueprint for designing spatial and temporal modules inspectral domains. Building on these insights and to demonstrate how powerfulspectral-temporal GNNs are based on our framework, we propose a simpleinstantiation named Temporal Graph GegenConv (TGC), which significantlyoutperforms most existing models with only linear components and shows bettermodel efficiency.", "output": "How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhileendeavor. HKG consists of hyper-relational facts (H-Facts), composed of a maintriple and several auxiliary attribute-value qualifiers, which can effectivelyrepresent factually comprehensive information. The internal structure of HKGcan be represented as a hypergraph-based representation globally and a semanticsequence-based representation locally. However, existing research seldomsimultaneously models the graphical and sequential structure of HKGs, limitingHKGs' representation. To overcome this limitation, we propose a novelHierarchical Attention model for HKG Embedding (HAHE), including global-leveland local-level attention. The global-level attention can model the graphicalstructure of HKG using hypergraph dual-attention layers, while the local-levelattention can learn the sequential structure inside H-Facts via heterogeneousself-attention layers. Experiment results indicate that HAHE achievesstate-of-the-art performance in link prediction tasks on HKG standard datasets.In addition, HAHE addresses the issue of HKG multi-position prediction for thefirst time, increasing the applicability of the HKG link prediction task. Ourcode is publicly available.", "output": "HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In real world applications, knowledge graphs (KG) are widely used in variousdomains (e.g. medical applications and dialogue agents). However, for factverification, KGs have not been adequately utilized as a knowledge source. KGscan be a valuable knowledge source in fact verification due to theirreliability and broad applicability. A KG consists of nodes and edges whichmakes it clear how concepts are linked together, allowing machines to reasonover chains of topics. However, there are many challenges in understanding howthese machine-readable concepts map to information in text. To enable thecommunity to better use KGs, we introduce a new dataset, FactKG: FactVerification via Reasoning on Knowledge Graphs. It consists of 108k naturallanguage claims with five types of reasoning: One-hop, Conjunction, Existence,Multi-hop, and Negation. Furthermore, FactKG contains various linguisticpatterns, including colloquial style claims as well as written style claims toincrease practicality. Lastly, we develop a baseline approach and analyzeFactKG over these reasoning types. We believe FactKG can advance bothreliability and practicality in KG-based fact verification.", "output": "FactKG: Fact Verification via Reasoning on Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Though majority vote among annotators is typically used for ground truthlabels in natural language processing, annotator disagreement in tasks such ashate speech detection may reflect differences among group opinions, not noise.Thus, a crucial problem in hate speech detection is whether a statement isoffensive to the demographic group that it targets, which may constitute asmall fraction of the annotator pool. We construct a model that predictsindividual annotator ratings on potentially offensive text and combines thisinformation with the predicted target group of the text to model the opinionsof target group members. We show gains across a range of metrics, includingraising performance over the baseline by 22% at predicting individualannotators' ratings and 33% at predicting variance among annotators, whichprovides a method of measuring model uncertainty downstream. We find thatannotators' ratings can be predicted using their demographic information andopinions on online content, without the need to track identifying annotator IDsthat link each annotator to their ratings. We also find that use ofnon-invasive survey questions on annotators' online experiences helps tomaximize privacy and minimize unnecessary collection of demographic informationwhen predicting annotators' opinions.", "output": "When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Speaker protection algorithm is to leverage the playback signal properties toprevent over excursion while maintaining maximum loudness, especially for themobile phone with tiny loudspeakers. This paper proposes efficient DL solutionsto accurately model and predict the nonlinear excursion, which is challengingfor conventional solutions. Firstly, we build the experiment and pre-processingpipeline, where the feedback current and voltage are sampled as input, andlaser is employed to measure the excursion as ground truth. Secondly, oneFFTNet model is proposed to explore the dominant low-frequency and otherunknown harmonics, and compares to a baseline ConvNet model. In addition, BNre-estimation is designed to explore the online adaptation; and INT8quantization based on AI Model efficiency toolkit (AIMETfootnote{AIMET is aproduct of Qualcomm Innovation Center, Inc.}) is applied to further reduce thecomplexity. The proposed algorithm is verified in two speakers and 3 typicaldeployment scenarios, and $&gt;$99% residual DC is less than 0.1 mm, much betterthan traditional solutions.", "output": "Speaker Diaphragm Excursion Prediction: deep attention and online adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robust reinforcement learning (RL) aims at learning a policy that optimizesthe worst-case performance over an uncertainty set. Given nominal Markovdecision process (N-MDP) that generates samples for training, the set containsMDPs obtained by some perturbations from N-MDP. In this paper, we introduce anew uncertainty set containing more realistic MDPs in practice than theexisting sets. Using this uncertainty set, we present a robust RL, namedARQ-Learning, for tabular cases. Also, we characterize the finite-time errorbounds and prove that it converges as fast as Q-Learning and robust Q-Learning(i.e., the state-of-the-art robust RL method) while providing better robustnessfor real applications. We propose {em pessimistic agent} that efficientlytackles the key bottleneck for the extension of ARQ-Learning into large orcontinuous state spaces. Using this technique, we first propose PRQ-Learning.To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG,respectively. We emphasize that our technique can be easily combined with theother popular model-free methods. Via experiments, we demonstrate thesuperiority of the proposed methods in various RL applications with modeluncertainties.", "output": "On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A salient characteristic of large pre-trained language models (PTLMs) is aremarkable improvement in their generalization capability and emergence of newcapabilities with increasing model capacity and pre-training dataset size.Consequently, we are witnessing the development of enormous models pushing thestate-of-the-art. It is, however, imperative to realize that this inevitablyleads to prohibitively long training times, extortionate computing costs, and adetrimental environmental impact. Significant efforts are underway to make PTLMtraining more efficient through innovations in model architectures, trainingpipelines, and loss function design, with scant attention being paid tooptimizing the utility of training data. The key question that we ask iswhether it is possible to train PTLMs by employing only highly informativesubsets of the training data while maintaining downstream performance? Buildingupon the recent progress in informative data subset selection, we show how wecan employ submodular optimization to select highly representative subsets ofthe training corpora. Our results demonstrate that the proposed framework canbe applied to efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) usingonly a fraction of data while retaining up to $sim99%$ of the performance ofthe fully-trained models.", "output": "INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a novel worker selection algorithm, enhancingannotation quality and reducing costs in challenging span-based sequencelabeling tasks in Natural Language Processing (NLP). Unlike previous studiestargeting simpler tasks, this study contends with the complexities of labelinterdependencies in sequence labeling tasks. The proposed algorithm utilizes aCombinatorial Multi-Armed Bandit (CMAB) approach for worker selection. Thechallenge of dealing with imbalanced and small-scale datasets, which hindersoffline simulation of worker selection, is tackled using an innovative dataaugmentation method termed shifting, expanding, and shrinking (SES). The SESmethod is designed specifically for sequence labeling tasks. Rigorous testingon CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm'sefficiency, with an increase in F1 score up to 100.04% of the expert-onlybaseline, alongside cost savings up to 65.97%. The paper also encompasses adataset-independent test emulating annotation evaluation through a Bernoullidistribution, which still led to an impressive 97.56% F1 score of the expertbaseline and 59.88% cost savings. This research addresses and overcomesnumerous obstacles in worker selection for complex NLP tasks.", "output": "Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual as well as genetic biometrics are routinely employed to identifyspecies and individuals in biological applications. However, no attempts havebeen made in this domain to computationally enhance visual classification ofrare classes with little image data via genetics. In this paper, we thuspropose aligned visual-genetic inference spaces with the aim to implicitlyencode cross-domain associations for improved performance. We demonstrate forthe first time that such alignment can be achieved via deep embedding modelsand that the approach is directly applicable to boosting long-tailedrecognition (LTR) particularly for rare species. We experimentally demonstratethe efficacy of the concept via application to microscopic imagery of 30k+planktic foraminifer shells across 32 species when used together withindependent genetic data samples. Most importantly for practitioners, we showthat visual-genetic alignment can significantly benefit visual-only recognitionof the rarest species. Technically, we pre-train a visual ResNet50 deeplearning model using triplet loss formulations to create an initial embeddingspace. We re-structure this space based on genetic anchors embedded via aSequence Graph Transform (SGT) and linked to visual data by cross-domain cosinealignment. We show that an LTR approach improves the state-of-the-art acrossall benchmarks and that adding our visual-genetic alignment improves per-classand particularly rare tail class benchmarks significantly further. We concludethat visual-genetic alignment can be a highly effective tool for complementingvisual biological data containing rare classes. The concept proposed may serveas an important future tool for integrating genetics and imageomics towards amore complete scientific representation of taxonomic spaces and life itself.Code, weights, and data splits are published for full reproducibility.", "output": "Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time-to-event modelling, known as survival analysis, differs from standardregression as it addresses censoring in patients who do not experience theevent of interest. Despite competitive performances in tackling this problem,machine learning methods often ignore other competing risks that preclude theevent of interest. This practice biases the survival estimation. Extensions toaddress this challenge often rely on parametric assumptions or numericalestimations leading to sub-optimal survival approximations. This paperleverages constrained monotonic neural networks to model each competingsurvival distribution. This modelling choice ensures the exact likelihoodmaximisation at a reduced computational cost by using automaticdifferentiation. The effectiveness of the solution is demonstrated on onesynthetic and three medical datasets. Finally, we discuss the implications ofconsidering competing risks when developing risk scores for medical practice.", "output": "Neural Fine-Gray: Monotonic neural networks for competing risks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rutting of asphalt pavements is a crucial design criterion in variouspavement design guides. A good road transportation base can provide securityfor the transportation of oil and gas in road transportation. This studyattempts to develop a robust artificial intelligence model to estimatedifferent asphalt pavements' rutting depth clips, temperature, and load axes asprimary characteristics. The experiment data were obtained from 19 asphaltpavements with different crude oil sources on a 2.038 km long full-scale fieldaccelerated pavement test track (RIOHTrack, Road Track Institute) in Tongzhou,Beijing. In addition, this paper also proposes to build complex networks withdifferent pavement rutting depths through complex network methods and theLouvain algorithm for community detection. The most critical structuralelements can be selected from different asphalt pavement rutting data, andsimilar structural elements can be found. An extreme learning machine algorithmwith residual correction (RELM) is designed and optimized using an independentadaptive particle swarm algorithm. The experimental results of the proposedmethod are compared with several classical machine learning algorithms, withpredictions of Average Root Mean Squared Error, Average Mean Absolute Error,and Average Mean Absolute Percentage Error for 19 asphalt pavements reaching1.742, 1.363, and 1.94% respectively. The experiments demonstrate that theRELM algorithm has an advantage over classical machine learning methods indealing with non-linear problems in road engineering. Notably, the methodensures the adaptation of the simulated environment to different levels ofabstraction through the cognitive analysis of the production environmentparameters.", "output": "A data-driven rutting depth short-time prediction model with metaheuristic optimization for asphalt pavements based on RIOHTrack."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classifier-free guidance is an effective sampling technique in diffusionmodels that has been widely adopted. The main idea is to extrapolate the modelin the direction of text guidance and away from null-text guidance. In thispaper, we demonstrate that null-text guidance in diffusion models is secretly acartoon-style creator, i.e., the generated images can be efficientlytransformed into cartoons by simply perturbing the null-text guidance.Specifically, we proposed two disturbance methods, i.e., Rollback disturbance(Back-D) and Image disturbance (Image-D), to construct misalignment between thenoisy images used for predicting null-text guidance and text guidance(subsequently referred to as textbf{null-text noisy image} and textbf{textnoisy image} respectively) in the sampling process. Back-D achievescartoonization by altering the noise level of null-text noisy image viareplacing $x_t$ with $x_{t+Delta t}$. Image-D, alternatively, produceshigh-fidelity, diverse cartoons by defining $x_t$ as a clean input image, whichfurther improves the incorporation of finer image details. Throughcomprehensive experiments, we delved into the principle of noise disturbing fornull-text and uncovered that the efficacy of disturbance depends on thecorrelation between the null-text noisy image and the source image. Moreover,our proposed techniques, which can generate cartoon images and cartoonizespecific ones, are training-free and easily integrated as a plug-and-playcomponent in any classifier-free guided diffusion model. Project page isavailable at url{", "output": "Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continuous-time models such as Neural ODEs and Neural Flows have shownpromising results in analyzing irregularly sampled time series frequentlyencountered in electronic health records. Based on these models, time seriesare typically processed with a hybrid of an initial value problem (IVP) solverand a recurrent neural network within the variational autoencoder architecture.Sequentially solving IVPs makes such models computationally less efficient. Inthis paper, we propose to model time series purely with continuous processeswhose state evolution can be approximated directly by IVPs. This eliminates theneed for recurrent computation and enables multiple states to evolve inparallel. We further fuse the encoder and decoder with one IVP solver based onits invertibility, which leads to fewer parameters and faster convergence.Experiments on three real-world datasets show that the proposed approachachieves comparable extrapolation and classification performance while gainingmore than one order of magnitude speedup over other continuous-timecounterparts.", "output": "IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Auction-based Federated Learning (AFL) has attracted extensive researchinterest due to its ability to motivate data owners to join FL through economicmeans. Existing works assume that only one data consumer and multiple dataowners exist in an AFL marketplace (i.e., a monopoly market). Therefore, dataowners bid to join the data consumer for FL. However, this assumption is notrealistic in practical AFL marketplaces in which multiple data consumers cancompete to attract data owners to join their respective FL tasks. In thispaper, we bridge this gap by proposing a first-of-its-kind utility-maximizingbidding strategy for data consumers in federated learning (Fed-Bidder). Itenables multiple FL data consumers to compete for data owners via AFLeffectively and efficiently by providing with utility estimation capabilitieswhich can accommodate diverse forms of winning functions, each reflectingdifferent market dynamics. Extensive experiments based on six commonly adoptedbenchmark datasets show that Fed-Bidder is significantly more advantageouscompared to four state-of-the-art approaches.", "output": "Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) mimics how humans and animals interact with theenvironment. The setting is somewhat idealized because, in actual tasks, otheragents in the environment have their own goals and behave adaptively to the egoagent. To thrive in those environments, the agent needs to influence otheragents so their actions become more helpful and less harmful. Research incomputational economics distills two ways to influence others directly: byproviding tangible goods (mechanism design) and by providing information(information design). This work investigates information design problems for agroup of RL agents. The main challenges are two-fold. One is the informationprovided will immediately affect the transition of the agent trajectories,which introduces additional non-stationarity. The other is the information canbe ignored, so the sender must provide information that the receivers arewilling to respect. We formulate the Markov signaling game, and develop thenotions of signaling gradient and the extended obedience constraints thataddress these challenges. Our algorithm is efficient on various mixed-motivetasks and provides further insights into computational economics. Our code isavailable at ", "output": "Information Design in Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In most major cities and urban areas, residents form homogeneousneighborhoods along ethnic or socioeconomic lines. This phenomenon is widelyknown as residential segregation and has been studied extensively. Fifty yearsago, Schelling proposed a landmark model that explains residential segregationin an elegant agent-based way. A recent stream of papers analyzed Schelling'smodel using game-theoretic approaches. However, all these works consideredmodels with a given number of discrete types modeling different ethnic groups.We focus on segregation caused by non-categorical attributes, such ashousehold income or position in a political left-right spectrum. For this, weconsider agent types that can be represented as real numbers. This opens up agreat variety of reasonable models and, as a proof of concept, we focus onseveral natural candidates. In particular, we consider agents that evaluatetheir location by the average type-difference or the maximum type-difference totheir neighbors, or by having a certain tolerance range for type-values ofneighboring agents. We study the existence and computation of equilibria andprovide bounds on the Price of Anarchy and Stability. Also, we presentsimulation results that compare our models and shed light on the obtainedequilibria for our variants.", "output": "Schelling Games with Continuous Types."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While the Large Language Models (LLMs) dominate a majority of languageunderstanding tasks, previous work shows that some of these results aresupported by modelling spurious correlations of training datasets. Authorscommonly assess model robustness by evaluating their models onout-of-distribution (OOD) datasets of the same task, but these datasets mightshare the bias of the training dataset.We propose a simple method for measuring a scale of models' reliance on anyidentified spurious feature and assess the robustness towards a large set ofknown and newly found prediction biases for various pre-trained models anddebiasing methods in Question Answering (QA). We find that the reported OODgains of debiasing methods can not be explained by mitigated reliance on biasedfeatures, suggesting that biases are shared among QA datasets. We furtherevidence this by measuring that performance of OOD models depends on biasfeatures comparably to the ID model, motivating future work to refine thereports of LLMs' robustness to a level of known spurious features.", "output": "Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human communication is the vocal and non verbal signal to communicate withothers. Human expression is a significant biometric object in picture andrecord databases of surveillance systems. Face appreciation has a serious rolein biometric methods and is good-looking for plentiful applications, includingvisual scrutiny and security. Facial expressions are a form of nonverbalcommunication; recognizing them helps improve the human machine interaction.This paper proposes an idea for face and enlightenment invariant credit offacial expressions by the images. In order on, the person's face can becomputed. Face expression is used in CNN classifier to categorize the acquiredpicture into different emotion categories. It is a deep, feed-forwardartificial neural network. Outcome surpasses human presentation and shows posesalternate performance. Varying lighting conditions can influence the fittingprocess and reduce recognition precision. Results illustrate that dependablefacial appearance credited with changing lighting conditions for separatingreasonable facial terminology display emotions is an efficient representationof clean and assorted moving expressions. This process can also manage theproportions of dissimilar basic affecting expressions of those mixed jointly toproduce sensible emotional facial expressions. Our system contains apre-defined data set, which was residential by a statistics scientist andincludes all pure and varied expressions. On average, a data set has achieved92.4% exact validation of the expressions synthesized by our technique. Thesefacial expressions are compared through the pre-defined data-position insideour system. If it recognizes the person in an abnormal condition, an alert willbe passed to the nearby hospital/doctor seeing that a message.", "output": "Emotion Recognition for Challenged People Facial Appearance in Social using Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Long-form question answering (LFQA) aims at answering complex, open-endedquestions with detailed, paragraph-length responses. The de facto paradigm ofLFQA necessitates two procedures: information retrieval, which searches forrelevant supporting facts, and information synthesis, which integrates thesefacts into a coherent answer. In this paper, we introduce WebCPM, the firstChinese LFQA dataset. One unique feature of WebCPM is that its informationretrieval is based on interactive web search, which engages with a searchengine in real time. Following WebGPT, we develop a web search interface. Werecruit annotators to search for relevant information using our interface andthen answer questions. Meanwhile, the web search behaviors of our annotatorswould be recorded. In total, we collect 5,500 high-quality question-answerpairs, together with 14,315 supporting facts and 121,330 web search actions. Wefine-tune pre-trained language models to imitate human behaviors for web searchand to generate answers based on the collected facts. Our LFQA pipeline, builton these fine-tuned models, generates answers that are no worse thanhuman-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader,respectively.", "output": "WebCPM: Interactive Web Search for Chinese Long-form Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Datalog reasoning based on the semina\"ive evaluation strategy evaluatesrules using traditional join plans, which often leads to redundancy andinefficiency in practice, especially when the rules are complex. Hypertreedecompositions help identify efficient query plans and reduce similarredundancy in query answering. However, it is unclear how this can be appliedto materialisation and incremental reasoning with recursive Datalog programs.Moreover, hypertree decompositions require additional data structures and thusintroduce nonnegligible overhead in both runtime and memory consumption. Inthis paper, we provide algorithms that exploit hypertree decompositions for thematerialisation and incremental evaluation of Datalog programs. Furthermore, wecombine this approach with standard Datalog reasoning algorithms in a modularfashion so that the overhead caused by the decompositions is reduced. Ourempirical evaluation shows that, when the program contains complex rules, thecombined approach is usually significantly faster than the baseline approach,sometimes by orders of magnitude.", "output": "Enhancing Datalog Reasoning with Hypertree Decompositions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce the notion of a risk-limiting financial auditing (RLFA): given$N$ transactions, the goal is to estimate the total misstated monetaryfraction~($m^*$) to a given accuracy $epsilon$, with confidence $1-delta$. Wedo this by constructing new confidence sequences (CSs) for the weighted averageof $N$ unknown values, based on samples drawn without replacement according toa (randomized) weighted sampling scheme. Using the idea of importance weightingto construct test martingales, we first develop a framework to construct CSsfor arbitrary sampling strategies. Next, we develop methods to improve thequality of CSs by incorporating side information about the unknown valuesassociated with each item. We show that when the side information issufficiently predictive, it can directly drive the sampling. Addressing thecase where the accuracy is unknown a priori, we introduce a method thatincorporates side information via control variates. Crucially, our constructionis adaptive: if the side information is highly predictive of the unknownmisstated amounts, then the benefits of incorporating it are significant; butif the side information is uncorrelated, our methods learn to ignore it. Ourmethods recover state-of-the-art bounds for the special case when the weightsare equal, which has already found applications in election auditing. Theharder weighted case solves our more challenging problem of AI-assistedfinancial auditing.", "output": "Risk-limiting Financial Audits via Weighted Sampling without Replacement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "African languages have far less in-language content available digitally,making it challenging for question answering systems to satisfy the informationneeds of users. Cross-lingual open-retrieval question answering (XOR QA)systems -- those that retrieve answer content from other languages whileserving people in their native language -- offer a means of filling this gap.To this end, we create AfriQA, the first cross-lingual QA dataset with a focuson African languages. AfriQA includes 12,000+ XOR QA examples across 10 Africanlanguages. While previous datasets have focused primarily on languages wherecross-lingual QA augments coverage from the target language, AfriQA focuses onlanguages where cross-lingual answer content is the only high-coverage sourceof answer content. Because of this, we argue that African languages are one ofthe most important and realistic use cases for XOR QA. Our experimentsdemonstrate the poor performance of automatic translation and multilingualretrieval methods. Overall, AfriQA proves challenging for state-of-the-art QAmodels. We hope that the dataset enables the development of more equitable QAtechnology.", "output": "AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusion probabilistic models (DDPMs) have shown promisingperformance for speech synthesis. However, a large number of iterative stepsare required to achieve high sample quality, which restricts the inferencespeed. Maintaining sample quality while increasing sampling speed has become achallenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based\"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through asingle diffusion sampling step while achieving high audio quality. Theconsistency constraint is applied to distill a consistency model from awell-designed diffusion-based teacher model, which ultimately yields superiorperformances in the distilled CoMoSpeech. Our experiments show that bygenerating audio recordings by a single sampling step, the CoMoSpeech achievesan inference speed more than 150 times faster than real-time on a single NVIDIAA100 GPU, which is comparable to FastSpeech2, making diffusion-sampling basedspeech synthesis truly practical. Meanwhile, objective and subjectiveevaluations on text-to-speech and singing voice synthesis show that theproposed teacher models yield the best audio quality, and the one-step samplingbased CoMoSpeech achieves the best inference speed with better or comparableaudio quality to other conventional multi-step diffusion model baselines. Audiosamples are available at ", "output": "CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the unavailability of routing information in design stages prior todetailed routing (DR), the tasks of timing prediction and optimization posemajor challenges. Inaccurate timing prediction wastes design effort, hurtscircuit performance, and may lead to design failure. This work focuses ontiming prediction after clock tree synthesis and placement legalization, whichis the earliest opportunity to time and optimize a \"complete\" netlist. Thepaper first documents that having \"oracle knowledge\" of the final post-DRparasitics enables post-global routing (GR) optimization to produce improvedfinal timing outcomes. To bridge the gap between GR-based parasitic and timingestimation and post-DR results during post-GR optimization, machine learning(ML)-based models are proposed, including the use of features for macroblockages for accurate predictions for designs with macros. Based on a set ofexperimental evaluations, it is demonstrated that these models show higheraccuracy than GR-based timing estimation. When used during post-GRoptimization, the ML-based models show demonstrable improvements in post-DRcircuit performance. The methodology is applied to two different tool flows -OpenROAD and a commercial tool flow - and results on 45nm bulk and 12nm FinFETenablements show improvements in post-DR slack metrics without increasingcongestion. The models are demonstrated to be generalizable to designsgenerated under different clock period constraints and are robust to trainingdata with small levels of noise.", "output": "A Machine Learning Approach to Improving Timing Consistency between Global Route and Detailed Route."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This two-part paper develops a paradigmatic theory and detailed methods ofthe joint electricity market design using reinforcement-learning (RL)-basedsimulation. In Part 2, this theory is further demonstrated by elaboratingdetailed methods of designing an electricity spot market (ESM), together with areserved capacity product (RC) in the ancillary service market (ASM) and avirtual bidding (VB) product in the financial market (FM). Following the theoryproposed in Part 1, firstly, market design options in the joint market arespecified. Then, the Markov game model is developed, in which we show how toincorporate market design options and uncertain risks in model formulation. Amulti-agent policy proximal optimization (MAPPO) algorithm is elaborated, as apractical implementation of the generalized market simulation method developedin Part 1. Finally, the case study demonstrates how to pick the best marketdesign options by using some of the market operation performance indicatorsproposed in Part 1, based on the simulation results generated by implementingthe MAPPO algorithm. The impacts of different market design options on marketparticipants' bidding strategy preference are also discussed.", "output": "How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Nash Equilibrium (NE) estimation in bidding games of electricity marketsis the key concern of both generation companies (GENCOs) for bidding strategyoptimization and the Independent System Operator (ISO) for market surveillance.However, existing methods for NE estimation in emerging modern electricitymarkets (FEM) are inaccurate and inefficient because the priori knowledge ofbidding strategies before any environment changes, such as load demandvariations, network congestion, and modifications of market design, is notfully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM(BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategyoptimization considering the priori knowledge. A novel Multi-Agent GenerativeAdversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed toenable GENCOs to learn simultaneously from priori knowledge and interactionswith changing environments. The obtained NE is a Bayesian Nash Equilibrium(BNE) with priori knowledge transferred from the previous environment. In thecase study, the superiority of this proposed algorithm in terms of convergencespeed compared with conventional methods is verified. It is concluded that theoptimal bidding strategies in the obtained BNE can always lead to more profitsthan NE due to the effective learning from the priori knowledge. Also, BNE ismore accurate and consistent with situations in real-world markets.", "output": "An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since the release of ChatGPT, numerous studies have highlighted theremarkable performance of ChatGPT, which often rivals or even surpasses humancapabilities in various tasks and domains. However, this paper presents acontrasting perspective by demonstrating an instance where human performanceexcels in typical tasks suited for ChatGPT, specifically in the domain ofcomputer programming. We utilize the IEEExtreme Challenge competition as abenchmark, a prestigious, annual international programming contest encompassinga wide range of problems with different complexities. To conduct a thoroughevaluation, we selected and executed a diverse set of 102 challenges, drawnfrom five distinct IEEExtreme editions, using three major programminglanguages: Python, Java, and C++. Our empirical analysis provides evidence thatcontrary to popular belief, human programmers maintain a competitive edge overChatGPT in certain aspects of problem-solving within the programming context.In fact, we found that the average score obtained by ChatGPT on the set ofIEEExtreme programming problems is 3.9 to 5.8 times lower than the averagehuman score, depending on the programming language. This paper elaborates onthese findings, offering critical insights into the limitations and potentialareas of improvement for AI-based language models like ChatGPT.", "output": "Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Constraint-based applications attempt to identify a solution that meets alldefined user requirements. If the requirements are inconsistent with theunderlying constraint set, algorithms that compute diagnoses for inconsistentconstraints should be implemented to help users resolve the \"no solution couldbe found\" dilemma. FastDiag is a typical direct diagnosis algorithm thatsupports diagnosis calculation without predetermining conflicts. However, thisapproach faces runtime performance issues, especially when analyzing complexand large-scale knowledge bases. In this paper, we propose a novel algorithm,so-called FastDiagP, which is based on the idea of speculative programming.This algorithm extends FastDiag by integrating a parallelization mechanism thatanticipates and pre-calculates consistency checks requested by FastDiag. Thismechanism helps to provide consistency checks with fast answers and boosts thealgorithm's runtime performance. The performance improvements of our proposedalgorithm have been shown through empirical results using the Linux-2.6.3.33configuration knowledge base.", "output": "FastDiagP: An Algorithm for Parallelized Direct Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "AI systems are not intrinsically neutral and biases trickle in any type oftechnological tool. In particular when dealing with people, AI algorithmsreflect technical errors originating with mislabeled data. As they feed wrongand discriminatory classifications, perpetuating structural racism andmarginalization, these systems are not systematically guarded against bias. Inthis article we consider the problem of bias in AI systems from the point ofview of Information Quality dimensions. We illustrate potential improvements ofa bias mitigation tool in gender classification errors, referring to twotypically difficult contexts: the classification of non-binary individuals andthe classification of transgender individuals. The identification of dataquality dimensions to implement in bias mitigation tool may help achieve morefairness. Hence, we propose to consider this issue in terms of completeness,consistency, timeliness and reliability, and offer some theoretical results.", "output": "Data quality dimensions for fair AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress in artificial intelligence (AI), particularly in the domainof large language models (LLMs), has resulted in powerful and versatiledual-use systems. Indeed, cognition can be put towards a wide variety of tasks,some of which can result in harm. This study investigates how LLMs can be usedfor spear phishing, a prevalent form of cybercrime that involves manipulatingtargets into divulging sensitive information. I first explore LLMs' ability toassist with the reconnaissance and message generation stages of a successfulspear phishing attack, where I find that advanced LLMs are capable ofmeaningfully improving cybercriminals' efficiency during these stages. Next, Iconduct an empirical test by creating unique spear phishing messages for over600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. Myfindings reveal that these messages are not only realistic but also remarkablycost-effective, as each email cost only a fraction of a cent to generate. Next,I demonstrate how basic prompt engineering can circumvent safeguards installedin LLMs by the reinforcement learning from human feedback fine-tuning process,highlighting the need for more robust governance interventions aimed atmitigating misuse. To address these evolving risks, I propose two potentialsolutions: structured access schemes, such as application programminginterfaces, and LLM-based defensive systems.", "output": "Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain shift and label scarcity heavily limit deep learning applications tovarious medical image analysis tasks. Unsupervised domain adaptation (UDA)techniques have recently achieved promising cross-modality medical imagesegmentation by transferring knowledge from a label-rich source domain to anunlabeled target domain. However, it is also difficult to collect annotationsfrom the source domain in many clinical applications, rendering most priorworks suboptimal with the label-scarce source domain, particularly for few-shotscenarios, where only a few source labels are accessible. To achieve efficientfew-shot cross-modality segmentation, we propose a noveltransformation-consistent meta-hallucination framework, meta-hallucinator, withthe goal of learning to diversify data distributions and generate usefulexamples for enhancing cross-modality performance. In our framework,hallucination and segmentation models are jointly trained with thegradient-based meta-learning strategy to synthesize examples that lead to goodsegmentation performance on the target domain. To further facilitate datahallucination and cross-domain knowledge transfer, we develop a self-ensemblingmodel with a hallucination-consistent property. Our meta-hallucinator canseamlessly collaborate with the meta-segmenter for learning to hallucinate withmutual benefits from a combined view of meta-learning and self-ensemblinglearning. Extensive studies on MM-WHS 2017 dataset for cross-modality cardiacsegmentation demonstrate that our method performs favorably against variousapproaches by a lot in the few-shot UDA scenario.", "output": "Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have shown promising results on utilizing pre-trainedimage-language models for video question answering. While these image-languagemodels can efficiently bootstrap the representation learning of video-languagemodels, they typically concatenate uniformly sampled video frames as visualinputs without explicit language-aware, temporal modeling. When only a portionof a video input is relevant to the language query, such uniform frame samplingcan often lead to missing important visual cues. Although humans often find avideo moment to focus on and rewind the moment to answer questions, training aquery-aware video moment localizer often requires expensive annotations andhigh computational costs. To address this issue, we propose Self-Chained VideoLocalization-Answering (SeViLA), a novel framework that leverages a singleimage-language model (BLIP-2) to tackle both temporal keyframe localization andQA on videos. SeViLA framework consists of two modules: Localizer and Answerer,where both are parameter-efficiently fine-tuned from BLIP-2. We chain thesemodules for cascaded inference and self-refinement. First, in the forwardchain, the Localizer finds multiple language-aware keyframes in a video, whichthe Answerer uses to predict the answer. Second, in the reverse chain, theAnswerer generates keyframe pseudo-labels to refine the Localizer, alleviatingthe need for expensive video moment localization annotations. SeViLAoutperforms several strong baselines/previous works on five video QA and eventprediction tasks, and achieves the state-of-the-art in both fine-tuning(NExT-QA, STAR) and zero-shot (NExT-QA, STAR, How2QA, VLEP) settings. We show acomprehensive analysis, e.g., the impact of Localizer, comparisons of Localizerwith other temporal localization models, pre-training/self-refinement ofLocalizer, and varying the number of keyframes.", "output": "Self-Chained Image-Language Model for Video Localization and Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Smatch metric is a popular method for evaluating graph distances, as isnecessary, for instance, to assess the performance of semantic graph parsingsystems. However, we observe some issues in the metric that jeopardizemeaningful evaluation. E.g., opaque pre-processing choices can affect results,and current graph-alignment solvers do not provide us with upper-bounds.Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore,adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)are spread out, and lack a unifying framework.For better inspection, we divide the metric into three modules:pre-processing, alignment, and scoring. Examining each module, we specify itsgoals and diagnose potential issues, for which we discuss and test mitigationstrategies. For pre-processing, we show how to fully conform to annotationguidelines that allow structurally deviating but valid graphs. For safer andenhanced alignment, we show the feasibility of optimal alignment in a standardevaluation setup, and develop a lossless graph compression method that shrinksthe search space and significantly increases efficiency. For improved scoring,we propose standardized and extended metric calculation of fine-grainedsub-graph meaning aspects. Our code is available at", "output": "SMATCH++: Standardized and Extended Evaluation of Semantic Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - acontrastive image-text pretraining recipe to bridge the gap between image-levelpretraining and open-vocabulary object detection. At the pretraining phase, wepropose to randomly crop and resize regions of positional embeddings instead ofusing the whole image positional embeddings. This better matches the use ofpositional embeddings at region-level in the detection finetuning phase. Inaddition, we replace the common softmax cross entropy loss in contrastivelearning with focal loss to better learn the informative yet difficultexamples. Finally, we leverage recent advances in novel object proposals toimprove open-vocabulary detection finetuning. We evaluate our full model on theLVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the bestexisting approach by +5.8 points in addition to competitive zero-shot transferdetection. Surprisingly, RO-ViT improves the image-level representation as welland achieves the state of the art on 9 out of 12 metrics on COCO and Flickrimage-text retrieval benchmarks, outperforming competitive approaches withlarger models.", "output": "Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Finding the seed set that maximizes the influence spread over a network is awell-known NP-hard problem. Though a greedy algorithm can provide near-optimalsolutions, the subproblem of influence estimation renders the solutionsinefficient. In this work, we propose textsc{Glie}, a graph neural networkthat learns how to estimate the influence spread of the independent cascade.GLIE relies on a theoretical upper bound that is tightened through supervisedtraining.Experiments indicate that it provides accurate influence estimationfor real graphs up to 10 times larger than the train set.Subsequently, weincorporate it into three influence maximization techniques.We first utilizeCost Effective Lazy Forward optimization substituting Monte Carlo simulationswith GLIE, surpassing the benchmarks albeit with a computational overhead. Toimprove computational efficiency we first devise a Q-learning method thatlearns to choose seeds sequentially using GLIE's predictions. Finally, wearrive at the most efficient approach by developing a provably submodularinfluence spread based on GLIE's representations, to rank nodes while buildingthe seed set adaptively. The proposed algorithms are inductive, meaning theyare trained on graphs with less than 300 nodes and up to 5 seeds, and tested ongraphs with millions of nodes and up to 200 seeds. The final method exhibitsthe most promising combination of time efficiency and influence quality,outperforming several baselines.", "output": "Maximizing Influence with Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works indicate that convolutional neural networks (CNN) need largereceptive fields (RF) to compete with visual transformers and their attentionmechanism. In CNNs, RFs can simply be enlarged by increasing the convolutionkernel sizes. Yet the number of trainable parameters, which scalesquadratically with the kernel's size in the 2D case, rapidly becomesprohibitive, and the training is notoriously difficult. This paper presents anew method to increase the RF size without increasing the number of parameters.The dilated convolution (DC) has already been proposed for the same purpose. DCcan be seen as a convolution with a kernel that contains only a few non-zeroelements placed on a regular grid. Here we present a new version of the DC inwhich the spacings between the non-zero elements, or equivalently theirpositions, are no longer fixed but learnable via backpropagation thanks to aninterpolation technique. We call this method \"Dilated Convolution withLearnable Spacings\" (DCLS) and generalize it to the n-dimensional convolutioncase. However, our main focus here will be on the 2D case. We first tried ourapproach on ResNet50: we drop-in replaced the standard convolutions with DCLSones, which increased the accuracy of ImageNet1k classification atiso-parameters, but at the expense of the throughput. Next, we used the recentConvNeXt state-of-the-art convolutional architecture and drop-in replaced thedepthwise convolutions with DCLS ones. This not only increased the accuracy ofImageNet1k classification but also of typical downstream and robustness tasks,again at iso-parameters but this time with negligible cost on throughput, asConvNeXt uses separable convolutions. Conversely, classic DC led to poorperformance with both ResNet50 and ConvNeXt. The code of the method isavailable at:", "output": "Dilated convolution with learnable spacings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advances in autonomy offer the potential for dramatic positive outcomes in anumber of domains, yet enabling their safe deployment remains an open problem.This work's motivating question is: In safety-critical settings, can we avoidthe need to have one human supervise one machine at all times? The workformalizes this scalable supervision problem by considering remotely locatedhuman supervisors and investigating how autonomous agents can cooperate toachieve safety. This article focuses on the safety-critical context ofautonomous vehicles (AVs) merging into traffic consisting of a mixture of AVsand human drivers. The analysis establishes high reliability upper bounds onhuman supervision requirements. It further shows that AV cooperation canimprove supervision reliability by orders of magnitude and counterintuitivelyrequires fewer supervisors (per AV) as more AVs are adopted. These analyticalresults leverage queuing-theoretic analysis, order statistics, and aconservative, reachability-based approach. A key takeaway is the potentialvalue of cooperation in enabling the deployment of autonomy at scale. Whilethis work focuses on AVs, the scalable supervision framework may be ofindependent interest to a broader array of autonomous control challenges.", "output": "Cooperation for Scalable Supervision of Autonomy in Mixed Traffic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in Artificial Intelligence (AI) have leveraged promisingresults in solving complex problems in the area of Natural Language Processing(NLP), being an important tool to help in the expeditious resolution ofjudicial proceedings in the legal area. In this context, this work targets theproblem of detecting the degree of similarity between judicial documents thatcan be achieved in the inference group, by applying six NLP techniques based onthe transformers architecture to a case study of legal proceedings in theBrazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2and RoBERTa, were pre-trained using a general purpose corpora of the BrazilianPortuguese language, and then were fine-tuned and specialised for the legalsector using 210,000 legal proceedings. Vector representations of each legaldocument were calculated based on their embeddings, which were used to clusterthe lawsuits, calculating the quality of each model based on the cosine of thedistance between the elements of the group to its centroid. We noticed thatmodels based on transformers presented better performance when compared toprevious traditional NLP techniques, with the RoBERTa model specialised for theBrazilian Portuguese language presenting the best results. This methodology canbe also applied to other case studies for different languages, making itpossible to advance in the current state of the art in the area of NLP appliedto the legal sector.", "output": "Analysing similarities between legal court documents using natural language processing approaches based on Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Technology for open-ended language generation, a key application ofartificial intelligence, has advanced to a great extent in recent years.Large-scale language models, which are trained on large corpora of text, arebeing used in a wide range of applications everywhere, from virtual assistantsto conversational bots. While these language models output fluent text,existing research shows that these models can and do capture human biases. Manyof these biases, especially those that could potentially cause harm, are beingwell-investigated. On the other hand, studies that infer and change humanpersonality traits inherited by these models have been scarce or non-existent.Our work seeks to address this gap by exploring the personality traits ofseveral large-scale language models designed for open-ended text generation andthe datasets used for training them. We build on the popular Big Five factorsand develop robust methods that quantify the personality traits of these modelsand their underlying datasets. In particular, we trigger the models with aquestionnaire designed for personality assessment and subsequently classify thetext responses into quantifiable traits using a Zero-shot classifier. Ourestimation scheme sheds light on an important anthropomorphic element found insuch AI models and can help stakeholders decide how they should be applied aswell as how society could perceive them. Additionally, we examined approachesto alter these personalities, adding to our understanding of how AI models canbe adapted to specific contexts.", "output": "Estimating the Personality of White-Box Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Miller et al. showed that a model's in-distribution (ID) accuracyhas a strong linear correlation with its out-of-distribution (OOD) accuracy onseveral OOD benchmarks -- a phenomenon they dubbed ''accuracy-on-the-line''.While a useful tool for model selection (i.e., the model most likely to performthe best OOD is the one with highest ID accuracy), this fact does not helpestimate the actual OOD performance of models without access to a labeled OODvalidation set. In this paper, we show a similar but surprising phenomenon alsoholds for the agreement between pairs of neural network classifiers: wheneveraccuracy-on-the-line holds, we observe that the OOD agreement between thepredictions of any two pairs of neural networks (with potentially differentarchitectures) also observes a strong linear correlation with their IDagreement. Furthermore, we observe that the slope and bias of OOD vs IDagreement closely matches that of OOD vs ID accuracy. This phenomenon, which wecall agreement-on-the-line, has important practical applications: without anylabeled data, we can predict the OOD accuracy of classifiers}, since OODagreement can be estimated with just unlabeled data. Our prediction algorithmoutperforms previous methods both in shifts where agreement-on-the-line holdsand, surprisingly, when accuracy is not on the line. This phenomenon alsoprovides new insights into deep neural networks: unlike accuracy-on-the-line,agreement-on-the-line appears to only hold for neural network classifiers.", "output": "Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "NLP in the legal domain has seen increasing success with the emergence ofTransformer-based Pre-trained Language Models (PLMs) pre-trained on legal text.PLMs trained over European and US legal text are available publicly; however,legal text from other domains (countries), such as India, have a lot ofdistinguishing characteristics. With the rapidly increasing volume of Legal NLPapplications in various countries, it has become necessary to pre-train suchLMs over legal text of other countries as well. In this work, we attempt toinvestigate pre-training in the Indian legal domain. We re-train (continuepre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indianlegal data, as well as train a model from scratch with a vocabulary based onIndian legal text. We apply these PLMs over three benchmark legal NLP tasks --Legal Statute Identification from facts, Semantic Segmentation of CourtJudgment Documents, and Court Appeal Judgment Prediction -- over both Indianand non-Indian (EU, UK) datasets. We observe that our approach not onlyenhances performance on the new domain (Indian texts) but also over theoriginal domain (European and UK texts). We also conduct explainabilityexperiments for a qualitative comparison of all these different PLMs.", "output": "Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-task learning based video anomaly detection methods combine multipleproxy tasks in different branches to detect video anomalies in differentsituations. Most existing methods either do not combine complementary tasks toeffectively cover all motion patterns, or the class of the objects is notexplicitly considered. To address the aforementioned shortcomings, we propose anovel multi-task learning based method that combines complementary proxy tasksto better consider the motion and appearance features. We combine the semanticsegmentation and future frame prediction tasks in a single branch to learn theobject class and consistent motion patterns, and to detect respective anomaliessimultaneously. In the second branch, we added several attention mechanisms todetect motion anomalies with attention to object parts, the direction ofmotion, and the distance of the objects from the camera. Our qualitativeresults show that the proposed method considers the object class effectivelyand learns motion with attention to the aforementioned important factors whichresults in a precise motion modeling and a better motion anomaly detection.Additionally, quantitative results show the superiority of our method comparedwith state-of-the-art methods.", "output": "Multi-Task Learning based Video Anomaly Detection with Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work concerns the development of deep networks that are certifiablyrobust to adversarial attacks. Joint robust classification-detection wasrecently introduced as a certified defense mechanism, where adversarialexamples are either correctly classified or assigned to the \"abstain\" class. Inthis work, we show that such a provable framework can benefit by extension tonetworks with multiple explicit abstain classes, where the adversarial examplesare adaptively assigned to those. We show that naively adding multiple abstainclasses can lead to \"model degeneracy\", then we propose a regularizationapproach and a training method to counter this degeneracy by promoting full useof the multiple abstain classes. Our experiments demonstrate that the proposedapproach consistently achieves favorable standard vs. robust verified accuracytradeoffs, outperforming state-of-the-art algorithms for various choices ofnumber of abstain classes.", "output": "Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Speech emotion recognition (SER) has been a popular research topic inhuman-computer interaction (HCI). As edge devices are rapidly springing up,applying SER to edge devices is promising for a huge number of HCIapplications. Although deep learning has been investigated to improve theperformance of SER by training complex models, the memory space andcomputational capability of edge devices represents a constraint for embeddingdeep learning models. We propose a neural structured learning (NSL) frameworkthrough building synthesized graphs. An SER model is trained on a sourcedataset and used to build graphs on a target dataset. A relatively lightweightmodel is then trained with the speech samples and graphs together as the input.Our experiments demonstrate that training a lightweight SER model on the targetdataset with speech samples and graphs can not only produce small SER models,but also enhance the model performance compared to models with speech samplesonly and those using classic transfer learning strategies.", "output": "Knowledge Transfer For On-Device Speech Emotion Recognition with Neural Structured Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While Nash equilibrium has emerged as the central game-theoretic solutionconcept, many important games contain several Nash equilibria and we mustdetermine how to select between them in order to create real strategic agents.Several Nash equilibrium refinement concepts have been proposed and studied forsequential imperfect-information games, the most prominent being trembling-handperfect equilibrium, quasi-perfect equilibrium, and recently one-sidedquasi-perfect equilibrium. These concepts are robust to certain arbitrarilysmall mistakes, and are guaranteed to always exist; however, we argue thatneither of these is the correct concept for developing strong agents insequential games of imperfect information. We define a new equilibriumrefinement concept for extensive-form games called observable perfectequilibrium in which the solution is robust over trembles inpublicly-observable action probabilities (not necessarily over all actionprobabilities that may not be observable by opposing players). Observableperfect equilibrium correctly captures the assumption that the opponent isplaying as rationally as possible given mistakes that have been observed (whileprevious solution concepts do not). We prove that observable perfectequilibrium is always guaranteed to exist, and demonstrate that it leads to adifferent solution than the prior extensive-form refinements in no-limit poker.We expect observable perfect equilibrium to be a useful equilibrium refinementconcept for modeling many important imperfect-information games of interest inartificial intelligence.", "output": "Observable Perfect Equilibrium."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models often pre-train on large unsupervised text corpora, thenfine-tune on additional task-specific data. However, typical fine-tuningschemes do not prioritize the examples that they tune on. We show that, if youcan prioritize informative training data, you can achieve better performancewhile using fewer labels. To do this we augment a language model with anepinet: a small additional network that helps to estimate model uncertainty andforms an textit{epistemic neural network} (ENN). ENNs are neural networks thatcan know what they don't know. Using an epinet to prioritize uncertain data, wecan fine-tune BERT on GLUE tasks to the same performance while using 2x lessdata than training without prioritization. We also investigate performance insynthetic neural network generative models designed to build understanding. Ineach setting, using an epinet outperforms heuristic active learning schemes.", "output": "Fine-Tuning Language Models via Epistemic Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the huge amount of parameters, fine-tuning of pretrained languagemodels (PLMs) is prone to overfitting in the low resource scenarios. In thiswork, we present a novel method that operates on the hidden representations ofa PLM to reduce overfitting. During fine-tuning, our method inserts randomautoencoders between the hidden layers of a PLM, which transform activationsfrom the previous layers into multi-view compressed representations beforefeeding them into the upper layers. The autoencoders are plugged out afterfine-tuning, so our method does not add extra parameters or increasecomputation cost during inference. Our method demonstrates promisingperformance improvement across a wide range of sequence- and token-levellow-resource NLP tasks.", "output": "Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning (CL) is a learning paradigm that emulates the humancapability of learning and accumulating knowledge continually withoutforgetting the previously learned knowledge and also transferring the learnedknowledge to help learn new tasks better. This survey presents a comprehensivereview and analysis of the recent progress of CL in NLP, which has significantdifferences from CL in computer vision and machine learning. It covers (1) allCL settings with a taxonomy of existing techniques; (2) catastrophic forgetting(CF) prevention, (3) knowledge transfer (KT), which is particularly importantfor NLP tasks; and (4) some theory and the hidden challenge of inter-task classseparation (ICS). (1), (3) and (4) have not been included in the existingsurvey. Finally, a list of future directions is discussed.", "output": "Continual Learning of Natural Language Processing Tasks: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The quality of training datasets for deep neural networks is a key factorcontributing to the accuracy of resulting models. This effect is amplified indifficult tasks such as object detection. Dealing with errors in datasets isoften limited to accepting that some fraction of examples is incorrect,estimating their confidence and assigning appropriate weights or ignoringuncertain ones during training. In this work, we propose a different approach.We introduce the Confident Learning for Object Detection (CLOD) algorithm forassessing the quality of each label in object detection datasets, identifyingmissing, spurious, mislabeled and mislocated bounding boxes and suggestingcorrections. By focusing on finding incorrect examples in the trainingdatasets, we can eliminate them at the root. Suspicious bounding boxes can bereviewed in order to improve the quality of the dataset, leading to bettermodels without further complicating their already complex architectures. Theproposed method is able to point out 99% of artificially disturbed boundingboxes with a false positive rate below 0.3. We see this method as a promisingpath to correcting popular object detection datasets.", "output": "Combating noisy labels in object detection datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot relation extraction aims to recognize novel relations with fewlabeled sentences in each relation. Previous metric-based few-shot relationextraction algorithms identify relationships by comparing the prototypesgenerated by the few labeled sentences embedding with the embeddings of thequery sentences using a trained metric function. However, as these domainsalways have considerable differences from those in the training dataset, thegeneralization ability of these approaches on unseen relations in many domainsis limited. Since the prototype is necessary for obtaining relationshipsbetween entities in the latent space, we suggest learning more interpretableand efficient prototypes from prior knowledge and the intrinsic semantics ofrelations to extract new relations in various domains more effectively. Byexploring the relationships between relations using prior information, weeffectively improve the prototype representation of relations. By usingcontrastive learning to make the classification margins between sentenceembedding more distinct, the prototype's geometric interpretability isenhanced. Additionally, utilizing a transfer learning approach for thecross-domain problem allows the generation process of the prototype to accountfor the gap between other domains, making the prototype more robust andenabling the better extraction of associations across multiple domains. Theexperiment results on the benchmark FewRel dataset demonstrate the advantagesof the suggested method over some state-of-the-art approaches.", "output": "Cross-Domain Few-Shot Relation Extraction via Representation Learning and Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel neurosymbolic framework called NeSyFOLD to extract logicrules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD'slearning pipeline is as follows: (i) We first pre-train a CNN on the inputimage dataset and extract activations of the last layer kernels as binaryvalues; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithmto generate a logic program that can classify an image -- represented as avector of binary activations corresponding to each kernel -- while producing alogical explanation. The rules generated by the FOLD-SE-M algorithm have kernelnumbers as predicates. We have devised a novel algorithm for automaticallymapping the CNN kernels to semantic concepts in the images. This mapping isused to replace predicate names (kernel numbers) in the rule-set withcorresponding semantic concept labels. The resulting rule-set is interpretable,and can be intuitively understood by humans. We compare our NeSyFOLD frameworkwith the ERIC system that uses a decision-tree like algorithm to obtain therules. Our framework has the following advantages over ERIC: (i) In most cases,NeSyFOLD generates smaller rule-sets without compromising on the accuracy andfidelity; (ii) NeSyFOLD generates the mapping of filter numbers to semanticlabels automatically.", "output": "NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Statutory reasoning is the task of reasoning with facts and statutes, whichare rules written in natural language by a legislature. It is a basic legalskill. In this paper we explore the capabilities of the most capable GPT-3model, text-davinci-003, on an established statutory-reasoning dataset calledSARA. We consider a variety of approaches, including dynamic few-shotprompting, chain-of-thought prompting, and zero-shot prompting. While weachieve results with GPT-3 that are better than the previous best publishedresults, we also identify several types of clear errors it makes. Weinvestigate why these errors happen. We discover that GPT-3 has imperfect priorknowledge of the actual U.S. statutes on which SARA is based. More importantly,we create simple synthetic statutes, which GPT-3 is guaranteed not to have seenduring training. We find GPT-3 performs poorly at answering straightforwardquestions about these simple synthetic statutes.", "output": "Can GPT-3 Perform Statutory Reasoning?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inspired by the theory of desirable gambles that is used to model uncertaintyin the field of imprecise probabilities, I present a theory of desirablethings. Its aim is to model a subject's beliefs about which things aredesirable. What the things are is not important, nor is what it means for themto be desirable. It can be applied to gambles, calling them desirable if asubject accepts them, but it can just as well be applied to pizzas, callingthem desirable if my friend Arthur likes to eat them. Other useful examples ofthings one might apply this theory to are propositions, horse lotteries, orpreferences between any of the above. Regardless of the particular things thatare considered, inference rules are imposed by means of an abstract closureoperator, and models that adhere to these rules are called coherent. I considertwo types of models, each of which can capture a subject's beliefs about whichthings are desirable: sets of desirable things and sets of desirable sets ofthings. A crucial result is that the latter type can be represented by a set ofthe former.", "output": "A theory of desirable things."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) learns high-quality representations from largepools of unlabeled training data. As datasets grow larger, it becomes crucialto identify the examples that contribute the most to learning suchrepresentations. This enables efficient SSL by reducing the volume of datarequired for learning high-quality representations. Nevertheless, quantifyingthe value of examples for SSL has remained an open question. In this work, weaddress this for the first time, by proving that examples that contribute themost to contrastive SSL are those that have the most similar augmentations toother examples, in expectation. We provide rigorous guarantees for thegeneralization performance of SSL on such subsets. Empirically, we discover,perhaps surprisingly, the subsets that contribute the most to SSL are thosethat contribute the least to supervised learning. Through extensiveexperiments, we show that our subsets outperform random subsets by more than 3%on CIFAR100, CIFAR10, and STL10. Interestingly, we also find that we can safelyexclude 20% of examples from CIFAR100 and 40% from STL10, without affectingdownstream task performance.", "output": "Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The European Union has proposed the Artificial Intelligence Act whichintroduces a proportional risk-based approach to AI regulation includingdetailed requirements for transparency and explainability. Many of theserequirements may be addressed in practice by the field of explainable AI (XAI),however, there are fundamental differences between XAI and the Act regardingwhat transparency and explainability are. These basic definitions should bealigned to assure that regulation continually translates into appropriatetechnical practices. To facilitate this alignment, we first give an overview ofhow XAI and European regulation view basic definitions of transparency with aparticular focus on the AI Act and the related General Data ProtectionRegulation (GDPR). We then present a comparison of XAI and regulatoryapproaches to identify the main points that would improve alignment between thefields: clarification of the scope of transparency, the legal status of XAI,oversight issues in conformity assessments, and dataset-related transparency.", "output": "Get Your Act Together: A Comparative View on Transparency in the AI Act and Technology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Calibrating agent-based models (ABMs) in economics and finance typicallyinvolves a derivative-free search in a very large parameter space. In thiswork, we benchmark a number of search methods in the calibration of awell-known macroeconomic ABM on real data, and further assess the performanceof \"mixed strategies\" made by combining different methods. We find that methodsbased on random-forest surrogates are particularly efficient, and thatcombining search methods generally increases performance since the biases ofany single method are mitigated. Moving from these observations, we propose areinforcement learning (RL) scheme to automatically select and combine searchmethods on-the-fly during a calibration run. The RL agent keeps exploiting aspecific method only as long as this keeps performing well, but explores newstrategies when the specific method reaches a performance plateau. Theresulting RL search scheme outperforms any other method or method combinationtested, and does not rely on any prior information or trial and errorprocedure.", "output": "Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several approaches have been developed that generate embeddings forDescription Logic ontologies and use these embeddings in machine learning. Oneapproach of generating ontologies embeddings is by first embedding theontologies into a graph structure, i.e., introducing a set of nodes and edgesfor named entities and logical axioms, and then applying a graph embedding toembed the graph in $mathbb{R}^n$. Methods that embed ontologies in graphs(graph projections) have different formal properties related to the type ofaxioms they can utilize, whether the projections are invertible or not, andwhether they can be applied to asserted axioms or their deductive closure. Weanalyze, qualitatively and quantitatively, several graph projection methodsthat have been used to embed ontologies, and we demonstrate the effect of theproperties of graph projections on the performance of predicting axioms fromontology embeddings. We find that there are substantial differences betweendifferent projection methods, and both the projection of axioms into nodes andedges as well ontological choices in representing knowledge will impact thesuccess of using ontology embeddings to predict axioms.", "output": "From axioms over graphs to vectors, and back again: evaluating the properties of graph-based ontology embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep machine learning models including Convolutional Neural Networks (CNN)have been successful in the detection of Mild Cognitive Impairment (MCI) usingmedical images, questionnaires, and videos. This paper proposes a novelMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model todistinguish MCI from those with normal cognition by analyzing facial features.The data comes from the I-CONECT, a behavioral intervention trial aimed atimproving cognitive function by providing frequent video chats. MC-ViViTextracts spatiotemporal features of videos in one branch and augmentsrepresentations by the MC module. The I-CONECT dataset is challenging as thedataset is imbalanced containing Hard-Easy and Positive-Negative samples, whichimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easyand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORREloss to address the imbalanced problem. Our experimental results on theI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with ahigh accuracy of 90.63% accuracy on some of the interview videos.", "output": "MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we aim to develop a large language model (LLM) with thereasoning ability on complex graph data. Currently, LLMs have achieved veryimpressive performance on various natural language learning tasks, extensionsof which have also been applied to study the vision tasks with multi-modaldata. However, when it comes to the graph learning tasks, existing LLMs presentvery serious flaws due to their several inherited weaknesses in performing{multi-step logic reasoning}, {precise mathematical calculation} and{perception about the spatial and temporal factors}.To address such challenges, in this paper, we will investigate theprinciples, methodologies and algorithms to empower existing LLMs with graphreasoning ability, which will have tremendous impacts on the current researchof both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformermodels, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer)framework to teach LLMs themselves with prompts augmented by ChatGPT to useexternal graph reasoning API tools. Specifically, we will investigate to teachGraph-ToolFormer to handle various graph data reasoning tasks in this paper,including both (1) very basic graph data loading and graph property reasoningtasks, ranging from simple graph order and size to the graph diameter andperiphery, and (2) more advanced reasoning tasks on real-world graph data, suchas bibliographic networks, protein molecules, sequential recommender systems,social networks and knowledge graphs.", "output": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Private and public sector structures and norms refine how emerging technologyis used in practice. In healthcare, despite a proliferation of AI adoption, theorganizational governance surrounding its use and integration is often poorlyunderstood. What the Health AI Partnership (HAIP) aims to do in this researchis to better define the requirements for adequate organizational governance ofAI systems in healthcare settings and support health system leaders to makemore informed decisions around AI adoption. To work towards this understanding,we first identify how the standards for the AI adoption in healthcare may bedesigned to be used easily and efficiently. Then, we map out the precisedecision points involved in the practical institutional adoption of AItechnology within specific health systems. Practically, we achieve this througha multi-organizational collaboration with leaders from major health systemsacross the United States and key informants from related fields. Working withthe consultancy IDEO [dot] org, we were able to conduct usability-testingsessions with healthcare and AI ethics professionals. Usability analysisrevealed a prototype structured around mock key decision points that align withhow organizational leaders approach technology adoption. Concurrently, weconducted semi-structured interviews with 89 professionals in healthcare andother relevant fields. Using a modified grounded theory approach, we were ableto identify 8 key decision points and comprehensive procedures throughout theAI adoption lifecycle. This is one of the most detailed qualitative analyses todate of the current governance structures and processes involved in AI adoptionby health systems in the United States. We hope these findings can informfuture efforts to build capabilities to promote the safe, effective, andresponsible adoption of emerging technologies in healthcare.", "output": "Organizational Governance of Emerging Technologies: AI Adoption in Healthcare."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Approaches for appraising feature importance approximations, alternativelyreferred to as attribution methods, have been established across an extensivearray of contexts. The development of resilient techniques for performancebenchmarking constitutes a critical concern in the sphere of explainable deeplearning. This study scrutinizes the dependability of the RemOve-And-Retrain(ROAR) procedure, which is prevalently employed for gauging the performance offeature importance estimates. The insights gleaned from our theoreticalfoundation and empirical investigations reveal that attributions containinglesser information about the decision function may yield superior results inROAR benchmarks, contradicting the original intent of ROAR. This occurrence issimilarly observed in the recently introduced variant RemOve-And-Debias (ROAD),and we posit a persistent pattern of blurriness bias in ROAR attributionmetrics. Our findings serve as a warning against indiscriminate use on ROARmetrics. The code is available as open source.", "output": "On Pitfalls of $\\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Standpoint EL is a multi-modal extension of the popular description logic ELthat allows for the integrated representation of domain knowledge relative todiverse standpoints or perspectives. Advantageously, its satisfiability problemhas recently been shown to be in PTime, making it a promising framework forlarge-scale knowledge integration.In this paper, we show that we can further push the expressivity of thisformalism, arriving at an extended logic, called Standpoint EL+, which allowsfor axiom negation, role chain axioms, self-loops, and other features, whilemaintaining tractability. This is achieved by designing asatisfiability-checking deduction calculus, which at the same time addressesthe need for practical algorithms. We demonstrate the feasibility of ourcalculus by presenting a prototypical Datalog implementation of its deductionrules.", "output": "Pushing the Boundaries of Tractable Multiperspective Reasoning: A Deduction Calculus for Standpoint EL+."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we investigate the relationships between a multipreferentialsemantics for defeasible reasoning in knowledge representation and a multilayerneural network model. Weighted knowledge bases for a simple description logicwith typicality are considered under a (many-valued) ``concept-wise\"multipreference semantics. The semantics is used to provide a preferentialinterpretation of MultiLayer Perceptrons (MLPs). A model checking and anentailment based approach are exploited in the verification of conditionalproperties of MLPs.", "output": "A preferential interpretation of MultiLayer Perceptrons in a conditional logic with typicality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, remarkable results have been achieved in self-supervisedaction recognition using skeleton sequences with contrastive learning. It hasbeen observed that the semantic distinction of human action features is oftenrepresented by local body parts, such as legs or hands, which are advantageousfor skeleton-based action recognition. This paper proposes an attention-basedcontrastive learning framework for skeleton representation learning, calledSkeAttnCLR, which integrates local similarity and global features forskeleton-based action representations. To achieve this, a multi-head attentionmask module is employed to learn the soft attention mask features from theskeletons, suppressing non-salient local features while accentuating localsalient features, thereby bringing similar local features closer in the featurespace. Additionally, ample contrastive pairs are generated by expandingcontrastive pairs based on salient and non-salient features with globalfeatures, which guide the network to learn the semantic representations of theentire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLRlearns local features under different data augmentation views. The experimentresults demonstrate that the inclusion of local feature similaritysignificantly enhances skeleton-based action representation. Our proposedSkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, andPKU-MMD datasets.", "output": "Part Aware Contrastive Learning for Self-Supervised Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is extensive literature on perceiving road structures by fusing varioussensor inputs such as lidar point clouds and camera images using deep neuralnets. Leveraging the latest advance of neural architects (such as transformers)and bird-eye-view (BEV) representation, the road cognition accuracy keepsimproving. However, how to cognize the ``road'' for automated vehicles wherethere is no well-defined ``roads'' remains an open problem. For example, how tofind paths inside intersections without HD maps is hard since there is neitheran explicit definition for ``roads'' nor explicit features such as lanemarkings. The idea of this paper comes from a proverb: it becomes a way whenpeople walk on it. Although there are no ``roads'' from sensor readings, thereare ``roads'' from tracks of other vehicles. In this paper, we propose FlowMap,a path generation framework for automated vehicles based on traffic flows.FlowMap is built by extending our previous work RoadMap, a light-weightsemantic map, with an additional traffic flow layer. A path generationalgorithm on traffic flow fields (TFFs) is proposed to generate human-likepaths. The proposed framework is validated using real-world driving data and isamenable to generating paths for super complicated intersections without usingHD maps.", "output": "FlowMap: Path Generation for Automated Vehicles in Open Space Using Traffic Flow."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper describes the results of SemEval 2023 task 7 -- Multi-EvidenceNatural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2tasks, a Natural Language Inference (NLI) task, and an evidence selection taskon clinical trial data. The proposed challenges require multi-hop biomedicaland numerical reasoning, which are of significant importance to the developmentof systems capable of large-scale interpretation and retrieval of medicalevidence, to provide personalized evidence-based care.Task 1, the entailment task, received 643 submissions from 40 participants,and Task 2, the evidence selection task, received 364 submissions from 23participants. The tasks are challenging, with the majority of submitted systemsfailing to significantly outperform the majority class baseline on theentailment task, and we observe significantly better performance on theevidence selection task than on the entailment task. Increasing the number ofmodel parameters leads to a direct increase in performance, far moresignificant than the effect of biomedical pre-training. Future works couldexplore the limitations of large models for generalization and numericalinference, and investigate methods to augment clinical datasets to allow formore rigorous testing and to facilitate fine-tuning.We envisage that the dataset, models, and results of this task will be usefulto the biomedical NLI and evidence retrieval communities. The dataset,competition leaderboard, and website are publicly available.", "output": "SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As Autonomous Systems (AS) become more ubiquitous in society, moreresponsible for our safety and our interaction with them more frequent, it isessential that they are trustworthy. Assessing the trustworthiness of AS is amandatory challenge for the verification and development community. This willrequire appropriate standards and suitable metrics that may serve toobjectively and comparatively judge trustworthiness of AS across the broadrange of current and future applications. The meta-expression `trustworthiness'is examined in the context of AS capturing the relevant qualities that comprisethis term in the literature. Recent developments in standards and frameworksthat support assurance of autonomous systems are reviewed. A list of keychallenges are identified for the community and we present an outline of aprocess that can be used as a trustworthiness assessment framework for AS.", "output": "Assessing Trustworthiness of Autonomous Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, on-device deep learning has gained attention as a means ofdeveloping affordable deep learning applications for mobile devices. However,on-device models are constrained by limited energy and computation resources.In the mean time, a poisoning attack known as sponge poisoning has beendeveloped.This attack involves feeding the model with poisoned examples toincrease the energy consumption during inference. As previous work is focusingon server hardware accelerators, in this work, we extend the sponge poisoningattack to an on-device scenario to evaluate the vulnerability of mobile deviceprocessors. We present an on-device sponge poisoning attack pipeline tosimulate the streaming and consistent inference scenario to bridge theknowledge gap in the on-device setting. Our exclusive experimental analysiswith processors and on-device networks shows that sponge poisoning attacks caneffectively pollute the modern processor with its built-in accelerator. Weanalyze the impact of different factors in the sponge poisoning algorithm andhighlight the need for improved defense mechanisms to prevent such attacks onon-device deep learning applications.", "output": "Energy-Latency Attacks to On-Device Neural Networks via Sponge Poisoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The increasing popularity of deep learning (DL) models and the advantages ofcomputing, including low latency and bandwidth savings on smartphones, have ledto the emergence of intelligent mobile applications, also known as DL apps, inrecent years. However, this technological development has also given rise toseveral security concerns, including adversarial examples, model stealing, anddata poisoning issues. Existing works on attacks and countermeasures foron-device DL models have primarily focused on the models themselves. However,scant attention has been paid to the impact of data processing disturbance onthe model inference. This knowledge disparity highlights the need foradditional research to fully comprehend and address security issues related todata processing for on-device models. In this paper, we introduce a dataprocessing-based attacks against real-world DL apps. In particular, our attackcould influence the performance and latency of the model without affecting theoperation of a DL app. To demonstrate the effectiveness of our attack, we carryout an empirical study on 517 real-world DL apps collected from Google Play.Among 320 apps utilizing MLkit, we find that 81.56% of them can besuccessfully attacked.The results emphasize the importance of DL app developers being aware of andtaking actions to secure on-device models from the perspective of dataprocessing.", "output": "Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Developing the next generation of household robot helpers requires combininglocomotion and interaction capabilities, which is generally referred to asmobile manipulation (MoMa). MoMa tasks are difficult due to the large actionspace of the robot and the common multi-objective nature of the task, e.g.,efficiently reaching a goal while avoiding obstacles. Current approaches oftensegregate tasks into navigation without manipulation and stationarymanipulation without locomotion by manually matching parts of the action spaceto MoMa sub-objectives (e.g. base actions for locomotion objectives and armactions for manipulation). This solution prevents simultaneous combinations oflocomotion and interaction degrees of freedom and requires human domainknowledge for both partitioning the action space and matching the action partsto the sub-objectives. In this paper, we introduce Causal MoMa, a new frameworkto train policies for typical MoMa tasks that makes use of the most favorablesubspace of the robot's action space to address each sub-objective. Causal MoMaautomatically discovers the causal dependencies between actions and terms ofthe reward function and exploits these dependencies in a causal policy learningprocedure that reduces gradient variance compared to previous state-of-the-artpolicy gradient algorithms, improving convergence and results. We evaluate theperformance of Causal MoMa on three types of simulated robots across differentMoMa tasks and demonstrate success in transferring the policies trained insimulation directly to a real robot, where our agent is able to follow movinggoals and react to dynamic obstacles while simultaneously and synergisticallycontrolling the whole-body: base, arm, and head. More information at", "output": "Causal Policy Gradient for Whole-Body Mobile Manipulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) pre-trained on massive corpora have demonstratedimpressive few-shot learning ability on many NLP tasks. A common practice is torecast the task into a text-to-text format such that generative LLMs of naturallanguage (NL-LLMs) like GPT-3 can be prompted to solve it. However, it isnontrivial to perform information extraction (IE) tasks with NL-LLMs since theoutput of the IE task is usually structured and therefore is hard to beconverted into plain text. In this paper, we propose to recast the structuredoutput in the form of code instead of natural language and utilize generativeLLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular,named entity recognition and relation extraction. In contrast to NL-LLMs, weshow that Code-LLMs can be well-aligned with these IE tasks by designingcode-style prompts and formulating these IE tasks as code generation tasks.Experiment results on seven benchmarks show that our method consistentlyoutperforms fine-tuning moderate-size pre-trained models specially designed forIE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We furtherconduct a series of in-depth analyses to demonstrate the merits of leveragingCode-LLMs for IE tasks.", "output": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Event-based cameras are becoming increasingly popular for their ability tocapture high-speed motion with low latency and high dynamic range. However,generating videos from events remains challenging due to the highly sparse andvarying nature of event data. To address this, in this study, we proposeHyperE2VID, a dynamic neural network architecture for event-based videoreconstruction. Our approach uses hypernetworks and dynamic convolutions togenerate per-pixel adaptive filters guided by a context fusion module thatcombines information from event voxel grids and previously reconstructedintensity images. We also employ a curriculum learning strategy to train thenetwork more robustly. Experimental results demonstrate that HyperE2VIDachieves better reconstruction quality with fewer parameters and fasterinference time than the state-of-the-art methods.", "output": "HyperE2VID: Improving Event-Based Video Reconstruction via Hypernetworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We observe that the mapping between an image's representation in one model toits representation in another can be learned surprisingly well with just alinear layer, even across diverse models. Building on this observation, wepropose $textit{text-to-concept}$, where features from a fixed pretrainedmodel are aligned linearly to the CLIP space, so that text embeddings fromCLIP's text encoder become directly comparable to the aligned features. Withtext-to-concept, we convert fixed off-the-shelf vision encoders to surprisinglystrong zero-shot classifiers for free, with accuracy at times even surpassingthat of CLIP, despite being much smaller models and trained on a small fractionof the data compared to CLIP. We show other immediate use-cases oftext-to-concept, like building concept bottleneck models with no conceptsupervision, diagnosing distribution shifts in terms of human concepts, andretrieving images satisfying a set of text-based constraints. Lastly, wedemonstrate the feasibility of $textit{concept-to-text}$, where vectors in amodel's feature space are decoded by first aligning to the CLIP before beingfed to a GPT-based generative model. Our work suggests existing deep models,with presumably diverse architectures and training, represent input samplesrelatively similarly, and a two-way communication across model representationspaces and to humans (through language) is viable.", "output": "Text-To-Concept (and Back) via Cross-Model Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous robots operating in real-world environments encounter a variety ofobjects that can be both rigid and articulated in nature. Having knowledge ofthese specific object properties not only helps in designing appropriatemanipulation strategies but also aids in developing reliable tracking and poseestimation techniques for many robotic and vision applications. In thiscontext, this paper presents a registration-based local region-to-regionmapping approach to classify an object as either articulated or rigid. Usingthe point clouds of the intended object, the proposed method performsclassification by estimating unique local transformations between point cloudsover the observed sequence of movements of the object. The significantadvantage of the proposed method is that it is a constraint-free approach thatcan classify any articulated object and is not limited to a specific type ofarticulation. Additionally, it is a model-free approach with no learningcomponents, which means it can classify whether an object is articulatedwithout requiring any object models or labelled data. We analyze theperformance of the proposed method on two publicly available benchmark datasetswith a combination of articulated and rigid objects. It is observed that theproposed method can classify articulated and rigid objects with good accuracy.", "output": "Local Region-to-Region Mapping-based Approach to Classify Articulated Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models are becoming increasingly popular in synthetic datageneration and image editing applications. However, these models can amplifyexisting biases and propagate them to downstream applications. Therefore, it iscrucial to understand the sources of bias in their outputs. In this paper, weinvestigate the presence of bias in diffusion-based face generation models withrespect to attributes such as gender, race, and age. Moreover, we examine howdataset size affects the attribute composition and perceptual quality of bothdiffusion and Generative Adversarial Network (GAN) based face generation modelsacross various attribute classes. Our findings suggest that diffusion modelstend to worsen distribution bias in the training data for various attributes,which is heavily influenced by the size of the dataset. Conversely, GAN modelstrained on balanced datasets with a larger number of samples show less biasacross different attributes.", "output": "Analyzing Bias in Diffusion-based Face Generation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Outside-knowledge visual question answering is a challenging task thatrequires both the acquisition and the use of open-ended real-world knowledge.Some existing solutions draw external knowledge into the cross-modality spacewhich overlooks the much vaster textual knowledge in natural-language space,while others transform the image into a text that further fuses with thetextual knowledge into the natural-language space and completely abandons theuse of visual features. In this paper, we are inspired to constrain thecross-modality space into the same space of natural-language space which makesthe visual features preserved directly, and the model still benefits from thevast knowledge in natural-language space. To this end, we propose a novelframework consisting of a multimodal encoder, a textual encoder and an answerdecoder. Such structure allows us to introduce more types of knowledgeincluding explicit and implicit multimodal and textual knowledge. Extensiveexperiments validate the superiority of the proposed method which outperformsthe state-of-the-art by 6.17% accuracy. We also conduct comprehensive ablationsof each component, and systematically study the roles of varying types ofknowledge. Codes and knowledge data can be found at", "output": "Combo of Thinking and Observing for Outside-Knowledge VQA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Segment Anything Model (SAM) is a foundation model for general imagesegmentation. Although it exhibits impressive performance predominantly onnatural images, understanding its robustness against various imageperturbations and domains is critical for real-world applications where suchchallenges frequently arise. In this study we conduct a comprehensiverobustness investigation of SAM under diverse real-world conditions. Ourexperiments encompass a wide range of image perturbations. Our experimentalresults demonstrate that SAM's performance generally declines under perturbedimages, with varying degrees of vulnerability across different perturbations.By customizing prompting techniques and leveraging domain knowledge based onthe unique characteristics of each dataset, the model's resilience to theseperturbations can be enhanced, addressing dataset-specific challenges. Thiswork sheds light on the limitations and strengths of SAM in real-worldapplications, promoting the development of more robust and versatile imagesegmentation solutions.", "output": "An Empirical Study on the Robustness of the Segment Anything Model (SAM)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised video representation learning aimed at maximizing similaritybetween different temporal segments of one video, in order to enforce featurepersistence over time. This leads to loss of pertinent information related totemporal relationships, rendering actions such as `enter' and `leave' to beindistinguishable. To mitigate this limitation, we propose Latent TimeNavigation (LTN), a time-parameterized contrastive learning strategy that isstreamlined to capture fine-grained motions. Specifically, we maximize therepresentation similarity between different video segments from one video,while maintaining their representations time-aware along a subspace of thelatent representation code including an orthogonal basis to represent temporalchanges. Our extensive experimental analysis suggests that learning videorepresentations by LTN consistently improves performance of actionclassification in fine-grained and human-oriented tasks (e.g., on ToyotaSmarthome dataset). In addition, we demonstrate that our proposed model, whenpre-trained on Kinetics-400, generalizes well onto the unseen real world videobenchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance inaction recognition.", "output": "Self-Supervised Video Representation Learning via Latent Time Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding human affective behaviour, especially in the dynamics ofreal-world settings, requires Facial Expression Recognition (FER) models tocontinuously adapt to individual differences in user expression, contextualattributions, and the environment. Current (deep) Machine Learning (ML)-basedFER approaches pre-trained in isolation on benchmark datasets fail to capturethe nuances of real-world interactions where data is available onlyincrementally, acquired by the agent or robot during interactions. New learningcomes at the cost of previous knowledge, resulting in catastrophic forgetting.Lifelong or Continual Learning (CL), on the other hand, enables adaptability inagents by being sensitive to changing data distributions, integrating newinformation without interfering with previously learnt knowledge. Positing CLas an effective learning paradigm for FER, this work presents the ContinualFacial Expression Recognition (ConFER) benchmark that evaluates popular CLtechniques on FER tasks. It presents a comparative analysis of several CL-basedapproaches on popular FER datasets such as CK+, RAF-DB, and AffectNet andpresent strategies for a successful implementation of ConFER for AffectiveComputing (AC) research. CL techniques, under different learning settings, areshown to achieve state-of-the-art (SOTA) performance across several datasets,thus motivating a discussion on the benefits of applying CL principles towardshuman behaviour understanding, particularly from facial expressions, as wellthe challenges entailed.", "output": "Continual Facial Expression Recognition: A Benchmark."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a physics-based humanoid controller that achieves high-fidelitymotion imitation and fault-tolerant behavior in the presence of noisy input(e.g. pose estimates from video or generated from language) and unexpectedfalls. Our controller scales up to learning ten thousand motion clips withoutusing any external stabilizing forces and learns to naturally recover fromfail-state. Given reference motion, our controller can perpetually controlsimulated avatars without requiring resets. At its core, we propose theprogressive multiplicative control policy (PMCP), which dynamically allocatesnew network capacity to learn harder and harder motion sequences. PMCP allowsefficient scaling for learning from large-scale motion databases and adding newtasks, such as fail-state recovery, without catastrophic forgetting. Wedemonstrate the effectiveness of our controller by using it to imitate noisyposes from video-based pose estimators and language-based motion generators ina live and real-time multi-person avatar use case.", "output": "Perpetual Humanoid Control for Real-time Simulated Avatars."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deformable tracking and real-time estimation of 3D tissue motion is essentialto enable automation and image guidance applications in robotically assistedsurgery. Our model, Sparse Efficient Neural Depth and Deformation (SENDD),extends prior 2D tracking work to estimate flow in 3D space. SENDD introducesnovel contributions of learned detection, and sparse per-point depth and 3Dflow estimation, all with less than half a million parameters. SENDD does thisby using graph neural networks of sparse keypoint matches to estimate bothdepth and 3D flow. We quantify and benchmark SENDD on a comprehensivelylabelled tissue dataset, and compare it to an equivalent 2D flow model. SENDDperforms comparably while enabling applications that 2D flow cannot. SENDD cantrack points and estimate depth at 10fps on an NVIDIA RTX 4000 for 1280 tracked(query) points and its cost scales linearly with an increasing/decreasingnumber of points. SENDD enables multiple downstream applications that require3D motion estimation.", "output": "SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes a novel concept for tree and plant reconstruction bydirectly inferring a Lindenmayer-System (L-System) word representation fromimage data in an image captioning approach. We train a model end-to-end whichis able to translate given images into L-System words as a description of thedisplayed tree. To prove this concept, we demonstrate the applicability on 2Dtree topologies. Transferred to real image data, this novel idea could lead tomore efficient, accurate and semantically meaningful tree and plantreconstruction without using error-prone point cloud extraction, and otherprocesses usually utilized in tree reconstruction. Furthermore, this approachbypasses the need for a predefined L-System grammar and enablesspecies-specific L-System inference without biological knowledge.", "output": "Towards L-System Captioning for Tree Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has enabled various Internet of Things (IoT) applications.Still, designing models with high accuracy and computational efficiency remainsa significant challenge, especially in real-time video processing applications.Such applications exhibit high inter- and intra-frame redundancy, allowingfurther improvement. This paper proposes a similarity-aware trainingmethodology that exploits data redundancy in video frames for efficientprocessing. Our approach introduces a per-layer regularization that enhancescomputation reuse by increasing the similarity of weights during training. Wevalidate our methodology on two critical real-time applications, lane detectionand scene parsing. We observe an average compression ratio of approximately 50%and a speedup of sim 1.5x for different models while maintaining the sameaccuracy.", "output": "Treasure What You Have: Exploiting Similarity in Deep Neural Networks for Efficient Video Processing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "General-purpose language models that can solve various language-domain taskshave emerged driven by the pre-training and instruction-tuning pipeline.However, building general-purpose vision-language models is challenging due tothe increased task discrepancy introduced by the additional visual input.Although vision-language pre-training has been widely studied, vision-languageinstruction tuning remains relatively less explored. In this paper, we conducta systematic and comprehensive study on vision-language instruction tuningbased on the pre-trained BLIP-2 models. We gather a wide variety of 26 publiclyavailable datasets, transform them into instruction tuning format andcategorize them into two clusters for held-in instruction tuning and held-outzero-shot evaluation. Additionally, we introduce instruction-aware visualfeature extraction, a crucial method that enables the model to extractinformative features tailored to the given instruction. The resultingInstructBLIP models achieve state-of-the-art zero-shot performance across all13 held-out datasets, substantially outperforming BLIP-2 and the largerFlamingo. Our models also lead to state-of-the-art performance when finetunedon individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG).Furthermore, we qualitatively demonstrate the advantages of InstructBLIP overconcurrent multimodal models. All InstructBLIP models have been open-sourced at", "output": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In practice, digital pathology images are often affected by various factors,resulting in very large differences in color and brightness. Stainnormalization can effectively reduce the differences in color and brightness ofdigital pathology images, thus improving the performance of computer-aideddiagnostic systems. Conventional stain normalization methods rely on one orseveral reference images, but one or several images are difficult to representthe entire dataset. Although learning-based stain normalization methods are ageneral approach, they use complex deep networks, which not only greatly reducecomputational efficiency, but also risk introducing artifacts. StainNet is afast and robust stain normalization network, but it has not a sufficientcapability for complex stain normalization due to its too simple networkstructure. In this study, we proposed a parameter-variable stain normalizationnetwork, ParamNet. ParamNet contains a parameter prediction sub-network and acolor mapping sub-network, where the parameter prediction sub-network canautomatically determine the appropriate parameters for the color mappingsub-network according to each input image. The feature of parameter variableensures that our network has a sufficient capability for various stainnormalization tasks. The color mapping sub-network is a fully 1x1 convolutionalnetwork with a total of 59 variable parameters, which allows our network to beextremely computationally efficient and does not introduce artifacts. Theresults on cytopathology and histopathology datasets show that our ParamNetoutperforms state-of-the-art methods and can effectively improve thegeneralization of classifiers on pathology diagnosis tasks. The code has beenavailable at ", "output": "ParamNet: A Parameter-variable Network for Fast Stain Normalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The primary challenge in video super-resolution (VSR) is to handle largemotions in the input frames, which makes it difficult to accurately aggregateinformation from multiple frames. Existing works either adopt deformableconvolutions or estimate optical flow as a prior to establish correspondencesbetween frames for the effective alignment and fusion. However, they fail totake into account the valuable semantic information that can greatly enhanceit; and flow-based methods heavily rely on the accuracy of a flow estimatemodel, which may not provide precise flows given two low-resolution frames.In this paper, we investigate a more robust and semantic-aware prior forenhanced VSR by utilizing the Segment Anything Model (SAM), a powerfulfoundational model that is less susceptible to image degradation. To use theSAM-based prior, we propose a simple yet effective module -- SAM-guidEdrefinEment Module (SEEM), which can enhance both alignment and fusionprocedures by the utilization of semantic information. This light-weightplug-in module is specifically designed to not only leverage the attentionmechanism for the generation of semantic-aware feature but also be easily andseamlessly integrated into existing methods. Concretely, we apply our SEEM totwo representative methods, EDVR and BasicVSR, resulting in consistentlyimproved performance with minimal implementation effort, on three widely usedVSR datasets: Vimeo-90K, REDS and Vid4. More importantly, we found that theproposed SEEM can advance the existing methods in an efficient tuning manner,providing increased flexibility in adjusting the balance between performanceand the number of training parameters. Code will be open-source soon.", "output": "Can SAM Boost Video Super-Resolution?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a simple but effective technique to smooth out textures whilepreserving the prominent structures. Our method is built upon a key observation-- the coarsest level in a Gaussian pyramid often naturally eliminates texturesand summarizes the main image structures. This inspires our central idea fortexture filtering, which is to progressively upsample the very low-resolutioncoarsest Gaussian pyramid level to a full-resolution texture smoothing resultwith well-preserved structures, under the guidance of each fine-scale Gaussianpyramid level and its associated Laplacian pyramid level. We show that ourapproach is effective to separate structure from texture of different scales,local contrasts, and forms, without degrading structures or introducing visualartifacts. We also demonstrate the applicability of our method on variousapplications including detail enhancement, image abstraction, HDR tone mapping,inverse halftoning, and LDR image enhancement.", "output": "Pyramid Texture Filtering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning based video frame interpolation (VIF) method, aiming tosynthesis the intermediate frames to enhance video quality, have been highlydeveloped in the past few years. This paper investigates the adversarialrobustness of VIF models. We apply adversarial attacks to VIF models and findthat the VIF models are very vulnerable to adversarial examples. To improveattack efficiency, we suggest to make full use of the property of video frameinterpolation task. The intuition is that the gap between adjacent frames wouldbe small, leading to the corresponding adversarial perturbations being similaras well. Then we propose a novel attack method named Inter-frame AccelerateAttack (IAA) that initializes the perturbation as the perturbation for theprevious adjacent frame and reduces the number of attack iterations. It isshown that our method can improve attack efficiency greatly while achievingcomparable attack performance with traditional methods. Besides, we also extendour method to video recognition models which are higher level vision tasks andachieves great attack efficiency.", "output": "Inter-frame Accelerate Attack against Video Interpolation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce WeLayout, a novel system for segmenting thelayout of corporate documents, which stands for WeChat Layout Analysis System.Our approach utilizes a sophisticated ensemble of DINO and YOLO models,specifically developed for the ICDAR 2023 Competition on Robust LayoutSegmentation. Our method significantly surpasses the baseline, securing a topposition on the leaderboard with a mAP of 70.0. To achieve this performance, weconcentrated on enhancing various aspects of the task, such as datasetaugmentation, model architecture, bounding box refinement, and model ensembletechniques. Additionally, we trained the data separately for each documentcategory to ensure a higher mean submission score. We also developed analgorithm for cell matching to further improve our performance. To identify theoptimal weights and IoU thresholds for our model ensemble, we employed aBayesian optimization algorithm called the Tree-Structured Parzen Estimator.Our approach effectively demonstrates the benefits of combining query-based andanchor-free models for achieving robust layout segmentation in corporatedocuments.", "output": "WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This report presents a framework called Segment And Track Anything (SAMTrack)that allows users to precisely and effectively segment and track any object ina video. Additionally, SAM-Track employs multimodal interaction methods thatenable users to select multiple objects in videos for tracking, correspondingto their specific requirements. These interaction methods comprise click,stroke, and text, each possessing unique benefits and capable of being employedin combination. As a result, SAM-Track can be used across an array of fields,ranging from drone technology, autonomous driving, medical imaging, augmentedreality, to biological analysis. SAM-Track amalgamates Segment Anything Model(SAM), an interactive key-frame segmentation model, with our proposed AOT-basedtracking model (DeAOT), which secured 1st place in four tracks of the VOT 2022challenge, to facilitate object tracking in video. In addition, SAM-Trackincorporates Grounding-DINO, which enables the framework to support text-basedinteraction. We have demonstrated the remarkable capabilities of SAM-Track onDAVIS-2016 Val (92.0%), DAVIS-2017 Test (79.2%)and its practicability indiverse applications. The project page is available at:", "output": "Segment and Track Anything."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As emerging hardware begins to support mixed bit-width arithmeticcomputation, mixed-precision quantization is widely used to reduce thecomplexity of neural networks. However, Vision Transformers (ViTs) requirecomplex self-attention computation to guarantee the learning of powerfulfeature representations, which makes mixed-precision quantization of ViTs stillchallenging. In this paper, we propose a novel patch-wise mixed-precisionquantization (PMQ) for efficient inference of ViTs. Specifically, we design alightweight global metric, which is faster than existing methods, to measurethe sensitivity of each component in ViTs to quantization errors. Moreover, wealso introduce a pareto frontier approach to automatically allocate the optimalbit-precision according to the sensitivity. To further reduce the computationalcomplexity of self-attention in inference stage, we propose a patch-wise moduleto reallocate bit-width of patches in each layer. Extensive experiments on theImageNet dataset shows that our method greatly reduces the search cost andfacilitates the application of mixed-precision quantization to ViTs.", "output": "Patch-wise Mixed-Precision Quantization of Vision Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent renaissance in generative models, driven primarily by the adventof diffusion models and iterative improvement in GAN methods, has enabled manycreative applications. However, each advancement is also accompanied by a risein the potential for misuse. In the arena of deepfake generation this is a keysocietal issue. In particular, the ability to modify segments of videos usingsuch generative techniques creates a new paradigm of deepfakes which are mostlyreal videos altered slightly to distort the truth. Current deepfake detectionmethods in the academic literature are not evaluated on this paradigm. In thispaper, we present a deepfake detection method able to address this issue byperforming both frame and video level deepfake prediction. To facilitatetesting our method we create a new benchmark dataset where videos have bothreal and fake frame sequences. Our method utilizes the Vision Transformer,Scaling and Shifting pretraining and Timeseries Transformer to temporallysegment videos to help facilitate the interpretation of possible deepfakes.Extensive experiments on a variety of deepfake generation methods showexcellent results on temporal segmentation and classical video levelpredictions as well. In particular, the paradigm we introduce will form apowerful tool for the moderation of deepfakes, where human oversight can bebetter targeted to the parts of videos suspected of being deepfakes. Allexperiments can be reproduced at:", "output": "Undercover Deepfakes: Detecting Fake Segments in Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This research paper explores the application of style transfer in computervision using RGB images and their corresponding depth maps. We propose a novelmethod that incorporates the depth map and a heatmap of the RGB image togenerate more realistic style transfer results. We compare our method to thetraditional neural style transfer approach and find that our method outperformsit in terms of producing more realistic color and style. The proposed methodcan be applied to various computer vision applications, such as image editingand virtual reality, to improve the realism of generated images. Overall, ourfindings demonstrate the potential of incorporating depth information andheatmap of RGB images in style transfer for more realistic results.", "output": "Realization RGBD Image Stylization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Shape learning, or the ability to leverage shape information, could be adesirable property of convolutional neural networks (CNNs) when target objectshave specific shapes. While some research on the topic is emerging, there is nosystematic study to conclusively determine whether and under what circumstancesCNNs learn shape. Here, we present such a study in the context of segmentationnetworks where shapes are particularly important. We define shape and propose anew behavioral metric to measure the extent to which a CNN utilizes shapeinformation. We then execute a set of experiments with synthetic and real-worlddata to progressively uncover under which circumstances CNNs learn shape andwhat can be done to encourage such behavior. We conclude that (i) CNNs do notlearn shape in typical settings but rather rely on other features available toidentify the objects of interest, (ii) CNNs can learn shape, but only if theshape is the only feature available to identify the object, (iii) sufficientlylarge receptive field size relative to the size of target objects is necessaryfor shape learning; (iv) a limited set of augmentations can encourage shapelearning; (v) learning shape is indeed useful in the presence ofout-of-distribution data.", "output": "Convolutional Neural Networks Rarely Learn Shape for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unlike hiding bit-level messages, hiding image-level messages is morechallenging, which requires large capacity, high imperceptibility, and highsecurity. Although recent advances in hiding image-level messages have beenremarkable, existing schemes are limited to lossless spatial images as coversand cannot be directly applied to JPEG images, the ubiquitous lossy formatimages in daily life. The difficulties of migration are caused by the lack oftargeted design and the loss of details due to lossy decompression andre-compression. Considering that taking DCT densely on $8times8$ image patchesis the core of the JPEG compression standard, we design a novel model calledtextsf{EFDR}, which can comprehensively underline{E}xploitunderline{F}ine-grained underline{D}CT underline{R}epresentations and embedthe secret image into quantized DCT coefficients to avoid the lossy process.Specifically, we transform the JPEG cover image and hidden secret image intofine-grained DCT representations that compact the frequency and are associatedwith the inter-block and intra-block correlations. Subsequently, thefine-grained DCT representations are further enhanced by a sub-band featuresenhancement module. Afterward, a transformer-based invertibility module isdesigned to fuse enhanced sub-band features. Such a design enables afine-grained self-attention on each sub-band and captures long-rangedependencies while maintaining excellent reversibility for hiding and recovery.To our best knowledge, this is the first attempt to embed a color image ofequal size in a color JPEG image. Extensive experiments demonstrate theeffectiveness of our textsf{EFDR} with superior performance.", "output": "Exploiting Fine-Grained DCT Representations for Hiding Image-Level Messages within JPEG Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating high quality music that complements the visual content of a videois a challenging task. Most existing visual conditioned music generationsystems generate symbolic music data, such as MIDI files, instead of raw audiowaveform. Given the limited availability of symbolic music data, such methodscan only generate music for a few instruments or for specific types of visualinput. In this paper, we propose a novel approach called V2Meow that cangenerate high-quality music audio that aligns well with the visual semantics ofa diverse range of video input types. Specifically, the proposed musicgeneration system is a multi-stage autoregressive model which is trained with anumber of O(100K) music audio clips paired with video frames, which are minedfrom in-the-wild music videos, and no parallel symbolic music data is involved.V2Meow is able to synthesize high-fidelity music audio waveform solelyconditioned on pre-trained visual features extracted from an arbitrary silentvideo clip, and it also allows high-level control over the music style ofgeneration examples via supporting text prompts in addition to the video framesconditioning. Through both qualitative and quantitative evaluations, wedemonstrate that our model outperforms several existing music generationsystems in terms of both visual-audio correspondence and audio quality.", "output": "V2Meow: Meowing to the Visual Beat via Music Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep representation learning is a ubiquitous part of modern computer vision.While Euclidean space has been the de facto standard manifold for learningvisual representations, hyperbolic space has recently gained rapid traction forlearning in computer vision. Specifically, hyperbolic learning has shown astrong potential to embed hierarchical structures, learn from limited samples,quantify uncertainty, add robustness, limit error severity, and more. In thispaper, we provide a categorization and in-depth overview of current literatureon hyperbolic learning for computer vision. We research both supervised andunsupervised literature and identify three main research themes in eachdirection. We outline how hyperbolic learning is performed in all themes anddiscuss the main research problems that benefit from current advances inhyperbolic learning for computer vision. Moreover, we provide a high-levelintuition behind hyperbolic geometry and outline open research questions tofurther advance research in this direction.", "output": "Hyperbolic Deep Learning in Computer Vision: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent Transformer-based 3D object detectors learn point cloud featureseither from point- or voxel-based representations. However, the former requirestime-consuming sampling while the latter introduces quantization errors. Inthis paper, we present a novel Point-Voxel Transformer for single-stage 3Ddetection (PVT-SSD) that takes advantage of these two representations.Specifically, we first use voxel-based sparse convolutions for efficientfeature encoding. Then, we propose a Point-Voxel Transformer (PVT) module thatobtains long-range contexts in a cheap manner from voxels while attainingaccurate positions from points. The key to associating the two differentrepresentations is our introduced input-dependent Query Initialization module,which could efficiently generate reference points and content queries. Then,PVT adaptively fuses long-range contextual and local geometric informationaround reference points into content queries. Further, to quickly find theneighboring points of reference points, we design the Virtual Range Imagemodule, which generalizes the native range image to multi-sensor andmulti-frame. The experiments on several autonomous driving benchmarks verifythe effectiveness and efficiency of the proposed method. Code will be availableat ", "output": "PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop a computational framework to quantify uncertainty in shearelastography imaging of anomalies in tissues. We adopt a Bayesian inferenceformulation. Given the observed data, a forward model and their uncertainties,we find the posterior probability of parameter fields representing the geometryof the anomalies and their shear moduli. To construct a prior probability, weexploit the topological energies of associated objective functions. Wedemonstrate the approach on synthetic two dimensional tests with smooth andirregular shapes. Sampling the posterior distribution by Markov Chain MonteCarlo (MCMC) techniques we obtain statistical information on the shear moduliand the geometrical properties of the anomalies. General affine-invariantensemble MCMC samplers are adequate for shapes characterized by parameter setsof low to moderate dimension. However, MCMC methods are computationallyexpensive. For simple shapes, we devise a fast optimization scheme to calculatethe maximum a posteriori (MAP) estimate representing the most likely parametervalues. Then, we approximate the posterior distribution by a Gaussiandistribution found by linearization about the MAP point to capture the mainmode at a low computational cost.", "output": "Object based Bayesian full-waveform inversion for shear elastography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In few-shot image generation, directly training GAN models on just a handfulof images faces the risk of overfitting. A popular solution is to transfer themodels pretrained on large source domains to small target ones. In this work,we introduce WeditGAN, which realizes model transfer by editing theintermediate latent codes $w$ in StyleGANs with learned constant offsets($Delta w$), discovering and constructing target latent spaces via simplyrelocating the distribution of source latent spaces. The established one-to-onemapping between latent spaces can naturally prevents mode collapse andoverfitting. Besides, we also propose variants of WeditGAN to further enhancethe relocation process by regularizing the direction or finetuning theintensity of $Delta w$. Experiments on a collection of widely usedsource/target datasets manifest the capability of WeditGAN in generatingrealistic and diverse images, which is simple yet highly effective in theresearch area of few-shot image generation.", "output": "WeditGAN: Few-shot Image Generation via Latent Space Relocation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual as well as genetic biometrics are routinely employed to identifyspecies and individuals in biological applications. However, no attempts havebeen made in this domain to computationally enhance visual classification ofrare classes with little image data via genetics. In this paper, we thuspropose aligned visual-genetic inference spaces with the aim to implicitlyencode cross-domain associations for improved performance. We demonstrate forthe first time that such alignment can be achieved via deep embedding modelsand that the approach is directly applicable to boosting long-tailedrecognition (LTR) particularly for rare species. We experimentally demonstratethe efficacy of the concept via application to microscopic imagery of 30k+planktic foraminifer shells across 32 species when used together withindependent genetic data samples. Most importantly for practitioners, we showthat visual-genetic alignment can significantly benefit visual-only recognitionof the rarest species. Technically, we pre-train a visual ResNet50 deeplearning model using triplet loss formulations to create an initial embeddingspace. We re-structure this space based on genetic anchors embedded via aSequence Graph Transform (SGT) and linked to visual data by cross-domain cosinealignment. We show that an LTR approach improves the state-of-the-art acrossall benchmarks and that adding our visual-genetic alignment improves per-classand particularly rare tail class benchmarks significantly further. We concludethat visual-genetic alignment can be a highly effective tool for complementingvisual biological data containing rare classes. The concept proposed may serveas an important future tool for integrating genetics and imageomics towards amore complete scientific representation of taxonomic spaces and life itself.Code, weights, and data splits are published for full reproducibility.", "output": "Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classifier-free guidance is an effective sampling technique in diffusionmodels that has been widely adopted. The main idea is to extrapolate the modelin the direction of text guidance and away from null-text guidance. In thispaper, we demonstrate that null-text guidance in diffusion models is secretly acartoon-style creator, i.e., the generated images can be efficientlytransformed into cartoons by simply perturbing the null-text guidance.Specifically, we proposed two disturbance methods, i.e., Rollback disturbance(Back-D) and Image disturbance (Image-D), to construct misalignment between thenoisy images used for predicting null-text guidance and text guidance(subsequently referred to as textbf{null-text noisy image} and textbf{textnoisy image} respectively) in the sampling process. Back-D achievescartoonization by altering the noise level of null-text noisy image viareplacing $x_t$ with $x_{t+Delta t}$. Image-D, alternatively, produceshigh-fidelity, diverse cartoons by defining $x_t$ as a clean input image, whichfurther improves the incorporation of finer image details. Throughcomprehensive experiments, we delved into the principle of noise disturbing fornull-text and uncovered that the efficacy of disturbance depends on thecorrelation between the null-text noisy image and the source image. Moreover,our proposed techniques, which can generate cartoon images and cartoonizespecific ones, are training-free and easily integrated as a plug-and-playcomponent in any classifier-free guided diffusion model. Project page isavailable at url{", "output": "Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current adversarial attacks on motion estimation, or optical flow, optimizesmall per-pixel perturbations, which are unlikely to appear in the real world.In contrast, adverse weather conditions constitute a much more realistic threatscenario. Hence, in this work, we present a novel attack on motion estimationthat exploits adversarially optimized particles to mimic weather effects likesnowflakes, rain streaks or fog clouds. At the core of our attack framework isa differentiable particle rendering system that integrates particles (i)consistently over multiple time steps (ii) into the 3D space (iii) with aphoto-realistic appearance. Through optimization, we obtain adversarial weatherthat significantly impacts the motion estimation. Surprisingly, methods thatpreviously showed good robustness towards small per-pixel perturbations areparticularly vulnerable to adversarial weather. At the same time, augmentingthe training with non-optimized weather increases a method's robustness towardsweather effects and improves generalizability at almost no additional cost.", "output": "Distracting Downpour: Adversarial Weather Attacks for Motion Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, multi-modality scene perception tasks, e.g., image fusion and sceneunderstanding, have attracted widespread attention for intelligent visionsystems. However, early efforts always consider boosting a single taskunilaterally and neglecting others, seldom investigating their underlyingconnections for joint promotion. To overcome these limitations, we establishthe hierarchical dual tasks-driven deep model to bridge these tasks.Concretely, we firstly construct an image fusion module to fuse complementarycharacteristics and cascade dual task-related modules, including adiscriminator for visual effects and a semantic network for featuremeasurement. We provide a bi-level perspective to formulate image fusion andfollow-up downstream tasks. To incorporate distinct task-related responses forimage fusion, we consider image fusion as a primary goal and dual modules aslearnable constraints. Furthermore, we develop an efficient first-orderapproximation to compute corresponding gradients and present dynamic weightedaggregation to balance the gradients for fusion learning. Extensive experimentsdemonstrate the superiority of our method, which not only produces visuallypleasant fused results but also realizes significant promotion for detectionand segmentation than the state-of-the-art approaches.", "output": "Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Motion represents one of the major challenges in magnetic resonance imaging(MRI). Since the MR signal is acquired in frequency space, any motion of theimaged object leads to complex artefacts in the reconstructed image in additionto other MR imaging artefacts. Deep learning has been frequently proposed formotion correction at several stages of the reconstruction process. The widerange of MR acquisition sequences, anatomies and pathologies of interest, andmotion patterns (rigid vs. deformable and random vs. regular) makes acomprehensive solution unlikely. To facilitate the transfer of ideas betweendifferent applications, this review provides a detailed overview of proposedmethods for learning-based motion correction in MRI together with their commonchallenges and potentials. This review identifies differences and synergies inunderlying data usage, architectures and evaluation strategies. We criticallydiscuss general trends and outline future directions, with the aim to enhanceinteraction between different application areas and research fields.", "output": "Deep Learning for Retrospective Motion Correction in MRI: A Comprehensive Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computer vision is a broad field of study that encompasses different tasks(e.g., object detection, semantic segmentation, 3D reconstruction). Althoughcomputer vision is relevant to the African communities in various applications,yet computer vision research is under-explored in the continent and constructsonly 0.06% of top-tier publications in the last 10 years. In this paper, ourgoal is to have a better understanding of the computer vision researchconducted in Africa and provide pointers on whether there is equity in researchor not. We do this through an empirical analysis of the African computer visionpublications that are Scopus indexed. We first study the opportunitiesavailable for African institutions to publish in top-tier computer visionvenues. We show that African publishing trends in top-tier venues over theyears do not exhibit consistent growth. We also devise a novel way to retrieveAfrican authors through their affiliation history to have a betterunderstanding of their contributions in top-tier venues. Moreover, we study allcomputer vision publications beyond top-tier venues in different Africanregions to find that mainly Northern and Southern Africa are publishing incomputer vision with more than 85% of African publications. Finally, we presentthe most recurring keywords in computer vision publications. In summary, ouranalysis reveals that African researchers are key contributors to Africanresearch, yet there exists multiple barriers to publish in top-tier venues andthe current trend of topics published in the continent might not necessarilyreflect the communities' needs. This work is part of a community based effortthat is focused on improving computer vision research in Africa.", "output": "Towards a Better Understanding of the Computer Vision Research Community in Africa."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-destructive assessments of plant phenotypic traits using high-qualitythree-dimensional (3D) and multispectral data can deepen breeders'understanding of plant growth and allow them to make informed managerialdecisions. However, subjective viewpoint selection and complex illuminationeffects under natural light conditions decrease the data quality and increasethe difficulty of resolving phenotypic parameters. We proposed methods foradaptive data acquisition and reflectance correction respectively, to generatehigh-quality 3D multispectral point clouds (3DMPCs) of plants. In the firststage, we proposed an efficient next-best-view (NBV) planning method based on anovel UGV platform with a multi-sensor-equipped robotic arm. In the secondstage, we eliminated the illumination effects by using the neural referencefield (NeREF) to predict the digital number (DN) of the reference. We testedthem on 6 perilla and 6 tomato plants, and selected 2 visible leaves and 4regions of interest (ROIs) for each plant to assess the biomass and thechlorophyll content. For NBV planning, the average execution time for singleperilla and tomato plant at a joint speed of 1.55 rad/s was 58.70 s and 53.60 srespectively. The whole-plant data integrity was improved by an average of 27%compared to using fixed viewpoints alone, and the coefficients of determination(R2) for leaf biomass estimation reached 0.99 and 0.92. For reflectancecorrection, the average root mean squared error of the reflectance spectra withhemisphere reference-based correction at different ROIs was 0.08 and 0.07 forperilla and tomato. The R2 of chlorophyll content estimation was 0.91 and 0.93respectively when principal component analysis and Gaussian process regressionwere applied. Our approach is promising for generating high-quality 3DMPCs ofplants under natural light conditions and facilitates accurate plantphenotyping.", "output": "Generating high-quality 3DMPCs by adaptive data acquisition and NeREF-based reflectance correction to facilitate efficient plant phenotyping."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Watermarking is one of the most important copyright protection tools fordigital media. The most challenging type of watermarking is the imperceptibleone, which embeds identifying information in the data while retaining thelatter's original quality. To fulfill its purpose, watermarks need to withstandvarious distortions whose goal is to damage their integrity. In this study, weinvestigate a novel deep learning-based architecture for embeddingimperceptible watermarks. The key insight guiding our architecture design isthe need to correlate the dimensions of our watermarks with the sizes ofreceptive fields (RF) of modules of our architecture. This adaptation makes ourwatermarks more robust, while also enabling us to generate them in a way thatbetter maintains image quality. Extensive evaluations on a wide variety ofdistortions show that the proposed method is robust against most commondistortions on watermarks including collusive distortion.", "output": "ReMark: Receptive Field based Spatial WaterMark Embedding Optimization using Deep Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D single object tracking plays a crucial role in computer vision. Mainstreammethods mainly rely on point clouds to achieve geometry matching between targettemplate and search area. However, textureless and incomplete point clouds makeit difficult for single-modal trackers to distinguish objects with similarstructures. To overcome the limitations of geometry matching, we propose aMulti-modal Multi-level Fusion Tracker (MMF-Track), which exploits the imagetexture and geometry characteristic of point clouds to track 3D target.Specifically, we first propose a Space Alignment Module (SAM) to align RGBimages with point clouds in 3D space, which is the prerequisite forconstructing inter-modal associations. Then, in feature interaction level, wedesign a Feature Interaction Module (FIM) based on dual-stream structure, whichenhances intra-modal features in parallel and constructs inter-modal semanticassociations. Meanwhile, in order to refine each modal feature, we introduce aCoarse-to-Fine Interaction Module (CFIM) to realize the hierarchical featureinteraction at different scales. Finally, in similarity fusion level, wepropose a Similarity Fusion Module (SFM) to aggregate geometry and textureclues from the target. Experiments show that our method achievesstate-of-the-art performance on KITTI (39% Success and 42% Precision gainsagainst previous multi-modal method) and is also competitive on NuScenes.", "output": "Multi-modal Multi-level Fusion for 3D Single Object Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-view clustering can partition data samples into their categories bylearning a consensus representation in unsupervised way and has received moreand more attention in recent years. However, most existing deep clusteringmethods learn consensus representation or view-specific representations frommultiple views via view-wise aggregation way, where they ignore structurerelationship of all samples. In this paper, we propose a novel multi-viewclustering network to address these problems, called Global and Cross-viewFeature Aggregation for Multi-View Clustering (GCFAggMVC). Specifically, theconsensus data presentation from multiple views is obtained via cross-sampleand cross-view feature aggregation, which fully explores the complementaryofsimilar samples. Moreover, we align the consensus representation and theview-specific representation by the structure-guided contrastive learningmodule, which makes the view-specific representations from different sampleswith high structure relationship similar. The proposed module is a flexiblemulti-view data representation module, which can be also embedded to theincomplete multi-view data clustering task via plugging our module into otherframeworks. Extensive experiments show that the proposed method achievesexcellent performance in both complete multi-view data clustering tasks andincomplete multi-view data clustering tasks.", "output": "GCFAgg: Global and Cross-view Feature Aggregation for Multi-view Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce the Collection Space Navigator (CSN), a browser-basedvisualization tool to explore, research, and curate large collections of visualdigital artifacts that are associated with multidimensional data, such asvector embeddings or tables of metadata. Media objects such as images are oftenencoded as numerical vectors, for e.g. based on metadata or using machinelearning to embed image information. Yet, while such procedures are widespreadfor a range of applications, it remains a challenge to explore, analyze, andunderstand the resulting multidimensional spaces in a more comprehensivemanner. Dimensionality reduction techniques such as t-SNE or UMAP often serveto project high-dimensional data into low dimensional visualizations, yetrequire interpretation themselves as the remaining dimensions are typicallyabstract. Here, the Collection Space Navigator provides a customizableinterface that combines two-dimensional projections with a set of configurablemultidimensional filters. As a result, the user is able to view and investigatecollections, by zooming and scaling, by transforming between projections, byfiltering dimensions via range sliders, and advanced text filters. Insightsthat are gained during the interaction can be fed back into the original datavia ad hoc exports of filtered metadata and projections. This paper comes witha functional showcase demo using a large digitized collection of classicalWestern art. The Collection Space Navigator is open source. Users canreconfigure the interface to fit their own data and research needs, includingprojections and filter controls. The CSN is ready to serve a broad community.", "output": "Collection Space Navigator: An Interactive Visualization Interface for Multidimensional Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a new technique for generating retinal fundus images that haveanatomically accurate vascular structures, using diffusion models. We generateartery/vein masks to create the vascular structure, which we then condition toproduce retinal fundus images. The proposed method can generate high-qualityimages with more realistic vascular structures and can create a diverse rangeof images based on the strengths of the diffusion model. We presentquantitative evaluations that demonstrate the performance improvement using ourmethod for data augmentation on vessel segmentation and artery/veinclassification. We also present Turing test results by clinical experts,showing that our generated images are difficult to distinguish with realimages. We believe that our method can be applied to construct stand-alonedatasets that are irrelevant of patient privacy.", "output": "Generation of Structurally Realistic Retinal Fundus Images with Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous vehicles demand high accuracy and robustness of perceptionalgorithms. To develop efficient and scalable perception algorithms, themaximum information should be extracted from the available sensor data. In thiswork, we present our concept for an end-to-end perception architecture, namedDeepSTEP. The deep learning-based architecture processes raw sensor data fromthe camera, LiDAR, and RaDAR, and combines the extracted data in a deep fusionnetwork. The output of this deep fusion network is a shared feature space,which is used by perception head networks to fulfill several perception tasks,such as object detection or local mapping. DeepSTEP incorporates multiple ideasto advance state of the art: First, combining detection and localization into asingle pipeline allows for efficient processing to reduce computationaloverhead and further improves overall performance. Second, the architectureleverages the temporal domain by using a self-attention mechanism that focuseson the most important features. We believe that our concept of DeepSTEP willadvance the development of end-to-end perception systems. The network will bedeployed on our research vehicle, which will be used as a platform for datacollection, real-world testing, and validation. In conclusion, DeepSTEPrepresents a significant advancement in the field of perception for autonomousvehicles. The architecture's end-to-end design, time-aware attention mechanism,and integration of multiple perception tasks make it a promising solution forreal-world deployment. This research is a work in progress and presents thefirst concept of establishing a novel perception pipeline.", "output": "DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose an approach for cardiac magnetic resonance imaging(MRI), which aims to reconstruct a real-time video of a beating heart fromcontinuous highly under-sampled measurements. This task is challenging sincethe object to be reconstructed (the heart) is continuously changing duringsignal acquisition. To address this challenge, we represent the beating heartwith an implicit neural network and fit the network so that the representationof the heart is consistent with the measurements. The network in the form of amulti-layer perceptron with Fourier-feature inputs acts as an effective signalprior and enables adjusting the regularization strength in both the spatial andtemporal dimensions of the signal. We examine the proposed approach for 2Dfree-breathing cardiac real-time MRI in different operating regimes, i.e., fordifferent image resolutions, slice thicknesses, and acquisition lengths. Ourmethod achieves reconstruction quality on par with or slightly better thanstate-of-the-art untrained convolutional neural networks and superior imagequality compared to a recent method that fits an implicit representationdirectly to Fourier-domain measurements. However, this comes at a highercomputational cost. Our approach does not require any additional patient dataor biosensors including electrocardiography, making it potentially applicablein a wide range of clinical scenarios.", "output": "Implicit Neural Networks with Fourier-Feature Inputs for Free-breathing Cardiac MRI Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human communication is the vocal and non verbal signal to communicate withothers. Human expression is a significant biometric object in picture andrecord databases of surveillance systems. Face appreciation has a serious rolein biometric methods and is good-looking for plentiful applications, includingvisual scrutiny and security. Facial expressions are a form of nonverbalcommunication; recognizing them helps improve the human machine interaction.This paper proposes an idea for face and enlightenment invariant credit offacial expressions by the images. In order on, the person's face can becomputed. Face expression is used in CNN classifier to categorize the acquiredpicture into different emotion categories. It is a deep, feed-forwardartificial neural network. Outcome surpasses human presentation and shows posesalternate performance. Varying lighting conditions can influence the fittingprocess and reduce recognition precision. Results illustrate that dependablefacial appearance credited with changing lighting conditions for separatingreasonable facial terminology display emotions is an efficient representationof clean and assorted moving expressions. This process can also manage theproportions of dissimilar basic affecting expressions of those mixed jointly toproduce sensible emotional facial expressions. Our system contains apre-defined data set, which was residential by a statistics scientist andincludes all pure and varied expressions. On average, a data set has achieved92.4% exact validation of the expressions synthesized by our technique. Thesefacial expressions are compared through the pre-defined data-position insideour system. If it recognizes the person in an abnormal condition, an alert willbe passed to the nearby hospital/doctor seeing that a message.", "output": "Emotion Recognition for Challenged People Facial Appearance in Social using Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In 3D point cloud-based visual self-localization, pole landmarks have a greatpotential as landmarks for accurate and reliable localization due to theirlong-term stability under seasonal and weather changes. In this study, we aimto explore the use of recently developed deep learning models for poleclassification in the context of pole landmark-based self-localization.Specifically, the proposed scheme consists of two main modules: pole mapmatching and pole class matching. In the former module, local pole map isconstructed and its configuration is compared against a precomputed global polemap. An efficient RANSAC map matching is employed to achieve a good tradeoffbetween computational efficiency and accuracy. In the latter pole classmatching module, the local and global poles paired by the RANSAC map-matchingare further compared by means of pole attribute class. To this end, apredefined set of pseudo pole classes is learned via k-means clustering in aself-supervised manner. Experiments using publicly available NCLT datasetshowed that the pole-like landmark classification method has an improved effecton the visual self-localization system compared with the baseline method.", "output": "Detection and Classification of Pole-like Landmarks for Domain-invariant 3D Point Cloud Map Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Disentangling the factors of variation in data is a fundamental concept inmachine learning and has been studied in various ways by different researchers,leading to a multitude of definitions. Despite the numerous empirical studies,more theoretical research is needed to fully understand the defining propertiesof disentanglement and how different definitions relate to each other. Thispaper presents a meta-analysis of existing definitions of disentanglement,using category theory as a unifying and rigorous framework. We propose that theconcepts of the cartesian and monoidal products should serve as the core ofdisentanglement. With these core concepts, we show the similarities and crucialdifferences in dealing with (i) functions, (ii) equivariant maps, (iii)relations, and (iv) stochastic maps. Overall, our meta-analysis deepens ourunderstanding of disentanglement and its various formulations and can helpresearchers navigate different definitions and choose the most appropriate onefor their specific context.", "output": "A Category-theoretical Meta-analysis of Definitions of Disentanglement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most uses of Meta-Learning in visual recognition are very often applied toimage classification, with a relative lack of works in other tasks {such} assegmentation and detection. We propose a generic Meta-Learning framework forfew-shot weakly-supervised segmentation in medical imaging domains. We conducta comparative analysis of meta-learners from distinct paradigms adapted tofew-shot image segmentation in different sparsely annotated radiological tasks.The imaging modalities include 2D chest, mammographic and dental X-rays, aswell as 2D slices of volumetric tomography and resonance images. Ourexperiments consider a total of 9 meta-learners, 4 backbones and multipletarget organ segmentation tasks. We explore small-data scenarios in radiologywith varying weak annotation styles and densities. Our analysis shows thatmetric-based meta-learning approaches achieve better segmentation results intasks with smaller domain shifts in comparison to the meta-training datasets,while some gradient- and fusion-based meta-learners are more generalizable tolarger domain shifts.", "output": "Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the recent past, complex deep neural networks have received huge interestin various document understanding tasks such as document image classificationand document retrieval. As many document types have a distinct visual style,learning only visual features with deep CNNs to classify document images haveencountered the problem of low inter-class discrimination, and high intra-classstructural variations between its categories. In parallel, text-levelunderstanding jointly learned with the corresponding visual properties within agiven document image has considerably improved the classification performancein terms of accuracy. In this paper, we design a self-attention-based fusionmodule that serves as a block in our ensemble trainable network. It allows tosimultaneously learn the discriminant features of image and text modalitiesthroughout the training stage. Besides, we encourage mutual learning bytransferring the positive knowledge between image and text modalities duringthe training stage. This constraint is realized by adding atruncated-Kullback-Leibler divergence loss Tr-KLD-Reg as a new regularizationterm, to the conventional supervised setting. To the best of our knowledge,this is the first time to leverage a mutual learning approach along with aself-attention-based fusion module to perform document image classification.The experimental results illustrate the effectiveness of our approach in termsof accuracy for the single-modal and multi-modal modalities. Thus, the proposedensemble self-attention-based mutual learning model outperforms thestate-of-the-art classification results based on the benchmark RVL-CDIP andTobacco-3482 datasets.", "output": "EAML: Ensemble Self-Attention-based Mutual Learning Network for Document Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection (OD) is crucial to autonomous driving. Unknown objects areone of the reasons that hinder autonomous vehicles from driving beyond theoperational domain. We propose a saliency-based OD algorithm (SalienDet) todetect objects that do not appear in the training sample set. SalienDetutilizes a saliency-based algorithm to enhance image features for objectproposal generation. Then, we design a dataset relabeling approach todifferentiate the unknown objects from all objects to achieve open-worlddetection. We evaluate SalienDet on KITTI, NuScenes, and BDD datasets, and theresult indicates that it outperforms existing algorithms for unknown objectdetection. Additionally, SalienDet can be easily adapted for incrementallearning in open-world detection tasks.", "output": "SalienDet: A Saliency-based Feature Enhancement Algorithm for Object Detection for Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Whole-Slide Imaging allows for the capturing and digitization ofhigh-resolution images of histological specimen. An automated analysis of suchimages using deep learning models is therefore of high demand. The transformerarchitecture has been proposed as a possible candidate for effectivelyleveraging the high-resolution information. Here, the whole-slide image ispartitioned into smaller image patches and feature tokens are extracted fromthese image patches. However, while the conventional transformer allows for asimultaneous processing of a large set of input tokens, the computationaldemand scales quadratically with the number of input tokens and thusquadratically with the number of image patches. To address this problem wepropose a novel cascaded cross-attention network (CCAN) based on thecross-attention mechanism that scales linearly with the number of extractedpatches. Our experiments demonstrate that this architecture is at least on-parwith and even outperforms other attention-based state-of-the-art methods on twopublic datasets: On the use-case of lung cancer (TCGA NSCLC) our model reachesa mean area under the receiver operating characteristic (AUC) of 0.970 $pm$0.008 and on renal cancer (TCGA RCC) reaches a mean AUC of 0.985 $pm$ 0.004.Furthermore, we show that our proposed model is efficient in low-data regimes,making it a promising approach for analyzing whole-slide images inresource-limited settings. To foster research in this direction, we make ourcode publicly available on GitHub: XXX.", "output": "Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computed Tomography (CT) scans provide detailed and accurate information ofinternal structures in the body. They are constructed by sending x-rays throughthe body from different directions and combining this information into athree-dimensional volume. Such volumes can then be used to diagnose a widerange of conditions and allow for volumetric measurements of organs. In thiswork, we tackle the problem of reconstructing CT images from biplanar x-raysonly. X-rays are widely available and even if the CT reconstructed from theseradiographs is not a replacement of a complete CT in the diagnostic setting, itmight serve to spare the patients from radiation where a CT is only acquiredfor rough measurements such as determining organ size. We propose a novelmethod based on the transformer architecture, by framing the underlying task asa language translation problem. Radiographs and CT images are first embeddedinto latent quantized codebook vectors using two different autoencodernetworks. We then train a GPT model, to reconstruct the codebook vectors of theCT image, conditioned on the codebook vectors of the x-rays and show that thisapproach leads to realistic looking images. To encourage further research inthis direction, we make our code publicly available on GitHub: XXX.", "output": "Transformers for CT Reconstruction From Monoplanar and Biplanar Radiographs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monocular 3D human pose and shape estimation is an ill-posed problem sincemultiple 3D solutions can explain a 2D image of a subject. Recent approachespredict a probability distribution over plausible 3D pose and shape parametersconditioned on the image. We show that these approaches exhibit a trade-offbetween three key properties: (i) accuracy - the likelihood of the ground-truth3D solution under the predicted distribution, (ii) sample-input consistency -the extent to which 3D samples from the predicted distribution match thevisible 2D image evidence, and (iii) sample diversity - the range of plausible3D solutions modelled by the predicted distribution. Our method, HuManiFlow,predicts simultaneously accurate, consistent and diverse distributions. We usethe human kinematic tree to factorise full body pose into ancestor-conditionedper-body-part pose distributions in an autoregressive manner. Per-body-partdistributions are implemented using normalising flows that respect the manifoldstructure of SO(3), the Lie group of per-body-part poses. We show thatill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,and employ only probabilistic training losses. Code is available at:", "output": "HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Instance segmentation of point clouds is a crucial task in 3D field withnumerous applications that involve localizing and segmenting objects in ascene. However, achieving satisfactory results requires a large number ofmanual annotations, which is a time-consuming and expensive process. Toalleviate dependency on annotations, we propose a method, called FreePoint, forunderexplored unsupervised class-agnostic instance segmentation on pointclouds. In detail, we represent the point features by combining coordinates,colors, normals, and self-supervised deep features. Based on the pointfeatures, we perform a multicut algorithm to segment point clouds into coarseinstance masks as pseudo labels, which are used to train a point cloud instancesegmentation model. To alleviate the inaccuracy of coarse masks duringtraining, we propose a weakly-supervised training strategy and correspondingloss. Our work can also serve as an unsupervised pre-training pretext forsupervised semantic instance segmentation with limited annotations. Forclass-agnostic instance segmentation on point clouds, FreePoint largely fillsthe gap with its fully-supervised counterpart based on the state-of-the-artinstance segmentation model Mask3D and even surpasses some previousfully-supervised methods. When serving as a pretext task and fine-tuning onS3DIS, FreePoint outperforms training from scratch by 5.8% AP with only 10%mask annotations.", "output": "FreePoint: Unsupervised Point Cloud Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain shift and label scarcity heavily limit deep learning applications tovarious medical image analysis tasks. Unsupervised domain adaptation (UDA)techniques have recently achieved promising cross-modality medical imagesegmentation by transferring knowledge from a label-rich source domain to anunlabeled target domain. However, it is also difficult to collect annotationsfrom the source domain in many clinical applications, rendering most priorworks suboptimal with the label-scarce source domain, particularly for few-shotscenarios, where only a few source labels are accessible. To achieve efficientfew-shot cross-modality segmentation, we propose a noveltransformation-consistent meta-hallucination framework, meta-hallucinator, withthe goal of learning to diversify data distributions and generate usefulexamples for enhancing cross-modality performance. In our framework,hallucination and segmentation models are jointly trained with thegradient-based meta-learning strategy to synthesize examples that lead to goodsegmentation performance on the target domain. To further facilitate datahallucination and cross-domain knowledge transfer, we develop a self-ensemblingmodel with a hallucination-consistent property. Our meta-hallucinator canseamlessly collaborate with the meta-segmenter for learning to hallucinate withmutual benefits from a combined view of meta-learning and self-ensemblinglearning. Extensive studies on MM-WHS 2017 dataset for cross-modality cardiacsegmentation demonstrate that our method performs favorably against variousapproaches by a lot in the few-shot UDA scenario.", "output": "Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have shown promising results on utilizing pre-trainedimage-language models for video question answering. While these image-languagemodels can efficiently bootstrap the representation learning of video-languagemodels, they typically concatenate uniformly sampled video frames as visualinputs without explicit language-aware, temporal modeling. When only a portionof a video input is relevant to the language query, such uniform frame samplingcan often lead to missing important visual cues. Although humans often find avideo moment to focus on and rewind the moment to answer questions, training aquery-aware video moment localizer often requires expensive annotations andhigh computational costs. To address this issue, we propose Self-Chained VideoLocalization-Answering (SeViLA), a novel framework that leverages a singleimage-language model (BLIP-2) to tackle both temporal keyframe localization andQA on videos. SeViLA framework consists of two modules: Localizer and Answerer,where both are parameter-efficiently fine-tuned from BLIP-2. We chain thesemodules for cascaded inference and self-refinement. First, in the forwardchain, the Localizer finds multiple language-aware keyframes in a video, whichthe Answerer uses to predict the answer. Second, in the reverse chain, theAnswerer generates keyframe pseudo-labels to refine the Localizer, alleviatingthe need for expensive video moment localization annotations. SeViLAoutperforms several strong baselines/previous works on five video QA and eventprediction tasks, and achieves the state-of-the-art in both fine-tuning(NExT-QA, STAR) and zero-shot (NExT-QA, STAR, How2QA, VLEP) settings. We show acomprehensive analysis, e.g., the impact of Localizer, comparisons of Localizerwith other temporal localization models, pre-training/self-refinement ofLocalizer, and varying the number of keyframes.", "output": "Self-Chained Image-Language Model for Video Localization and Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - acontrastive image-text pretraining recipe to bridge the gap between image-levelpretraining and open-vocabulary object detection. At the pretraining phase, wepropose to randomly crop and resize regions of positional embeddings instead ofusing the whole image positional embeddings. This better matches the use ofpositional embeddings at region-level in the detection finetuning phase. Inaddition, we replace the common softmax cross entropy loss in contrastivelearning with focal loss to better learn the informative yet difficultexamples. Finally, we leverage recent advances in novel object proposals toimprove open-vocabulary detection finetuning. We evaluate our full model on theLVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the bestexisting approach by +5.8 points in addition to competitive zero-shot transferdetection. Surprisingly, RO-ViT improves the image-level representation as welland achieves the state of the art on 9 out of 12 metrics on COCO and Flickrimage-text retrieval benchmarks, outperforming competitive approaches withlarger models.", "output": "Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For augmented reality (AR), it is important that virtual assets appear to`sit among' real world objects. The virtual element should variously occludeand be occluded by real matter, based on a plausible depth ordering. Thisocclusion should be consistent over time as the viewer's camera moves.Unfortunately, small mistakes in the estimated scene depth can ruin thedownstream occlusion mask, and thereby the AR illusion. Especially in real-timesettings, depths inferred near boundaries or across time can be inconsistent.In this paper, we challenge the need for depth-regression as an intermediatestep.We instead propose an implicit model for depth and use that to predict theocclusion mask directly. The inputs to our network are one or more colorimages, plus the known depths of any virtual geometry. We show how ourocclusion predictions are more accurate and more temporally stable thanpredictions derived from traditional depth-estimation models. We obtainstate-of-the-art occlusion results on the challenging ScanNetv2 dataset andsuperior qualitative results on real scenes.", "output": "Virtual Occlusions Through Implicit Depth."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel approach to leverage prior knowledge encapsulated inpre-trained text-to-image diffusion models for blind super-resolution (SR).Specifically, by employing our time-aware encoder, we can achieve promisingrestoration results without altering the pre-trained synthesis model, therebypreserving the generative prior and minimizing training cost. To remedy theloss of fidelity caused by the inherent stochasticity of diffusion models, weintroduce a controllable feature wrapping module that allows users to balancequality and fidelity by simply adjusting a scalar value during the inferenceprocess. Moreover, we develop a progressive aggregation sampling strategy toovercome the fixed-size constraints of pre-trained diffusion models, enablingadaptation to resolutions of any size. A comprehensive evaluation of our methodusing both synthetic and real-world benchmarks demonstrates its superiorityover current state-of-the-art approaches.", "output": "Exploiting Diffusion Prior for Real-World Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "CLIP, the first foundation model that connects images and text, has enabledmany recent breakthroughs in computer vision. However, its associated trainingcost is prohibitively high, imposing a significant barrier to its widespreadexploration. In this paper, we present a surprising finding that there existsan inverse scaling law for CLIP training, whereby the larger the image/textencoders used, the shorter the sequence length of image/text tokens that can beapplied in training. Moreover, we showcase that the strategy for reducingimage/text token length plays a crucial role in determining the quality of thisscaling law.As a result of this finding, we are able to successfully train CLIP even byusing academic resources. For example, on an A100 eight-GPU server, our CLIPmodels achieve zero-shot top-1 ImageNet accuracies of 63.2% in ~2 days, 67.8%in ~3 days, and 69.3% in ~4 days. By reducing the computation barrierassociated with CLIP, we hope to inspire more research in this field,particularly from academics. Our code is available at", "output": "An Inverse Scaling Law for CLIP Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a sequence-to-sequence vision-language model whose parameters arejointly trained on all tasks (all for one) and fully shared among multipletasks (one for all), resulting in a single model which we named Musketeer. Theintegration of knowledge across heterogeneous tasks is enabled by a novelfeature called Task Explanation Prompt (TEP). TEP reduces interference amongtasks, allowing the model to focus on their shared structure. With a singlemodel, Musketeer achieves results comparable to or better than strong baselinestrained on single tasks, almost uniformly across multiple tasks.", "output": "Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to judge whether a caption correctly describes an image is acritical part of vision-language understanding. However, state-of-the-artmodels often misinterpret the correctness of fine-grained details, leading toerrors in outputs such as hallucinating objects in generated captions or poorcompositional reasoning. In this work, we explore Token-Level Confidence, orTLC, as a simple yet surprisingly effective method to assess captioncorrectness. Specifically, we fine-tune a vision-language model on imagecaptioning, input an image and proposed caption to the model, and aggregateeither algebraic or learned token confidences over words or sequences toestimate image-caption consistency. Compared to sequence-level scores frompretrained models, TLC with algebraic confidence measures achieves a relativeimprovement in accuracy by 10% on verb understanding in SVO-Probes andoutperforms prior state-of-the-art in image and group scores for compositionalreasoning in Winoground by a relative 37% and 9%, respectively. When trainingdata are available, a learned confidence estimator provides further improvedperformance, reducing object hallucination rates in MS COCO Captions by arelative 30% over the original model and setting a new state-of-the-art.", "output": "Simple Token-Level Confidence Improves Caption Correctness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study to generate novel views of indoor scenes given sparse input views.The challenge is to achieve both photorealism and view consistency. We presentSparseGNV: a learning framework that incorporates 3D structures and imagegenerative models to generate novel views with three modules. The first modulebuilds a neural point cloud as underlying geometry, providing contextualinformation and guidance for the target novel view. The second module utilizesa transformer-based network to map the scene context and the guidance into ashared latent space and autoregressively decodes the target view in the form ofdiscrete image tokens. The third module reconstructs the tokens into the imageof the target view. SparseGNV is trained across a large indoor scene dataset tolearn generalizable priors. Once trained, it can efficiently generate novelviews of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNVon both real-world and synthetic indoor scenes and demonstrate that itoutperforms state-of-the-art methods based on either neural radiance fields orconditional image generation.", "output": "SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scaling to arbitrarily large bundle adjustment problems requires data andcompute to be distributed across multiple devices. Centralized methods in priorworks are only able to solve small or medium size problems due to overhead incomputation and communication. In this paper, we present a fully decentralizedmethod that alleviates computation and communication bottlenecks to solvearbitrarily large bundle adjustment problems. We achieve this by reformulatingthe reprojection error and deriving a novel surrogate function that decouplesoptimization variables from different devices. This function makes it possibleto use majorization minimization techniques and reduces bundle adjustment toindependent optimization subproblems that can be solved in parallel. We furtherapply Nesterov's acceleration and adaptive restart to improve convergence whilemaintaining its theoretical guarantees. Despite limited peer-to-peercommunication, our method has provable convergence to first-order criticalpoints under mild conditions. On extensive benchmarks with public datasets, ourmethod converges much faster than decentralized baselines with similar memoryusage and communication load. Compared to centralized baselines using a singledevice, our method, while being decentralized, yields more accurate solutionswith significant speedups of up to 940.7x over Ceres and 175.2x over DeepLM.Code: ", "output": "Decentralization and Acceleration Enables Large-Scale Bundle Adjustment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision transformers have shown great success due to their high modelcapabilities. However, their remarkable performance is accompanied by heavycomputation costs, which makes them unsuitable for real-time applications. Inthis paper, we propose a family of high-speed vision transformers namedEfficientViT. We find that the speed of existing transformer models is commonlybounded by memory inefficient operations, especially the tensor reshaping andelement-wise functions in MHSA. Therefore, we design a new building block witha sandwich layout, i.e., using a single memory-bound MHSA between efficient FFNlayers, which improves memory efficiency while enhancing channel communication.Moreover, we discover that the attention maps share high similarities acrossheads, leading to computational redundancy. To address this, we present acascaded group attention module feeding attention heads with different splitsof the full feature, which not only saves computation cost but also improvesattention diversity. Comprehensive experiments demonstrate EfficientViToutperforms existing efficient models, striking a good trade-off between speedand accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on NvidiaV100 GPU and Intel Xeon CPU, respectively. Compared to the recent efficientmodel MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, whilerunning 5.8x/3.7x faster on the GPU/CPU, and 7.4x faster when converted to ONNXformat. Code and models are available at", "output": "EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is well known that semantic segmentation neural networks (SSNNs) producedense segmentation maps to resolve the objects' boundaries while restrict theprediction on down-sampled grids to alleviate the computational cost. Astriking balance between the accuracy and the training cost of the SSNNs suchas U-Net exists. We propose a spectral analysis to investigate the correlationsamong the resolution of the down sampled grid, the loss function and theaccuracy of the SSNNs. By analyzing the network back-propagation process infrequency domain, we discover that the traditional loss function,cross-entropy, and the key features of CNN are mainly affected by thelow-frequency components of segmentation labels. Our discoveries can be appliedto SSNNs in several ways including (i) determining an efficient low resolutiongrid for resolving the segmentation maps (ii) pruning the networks bytruncating the high frequency decoder features for saving computation costs,and (iii) using block-wise weak annotation for saving the labeling time.Experimental results shown in this paper agree with our spectral analysis forthe networks such as DeepLab V3+ and Deep Aggregation Net (DAN).", "output": "Spectral Analysis for Semantic Segmentation with Applications on Feature Truncation and Weak Annotation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose Mutual Information Maximization KnowledgeDistillation (MIMKD). Our method uses a contrastive objective to simultaneouslyestimate and maximize a lower bound on the mutual information of local andglobal feature representations between a teacher and a student network. Wedemonstrate through extensive experiments that this can be used to improve theperformance of low capacity models by transferring knowledge from moreperformant but computationally expensive models. This can be used to producebetter models that can be run on devices with low computational resources. Ourmethod is flexible, we can distill knowledge from teachers with arbitrarynetwork architectures to arbitrary student networks. Our empirical results showthat MIMKD outperforms competing approaches across a wide range ofstudent-teacher pairs with different capacities, with different architectures,and when student networks are with extremely low capacity. We are able toobtain 74.55% accuracy on CIFAR100 with a ShufflenetV2 from a baseline accuracyof 69.8% by distilling knowledge from ResNet-50. On Imagenet we improve aResNet-18 network from 68.88% to 70.32% accuracy (1.44%+) using a ResNet-34teacher network.", "output": "Estimating and Maximizing Mutual Information for Knowledge Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works indicate that convolutional neural networks (CNN) need largereceptive fields (RF) to compete with visual transformers and their attentionmechanism. In CNNs, RFs can simply be enlarged by increasing the convolutionkernel sizes. Yet the number of trainable parameters, which scalesquadratically with the kernel's size in the 2D case, rapidly becomesprohibitive, and the training is notoriously difficult. This paper presents anew method to increase the RF size without increasing the number of parameters.The dilated convolution (DC) has already been proposed for the same purpose. DCcan be seen as a convolution with a kernel that contains only a few non-zeroelements placed on a regular grid. Here we present a new version of the DC inwhich the spacings between the non-zero elements, or equivalently theirpositions, are no longer fixed but learnable via backpropagation thanks to aninterpolation technique. We call this method \"Dilated Convolution withLearnable Spacings\" (DCLS) and generalize it to the n-dimensional convolutioncase. However, our main focus here will be on the 2D case. We first tried ourapproach on ResNet50: we drop-in replaced the standard convolutions with DCLSones, which increased the accuracy of ImageNet1k classification atiso-parameters, but at the expense of the throughput. Next, we used the recentConvNeXt state-of-the-art convolutional architecture and drop-in replaced thedepthwise convolutions with DCLS ones. This not only increased the accuracy ofImageNet1k classification but also of typical downstream and robustness tasks,again at iso-parameters but this time with negligible cost on throughput, asConvNeXt uses separable convolutions. Conversely, classic DC led to poorperformance with both ResNet50 and ConvNeXt. The code of the method isavailable at:", "output": "Dilated convolution with learnable spacings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose CLIP-Lite, an information efficient method for visualrepresentation learning by feature alignment with textual annotations. Comparedto the previously proposed CLIP model, CLIP-Lite requires only one negativeimage-text sample pair for every positive image-text sample during theoptimization of its contrastive learning objective. We accomplish this bytaking advantage of an information efficient lower-bound to maximize the mutualinformation between the two input modalities. This allows CLIP-Lite to betrained with significantly reduced amounts of data and batch sizes whileobtaining better performance than CLIP at the same scale. We evaluate CLIP-Liteby pretraining on the COCO-Captions dataset and testing transfer learning toother datasets. CLIP-Lite obtains a +14.0% mAP absolute gain in performance onPascal VOC classification, and a +22.1% top-1 accuracy gain on ImageNet, whilebeing comparable or superior to other, more complex, text-supervised models.CLIP-Lite is also superior to CLIP on image and text retrieval, zero-shotclassification, and visual grounding. Finally, we show that CLIP-Lite canleverage language semantics to encourage bias-free visual representations thatcan be used in downstream tasks. Implementation:", "output": "CLIP-Lite: Information Efficient Visual Representation Learning with Language Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Facial action units (AUs) play an indispensable role in human emotionanalysis. We observe that although AU-based high-level emotion analysis isurgently needed by real-world applications, frame-level AU results provided byprevious works cannot be directly used for such analysis. Moreover, as AUs aredynamic processes, the utilization of global temporal information is importantbut has been gravely ignored in the literature. To this end, we proposeEventFormer for AU event detection, which is the first work directly detectingAU events from a video sequence by viewing AU event detection as a multipleclass-specific sets prediction problem. Extensive experiments conducted on acommonly used AU benchmark dataset, BP4D, show the superiority of EventFormerunder suitable metrics.", "output": "EventFormer: AU Event Transformer for Facial Action Unit Event Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal learning from document data has achieved great success lately asit allows to pre-train semantically meaningful features as a prior into alearnable downstream task. In this paper, we approach the documentclassification problem by learning cross-modal representations through languageand vision cues, considering intra- and inter-modality relationships. Insteadof merging features from different modalities into a joint representationspace, the proposed method exploits high-level interactions and learns relevantsemantic information from effective attention flows within and acrossmodalities. The proposed learning objective is devised between intra- andinter-modality alignment tasks, where the similarity distribution per task iscomputed by contracting positive sample pairs while simultaneously contrastingnegative ones in the joint representation space}. Extensive experiments onpublic document classification datasets demonstrate the effectiveness and thegenerality of our model on low-scale and large-scale datasets.", "output": "VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Not everybody can be equipped with professional photography skills andsufficient shooting time, and there can be some tilts in the captured imagesoccasionally. In this paper, we propose a new and practical task, namedRotation Correction, to automatically correct the tilt with high contentfidelity in the condition that the rotated angle is unknown. This task can beeasily integrated into image editing applications, allowing users to correctthe rotated images without any manual operations. To this end, we leverage aneural network to predict the optical flows that can warp the tilted images tobe perceptually horizontal. Nevertheless, the pixel-wise optical flowestimation from a single image is severely unstable, especially in large-angletilted images. To enhance its robustness, we propose a simple but effectiveprediction strategy to form a robust elastic warp. Particularly, we firstregress the mesh deformation that can be transformed into robust initialoptical flows. Then we estimate residual optical flows to facilitate ournetwork the flexibility of pixel-wise deformation, further correcting thedetails of the tilted images. To establish an evaluation benchmark and trainthe learning framework, a comprehensive rotation correction dataset ispresented with a large diversity in scenes and rotated angles. Extensiveexperiments demonstrate that even in the absence of the angle prior, ouralgorithm can outperform other state-of-the-art solutions requiring this prior.The code and dataset are available at", "output": "Deep Rotation Correction without Angle Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce PointConvFormer, a novel building block for point cloud baseddeep network architectures. Inspired by generalization theory, PointConvFormercombines ideas from point convolution, where filter weights are only based onrelative position, and Transformers which utilize feature-based attention. InPointConvFormer, attention computed from feature difference between points inthe neighborhood is used to modify the convolutional weights at each point.Hence, we preserved the invariances from point convolution, whereas attentionhelps to select relevant points in the neighborhood for convolution.PointConvFormer is suitable for multiple tasks that require details at thepoint level, such as segmentation and scene flow estimation tasks. Weexperiment on both tasks with multiple datasets including ScanNet,SemanticKitti, FlyingThings3D and KITTI. Our results show that PointConvFormeroffers a better accuracy-speed tradeoff than classic convolutions, regulartransformers, and voxelized sparse convolution approaches. Visualizations showthat PointConvFormer performs similarly to convolution on flat areas, whereasthe neighborhood selection effect is stronger on object boundaries, showingthat it has got the best of both worlds.", "output": "PointConvFormer: Revenge of the Point-based Convolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatically discovering failures in vision models under real-world settingsremains an open challenge. This work demonstrates how off-the-shelf,large-scale, image-to-text and text-to-image models, trained on vast amounts ofdata, can be leveraged to automatically find such failures. In essence, aconditional text-to-image generative model is used to generate large amounts ofsynthetic, yet realistic, inputs given a ground-truth label. Misclassifiedinputs are clustered and a captioning model is used to describe each cluster.Each cluster's description is used in turn to generate more inputs and assesswhether specific clusters induce more failures than expected. We use thispipeline to demonstrate that we can effectively interrogate classifiers trainedon ImageNet to find specific failure cases and discover spurious correlations.We also show that we can scale the approach to generate adversarial datasetstargeting specific classifier architectures. This work serves as aproof-of-concept demonstrating the utility of large-scale generative models toautomatically discover bugs in vision models in an open-ended manner. We alsodescribe a number of limitations and pitfalls related to this approach.", "output": "Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fully supervised skeleton-based action recognition has achieved greatprogress with the blooming of deep learning techniques. However, these methodsrequire sufficient labeled data which is not easy to obtain. In contrast,self-supervised skeleton-based action recognition has attracted more attention.With utilizing the unlabeled data, more generalizable features can be learnedto alleviate the overfitting problem and reduce the demand of massive labeledtraining data. Inspired by the MAE, we propose a spatial-temporal maskedautoencoder framework for self-supervised 3D skeleton-based action recognition(SkeletonMAE). Following MAE's masking and reconstruction pipeline, we utilizea skeleton-based encoder-decoder transformer architecture to reconstruct themasked skeleton sequences. A novel masking strategy, named Spatial-TemporalMasking, is introduced in terms of both joint-level and frame-level for theskeleton sequence. This pre-training strategy makes the encoder outputgeneralizable skeleton features with spatial and temporal dependencies. Giventhe unmasked skeleton sequence, the encoder is fine-tuned for the actionrecognition task. Extensive experiments show that our SkeletonMAE achievesremarkable performance and outperforms the state-of-the-art methods on both NTURGB+D and NTU RGB+D 120 datasets.", "output": "SkeletonMAE: Spatial-Temporal Masked Autoencoders for Self-supervised Skeleton Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video conferencing systems suffer from poor user experience when networkconditions deteriorate because current video codecs simply cannot operate atextremely low bitrates. Recently, several neural alternatives have beenproposed that reconstruct talking head videos at very low bitrates using sparserepresentations of each frame such as facial landmark information. However,these approaches produce poor reconstructions in scenarios with major movementor occlusions over the course of a call, and do not scale to higherresolutions. We design Gemino, a new neural compression system for videoconferencing based on a novel high-frequency-conditional super-resolutionpipeline. Gemino upsamples a very low-resolution version of each target framewhile enhancing high-frequency details (e.g., skin texture, hair, etc.) basedon information extracted from a single high-resolution reference image. We usea multi-scale architecture that runs different components of the model atdifferent resolutions, allowing it to scale to resolutions comparable to 720p,and we personalize the model to learn specific details of each person,achieving much better fidelity at low bitrates. We implement Gemino atopaiortc, an open-source Python implementation of WebRTC, and show that itoperates on 1024x1024 videos in real-time on a Titan X GPU, and achieves 2.2-5xlower bitrate than traditional video codecs for the same perceptual quality.", "output": "Gemino: Practical and Robust Neural Compression for Video Conferencing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Field (NeRF), a new novel view synthesis with implicit scenerepresentation has taken the field of Computer Vision by storm. As a novel viewsynthesis and 3D reconstruction method, NeRF models find applications inrobotics, urban mapping, autonomous navigation, virtual reality/augmentedreality, and more. Since the original paper by Mildenhall et al., more than 250preprints were published, with more than 100 eventually being accepted in tierone Computer Vision Conferences. Given NeRF popularity and the current interestin this research area, we believe it necessary to compile a comprehensivesurvey of NeRF papers from the past two years, which we organized into botharchitecture, and application based taxonomies. We also provide an introductionto the theory of NeRF based novel view synthesis, and a benchmark comparison ofthe performance and speed of key NeRF models. By creating this survey, we hopeto introduce new researchers to NeRF, provide a helpful reference forinfluential works in this field, as well as motivate future research directionswith our discussion section.", "output": "NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-task learning based video anomaly detection methods combine multipleproxy tasks in different branches to detect video anomalies in differentsituations. Most existing methods either do not combine complementary tasks toeffectively cover all motion patterns, or the class of the objects is notexplicitly considered. To address the aforementioned shortcomings, we propose anovel multi-task learning based method that combines complementary proxy tasksto better consider the motion and appearance features. We combine the semanticsegmentation and future frame prediction tasks in a single branch to learn theobject class and consistent motion patterns, and to detect respective anomaliessimultaneously. In the second branch, we added several attention mechanisms todetect motion anomalies with attention to object parts, the direction ofmotion, and the distance of the objects from the camera. Our qualitativeresults show that the proposed method considers the object class effectivelyand learns motion with attention to the aforementioned important factors whichresults in a precise motion modeling and a better motion anomaly detection.Additionally, quantitative results show the superiority of our method comparedwith state-of-the-art methods.", "output": "Multi-Task Learning based Video Anomaly Detection with Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work concerns the development of deep networks that are certifiablyrobust to adversarial attacks. Joint robust classification-detection wasrecently introduced as a certified defense mechanism, where adversarialexamples are either correctly classified or assigned to the \"abstain\" class. Inthis work, we show that such a provable framework can benefit by extension tonetworks with multiple explicit abstain classes, where the adversarial examplesare adaptively assigned to those. We show that naively adding multiple abstainclasses can lead to \"model degeneracy\", then we propose a regularizationapproach and a training method to counter this degeneracy by promoting full useof the multiple abstain classes. Our experiments demonstrate that the proposedapproach consistently achieves favorable standard vs. robust verified accuracytradeoffs, outperforming state-of-the-art algorithms for various choices ofnumber of abstain classes.", "output": "Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation models classifying hyperspectral images (HSI) arevulnerable to adversarial examples. Traditional approaches to adversarialrobustness focus on training or retraining a single network on attacked data,however, in the presence of multiple attacks these approaches decrease inperformance compared to networks trained individually on each attack. To combatthis issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net)which focuses on attack type detection and adversarial robustness under aunified model to preserve per data-type weight optimally while robustifiyingthe overall network. In the proposed method, a discriminator network is used toseparate data by attack type into their specific attack-expert ensemblenetwork.", "output": "Improving Hyperspectral Adversarial Robustness Under Multiple Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an efficient text-to-video generation framework based on latentdiffusion models, termed MagicVideo. MagicVideo can generate smooth video clipsthat are concordant with the given text descriptions. Due to a novel andefficient 3D U-Net design and modeling video distributions in a low-dimensionalspace, MagicVideo can synthesize video clips with 256x256 spatial resolution ona single GPU card, which takes around 64x fewer computations than the VideoDiffusion Models (VDM) in terms of FLOPs. In specific, unlike existing worksthat directly train video models in the RGB space, we use a pre-trained VAE tomap video clips into a low-dimensional latent space and learn the distributionof videos' latent codes via a diffusion model. Besides, we introduce two newdesigns to adapt the U-Net denoiser trained on image tasks to video data: aframe-wise lightweight adaptor for the image-to-video distribution adjustmentand a directed temporal attention module to capture temporal dependenciesacross frames. Thus, we can exploit the informative weights of convolutionoperators from a text-to-image model for accelerating video training. Toameliorate the pixel dithering in the generated videos, we also propose a novelVideoVAE auto-encoder for better RGB reconstruction. We conduct extensiveexperiments and demonstrate that MagicVideo can generate high-quality videoclips with either realistic or imaginary content. Refer tourl{ for more examples.", "output": "MagicVideo: Efficient Video Generation With Latent Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The quality of training datasets for deep neural networks is a key factorcontributing to the accuracy of resulting models. This effect is amplified indifficult tasks such as object detection. Dealing with errors in datasets isoften limited to accepting that some fraction of examples is incorrect,estimating their confidence and assigning appropriate weights or ignoringuncertain ones during training. In this work, we propose a different approach.We introduce the Confident Learning for Object Detection (CLOD) algorithm forassessing the quality of each label in object detection datasets, identifyingmissing, spurious, mislabeled and mislocated bounding boxes and suggestingcorrections. By focusing on finding incorrect examples in the trainingdatasets, we can eliminate them at the root. Suspicious bounding boxes can bereviewed in order to improve the quality of the dataset, leading to bettermodels without further complicating their already complex architectures. Theproposed method is able to point out 99% of artificially disturbed boundingboxes with a false positive rate below 0.3. We see this method as a promisingpath to correcting popular object detection datasets.", "output": "Combating noisy labels in object detection datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Taking inspiration from recent developments in visual generative tasks usingdiffusion models, we propose a method for end-to-end speech-driven videoediting using a denoising diffusion model. Given a video of a talking person,and a separate auditory speech recording, the lip and jaw motions arere-synchronized without relying on intermediate structural representations suchas facial landmarks or a 3D face model. We show this is possible byconditioning a denoising diffusion model on audio mel spectral features togenerate synchronised facial motion. Proof of concept results are demonstratedon both single-speaker and multi-speaker video editing, providing a baselinemodel on the CREMA-D audiovisual data set. To the best of our knowledge, thisis the first work to demonstrate and validate the feasibility of applyingend-to-end denoising diffusion models to the task of audio-driven videoediting.", "output": "Speech Driven Video Editing via an Audio-Conditioned Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce here a predictive coding based model that aims to generateaccurate and sharp future frames. Inspired by the predictive coding hypothesisand related works, the total model is updated through a combination ofbottom-up and top-down information flows, which can enhance the interactionbetween different network levels. Most importantly, We propose and improveseveral artifacts to ensure that the neural networks generate clear and naturalframes. Different inputs are no longer simply concatenated or added, they arecalculated in a modulated manner to avoid being roughly fused. The downsamplingand upsampling modules have been redesigned to ensure that the network can moreeasily construct images from Fourier features of low-frequency inputs.Additionally, the training strategies are also explored and improved togenerate believable results and alleviate inconsistency between the inputpredicted frames and ground truth. Our proposals achieve results that betterbalance pixel accuracy and visualization effect.", "output": "Anti-aliasing Predictive Coding Network for Future Video Frame Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel neurosymbolic framework called NeSyFOLD to extract logicrules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD'slearning pipeline is as follows: (i) We first pre-train a CNN on the inputimage dataset and extract activations of the last layer kernels as binaryvalues; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithmto generate a logic program that can classify an image -- represented as avector of binary activations corresponding to each kernel -- while producing alogical explanation. The rules generated by the FOLD-SE-M algorithm have kernelnumbers as predicates. We have devised a novel algorithm for automaticallymapping the CNN kernels to semantic concepts in the images. This mapping isused to replace predicate names (kernel numbers) in the rule-set withcorresponding semantic concept labels. The resulting rule-set is interpretable,and can be intuitively understood by humans. We compare our NeSyFOLD frameworkwith the ERIC system that uses a decision-tree like algorithm to obtain therules. Our framework has the following advantages over ERIC: (i) In most cases,NeSyFOLD generates smaller rule-sets without compromising on the accuracy andfidelity; (ii) NeSyFOLD generates the mapping of filter numbers to semanticlabels automatically.", "output": "NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While manipulating rigid objects is an extensively explored research topic,deformable linear object (DLO) manipulation seems significantly underdeveloped.A potential reason for this is the inherent difficulty in describing andobserving the state of the DLO as its geometry changes during manipulation.This paper proposes an algorithm for fast-tracking the shape of a DLO based onthe masked image. Having no prior knowledge about the tracked object, theproposed method finds a reliable representation of the shape of the trackedobject within tens of milliseconds. This algorithm's main idea is to firstskeletonize the DLO mask image, walk through the parts of the DLO skeleton,arrange the segments into an ordered path, and finally fit a B-spline into it.Experiments show that our solution outperforms the State-of-the-Art approachesin DLO's shape reconstruction accuracy and algorithm running time and canhandle challenging scenarios such as severe occlusions, self-intersections, andmultiple DLOs in a single image.", "output": "DLOFTBs -- Fast Tracking of Deformable Linear Objects with B-splines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available at and dataset will be releasedupon paper acceptance.", "output": "Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Fields (NeRF) have been proposed for photorealistic novelview rendering. However, it requires many different views of one scene fortraining. Moreover, it has poor generalizations to new scenes and requiresretraining or fine-tuning on each scene. In this paper, we develop a new NeRFmodel for novel view synthesis using only a single image as input. We proposeto combine the (coarse) planar rendering and the (fine) volume rendering toachieve higher rendering quality and better generalizations. We also design adepth teacher net that predicts dense pseudo depth maps to supervise the jointrendering mechanism and boost the learning of consistent 3D geometry. Weevaluate our method on three challenging datasets. It outperformsstate-of-the-art single-view NeRFs by achieving 5$sim$20% improvements inPSNR and reducing 20$sim$50% of the errors in the depth rendering. It alsoshows excellent generalization abilities to unseen data without the need tofine-tune on each new scene.", "output": "Single-view Neural Radiance Fields with Depth Teacher."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the challenges of studying common neurological disorders is diseaseheterogeneity including differences in causes, neuroimaging characteristics,comorbidities, or genetic variation. Normative modelling has become a popularmethod for studying such cohorts where the 'normal' behaviour of aphysiological system is modelled and can be used at subject level to detectdeviations relating to disease pathology. For many heterogeneous diseases, weexpect to observe abnormalities across a range of neuroimaging and biologicalvariables. However, thus far, normative models have largely been developed forstudying a single imaging modality. We aim to develop a multi-modal normativemodelling framework where abnormality is aggregated across variables ofmultiple modalities and is better able to detect deviations than uni-modalbaselines. We propose two multi-modal VAE normative models to detect subjectlevel deviations across T1 and DTI data. Our proposed models were better ableto detect diseased individuals, capture disease severity, and correlate withpatient cognition than baseline approaches. We also propose a multivariatelatent deviation metric, measuring deviations from the joint latent space,which outperformed feature-based metrics.", "output": "Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Title: Comparison between layer-to-layer network training and conventionalnetwork training using Deep Convolutional Neural NetworksAbstract: Convolutional neural networks (CNNs) are widely used in variousapplications due to their effectiveness in extracting features from data.However, the performance of a CNN heavily depends on its architecture andtraining process. In this study, we propose a layer-to-layer training methodand compare its performance with the conventional training method.In the layer-to-layer training approach, we treat a portion of the earlylayers as a student network and the later layers as a teacher network. Duringeach training step, we incrementally train the student network to learn fromthe output of the teacher network, and vice versa. We evaluate this approach onVGG16, ResNext, and DenseNet networks without pre-trained ImageNet weights anda regular CNN model.Our experiments show that the layer-to-layer training method outperforms theconventional training method for both models. Specifically, we achieve higheraccuracy on the test set for the VGG16, ResNext, and DeseNet networks and theCNN model using layer-to-layer training compared to the conventional trainingmethod.Overall, our study highlights the importance of layer-wise training in CNNsand suggests that layer-to-layer training can be a promising approach forimproving the accuracy of CNNs.", "output": "Comparison between layer-to-layer network training and conventional network training using Deep Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Learning based diagnostics systems can provide accurate and robustquantitative analysis in digital pathology. These algorithms require largeamounts of annotated training data which is impractical in pathology due to thehigh resolution of histopathological images. Hence, self-supervised methodshave been proposed to learn features using ad-hoc pretext tasks. Theself-supervised training process is time consuming and often leads to subparfeature representation due to a lack of constrain on the learnt feature space,particularly prominent under data imbalance. In this work, we propose toactively sample the training set using a handful of labels and a small proxynetwork, decreasing sample requirement by 93% and training time by 62%.", "output": "Data Efficient Contrastive Learning in Histopathology using Active Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision is a popular and effective sensor for robotics from which we canderive rich information about the environment: the geometry and semantics ofthe scene, as well as the age, gender, identity, activity and even emotionalstate of humans within that scene. This raises important questions about thereach, lifespan, and potential misuse of this information. This paper is a callto action to consider privacy in the context of robotic vision. We propose aspecific form privacy preservation in which no images are captured or could bereconstructed by an attacker even with full remote access. We present a set ofprinciples by which such systems can be designed, and through a case study inlocalisation demonstrate in simulation a specific implementation that deliversan important robotic capability in an inherently privacy-preserving manner.This is a first step, and we hope to inspire future works that expand the rangeof applications open to sighted robotic systems.", "output": "The Need for Inherently Privacy-Preserving Vision in Trustworthy Autonomous Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) have been shown to be effective in various visiontasks. However, resizing them to a mobile-friendly size leads to significantperformance degradation. Therefore, developing lightweight vision transformershas become a crucial area of research. This paper introduces CloFormer, alightweight vision transformer that leverages context-aware local enhancement.CloFormer explores the relationship between globally shared weights often usedin vanilla convolutional operators and token-specific context-aware weightsappearing in attention, then proposes an effective and straightforward moduleto capture high-frequency local information. In CloFormer, we introduceAttnConv, a convolution operator in attention's style. The proposed AttnConvuses shared weights to aggregate local information and deploys carefullydesigned context-aware weights to enhance local features. The combination ofthe AttnConv and vanilla attention which uses pooling to reduce FLOPs inCloFormer enables the model to perceive high-frequency and low-frequencyinformation. Extensive experiments were conducted in image classification,object detection, and semantic segmentation, demonstrating the superiority ofCloFormer.", "output": "Rethinking Local Perception in Lightweight Vision Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A biometric recognition system can operate in two distinct modes,identification or verification. In the first mode, the system recognizes anindividual by searching the enrolled templates of all the users for a match. Inthe second mode, the system validates a user's identity claim by comparing thefresh provided template with the enrolled template. The biometrictransformation schemes usually produce binary templates that are better handledby cryptographic schemes, and the comparison is based on a distance that leaksinformation about the similarities between two biometric templates. Both theexperimentally determined false match rate and false non-match rate throughrecognition threshold adjustment define the recognition accuracy, and hence thesecurity of the system. To the best of our knowledge, few works provide aformal treatment of the security under minimum leakage of information, i.e.,the binary outcome of a comparison with a threshold. In this paper, we rely onprobabilistic modelling to quantify the security strength of binary templates.We investigate the influence of template size, database size and threshold onthe probability of having a near-collision. We highlight several untargetedattacks on biometric systems considering naive and adaptive adversaries.Interestingly, these attacks can be launched both online and offline and, bothin the identification mode and in the verification mode. We discuss the choiceof parameters through the generic presented attacks.", "output": "Untargeted Near-collision Attacks in Biometric Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep machine learning models including Convolutional Neural Networks (CNN)have been successful in the detection of Mild Cognitive Impairment (MCI) usingmedical images, questionnaires, and videos. This paper proposes a novelMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model todistinguish MCI from those with normal cognition by analyzing facial features.The data comes from the I-CONECT, a behavioral intervention trial aimed atimproving cognitive function by providing frequent video chats. MC-ViViTextracts spatiotemporal features of videos in one branch and augmentsrepresentations by the MC module. The I-CONECT dataset is challenging as thedataset is imbalanced containing Hard-Easy and Positive-Negative samples, whichimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easyand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORREloss to address the imbalanced problem. Our experimental results on theI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with ahigh accuracy of 90.63% accuracy on some of the interview videos.", "output": "MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Breast cancer has reached the highest incidence rate worldwide among allmalignancies since 2020. Breast imaging plays a significant role in earlydiagnosis and intervention to improve the outcome of breast cancer patients. Inthe past decade, deep learning has shown remarkable progress in breast cancerimaging analysis, holding great promise in interpreting the rich informationand complex context of breast imaging modalities. Considering the rapidimprovement in the deep learning technology and the increasing severity ofbreast cancer, it is critical to summarize past progress and identify futurechallenges to be addressed. In this paper, we provide an extensive survey ofdeep learning-based breast cancer imaging research, covering studies onmammogram, ultrasound, magnetic resonance imaging, and digital pathology imagesover the past decade. The major deep learning methods, publicly availabledatasets, and applications on imaging-based screening, diagnosis, treatmentresponse prediction, and prognosis are described in detail. Drawn from thefindings of this survey, we present a comprehensive discussion of thechallenges and potential avenues for future research in deep learning-basedbreast cancer imaging.", "output": "Deep Learning in Breast Cancer Imaging: A Decade of Progress and Future Directions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Segment Anything Model (SAM) has recently gained popularity in the fieldof image segmentation. Thanks to its impressive capabilities in all-roundsegmentation tasks and its prompt-based interface, SAM has sparked intensivediscussion within the community. It is even said by many prestigious expertsthat image segmentation task has been \"finished\" by SAM. However, medical imagesegmentation, although an important branch of the image segmentation family,seems not to be included in the scope of Segmenting \"Anything\". Many individualexperiments and recent studies have shown that SAM performs subpar in medicalimage segmentation. A natural question is how to find the missing piece of thepuzzle to extend the strong segmentation capability of SAM to medical imagesegmentation. In this paper, instead of fine-tuning the SAM model, we proposeMed SAM Adapter, which integrates the medical specific domain knowledge to thesegmentation model, by a simple yet effective adaptation technique. Althoughthis work is still one of a few to transfer the popular NLP technique Adapterto computer vision cases, this simple implementation shows surprisingly goodperformance on medical image segmentation. A medical image adapted SAM, whichwe have dubbed Medical SAM Adapter (MSA), shows superior performance on 19medical image segmentation tasks with various image modalities including CT,MRI, ultrasound image, fundus image, and dermoscopic images. MSA outperforms awide range of state-of-the-art (SOTA) medical image segmentation methods, suchas nnUNet, TransUNet, UNetr, MedSegDiff, and also outperforms the fullyfine-turned MedSAM with a considerable performance gap. Code will be releasedat: ", "output": "Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Approaches for appraising feature importance approximations, alternativelyreferred to as attribution methods, have been established across an extensivearray of contexts. The development of resilient techniques for performancebenchmarking constitutes a critical concern in the sphere of explainable deeplearning. This study scrutinizes the dependability of the RemOve-And-Retrain(ROAR) procedure, which is prevalently employed for gauging the performance offeature importance estimates. The insights gleaned from our theoreticalfoundation and empirical investigations reveal that attributions containinglesser information about the decision function may yield superior results inROAR benchmarks, contradicting the original intent of ROAR. This occurrence issimilarly observed in the recently introduced variant RemOve-And-Debias (ROAD),and we posit a persistent pattern of blurriness bias in ROAR attributionmetrics. Our findings serve as a warning against indiscriminate use on ROARmetrics. The code is available as open source.", "output": "On Pitfalls of $\\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, remarkable results have been achieved in self-supervisedaction recognition using skeleton sequences with contrastive learning. It hasbeen observed that the semantic distinction of human action features is oftenrepresented by local body parts, such as legs or hands, which are advantageousfor skeleton-based action recognition. This paper proposes an attention-basedcontrastive learning framework for skeleton representation learning, calledSkeAttnCLR, which integrates local similarity and global features forskeleton-based action representations. To achieve this, a multi-head attentionmask module is employed to learn the soft attention mask features from theskeletons, suppressing non-salient local features while accentuating localsalient features, thereby bringing similar local features closer in the featurespace. Additionally, ample contrastive pairs are generated by expandingcontrastive pairs based on salient and non-salient features with globalfeatures, which guide the network to learn the semantic representations of theentire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLRlearns local features under different data augmentation views. The experimentresults demonstrate that the inclusion of local feature similaritysignificantly enhances skeleton-based action representation. Our proposedSkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, andPKU-MMD datasets.", "output": "Part Aware Contrastive Learning for Self-Supervised Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image captioning, a.k.a. \"image-to-text,\" which generates descriptive textfrom given images, has been rapidly developing throughout the era of deeplearning. To what extent is the information in the original image preserved inthe descriptive text generated by an image captioner? To answer that question,we have performed experiments involving the classification of images fromdescriptive text alone, without referring to the images at all, and comparedresults with those from standard image-based classifiers. We have evaluateseveral image captioning models with respect to a disaster image classificationtask, CrisisNLP, and show that descriptive text classifiers can sometimesachieve higher accuracy than standard image-based classifiers. Further, we showthat fusing an image-based classifier with a descriptive text classifier canprovide improvement in accuracy.", "output": "Image Captioners Sometimes Tell More Than Images They See."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The increasing popularity of deep learning (DL) models and the advantages ofcomputing, including low latency and bandwidth savings on smartphones, have ledto the emergence of intelligent mobile applications, also known as DL apps, inrecent years. However, this technological development has also given rise toseveral security concerns, including adversarial examples, model stealing, anddata poisoning issues. Existing works on attacks and countermeasures foron-device DL models have primarily focused on the models themselves. However,scant attention has been paid to the impact of data processing disturbance onthe model inference. This knowledge disparity highlights the need foradditional research to fully comprehend and address security issues related todata processing for on-device models. In this paper, we introduce a dataprocessing-based attacks against real-world DL apps. In particular, our attackcould influence the performance and latency of the model without affecting theoperation of a DL app. To demonstrate the effectiveness of our attack, we carryout an empirical study on 517 real-world DL apps collected from Google Play.Among 320 apps utilizing MLkit, we find that 81.56% of them can besuccessfully attacked.The results emphasize the importance of DL app developers being aware of andtaking actions to secure on-device models from the perspective of dataprocessing.", "output": "Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generally, image-to-image translation (i2i) methods aim at learning mappingsacross domains with the assumption that the images used for translation sharecontent (e.g., pose) but have their own domain-specific information (a.k.a.style). Conditioned on a target image, such methods extract the target styleand combine it with the source image content, keeping coherence between thedomains. In our proposal, we depart from this traditional view and insteadconsider the scenario where the target domain is represented by a verylow-resolution (LR) image, proposing a domain-agnostic i2i method forfine-grained problems, where the domains are related. More specifically, ourdomain-agnostic approach aims at generating an image that combines visualfeatures from the source image with low-frequency information (e.g. pose,color) of the LR target image. To do so, we present a novel approach thatrelies on training the generative model to produce images that both sharedistinctive information of the associated source image and correctly match theLR target image when downscaled. We validate our method on the CelebA-HQ andAFHQ datasets by demonstrating improvements in terms of visual quality.Qualitative and quantitative results show that when dealing with intra-domainimage translation, our method generates realistic samples compared tostate-of-the-art methods such as StarGAN v2. Ablation studies also reveal thatour method is robust to changes in color, it can be applied toout-of-distribution images, and it allows for manual control over the finalresults.", "output": "Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This technical report presents our Restormer-Plus approach, which wassubmitted to the GT-RAIN Challenge (CVPR 2023 UG$^2$+ Track 3). Detailsregarding the challenge are available at<a href=\" http URL</a> Our Restormer-Plus outperformedall other submitted solutions in terms of peak signal-to-noise ratio (PSNR). Itconsists mainly of four modules: the single image de-raining module, the medianfiltering module, the weighted averaging module, and the post-processingmodule. We named the single-image de-raining module Restormer-X, which is builton Restormer and performed on each rainy image. The median filtering module isemployed as a median operator for the 300 rainy images associated with eachscene. The weighted averaging module combines the median filtering results withthat of Restormer-X to alleviate overfitting if we only use Restormer-X.Finally, the post-processing module is used to improve the brightnessrestoration. Together, these modules render Restormer-Plus to be onestate-of-the-art solution to the GT-RAIN Challenge. Our code is available at", "output": "Restormer-Plus for Real World Image Deraining: One State-of-the-Art Solution to the GT-RAIN Challenge (CVPR 2023 UG$^2$+ Track 3)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an interactive visual framework named InternGPT, or iGPT forshort. The framework integrates chatbots that have planning and reasoningcapabilities, such as ChatGPT, with non-verbal instructions like pointingmovements that enable users to directly manipulate images or videos on thescreen. Pointing (including gestures, cursors, etc.) movements can provide moreflexibility and precision in performing vision-centric tasks that requirefine-grained control, editing, and generation of visual content. The nameInternGPT stands for textbf{inter}action, textbf{n}onverbal, andtextbf{chat}bots. Different from existing interactive systems that rely onpure language, by incorporating pointing instructions, the proposed iGPTsignificantly improves the efficiency of communication between users andchatbots, as well as the accuracy of chatbots in vision-centric tasks,especially in complicated visual scenarios where the number of objects isgreater than 2. Additionally, in iGPT, an auxiliary control mechanism is usedto improve the control capability of LLM, and a large vision-language modeltermed Husky is fine-tuned for high-quality multi-modal dialogue (impressingChatGPT-3.5-turbo with 93.89% GPT-4 Quality). We hope this work can spark newideas and directions for future interactive visual systems. Welcome to watchthe code at ", "output": "InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a feedback mechanism to 'break bad habits' using thePavlok device. Pavlok utilises beeps, vibration and shocks as a mode ofaversion technique to help individuals with behaviour modification. While thedevice can be useful in certain periodic daily life situations, like alarms andexercise notifications, the device relies on manual operations that limit itsusage. To this end, we design a user interface to generate an automaticfeedback mechanism that integrates Pavlok and a deep learning based model todetect certain behaviours via an integrated user interface i.e. mobile ordesktop application. Our proposed solution is implemented and verified in thecontext of snoring, which first detects audio from the environment following aprediction of whether the audio content is a snore or not. Based on theprediction of the deep learning model, we use Pavlok to alert users forpreventive measures. We believe that this simple solution can help people tochange their atomic habits, which may lead to long-term benefits.", "output": "Pavlok-Nudge: A Feedback Mechanism for Atomic Behaviour Modification with Snoring Usecase."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual place classification from a first-person-view monocular RGB image is afundamental problem in long-term robot navigation. A difficulty arises from thefact that RGB image classifiers are often vulnerable to spatial and appearancechanges and degrade due to domain shifts, such as seasonal, weather, andlighting differences. To address this issue, multi-sensor fusion approachescombining RGB and depth (D) (e.g., LIDAR, radar, stereo) have gained popularityin recent years. Inspired by these efforts in multimodal RGB-D fusion, weexplore the use of pseudo-depth measurements from recently-developed techniquesof ``domain invariant\" monocular depth estimation as an additional pseudo depthmodality, by reformulating the single-modal RGB image classification task as apseudo multi-modal RGB-D classification problem. Specifically, a practical,fully self-supervised framework for training, appropriately processing, fusing,and classifying these two modalities, RGB and pseudo-D, is described.Experiments on challenging cross-domain scenarios using public NCLT datasetsvalidate effectiveness of the proposed framework.", "output": "A Multi-modal Approach to Single-modal Visual Place Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel deep learning-based framework: Embedded Feature SimilarityOptimization with Specific Parameter Initialization (SOPI) for 2D/3Dregistration which is a most challenging problem due to the difficulty such asdimensional mismatch, heavy computation load and lack of golden evaluatingstandard. The framework we designed includes a parameter specification moduleto efficiently choose initialization pose parameter and a fine-registrationnetwork to align images. The proposed framework takes extracting multi-scalefeatures into consideration using a novel composite connection encoder withspecial training techniques. The method is compared with both learning-basedmethods and optimization-based methods to further evaluate the performance. Ourexperiments demonstrate that the method in this paper has improved theregistration performance, and thereby outperforms the existing methods in termsof accuracy and running time. We also show the potential of the proposed methodas an initial pose estimator.", "output": "Embedded Feature Similarity Optimization with Specific Parameter Initialization for 2D/3D Registration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Representing human performance at high-fidelity is an essential buildingblock in diverse applications, such as film production, computer games orvideoconferencing. To close the gap to production-level quality, we introduceHumanRF, a 4D dynamic neural scene representation that captures full-bodyappearance in motion from multi-view video input, and enables playback fromnovel, unseen viewpoints. Our novel representation acts as a dynamic videoencoding that captures fine details at high compression rates by factorizingspace-time into a temporal matrix-vector decomposition. This allows us toobtain temporally coherent reconstructions of human actors for long sequences,while representing high-resolution details even in the context of challengingmotion. While most research focuses on synthesizing at resolutions of 4MP orlower, we address the challenge of operating at 12MP. To this end, we introduceActorsHQ, a novel multi-view dataset that provides 12MP footage from 160cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. Wedemonstrate challenges that emerge from using such high-resolution data andshow that our newly introduced HumanRF effectively leverages this data, makinga significant step towards production-level quality novel view synthesis.", "output": "HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine unlearning (MU) is a field that is gaining increasing attention dueto the need to remove or modify predictions made by machine learning (ML)models. While training models have become more efficient and accurate, theimportance of unlearning previously learned information has become increasinglysignificant in fields such as privacy, security, and fairness. This paperpresents a comprehensive survey of MU, covering current state-of-the-arttechniques and approaches, including data deletion, perturbation, and modelupdates. In addition, commonly used metrics and datasets are also presented.The paper also highlights the challenges that need to be addressed, includingattack sophistication, standardization, transferability, interpretability,training data, and resource constraints. The contributions of this paperinclude discussions about the potential benefits of MU and its futuredirections in Natural Language Processing, Computer vision, and RecommenderSystems. Additionally, the paper emphasizes the need for researchers andpractitioners to continue exploring and refining unlearning techniques toensure that ML models can adapt to changing circumstances while maintaininguser trust. The importance of unlearning is further highlighted in makingArtificial Intelligence (AI) more trustworthy and transparent, especially withthe increasing importance of AI in various domains that involve large amountsof personal user data", "output": "Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficiently training a multi-task neural solver for various combinatorialoptimization problems (COPs) has been less studied so far. In this paper, wepropose a general and efficient training paradigm based on multi-armed banditsto deliver a unified multi-task neural solver. To this end, we resort to thetheoretical loss decomposition for multiple tasks under an encoder-decoderframework, which enables more efficient training via proper bandittask-sampling algorithms through an intra-task influence matrix. Our methodachieves much higher overall performance with either limited training budgetsor the same training epochs, compared to standard training schedules, which canbe promising for advising efficient training of other multi-task large models.Additionally, the influence matrix can provide empirical evidence of somecommon practices in the area of learning to optimize, which in turn supportsthe validity of our approach.", "output": "Efficient Training of Multi-task Neural Solver with Multi-armed Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recently introduced Quantum Lego framework provides a powerful method forgenerating complex quantum error correcting codes (QECCs) out of simple ones.We gamify this process and unlock a new avenue for code design and discoveryusing reinforcement learning (RL). One benefit of RL is that we can specifytextit{arbitrary} properties of the code to be optimized. We train on two suchproperties, maximizing the code distance, and minimizing the probability oflogical error under biased Pauli noise. For the first, we show that the trainedagent identifies ways to increase code distance beyond naive concatenation,saturating the linear programming bound for CSS codes on 13 qubits. With alearning objective to minimize the logical error probability under biased Paulinoise, we find the best known CSS code at this task for $lesssim 20$ qubits.Compared to other (locally deformed) CSS codes, including Surface, XZZX, and 2DColor codes, our $[[17,1,3]]$ code construction actually has textit{lower}adversarial distance, yet better protects the logical information, highlightingthe importance of QECC desiderata. Lastly, we comment on how this RL frameworkcan be used in conjunction with physical quantum devices to tailor a codewithout explicit characterization of the noise model.", "output": "Discovery of Optimal Quantum Error Correcting Codes via Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We observe that the mapping between an image's representation in one model toits representation in another can be learned surprisingly well with just alinear layer, even across diverse models. Building on this observation, wepropose $textit{text-to-concept}$, where features from a fixed pretrainedmodel are aligned linearly to the CLIP space, so that text embeddings fromCLIP's text encoder become directly comparable to the aligned features. Withtext-to-concept, we convert fixed off-the-shelf vision encoders to surprisinglystrong zero-shot classifiers for free, with accuracy at times even surpassingthat of CLIP, despite being much smaller models and trained on a small fractionof the data compared to CLIP. We show other immediate use-cases oftext-to-concept, like building concept bottleneck models with no conceptsupervision, diagnosing distribution shifts in terms of human concepts, andretrieving images satisfying a set of text-based constraints. Lastly, wedemonstrate the feasibility of $textit{concept-to-text}$, where vectors in amodel's feature space are decoded by first aligning to the CLIP before beingfed to a GPT-based generative model. Our work suggests existing deep models,with presumably diverse architectures and training, represent input samplesrelatively similarly, and a two-way communication across model representationspaces and to humans (through language) is viable.", "output": "Text-To-Concept (and Back) via Cross-Model Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised knowledge-graph completion (KGC) relies on estimating ascoring model over (entity, relation, entity)-tuples, for example, by embeddingan initial knowledge graph. Prediction quality can be improved by calibratingthe scoring model, typically by adjusting the prediction thresholds usingmanually annotated examples. In this paper, we attempt for the first timecold-start calibration for KGC, where no annotated examples exist initially forcalibration, and only a limited number of tuples can be selected forannotation. Our new method ACTC finds good per-relation thresholds efficientlybased on a limited set of annotated tuples. Additionally to a few annotatedtuples, ACTC also leverages unlabeled tuples by estimating their correctnesswith Logistic Regression or Gaussian Process classifiers. We also experimentwith different methods for selecting candidate tuples for annotation:density-based and random selection. Experiments with five scoring models and anoracle annotator show an improvement of 7% points when using ACTC in thechallenging setting with an annotation budget of only 10 tuples, and an averageimprovement of 4% points over different budgets.", "output": "ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adaptive learning is an area of educational technology that consists indelivering personalized learning experiences to address the unique needs ofeach learner. An important subfield of adaptive learning is learning pathpersonalization: it aims at designing systems that recommend sequences ofeducational activities to maximize students' learning outcomes. Many machinelearning approaches have already demonstrated significant results in a varietyof contexts related to learning path personalization. However, most of themwere designed for very specific settings and are not very reusable. This isaccentuated by the fact that they often rely on non-scalable models, which areunable to integrate new elements after being trained on a specific set ofeducational resources. In this paper, we introduce a flexible and scalableapproach towards the problem of learning path personalization, which weformalize as a reinforcement learning problem. Our model is a sequentialrecommender system based on a graph neural network, which we evaluate on apopulation of simulated learners. Our results demonstrate that it can learn tomake good recommendations in the small-data regime.", "output": "Towards Scalable Adaptive Learning with Graph Neural Networks and Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A major problem with Active Learning (AL) is high training costs since modelsare typically retrained from scratch after every query round. We start bydemonstrating that standard AL on neural networks with warm starting fails,both to accelerate training and to avoid catastrophic forgetting when usingfine-tuning over AL query rounds. We then develop a new class of techniques,circumventing this problem, by biasing further training towards previouslylabeled sets. We accomplish this by employing existing, and developing novel,replay-based Continual Learning (CL) algorithms that are effective at quicklylearning the new without forgetting the old, especially when data comes from anevolving distribution. We call this paradigm Continual Active Learning (CAL).We show CAL achieves significant speedups using a plethora of replay schemesthat use model distillation and that select diverse, uncertain points from thehistory. We conduct experiments across many data domains, including naturallanguage, vision, medical imaging, and computational biology, each withdifferent neural architectures and dataset sizes. CAL consistently provides a3x reduction in training time, while retaining performance.", "output": "Accelerating Batch Active Learning Using Continual Learning Techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generation of automated clinical notes have been posited as a strategy tomitigate physician burnout. In particular, an automated narrative summary of apatient's hospital stay could supplement the hospital course section of thedischarge summary that inpatient physicians document in electronic healthrecord (EHR) systems. In the current study, we developed and evaluated anautomated method for summarizing the hospital course section usingencoder-decoder sequence-to-sequence transformer models. We fine tuned BERT andBART models and optimized for factuality through constraining beam search,which we trained and tested using EHR data from patients admitted to theneurology unit of an academic medical center. The approach demonstrated goodROUGE scores with an R-2 of 13.76. In a blind evaluation, two board-certifiedphysicians rated 62% of the automated summaries as meeting the standard ofcare, which suggests the method may be useful clinically. To our knowledge,this study is among the first to demonstrate an automated method for generatinga discharge summary hospital course that approaches a quality level of what aphysician would write.", "output": "A Method to Automate the Discharge Summary Hospital Course for Neurology Patients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In Islam, readers must apply a set of pronunciation rules called Tajweedrules to recite the Quran in the same way that the angel Jibrael taught theProphet, Muhammad. The traditional process of learning the correct applicationof these rules requires a human who must have a license and great experience todetect mispronunciation. Due to the increasing number of Muslims around theworld, the number of Tajweed teachers is not enough nowadays for dailyrecitation practice for every Muslim. Therefore, lots of work has been done forautomatic Tajweed rules' mispronunciation detection to help readers reciteQuran correctly in an easier way and shorter time than traditional learningways. All previous works have three common problems. First, most of themfocused on machine learning algorithms only. Second, they used private datasetswith no benchmark to compare with. Third, they did not take into considerationthe sequence of input data optimally, although the speech signal is timeseries. To overcome these problems, we proposed a solution that consists ofMel-Frequency Cepstral Coefficient (MFCC) features with Long Short-Term Memory(LSTM) neural networks which use the time series, to detect mispronunciation inTajweed rules. In addition, our experiments were performed on a public dataset,the QDAT dataset, which contains more than 1500 voices of the correct andincorrect recitation of three Tajweed rules (Separate stretching , Tight Noon ,and Hide ). To the best of our knowledge, the QDAT dataset has not been used byany research paper yet. We compared the performance of the proposed LSTM modelwith traditional machine learning algorithms used in SoTA. The LSTM model withtime series showed clear superiority over traditional machine learning. Theaccuracy achieved by LSTM on the QDAT dataset was 96%, 95%, and 96% for thethree rules (Separate stretching, Tight Noon, and Hide), respectively.", "output": "Mispronunciation Detection of Basic Quranic Recitation Rules using Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate estimates of long-term risk probabilities and their gradients arecritical for many stochastic safe control methods. However, computing such riskprobabilities in real-time and in unseen or changing environments ischallenging. Monte Carlo (MC) methods cannot accurately evaluate theprobabilities and their gradients as an infinitesimal devisor can amplify thesampling noise. In this paper, we develop an efficient method to evaluate theprobabilities of long-term risk and their gradients. The proposed methodexploits the fact that long-term risk probability satisfies certain partialdifferential equations (PDEs), which characterize the neighboring relationsbetween the probabilities, to integrate MC methods and physics-informed neuralnetworks. We provide theoretical guarantees of the estimation error givencertain choices of training configurations. Numerical results show the proposedmethod has better sample efficiency, generalizes well to unseen regions, andcan adapt to systems with changing parameters. The proposed method can alsoaccurately estimate the gradients of risk probabilities, which enables first-and second-order techniques on risk probabilities to be used for learning andcontrol.", "output": "A Generalizable Physics-informed Learning Framework for Risk Probability Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Convolutional Networks (GCNs) have shown strong performance in learningtext representations for various tasks such as text classification, due to itsexpressive power in modeling graph structure data (e.g., a literature citationnetwork). Most existing GCNs are limited to deal with documents included in apre-defined graph, i.e., it cannot be generalized to out-of-graph documents. Toaddress this issue, we propose to transform the document graph into a wordgraph, to decouple data samples (i.e., documents in training and test sets) anda GCN model by using a document-independent graph. Such word-level GCN couldtherefore naturally inference out-of-graph documents in an inductive way. Theproposed Word-level Graph (WGraph) can not only implicitly learning wordpresentation with commonly-used word co-occurrences in corpora, but alsoincorporate extra global semantic dependency derived from inter-documentrelationships (e.g., literature citations). An inductive Word-grounded GraphConvolutional Network (WGCN) is proposed to learn word and documentrepresentations based on WGraph in a supervised manner. Experiments on textclassification with and without citation networks evidence that the proposedWGCN model outperforms existing methods in terms of effectiveness andefficiency.", "output": "Word Grounded Graph Convolutional Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of mini-batches of data in training artificial neural networks isnowadays very common. Despite its broad usage, theories explainingquantitatively how large or small the optimal mini-batch size should be aremissing. This work presents a systematic attempt at understanding the role ofthe mini-batch size in training two-layer neural networks. Working in theteacher-student scenario, with a sparse teacher, and focusing on tasks ofdifferent complexity, we quantify the effects of changing the mini-batch size$m$. We find that often the generalization performances of the student stronglydepend on $m$ and may undergo sharp phase transitions at a critical value$m_c$, such that for $m&lt;m_c$ the training process fails, while for $m&gt;m_c$ thestudent learns perfectly or generalizes very well the teacher. Phasetransitions are induced by collective phenomena firstly discovered instatistical mechanics and later observed in many fields of science. Finding aphase transition varying the mini-batch size raises several important questionson the role of a hyperparameter which have been somehow overlooked until now.", "output": "Phase transitions in the mini-batch size for sparse and dense neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a consequence of the increasing influence of machine learning on ourlives, everyone needs competencies to understand corresponding phenomena, butalso to get involved in shaping our world and making informed decisionsregarding the influences on our society. Therefore, in K-12 education, studentsneed to learn about core ideas and principles of machine learning. However, forthis target group, achieving all of the aforementioned goals presents anenormous challenge. To this end, we present a teaching concept that combines aplayful and accessible unplugged approach focusing on conceptual understandingwith empowering students to actively apply machine learning methods and reflecttheir influence on society, building upon decision tree learning.", "output": "Data, Trees, and Forests -- Decision Tree Learning in K-12 Education."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study multi-agent reinforcement learning in the setting of episodic Markovdecision processes, where multiple agents cooperate via communication through acentral server. We propose a provably efficient algorithm based on valueiteration that enable asynchronous communication while ensuring the advantageof cooperation with low communication overhead. With linear functionapproximation, we prove that our algorithm enjoys an$tilde{mathcal{O}}(d^{3/2}H^2sqrt{K})$ regret with$tilde{mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the featuredimension, $H$ is the horizon length, $M$ is the total number of agents, and$K$ is the total number of episodes. We also provide a lower bound showing thata minimal $Omega(dM)$ communication complexity is required to improve theperformance through collaboration.", "output": "Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Early detection of mental disorder is crucial as it enables promptintervention and treatment, which can greatly improve outcomes for individualssuffering from debilitating mental affliction. The recent proliferation ofmental health discussions on social media platforms presents researchopportunities to investigate mental health and potentially detect instances ofmental illness. However, existing depression detection methods are constraineddue to two major limitations: (1) the reliance on feature engineering and (2)the lack of consideration for time-varying factors. Specifically, these methodsrequire extensive feature engineering and domain knowledge, which heavily relyon the amount, quality, and type of user-generated content. Moreover, thesemethods ignore the important impact of time-varying factors on depressiondetection, such as the dynamics of linguistic patterns and interpersonalinteractive behaviors over time on social media (e.g., replies, mentions, andquote-tweets). To tackle these limitations, we propose an early depressiondetection framework, ContrastEgo treats each user as a dynamic time-evolvingattributed graph (ego-network) and leverages supervised contrastive learning tomaximize the agreement of users' representations at different scales whileminimizing the agreement of users' representations to differentiate betweendepressed and control groups. ContrastEgo embraces four modules, (1)constructing users' heterogeneous interactive graphs, (2) extracting therepresentations of users' interaction snapshots using graph neural networks,(3) modeling the sequences of snapshots using attention mechanism, and (4)depression detection using contrastive learning. Extensive experiments onTwitter data demonstrate that ContrastEgo significantly outperforms thestate-of-the-art methods in terms of all the effectiveness metrics in variousexperimental settings.", "output": "Dynamic Graph Representation Learning for Depression Screening with Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding human affective behaviour, especially in the dynamics ofreal-world settings, requires Facial Expression Recognition (FER) models tocontinuously adapt to individual differences in user expression, contextualattributions, and the environment. Current (deep) Machine Learning (ML)-basedFER approaches pre-trained in isolation on benchmark datasets fail to capturethe nuances of real-world interactions where data is available onlyincrementally, acquired by the agent or robot during interactions. New learningcomes at the cost of previous knowledge, resulting in catastrophic forgetting.Lifelong or Continual Learning (CL), on the other hand, enables adaptability inagents by being sensitive to changing data distributions, integrating newinformation without interfering with previously learnt knowledge. Positing CLas an effective learning paradigm for FER, this work presents the ContinualFacial Expression Recognition (ConFER) benchmark that evaluates popular CLtechniques on FER tasks. It presents a comparative analysis of several CL-basedapproaches on popular FER datasets such as CK+, RAF-DB, and AffectNet andpresent strategies for a successful implementation of ConFER for AffectiveComputing (AC) research. CL techniques, under different learning settings, areshown to achieve state-of-the-art (SOTA) performance across several datasets,thus motivating a discussion on the benefits of applying CL principles towardshuman behaviour understanding, particularly from facial expressions, as wellthe challenges entailed.", "output": "Continual Facial Expression Recognition: A Benchmark."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prognostics and health management (PHM) technology plays a critical role inindustrial production and equipment maintenance by identifying and predictingpossible equipment failures and damages, thereby allowing necessary maintenancemeasures to be taken to enhance equipment service life and reliability whilereducing production costs and downtime. In recent years, PHM technology basedon artificial intelligence (AI) has made remarkable achievements in the contextof the industrial IoT and big data, and it is widely used in variousindustries, such as railway, energy, and aviation, for condition monitoring,fault prediction, and health management. The emergence of large-scalefoundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry ofAI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolvedfrom a research paradigm of single-modal, single-task, and limited-data to amulti-modal, multi-task, massive data, and super-large model paradigm. ChatGPTrepresents a landmark achievement in this research paradigm, offering hope forgeneral artificial intelligence due to its highly intelligent natural languageunderstanding ability. However, the PHM field lacks a consensus on how torespond to this significant change in the AI field, and a systematic review androadmap is required to elucidate future development directions. To fill thisgap, this paper systematically expounds on the key components and latestdevelopments of LSF-Models. Then, we systematically answered how to build theLSF-Model applicable to PHM tasks and outlined the challenges and futuredevelopment roadmaps for this research paradigm.", "output": "ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a holistic approach to gradient leakage resilientdistributed Stochastic Gradient Descent (SGD). First, we analyze two types ofstrategies for privacy-enhanced federated learning: (i) gradient pruning withrandom selection or low-rank filtering and (ii) gradient perturbation withadditive random noise or differential privacy noise. We analyze the inherentlimitations of these approaches and their underlying impact on privacyguarantee, model accuracy, and attack resilience. Next, we present a gradientleakage resilient approach to securing distributed SGD in federated learning,with differential privacy controlled noise as the tool. Unlike conventionalmethods with the per-client federated noise injection and fixed noise parameterstrategy, our approach keeps track of the trend of per-example gradientupdates. It makes adaptive noise injection closely aligned throughout thefederated model training. Finally, we provide an empirical privacy analysis onthe privacy guarantee, model utility, and attack resilience of the proposedapproach. Extensive evaluation using five benchmark datasets demonstrates thatour gradient leakage resilient approach can outperform the state-of-the-artmethods with competitive accuracy performance, strong differential privacyguarantee, and high resilience against gradient leakage attacks. The codeassociated with this paper can be found:", "output": "Securing Distributed SGD against Gradient Leakage Threats."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have demonstrated exceptional capabilities ingeneralizing to new tasks in a zero-shot or few-shot manner. However, theextent to which LLMs can comprehend user preferences based on their previousbehavior remains an emerging and still unclear research question.Traditionally, Collaborative Filtering (CF) has been the most effective methodfor these tasks, predominantly relying on the extensive volume of rating data.In contrast, LLMs typically demand considerably less data while maintaining anexhaustive world knowledge about each item, such as movies or products. In thispaper, we conduct a thorough examination of both CF and LLMs within the classictask of user rating prediction, which involves predicting a user's rating for acandidate item based on their past ratings. We investigate various LLMs indifferent sizes, ranging from 250M to 540B parameters and evaluate theirperformance in zero-shot, few-shot, and fine-tuning scenarios. We conductcomprehensive analysis to compare between LLMs and strong CF methods, and findthat zero-shot LLMs lag behind traditional recommender models that have theaccess to user interaction data, indicating the importance of user interactiondata. However, through fine-tuning, LLMs achieve comparable or even betterperformance with only a small fraction of the training data, demonstratingtheir potential through data efficiency.", "output": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traffic data serves as a fundamental component in both research andapplications within intelligent transportation systems. However, real-worldtransportation data, collected from loop detectors or similar sources, oftencontain missing values (MVs), which can adversely impact associatedapplications and research. Instead of discarding this incomplete data,researchers have sought to recover these missing values through numericalstatistics, tensor decomposition, and deep learning techniques. In this paper,we propose an innovative deep-learning approach for imputing missing data. Agraph attention architecture is employed to capture the spatial correlationspresent in traffic data, while a bidirectional neural network is utilized tolearn temporal information. Experimental results indicate that our proposedmethod outperforms all other benchmark techniques, thus demonstrating itseffectiveness.", "output": "ST-GIN: An Uncertainty Quantification Approach in Traffic Data Imputation with Spatio-temporal Graph Attention and Bidirectional Recurrent United Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "General-purpose language models that can solve various language-domain taskshave emerged driven by the pre-training and instruction-tuning pipeline.However, building general-purpose vision-language models is challenging due tothe increased task discrepancy introduced by the additional visual input.Although vision-language pre-training has been widely studied, vision-languageinstruction tuning remains relatively less explored. In this paper, we conducta systematic and comprehensive study on vision-language instruction tuningbased on the pre-trained BLIP-2 models. We gather a wide variety of 26 publiclyavailable datasets, transform them into instruction tuning format andcategorize them into two clusters for held-in instruction tuning and held-outzero-shot evaluation. Additionally, we introduce instruction-aware visualfeature extraction, a crucial method that enables the model to extractinformative features tailored to the given instruction. The resultingInstructBLIP models achieve state-of-the-art zero-shot performance across all13 held-out datasets, substantially outperforming BLIP-2 and the largerFlamingo. Our models also lead to state-of-the-art performance when finetunedon individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG).Furthermore, we qualitatively demonstrate the advantages of InstructBLIP overconcurrent multimodal models. All InstructBLIP models have been open-sourced at", "output": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large time-varying graphs are increasingly common in financial, social andbiological settings. Feature extraction that efficiently encodes the complexstructure of sparse, multi-layered, dynamic graphs presents computational andmethodological challenges. In the past decade, a persistence diagram (PD) fromtopological data analysis (TDA) has become a popular descriptor of shape ofdata with a well-defined distance between points. However, applications of TDAto graphs, where there is no intrinsic concept of distance between the nodes,remain largely unexplored. This paper addresses this gap in the literature byintroducing a computationally efficient framework to extract shape informationfrom graph data. Our framework has two main steps: first, we compute a PD usingthe so-called lower-star filtration which utilizes quantitative nodeattributes, and then vectorize it by averaging the associated Betti functionover successive scale values on a one-dimensional grid. Our approach avoidsembedding a graph into a metric space and has stability properties againstinput noise. In simulation studies, we show that the proposed vector summaryleads to improved change point detection rate in time-varying graphs. In a realdata application, our approach provides up to 22% gain in anomalous priceprediction for the Ethereum cryptocurrency transaction networks.", "output": "A fast topological approach for predicting anomalies in time-varying graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in Natural Language Processing (NLP) has led to theproliferation of large pretrained language models. These models have been shownto yield good performance, using in-context learning, even on unseen tasks andlanguages. They have also been exposed as commercial APIs as a form oflanguage-model-as-a-service, with great adoption. However, their performance onAfrican languages is largely unknown. We present a preliminary analysis ofcommercial large language models on two tasks (machine translation and textclassification) across eight African languages, spanning different languagefamilies and geographical areas. Our results suggest that commercial languagemodels produce below-par performance on African languages. We also find thatthey perform better on text classification than machine translation. Ingeneral, our findings present a call-to-action to ensure African languages arewell represented in commercial large language models, given their growingpopularity.", "output": "How Good are Commercial Large Language Models on African Languages?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we focus on the graph representation learning (a.k.a. networkembedding) in attributed graphs. Different from existing embedding methods thattreat the incorporation of graph structure and semantic as the simplecombination of two optimization objectives, we propose a novel semantic graphrepresentation (SGR) method to formulate the joint optimization of the twoheterogeneous sources into a common high-order proximity based framework.Concretely, we first construct an auxiliary weighted graph, where the complexhomogeneous and heterogeneous relations among nodes and attributes in theoriginal graph are comprehensively encoded. Conventional embedding methods thatconsider high-order topology proximities can then be easily applied to thenewly constructed graph to learn the representations of both node and attributewhile capturing the nonlinear high-order intrinsic correlation inside or amonggraph structure and semantic. The learned attribute embeddings can alsoeffectively support some semantic-oriented inference tasks (e.g., semanticcommunity detection), helping to reveal the graph's deep semantic. Theeffectiveness of SGR is further verified on a series of real graphs, where itachieves impressive performance over other baselines.", "output": "Semantic Random Walk for Graph Representation Learning in Attributed Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spectral clustering is one of the most popular clustering algorithms that hasstood the test of time. It is simple to describe, can be implemented usingstandard linear algebra, and often finds better clusters than traditionalclustering algorithms like $k$-means and $k$-centers. The foundationalalgorithm for two-way spectral clustering, by Shi and Malik, creates ageometric graph from data and finds a spectral cut of the graph.In modern machine learning, many data sets are modeled as a large number ofpoints drawn from a probability density function. Little is known about whenspectral clustering works in this setting -- and when it doesn't. Pastresearchers justified spectral clustering by appealing to the graph Cheegerinequality (which states that the spectral cut of a graph approximates the``Normalized Cut''), but this justification is known to break down on largedata sets.We provide theoretically-informed intuition about spectral clustering onlarge data sets drawn from probability densities, by proving when a continuousform of spectral clustering considered by past researchers (the unweightedspectral cut of a probability density) finds good clusters of the underlyingdensity itself. Our work suggests that Shi-Malik spectral clustering works wellon data drawn from mixtures of Laplace distributions, and works poorly on datadrawn from certain other densities, such as a density we call the `square-roottrough'.Our core theorem proves that weighted spectral cuts have low weightedisoperimetry for all probability densities. Our key tool is a new Cheeger-Buserinequality for all probability densities, including discontinuous ones.", "output": "Spectral Clustering on Large Datasets: When Does it Work? Theory from Continuous Clustering and Density Cheeger-Buser."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While ensuring stability for linear systems is well understood, it remains amajor challenge for systems with nonlinear dynamics. A general approach in suchcases is to leverage Lyapunov stability theory to compute a combination of aLyapunov control function and an associated control policy. However, findingLyapunov functions for general nonlinear systems is a challenging task. Toaddress this challenge, several methods have been recently proposed thatrepresent Lyapunov functions using neural networks. However, such approacheshave been designed exclusively for continuous-time systems. We propose thefirst approach for learning neural Lyapunov control in discrete-time systems.Three key ingredients enable us to effectively learn provably stable controlpolicies. The first is a novel mixed-integer linear programming approach forverifying the stability conditions in discrete-time systems. The second is anovel approach for computing sub-level sets which characterize the region ofattraction. Finally, we rely on a heuristic gradient-based approach for quicklyfinding counterexamples to significantly speed up Lyapunov function learning.Our experiments on four standard benchmarks demonstrate that our approachsignificantly outperforms state-of-the-art baselines. For example, on the pathtracking benchmark, we outperform recent neural Lyapunov control baselines byan order of magnitude in both running time and the size of the region ofattraction, and on two of the four benchmarks (cartpole and PVTOL), ours is thefirst automated approach to return a provably stable controller.", "output": "Neural Lyapunov Control for Discrete-Time Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lifelong learning (LL) is an important ability for NLP models to learn newtasks continuously. Architecture-based approaches are reported to be effectiveimplementations for LL models. However, it is non-trivial to extend previousapproaches to domain incremental LL scenarios since they either require accessto task identities in the testing phase or cannot handle samples from unseentasks. In this paper, we propose textbf{Diana}: aunderline{d}ynamunderline{i}c underline{a}rchitecture-basedlifelounderline{n}g leunderline{a}rning model that tries to learn a sequenceof tasks with a prompt-enhanced language model. Four types of hierarchicallyorganized prompts are used in Diana to capture knowledge from differentgranularities. Specifically, we dedicate task-level prompts to capturetask-specific knowledge to retain high LL performances and maintaininstance-level prompts to learn knowledge shared across input samples toimprove the model's generalization performance. Moreover, we dedicate separateprompts to explicitly model unseen tasks and introduce a set of prompt keyvectors to facilitate knowledge sharing between tasks. Extensive experimentsdemonstrate that Diana outperforms state-of-the-art LL models, especially inhandling unseen tasks. We release the code and data aturl{", "output": "Domain Incremental Lifelong Learning in an Open World."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world data often have an open long-tailed distribution, and building aunified QA model supporting various tasks is vital for practical QAapplications. However, it is non-trivial to extend previous QA approaches sincethey either require access to seen tasks of adequate samples or do notexplicitly model samples from unseen tasks. In this paper, we define OpenLong-Tailed QA (OLTQA) as learning from long-tailed distributed data andoptimizing performance over seen and unseen QA tasks. We propose an OLTQA modelthat encourages knowledge sharing between head, tail and unseen tasks, andexplicitly mines knowledge from a large pre-trained language model (LM).Specifically, we organize our model through a pool of fine-grained componentsand dynamically combine these components for an input to facilitate knowledgesharing. A retrieve-then-rerank frame is further introduced to selectin-context examples, which guild the LM to generate text that express knowledgefor QA tasks. Moreover, a two-stage training approach is introduced topre-train the framework by knowledge distillation (KD) from the LM and thenjointly train the frame and a QA model through an adaptive mutual KD method. Ona large-scale OLTQA dataset we curate from 43 existing QA datasets, our modelconsistently outperforms the state-of-the-art. We release the code and data aturl{", "output": "Long-Tailed Question Answering in an Open World."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatiotemporal traffic data imputation (STDI), estimating the missing datafrom partially observed traffic data, is an inevitable and challenging task indata-driven intelligent transportation systems (ITS). Due to traffic data'smultidimensional and spatiotemporal properties, we treat the missing dataimputation as a tensor completion problem. Many studies have been on STDI basedon tensor decomposition in the past decade. However, how to use spatiotemporalcorrelations and core tensor sparsity to improve the imputation performancestill needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor andproposes an innovative manifold regularized Tucker decomposition (ManiRTD)model for STDI. Expressly, we represent the sensory traffic state data as the3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then,ManiRTD improves the sparsity of the Tucker core using a sparse regularizationterm and employs manifold regularization and temporal constraint terms offactor matrices to characterize the spatiotemporal correlations. Finally, weaddress the ManiRTD model through a block coordinate descent framework underalternating proximal gradient updating rules with convergence-guaranteed.Numerical experiments are conducted on real-world spatiotemporal trafficdatasets (STDs). Our results demonstrate that the proposed model outperformsthe other factorization approaches and reconstructs the STD more preciselyunder various missing scenarios.", "output": "Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recommendation foundation model utilizes large language models (LLM) forrecommendation by converting recommendation tasks into natural language tasks.It enables generative recommendation which directly generates the item(s) torecommend rather than calculating a ranking score for each and every candidateitem in traditional recommendation models, simplifying the recommendationpipeline from multi-stage filtering to single-stage filtering. To avoidgenerating excessively long text when deciding which item(s) to recommend,creating LLM-compatible item IDs is essential for recommendation foundationmodels. In this study, we systematically examine the item indexing problem forrecommendation foundation models, using P5 as the representative backbone modeland replicating its results with various indexing methods. To emphasize theimportance of item indexing, we first discuss the issues of several trivialitem indexing methods, such as independent indexing, title indexing, and randomindexing. We then propose four simple yet effective solutions, includingsequential indexing, collaborative indexing, semantic (content-based) indexing,and hybrid indexing. Our reproducibility study of P5 highlights the significantinfluence of item indexing methods on the model performance, and our results onreal-world datasets validate the effectiveness of our proposed solutions.", "output": "How to Index Item IDs for Recommendation Foundation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a node clustering method for time-varying graphs based on theassumption that the cluster labels are changed smoothly over time. Clusteringis one of the fundamental tasks in many science and engineering fieldsincluding signal processing, machine learning, and data mining. Although mostexisting studies focus on the clustering of nodes in static graphs, we oftenencounter time-varying graphs for time-series data, e.g., social networks,brain functional connectivity, and point clouds. In this paper, we formulate anode clustering of time-varying graphs as an optimization problem based onspectral clustering, with a smoothness constraint of the node labels. We solvethe problem with a primal-dual splitting algorithm. Experiments on syntheticand real-world time-varying graphs are performed to validate the effectivenessof the proposed approach.", "output": "Clustering of Time-Varying Graphs Based on Temporal Label Smoothness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop the first active learning method in the predict-then-optimizeframework. Specifically, we develop a learning method that sequentially decideswhether to request the \"labels\" of feature samples from an unlabeled datastream, where the labels correspond to the parameters of an optimization modelfor decision-making. Our active learning method is the first to be directlyinformed by the decision error induced by the predicted parameters, which isreferred to as the Smart Predict-then-Optimize (SPO) loss. Motivated by thestructure of the SPO loss, our algorithm adopts a margin-based criterionutilizing the concept of distance to degeneracy and minimizes a tractablesurrogate of the SPO loss on the collected data. In particular, we develop anefficient active learning algorithm with both hard and soft rejection variants,each with theoretical excess risk (i.e., generalization) guarantees. We furtherderive bounds on the label complexity, which refers to the number of sampleswhose labels are acquired to achieve a desired small level of SPO risk. Undersome natural low-noise conditions, we show that these bounds can be better thanthe naive supervised learning approach that labels all samples. Furthermore,when using the SPO+ loss function, a specialized surrogate of the SPO loss, wederive a significantly smaller label complexity under separability conditions.We also present numerical evidence showing the practical value of our proposedalgorithms in the settings of personalized pricing and the shortest pathproblem.", "output": "Active Learning in the Predict-then-Optimize Framework: A Margin-Based Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spectral-temporal graph neural network is a promising abstraction underlyingmost time series forecasting models that are based on graph neural networks(GNNs). However, more is needed to know about the underpinnings of this branchof methods. In this paper, we establish a theoretical framework that unravelsthe expressive power of spectral-temporal GNNs. Our results show that linearspectral-temporal GNNs are universal under mild assumptions, and theirexpressive power is bounded by our extended first-order Weisfeiler-Lemanalgorithm on discrete-time dynamic graphs. To make our findings useful inpractice on valid instantiations, we discuss related constraints in detail andoutline a theoretical blueprint for designing spatial and temporal modules inspectral domains. Building on these insights and to demonstrate how powerfulspectral-temporal GNNs are based on our framework, we propose a simpleinstantiation named Temporal Graph GegenConv (TGC), which significantlyoutperforms most existing models with only linear components and shows bettermodel efficiency.", "output": "How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhileendeavor. HKG consists of hyper-relational facts (H-Facts), composed of a maintriple and several auxiliary attribute-value qualifiers, which can effectivelyrepresent factually comprehensive information. The internal structure of HKGcan be represented as a hypergraph-based representation globally and a semanticsequence-based representation locally. However, existing research seldomsimultaneously models the graphical and sequential structure of HKGs, limitingHKGs' representation. To overcome this limitation, we propose a novelHierarchical Attention model for HKG Embedding (HAHE), including global-leveland local-level attention. The global-level attention can model the graphicalstructure of HKG using hypergraph dual-attention layers, while the local-levelattention can learn the sequential structure inside H-Facts via heterogeneousself-attention layers. Experiment results indicate that HAHE achievesstate-of-the-art performance in link prediction tasks on HKG standard datasets.In addition, HAHE addresses the issue of HKG multi-position prediction for thefirst time, increasing the applicability of the HKG link prediction task. Ourcode is publicly available.", "output": "HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating high quality music that complements the visual content of a videois a challenging task. Most existing visual conditioned music generationsystems generate symbolic music data, such as MIDI files, instead of raw audiowaveform. Given the limited availability of symbolic music data, such methodscan only generate music for a few instruments or for specific types of visualinput. In this paper, we propose a novel approach called V2Meow that cangenerate high-quality music audio that aligns well with the visual semantics ofa diverse range of video input types. Specifically, the proposed musicgeneration system is a multi-stage autoregressive model which is trained with anumber of O(100K) music audio clips paired with video frames, which are minedfrom in-the-wild music videos, and no parallel symbolic music data is involved.V2Meow is able to synthesize high-fidelity music audio waveform solelyconditioned on pre-trained visual features extracted from an arbitrary silentvideo clip, and it also allows high-level control over the music style ofgeneration examples via supporting text prompts in addition to the video framesconditioning. Through both qualitative and quantitative evaluations, wedemonstrate that our model outperforms several existing music generationsystems in terms of both visual-audio correspondence and audio quality.", "output": "V2Meow: Meowing to the Visual Beat via Music Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tropical semiring has proven successful in several research areas, includingoptimal control, bioinformatics, discrete event systems, or solving a decisionproblem. In previous studies, a matrix two-factorization algorithm based on thetropical semiring has been applied to investigate bipartite and tripartitenetworks. Tri-factorization algorithms based on standard linear algebra areused for solving tasks such as data fusion, co-clustering, matrix completion,community detection, and more. However, there is currently no tropical matrixtri-factorization approach, which would allow for the analysis of multipartitenetworks with a high number of parts. To address this, we propose thetriFastSTMF algorithm, which performs tri-factorization over the tropicalsemiring. We apply it to analyze a four-partition network structure and recoverthe edge lengths of the network. We show that triFastSTMF performs similarly toFast-NMTF in terms of approximation and prediction performance when fitted onthe whole network. When trained on a specific subnetwork and used to predictthe whole network, triFastSTMF outperforms Fast-NMTF by several orders ofmagnitude smaller error. The robustness of triFastSTMF is due to tropicaloperations, which are less prone to predict large values compared to standardoperations.", "output": "Matrix tri-factorization over the tropical semiring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Even though dropout is a popular regularization technique, its theoreticalproperties are not fully understood. In this paper we study dropoutregularization in extended generalized linear models based on doubleexponential families, for which the dispersion parameter can vary with thefeatures. A theoretical analysis shows that dropout regularization prefers rarebut important features in both the mean and dispersion, generalizing an earlierresult for conventional generalized linear models. Training is performed usingstochastic gradient descent with adaptive learning rate. To illustrate, weapply dropout to adaptive smoothing with B-splines, where both the mean anddispersion parameters are modelled flexibly. The important B-spline basisfunctions can be thought of as rare features, and we confirm in experimentsthat dropout is an effective form of regularization for mean and dispersionparameters that improves on a penalized maximum likelihood approach with anexplicit smoothness penalty.", "output": "Dropout Regularization in Extended Generalized Linear Models based on Double Exponential Families."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A change point detection (CPD) framework assisted by a predictive machinelearning model called ''Predict and Compare'' is introduced and characterisedin relation to other state-of-the-art online CPD routines which it outperformsin terms of false positive rate and out-of-control average run length. Themethod's focus is on improving standard methods from sequential analysis suchas the CUSUM rule in terms of these quality measures.This is achieved by replacing typically used trend estimation functionalssuch as the running mean with more sophisticated predictive models (Predictstep), and comparing their prognosis with actual data (Compare step). The twomodels used in the Predict step are the ARIMA model and the LSTM recursiveneural network. However, the framework is formulated in general terms, so as toallow the use of other prediction or comparison methods than those tested here.The power of the method is demonstrated in a tribological case study in whichchange points separating the run-in, steady-state, and divergent wear phasesare detected in the regime of very few false positives.", "output": "Predictive change point detection for heterogeneous data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural ordinary differential equations (neural ODEs) are a popular family ofcontinuous-depth deep learning models. In this work, we consider a large familyof parameterized ODEs with continuous-in-time parameters, which includetime-dependent neural ODEs. We derive a generalization bound for this class bya Lipschitz-based argument. By leveraging the analogy between neural ODEs anddeep residual networks, our approach yields in particular a generalizationbound for a class of deep residual networks. The bound involves the magnitudeof the difference between successive weight matrices. We illustrate numericallyhow this quantity affects the generalization capability of neural networks.", "output": "Generalization bounds for neural ordinary differential equations and deep residual networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robust reinforcement learning (RL) aims at learning a policy that optimizesthe worst-case performance over an uncertainty set. Given nominal Markovdecision process (N-MDP) that generates samples for training, the set containsMDPs obtained by some perturbations from N-MDP. In this paper, we introduce anew uncertainty set containing more realistic MDPs in practice than theexisting sets. Using this uncertainty set, we present a robust RL, namedARQ-Learning, for tabular cases. Also, we characterize the finite-time errorbounds and prove that it converges as fast as Q-Learning and robust Q-Learning(i.e., the state-of-the-art robust RL method) while providing better robustnessfor real applications. We propose {em pessimistic agent} that efficientlytackles the key bottleneck for the extension of ARQ-Learning into large orcontinuous state spaces. Using this technique, we first propose PRQ-Learning.To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG,respectively. We emphasize that our technique can be easily combined with theother popular model-free methods. Via experiments, we demonstrate thesuperiority of the proposed methods in various RL applications with modeluncertainties.", "output": "On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When fitting the learning data of an individual to algorithm-like learningmodels, the observations are so dependent and non-stationary that one maywonder what the classical Maximum Likelihood Estimator (MLE) could do, even ifit is the usual tool applied to experimental cognition. Our objective in thiswork is to show that the estimation of the learning rate cannot be efficient ifthe learning rate is constant in the classical Exp3 (Exponential weights forExploration and Exploitation) algorithm. Secondly, we show that if the learningrate decreases polynomially with the sample size, then the prediction error andin some cases the estimation error of the MLE satisfy bounds in probabilitythat decrease at a polynomial rate.", "output": "On the convergence of the MLE as an estimator of the learning rate in the Exp3 algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A salient characteristic of large pre-trained language models (PTLMs) is aremarkable improvement in their generalization capability and emergence of newcapabilities with increasing model capacity and pre-training dataset size.Consequently, we are witnessing the development of enormous models pushing thestate-of-the-art. It is, however, imperative to realize that this inevitablyleads to prohibitively long training times, extortionate computing costs, and adetrimental environmental impact. Significant efforts are underway to make PTLMtraining more efficient through innovations in model architectures, trainingpipelines, and loss function design, with scant attention being paid tooptimizing the utility of training data. The key question that we ask iswhether it is possible to train PTLMs by employing only highly informativesubsets of the training data while maintaining downstream performance? Buildingupon the recent progress in informative data subset selection, we show how wecan employ submodular optimization to select highly representative subsets ofthe training corpora. Our results demonstrate that the proposed framework canbe applied to efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) usingonly a fraction of data while retaining up to $sim99%$ of the performance ofthe fully-trained models.", "output": "INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual as well as genetic biometrics are routinely employed to identifyspecies and individuals in biological applications. However, no attempts havebeen made in this domain to computationally enhance visual classification ofrare classes with little image data via genetics. In this paper, we thuspropose aligned visual-genetic inference spaces with the aim to implicitlyencode cross-domain associations for improved performance. We demonstrate forthe first time that such alignment can be achieved via deep embedding modelsand that the approach is directly applicable to boosting long-tailedrecognition (LTR) particularly for rare species. We experimentally demonstratethe efficacy of the concept via application to microscopic imagery of 30k+planktic foraminifer shells across 32 species when used together withindependent genetic data samples. Most importantly for practitioners, we showthat visual-genetic alignment can significantly benefit visual-only recognitionof the rarest species. Technically, we pre-train a visual ResNet50 deeplearning model using triplet loss formulations to create an initial embeddingspace. We re-structure this space based on genetic anchors embedded via aSequence Graph Transform (SGT) and linked to visual data by cross-domain cosinealignment. We show that an LTR approach improves the state-of-the-art acrossall benchmarks and that adding our visual-genetic alignment improves per-classand particularly rare tail class benchmarks significantly further. We concludethat visual-genetic alignment can be a highly effective tool for complementingvisual biological data containing rare classes. The concept proposed may serveas an important future tool for integrating genetics and imageomics towards amore complete scientific representation of taxonomic spaces and life itself.Code, weights, and data splits are published for full reproducibility.", "output": "Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time-to-event modelling, known as survival analysis, differs from standardregression as it addresses censoring in patients who do not experience theevent of interest. Despite competitive performances in tackling this problem,machine learning methods often ignore other competing risks that preclude theevent of interest. This practice biases the survival estimation. Extensions toaddress this challenge often rely on parametric assumptions or numericalestimations leading to sub-optimal survival approximations. This paperleverages constrained monotonic neural networks to model each competingsurvival distribution. This modelling choice ensures the exact likelihoodmaximisation at a reduced computational cost by using automaticdifferentiation. The effectiveness of the solution is demonstrated on onesynthetic and three medical datasets. Finally, we discuss the implications ofconsidering competing risks when developing risk scores for medical practice.", "output": "Neural Fine-Gray: Monotonic neural networks for competing risks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In multivariate time series systems, key insights can be obtained bydiscovering lead-lag relationships inherent in the data, which refer to thedependence between two time series shifted in time relative to one another, andwhich can be leveraged for the purposes of control, forecasting or clustering.We develop a clustering-driven methodology for the robust detection of lead-lagrelationships in lagged multi-factor models. Within our framework, theenvisioned pipeline takes as input a set of time series, and creates anenlarged universe of extracted subsequence time series from each input timeseries, by using a sliding window approach. We then apply various clusteringtechniques (e.g, K-means++ and spectral clustering), employing a variety ofpairwise similarity measures, including nonlinear ones. Once the clusters havebeen extracted, lead-lag estimates across clusters are aggregated to enhancethe identification of the consistent relationships in the original universe.Since multivariate time series are ubiquitous in a wide range of domains, wedemonstrate that our method is not only able to robustly detect lead-lagrelationships in financial markets, but can also yield insightful results whenapplied to an environmental data set.", "output": "Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rutting of asphalt pavements is a crucial design criterion in variouspavement design guides. A good road transportation base can provide securityfor the transportation of oil and gas in road transportation. This studyattempts to develop a robust artificial intelligence model to estimatedifferent asphalt pavements' rutting depth clips, temperature, and load axes asprimary characteristics. The experiment data were obtained from 19 asphaltpavements with different crude oil sources on a 2.038 km long full-scale fieldaccelerated pavement test track (RIOHTrack, Road Track Institute) in Tongzhou,Beijing. In addition, this paper also proposes to build complex networks withdifferent pavement rutting depths through complex network methods and theLouvain algorithm for community detection. The most critical structuralelements can be selected from different asphalt pavement rutting data, andsimilar structural elements can be found. An extreme learning machine algorithmwith residual correction (RELM) is designed and optimized using an independentadaptive particle swarm algorithm. The experimental results of the proposedmethod are compared with several classical machine learning algorithms, withpredictions of Average Root Mean Squared Error, Average Mean Absolute Error,and Average Mean Absolute Percentage Error for 19 asphalt pavements reaching1.742, 1.363, and 1.94% respectively. The experiments demonstrate that theRELM algorithm has an advantage over classical machine learning methods indealing with non-linear problems in road engineering. Notably, the methodensures the adaptation of the simulated environment to different levels ofabstraction through the cognitive analysis of the production environmentparameters.", "output": "A data-driven rutting depth short-time prediction model with metaheuristic optimization for asphalt pavements based on RIOHTrack."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "NUBO, short for Newcastle University Bayesian Optimisation, is a Bayesianoptimisation framework for the optimisation of expensive-to-evaluate black-boxfunctions, such as physical experiments and computer simulators. Bayesianoptimisation is a cost-efficient optimisation strategy that uses surrogatemodelling via Gaussian processes to represent an objective function andacquisition functions to guide the selection of candidate points to approximatethe global optimum of the objective function. NUBO itself focuses ontransparency and user experience to make Bayesian optimisation easilyaccessible to researchers from all disciplines. Clean and understandable code,precise references, and thorough documentation ensure transparency, while userexperience is ensured by a modular and flexible design, easy-to-write syntax,and careful selection of Bayesian optimisation algorithms. NUBO allows users totailor Bayesian optimisation to their specific problem by writing theoptimisation loop themselves using the provided building blocks. It supportssequential single-point, parallel multi-point, and asynchronous optimisation ofbounded, constrained, and/or mixed (discrete and continuous) parameter inputspaces. Only algorithms and methods that are extensively tested and validatedto perform well are included in NUBO. This ensures that the package remainscompact and does not overwhelm the user with an unnecessarily large number ofoptions. The package is written in Python but does not require expert knowledgeof Python to optimise your simulators and experiments. NUBO is distributed asopen-source software under the BSD 3-Clause licence.", "output": "NUBO: A Transparent Python Package for Bayesian Optimisation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Motion represents one of the major challenges in magnetic resonance imaging(MRI). Since the MR signal is acquired in frequency space, any motion of theimaged object leads to complex artefacts in the reconstructed image in additionto other MR imaging artefacts. Deep learning has been frequently proposed formotion correction at several stages of the reconstruction process. The widerange of MR acquisition sequences, anatomies and pathologies of interest, andmotion patterns (rigid vs. deformable and random vs. regular) makes acomprehensive solution unlikely. To facilitate the transfer of ideas betweendifferent applications, this review provides a detailed overview of proposedmethods for learning-based motion correction in MRI together with their commonchallenges and potentials. This review identifies differences and synergies inunderlying data usage, architectures and evaluation strategies. We criticallydiscuss general trends and outline future directions, with the aim to enhanceinteraction between different application areas and research fields.", "output": "Deep Learning for Retrospective Motion Correction in MRI: A Comprehensive Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continuous-time models such as Neural ODEs and Neural Flows have shownpromising results in analyzing irregularly sampled time series frequentlyencountered in electronic health records. Based on these models, time seriesare typically processed with a hybrid of an initial value problem (IVP) solverand a recurrent neural network within the variational autoencoder architecture.Sequentially solving IVPs makes such models computationally less efficient. Inthis paper, we propose to model time series purely with continuous processeswhose state evolution can be approximated directly by IVPs. This eliminates theneed for recurrent computation and enables multiple states to evolve inparallel. We further fuse the encoder and decoder with one IVP solver based onits invertibility, which leads to fewer parameters and faster convergence.Experiments on three real-world datasets show that the proposed approachachieves comparable extrapolation and classification performance while gainingmore than one order of magnitude speedup over other continuous-timecounterparts.", "output": "IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Implicitly Normalized Forecaster (online mirror descent with Tsallis entropyas prox-function) is known to be an optimal algorithm for adversarialmulti-armed problems (MAB). However, most of the complexity results rely onbounded rewards or other restrictive assumptions. Recently closely relatedbest-of-both-worlds algorithm were proposed for both adversarial and stochasticheavy-tailed MAB settings. This algorithm is known to be optimal in bothsettings, but fails to exploit data fully. In this paper, we propose ImplicitlyNormalized Forecaster with clipping for MAB problems with heavy-taileddistribution on rewards. We derive convergence results under mild assumptionson rewards distribution and show that the proposed method is optimal for bothlinear and non-linear heavy-tailed stochastic MAB problems. Also we show thatalgorithm usually performs better compared to best-of-two-worlds algorithm.", "output": "Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative neural networks can produce data samples according to thestatistical properties of their training distribution. This feature can be usedto test modern computational neuroscience hypotheses suggesting thatspontaneous brain activity is partially supported by top-down generativeprocessing. A widely studied class of generative models is that of RestrictedBoltzmann Machines (RBMs), which can be used as building blocks forunsupervised deep learning architectures. In this work, we systematicallyexplore the generative dynamics of RBMs, characterizing the number of statesvisited during top-down sampling and investigating whether the heterogeneity ofvisited attractors could be increased by starting the generation process frombiased hidden states. By considering an RBM trained on a classic dataset ofhandwritten digits, we show that the capacity to produce diverse dataprototypes can be increased by initiating top-down sampling from chimerastates, which encode high-level visual features of multiple digits. We alsofound that the model is not capable of transitioning between all possible digitstates within a single generation trajectory, suggesting that the top-downdynamics is heavily constrained by the shape of the energy function.", "output": "Investigating the generative dynamics of energy-based neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vibration-based condition monitoring systems are receiving increasingattention due to their ability to accurately identify different conditions bycapturing dynamic features over a broad frequency range. However, there islittle research on clustering approaches in vibration data and the resultingsolutions are often optimized for a single data set. In this work, we presentan extensive comparison of the clustering algorithms K-means clustering,OPTICS, and Gaussian mixture model clustering (GMM) applied to statisticalfeatures extracted from the time and frequency domains of vibration data sets.Furthermore, we investigate the influence of feature combinations, featureselection using principal component analysis (PCA), and the specified number ofclusters on the performance of the clustering algorithms. We conducted thiscomparison in terms of a grid search using three different benchmark data sets.Our work showed that averaging (Mean, Median) and variance-based features(Standard Deviation, Interquartile Range) performed significantly better thanshape-based features (Skewness, Kurtosis). In addition, K-means outperformedGMM slightly for these data sets, whereas OPTICS performed significantly worse.We were also able to show that feature combinations as well as PCA featureselection did not result in any significant performance improvements. With anincrease in the specified number of clusters, clustering algorithms performedbetter, although there were some specific algorithmic restrictions.", "output": "Comparison of Clustering Algorithms for Statistical Features of Vibration Data Sets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-destructive assessments of plant phenotypic traits using high-qualitythree-dimensional (3D) and multispectral data can deepen breeders'understanding of plant growth and allow them to make informed managerialdecisions. However, subjective viewpoint selection and complex illuminationeffects under natural light conditions decrease the data quality and increasethe difficulty of resolving phenotypic parameters. We proposed methods foradaptive data acquisition and reflectance correction respectively, to generatehigh-quality 3D multispectral point clouds (3DMPCs) of plants. In the firststage, we proposed an efficient next-best-view (NBV) planning method based on anovel UGV platform with a multi-sensor-equipped robotic arm. In the secondstage, we eliminated the illumination effects by using the neural referencefield (NeREF) to predict the digital number (DN) of the reference. We testedthem on 6 perilla and 6 tomato plants, and selected 2 visible leaves and 4regions of interest (ROIs) for each plant to assess the biomass and thechlorophyll content. For NBV planning, the average execution time for singleperilla and tomato plant at a joint speed of 1.55 rad/s was 58.70 s and 53.60 srespectively. The whole-plant data integrity was improved by an average of 27%compared to using fixed viewpoints alone, and the coefficients of determination(R2) for leaf biomass estimation reached 0.99 and 0.92. For reflectancecorrection, the average root mean squared error of the reflectance spectra withhemisphere reference-based correction at different ROIs was 0.08 and 0.07 forperilla and tomato. The R2 of chlorophyll content estimation was 0.91 and 0.93respectively when principal component analysis and Gaussian process regressionwere applied. Our approach is promising for generating high-quality 3DMPCs ofplants under natural light conditions and facilitates accurate plantphenotyping.", "output": "Generating high-quality 3DMPCs by adaptive data acquisition and NeREF-based reflectance correction to facilitate efficient plant phenotyping."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Auction-based Federated Learning (AFL) has attracted extensive researchinterest due to its ability to motivate data owners to join FL through economicmeans. Existing works assume that only one data consumer and multiple dataowners exist in an AFL marketplace (i.e., a monopoly market). Therefore, dataowners bid to join the data consumer for FL. However, this assumption is notrealistic in practical AFL marketplaces in which multiple data consumers cancompete to attract data owners to join their respective FL tasks. In thispaper, we bridge this gap by proposing a first-of-its-kind utility-maximizingbidding strategy for data consumers in federated learning (Fed-Bidder). Itenables multiple FL data consumers to compete for data owners via AFLeffectively and efficiently by providing with utility estimation capabilitieswhich can accommodate diverse forms of winning functions, each reflectingdifferent market dynamics. Extensive experiments based on six commonly adoptedbenchmark datasets show that Fed-Bidder is significantly more advantageouscompared to four state-of-the-art approaches.", "output": "Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Treatment effect estimation is of high-importance for both researchers andpractitioners across many scientific and industrial domains. The abundance ofobservational data makes them increasingly used by researchers for theestimation of causal effects. However, these data suffer from biases, fromseveral weaknesses, leading to inaccurate causal effect estimations, if nothandled properly. Therefore, several machine learning techniques have beenproposed, most of them focusing on leveraging the predictive power of neuralnetwork models to attain more precise estimation of causal effects. In thiswork, we propose a new methodology, named Nearest Neighboring Information forCausal Inference (NNCI), for integrating valuable nearest neighboringinformation on neural network-based models for estimating treatment effects.The proposed NNCI methodology is applied to some of the most well establishedneural network-based models for treatment effect estimation with the use ofobservational data. Numerical experiments and analysis provide empirical andstatistical evidence that the integration of NNCI with state-of-the-art neuralnetwork models leads to considerably improved treatment effect estimations on avariety of well-known challenging benchmarks.", "output": "Integrating nearest neighbors on neural network models for treatment effect estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents an approach for data-driven policy refinement inreinforcement learning, specifically designed for safety-critical applications.Our methodology leverages the strengths of data-driven optimization andreinforcement learning to enhance policy safety and optimality throughiterative refinement. Our principal contribution lies in the mathematicalformulation of this data-driven policy refinement concept. This frameworksystematically improves reinforcement learning policies by learning fromcounterexamples surfaced during data-driven verification. Furthermore, wepresent a series of theorems elucidating key theoretical properties of ourapproach, including convergence, robustness bounds, generalization error, andresilience to model mismatch. These results not only validate the effectivenessof our methodology but also contribute to a deeper understanding of itsbehavior in different environments and scenarios.", "output": "Towards Theoretical Understanding of Data-Driven Policy Refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Schr\"{o}dinger equation with random potentials is a fundamental modelfor understanding the behaviour of particles in disordered systems. Disorderedmedia are characterised by complex potentials that lead to the localisation ofwavefunctions, also called Anderson localisation. These wavefunctions may havesimilar scales of eigenenergies which poses difficulty in their discovery. Ithas been a longstanding challenge due to the high computational cost andcomplexity of solving the Schr\"{o}dinger equation. Recently, machine-learningtools have been adopted to tackle these challenges. In this paper, based uponrecent advances in machine learning, we present a novel approach fordiscovering localised eigenstates in disordered media using physics-informedneural networks (PINNs). We focus on the spectral approximation of Hamiltoniansin one dimension with potentials that are randomly generated according to theBernoulli, normal, and uniform distributions. We introduce a novel feature tothe loss function that exploits known physical phenomena occurring in theseregions to scan across the domain and successfully discover these eigenstates,regardless of the similarity of their eigenenergies. We present variousexamples to demonstrate the performance of the proposed approach and compare itwith isogeometric analysis.", "output": "Physics-Informed Neural Networks for Discovering Localised Eigenstates in Disordered Media."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) mimics how humans and animals interact with theenvironment. The setting is somewhat idealized because, in actual tasks, otheragents in the environment have their own goals and behave adaptively to the egoagent. To thrive in those environments, the agent needs to influence otheragents so their actions become more helpful and less harmful. Research incomputational economics distills two ways to influence others directly: byproviding tangible goods (mechanism design) and by providing information(information design). This work investigates information design problems for agroup of RL agents. The main challenges are two-fold. One is the informationprovided will immediately affect the transition of the agent trajectories,which introduces additional non-stationarity. The other is the information canbe ignored, so the sender must provide information that the receivers arewilling to respect. We formulate the Markov signaling game, and develop thenotions of signaling gradient and the extended obedience constraints thataddress these challenges. Our algorithm is efficient on various mixed-motivetasks and provides further insights into computational economics. Our code isavailable at ", "output": "Information Design in Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-awareness is the key capability of autonomous systems, e.g., autonomousdriving network, which relies on highly efficient time series forecastingalgorithm to enable the system to reason about the future state of theenvironment, as well as its effect on the system behavior as time progresses.Recently, a large number of forecasting algorithms using either convolutionalneural networks or graph neural networks have been developed to exploit thecomplex temporal and spatial dependencies present in the time series. Whilethese solutions have shown significant advantages over statistical approaches,one open question is to effectively incorporate the global information whichrepresents the seasonality patterns via the time component of time series intothe forecasting models to improve their accuracy. This paper presents a generalapproach to integrating the time component into forecasting models. The mainidea is to employ conditional neural fields to represent the auxiliary featuresextracted from the time component to obtain the global information, which willbe effectively combined with the local information extracted fromautoregressive neural networks through a layer-wise gated fusion module.Extensive experiments on road traffic and cellular network traffic datasetsprove the effectiveness of the proposed approach.", "output": "A Generic Approach to Integrating Time into Spatial-Temporal Forecasting via Conditional Neural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Direct policy optimization in reinforcement learning is usually solved withpolicy-gradient algorithms, which optimize policy parameters via stochasticgradient ascent. This paper provides a new theoretical interpretation andjustification of these algorithms. First, we formulate direct policyoptimization in the optimization by continuation framework. The latter is aframework for optimizing nonconvex functions where a sequence of surrogateobjective functions, called continuations, are locally optimized. Second, weshow that optimizing affine Gaussian policies and performing entropyregularization can be interpreted as implicitly optimizing deterministicpolicies by continuation. Based on these theoretical results, we argue thatexploration in policy-gradient algorithms consists in computing a continuationof the return of the policy at hand, and that the variance of policies shouldbe history-dependent functions adapted to avoid local extrema rather than tomaximize the return of the policy.", "output": "Policy Gradient Algorithms Implicitly Optimize by Continuation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a general framework for visualizing any intermediate embeddingrepresentation used by any neural survival analysis model. Our framework isbased on so-called anchor directions in an embedding space. We show how toestimate these anchor directions using clustering or, alternatively, usinguser-supplied \"concepts\" defined by collections of raw inputs (e.g., featurevectors all from female patients could encode the concept \"female\"). Fortabular data, we present visualization strategies that reveal how anchordirections relate to raw clinical features and to survival time distributions.We then show how these visualization ideas extend to handling raw inputs thatare images. Our framework is built on looking at angles between vectors in anembedding space, where there could be \"information loss\" by ignoring magnitudeinformation. We show how this loss results in a \"clumping\" artifact thatappears in our visualizations, and how to reduce this information loss inpractice.", "output": "A General Framework for Visualizing Embedding Spaces of Neural Survival Analysis Models Based on Angular Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL), which addresses data privacy issues by trainingmodels on resource-constrained mobile devices in a distributed manner, hasattracted significant research attention. However, the problem of optimizing FLclient selection in mobile federated learning networks (MFLNs), where devicesmove in and out of each others' coverage and no FL server knows all the dataowners, remains open. To bridge this gap, we propose a first-of-its-kindunderline{Soc}ially-aware underline{Fed}erated underline{C}lientunderline{S}election (SocFedCS) approach to minimize costs and trainhigh-quality FL models. SocFedCS enriches the candidate FL client pool byenabling data owners to propagate FL task information through their localnetworks of trust, even as devices are moving into and out of each others'coverage. Based on Lyapunov optimization, we first transform this time-coupledproblem into a step-by-step optimization problem. Then, we design a methodbased on alternating minimization and self-adaptive global best harmony searchto solve this mixed-integer optimization problem. Extensive experimentscomparing SocFedCS against five state-of-the-art approaches based on fourreal-world multimedia datasets demonstrate that it achieves 2.06% higher testaccuracy and 12.24% lower cost on average than the best-performing baseline.", "output": "Multi-Tier Client Selection for Mobile Federated Learning Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convex optimization methods have been extensively used in the fields ofcommunications and signal processing. However, the theory of quaternionoptimization is currently not as fully developed and systematic as that ofcomplex and real optimization. To this end, we establish an essential theory ofconvex quaternion optimization for signal processing based on the generalizedHamilton-real (GHR) calculus. This is achieved in a way which conforms withtraditional complex and real optimization theory. For rigorous, We present fivediscriminant theorems for convex quaternion functions, and four discriminantcriteria for strongly convex quaternion functions. Furthermore, we provide afundamental theorem for the optimality of convex quaternion optimizationproblems, and demonstrate its utility through three applications in quaternionsignal processing. These results provide a solid theoretical foundation forconvex quaternion optimization and open avenues for further developments insignal processing applications.", "output": "Convex Quaternion Optimization for Signal Processing: Theory and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce the notion of a risk-limiting financial auditing (RLFA): given$N$ transactions, the goal is to estimate the total misstated monetaryfraction~($m^*$) to a given accuracy $epsilon$, with confidence $1-delta$. Wedo this by constructing new confidence sequences (CSs) for the weighted averageof $N$ unknown values, based on samples drawn without replacement according toa (randomized) weighted sampling scheme. Using the idea of importance weightingto construct test martingales, we first develop a framework to construct CSsfor arbitrary sampling strategies. Next, we develop methods to improve thequality of CSs by incorporating side information about the unknown valuesassociated with each item. We show that when the side information issufficiently predictive, it can directly drive the sampling. Addressing thecase where the accuracy is unknown a priori, we introduce a method thatincorporates side information via control variates. Crucially, our constructionis adaptive: if the side information is highly predictive of the unknownmisstated amounts, then the benefits of incorporating it are significant; butif the side information is uncorrelated, our methods learn to ignore it. Ourmethods recover state-of-the-art bounds for the special case when the weightsare equal, which has already found applications in election auditing. Theharder weighted case solves our more challenging problem of AI-assistedfinancial auditing.", "output": "Risk-limiting Financial Audits via Weighted Sampling without Replacement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Disentangling the factors of variation in data is a fundamental concept inmachine learning and has been studied in various ways by different researchers,leading to a multitude of definitions. Despite the numerous empirical studies,more theoretical research is needed to fully understand the defining propertiesof disentanglement and how different definitions relate to each other. Thispaper presents a meta-analysis of existing definitions of disentanglement,using category theory as a unifying and rigorous framework. We propose that theconcepts of the cartesian and monoidal products should serve as the core ofdisentanglement. With these core concepts, we show the similarities and crucialdifferences in dealing with (i) functions, (ii) equivariant maps, (iii)relations, and (iv) stochastic maps. Overall, our meta-analysis deepens ourunderstanding of disentanglement and its various formulations and can helpresearchers navigate different definitions and choose the most appropriate onefor their specific context.", "output": "A Category-theoretical Meta-analysis of Definitions of Disentanglement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "If $X,Y,Z$ denote sets of random variables, two different data sources maycontain samples from $P_{X,Y}$ and $P_{Y,Z}$, respectively. We argue thatcausal discovery can help inferring properties of the `unobserved jointdistributions' $P_{X,Y,Z}$ or $P_{X,Z}$. The properties may be conditionalindependences (as in `integrative causal inference') or also quantitativestatements about dependences.More generally, we define a learning scenario where the input is a subset ofvariables and the label is some statistical property of that subset. Sets ofjointly observed variables define the training points, while unobserved setsare possible test points. To solve this learning task, we infer, as anintermediate step, a causal model from the observations that then entailsproperties of unobserved sets. Accordingly, we can define the VC dimension of aclass of causal models and derive generalization bounds for the predictions.Here, causal discovery becomes more modest and better accessible to empiricaltests than usual: rather than trying to find a causal hypothesis that is `true'a causal hypothesis is {it useful} whenever it correctly predicts statisticalproperties of unobserved joint distributions. This way, a sparse causal graphthat omits weak influences may be more useful than a dense one (despite beingless accurate) because it is able to reconstruct the full joint distributionfrom marginal distributions of smaller subsets.Within such a `pragmatic' application of causal discovery, some popularheuristic approaches become justified in retrospect. It is, for instance,allowed to infer DAGs from partial correlations instead of conditionalindependences if the DAGs are only used to predict partial correlations.", "output": "Reinterpreting causal discovery as the task of predicting unobserved joint statistics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusion probabilistic models (DDPMs) have shown promisingperformance for speech synthesis. However, a large number of iterative stepsare required to achieve high sample quality, which restricts the inferencespeed. Maintaining sample quality while increasing sampling speed has become achallenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based\"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through asingle diffusion sampling step while achieving high audio quality. Theconsistency constraint is applied to distill a consistency model from awell-designed diffusion-based teacher model, which ultimately yields superiorperformances in the distilled CoMoSpeech. Our experiments show that bygenerating audio recordings by a single sampling step, the CoMoSpeech achievesan inference speed more than 150 times faster than real-time on a single NVIDIAA100 GPU, which is comparable to FastSpeech2, making diffusion-sampling basedspeech synthesis truly practical. Meanwhile, objective and subjectiveevaluations on text-to-speech and singing voice synthesis show that theproposed teacher models yield the best audio quality, and the one-step samplingbased CoMoSpeech achieves the best inference speed with better or comparableaudio quality to other conventional multi-step diffusion model baselines. Audiosamples are available at ", "output": "CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most uses of Meta-Learning in visual recognition are very often applied toimage classification, with a relative lack of works in other tasks {such} assegmentation and detection. We propose a generic Meta-Learning framework forfew-shot weakly-supervised segmentation in medical imaging domains. We conducta comparative analysis of meta-learners from distinct paradigms adapted tofew-shot image segmentation in different sparsely annotated radiological tasks.The imaging modalities include 2D chest, mammographic and dental X-rays, aswell as 2D slices of volumetric tomography and resonance images. Ourexperiments consider a total of 9 meta-learners, 4 backbones and multipletarget organ segmentation tasks. We explore small-data scenarios in radiologywith varying weak annotation styles and densities. Our analysis shows thatmetric-based meta-learning approaches achieve better segmentation results intasks with smaller domain shifts in comparison to the meta-training datasets,while some gradient- and fusion-based meta-learners are more generalizable tolarger domain shifts.", "output": "Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the unavailability of routing information in design stages prior todetailed routing (DR), the tasks of timing prediction and optimization posemajor challenges. Inaccurate timing prediction wastes design effort, hurtscircuit performance, and may lead to design failure. This work focuses ontiming prediction after clock tree synthesis and placement legalization, whichis the earliest opportunity to time and optimize a \"complete\" netlist. Thepaper first documents that having \"oracle knowledge\" of the final post-DRparasitics enables post-global routing (GR) optimization to produce improvedfinal timing outcomes. To bridge the gap between GR-based parasitic and timingestimation and post-DR results during post-GR optimization, machine learning(ML)-based models are proposed, including the use of features for macroblockages for accurate predictions for designs with macros. Based on a set ofexperimental evaluations, it is demonstrated that these models show higheraccuracy than GR-based timing estimation. When used during post-GRoptimization, the ML-based models show demonstrable improvements in post-DRcircuit performance. The methodology is applied to two different tool flows -OpenROAD and a commercial tool flow - and results on 45nm bulk and 12nm FinFETenablements show improvements in post-DR slack metrics without increasingcongestion. The models are demonstrated to be generalizable to designsgenerated under different clock period constraints and are robust to trainingdata with small levels of noise.", "output": "A Machine Learning Approach to Improving Timing Consistency between Global Route and Detailed Route."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Identifying the underlying dynamics of physical systems can be challengingwhen only provided with observational data. In this work, we consider systemsthat can be modelled as first-order ordinary differential equations. Byassuming a certain pseudo-Hamiltonian formulation, we are able to learn theanalytic terms of internal dynamics even if the model is trained on data wherethe system is affected by unknown damping and external disturbances. In caseswhere it is difficult to find analytic terms for the disturbances, a hybridmodel that uses a neural network to learn these can still accurately identifythe dynamics of the system as if under ideal conditions. This makes the modelsapplicable in situations where other system identification models fail.Furthermore, we propose to use a fourth-order symmetric integration scheme inthe loss function and avoid actual integration in the training, and demonstrateon varied examples how this leads to increased performance on noisy data.", "output": "Pseudo-Hamiltonian system identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This two-part paper develops a paradigmatic theory and detailed methods ofthe joint electricity market design using reinforcement-learning (RL)-basedsimulation. In Part 2, this theory is further demonstrated by elaboratingdetailed methods of designing an electricity spot market (ESM), together with areserved capacity product (RC) in the ancillary service market (ASM) and avirtual bidding (VB) product in the financial market (FM). Following the theoryproposed in Part 1, firstly, market design options in the joint market arespecified. Then, the Markov game model is developed, in which we show how toincorporate market design options and uncertain risks in model formulation. Amulti-agent policy proximal optimization (MAPPO) algorithm is elaborated, as apractical implementation of the generalized market simulation method developedin Part 1. Finally, the case study demonstrates how to pick the best marketdesign options by using some of the market operation performance indicatorsproposed in Part 1, based on the simulation results generated by implementingthe MAPPO algorithm. The impacts of different market design options on marketparticipants' bidding strategy preference are also discussed.", "output": "How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Nash Equilibrium (NE) estimation in bidding games of electricity marketsis the key concern of both generation companies (GENCOs) for bidding strategyoptimization and the Independent System Operator (ISO) for market surveillance.However, existing methods for NE estimation in emerging modern electricitymarkets (FEM) are inaccurate and inefficient because the priori knowledge ofbidding strategies before any environment changes, such as load demandvariations, network congestion, and modifications of market design, is notfully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM(BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategyoptimization considering the priori knowledge. A novel Multi-Agent GenerativeAdversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed toenable GENCOs to learn simultaneously from priori knowledge and interactionswith changing environments. The obtained NE is a Bayesian Nash Equilibrium(BNE) with priori knowledge transferred from the previous environment. In thecase study, the superiority of this proposed algorithm in terms of convergencespeed compared with conventional methods is verified. It is concluded that theoptimal bidding strategies in the obtained BNE can always lead to more profitsthan NE due to the effective learning from the priori knowledge. Also, BNE ismore accurate and consistent with situations in real-world markets.", "output": "An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The properties of lithium metal are key parameters in the design of lithiumion and lithium metal batteries. They are difficult to probe experimentally dueto the high reactivity and low melting point of lithium as well as themicroscopic scales at which lithium exists in batteries where it is found tohave enhanced strength, with implications for dendrite suppression strategies.Computationally, there is a lack of empirical potentials that are consistentlyquantitatively accurate across all properties and ab-initio calculations aretoo costly. In this work, we train Machine Learning Interaction Potentials(MLIPs) on Density Functional Theory (DFT) data to state-of-the-art accuracy inreproducing experimental and ab-initio results across a wide range ofsimulations at large length and time scales. We accurately predictthermodynamic properties, phonon spectra, temperature dependence of elasticconstants and various surface properties inaccessible using DFT. We establishthat there exists a Bell-Evans-Polanyi relation correlating the self-adsorptionenergy and the minimum surface diffusion barrier for high Miller index facets.", "output": "Accurate Surface and Finite Temperature Bulk Properties of Lithium Metal at Large Scales using Machine Learning Interaction Potentials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider alternating gradient descent (AGD) with fixed step size $eta &gt;0$, applied to the asymmetric matrix factorization objective. We show that, fora rank-$r$ matrix $mathbf{A} in mathbb{R}^{m times n}$, $T = left(left(frac{sigma_1(mathbf{A})}{sigma_r(mathbf{A})}right)^2log(1/epsilon)right)$ iterations of alternating gradient descent suffice toreach an $epsilon$-optimal factorization $| mathbf{A} -mathbf{X}_T^{vphantom{intercal}} mathbf{Y}_T^{intercal} |_{rm F}^2 leqepsilon | mathbf{A} |_{rm F}^2$ with high probability starting from anatypical random initialization. The factors have rank $d&gt;r$ so that$mathbf{X}_Tinmathbb{R}^{m times d}$ and $mathbf{Y}_T inmathbb{R}^{ntimes d}$. Experiments suggest that our proposed initialization is not merelyof theoretical benefit, but rather significantly improves convergence ofgradient descent in practice. Our proof is conceptually simple: a uniformPL-inequality and uniform Lipschitz smoothness constant are guaranteed for asufficient number of iterations, starting from our random initialization. Ourproof method should be useful for extending and simplifying convergenceanalyses for a broader class of nonconvex low-rank factorization problems.", "output": "Convergence of Alternating Gradient Descent for Matrix Factorization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since the release of ChatGPT, numerous studies have highlighted theremarkable performance of ChatGPT, which often rivals or even surpasses humancapabilities in various tasks and domains. However, this paper presents acontrasting perspective by demonstrating an instance where human performanceexcels in typical tasks suited for ChatGPT, specifically in the domain ofcomputer programming. We utilize the IEEExtreme Challenge competition as abenchmark, a prestigious, annual international programming contest encompassinga wide range of problems with different complexities. To conduct a thoroughevaluation, we selected and executed a diverse set of 102 challenges, drawnfrom five distinct IEEExtreme editions, using three major programminglanguages: Python, Java, and C++. Our empirical analysis provides evidence thatcontrary to popular belief, human programmers maintain a competitive edge overChatGPT in certain aspects of problem-solving within the programming context.In fact, we found that the average score obtained by ChatGPT on the set ofIEEExtreme programming problems is 3.9 to 5.8 times lower than the averagehuman score, depending on the programming language. This paper elaborates onthese findings, offering critical insights into the limitations and potentialareas of improvement for AI-based language models like ChatGPT.", "output": "Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A large variety of real-world Reinforcement Learning (RL) tasks ischaracterized by a complex and heterogeneous structure that makes end-to-end(or flat) approaches hardly applicable or even infeasible. HierarchicalReinforcement Learning (HRL) provides general solutions to address theseproblems thanks to a convenient multi-level decomposition of the tasks, makingtheir solution accessible. Although often used in practice, few works providetheoretical guarantees to justify this outcome effectively. Thus, it is not yetclear when to prefer such approaches compared to standard flat ones. In thiswork, we provide an option-dependent upper bound to the regret suffered byregret minimization algorithms in finite-horizon problems. We illustrate thatthe performance improvement derives from the planning horizon reduction inducedby the temporal abstraction enforced by the hierarchical structure. Then,focusing on a sub-setting of HRL approaches, the options framework, wehighlight how the average duration of the available options affects theplanning horizon and, consequently, the regret itself. Finally, we relax theassumption of having pre-trained options to show how in particular situations,learning hierarchically from scratch could be preferable to using a standardapproach.", "output": "An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-Markov Decision Processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep multi-view subspace clustering (DMVSC) has recently attracted increasingattention due to its promising performance. However, existing DMVSC methodsstill have two issues: (1) they mainly focus on using autoencoders tononlinearly embed the data, while the embedding may be suboptimal forclustering because the clustering objective is rarely considered inautoencoders, and (2) existing methods typically have a quadratic or even cubiccomplexity, which makes it challenging to deal with large-scale data. Toaddress these issues, in this paper we propose a novel deep multi-view subspaceclustering method with anchor graph (DMCAG). To be specific, DMCAG firstlylearns the embedded features for each view independently, which are used toobtain the subspace representations. To significantly reduce the complexity, weconstruct an anchor graph with small size for each view. Then, spectralclustering is performed on an integrated anchor graph to obtain pseudo-labels.To overcome the negative impact caused by suboptimal embedded features, we usepseudo-labels to refine the embedding process to make it more suitable for theclustering task. Pseudo-labels and embedded features are updated alternately.Furthermore, we design a strategy to keep the consistency of the labels basedon contrastive learning to enhance the clustering performance. Empiricalstudies on real-world datasets show that our method achieves superiorclustering performance over other state-of-the-art methods.", "output": "Deep Multi-View Subspace Clustering with Anchor Graph."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Whole-Slide Imaging allows for the capturing and digitization ofhigh-resolution images of histological specimen. An automated analysis of suchimages using deep learning models is therefore of high demand. The transformerarchitecture has been proposed as a possible candidate for effectivelyleveraging the high-resolution information. Here, the whole-slide image ispartitioned into smaller image patches and feature tokens are extracted fromthese image patches. However, while the conventional transformer allows for asimultaneous processing of a large set of input tokens, the computationaldemand scales quadratically with the number of input tokens and thusquadratically with the number of image patches. To address this problem wepropose a novel cascaded cross-attention network (CCAN) based on thecross-attention mechanism that scales linearly with the number of extractedpatches. Our experiments demonstrate that this architecture is at least on-parwith and even outperforms other attention-based state-of-the-art methods on twopublic datasets: On the use-case of lung cancer (TCGA NSCLC) our model reachesa mean area under the receiver operating characteristic (AUC) of 0.970 $pm$0.008 and on renal cancer (TCGA RCC) reaches a mean AUC of 0.985 $pm$ 0.004.Furthermore, we show that our proposed model is efficient in low-data regimes,making it a promising approach for analyzing whole-slide images inresource-limited settings. To foster research in this direction, we make ourcode publicly available on GitHub: XXX.", "output": "Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The widespread adoption of Machine Learning systems, especially in moredecision-critical applications such as criminal sentencing and bank loans, hasled to increased concerns about fairness implications. Algorithms and metricshave been developed to mitigate and measure these discriminations. Morerecently, works have identified a more challenging form of bias calledintersectional bias, which encompasses multiple sensitive attributes, such asrace and gender, together. In this survey, we review the state-of-the-art inintersectional fairness. We present a taxonomy for intersectional notions offairness and mitigation. Finally, we identify the key challenges and provideresearchers with guidelines for future directions.", "output": "A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the remarkable ability of large language models (LMs) to comprehendand generate language, they have a tendency to hallucinate and create factuallyinaccurate output. Augmenting LMs by retrieving information from externalknowledge resources is one promising solution. Most existingretrieval-augmented LMs employ a retrieve-and-generate setup that onlyretrieves information once based on the input. This is limiting, however, inmore general scenarios involving generation of long texts, where continuallygathering information throughout the generation process is essential. Therehave been some past efforts to retrieve information multiple times whilegenerating outputs, which mostly retrieve documents at fixed intervals usingthe previous context as queries. In this work, we provide a generalized view ofactive retrieval augmented generation, methods that actively decide when andwhat to retrieve across the course of the generation. We proposeForward-Looking Active REtrieval augmented generation (FLARE), a genericretrieval-augmented generation method which iteratively uses a prediction ofthe upcoming sentence to anticipate future content, which is then utilized as aquery to retrieve relevant documents to regenerate the sentence if it containslow-confidence tokens. We test FLARE along with baselines comprehensively over4 long-form knowledge-intensive generation tasks/datasets. FLARE achievessuperior or competitive performance on all tasks, demonstrating theeffectiveness of our method. Code and datasets are available at", "output": "Active Retrieval Augmented Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the central questions in the theory of deep learning is to understandhow neural networks learn hierarchical features. The ability of deep networksto extract salient features is crucial to both their outstanding generalizationability and the modern deep learning paradigm of pretraining and finetuneing.However, this feature learning process remains poorly understood from atheoretical perspective, with existing analyses largely restricted to two-layernetworks. In this work we show that three-layer neural networks have provablyricher feature learning capabilities than two-layer networks. We analyze thefeatures learned by a three-layer network trained with layer-wise gradientdescent, and present a general purpose theorem which upper bounds the samplecomplexity and width needed to achieve low test error when the target hasspecific hierarchical structure. We instantiate our framework in specificstatistical learning settings -- single-index models and functions of quadraticfeatures -- and show that in the latter setting three-layer networks obtain asample complexity improvement over all existing guarantees for two-layernetworks. Crucially, this sample complexity improvement relies on the abilityof three-layer networks to efficiently learn nonlinear features. We thenestablish a concrete optimization-based depth separation by constructing afunction which is efficiently learnable via gradient descent on a three-layernetwork, yet cannot be learned efficiently by a two-layer network. Our workmakes progress towards understanding the provable benefit of three-layer neuralnetworks over two-layer networks in the feature learning regime.", "output": "Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have shown promising results on utilizing pre-trainedimage-language models for video question answering. While these image-languagemodels can efficiently bootstrap the representation learning of video-languagemodels, they typically concatenate uniformly sampled video frames as visualinputs without explicit language-aware, temporal modeling. When only a portionof a video input is relevant to the language query, such uniform frame samplingcan often lead to missing important visual cues. Although humans often find avideo moment to focus on and rewind the moment to answer questions, training aquery-aware video moment localizer often requires expensive annotations andhigh computational costs. To address this issue, we propose Self-Chained VideoLocalization-Answering (SeViLA), a novel framework that leverages a singleimage-language model (BLIP-2) to tackle both temporal keyframe localization andQA on videos. SeViLA framework consists of two modules: Localizer and Answerer,where both are parameter-efficiently fine-tuned from BLIP-2. We chain thesemodules for cascaded inference and self-refinement. First, in the forwardchain, the Localizer finds multiple language-aware keyframes in a video, whichthe Answerer uses to predict the answer. Second, in the reverse chain, theAnswerer generates keyframe pseudo-labels to refine the Localizer, alleviatingthe need for expensive video moment localization annotations. SeViLAoutperforms several strong baselines/previous works on five video QA and eventprediction tasks, and achieves the state-of-the-art in both fine-tuning(NExT-QA, STAR) and zero-shot (NExT-QA, STAR, How2QA, VLEP) settings. We show acomprehensive analysis, e.g., the impact of Localizer, comparisons of Localizerwith other temporal localization models, pre-training/self-refinement ofLocalizer, and varying the number of keyframes.", "output": "Self-Chained Image-Language Model for Video Localization and Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding superfluidity remains a major goal of condensed matter physics.Here we tackle this challenge utilizing the recently developed Fermionic neuralnetwork (FermiNet) wave function Ansatz for variational Monte Carlocalculations. We study the unitary Fermi gas, a system with strong,short-range, two-body interactions known to possess a superfluid ground statebut difficult to describe quantitively. We demonstrate key limitations of theFermiNet Ansatz in studying the unitary Fermi gas and propose a simplemodification that outperforms the original FermiNet significantly, givinghighly accurate results. We prove mathematically that the new Ansatz is astrict generalization of the original FermiNet architecture, despite the use offewer parameters. Our approach shares several advantanges with the FermiNet:the use of a neural network removes the need for an underlying basis set; andthe flexiblity of the network yields extremely accurate results within avariational quantum Monte Carlo framework that provides access to unbiasedestimates of arbitrary ground-state expectation values. We discuss how themethod can be extended to study other superfluids.", "output": "Neural Wave Functions for Superfluids."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of machine learning models in decision support systems with highsocietal impact raised concerns about unfair (disparate) results for differentgroups of people. When evaluating such unfair decisions, one generally relieson predefined groups that are determined by a set of features that areconsidered sensitive. However, such an approach is subjective and does notguarantee that these features are the only ones to be considered as sensitivenor that they entail unfair (disparate) outcomes.In this paper, we propose a preprocessing step to address the task ofautomatically recognizing sensitive features that does not require a trainedmodel to verify unfair results. Our proposal is based on the Hilber-Schmidtindependence criterion, which measures the statistical dependence of variabledistributions. We hypothesize that if the dependence between the label vectorand a candidate is high for a sensitive feature, then the information providedby this feature will entail disparate performance measures between groups. Ourempirical results attest our hypothesis and show that several featuresconsidered as sensitive in the literature do not necessarily entail disparate(unfair) results.", "output": "A statistical approach to detect sensitive features in a group fairness setting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning the optimal ordering of content is an important challenge in websitedesign. The learning to rank (LTR) framework models this problem as asequential problem of selecting lists of content and observing where usersdecide to click. Most previous work on LTR assumes that the user considers eachitem in the list in isolation, and makes binary choices to click or not oneach. We introduce a multinomial logit (MNL) choice model to the LTR framework,which captures the behaviour of users who consider the ordered list of items asa whole and make a single choice among all the items and a no-click option.Under the MNL model, the user favours items which are either inherently moreattractive, or placed in a preferable position within the list. We proposeupper confidence bound (UCB) algorithms to minimise regret in two settings -where the position dependent parameters are known, and unknown. We presenttheoretical analysis leading to an $Omega(sqrt{JT})$ lower bound for theproblem, an $tilde{O}(sqrt{JT})$ upper bound on regret of the UCB algorithmin the known-parameter setting, and an $tilde{O}(K^2sqrt{JT})$ upper bound onregret, the first, in the more challenging unknown-position-parameter setting.Our analyses are based on tight new concentration results for Geometric randomvariables, and novel functional inequalities for maximum likelihood estimatorscomputed on discrete data.", "output": "Learning to Rank under Multinomial Logit Choice."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "False data injection attacks (FDIAs) pose a significant security threat topower system state estimation. To detect such attacks, recent studies haveproposed machine learning (ML) techniques, particularly deep neural networks(DNNs). However, most of these methods fail to account for the risk posed byadversarial measurements, which can compromise the reliability of DNNs invarious ML applications. In this paper, we present a DNN-based FDIA detectionapproach that is resilient to adversarial attacks. We first analyze severaladversarial defense mechanisms used in computer vision and show their inherentlimitations in FDIA detection. We then propose an adversarial-resilient DNNdetection framework for FDIA that incorporates random input padding in both thetraining and inference phases. Our simulations, based on an IEEE standard powersystem, demonstrate that this framework significantly reduces the effectivenessof adversarial attacks while having a negligible impact on the DNNs' detectionperformance.", "output": "Towards Adversarial-Resilient Deep Neural Networks for False Data Injection Attack Detection in Power Grids."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing risk-aware multi-armed bandit models typically focus on riskmeasures of individual options such as variance. As a result, they cannot bedirectly applied to important real-world online decision making problems withcorrelated options. In this paper, we propose a novel ContinuousMean-Covariance Bandit (CMCB) model to explicitly take into account optioncorrelation. Specifically, in CMCB, there is a learner who sequentially choosesweight vectors on given options and observes random feedback according to thedecisions. The agent's objective is to achieve the best trade-off betweenreward and risk, measured with option covariance. To capture different rewardobservation scenarios in practice, we consider three feedback settings, i.e.,full-information, semi-bandit and full-bandit feedback. We propose novelalgorithms with optimal regrets (within logarithmic factors), and providematching lower bounds to validate their optimalities. The experimental resultsalso demonstrate the superiority of our algorithms. To the best of ourknowledge, this is the first work that considers option correlation inrisk-aware bandits and explicitly quantifies how arbitrary covariancestructures impact the learning performance. The novel analytical techniques wedeveloped for exploiting the estimated covariance to build concentration andbounding the risk of selected actions based on sampling strategy properties canlikely find applications in other bandit analysis and be of independentinterests.", "output": "Continuous Mean-Covariance Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Finding the seed set that maximizes the influence spread over a network is awell-known NP-hard problem. Though a greedy algorithm can provide near-optimalsolutions, the subproblem of influence estimation renders the solutionsinefficient. In this work, we propose textsc{Glie}, a graph neural networkthat learns how to estimate the influence spread of the independent cascade.GLIE relies on a theoretical upper bound that is tightened through supervisedtraining.Experiments indicate that it provides accurate influence estimationfor real graphs up to 10 times larger than the train set.Subsequently, weincorporate it into three influence maximization techniques.We first utilizeCost Effective Lazy Forward optimization substituting Monte Carlo simulationswith GLIE, surpassing the benchmarks albeit with a computational overhead. Toimprove computational efficiency we first devise a Q-learning method thatlearns to choose seeds sequentially using GLIE's predictions. Finally, wearrive at the most efficient approach by developing a provably submodularinfluence spread based on GLIE's representations, to rank nodes while buildingthe seed set adaptively. The proposed algorithms are inductive, meaning theyare trained on graphs with less than 300 nodes and up to 5 seeds, and tested ongraphs with millions of nodes and up to 200 seeds. The final method exhibitsthe most promising combination of time efficiency and influence quality,outperforming several baselines.", "output": "Maximizing Influence with Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Peer reviewed publications are considered the gold standard in certifying anddisseminating ideas that a research community considers valuable. However, weidentify two major drawbacks of the current system: (1) the overwhelming demandfor reviewers due to a large volume of submissions, and (2) the lack ofincentives for reviewers to participate and expend the necessary effort toprovide high-quality reviews. In this work, we adopt a mechanism-designapproach to propose improvements to the peer review process, tying together thepaper submission and review processes and simultaneously incentivizinghigh-quality submissions and reviews. In the submission stage, authorsparticipate in a VCG auction for review slots by submitting their papers alongwith a bid that represents their expected value for having their paperreviewed. For the reviewing stage, we propose a novel peer prediction mechanism(H-DIPP) building on recent work in the information elicitation literature,which incentivizes participating reviewers to provide honest and effortfulreviews. The revenue raised in the submission stage auction is used to payreviewers based on the quality of their reviews in the reviewing stage.", "output": "Auctions and Peer Prediction for Academic Peer Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Koopman operators are infinite-dimensional operators that globally linearizenonlinear dynamical systems, making their spectral information valuable forunderstanding dynamics. However, Koopman operators can have continuous spectraand infinite-dimensional invariant subspaces, making computing their spectralinformation a considerable challenge. This paper describes data-drivenalgorithms with rigorous convergence guarantees for computing spectralinformation of Koopman operators from trajectory data. We introduce residualdynamic mode decomposition (ResDMD), which provides the first scheme forcomputing the spectra and pseudospectra of general Koopman operators fromsnapshot data without spectral pollution. Using the resolvent operator andResDMD, we compute smoothed approximations of spectral measures associated withgeneral measure-preserving dynamical systems. We prove explicit convergencetheorems for our algorithms, which can achieve high-order convergence even forchaotic systems when computing the density of the continuous spectrum and thediscrete spectrum. Since our algorithms come with error control, ResDMD allowsaposteri verification of spectral quantities, Koopman mode decompositions, andlearned dictionaries. We demonstrate our algorithms on the tent map, circlerotations, Gauss iterated map, nonlinear pendulum, double pendulum, and Lorenzsystem. Finally, we provide kernelized variants of our algorithms for dynamicalsystems with a high-dimensional state space. This allows us to compute thespectral measure associated with the dynamics of a protein molecule with a20,046-dimensional state space and compute nonlinear Koopman modes with errorbounds for turbulent flow past aerofoils with Reynolds number $&gt;10^5$ that hasa 295,122-dimensional state space.", "output": "Rigorous data-driven computation of spectral properties of Koopman operators for dynamical systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the upper confidence bound strategy for Gaussian multi-armedbandits with known control horizon sizes $N$ and build its limiting descriptionwith a system of stochastic differential equations and ordinary differentialequations. Rewards for the arms are assumed to have unknown expected values andknown variances. A set of Monte-Carlo simulations was performed for the case ofclose distributions of rewards, when mean rewards differ by the magnitude oforder $N^{-1/2}$, as it yields the highest normalized regret, to verify thevalidity of the obtained description. The minimal size of the control horizonwhen the normalized regret is not noticeably larger than maximum possible wasestimated.", "output": "Stochastic differential equations for limiting description of UCB rule for Gaussian multi-armed bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advances in autonomy offer the potential for dramatic positive outcomes in anumber of domains, yet enabling their safe deployment remains an open problem.This work's motivating question is: In safety-critical settings, can we avoidthe need to have one human supervise one machine at all times? The workformalizes this scalable supervision problem by considering remotely locatedhuman supervisors and investigating how autonomous agents can cooperate toachieve safety. This article focuses on the safety-critical context ofautonomous vehicles (AVs) merging into traffic consisting of a mixture of AVsand human drivers. The analysis establishes high reliability upper bounds onhuman supervision requirements. It further shows that AV cooperation canimprove supervision reliability by orders of magnitude and counterintuitivelyrequires fewer supervisors (per AV) as more AVs are adopted. These analyticalresults leverage queuing-theoretic analysis, order statistics, and aconservative, reachability-based approach. A key takeaway is the potentialvalue of cooperation in enabling the deployment of autonomy at scale. Whilethis work focuses on AVs, the scalable supervision framework may be ofindependent interest to a broader array of autonomous control challenges.", "output": "Cooperation for Scalable Supervision of Autonomy in Mixed Traffic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the quantum computing community gravitates towards understanding thepractical benefits of quantum computers, having a clear definition andevaluation scheme for assessing practical quantum advantage in the context ofspecific applications is paramount. Generative modeling, for example, is awidely accepted natural use case for quantum computers, and yet has lacked aconcrete approach for quantifying success of quantum models over classicalones. In this work, we construct a simple and unambiguous approach to probepractical quantum advantage for generative modeling by measuring thealgorithm's generalization performance. Using the sample-based approachproposed here, any generative model, from state-of-the-art classical generativemodels such as GANs to quantum models such as Quantum Circuit Born Machines,can be evaluated on the same ground on a concrete well-defined framework. Incontrast to other sample-based metrics for probing practical generalization, weleverage constrained optimization problems (e.g., cardinality-constrainedproblems) and use these discrete datasets to define specific metrics capable ofunambiguously measuring the quality of the samples and the model'sgeneralization capabilities for generating data beyond the training set butstill within the valid solution space. Additionally, our metrics can diagnosetrainability issues such as mode collapse and overfitting, as we illustratewhen comparing GANs to quantum-inspired models built out of tensor networks.Our simulation results show that our quantum-inspired models have up to a $68times$ enhancement in generating unseen unique and valid samples compared toGANs, and a ratio of 61:2 for generating samples with better quality than thoseobserved in the training set. We foresee these metrics as valuable tools forrigorously defining practical quantum advantage in the domain of generativemodeling.", "output": "Generalization Metrics for Practical Quantum Advantage in Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum density matrix represents all the information of the entire quantumsystem, and novel models of meaning employing density matrices naturally modellinguistic phenomena such as hyponymy and linguistic ambiguity, among others inquantum question answering tasks. Naturally, we argue that applying the quantumdensity matrix into classical Question Answering (QA) tasks can show moreeffective performance. Specifically, we (i) design a new mechanism based onLong Short-Term Memory (LSTM) to accommodate the case when the inputs arematrixes; (ii) apply the new mechanism to QA problems with Convolutional NeuralNetwork (CNN) and gain the LSTM-based QA model with the quantum density matrix.Experiments of our new model on TREC-QA and WIKI-QA data sets show encouragingresults. Similarly, we argue that the quantum density matrix can also enhancethe image feature information and the relationship between the features for theclassical image classification. Thus, we (i) combine density matrices and CNNto design a new mechanism; (ii) apply the new mechanism to some representativeclassical image classification tasks. A series of experiments show that theapplication of quantum density matrix in image classification has thegeneralization and high efficiency on different datasets. The application ofquantum density matrix both in classical question answering tasks and classicalimage classification tasks show more effective performance.", "output": "Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern language models can generate high-quality short texts. However, theyoften meander or are incoherent when generating longer texts. These issuesarise from the next-token-only language modeling objective. Recent work inself-supervised learning suggests that models can learn good latentrepresentations via contrastive learning, which can be effective fordiscriminative tasks. Our work analyzes the application of contrastiverepresentations for generative tasks, like long text generation. We propose oneapproach for leveraging constrastive representations, which we call TimeControl (TC). TC first learns a contrastive representation of the target textdomain, then generates text by decoding from these representations. Compared todomain-specific methods and fine-tuning GPT2 across a variety of text domains,TC performs competitively to methods specific for learning sentencerepresentations on discourse coherence. On long text generation settings, TCpreserves the text structure both in terms of ordering (up to $+15%$ better)and text length consistency (up to $+90%$ better).", "output": "Language modeling via stochastic processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in Artificial Intelligence (AI) have leveraged promisingresults in solving complex problems in the area of Natural Language Processing(NLP), being an important tool to help in the expeditious resolution ofjudicial proceedings in the legal area. In this context, this work targets theproblem of detecting the degree of similarity between judicial documents thatcan be achieved in the inference group, by applying six NLP techniques based onthe transformers architecture to a case study of legal proceedings in theBrazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2and RoBERTa, were pre-trained using a general purpose corpora of the BrazilianPortuguese language, and then were fine-tuned and specialised for the legalsector using 210,000 legal proceedings. Vector representations of each legaldocument were calculated based on their embeddings, which were used to clusterthe lawsuits, calculating the quality of each model based on the cosine of thedistance between the elements of the group to its centroid. We noticed thatmodels based on transformers presented better performance when compared toprevious traditional NLP techniques, with the RoBERTa model specialised for theBrazilian Portuguese language presenting the best results. This methodology canbe also applied to other case studies for different languages, making itpossible to advance in the current state of the art in the area of NLP appliedto the legal sector.", "output": "Analysing similarities between legal court documents using natural language processing approaches based on Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The COVID-19 pandemic has disproportionately impacted the lives ofminorities, such as members of the LGBTQ community (lesbian, gay, bisexual,transgender, and queer) due to pre-existing social disadvantages and healthdisparities. Although extensive research has been carried out on the impact ofthe COVID-19 pandemic on different aspects of the general population's lives,few studies are focused on the LGBTQ population. In this paper, we develop andevaluate two sets of machine learning classifiers using a pre-pandemic and aduring-pandemic dataset to identify Twitter posts exhibiting minority stress,which is a unique pressure faced by the members of the LGBTQ population due totheir sexual and gender identities. We demonstrate that our best pre- andduring-pandemic models show strong and stable performance for detecting poststhat contain minority stress. We investigate the linguistic differences inminority stress posts across pre- and during-pandemic periods. We find thatanger words are strongly associated with minority stress during the COVID-19pandemic. We explore the impact of the pandemic on the emotional states of theLGBTQ population by adopting propensity score-based matching to perform acausal analysis. The results show that the LGBTQ population have a greaterincrease in the usage of cognitive words and worsened observable attribute inthe usage of positive emotion words than the group of the general populationwith similar pre-pandemic behavioral attributes. Our findings have implicationsfor the public health domain and policy-makers to provide adequate support,especially with respect to mental health, to the LGBTQ population during futurecrises.", "output": "Minority Stress Experienced by LGBTQ Online Communities during the COVID-19 Pandemic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most of the developed localization solutions rely on RSSI fingerprinting.However, in the LoRa networks, due to the spreading factor (SF) in the networksetting, traditional fingerprinting may lack representativeness of the radiomap, leading to inaccurate position estimates. As such, in this work, wepropose a novel LoRa RSSI fingerprinting approach that takes into account theSF. The performance evaluation shows the prominence of our proposed approachsince we achieved an improvement in localization accuracy by up to 6.67%compared to the state-of-the-art methods. The evaluation has been done using afully connected deep neural network (DNN) set as the baseline. To furtherimprove the localization accuracy, we propose a deep reinforcement learningmodel that captures the ever-growing complexity of LoRa networks and copes withtheir scalability. The obtained results show an improvement of 48.10% in thelocalization accuracy compared to the baseline DNN model.", "output": "Spreading Factor assisted LoRa Localization with Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study a novel episodic risk-sensitive ReinforcementLearning (RL) problem, named Iterated CVaR RL, which aims to maximize the tailof the reward-to-go at each step, and focuses on tightly controlling the riskof getting into catastrophic situations at each stage. This formulation isapplicable to real-world tasks that demand strong risk avoidance throughout thedecision process, such as autonomous driving, clinical treatment planning androbotics. We investigate two performance metrics under Iterated CVaR RL, i.e.,Regret Minimization and Best Policy Identification. For both metrics, we designefficient algorithms ICVaR-RM and ICVaR-BPI, respectively, and provide nearlymatching upper and lower bounds with respect to the number of episodes $K$. Wealso investigate an interesting limiting case of Iterated CVaR RL, called WorstPath RL, where the objective becomes to maximize the minimum possiblecumulative reward. For Worst Path RL, we propose an efficient algorithm withconstant upper and lower bounds. Finally, our techniques for bounding thechange of CVaR due to the value function shift and decomposing the regret via adistorted visitation distribution are novel, and can find applications in otherrisk-sensitive RL problems.", "output": "Provably Efficient Risk-Sensitive Reinforcement Learning: Iterated CVaR and Worst Path."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Miller et al. showed that a model's in-distribution (ID) accuracyhas a strong linear correlation with its out-of-distribution (OOD) accuracy onseveral OOD benchmarks -- a phenomenon they dubbed ''accuracy-on-the-line''.While a useful tool for model selection (i.e., the model most likely to performthe best OOD is the one with highest ID accuracy), this fact does not helpestimate the actual OOD performance of models without access to a labeled OODvalidation set. In this paper, we show a similar but surprising phenomenon alsoholds for the agreement between pairs of neural network classifiers: wheneveraccuracy-on-the-line holds, we observe that the OOD agreement between thepredictions of any two pairs of neural networks (with potentially differentarchitectures) also observes a strong linear correlation with their IDagreement. Furthermore, we observe that the slope and bias of OOD vs IDagreement closely matches that of OOD vs ID accuracy. This phenomenon, which wecall agreement-on-the-line, has important practical applications: without anylabeled data, we can predict the OOD accuracy of classifiers}, since OODagreement can be estimated with just unlabeled data. Our prediction algorithmoutperforms previous methods both in shifts where agreement-on-the-line holdsand, surprisingly, when accuracy is not on the line. This phenomenon alsoprovides new insights into deep neural networks: unlike accuracy-on-the-line,agreement-on-the-line appears to only hold for neural network classifiers.", "output": "Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce PointConvFormer, a novel building block for point cloud baseddeep network architectures. Inspired by generalization theory, PointConvFormercombines ideas from point convolution, where filter weights are only based onrelative position, and Transformers which utilize feature-based attention. InPointConvFormer, attention computed from feature difference between points inthe neighborhood is used to modify the convolutional weights at each point.Hence, we preserved the invariances from point convolution, whereas attentionhelps to select relevant points in the neighborhood for convolution.PointConvFormer is suitable for multiple tasks that require details at thepoint level, such as segmentation and scene flow estimation tasks. Weexperiment on both tasks with multiple datasets including ScanNet,SemanticKitti, FlyingThings3D and KITTI. Our results show that PointConvFormeroffers a better accuracy-speed tradeoff than classic convolutions, regulartransformers, and voxelized sparse convolution approaches. Visualizations showthat PointConvFormer performs similarly to convolution on flat areas, whereasthe neighborhood selection effect is stronger on object boundaries, showingthat it has got the best of both worlds.", "output": "PointConvFormer: Revenge of the Point-based Convolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale neural networks possess considerable expressive power. They arewell-suited for complex learning tasks in industrial applications. However,large-scale models pose significant challenges for training under the currentFederated Learning (FL) paradigm. Existing approaches for efficient FL trainingoften leverage model parameter dropout. However, manipulating individual modelparameters is not only inefficient in meaningfully reducing the communicationoverhead when training large-scale FL models, but may also be detrimental tothe scaling efforts and model performance as shown by recent research. Toaddress these issues, we propose the Federated Opportunistic Block Dropout(FedOBD) approach. The key novelty is that it decomposes large-scale modelsinto semantic blocks so that FL participants can opportunistically uploadquantized blocks, which are deemed to be significant towards training themodel, to the FL server for aggregation. Extensive experiments evaluatingFedOBD against four state-of-the-art approaches based on multiple real-worlddatasets show that it reduces the overall communication overhead by more than88% compared to the best performing baseline approach, while achieving thehighest test accuracy. To the best of our knowledge, FedOBD is the firstapproach to perform dropout on FL models at the block level rather than at theindividual parameter level.", "output": "FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatically discovering failures in vision models under real-world settingsremains an open challenge. This work demonstrates how off-the-shelf,large-scale, image-to-text and text-to-image models, trained on vast amounts ofdata, can be leveraged to automatically find such failures. In essence, aconditional text-to-image generative model is used to generate large amounts ofsynthetic, yet realistic, inputs given a ground-truth label. Misclassifiedinputs are clustered and a captioning model is used to describe each cluster.Each cluster's description is used in turn to generate more inputs and assesswhether specific clusters induce more failures than expected. We use thispipeline to demonstrate that we can effectively interrogate classifiers trainedon ImageNet to find specific failure cases and discover spurious correlations.We also show that we can scale the approach to generate adversarial datasetstargeting specific classifier architectures. This work serves as aproof-of-concept demonstrating the utility of large-scale generative models toautomatically discover bugs in vision models in an open-ended manner. We alsodescribe a number of limitations and pitfalls related to this approach.", "output": "Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, ML researchers have wrestled with defining and improvingmachine learning (ML) benchmarks and datasets. In parallel, some have trained acritical lens on the ethics of dataset creation and ML research. In thisposition paper, we highlight the entanglement of ethics with seemingly``technical'' or ``scientific'' decisions about the design of ML benchmarks.Our starting point is the existence of multiple overlooked structuralsimilarities between human intelligence benchmarks and ML benchmarks. Bothtypes of benchmarks set standards for describing, evaluating, and comparingperformance on tasks relevant to intelligence -- standards that many scholarsof human intelligence have long recognized as value-laden. We use perspectivesfrom feminist philosophy of science on IQ benchmarks and thick concepts insocial science to argue that values need to be considered and documented whencreating ML benchmarks. It is neither possible nor desirable to avoid thischoice by creating value-neutral benchmarks. Finally, we outline practicalrecommendations for ML benchmark research ethics and ethics review.", "output": "Making Intelligence: Ethical Values in IQ and ML Benchmarks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "NLP in the legal domain has seen increasing success with the emergence ofTransformer-based Pre-trained Language Models (PLMs) pre-trained on legal text.PLMs trained over European and US legal text are available publicly; however,legal text from other domains (countries), such as India, have a lot ofdistinguishing characteristics. With the rapidly increasing volume of Legal NLPapplications in various countries, it has become necessary to pre-train suchLMs over legal text of other countries as well. In this work, we attempt toinvestigate pre-training in the Indian legal domain. We re-train (continuepre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indianlegal data, as well as train a model from scratch with a vocabulary based onIndian legal text. We apply these PLMs over three benchmark legal NLP tasks --Legal Statute Identification from facts, Semantic Segmentation of CourtJudgment Documents, and Court Appeal Judgment Prediction -- over both Indianand non-Indian (EU, UK) datasets. We observe that our approach not onlyenhances performance on the new domain (Indian texts) but also over theoriginal domain (European and UK texts). We also conduct explainabilityexperiments for a qualitative comparison of all these different PLMs.", "output": "Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural diffusion on graphs is a novel class of graph neural networks that hasattracted increasing attention recently. The capability of graph neural partialdifferential equations (PDEs) in addressing common hurdles of graph neuralnetworks (GNNs), such as the problems of over-smoothing and bottlenecks, hasbeen investigated but not their robustness to adversarial attacks. In thiswork, we explore the robustness properties of graph neural PDEs. We empiricallydemonstrate that graph neural PDEs are intrinsically more robust againsttopology perturbation as compared to other GNNs. We provide insights into thisphenomenon by exploiting the stability of the heat semigroup under graphtopology perturbations. We discuss various graph diffusion operators and relatethem to existing graph neural PDEs. Furthermore, we propose a general graphneural PDE framework based on which a new class of robust GNNs can be defined.We verify that the new model achieves comparable state-of-the-art performanceon several benchmark datasets.", "output": "On the Robustness of Graph Neural Diffusion to Topology Perturbations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artistic pieces can be studied from several perspectives, one example beingtheir reception among readers over time. In the present work, we approach thisinteresting topic from the standpoint of literary works, particularly assessingthe task of predicting whether a book will become a best seller. Dissimilarlyfrom previous approaches, we focused on the full content of books andconsidered visualization and classification tasks. We employed visualizationfor the preliminary exploration of the data structure and properties, involvingSemAxis and linear discriminant analyses. Then, to obtain quantitative and moreobjective results, we employed various classifiers. Such approaches were usedalong with a dataset containing (i) books published from 1895 to 1924 andconsecrated as best sellers by the Publishers Weekly Bestseller Lists and (ii)literary works published in the same period but not being mentioned in thatlist. Our comparison of methods revealed that the best-achieved result -combining a bag-of-words representation with a logistic regression classifier -led to an average accuracy of 0.75 both for the leave-one-out and 10-foldcross-validations. Such an outcome suggests that it is unfeasible to predictthe success of books with high accuracy using only the full content of thetexts. Nevertheless, our findings provide insights into the factors leading tothe relative success of a literary work.", "output": "Using Full-Text Content to Characterize and Identify Best Seller Books."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper revisits the bandit problem in the Bayesian setting. The Bayesianapproach formulates the bandit problem as an optimization problem, and the goalis to find the optimal policy which minimizes the Bayesian regret. One of themain challenges facing the Bayesian approach is that computation of the optimalpolicy is often intractable, especially when the length of the problem horizonor the number of arms is large. In this paper, we first show that under asuitable rescaling, the Bayesian bandit problem converges toward a continuousHamilton-Jacobi-Bellman (HJB) equation. The optimal policy for the limiting HJBequation can be explicitly obtained for several common bandit problems, and wegive numerical methods to solve the HJB equation when an explicit solution isnot available. Based on these results, we propose an approximate Bayes-optimalpolicy for solving Bayesian bandit problems with large horizons. Our method hasthe added benefit that its computational cost does not increase as the horizonincreases.", "output": "Continuous-in-time Limit for Bayesian Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The deep linear network (DLN) is a model for implicit regularization ingradient based optimization of overparametrized learning architectures.Training the DLN corresponds to a Riemannian gradient flow, where theRiemannian metric is defined by the architecture of the network and the lossfunction is defined by the learning task. We extend this geometric framework,obtaining explicit expressions for the volume form, including the case when thenetwork has infinite depth. We investigate the link between the Riemanniangeometry and the training asymptotics for matrix completion with rigorousanalysis and numerics. We propose that implicit regularization is a result ofbias towards high state space volume.", "output": "Deep Linear Networks for Matrix Completion -- An Infinite Depth Limit."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the variance of stochastic policy gradients (SPGs) with many actionsamples per state. We derive a many-actions optimality condition, whichdetermines when many-actions SPG yields lower variance as compared to asingle-action agent with proportionally extended trajectory. We proposeModel-Based Many-Actions (MBMA), an approach leveraging dynamics models formany-actions sampling in the context of SPG. MBMA addresses issues associatedwith existing implementations of many-actions SPG and yields lower bias andcomparable variance to SPG estimated from states in model-simulated rollouts.We find that MBMA bias and variance structure matches that predicted by theory.As a result, MBMA achieves improved sample efficiency and higher returns on arange of continuous action environments as compared to model-free,many-actions, and model-based on-policy SPG baselines.", "output": "On Many-Actions Policy Gradient."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work concerns the development of deep networks that are certifiablyrobust to adversarial attacks. Joint robust classification-detection wasrecently introduced as a certified defense mechanism, where adversarialexamples are either correctly classified or assigned to the \"abstain\" class. Inthis work, we show that such a provable framework can benefit by extension tonetworks with multiple explicit abstain classes, where the adversarial examplesare adaptively assigned to those. We show that naively adding multiple abstainclasses can lead to \"model degeneracy\", then we propose a regularizationapproach and a training method to counter this degeneracy by promoting full useof the multiple abstain classes. Our experiments demonstrate that the proposedapproach consistently achieves favorable standard vs. robust verified accuracytradeoffs, outperforming state-of-the-art algorithms for various choices ofnumber of abstain classes.", "output": "Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation models classifying hyperspectral images (HSI) arevulnerable to adversarial examples. Traditional approaches to adversarialrobustness focus on training or retraining a single network on attacked data,however, in the presence of multiple attacks these approaches decrease inperformance compared to networks trained individually on each attack. To combatthis issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net)which focuses on attack type detection and adversarial robustness under aunified model to preserve per data-type weight optimally while robustifiyingthe overall network. In the proposed method, a discriminator network is used toseparate data by attack type into their specific attack-expert ensemblenetwork.", "output": "Improving Hyperspectral Adversarial Robustness Under Multiple Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Models trained under assumptions in the complete market usually don't takeeffect in the incomplete market. This paper solves the hedging problem inincomplete market with three sources of incompleteness: risk factor,illiquidity, and discrete transaction dates. A new jump-diffusion model isproposed to describe stochastic asset prices. Three neutral networks, includingRNN, LSTM, Mogrifier-LSTM are used to attain hedging strategies with MSE Lossand Huber Loss implemented and compared.As a result, Mogrifier-LSTM is thefastest model with the best results under MSE and Huber Loss.", "output": "Inflexible Multi-Asset Hedging of incomplete market."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, $2times2$ zero-sum games are studied under the followingassumptions: $(1)$ One of the players (the leader) commits to choose itsactions by sampling a given probability measure (strategy); $(2)$ The leaderannounces its action, which is observed by its opponent (the follower) througha binary channel; and $(3)$ the follower chooses its strategy based on theknowledge of the leader's strategy and the noisy observation of the leader'saction. Under these conditions, the equilibrium is shown to always exist.Interestingly, even subject to noise, observing the actions of the leader isshown to be either beneficial or immaterial for the follower. Morespecifically, the payoff at the equilibrium of this game is upper bounded bythe payoff at the Stackelberg equilibrium (SE) in pure strategies; and lowerbounded by the payoff at the Nash equilibrium, which is equivalent to the SE inmixed strategies.Finally, necessary and sufficient conditions for observing thepayoff at equilibrium to be equal to its lower bound are presented. Sufficientconditions for the payoff at equilibrium to be equal to its upper bound arealso presented.", "output": "$2 \\times 2$ Zero-Sum Games with Commitments and Noisy Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusions are state-of-the-art generative models exhibitingremarkable empirical performance. They work by diffusing the data distributioninto a Gaussian distribution and then learning to reverse this noising processto obtain synthetic datapoints. The denoising diffusion relies onapproximations of the logarithmic derivatives of the noised data densitiesusing score matching. Such models can also be used to perform approximateposterior simulation when one can only sample from the prior and likelihood. Wepropose a unifying framework generalising this approach to a wide class ofspaces and leading to an original extension of score matching. We illustratethe resulting models on various applications.", "output": "From Denoising Diffusions to Denoising Markov Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the huge amount of parameters, fine-tuning of pretrained languagemodels (PLMs) is prone to overfitting in the low resource scenarios. In thiswork, we present a novel method that operates on the hidden representations ofa PLM to reduce overfitting. During fine-tuning, our method inserts randomautoencoders between the hidden layers of a PLM, which transform activationsfrom the previous layers into multi-view compressed representations beforefeeding them into the upper layers. The autoencoders are plugged out afterfine-tuning, so our method does not add extra parameters or increasecomputation cost during inference. Our method demonstrates promisingperformance improvement across a wide range of sequence- and token-levellow-resource NLP tasks.", "output": "Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning (CL) is a learning paradigm that emulates the humancapability of learning and accumulating knowledge continually withoutforgetting the previously learned knowledge and also transferring the learnedknowledge to help learn new tasks better. This survey presents a comprehensivereview and analysis of the recent progress of CL in NLP, which has significantdifferences from CL in computer vision and machine learning. It covers (1) allCL settings with a taxonomy of existing techniques; (2) catastrophic forgetting(CF) prevention, (3) knowledge transfer (KT), which is particularly importantfor NLP tasks; and (4) some theory and the hidden challenge of inter-task classseparation (ICS). (1), (3) and (4) have not been included in the existingsurvey. Finally, a list of future directions is discussed.", "output": "Continual Learning of Natural Language Processing Tasks: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study kernel methods in machine learning from the perspective of featuresubspace. We establish a one-to-one correspondence between feature subspacesand kernels and propose an information-theoretic measure for kernels. Inparticular, we construct a kernel from Hirschfeld--Gebelein--R'{e}nyi maximalcorrelation functions, coined the maximal correlation kernel, and demonstrateits information-theoretic optimality. We use the support vector machine (SVM)as an example to illustrate a connection between kernel methods and featureextraction approaches. We show that the kernel SVM on maximal correlationkernel achieves minimum prediction error. Finally, we interpret the Fisherkernel as a special maximal correlation kernel and establish its optimality.", "output": "Kernel Subspace and Feature Extraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Taking inspiration from recent developments in visual generative tasks usingdiffusion models, we propose a method for end-to-end speech-driven videoediting using a denoising diffusion model. Given a video of a talking person,and a separate auditory speech recording, the lip and jaw motions arere-synchronized without relying on intermediate structural representations suchas facial landmarks or a 3D face model. We show this is possible byconditioning a denoising diffusion model on audio mel spectral features togenerate synchronised facial motion. Proof of concept results are demonstratedon both single-speaker and multi-speaker video editing, providing a baselinemodel on the CREMA-D audiovisual data set. To the best of our knowledge, thisis the first work to demonstrate and validate the feasibility of applyingend-to-end denoising diffusion models to the task of audio-driven videoediting.", "output": "Speech Driven Video Editing via an Audio-Conditioned Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel neurosymbolic framework called NeSyFOLD to extract logicrules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD'slearning pipeline is as follows: (i) We first pre-train a CNN on the inputimage dataset and extract activations of the last layer kernels as binaryvalues; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithmto generate a logic program that can classify an image -- represented as avector of binary activations corresponding to each kernel -- while producing alogical explanation. The rules generated by the FOLD-SE-M algorithm have kernelnumbers as predicates. We have devised a novel algorithm for automaticallymapping the CNN kernels to semantic concepts in the images. This mapping isused to replace predicate names (kernel numbers) in the rule-set withcorresponding semantic concept labels. The resulting rule-set is interpretable,and can be intuitively understood by humans. We compare our NeSyFOLD frameworkwith the ERIC system that uses a decision-tree like algorithm to obtain therules. Our framework has the following advantages over ERIC: (i) In most cases,NeSyFOLD generates smaller rule-sets without compromising on the accuracy andfidelity; (ii) NeSyFOLD generates the mapping of filter numbers to semanticlabels automatically.", "output": "NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work aims to predict channels in wireless communication systems based onnoisy observations, utilizing sequence-to-sequence models with attention(Seq2Seq-attn) and transformer models. Both models are adapted from naturallanguage processing to tackle the complex challenge of channel prediction.Additionally, a new technique called reverse positional encoding is introducedin the transformer model to improve the robustness of the model against varyingsequence lengths. Similarly, the encoder outputs of the Seq2Seq-attn model arereversed before applying attention. Simulation results demonstrate that theproposed ordering techniques allow the models to better capture therelationships between the channel snapshots within the sequence, irrespectiveof the sequence length, as opposed to existing methods.", "output": "Reverse Ordering Techniques for Attention-Based Channel Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In data poisoning attacks, an adversary tries to change a model's predictionby adding, modifying, or removing samples in the training data. Recently,ensemble-based approaches for obtaining provable defenses against datapoisoning have been proposed where predictions are done by taking a majorityvote across multiple base models. In this work, we show that merely consideringthe majority vote in ensemble defenses is wasteful as it does not effectivelyutilize available information in the logits layers of the base models. Instead,we propose Run-Off Election (ROE), a novel aggregation method based on atwo-round election across the base models: In the first round, models vote fortheir preferred class and then a second, Run-Off election is held between thetop two classes in the first round. Based on this approach, we propose DPA+ROEand FA+ROE defense methods based on Deep Partition Aggregation (DPA) and FiniteAggregation (FA) approaches from prior work. We evaluate our methods on MNIST,CIFAR-10, and GTSRB and obtain improvements in certified accuracy by up to3%-4%. Also, by applying ROE on a boosted version of DPA, we gain improvementsaround 12%-27% comparing to the current state-of-the-art, establishing a newstate-of-the-art in (pointwise) certified robustness against data poisoning. Inmany cases, our approach outperforms the state-of-the-art, even when using 32times less computational power.", "output": "Run-Off Election: Improved Provable Defense against Data Poisoning Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) learns high-quality representations from largepools of unlabeled training data. As datasets grow larger, it becomes crucialto identify the examples that contribute the most to learning suchrepresentations. This enables efficient SSL by reducing the volume of datarequired for learning high-quality representations. Nevertheless, quantifyingthe value of examples for SSL has remained an open question. In this work, weaddress this for the first time, by proving that examples that contribute themost to contrastive SSL are those that have the most similar augmentations toother examples, in expectation. We provide rigorous guarantees for thegeneralization performance of SSL on such subsets. Empirically, we discover,perhaps surprisingly, the subsets that contribute the most to SSL are thosethat contribute the least to supervised learning. Through extensiveexperiments, we show that our subsets outperform random subsets by more than 3%on CIFAR100, CIFAR10, and STL10. Interestingly, we also find that we can safelyexclude 20% of examples from CIFAR100 and 40% from STL10, without affectingdownstream task performance.", "output": "Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncertainty quantification and robustness to distribution shifts areimportant goals in machine learning and artificial intelligence. AlthoughBayesian neural networks (BNNs) allow for uncertainty in the predictions to beassessed, different sources of uncertainty are indistinguishable. We presentimprecise Bayesian neural networks (IBNNs); they generalize and overcome someof the drawbacks of standard BNNs. These latter are trained using a singleprior and likelihood distributions, whereas IBNNs are trained using credalprior and likelihood sets. They allow to distinguish between aleatoric andepistemic uncertainties, and to quantify them. In addition, IBNNs are robust inthe sense of Bayesian sensitivity analysis, and are more robust than BNNs todistribution shift. They can also be used to compute sets of outcomes thatenjoy PAC-like properties. We apply IBNNs to two case studies. One, to modelblood glucose and insulin dynamics for artificial pancreas control, and two,for motion prediction in autonomous driving scenarios. We show that IBNNsperforms better when compared to an ensemble of BNNs benchmark.", "output": "Imprecise Bayesian Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Calibrating agent-based models (ABMs) in economics and finance typicallyinvolves a derivative-free search in a very large parameter space. In thiswork, we benchmark a number of search methods in the calibration of awell-known macroeconomic ABM on real data, and further assess the performanceof \"mixed strategies\" made by combining different methods. We find that methodsbased on random-forest surrogates are particularly efficient, and thatcombining search methods generally increases performance since the biases ofany single method are mitigated. Moving from these observations, we propose areinforcement learning (RL) scheme to automatically select and combine searchmethods on-the-fly during a calibration run. The RL agent keeps exploiting aspecific method only as long as this keeps performing well, but explores newstrategies when the specific method reaches a performance plateau. Theresulting RL search scheme outperforms any other method or method combinationtested, and does not rely on any prior information or trial and errorprocedure.", "output": "Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present counterfactual situation testing (CST), a causal data miningframework for detecting discrimination in classifiers. CST aims to answer in anactionable and meaningful way the intuitive question \"what would have been themodel outcome had the individual, or complainant, been of a different protectedstatus?\" It extends the legally-grounded situation testing of Thanh et al.(2011) by operationalizing the notion of fairness given the difference usingcounterfactual reasoning. For any complainant, we find and compare similarprotected and non-protected instances in the dataset used by the classifier toconstruct a control and test group, where a difference between the decisionoutcomes of the two groups implies potential individual discrimination. Unlikesituation testing, which builds both groups around the complainant, we buildthe test group on the complainant's counterfactual generated using causalknowledge. The counterfactual is intended to reflect how the protectedattribute when changed affects the seemingly neutral attributes used by theclassifier, which is taken for granted in many frameworks for discrimination.Under CST, we compare similar individuals within each group but dissimilarindividuals across both groups due to the possible difference between thecomplainant and its counterfactual. Evaluating our framework on twoclassification scenarios, we show that it uncovers a greater number of casesthan situation testing, even when the classifier satisfies the counterfactualfairness condition of Kusner et al. (2017).", "output": "Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Variational autoencoders (VAEs) are a popular generative model used toapproximate distributions. The encoder part of the VAE is used in amortizedlearning of latent variables, producing a latent representation for datasamples. Recently, VAEs have been used to characterize physical and biologicalsystems. In this case study, we qualitatively examine the amortizationproperties of a VAE used in biological applications. We find that in thisapplication the encoder bears a qualitative resemblance to more traditionalexplicit representation of latent variables.", "output": "Using VAEs to Learn Latent Variables: Observations on Applications in cryo-EM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For many decades, experimental solid mechanics has played a crucial role incharacterizing and understanding the mechanical properties of natural and novelmaterials. Recent advances in machine learning (ML) provide new opportunitiesfor the field, including experimental design, data analysis, uncertaintyquantification, and inverse problems. As the number of papers published inrecent years in this emerging field is exploding, it is timely to conduct acomprehensive and up-to-date review of recent ML applications in experimentalsolid mechanics. Here, we first provide an overview of common ML algorithms andterminologies that are pertinent to this review, with emphasis placed onphysics-informed and physics-based ML methods. Then, we provide thoroughcoverage of recent ML applications in traditional and emerging areas ofexperimental mechanics, including fracture mechanics, biomechanics, nano- andmicro-mechanics, architected materials, and 2D material. Finally, we highlightsome current challenges of applying ML to multi-modality and multi-fidelityexperimental datasets and propose several future research directions. Thisreview aims to provide valuable insights into the use of ML methods as well asa variety of examples for researchers in solid mechanics to integrate intotheir experiments.", "output": "Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the challenges of studying common neurological disorders is diseaseheterogeneity including differences in causes, neuroimaging characteristics,comorbidities, or genetic variation. Normative modelling has become a popularmethod for studying such cohorts where the 'normal' behaviour of aphysiological system is modelled and can be used at subject level to detectdeviations relating to disease pathology. For many heterogeneous diseases, weexpect to observe abnormalities across a range of neuroimaging and biologicalvariables. However, thus far, normative models have largely been developed forstudying a single imaging modality. We aim to develop a multi-modal normativemodelling framework where abnormality is aggregated across variables ofmultiple modalities and is better able to detect deviations than uni-modalbaselines. We propose two multi-modal VAE normative models to detect subjectlevel deviations across T1 and DTI data. Our proposed models were better ableto detect diseased individuals, capture disease severity, and correlate withpatient cognition than baseline approaches. We also propose a multivariatelatent deviation metric, measuring deviations from the joint latent space,which outperformed feature-based metrics.", "output": "Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern machine learning, attention computation is a fundamental task fortraining large language models such as Transformer, GPT-4 and ChatGPT. In thiswork, we study exponential regression problem which is inspired by thesoftmax/exp unit in the attention mechanism in large language models. Thestandard exponential regression is non-convex. We study the regularizationversion of exponential regression problem which is a convex problem. We useapproximate newton method to solve in input sparsity time.Formally, in this problem, one is given matrix $A in mathbb{R}^{n timesd}$, $b in mathbb{R}^n$, $w in mathbb{R}^n$ and any of functions $exp,cosh$ and $sinh$ denoted as $f$. The goal is to find the optimal $x$ thatminimize $ 0.5 | f(Ax) - b |_2^2 + 0.5 | mathrm{diag}(w) A x |_2^2$. Thestraightforward method is to use the naive Newton's method. Let$mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let$omega$ denote the exponent of matrix multiplication. Currently, $omegaapprox 2.373$. Let $epsilon$ denote the accuracy error. In this paper, wemake use of the input sparsity and purpose an algorithm that use $log ( |x_0- x^*|_2 / epsilon)$ iterations and $widetilde{O}(mathrm{nnz}(A) +d^{omega} )$ per iteration time to solve the problem.", "output": "Solving Regularized Exp, Cosh and Sinh Regression Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep machine learning models including Convolutional Neural Networks (CNN)have been successful in the detection of Mild Cognitive Impairment (MCI) usingmedical images, questionnaires, and videos. This paper proposes a novelMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model todistinguish MCI from those with normal cognition by analyzing facial features.The data comes from the I-CONECT, a behavioral intervention trial aimed atimproving cognitive function by providing frequent video chats. MC-ViViTextracts spatiotemporal features of videos in one branch and augmentsrepresentations by the MC module. The I-CONECT dataset is challenging as thedataset is imbalanced containing Hard-Easy and Positive-Negative samples, whichimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easyand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORREloss to address the imbalanced problem. Our experimental results on theI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with ahigh accuracy of 90.63% accuracy on some of the interview videos.", "output": "MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The performance of Large Language Models (LLMs) in reasoning tasks dependsheavily on prompt design, with Chain-of-Thought (CoT) and self-consistencybeing critical methods that enhance this ability. However, these methods do notfully exploit the answers generated by the LLM to guide subsequent responses.This paper proposes a new prompting method, named Progressive-Hint Prompting(PHP), that enables automatic multiple interactions between users and LLMs byusing previously generated answers as hints to progressively guide toward thecorrect answers. PHP is orthogonal to CoT and self-consistency, making it easyto combine with state-of-the-art techniques to further improve performance. Weconducted an extensive and comprehensive evaluation to demonstrate theeffectiveness of the proposed method. Our experimental results on sevenbenchmarks show that combining CoT and self-consistency with PHP significantlyimproves accuracy while remaining highly efficient. For instance, withtext-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decodingcompared to Complex CoT, and a 46.17% reduction in sample paths withself-consistency. With GPT-4 and PHP, we achieve state-of-the-art performanceson SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%), AQuA (76.4% -&gt; 79.9%) and MATH(50.3% -&gt; 53.9%).", "output": "Progressive-Hint Prompting Improves Reasoning in Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we aim to develop a large language model (LLM) with thereasoning ability on complex graph data. Currently, LLMs have achieved veryimpressive performance on various natural language learning tasks, extensionsof which have also been applied to study the vision tasks with multi-modaldata. However, when it comes to the graph learning tasks, existing LLMs presentvery serious flaws due to their several inherited weaknesses in performing{multi-step logic reasoning}, {precise mathematical calculation} and{perception about the spatial and temporal factors}.To address such challenges, in this paper, we will investigate theprinciples, methodologies and algorithms to empower existing LLMs with graphreasoning ability, which will have tremendous impacts on the current researchof both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformermodels, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer)framework to teach LLMs themselves with prompts augmented by ChatGPT to useexternal graph reasoning API tools. Specifically, we will investigate to teachGraph-ToolFormer to handle various graph data reasoning tasks in this paper,including both (1) very basic graph data loading and graph property reasoningtasks, ranging from simple graph order and size to the graph diameter andperiphery, and (2) more advanced reasoning tasks on real-world graph data, suchas bibliographic networks, protein molecules, sequential recommender systems,social networks and knowledge graphs.", "output": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the generalization error of statistical learning models in aFederated Learning (FL) setting. Specifically, there are $K$ devices orclients, each holding an independent own dataset of size $n$. Individualmodels, learned locally via Stochastic Gradient Descent, are aggregated(averaged) by a central server into a global model and then sent back to thedevices. We consider multiple (say $R in mathbb N^*$) rounds of modelaggregation and study the effect of $R$ on the generalization error of thefinal aggregated model. We establish an upper bound on the generalization errorthat accounts explicitly for the effect of $R$ (in addition to the number ofparticipating devices $K$ and dataset size $n$). It is observed that, for fixed$(n, K)$, the bound increases with $R$, suggesting that the generalization ofsuch learning algorithms is negatively affected by more frequent communicationwith the parameter server. Combined with the fact that the empirical risk,however, generally decreases for larger values of $R$, this indicates that $R$might be a parameter to optimize to reduce the population risk of FLalgorithms. The results of this paper, which extend straightforwardly to theheterogeneous data setting, are also illustrated through numerical examples.", "output": "More Communication Does Not Result in Smaller Generalization Error in Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Private and public sector structures and norms refine how emerging technologyis used in practice. In healthcare, despite a proliferation of AI adoption, theorganizational governance surrounding its use and integration is often poorlyunderstood. What the Health AI Partnership (HAIP) aims to do in this researchis to better define the requirements for adequate organizational governance ofAI systems in healthcare settings and support health system leaders to makemore informed decisions around AI adoption. To work towards this understanding,we first identify how the standards for the AI adoption in healthcare may bedesigned to be used easily and efficiently. Then, we map out the precisedecision points involved in the practical institutional adoption of AItechnology within specific health systems. Practically, we achieve this througha multi-organizational collaboration with leaders from major health systemsacross the United States and key informants from related fields. Working withthe consultancy IDEO [dot] org, we were able to conduct usability-testingsessions with healthcare and AI ethics professionals. Usability analysisrevealed a prototype structured around mock key decision points that align withhow organizational leaders approach technology adoption. Concurrently, weconducted semi-structured interviews with 89 professionals in healthcare andother relevant fields. Using a modified grounded theory approach, we were ableto identify 8 key decision points and comprehensive procedures throughout theAI adoption lifecycle. This is one of the most detailed qualitative analyses todate of the current governance structures and processes involved in AI adoptionby health systems in the United States. We hope these findings can informfuture efforts to build capabilities to promote the safe, effective, andresponsible adoption of emerging technologies in healthcare.", "output": "Organizational Governance of Emerging Technologies: AI Adoption in Healthcare."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gradient-boosted decision trees (GBDT) are widely used and highly effectivemachine learning approach for tabular data modeling. However, their complexstructure may lead to low robustness against small covariate perturbation inunseen data. In this study, we apply one-hot encoding to convert a GBDT modelinto a linear framework, through encoding of each tree leaf to one dummyvariable. This allows for the use of linear regression techniques, plus a novelrisk decomposition for assessing the robustness of a GBDT model againstcovariate perturbations. We propose to enhance the robustness of GBDT models byrefitting their linear regression forms with $L_1$ or $L_2$ regularization.Theoretical results are obtained about the effect of regularization on themodel performance and robustness. It is demonstrated through numericalexperiments that the proposed regularization approach can enhance therobustness of the one-hot-encoded GBDT models.", "output": "Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As human-robot interaction (HRI) systems advance, so does the difficulty ofevaluating and understanding the strengths and limitations of these systems indifferent environments and with different users. To this end, previous methodshave algorithmically generated diverse scenarios that reveal system failures ina shared control teleoperation task. However, these methods require directlyevaluating generated scenarios by simulating robot policies and human actions.The computational cost of these evaluations limits their applicability in morecomplex domains. Thus, we propose augmenting scenario generation systems withsurrogate models that predict both human and robot behaviors. In the sharedcontrol teleoperation domain and a more complex shared workspace collaborationtask, we show that surrogate assisted scenario generation efficientlysynthesizes diverse datasets of challenging scenarios. We demonstrate thatthese failures are reproducible in real-world interactions.", "output": "Surrogate Assisted Generation of Human-Robot Interaction Scenarios."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Approaches for appraising feature importance approximations, alternativelyreferred to as attribution methods, have been established across an extensivearray of contexts. The development of resilient techniques for performancebenchmarking constitutes a critical concern in the sphere of explainable deeplearning. This study scrutinizes the dependability of the RemOve-And-Retrain(ROAR) procedure, which is prevalently employed for gauging the performance offeature importance estimates. The insights gleaned from our theoreticalfoundation and empirical investigations reveal that attributions containinglesser information about the decision function may yield superior results inROAR benchmarks, contradicting the original intent of ROAR. This occurrence issimilarly observed in the recently introduced variant RemOve-And-Debias (ROAD),and we posit a persistent pattern of blurriness bias in ROAR attributionmetrics. Our findings serve as a warning against indiscriminate use on ROARmetrics. The code is available as open source.", "output": "On Pitfalls of $\\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Layer Normalization (LayerNorm) is an inherent component in allTransformer-based models. In this paper, we show that LayerNorm is crucial tothe expressivity of the multi-head attention layer that follows it. This is incontrast to the common belief that LayerNorm's only role is to normalize theactivations during the forward pass, and their gradients during the backwardpass. We consider a geometric interpretation of LayerNorm and show that itconsists of two components: (a) projection of the input vectors to a $d-1$space that is orthogonal to the $left[1,1,...,1right]$ vector, and (b)scaling of all vectors to the same norm of $sqrt{d}$. We show that each ofthese components is important for the attention layer that follows it inTransformers: (a) projection allows the attention mechanism to create anattention query that attends to all keys equally, offloading the need to learnthis operation by the attention; and (b) scaling allows each key to potentiallyreceive the highest attention, and prevents keys from being \"un-select-able\".We show empirically that Transformers do indeed benefit from these propertiesof LayeNorm in general language modeling and even in computing simple functionssuch as \"majority\". Our code is available at .", "output": "On the Expressivity Role of LayerNorm in Transformers' Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper describes the results of SemEval 2023 task 7 -- Multi-EvidenceNatural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2tasks, a Natural Language Inference (NLI) task, and an evidence selection taskon clinical trial data. The proposed challenges require multi-hop biomedicaland numerical reasoning, which are of significant importance to the developmentof systems capable of large-scale interpretation and retrieval of medicalevidence, to provide personalized evidence-based care.Task 1, the entailment task, received 643 submissions from 40 participants,and Task 2, the evidence selection task, received 364 submissions from 23participants. The tasks are challenging, with the majority of submitted systemsfailing to significantly outperform the majority class baseline on theentailment task, and we observe significantly better performance on theevidence selection task than on the entailment task. Increasing the number ofmodel parameters leads to a direct increase in performance, far moresignificant than the effect of biomedical pre-training. Future works couldexplore the limitations of large models for generalization and numericalinference, and investigate methods to augment clinical datasets to allow formore rigorous testing and to facilitate fine-tuning.We envisage that the dataset, models, and results of this task will be usefulto the biomedical NLI and evidence retrieval communities. The dataset,competition leaderboard, and website are publicly available.", "output": "SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This document presents some early explorations of applying Softly MaskedLanguage Modelling (SMLM) to symbolic music generation. SMLM can be seen as ageneralisation of masked language modelling (MLM), where instead of eachelement of the input set being either known or unknown, each element can beknown, unknown or partly known. We demonstrate some results of applying SMLM toconstrained symbolic music generation using a transformer encoder architecture.Several audio examples are available at", "output": "Exploring Softly Masked Language Modelling for Controllable Symbolic Music Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a novel spiking neural network model for learning distributedinternal representations from data in an unsupervised procedure. We achievedthis by transforming the non-spiking feedforward Bayesian ConfidencePropagation Neural Network (BCPNN) model, employing an online correlation-basedHebbian-Bayesian learning and rewiring mechanism, shown previously to performrepresentation learning, into a spiking neural network with Poisson statisticsand low firing rate comparable to in vivo cortical pyramidal neurons. Weevaluated the representations learned by our spiking model using a linearclassifier and show performance close to the non-spiking BCPNN, and competitivewith other Hebbian-based spiking networks when trained on MNIST and F-MNISTmachine learning benchmarks.", "output": "Spiking neural networks with Hebbian plasticity for unsupervised representation learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperparameter optimization (HPO) is a powerful technique for automating thetuning of machine learning (ML) models. However, in many real-worldapplications, accuracy is only one of multiple performance criteria that mustbe considered. Optimizing these objectives simultaneously on a complex anddiverse search space remains a challenging task. In this paper, we proposeMO-DEHB, an effective and flexible multi-objective (MO) optimizer that extendsthe recent evolutionary Hyperband method DEHB. We validate the performance ofMO-DEHB using a comprehensive suite of 15 benchmarks consisting of diverse andchallenging MO problems, including HPO, neural architecture search (NAS), andjoint NAS and HPO, with objectives including accuracy, latency and algorithmicfairness. A comparative study against state-of-the-art MO optimizersdemonstrates that MO-DEHB clearly achieves the best performance across our 15benchmarks.", "output": "MO-DEHB: Evolutionary-based Hyperband for Multi-Objective Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of overcoming exponential sample complexity indifferential entropy estimation under Gaussian convolutions. Specifically, weconsider the estimation of the differential entropy $h(X+Z)$ via $n$independently and identically distributed samples of $X$, where $X$ and $Z$ areindependent $D$-dimensional random variables with $X$ sub-Gaussian with boundedsecond moment and $Zsimmathcal{N}(0,sigma^2I_D)$. Under the absolute-errorloss, the above problem has a parametric estimation rate of$frac{c^D}{sqrt{n}}$, which is exponential in data dimension $D$ and oftenproblematic for applications. We overcome this exponential sample complexity byprojecting $X$ to a low-dimensional space via principal component analysis(PCA) before the entropy estimation, and show that the asymptotic erroroverhead vanishes as the unexplained variance of the PCA vanishes. This impliesnear-optimal performance for inherently low-dimensional structures embedded inhigh-dimensional spaces, including hidden-layer outputs of deep neural networks(DNN), which can be used to estimate mutual information (MI) in DNNs. Weprovide numerical results verifying the performance of our PCA approach onGaussian and spiral data. We also apply our method to analysis of informationflow through neural network layers (c.f. information bottleneck), with resultsmeasuring mutual information in a noisy fully connected network and a noisyconvolutional neural network (CNN) for MNIST classification.", "output": "High-Dimensional Smoothed Entropy Estimation via Dimensionality Reduction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Developing the next generation of household robot helpers requires combininglocomotion and interaction capabilities, which is generally referred to asmobile manipulation (MoMa). MoMa tasks are difficult due to the large actionspace of the robot and the common multi-objective nature of the task, e.g.,efficiently reaching a goal while avoiding obstacles. Current approaches oftensegregate tasks into navigation without manipulation and stationarymanipulation without locomotion by manually matching parts of the action spaceto MoMa sub-objectives (e.g. base actions for locomotion objectives and armactions for manipulation). This solution prevents simultaneous combinations oflocomotion and interaction degrees of freedom and requires human domainknowledge for both partitioning the action space and matching the action partsto the sub-objectives. In this paper, we introduce Causal MoMa, a new frameworkto train policies for typical MoMa tasks that makes use of the most favorablesubspace of the robot's action space to address each sub-objective. Causal MoMaautomatically discovers the causal dependencies between actions and terms ofthe reward function and exploits these dependencies in a causal policy learningprocedure that reduces gradient variance compared to previous state-of-the-artpolicy gradient algorithms, improving convergence and results. We evaluate theperformance of Causal MoMa on three types of simulated robots across differentMoMa tasks and demonstrate success in transferring the policies trained insimulation directly to a real robot, where our agent is able to follow movinggoals and react to dynamic obstacles while simultaneously and synergisticallycontrolling the whole-body: base, arm, and head. More information at", "output": "Causal Policy Gradient for Whole-Body Mobile Manipulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generally, image-to-image translation (i2i) methods aim at learning mappingsacross domains with the assumption that the images used for translation sharecontent (e.g., pose) but have their own domain-specific information (a.k.a.style). Conditioned on a target image, such methods extract the target styleand combine it with the source image content, keeping coherence between thedomains. In our proposal, we depart from this traditional view and insteadconsider the scenario where the target domain is represented by a verylow-resolution (LR) image, proposing a domain-agnostic i2i method forfine-grained problems, where the domains are related. More specifically, ourdomain-agnostic approach aims at generating an image that combines visualfeatures from the source image with low-frequency information (e.g. pose,color) of the LR target image. To do so, we present a novel approach thatrelies on training the generative model to produce images that both sharedistinctive information of the associated source image and correctly match theLR target image when downscaled. We validate our method on the CelebA-HQ andAFHQ datasets by demonstrating improvements in terms of visual quality.Qualitative and quantitative results show that when dealing with intra-domainimage translation, our method generates realistic samples compared tostate-of-the-art methods such as StarGAN v2. Ablation studies also reveal thatour method is robust to changes in color, it can be applied toout-of-distribution images, and it allows for manual control over the finalresults.", "output": "Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum machine learning (QML) has witnessed immense progress recently, withquantum support vector machines (QSVMs) emerging as a promising model. Thispaper focuses on the two existing QSVM methods: quantum kernel SVM (QK-SVM) andquantum variational SVM (QV-SVM). While both have yielded impressive results,we present a novel approach that synergizes the strengths of QK-SVM and QV-SVMto enhance accuracy. Our proposed model, quantum variational kernel SVM(QVK-SVM), leverages the quantum kernel and quantum variational algorithm. Weconducted extensive experiments on the Iris dataset and observed that QVK-SVMoutperforms both existing models in terms of accuracy, loss, and confusionmatrix indicators. Our results demonstrate that QVK-SVM holds tremendouspotential as a reliable and transformative tool for QML applications. Hence, werecommend its adoption in future QML research endeavors.", "output": "Enhancing Quantum Support Vector Machines through Variational Kernel Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal sentiment analysis is an important area for understanding theuser's internal states. Deep learning methods were effective, but the problemof poor interpretability has gradually gained attention. Previous works haveattempted to use attention weights or vector distributions to provideinterpretability. However, their explanations were not intuitive and can beinfluenced by different trained models. This study proposed a novel approach toprovide interpretability by converting nonverbal modalities into textdescriptions and by using large-scale language models for sentimentpredictions. This provides an intuitive approach to directly interpret whatmodels depend on with respect to making decisions from input texts, thussignificantly improving interpretability. Specifically, we convert descriptionsbased on two feature patterns for the audio modality and discrete action unitsfor the facial modality. Experimental results on two sentiment analysis tasksdemonstrated that the proposed approach maintained, or even improvedeffectiveness for sentiment analysis compared to baselines using conventionalfeatures, with the highest improvement of 2.49% on the F1 score. The resultsalso showed that multimodal descriptions have similar characteristics on fusingmodalities as those of conventional fusion methods. The results demonstratedthat the proposed approach is interpretable and effective for multimodalsentiment analysis.", "output": "Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Representing human performance at high-fidelity is an essential buildingblock in diverse applications, such as film production, computer games orvideoconferencing. To close the gap to production-level quality, we introduceHumanRF, a 4D dynamic neural scene representation that captures full-bodyappearance in motion from multi-view video input, and enables playback fromnovel, unseen viewpoints. Our novel representation acts as a dynamic videoencoding that captures fine details at high compression rates by factorizingspace-time into a temporal matrix-vector decomposition. This allows us toobtain temporally coherent reconstructions of human actors for long sequences,while representing high-resolution details even in the context of challengingmotion. While most research focuses on synthesizing at resolutions of 4MP orlower, we address the challenge of operating at 12MP. To this end, we introduceActorsHQ, a novel multi-view dataset that provides 12MP footage from 160cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. Wedemonstrate challenges that emerge from using such high-resolution data andshow that our newly introduced HumanRF effectively leverages this data, makinga significant step towards production-level quality novel view synthesis.", "output": "HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We obtain an upper bound on the expected supremum of a Bernoulli processindexed by the image of an index set under a uniformly Lipschitz function classin terms of properties of the index set and the function class, extending anearlier result of Maurer for Gaussian processes. The proof makes essential useof recent results of Bednorz and Latala on the boundedness of Bernoulliprocesses.", "output": "A Chain Rule for the Expected Suprema of Bernoulli Processes."}]