[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the context of neuroevolution, Quality-Diversity algorithms have proveneffective in generating repertoires of diverse and efficient policies byrelying on the definition of a behavior space. A natural goal induced by thecreation of such a repertoire is trying to achieve behaviors on demand, whichcan be done by running the corresponding policy from the repertoire. However,in uncertain environments, two problems arise. First, policies can lackrobustness and repeatability, meaning that multiple episodes under slightlydifferent conditions often result in very different behaviors. Second, due tothe discrete nature of the repertoire, solutions vary discontinuously. Here wepresent a new approach to achieve behavior-conditioned trajectory generationbased on two mechanisms: First, MAP-Elites Low-Spread (ME-LS), which constrainsthe selection of solutions to those that are the most consistent in thebehavior space. Second, the Quality-Diversity Transformer (QDT), aTransformer-based model conditioned on continuous behavior descriptors, whichtrains on a dataset generated by policies from a ME-LS repertoire and learns toautoregressively generate sequences of actions that achieve target behaviors.Results show that ME-LS produces consistent and robust policies, and that itscombination with the QDT yields a single policy capable of achieving diversebehaviors on demand with high accuracy.", "output": "The Quality-Diversity Transformer: Generating Behavior-Conditioned Trajectories with Decision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The network pruning algorithm based on evolutionary multi-objective (EMO) canbalance the pruning rate and performance of the network. However, itspopulation-based nature often suffers from the complex pruning optimizationspace and the highly resource-consuming pruning structure verification process,which limits its application. To this end, this paper proposes an EMO jointpruning with multiple sub-networks (EMO-PMS) to reduce space complexity andresource consumption. First, a divide-and-conquer EMO network pruning frameworkis proposed, which decomposes the complex EMO pruning task on the whole networkinto easier sub-tasks on multiple sub-networks. On the one hand, thisdecomposition reduces the pruning optimization space and decreases theoptimization difficulty; on the other hand, the smaller network structureconverges faster, so the computational resource consumption of the proposedalgorithm is lower. Secondly, a sub-network training method based oncross-network constraints is designed so that the sub-network can process thefeatures generated by the previous one through feature constraints. This methodallows sub-networks optimized independently to collaborate better and improvesthe overall performance of the pruned network. Finally, a multiple sub-networksjoint pruning method based on EMO is proposed. For one thing, it can accuratelymeasure the feature processing capability of the sub-networks with thepre-trained feature selector. For another, it can combine multi-objectivepruning results on multiple sub-networks through global performance impairmentranking to design a joint pruning scheme. The proposed algorithm is validatedon three datasets with different challenging. Compared with fifteen advancedpruning algorithms, the experiment results exhibit the effectiveness andefficiency of the proposed algorithm.", "output": "An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Object Constraint Language (OCL) is a declarative language that addsconstraints and object query expressions to MOF models. Despite its potentialto provide precision and conciseness to UML models, the unfamiliar syntax ofOCL has hindered its adoption. Recent advancements in LLMs, such as GPT-3, haveshown their capability in many NLP tasks, including semantic parsing and textgeneration. Codex, a GPT-3 descendant, has been fine-tuned on publiclyavailable code from GitHub and can generate code in many programming languages.We investigate the reliability of OCL constraints generated by Codex fromnatural language specifications. To achieve this, we compiled a dataset of 15UML models and 168 specifications and crafted a prompt template with slots topopulate with UML information and the target task, using both zero- andfew-shot learning methods. By measuring the syntactic validity and executionaccuracy metrics of the generated OCL constraints, we found that enriching theprompts with UML information and enabling few-shot learning increases thereliability of the generated OCL constraints. Furthermore, the results reveal aclose similarity based on sentence embedding between the generated OCLconstraints and the human-written ones in the ground truth, implying a level ofclarity and understandability in the generated OCL constraints by Codex.", "output": "On Codex Prompt Engineering for OCL Generation: An Empirical Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The traditional way of tackling discrete optimization problems is by usinglocal search on suitably defined cost or fitness landscapes. Such approachesare however limited by the slowing down that occurs when local minima, that area feature of the typically rugged landscapes encountered, arrest the progressof the search process. Another way of tackling optimization problems is by theuse of heuristic approximations to estimate a global cost minimum. Here wepresent a combination of these two approaches by using cover-encoding mapswhich map processes from a larger search space to subsets of the originalsearch space. The key idea is to construct cover-encoding maps with the help ofsuitable heuristics that single out near-optimal solutions and result inlandscapes on the larger search space that no longer exhibit trapping localminima. The processes that are typically employed involve some form ofcoarse-graining, and we suggest here that they can be viewed as avatars ofrenormalisation group transformations.", "output": "Optimisation via encodings: a renormalisation group perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrary to Google Search's mission of delivering information from \"manyangles so you can form your own understanding of the world,\" we find thatGoogle and its most prominent returned results -- Wikipedia and YouTube, simplyreflect the narrow set of cultural stereotypes tied to the search language forcomplex topics like \"Buddhism,\" \"Liberalism,\" \"colonization,\" \"Iran\" and\"America.\" Simply stated, they present, to varying degrees, distinctinformation across the same search in different languages (we call it 'languagebias'). Instead of presenting a global picture of a complex topic, our onlinesearches turn us into the proverbial blind person touching a small portion ofan elephant, ignorant of the existence of other cultural perspectives. Thelanguage we use to search ends up as a cultural filter to promote ethnocentricviews, where a person evaluates other people or ideas based on their ownculture. We also find that language bias is deeply embedded in ChatGPT. As itis primarily trained on English language data, it presents the Anglo-Americanperspective as the normative view, reducing the complexity of a multifacetedissue to the single Anglo-American standard. In this paper, we present evidenceand analysis of language bias and discuss its larger social implications.Toward the end of the paper, we propose a potential framework of usingautomatic translation to leverage language bias and argue that the task ofpiecing together a genuine depiction of the elephant is a challenging andimportant endeavor that deserves a new area of research in NLP and requirescollaboration with scholars from the humanities to create ethically sound andsocially responsible technology together.", "output": "A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable AI (XAI) has established itself as an important component ofAI-driven interactive systems. With Augmented Reality (AR) becoming moreintegrated in daily lives, the role of XAI also becomes essential in AR becauseend-users will frequently interact with intelligent services. However, it isunclear how to design effective XAI experiences for AR. We propose XAIR, adesign framework that addresses \"when\", \"what\", and \"how\" to provideexplanations of AI output in AR. The framework was based on amulti-disciplinary literature review of XAI and HCI research, a large-scalesurvey probing 500+ end-users' preferences for AR-based explanations, and threeworkshops with 12 experts collecting their insights about XAI design in AR.XAIR's utility and effectiveness was verified via a study with 10 designers andanother study with 12 end-users. XAIR can provide guidelines for designers,inspiring them to identify new design opportunities and achieve effective XAIdesigns in AR.", "output": "XAIR: A Framework of Explainable AI in Augmented Reality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The soft Dice loss (SDL) has taken a pivotal role in many automatedsegmentation pipelines in the medical imaging community. Over the last years,some reasons behind its superior functioning have been uncovered and furtheroptimizations have been explored. However, there is currently no implementationthat supports its direct use in settings with soft labels. Hence, a synergybetween the use of SDL and research leveraging the use of soft labels, also inthe context of model calibration, is still missing. In this work, we introduceDice semimetric losses (DMLs), which (i) are by design identical to SDL in astandard setting with hard labels, but (ii) can be used in settings with softlabels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirmthe potential synergy of DMLs with soft labels (e.g. averaging, labelsmoothing, and knowledge distillation) over hard labels (e.g. majority votingand random selection). As a result, we obtain superior Dice scores and modelcalibration, which supports the wider adoption of DMLs in practice. Code isavailable athref{", "output": "Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting crime using machine learning and deep learning techniques hasgained considerable attention from researchers in recent years, focusing onidentifying patterns and trends in crime occurrences. This review paperexamines over 150 articles to explore the various machine learning and deeplearning algorithms applied to predict crime. The study provides access to thedatasets used for crime prediction by researchers and analyzes prominentapproaches applied in machine learning and deep learning algorithms to predictcrime, offering insights into different trends and factors related to criminalactivities. Additionally, the paper highlights potential gaps and futuredirections that can enhance the accuracy of crime prediction. Finally, thecomprehensive overview of research discussed in this paper on crime predictionusing machine learning and deep learning approaches serves as a valuablereference for researchers in this field. By gaining a deeper understanding ofcrime prediction techniques, law enforcement agencies can develop strategies toprevent and respond to criminal activities more effectively.", "output": "Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Safety-critical cyber-physical systems require control strategies whoseworst-case performance is robust against adversarial disturbances and modelinguncertainties. In this paper, we present a framework for approximate controland learning in partially observed systems to minimize the worst-casediscounted cost over an infinite time-horizon. We model disturbances to thesystem as finite-valued uncertain variables with unknown probabilitydistributions. For problems with known system dynamics, we construct a dynamicprogramming (DP) decomposition to compute the optimal control strategy. Ourfirst contribution is to define information states that improve thecomputational tractability of this DP without loss of optimality. Then, wedescribe a simplification for a class of problems where the incurred cost isobservable at each time-instance. Our second contribution is a definition ofapproximate information states that can be constructed or learned directly fromobserved data for problems with observable costs. We derive bounds on theperformance loss of the resulting approximate control strategy.", "output": "Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FMAS, a fast multi-objective neural architecture search frameworkfor semantic segmentation. FMAS subsamples the structure and pre-trainedparameters of DeepLabV3+, without fine-tuning, dramatically reducing trainingtime during search. To further reduce candidate evaluation time, we use asubset of the validation dataset during the search. Only the final, Paretonon-dominated, candidates are ultimately fine-tuned using the complete trainingset. We evaluate FMAS by searching for models that effectively trade accuracyand computational cost on the PASCAL VOC 2012 dataset. FMAS finds competitivedesigns quickly, e.g., taking just 0.5 GPU days to discover a DeepLabV3+variant that reduces FLOPs and parameters by 10$%$ and 20$%$ respectively,for less than 3$%$ increased error. We also search on an edge device calledGAP8 and use its latency as the metric. FMAS is capable of finding 2.2$times$faster network with 7.61$%$ MIoU loss.", "output": "FMAS: Fast Multi-Objective SuperNet Architecture Search for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we address the emerging field of Streaming Federated Learning(SFL) and propose local cache update rules to manage dynamic data distributionsand limited cache capacity. Traditional federated learning relies on fixed datasets, whereas in SFL, data is streamed, and its distribution changes over time,leading to discrepancies between the local training dataset and long-termdistribution. To mitigate this problem, we propose three local cache updaterules - First-In-First-Out (FIFO), Static Ratio Selective Replacement (SRSR),and Dynamic Ratio Selective Replacement (DRSR) - that update the local cache ofeach client while considering the limited cache capacity. Furthermore, wederive a convergence bound for our proposed SFL algorithm as a function of thedistribution discrepancy between the long-term data distribution and theclient's local training dataset. We then evaluate our proposed algorithm on twodatasets: a network traffic classification dataset and an image classificationdataset. Our experimental results demonstrate that our proposed local cacheupdate rules significantly reduce the distribution discrepancy and outperformthe baseline methods. Our study advances the field of SFL and providespractical cache management solutions in federated learning.", "output": "On the Local Cache Update Rules in Streaming Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a self-supervised approach for learning to perform audio sourceseparation in videos based on natural language queries, using only unlabeledvideo and audio pairs as training data. A key challenge in this task islearning to associate the linguistic description of a sound-emitting object toits visual features and the corresponding components of the audio waveform, allwithout access to annotations during training. To overcome this challenge, weadapt off-the-shelf vision-language foundation models to provide pseudo-targetsupervision via two novel loss functions and encourage a stronger alignmentbetween the audio, visual and natural language modalities. During inference,our approach can separate sounds given text, video and audio input, or giventext and audio input alone. We demonstrate the effectiveness of ourself-supervised approach on three audio-visual separation datasets, includingMUSIC, SOLOS and AudioSet, where we outperform state-of-the-art stronglysupervised approaches despite not using object detectors or text labels duringtraining.", "output": "Language-Guided Audio-Visual Source Separation via Trimodal Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Block-based programming environments are increasingly used to introducecomputing concepts to beginners. However, novice students often struggle inthese environments, given the conceptual and open-ended nature of programmingtasks. To effectively support a student struggling to solve a given task, it isimportant to provide adaptive scaffolding that guides the student towards asolution. We introduce a scaffolding framework based on pop quizzes presentedas multi-choice programming tasks. To automatically generate these pop quizzes,we propose a novel algorithm, PQuizSyn. More formally, given a reference taskwith a solution code and the student's current attempt, PQuizSyn synthesizesnew tasks for pop quizzes with the following features: (a) Adaptive (i.e.,individualized to the student's current attempt), (b) Comprehensible (i.e.,easy to comprehend and solve), and (c) Concealing (i.e., do not reveal thesolution code). Our algorithm synthesizes these tasks using techniques based onsymbolic reasoning and graph-based code representations. We show that ouralgorithm can generate hundreds of pop quizzes for different student attemptson reference tasks from Hour of Code: Maze Challenge and Karel. We assess thequality of these pop quizzes through expert ratings using an evaluation rubric.Further, we have built an online platform for practicing block-basedprogramming tasks empowered via pop quiz based feedback, and report resultsfrom an initial user study.", "output": "Adaptive Scaffolding in Block-Based Programming via Synthesizing New Tasks as Pop Quizzes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is growing interest in searching for information from large videocorpora. Prior works have studied relevant tasks, such as text-based videoretrieval, moment retrieval, video summarization, and video captioning inisolation, without an end-to-end setup that can jointly search from videocorpora and generate summaries. Such an end-to-end setup would allow for manyinteresting applications, e.g., a text-based search that finds a relevant videofrom a video corpus, extracts the most relevant moment from that video, andsegments the moment into important steps with captions. To address this, wepresent the HiREST (HIerarchical REtrieval and STep-captioning) dataset andpropose a new benchmark that covers hierarchical information retrieval andvisual/textual stepwise summarization from an instructional video corpus.HiREST consists of 3.4K text-video pairs from an instructional video dataset,where 1.1K videos have annotations of moment spans relevant to text query andbreakdown of each moment into key instruction steps with caption and timestamps(totaling 8.6K step captions). Our hierarchical benchmark consists of videoretrieval, moment retrieval, and two novel moment segmentation and stepcaptioning tasks. In moment segmentation, models break down a video moment intoinstruction steps and identify start-end boundaries. In step captioning, modelsgenerate a textual summary for each step. We also present starting pointtask-specific and end-to-end joint baseline models for our new benchmark. Whilethe baseline models show some promising results, there still exists large roomfor future improvement by the community. Project website:", "output": "Hierarchical Video-Moment Retrieval and Step-Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "EEG-based recognition of activities and states involves the use of priorneuroscience knowledge to generate quantitative EEG features, which may limitBCI performance. Although neural network-based methods can effectively extractfeatures, they often encounter issues such as poor generalization acrossdatasets, high predicting volatility, and low model interpretability. Hence, wepropose a novel lightweight multi-dimensional attention network, calledLMDA-Net. By incorporating two novel attention modules designed specificallyfor EEG signals, the channel attention module and the depth attention module,LMDA-Net can effectively integrate features from multiple dimensions, resultingin improved classification performance across various BCI tasks. LMDA-Net wasevaluated on four high-impact public datasets, including motor imagery (MI) andP300-Speller paradigms, and was compared with other representative models. Theexperimental results demonstrate that LMDA-Net outperforms other representativemethods in terms of classification accuracy and predicting volatility,achieving the highest accuracy in all datasets within 300 training epochs.Ablation experiments further confirm the effectiveness of the channel attentionmodule and the depth attention module. To facilitate an in-depth understandingof the features extracted by LMDA-Net, we propose class-specific neural networkfeature interpretability algorithms that are suitable for event-relatedpotentials (ERPs) and event-related desynchronization/synchronization(ERD/ERS). By mapping the output of the specific layer of LMDA-Net to the timeor spatial domain through class activation maps, the resulting featurevisualizations can provide interpretable analysis and establish connectionswith EEG time-spatial analysis in neuroscience. In summary, LMDA-Net showsgreat potential as a general online decoding model for various EEG tasks.", "output": "LMDA-Net:A lightweight multi-dimensional attention network for general EEG-based brain-computer interface paradigms and interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a multimodal deep learning (MDL) framework for predicting physicalproperties of a 10-dimensional acrylic polymer composite material by mergingphysical attributes and chemical data. Our MDL model comprises four modules,including three generative deep learning models for material structurecharacterization and a fourth model for property prediction. Our approachhandles an 18-dimensional complexity, with 10 compositional inputs and 8property outputs, successfully predicting 913,680 property data points across114,210 composition conditions. This level of complexity is unprecedented incomputational materials science, particularly for materials with undefinedstructures. We propose a framework to analyze the high-dimensional informationspace for inverse material design, demonstrating flexibility and adaptabilityto various materials and scales, provided sufficient data is available. Thisstudy advances future research on different materials and the development ofmore sophisticated models, drawing us closer to the ultimate goal of predictingall properties of all materials.", "output": "A Comprehensive and Versatile Multimodal Deep Learning Approach for Predicting Diverse Properties of Advanced Materials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While decades of theoretical research have led to the invention of severalclasses of error-correction codes, the design of such codes is an extremelychallenging task, mostly driven by human ingenuity. Recent studies demonstratethat such designs can be effectively automated and accelerated via tools frommachine learning (ML), thus enabling ML-driven classes of error-correctioncodes with promising performance gains compared to classical designs. Afundamental challenge, however, is that it is prohibitively complex, if notimpossible, to design and train fully ML-driven encoder and decoder pairs forlarge code dimensions. In this paper, we propose Product Autoencoder(ProductAE) -- a computationally-efficient family of deep learning driven(encoder, decoder) pairs -- aimed at enabling the training of relatively largecodes (both encoder and decoder) with a manageable training complexity. Webuild upon ideas from classical product codes and propose constructing largeneural codes using smaller code components. ProductAE boils down the complexproblem of training the encoder and decoder for a large code dimension $k$ andblocklength $n$ to less-complex sub-problems of training encoders and decodersfor smaller dimensions and blocklengths. Our training results show successfultraining of ProductAEs of dimensions as large as $k = 300$ bits with meaningfulperformance gains compared to state-of-the-art classical and neural designs.Moreover, we demonstrate excellent robustness and adaptivity of ProductAEs tochannel models different than the ones used for training.", "output": "ProductAE: Toward Deep Learning Driven Error-Correction Codes of Large Dimensions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence (AI) has made incredible progress recently. On theone hand, advanced foundation models like ChatGPT can offer powerfulconversation, in-context learning and code generation abilities on a broadrange of open-domain tasks. They can also generate high-level solution outlinesfor domain-specific tasks based on the common sense knowledge they haveacquired. However, they still face difficulties with some specialized tasksbecause they lack enough domain-specific data during pre-training or they oftenhave errors in their neural network computations on those tasks that needaccurate executions. On the other hand, there are also many existing models andsystems (symbolic-based or neural-based) that can do some domain-specific tasksvery well. However, due to the different implementation or working mechanisms,they are not easily accessible or compatible with foundation models. Therefore,there is a clear and pressing need for a mechanism that can leverage foundationmodels to propose task solution outlines and then automatically match some ofthe sub-tasks in the outlines to the off-the-shelf models and systems withspecial functionalities to complete them. Inspired by this, we introduceTaskMatrix.AI as a new AI ecosystem that connects foundation models withmillions of APIs for task completion. Unlike most previous work that aimed toimprove a single AI model, TaskMatrix.AI focuses more on using existingfoundation models (as a brain-like central system) and APIs of other AI modelsand systems (as sub-task solvers) to achieve diversified tasks in both digitaland physical domains. As a position paper, we will present our vision of how tobuild such an ecosystem, explain each key component, and use study cases toillustrate both the feasibility of this vision and the main challenges we needto address next.", "output": "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Protein language models (pLMs), pre-trained via causal language modeling onprotein sequences, have been a promising tool for protein sequence design. Inreal-world protein engineering, there are many cases where the amino acids inthe middle of a protein sequence are optimized while maintaining otherresidues. Unfortunately, because of the left-to-right nature of pLMs, existingpLMs modify suffix residues by prompting prefix residues, which areinsufficient for the infilling task that considers the whole surroundingcontext. To find the more effective pLMs for protein engineering, we design anew benchmark, Secondary structureE InFilling rEcoveRy, SEIFER, whichapproximates infilling sequence design scenarios. With the evaluation ofexisting models on the benchmark, we reveal the weakness of existing languagemodels and show that language models trained via fill-in-middle transformation,called ProtFIM, are more appropriate for protein engineering. Also, we provethat ProtFIM generates protein sequences with decent protein representationsthrough exhaustive experiments and visualizations.", "output": "ProtFIM: Fill-in-Middle Protein Sequence Design via Protein Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There are plenty of graph neural network (GNN) accelerators being proposed.However, they highly rely on users' hardware expertise and are usuallyoptimized for one specific GNN model, making them challenging for practical use. Therefore, in this work, we propose GNNBuilder, the first automated, generic,end-to-end GNN accelerator generation framework. It features four advantages:(1) GNNBuilder can automatically generate GNN accelerators for a wide range ofGNN models arbitrarily defined by users; (2) GNNBuilder takes standard PyTorchprogramming interface, introducing zero overhead for algorithm developers; (3)GNNBuilder supports end-to-end code generation, simulation, acceleratoroptimization, and hardware deployment, realizing a push-button fashion for GNNaccelerator design; (4) GNNBuilder is equipped with accurate performance modelsof its generated accelerator, enabling fast and flexible design spaceexploration (DSE). In the experiments, first, we show that our acceleratorperformance model has errors within $36%$ for latency prediction and $18%$for BRAM count prediction. Second, we show that our generated accelerators canoutperform CPU by $6.33times$ and GPU by $6.87times$. This framework isopen-source, and the code is available at", "output": "GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Combined with demonstrations, deep reinforcement learning can efficientlydevelop policies for manipulators. However, it takes time to collect sufficienthigh-quality demonstrations in practice. And human demonstrations may beunsuitable for robots. The non-Markovian process and over-reliance ondemonstrations are further challenges. For example, we found that RL agents aresensitive to demonstration quality in manipulation tasks and struggle to adaptto demonstrations directly from humans. Thus it is challenging to leveragelow-quality and insufficient demonstrations to assist reinforcement learning intraining better policies, and sometimes, limited demonstrations even lead toworse performance.We propose a new algorithm named TD3fG (TD3 learning from a generator) tosolve these problems. It forms a smooth transition from learning from expertsto learning from experience. This innovation can help agents extract priorknowledge while reducing the detrimental effects of the demonstrations. Ouralgorithm performs well in Adroit manipulator and MuJoCo tasks with limiteddemonstrations.", "output": "Learning Complicated Manipulation Skills via Deterministic Policy with Limited Demonstrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trust is a key motivation in developing explainable artificial intelligence(XAI). However, researchers attempting to measure trust in AI face numerouschallenges, such as different trust conceptualizations, simplified experimentaltasks that may not induce uncertainty as a prerequisite for trust, and the lackof validated trust questionnaires in the context of AI. While acknowledgingthese issues, we have identified a further challenge that currently seemsunderappreciated - the potential distinction between trust as one construct andemph{distrust} as a second construct independent of trust. While there hasbeen long-standing academic discourse for this distinction and arguments forboth the one-dimensional and two-dimensional conceptualization of trust,distrust seems relatively understudied in XAI. In this position paper, we notonly highlight the theoretical arguments for distrust as a distinct constructfrom trust but also contextualize psychometric evidence that likewise favors adistinction between trust and distrust. It remains to be investigated whetherthe available psychometric evidence is sufficient for the existence of distrustor whether distrust is merely a measurement artifact. Nevertheless, the XAIcommunity should remain receptive to considering trust and distrust for a morecomprehensive understanding of these two relevant constructs in XAI.", "output": "Distrust in (X)AI -- Measurement Artifact or Distinct Construct?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-target regression is useful in a plethora of applications. Althoughrandom forest models perform well in these tasks, they are often difficult tointerpret. Interpretability is crucial in machine learning, especially when itcan directly impact human well-being. Although model-agnostic techniques existfor multi-target regression, specific techniques tailored to random forestmodels are not available. To address this issue, we propose a technique thatprovides rule-based interpretations for instances made by a random forest modelfor multi-target regression, influenced by a recent model-specific techniquefor random forest interpretability. The proposed technique was evaluatedthrough extensive experiments and shown to offer competitive interpretationscompared to state-of-the-art techniques.", "output": "Local Interpretability of Random Forests for Multi-Target Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Implicit neural representation has recently shown a promising ability inrepresenting images with arbitrary resolutions. In this paper, we present aLocal Implicit Transformer (LIT), which integrates the attention mechanism andfrequency encoding technique into a local implicit image function. We design across-scale local attention block to effectively aggregate local features. Tofurther improve representative power, we propose a Cascaded LIT (CLIT) thatexploits multi-scale features, along with a cumulative training strategy thatgradually increases the upsampling scales during training. We have conductedextensive experiments to validate the effectiveness of these components andanalyze various training strategies. The qualitative and quantitative resultsdemonstrate that LIT and CLIT achieve favorable results and outperform theprior works in arbitrary super-resolution tasks.", "output": "Cascaded Local Implicit Transformer for Arbitrary-Scale Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several approaches have been developed that generate embeddings forDescription Logic ontologies and use these embeddings in machine learning. Oneapproach of generating ontologies embeddings is by first embedding theontologies into a graph structure, i.e., introducing a set of nodes and edgesfor named entities and logical axioms, and then applying a graph embedding toembed the graph in $mathbb{R}^n$. Methods that embed ontologies in graphs(graph projections) have different formal properties related to the type ofaxioms they can utilize, whether the projections are invertible or not, andwhether they can be applied to asserted axioms or their deductive closure. Weanalyze, qualitatively and quantitatively, several graph projection methodsthat have been used to embed ontologies, and we demonstrate the effect of theproperties of graph projections on the performance of predicting axioms fromontology embeddings. We find that there are substantial differences betweendifferent projection methods, and both the projection of axioms into nodes andedges as well ontological choices in representing knowledge will impact thesuccess of using ontology embeddings to predict axioms.", "output": "From axioms over graphs to vectors, and back again: evaluating the properties of graph-based ontology embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How to ensure fairness is an important topic in federated learning (FL).Recent studies have investigated how to reward clients based on theircontribution (collaboration fairness), and how to achieve uniformity ofperformance across clients (performance fairness). Despite achieving progresson either one, we argue that it is critical to consider them together, in orderto engage and motivate more diverse clients joining FL to derive a high-qualityglobal model. In this work, we propose a novel method to optimize both types offairness simultaneously. Specifically, we propose to estimate clientcontribution in gradient and data space. In gradient space, we monitor thegradient direction differences of each client with respect to others. And indata space, we measure the prediction error on client data using an auxiliarymodel. Based on this contribution estimation, we propose a FL method, federatedtraining via contribution estimation (FedCE), i.e., using estimation as globalmodel aggregation weights. We have theoretically analyzed our method andempirically evaluated it on two real-world medical datasets. The effectivenessof our approach has been validated with significant performance improvements,better collaboration fairness, better performance fairness, and comprehensiveanalytical studies.", "output": "Fair Federated Medical Image Segmentation via Client Contribution Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a deep learning-based wound classification tool that canassist medical personnel in non-wound care specialization to classify five keywound conditions, namely deep wound, infected wound, arterial wound, venouswound, and pressure wound, given color images captured using readily availablecameras. The accuracy of the classification is vital for appropriate woundmanagement. The proposed wound classification method adopts a multi-task deeplearning framework that leverages the relationships among the five key woundconditions for a unified wound classification architecture. With differences inCohen's kappa coefficients as the metrics to compare our proposed model withhumans, the performance of our model was better or non-inferior to those of allhuman medical personnel. Our convolutional neural network-based model is thefirst to classify five tasks of deep, infected, arterial, venous, and pressurewounds simultaneously with good accuracy. The proposed model is compact andmatches or exceeds the performance of human doctors and nurses. Medicalpersonnel who do not specialize in wound care can potentially benefit from anapp equipped with the proposed deep learning model.", "output": "Development of a deep learning-based tool to assist wound classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Distributed ledger systems have become more prominent and successful inrecent years, with a focus on blockchains and cryptocurrency. This has led tovarious misunderstandings about both the technology itself and itscapabilities, as in many cases blockchain and cryptocurrency is usedsynonymously and other applications are often overlooked. Therefore, as awhole, the view of distributed ledger technology beyond blockchains andcryptocurrencies is very limited. Existing vocabularies and ontologies oftenfocus on single aspects of the technology, or in some cases even just on oneproduct. This potentially leads to other types of distributed ledgers and theirpossible use cases being neglected. In this paper, we present a knowledge graphand an ontology for distributed ledger technologies, which includes securityconsiderations to model aspects such as threats and vulnerabilities,application domains, as well as relevant standards and regulations. Such aknowledge graph improves the overall understanding of distributed ledgers,reveals their strengths, and supports the work of security personnel, i.e.analysts and system architects. We discuss potential uses and follow semanticweb best practices to evaluate and publish the ontology and knowledge graph.", "output": "Building a Knowledge Graph of Distributed Ledger Technologies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study presents a novel approach to bone age assessment (BAA) using amulti-view, multi-task classification model based on the Sauvegrain method. Astraightforward solution to automating the Sauvegrain method, which assesses amaturity score for each landmark in the elbow and predicts the bone age, is totrain classifiers independently to score each region of interest (RoI), butthis approach limits the accessible information to local morphologies andincreases computational costs. As a result, this work proposes aself-accumulative vision transformer (SAT) that mitigates anisotropic behavior,which usually occurs in multi-view, multi-task problems and limits theeffectiveness of a vision transformer, by applying token replay and regionalattention bias. A number of experiments show that SAT successfully exploits therelationships between landmarks and learns global morphological features,resulting in a mean absolute error of BAA that is 0.11 lower than that of theprevious work. Additionally, the proposed SAT has four times reduced parametersthan an ensemble of individual classifiers of the previous work. Lastly, thiswork also provides informative implications for clinical practice, improvingthe accuracy and efficiency of BAA in diagnosing abnormal growth inadolescents.", "output": "Self-accumulative Vision Transformer for Bone Age Assessment Using the Sauvegrain Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study building a multi-task agent in Minecraft. Without humandemonstrations, solving long-horizon tasks in this open-ended environment withreinforcement learning (RL) is extremely sample inefficient. To tackle thechallenge, we decompose solving Minecraft tasks into learning basic skills andplanning over the skills. We propose three types of fine-grained basic skillsin Minecraft, and use RL with intrinsic rewards to accomplish basic skills withhigh success rates. For skill planning, we use Large Language Models to findthe relationships between skills and build a skill graph in advance. When theagent is solving a task, our skill search algorithm walks on the skill graphand generates the proper skill plans for the agent. In experiments, our methodaccomplishes 24 diverse Minecraft tasks, where many tasks require sequentiallyexecuting for more than 10 skills. Our method outperforms baselines in mosttasks by a large margin. The project's website and code can be found at", "output": "Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The fairness of a deep neural network is strongly affected by dataset biasand spurious correlations, both of which are usually present in modernfeature-rich and complex visual datasets. Due to the difficulty and variabilityof the task, no single de-biasing method has been universally successful. Inparticular, implicit methods not requiring explicit knowledge of bias variablesare especially relevant for real-world applications. We propose a novelimplicit mitigation method using a Bayesian neural network, allowing us toleverage the relationship between epistemic uncertainties and the presence ofbias or spurious correlations in a sample. Our proposed posterior estimatesharpening procedure encourages the network to focus on core features that donot contribute to high uncertainties. Experimental results on three benchmarkdatasets demonstrate that Bayesian networks with sharpened posterior estimatesperform comparably to prior existing methods and show potential worthy offurther exploration.", "output": "Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a Bayesian Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Personalisation of language models for dialogue sensitises them to bettercapture the speaking patterns of people of specific characteristics, and/or inspecific environments. However, rich character annotations are difficult tocome by and to successfully leverage. In this work, we release and describe anovel set of manual annotations for 863 speakers from the popular Cornell MovieDialog Corpus, including features like characteristic quotes and characterdescriptions, and a set of six automatically extracted metadata for over 95% ofthe featured films. We perform extensive experiments on two corpora and showthat such annotations can be effectively used to personalise language models,reducing perplexity by up to 8.5%. Our method can be applied even zero-shot forspeakers for whom no prior training data is available, by relying oncombinations of characters' demographic characteristics. Since collecting suchmetadata is costly, we also contribute a cost-benefit analysis to highlightwhich annotations were most cost-effective relative to the reduction inperplexity.", "output": "Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spoken keyword spotting (KWS) is the task of identifying a keyword in anaudio stream and is widely used in smart devices at the edge in order toactivate voice assistants and perform hands-free tasks. The task is daunting asthere is a need, on the one hand, to achieve high accuracy while at the sametime ensuring that such systems continue to run efficiently on low power andpossibly limited computational capabilities devices. This work presents AraSpotfor Arabic keyword spotting trained on 40 Arabic keywords, using differentonline data augmentation, and introducing ConformerGRU model architecture.Finally, we further improve the performance of the model by training atext-to-speech model for synthetic data generation. AraSpot achieved aState-of-the-Art SOTA 99.59% result outperforming previous approaches.", "output": "AraSpot: Arabic Spoken Command Spotting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fairlearn is an open source project to help practitioners assess and improvefairness of artificial intelligence (AI) systems. The associated Pythonlibrary, also named fairlearn, supports evaluation of a model's output acrossaffected populations and includes several algorithms for mitigating fairnessissues. Grounded in the understanding that fairness is a sociotechnicalchallenge, the project integrates learning resources that aid practitioners inconsidering a system's broader societal context.", "output": "Fairlearn: Assessing and Improving Fairness of AI Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The quality of texts generated by natural language generation (NLG) systemsis hard to measure automatically. Conventional reference-based metrics, such asBLEU and ROUGE, have been shown to have relatively low correlation with humanjudgments, especially for tasks that require creativity and diversity. Recentstudies suggest using large language models (LLMs) as reference-free metricsfor NLG evaluation, which have the benefit of being applicable to new tasksthat lack human references. However, these LLM-based evaluators still havelower human correspondence than medium-size neural evaluators. In this work, wepresent GPTEval, a framework of using large language models withchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality ofNLG outputs. We experiment with two generation tasks, text summarization anddialogue generation. We show that GPTEval with GPT-4 as the backbone modelachieves a Spearman correlation of 0.514 with human on summarization task,outperforming all previous methods by a large margin. We also proposepreliminary analysis on the behavior of LLM-based evaluators, and highlight thepotential issue of LLM-based evaluators having a bias towards the LLM-generatedtexts.", "output": "GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Underlying relationships among Multi-Agent Systems (MAS) in hazardousscenarios can be represented as Game-theoretic models. This paper proposes anew hierarchical network-based model called Game-theoretic Utility Tree (GUT),which decomposes high-level strategies into executable low-level actions forcooperative MAS decisions. It combines with a new payoff measure based on agentneeds for real-time strategy games. We present an Explore game domain, where wemeasure the performance of MAS achieving tasks from the perspective ofbalancing the success probability and system costs. We evaluate the GUTapproach against state-of-the-art methods that greedily rely on rewards of thecomposite actions. Conclusive results on extensive numerical simulationsindicate that GUT can organize more complex relationships among MAScooperation, helping the group achieve challenging tasks with lower costs andhigher winning rates. Furthermore, we demonstrated the applicability of the GUTusing the simulator-hardware testbed - Robotarium. The performances verifiedthe effectiveness of the GUT in the real robot application and validated thatthe GUT could effectively organize MAS cooperation strategies, helping thegroup with fewer advantages achieve higher performance.", "output": "A Hierarchical Game-Theoretic Decision-Making for Cooperative Multi-Agent Systems Under the Presence of Adversarial Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose FLANDERS, a novel federated learning (FL)aggregation scheme robust to Byzantine attacks. FLANDERS considers the localmodel updates sent by clients at each FL round as a matrix-valued time series.Then, it identifies malicious clients as outliers of this time series bycomparing actual observations with those estimated by a matrix autoregressiveforecasting model. Experiments conducted on several datasets under different FLsettings demonstrate that FLANDERS matches the robustness of the most powerfulbaselines against Byzantine clients. Furthermore, FLANDERS remains highlyeffective even under extremely severe attack scenarios, as opposed to existingdefense strategies.", "output": "A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neuro-symbolic rule learning has attracted lots of attention as it offersbetter interpretability than pure neural models and scales better than symbolicrule learning. A recent approach named pix2rule proposes a neural DisjunctiveNormal Form (neural DNF) module to learn symbolic rules with feed-forwardlayers. Although proved to be effective in synthetic binary classification,pix2rule has not been applied to more challenging tasks such as multi-label andmulti-class classifications over real-world data. In this paper, we addressthis limitation by extending the neural DNF module to (i) support rule learningin real-world multi-class and multi-label classification tasks, (ii) enforcethe symbolic property of mutual exclusivity (i.e. predicting exactly one class)in multi-class classification, and (iii) explore its scalability over largeinputs and outputs. We train a vanilla neural DNF model similar to pix2rule'sneural DNF module for multi-label classification, and we propose a novelextended model called neural DNF-EO (Exactly One) which enforces mutualexclusivity in multi-class classification. We evaluate the classificationperformance, scalability and interpretability of our neural DNF-based models,and compare them against pure neural models and a state-of-the-art symbolicrule learner named FastLAS. We demonstrate that our neural DNF-based modelsperform similarly to neural networks, but provide better interpretability byenabling the extraction of logical rules. Our models also scale well when therule search space grows in size, in contrast to FastLAS, which fails to learnin multi-class classification tasks with 200 classes and in all multi-labelsettings.", "output": "Neuro-symbolic Rule Learning in Real-world Classification Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object-centric process discovery (OCPD) constitutes a paradigm shift inprocess mining. Instead of assuming a single case notion present in the eventlog, OCPD can handle events without a single case notion, but that are insteadrelated to a collection of objects each having a certain type. The object typesconstitute multiple, interacting case notions. The output of OCPD is anobject-centric Petri net, i.e. a Petri net with object-typed places, thatrepresents the parallel execution of multiple execution flows corresponding toobject types. Similar to classical process discovery, where we aim forbehaviorally sound process models as a result, in OCPD, we aim for soundness ofthe resulting object-centric Petri nets. However, the existing OCPD approachcan result in violations of soundness. As we will show, one violation arisesfor multiple interacting object types with loops that arise in collaborativesystems. This paper proposes an extended OCPD approach and proves that it doesnot suffer from this violation of soundness of the resulting object-centricPetri nets. We also show how we prevent the OCPD approach from introducingspurious interactions in the discovered object-centric Petri net. The proposedframework is prototypically implemented.", "output": "Preventing Object-centric Discovery of Unsound Process Models for Object Interactions with Loops in Collaborative Systems: Extended Version."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the continuous growth in communication network complexity and trafficvolume, communication load balancing solutions are receiving increasingattention. Specifically, reinforcement learning (RL)-based methods have shownimpressive performance compared with traditional rule-based methods. However,standard RL methods generally require an enormous amount of data to train, andgeneralize poorly to scenarios that are not encountered during training. Wepropose a policy reuse framework in which a policy selector chooses the mostsuitable pre-trained RL policy to execute based on the current trafficcondition. Our method hinges on a policy bank composed of policies trained on adiverse set of traffic scenarios. When deploying to an unknown trafficscenario, we select a policy from the policy bank based on the similaritybetween the previous-day traffic of the current scenario and the trafficobserved during training. Experiments demonstrate that this framework canoutperform classical and adaptive rule-based methods by a large margin.", "output": "Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Communication load balancing aims to balance the load between differentavailable resources, and thus improve the quality of service for networksystems. After formulating the load balancing (LB) as a Markov decision processproblem, reinforcement learning (RL) has recently proven effective inaddressing the LB problem. To leverage the benefits of classical RL for loadbalancing, however, we need an explicit reward definition. Engineering thisreward function is challenging, because it involves the need for expertknowledge and there lacks a general consensus on the form of an optimal rewardfunction. In this work, we tackle the communication load balancing problem froman inverse reinforcement learning (IRL) approach. To the best of our knowledge,this is the first time IRL has been successfully applied in the field ofcommunication load balancing. Specifically, first, we infer a reward functionfrom a set of demonstrations, and then learn a reinforcement learning loadbalancing policy with the inferred reward function. Compared to classicalRL-based solution, the proposed solution can be more general and more suitablefor real-world scenarios. Experimental evaluations implemented on differentsimulated traffic scenarios have shown our method to be effective and betterthan other baselines by a considerable margin.", "output": "Communication Load Balancing via Efficient Inverse Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unmanned Aerial Vehicles (UAVs) are increasingly used as aerial base stationsto provide ad hoc communications infrastructure. Building upon prior researchefforts which consider either static nodes, 2D trajectories or single UAVsystems, this paper focuses on the use of multiple UAVs for providing wirelesscommunication to mobile users in the absence of terrestrial communicationsinfrastructure. In particular, we jointly optimize UAV 3D trajectory and NOMApower allocation to maximize system throughput. Firstly, a weightedK-means-based clustering algorithm establishes UAV-user associations at regularintervals. The efficacy of training a novel Shared Deep Q-Network (SDQN) withaction masking is then explored. Unlike training each UAV separately using DQN,the SDQN reduces training time by using the experiences of multiple UAVsinstead of a single agent. We also show that SDQN can be used to train amulti-agent system with differing action spaces. Simulation results confirmthat: 1) training a shared DQN outperforms a conventional DQN in terms ofmaximum system throughput (+20%) and training time (-10%); 2) it can convergefor agents with different action spaces, yielding a 9% increase in throughputcompared to mutual learning algorithms; and 3) combining NOMA with an SDQNarchitecture enables the network to achieve a better sum rate compared withexisting baseline schemes.", "output": "Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled Mobile Communications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The potential for pre-trained large language models (LLMs) to use naturallanguage feedback at inference time has been an exciting recent development. Webuild upon this observation by formalizing an algorithm for learning fromnatural language feedback at training time instead, which we call Imitationlearning from Language Feedback (ILF). ILF requires only a small amount ofhuman-written feedback during training and does not require the same feedbackat test time, making it both user-friendly and sample-efficient. We furthershow that ILF can be seen as a form of minimizing the KL divergence to theground truth distribution and demonstrate a proof-of-concept on a neuralprogram synthesis task. We use ILF to improve a Codegen-Mono 6.1B model'spass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic PythonProblems (MBPP) benchmark, outperforming both fine-tuning on MBPP andfine-tuning on repaired programs written by humans. Overall, our resultssuggest that learning from human-written natural language feedback is both moreeffective and sample-efficient than training exclusively on demonstrations forimproving an LLM's performance on code generation tasks.", "output": "Improving Code Generation by Training with Natural Language Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In formal procedure of civil cases, the textual materials provided bydifferent parties describe the development process of the cases. It is adifficult but necessary task to extract the key information for the cases fromthese textual materials and to clarify the dispute focus of related parties.Currently, officers read the materials manually and use methods, such askeyword searching and regular matching, to get the target information. Theseapproaches are time-consuming and heavily depending on prior knowledge andcarefulness of the officers. To assist the officers to enhance workingefficiency and accuracy, we propose an approach to detect disputes from divorcecases based on a two-round-labeling event extracting technique in this paper.We implement the Judicial Intelligent Assistant (JIA) system according to theproposed approach to 1) automatically extract focus events from divorce casematerials, 2) align events by identifying co-reference among them, and 3)detect conflicts among events brought by the plaintiff and the defendant. Withthe JIA system, it is convenient for judges to determine the disputed issues.Experimental results demonstrate that the proposed approach and system canobtain the focus of cases and detect conflicts more effectively and efficientlycomparing with existing method.", "output": "Judicial Intelligent Assistant System: Extracting Events from Divorce Cases to Detect Disputes for the Judge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pretrained language models often generate outputs that are not in line withhuman preferences, such as harmful text or factually incorrect summaries.Recent work approaches the above issues by learning from a simple form of humanfeedback: comparisons between pairs of model-generated outputs. However,comparison feedback only conveys limited information about human preferences.In this paper, we introduce Imitation learning from Language Feedback (ILF), anew approach that utilizes more informative language feedback. ILF consists ofthree steps that are applied iteratively: first, conditioning the languagemodel on the input, an initial LM output, and feedback to generate refinements.Second, selecting the refinement incorporating the most feedback. Third,finetuning the language model to maximize the likelihood of the chosenrefinement given the input. We show theoretically that ILF can be viewed asBayesian Inference, similar to Reinforcement Learning from human feedback. Weevaluate ILF's effectiveness on a carefully-controlled toy task and a realisticsummarization task. Our experiments demonstrate that large language modelsaccurately incorporate feedback and that finetuning with ILF scales well withthe dataset size, even outperforming finetuning on human summaries. Learningfrom both language and comparison feedback outperforms learning from eachalone, achieving human-level summarization performance.", "output": "Training Language Models with Language Feedback at Scale."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The process of matching patients with suitable clinical trials is essentialfor advancing medical research and providing optimal care. However, currentapproaches face challenges such as data standardization, ethicalconsiderations, and a lack of interoperability between Electronic HealthRecords (EHRs) and clinical trial criteria. In this paper, we explore thepotential of large language models (LLMs) to address these challenges byleveraging their advanced natural language generation capabilities to improvecompatibility between EHRs and clinical trial descriptions. We propose aninnovative privacy-aware data augmentation approach for LLM-based patient-trialmatching (LLM-PTM), which balances the benefits of LLMs while ensuring thesecurity and confidentiality of sensitive patient data. Our experimentsdemonstrate a 7.32% average improvement in performance using the proposedLLM-PTM method, and the generalizability to new data is improved by 12.12%.Additionally, we present case studies to further illustrate the effectivenessof our approach and provide a deeper understanding of its underlyingprinciples.", "output": "LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The purpose of write-missing diagnosis detection is to find diseases thathave been clearly diagnosed from medical records but are missed in thedischarge diagnosis. Unlike the definition of missed diagnosis, thewrite-missing diagnosis is clearly manifested in the medical record withoutfurther reasoning. The write-missing diagnosis is a common problem, oftencaused by physician negligence. The write-missing diagnosis will result in anincomplete diagnosis of medical records. While under DRG grouping, thewrite-missing diagnoses will miss important additional diagnoses (CC, MCC),thus affecting the correct rate of DRG enrollment.Under the circumstance that countries generally start to adopt DRG enrollmentand payment, the problem of write-missing diagnosis is a common and seriousproblem. The current manual-based method is expensive due to the complexcontent of the full medical record. We think this problem is suitable to besolved as natural language processing. But to the best of our knowledge, noresearchers have conducted research on this problem based on natural languageprocessing methods.We propose a framework for solving the problem of write-missing diagnosis,which mainly includes three modules: disease recall module, disease contextlogic judgment module, and disease relationship comparison module. Through thisframework, we verify that the problem of write-missing diagnosis can be solvedwell, and the results are interpretable. At the same time, we propose advancedsolutions for the disease context logic judgment module and diseaserelationship comparison module, which have obvious advantages compared with themainstream methods of the same type of problems. Finally, we verified the valueof our proposed framework under DRG medical insurance payment in a tertiaryhospital.", "output": "How can Deep Learning Retrieve the Write-Missing Additional Diagnosis from Chinese Electronic Medical Record For DRG."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Swarm Intelligence algorithms have gained significant attention in recentyears as a means of solving complex and non-deterministic problems. Thesealgorithms are inspired by the collective behavior of natural creatures, andthey simulate this behavior to develop intelligent agents for computationaltasks. One such algorithm is Ant Colony Optimization (ACO), which is inspiredby the foraging behavior of ants and their pheromone laying mechanism. ACO isused for solving difficult problems that are discrete and combinatorial innature. Part-of-Speech (POS) tagging is a fundamental task in natural languageprocessing that aims to assign a part-of-speech role to each word in asentence. In this research paper, proposed a high-performance POS-taggingmethod based on ACO called ACO-tagger. This method achieved a high accuracyrate of 96.867%, outperforming several state-of-the-art methods. The proposedmethod is fast and efficient, making it a viable option for practicalapplications.", "output": "ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have witnessed an increasing amount of dialogue/conversation onthe web especially on social media. That inspires the development ofdialogue-based retrieval, in which retrieving videos based on dialogue is ofincreasing interest for recommendation systems. Different from other videoretrieval tasks, dialogue-to-video retrieval uses structured queries in theform of user-generated dialogue as the search descriptor. We present a noveldialogue-to-video retrieval system, incorporating structured conversationalinformation. Experiments conducted on the AVSD dataset show that our proposedapproach using plain-text queries improves over the previous counterpart modelby 15.8% on R@1. Furthermore, our approach using dialogue as a query, improvesretrieval performance by 4.2%, 6.2%, 8.6% on R@1, R@5 and R@10 and outperformsthe state-of-the-art model by 0.7%, 3.6% and 6.0% on R@1, R@5 and R@10respectively.", "output": "Dialogue-to-Video Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An abundance of information about cancer exists online, but categorizing andextracting useful information from it is difficult. Almost all research withinhealthcare data processing is concerned with formal clinical data, but there isvaluable information in non-clinical data too. The present study combinesmethods within distributed computing, text retrieval, clustering, andclassification into a coherent and computationally efficient system, that canclarify cancer patient trajectories based on non-clinical and freely availableinformation. We produce a fully-functional prototype that can retrieve, clusterand present information about cancer trajectories from non-clinical forumposts. We evaluate three clustering algorithms (MR-DBSCAN, DBSCAN, and HDBSCAN)and compare them in terms of Adjusted Rand Index and total run time as afunction of the number of posts retrieved and the neighborhood radius.Clustering results show that neighborhood radius has the most significantimpact on clustering performance. For small values, the data set is splitaccordingly, but high values produce a large number of possible partitions andsearching for the best partition is hereby time-consuming. With a properestimated radius, MR-DBSCAN can cluster 50000 forum posts in 46.1 seconds,compared to DBSCAN (143.4) and HDBSCAN (282.3). We conduct an interview withthe Danish Cancer Society and present our software prototype. The organizationsees a potential in software that can democratize online information aboutcancer and foresee that such systems will be required in the future.", "output": "Computationally Efficient Labeling of Cancer Related Forum Posts by Non-Clinical Text Information Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Measuring similarity between patents is an essential step to ensure noveltyof innovation. However, a large number of methods of measuring the similaritybetween patents still rely on manual classification of patents by experts.Another body of research has proposed automated methods; nevertheless, most ofit solely focuses on the semantic similarity of patents. In order to tacklethese limitations, we propose a hybrid method for automatically measuring thesimilarity between patents, considering both semantic and technologicalsimilarities. We measure the semantic similarity based on patent texts usingBERT, calculate the technological similarity with IPC codes using Jaccardsimilarity, and perform hybridization by assigning weights to the twosimilarity methods. Our evaluation result demonstrates that the proposed methodoutperforms the baseline that considers the semantic similarity only.", "output": "A Novel Patent Similarity Measurement Methodology: Semantic Distance and Technological Distance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cooking recipes allow individuals to exchange culinary ideas and provide foodpreparation instructions. Due to a lack of adequate labeled data, categorizingraw recipes found online to the appropriate food genres is a challenging taskin this domain. Utilizing the knowledge of domain experts to categorize recipescould be a solution. In this study, we present a novel dataset of two millionculinary recipes labeled in respective categories leveraging the knowledge offood experts and an active learning technique. To construct the dataset, wecollect the recipes from the RecipeNLG dataset. Then, we employ three humanexperts whose trustworthiness score is higher than 86.667% to categorize 300Krecipe by their Named Entity Recognition (NER) and assign it to one of the ninecategories: bakery, drinks, non-veg, vegetables, fast food, cereals, meals,sides and fusion. Finally, we categorize the remaining 1900K recipes usingActive Learning method with a blend of Query-by-Committee and Human In The Loop(HITL) approaches. There are more than two million recipes in our dataset, eachof which is categorized and has a confidence score linked with it. For the 9genres, the Fleiss Kappa score of this massive dataset is roughly 0.56026. Webelieve that the research community can use this dataset to perform variousmachine learning tasks such as recipe genre classification, recipe generationof a specific genre, new recipe creation, etc. The dataset can also be used totrain and evaluate the performance of various NLP tasks such as named entityrecognition, part-of-speech tagging, semantic role labeling, and so on. Thedataset will be available upon publication: ", "output": "Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Thistle, a fully functional vector database. Thistle is an entryinto the domain of latent knowledge use in answering search queries, an ongoingresearch topic at both start-ups and search engine companies. We implementThistle with several well-known algorithms, and benchmark results on the MSMARCO dataset. Results help clarify the latent knowledge domain as well as thegrowing Rust ML ecosystem.", "output": "Thistle: A Vector Database in Rust."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous agents that drive on roads shared with human drivers must reasonabout the nuanced interactions among traffic participants. This poses a highlychallenging decision making problem since human behavior is influenced by amultitude of factors (e.g., human intentions and emotions) that are hard tomodel. This paper presents a decision making approach for autonomous driving,focusing on the complex task of merging into moving traffic where uncertaintyemanates from the behavior of other drivers and imperfect sensor measurements.We frame the problem as a partially observable Markov decision process (POMDP)and solve it online with Monte Carlo tree search. The solution to the POMDP isa policy that performs high-level driving maneuvers, such as giving way to anapproaching car, keeping a safe distance from the vehicle in front or merginginto traffic. Our method leverages a model learned from data to predict thefuture states of traffic while explicitly accounting for interactions among thesurrounding agents. From these predictions, the autonomous vehicle cananticipate the future consequences of its actions on the environment andoptimize its trajectory accordingly. We thoroughly test our approach insimulation, showing that the autonomous vehicle can adapt its behavior todifferent situations. We also compare against other methods, demonstrating animprovement with respect to the considered performance metrics.", "output": "Decision Making for Autonomous Driving in Interactive Merge Scenarios via Learning-based Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a large-scale empirical investigation of the zero-shot learningphenomena in a specific recognizing textual entailment (RTE) task category,i.e. the automated mining of leaderboards for Empirical AI Research. The priorreported state-of-the-art models for leaderboards extraction formulated as anRTE task, in a non-zero-shot setting, are promising with above 90% reportedperformances. However, a central research question remains unexamined: did themodels actually learn entailment? Thus, for the experiments in this paper, twoprior reported state-of-the-art models are tested out-of-the-box for theirability to generalize or their capacity for entailment, given leaderboardlabels that were unseen during training. We hypothesize that if the modelslearned entailment, their zero-shot performances can be expected to bemoderately high as well--perhaps, concretely, better than chance. As a resultof this work, a zero-shot labeled dataset is created via distant labelingformulating the leaderboard extraction RTE task.", "output": "Zero-shot Entailment of Leaderboards for Empirical AI Research."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding 3D scenes from multi-view inputs has been proven to alleviatethe view discrepancy issue in 3D visual grounding. However, existing methodsnormally neglect the view cues embedded in the text modality and fail to weighthe relative importance of different views. In this paper, we proposeViewRefer, a multi-view framework for 3D visual grounding exploring how tograsp the view knowledge from both text and 3D modalities. For the text branch,ViewRefer leverages the diverse linguistic knowledge of large-scale languagemodels, e.g., GPT, to expand a single grounding text to multiplegeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformerfusion module with inter-view attention is introduced to boost the interactionof objects across views. On top of that, we further present a set of learnablemulti-view prototypes, which memorize scene-agnostic knowledge for differentviews, and enhance the framework from two perspectives: a view-guided attentionmodule for more robust text features, and a view-guided scoring strategy duringthe final prediction. With our designed paradigm, ViewRefer achieves superiorperformance on three benchmarks and surpasses the second-best by +2.8%, +1.2%,and +0.73% on Sr3D, Nr3D, and ScanRefer. Code will be released at", "output": "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inspired by the long-range modeling ability of ViTs, large-kernelconvolutions are widely studied and adopted recently to enlarge the receptivefield and improve model performance, like the remarkable work ConvNeXt whichemploys 7x7 depthwise convolution. Although such depthwise operator onlyconsumes a few FLOPs, it largely harms the model efficiency on powerfulcomputing devices due to the high memory access costs. For example, ConvNeXt-Thas similar FLOPs with ResNet-50 but only achieves 60% throughputs when trainedon A100 GPUs with full precision. Although reducing the kernel size of ConvNeXtcan improve speed, it results in significant performance degradation. It isstill unclear how to speed up large-kernel-based CNN models while preservingtheir performance. To tackle this issue, inspired by Inceptions, we propose todecompose large-kernel depthwise convolution into four parallel branches alongchannel dimension, i.e. small square kernel, two orthogonal band kernels, andan identity mapping. With this new Inception depthwise convolution, we build aseries of networks, namely IncepitonNeXt, which not only enjoy high throughputsbut also maintain competitive performance. For instance, InceptionNeXt-Tachieves 1.6x higher training throughputs than ConvNeX-T, as well as attains0.2% top-1 accuracy improvement on ImageNet-1K. We anticipate InceptionNeXt canserve as an economical baseline for future architecture design to reduce carbonfootprint. Code is available at ", "output": "InceptionNeXt: When Inception Meets ConvNeXt."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The capability of a reinforcement learning (RL) agent heavily depends on thediversity of the learning scenarios generated by the environment. Generation ofdiverse realistic scenarios is challenging for real-time strategy (RTS)environments. The RTS environments are characterized by intelligententities/non-RL agents cooperating and competing with the RL agents with largestate and action spaces over a long period of time, resulting in an infinitespace of feasible, but not necessarily realistic, scenarios involving complexinteraction among different RL and non-RL agents. Yet, most of the existingsimulators rely on randomly generating the environments based on predefinedsettings/layouts and offer limited flexibility and control over the environmentdynamics for researchers to generate diverse, realistic scenarios as per theirdemand. To address this issue, for the first time, we formally introduce thebenefits of adopting an existing formal scenario specification language,SCENIC, to assist researchers to model and generate diverse scenarios in an RTSenvironment in a flexible, systematic, and programmatic manner. To showcase thebenefits, we interfaced SCENIC to an existing RTS environment Google ResearchFootball(GRF) simulator and introduced a benchmark consisting of 32 realisticscenarios, encoded in SCENIC, to train RL agents and testing theirgeneralization capabilities. We also show how researchers/RL practitioners canincorporate their domain knowledge to expedite the training process byintuitively modeling stochastic programmatic policies with SCENIC.", "output": "Scenic4RL: Programmatic Modeling and Generation of Reinforcement Learning Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks have long been used to model human intelligence, capturingelements of behavior and cognition, and their neural basis. Recent advancementsin deep learning have enabled neural network models to reach and even surpasshuman levels of intelligence in many respects, yet unlike humans, their abilityto learn new tasks quickly remains a challenge. People can reason not only infamiliar domains, but can also rapidly learn to reason through novel problemsand situations, raising the question of how well modern neural network modelscapture human intelligence and in which ways they diverge. In this work, weexplore this gap by investigating human adults' ability to learn an abstractreasoning task based on Sudoku from a brief instructional tutorial withexplanatory feedback for incorrect responses using a narrow range of trainingexamples. We find that participants who master the task do so within a smallnumber of trials and generalize well to puzzles outside of the training range.We also find that most of those who master the task can describe a validsolution strategy, and such participants perform better on transfer puzzlesthan those whose strategy descriptions are vague or incomplete. Interestingly,fewer than half of our human participants were successful in acquiring a validsolution strategy, and this ability is associated with high school mathematicseducation. We consider the challenges these findings pose for buildingcomputational models that capture all aspects of our findings and point towarda possible role for learning to engage in explanation-based reasoning tosupport rapid learning and generalization.", "output": "Systematic human learning and generalization from a brief tutorial with explanatory feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The progress of some AI paradigms such as deep learning is said to be linkedto an exponential growth in the number of parameters. There are many studiescorroborating these trends, but does this translate into an exponentialincrease in energy consumption? In order to answer this question we focus oninference costs rather than training costs, as the former account for most ofthe computing effort, solely because of the multiplicative factors. Also, apartfrom algorithmic innovations, we account for more specific and powerfulhardware (leading to higher FLOPS) that is usually accompanied with importantenergy efficiency optimisations. We also move the focus from the firstimplementation of a breakthrough paper towards the consolidated version of thetechniques one or two year later. Under this distinctive and comprehensiveperspective, we study relevant models in the areas of computer vision andnatural language processing: for a sustained increase in performance we see amuch softer growth in energy consumption than previously anticipated. The onlycaveat is, yet again, the multiplicative factor, as future AI increasespenetration and becomes more pervasive.", "output": "Compute and Energy Consumption Trends in Deep Learning Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous robots can benefit greatly from human-provided semanticcharacterizations of uncertain task environments and states. However, thedevelopment of integrated strategies which let robots model, communicate, andact on such 'soft data' remains challenging. Here, the Human Assisted RoboticPlanning and Sensing (HARPS) framework is presented for active semantic sensingand planning in human-robot teams to address these gaps by formally combiningthe benefits of online sampling-based POMDP policies, multimodal semanticinteraction, and Bayesian data fusion. This approach lets humansopportunistically impose model structure and extend the range of semantic softdata in uncertain environments by sketching and labeling arbitrary landmarksacross the environment. Dynamic updating of the environment model while duringsearch allows robotic agents to actively query humans for novel and relevantsemantic data, thereby improving beliefs of unknown environments and states forimproved online planning. Simulations of a UAV-enabled target searchapplication in a large-scale partially structured environment show significantimprovements in time and belief state estimates required for interceptionversus conventional planning based solely on robotic sensing. Human subjectstudies in the same environment (n = 36) demonstrate an average doubling indynamic target capture rate compared to the lone robot case, and highlight therobustness of active probabilistic reasoning and semantic sensing over a rangeof user characteristics and interaction modalities.", "output": "HARPS: An Online POMDP Framework for Human-Assisted Robotic Planning and Sensing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In December 2019, a novel virus called COVID-19 had caused an enormous numberof causalities to date. The battle with the novel Coronavirus is baffling andhorrifying after the Spanish Flu 2019. While the front-line doctors and medicalresearchers have made significant progress in controlling the spread of thehighly contiguous virus, technology has also proved its significance in thebattle. Moreover, Artificial Intelligence has been adopted in many medicalapplications to diagnose many diseases, even baffling experienced doctors.Therefore, this survey paper explores the methodologies proposed that can aiddoctors and researchers in early and inexpensive methods of diagnosis of thedisease. Most developing countries have difficulties carrying out tests usingthe conventional manner, but a significant way can be adopted with Machine andDeep Learning. On the other hand, the access to different types of medicalimages has motivated the researchers. As a result, a mammoth number oftechniques are proposed. This paper first details the background knowledge ofthe conventional methods in the Artificial Intelligence domain. Following that,we gather the commonly used datasets and their use cases to date. In addition,we also show the percentage of researchers adopting Machine Learning over DeepLearning. Thus we provide a thorough analysis of this scenario. Lastly, in theresearch challenges, we elaborate on the problems faced in COVID-19 research,and we address the issues with our understanding to build a bright and healthyenvironment.", "output": "The Prominence of Artificial Intelligence in COVID-19."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) allows multiple medical institutions tocollaboratively learn a global model without centralizing client data. It isdifficult, if possible at all, for such a global model to commonly achieveoptimal performance for each individual client, due to the heterogeneity ofmedical images from various scanners and patient demographics. This problembecomes even more significant when deploying the global model to unseen clientsoutside the FL with unseen distributions not presented during federatedtraining. To optimize the prediction accuracy of each individual client formedical imaging tasks, we propose a novel unified framework for bothtextit{Inside and Outside model Personalization in FL} (IOP-FL). Our insidepersonalization uses a lightweight gradient-based approach that exploits thelocal adapted model for each client, by accumulating both the global gradientsfor common knowledge and the local gradients for client-specific optimization.Moreover, and importantly, the obtained local personalized models and theglobal model can form a diverse and informative routing space to personalize anadapted model for outside FL clients. Hence, we design a new test-time routingscheme using the consistency loss with a shape constraint to dynamicallyincorporate the models, given the distribution information conveyed by the testdata. Our extensive experimental results on two medical image segmentationtasks present significant improvements over SOTA methods on both inside andoutside personalization, demonstrating the potential of our IOP-FL scheme forclinical practice.", "output": "IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Block-based visual programming environments are increasingly used tointroduce computing concepts to beginners. Given that programming tasks areopen-ended and conceptual, novice students often struggle when learning inthese environments. AI-driven programming tutors hold great promise inautomatically assisting struggling students, and need several components torealize this potential. We investigate the crucial component of studentmodeling, in particular, the ability to automatically infer students'misconceptions for predicting (synthesizing) their behavior. We introduce anovel benchmark, StudentSyn, centered around the following challenge: For agiven student, synthesize the student's attempt on a new target task afterobserving the student's attempt on a fixed reference task. This challenge isakin to that of program synthesis; however, instead of synthesizing a{solution} (i.e., program an expert would write), the goal here is tosynthesize a {student attempt} (i.e., program that a given student wouldwrite). We first show that human experts (TutorSS) can achieve high performanceon the benchmark, whereas simple baselines perform poorly. Then, we develop twoneuro/symbolic techniques (NeurSS and SymSS) in a quest to close this gap withTutorSS.", "output": "From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In order for artificial agents to successfully perform tasks in changingenvironments, they must be able to both detect and adapt to novelty. However,visual novelty detection research often only evaluates on repurposed datasetssuch as CIFAR-10 originally intended for object classification, where imagesfocus on one distinct, well-centered object. New benchmarks are needed torepresent the challenges of navigating the complex scenes of an open world. Ournew NovelCraft dataset contains multimodal episodic data of the images andsymbolic world-states seen by an agent completing a pogo stick assembly taskwithin a modified Minecraft environment. In some episodes, we insert novelobjects of varying size within the complex 3D scene that may impact gameplay.Our visual novelty detection benchmark finds that methods that rank best onpopular area-under-the-curve metrics may be outperformed by simpleralternatives when controlling false positives matters most. Further multimodalnovelty detection experiments suggest that methods that fuse both visual andsymbolic information can improve time until detection as well as overalldiscrimination. Finally, our evaluation of recent generalized categorydiscovery methods suggests that adapting to new imbalanced categories incomplex scenes remains an exciting open problem.", "output": "NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated decision-making systems, especially those based on natural languageprocessing, are pervasive in our lives. They are not only behind the internetsearch engines we use daily, but also take more critical roles: selectingcandidates for a job, determining suspects of a crime, diagnosing autism andmore. Such automated systems make errors, which may be harmful in many ways, beit because of the severity of the consequences (as in health issues) or becauseof the sheer number of people they affect. When errors made by an automatedsystem affect a population more than others, we call the systemtextit{biased}.Most modern natural language technologies are based on artifacts obtainedfrom enormous volumes of text using machine learning, namely language modelsand word embeddings. Since they are created by applying subsymbolic machinelearning, mostly artificial neural networks, they are opaque and practicallyuninterpretable by direct inspection, thus making it very difficult to auditthem.In this paper, we present a methodology that spells out how socialscientists, domain experts, and machine learning experts can collaborativelyexplore biases and harmful stereotypes in word embeddings and large languagemodels. Our methodology is based on the following principles:* focus on the linguistic manifestations of discrimination on word embeddingsand language models, not on the mathematical properties of the models * reducethe technical barrier for discrimination experts%, be it social scientists,domain experts or other * characterize through a qualitative exploratoryprocess in addition to a metric-based approach * address mitigation as part ofthe training process, not as an afterthought", "output": "A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent research has revealed that Graph Neural Networks (GNNs) aresusceptible to adversarial attacks targeting the graph structure. A maliciousattacker can manipulate a limited number of edges, given the training labels,to impair the victim model's performance. Previous empirical studies indicatethat gradient-based attackers tend to add edges rather than remove them. Inthis paper, we present a theoretical demonstration revealing that attackerstend to increase inter-class edges due to the message passing mechanism ofGNNs, which explains some previous empirical observations. By connectingdissimilar nodes, attackers can more effectively corrupt node features, makingsuch attacks more advantageous. However, we demonstrate that the inherentsmoothness of GNN's message passing tends to blur node dissimilarity in thefeature space, leading to the loss of crucial information during the forwardprocess. To address this issue, we propose a novel surrogate model withmulti-level propagation that preserves the node dissimilarity information. Thismodel parallelizes the propagation of unaggregated raw features and multi-hopaggregated features, while introducing batch normalization to enhance thedissimilarity in node representations and counteract the smoothness resultingfrom topological aggregation. Our experiments show significant improvement withour approach.Furthermore, both theoretical and experimental evidence suggestthat adding inter-class edges constitutes an easily observable attack pattern.We propose an innovative attack loss that balances attack effectiveness andimperceptibility, sacrificing some attack effectiveness to attain greaterimperceptibility. We also provide experiments to validate the compromiseperformance achieved through this attack loss.", "output": "What Does the Gradient Tell When Attacking the Graph Structure."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Deepfake has drawn considerable public attention due to securityand privacy concerns in social media digital forensics. As the wildly spreadingDeepfake videos on the Internet become more realistic, traditional detectiontechniques have failed in distinguishing between real and fake. Most existingdeep learning methods mainly focus on local features and relations within theface image using convolutional neural networks as a backbone. However, localfeatures and relations are insufficient for model training to learn enoughgeneral information for Deepfake detection. Therefore, the existing Deepfakedetection methods have reached a bottleneck to further improve the detectionperformance. To address this issue, we propose a deep convolutional Transformerto incorporate the decisive image features both locally and globally.Specifically, we apply convolutional pooling and re-attention to enrich theextracted features and enhance efficacy. Moreover, we employ the barelydiscussed image keyframes in model training for performance improvement andvisualize the feature quantity gap between the key and normal image framescaused by video compression. We finally illustrate the transferability withextensive experiments on several Deepfake benchmark datasets. The proposedsolution consistently outperforms several state-of-the-art baselines on bothwithin- and cross-dataset experiments.", "output": "Deep Convolutional Pooling Transformer for Deepfake Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evolution of network virtualization and native artificial intelligence(AI) paradigms have conceptualized the vision of future wireless networks as acomprehensive entity operating in whole over a digital platform, with smartinteraction with the physical domain, paving the way for the blooming of theDigital Twin (DT) concept. The recent interest in the DT networks is fueled bythe emergence of novel wireless technologies and use-cases, that exacerbate thelevel of complexity to orchestrate the network and to manage its resources.Driven by AI, the key principle of the DT is to create a virtual twin for thephysical entities and network dynamics, where the virtual twin will beleveraged to generate synthetic data and offer an on-demand platform for AImodel training. Despite the common understanding that AI is the seed for DT, weanticipate that the DT and AI will be enablers for each other, in a way thatovercome their limitations and complement each other benefits. In this article,we dig into the fundamentals of DT, where we reveal the role of DT in unifyingmodel-driven and data-driven approaches, and explore the opportunities offeredby DT in order to achieve the optimistic vision of 6G networks. We furtherunfold the essential role of the theoretical underpinnings in unlocking furtheropportunities by AI, and hence, we unveil their pivotal impact on therealization of reliable, efficient, and low-latency DT.", "output": "The Interplay of AI and Digital Twin: Bridging the Gap between Data-Driven and Model-Driven Approaches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image completion is a task that aims to fill in the missing region of amasked image with plausible contents. However, existing image completionmethods tend to fill in the missing region with the surrounding texture insteadof hallucinating a visual instance that is suitable in accordance with thecontext of the scene. In this work, we propose a novel image completion model,dubbed ImComplete, that hallucinates the missing instance that harmonizes wellwith - and thus preserves - the original context. ImComplete first adopts atransformer architecture that considers the visible instances and the locationof the missing region. Then, ImComplete completes the semantic segmentationmasks within the missing region, providing pixel-level semantic and structuralguidance. Finally, the image synthesis blocks generate photo-realistic content.We perform a comprehensive evaluation of the results in terms of visual quality(LPIPS and FID) and contextual preservation scores (CLIPscore and objectdetection accuracy) with COCO-panoptic and Visual Genome datasets. Experimentalresults show the superiority of ImComplete on various natural images.", "output": "Instance-Aware Image Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Great endeavors have been made to study AI's ability in abstract reasoning,along with which different versions of RAVEN's progressive matrices (RPM) areproposed as benchmarks. Previous works give inkling that without sophisticateddesign or extra meta-data containing semantic information, neural networks maystill be indecisive in making decisions regarding RPM problems, afterrelentless training. Evidenced by thorough experiments and ablation studies, weshowcase that end-to-end neural networks embodied with felicitous inductivebias, intentionally design or serendipitously match, can solve RPM problemselegantly, without the augment of any extra meta-data or preferences of anyspecific backbone. Our work also reveals that multi-viewpoint withmulti-evaluation is a key learning strategy for successful reasoning. Finally,potential explanations for the failure of connectionist models ingeneralization are provided. We hope that these results will serve asinspections of AI's ability beyond perception and toward abstract reasoning.Source code can be found in ", "output": "Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In dynamic epistemic logic (Van Ditmarsch, Van Der Hoek, &amp; Kooi, 2008) it iscustomary to use an action frame (Baltag &amp; Moss, 2004; Baltag, Moss, &amp; Solecki,1998) to describe different views of a single action. In this article, actionframes are extended to add or remove agents, we call these agent-update frames.This can be done selectively so that only some specified agents get informationof the update, which can be used to model several interesting examples such asprivate update and deception, studied earlier by Baltag and Moss (2004); Sakama(2015); Van Ditmarsch, Van Eijck, Sietsma, and Wang (2012). The product updateof a Kripke model by an action frame is an abbreviated way of describing thetransformed Kripke model which is the result of performing the action. This issubstantially extended to a sum-product update of a Kripke model by anagent-update frame in the new setting. These ideas are applied to an AI problemof modelling a story. We show that dynamic epistemic logics, with updatemodalities now based on agent-update frames, continue to have sound andcomplete proof systems. Decision procedures for model checking andsatisfiability have expected complexity. For a sublanguage, there arepolynomial space algorithms.", "output": "Changing agents and ascribing beliefs in dynamic epistemic logic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of human reposing involves generating a realistic image of a personstanding in an arbitrary conceivable pose. There are multiple difficulties ingenerating perceptually accurate images, and existing methods suffer fromlimitations in preserving texture, maintaining pattern coherence, respectingcloth boundaries, handling occlusions, manipulating skin generation, etc. Thesedifficulties are further exacerbated by the fact that the possible space ofpose orientation for humans is large and variable, the nature of clothing itemsis highly non-rigid, and the diversity in body shape differs largely among thepopulation. To alleviate these difficulties and synthesize perceptuallyaccurate images, we propose VGFlow. Our model uses a visibility-guided flowmodule to disentangle the flow into visible and invisible parts of the targetfor simultaneous texture preservation and style manipulation. Furthermore, totackle distinct body shapes and avoid network artifacts, we also incorporate aself-supervised patch-wise \"realness\" loss to improve the output. VGFlowachieves state-of-the-art results as observed qualitatively and quantitativelyon different image quality metrics (SSIM, LPIPS, FID).", "output": "VGFlow: Visibility guided Flow Network for Human Reposing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attribution methods, which employ heatmaps to identify the most influentialregions of an image that impact model decisions, have gained widespreadpopularity as a type of explainability method. However, recent research hasexposed the limited practical value of these methods, attributed in part totheir narrow focus on the most prominent regions of an image -- revealing\"where\" the model looks, but failing to elucidate \"what\" the model sees inthose areas. In this work, we try to fill in this gap with CRAFT -- a novelapproach to identify both \"what\" and \"where\" by generating concept-basedexplanations. We introduce 3 new ingredients to the automatic conceptextraction literature: (i) a recursive strategy to detect and decomposeconcepts across layers, (ii) a novel method for a more faithful estimation ofconcept importance using Sobol indices, and (iii) the use of implicitdifferentiation to unlock Concept Attribution Maps.We conduct both human and computer vision experiments to demonstrate thebenefits of the proposed approach. We show that the proposed concept importanceestimation technique is more faithful to the model than previous methods. Whenevaluating the usefulness of the method for human experimenters on ahuman-centered utility benchmark, we find that our approach significantlyimproves on two of the three test scenarios. Our code is freely available atgithub.com/deel-ai/Craft.", "output": "CRAFT: Concept Recursive Activation FacTorization for Explainability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph convolutional networks (GCNs) is a class of artificial neural networksfor processing data that can be represented as graphs. Since financialtransactions can naturally be constructed as graphs, GCNs are widely applied inthe financial industry, especially for financial fraud detection. In thispaper, we focus on fraud detection on cryptocurrency truct networks. In theliterature, most works focus on static networks. Whereas in this study, weconsider the evolving nature of cryptocurrency networks, and use localstructural as well as the balance theory to guide the training process. Morespecifically, we compute motif matrices to capture the local topologicalinformation, then use them in the GCN aggregation process. The generatedembedding at each snapshot is a weighted average of embeddings within a timewindow, where the weights are learnable parameters. Since the trust networks issigned on each edge, balance theory is used to guide the training process.Experimental results on bitcoin-alpha and bitcoin-otc datasets show that theproposed model outperforms those in the literature.", "output": "Motif-aware temporal GCN for fraud detection in signed cryptocurrency trust networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When a large language model (LLM) performs complex reasoning by chain ofthought (CoT), it can be highly sensitive to individual mistakes. We have hadto train verifiers to address this issue. As we all know, after human inferringa conclusion, they often check it by re-verifying it, which can avoid somemistakes. We propose a new method called self-verification that uses theconclusion of the CoT as a condition to build a new sample and asks the LLM tore-predict the original conditions which be masked. We calculate an explainableverification score based on the accuracy. This method can improve the accuracyof multiple arithmetics and logical reasoning datasets when using few-shotlearning. we have demonstrated that LLMs can conduct explainableself-verification of their own conclusions and achieve competitive reasoningperformance. Extensive experimentals have demonstrated that our method can helpmultiple large language models with self-verification can avoid interferencefrom incorrect CoT. Code is available aturl{", "output": "Large Language Models are reasoners with Self-Verification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale neural network models combining text and images have madeincredible progress in recent years. However, it remains an open question towhat extent such models encode compositional representations of the conceptsover which they operate, such as correctly identifying ''red cube'' byreasoning over the constituents ''red'' and ''cube''. In this work, we focus onthe ability of a large pretrained vision and language model (CLIP) to encodecompositional concepts and to bind variables in a structure-sensitive way(e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). Inorder to inspect the performance of CLIP, we compare several architectures fromresearch on compositional distributional semantics models (CDSMs), a line ofresearch that attempts to implement traditional compositional linguisticstructures within embedding spaces. We find that CLIP can compose concepts in asingle-object setting, but in situations where concept binding is needed,performance drops dramatically. At the same time, CDSMs also perform poorly,with best performance at chance level.", "output": "Does CLIP Bind Concepts? Probing Compositionality in Large Image Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uniform-precision neural network quantization has gained popularity since itsimplifies densely packed arithmetic unit for high computing capability.However, it ignores heterogeneous sensitivity to the impact of quantizationerrors across the layers, resulting in sub-optimal inference accuracy. Thiswork proposes a novel neural architecture search called neural channelexpansion that adjusts the network structure to alleviate accuracy degradationfrom ultra-low uniform-precision quantization. The proposed method selectivelyexpands channels for the quantization sensitive layers while satisfyinghardware constraints (e.g., FLOPs, PARAMs). Based on in-depth analysis andexperiments, we demonstrate that the proposed method can adapt several popularnetworks channels to achieve superior 2-bit quantization accuracy on CIFAR10and ImageNet. In particular, we achieve the best-to-date Top-1/Top-5 accuracyfor 2-bit ResNet50 with smaller FLOPs and the parameter size.", "output": "Automatic Network Adaptation for Ultra-Low Uniform-Precision Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "IoU losses are surrogates that directly optimize the Jaccard index. Insemantic segmentation, leveraging IoU losses as part of the loss function isshown to perform better with respect to the Jaccard index measure thanoptimizing pixel-wise losses such as the cross-entropy loss alone. The mostnotable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss.However, these losses are incompatible with soft labels which are ubiquitous inmachine learning. In this paper, we propose Jaccard metric losses (JMLs), whichare identical to the soft Jaccard loss in a standard setting with hard labels,but are compatible with soft labels. With JMLs, we study two of the mostpopular use cases of soft labels: label smoothing and knowledge distillation.With a variety of architectures, our experiments show significant improvementsover the cross-entropy loss on three semantic segmentation datasets(Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approachoutperforms state-of-the-art knowledge distillation methods by a large margin.Code is available at:href{", "output": "Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT is a recent chatbot service released by OpenAI and is receivingincreasing attention over the past few months. While evaluations of variousaspects of ChatGPT have been done, its robustness, i.e., the performance tounexpected inputs, is still unclear to the public. Robustness is of particularconcern in responsible AI, especially for safety-critical applications. In thispaper, we conduct a thorough evaluation of the robustness of ChatGPT from theadversarial and out-of-distribution (OOD) perspective. To do so, we employ theAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkartreview and DDXPlus medical diagnosis datasets for OOD evaluation. We selectseveral popular foundation models as baselines. Results show that ChatGPT showsconsistent advantages on most adversarial and OOD classification andtranslation tasks. However, the absolute performance is far from perfection,which suggests that adversarial and OOD robustness remains a significant threatto foundation models. Moreover, ChatGPT shows astounding performance inunderstanding dialogue-related texts and we find that it tends to provideinformal suggestions for medical tasks instead of definitive answers. Finally,we present in-depth discussions of possible research directions.", "output": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most of the existing clustering methods are based on a single granularity ofinformation, such as the distance and density of each data. This mostfine-grained based approach is usually inefficient and susceptible to noise.Therefore, we propose a clustering algorithm that combines multi-granularityGranular-Ball and minimum spanning tree (MST). We construct coarsegrainedgranular-balls, and then use granular-balls and MST to implement the clusteringmethod based on \"large-scale priority\", which can greatly avoid the influenceof outliers and accelerate the construction process of MST. Experimentalresults on several data sets demonstrate the power of the algorithm. All codeshave been released at ", "output": "GBMST: An Efficient Minimum Spanning Tree Clustering Based on Granular-Ball Computing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we present I$^2$-SDF, a new method for intrinsic indoor scenereconstruction and editing using differentiable Monte Carlo raytracing onneural signed distance fields (SDFs). Our holistic neural SDF-based frameworkjointly recovers the underlying shapes, incident radiance and materials frommulti-view images. We introduce a novel bubble loss for fine-grained smallobjects and error-guided adaptive sampling scheme to largely improve thereconstruction quality on large-scale indoor scenes. Further, we propose todecompose the neural radiance field into spatially-varying material of thescene as a neural field through surface-based, differentiable Monte Carloraytracing and emitter semantic segmentations, which enables physically basedand photorealistic scene relighting and editing applications. Through a numberof qualitative and quantitative experiments, we demonstrate the superiorquality of our method on indoor scene reconstruction, novel view synthesis, andscene editing compared to state-of-the-art baselines.", "output": "I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The large language model called ChatGPT has drawn extensively attentionbecause of its human-like expression and reasoning abilities. In this study, weinvestigate the feasibility of using ChatGPT in experiments on using ChatGPT totranslate radiology reports into plain language for patients and healthcareproviders so that they are educated for improved healthcare. Radiology reportsfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRImetastases screening scans were collected in the first half of February forthis study. According to the evaluation by radiologists, ChatGPT cansuccessfully translate radiology reports into plain language with an averagescore of 4.27 in the five-point system with 0.08 places of information missingand 0.07 places of misinformation. In terms of the suggestions provided byChatGPT, they are general relevant such as keeping following-up with doctorsand closely monitoring any symptoms, and for about 37% of 138 cases in totalChatGPT offers specific suggestions based on findings in the report. ChatGPTalso presents some randomness in its responses with occasionallyover-simplified or neglected information, which can be mitigated using a moredetailed prompt. Furthermore, ChatGPT results are compared with a newlyreleased large model GPT-4, showing that GPT-4 can significantly improve thequality of translated reports. Our results show that it is feasible to utilizelarge language models in clinical education, and further efforts are needed toaddress limitations and maximize their potential.", "output": "Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial intelligence (AI) systems will increasingly be used to cause harmas they grow more capable. In fact, AI systems are already starting to be usedto automate fraudulent activities, violate human rights, create harmful fakeimages, and identify dangerous toxins. To prevent some misuses of AI, we arguethat targeted interventions on certain capabilities will be warranted. Theserestrictions may include controlling who can access certain types of AI models,what they can be used for, whether outputs are filtered or can be traced backto their user, and the resources needed to develop them. We also contend thatsome restrictions on non-AI capabilities needed to cause harm will be required.Though capability restrictions risk reducing use more than misuse (facing anunfavorable Misuse-Use Tradeoff), we argue that interventions on capabilitiesare warranted when other interventions are insufficient, the potential harmfrom misuse is high, and there are targeted ways to intervene on capabilities.We provide a taxonomy of interventions that can reduce AI misuse, focusing onthe specific steps required for a misuse to cause harm (the Misuse Chain), anda framework to determine if an intervention is warranted. We apply thisreasoning to three examples: predicting novel toxins, creating harmful images,and automating spear phishing campaigns.", "output": "Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Thanks to the augmented convenience, safety advantages, and potentialcommercial value, Intelligent vehicles (IVs) have attracted wide attentionthroughout the world. Although a few autonomous driving unicorns assert thatIVs will be commercially deployable by 2025, their implementation is stillrestricted to small-scale validation due to various issues, among which precisecomputation of control commands or trajectories by planning methods remains aprerequisite for IVs. This paper aims to review state-of-the-art planningmethods, including pipeline planning and end-to-end planning methods. In termsof pipeline methods, a survey of selecting algorithms is provided along with adiscussion of the expansion and optimization mechanisms, whereas in end-to-endmethods, the training approaches and verification scenarios of driving tasksare points of concern. Experimental platforms are reviewed to facilitatereaders in selecting suitable training and validation methods. Finally, thecurrent challenges and future directions are discussed. The side-by-sidecomparison presented in this survey not only helps to gain insights into thestrengths and limitations of the reviewed methods but also assists withsystem-level design choices.", "output": "Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human mesh recovery (HMR) provides rich human body information for variousreal-world applications such as gaming, human-computer interaction, and virtualreality. Compared to single image-based methods, video-based methods canutilize temporal information to further improve performance by incorporatinghuman body motion priors. However, many-to-many approaches such as VIBE sufferfrom motion smoothness and temporal inconsistency. While many-to-one approachessuch as TCMR and MPS-Net rely on the future frames, which is non-causal andtime inefficient during inference. To address these challenges, a novelDiffusion-Driven Transformer-based framework (DDT) for video-based HMR ispresented. DDT is designed to decode specific motion patterns from the inputsequence, enhancing motion smoothness and temporal consistency. As amany-to-many approach, the decoder of our DDT outputs the human mesh of all theframes, making DDT more viable for real-world applications where timeefficiency is crucial and a causal model is desired. Extensive experiments areconducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),which demonstrated the effectiveness and efficiency of our DDT.", "output": "DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Event cameras sense the intensity changes asynchronously and produce eventstreams with high dynamic range and low latency. This has inspired researchendeavors utilizing events to guide the challenging video superresolution (VSR)task. In this paper, we make the first attempt to address a novel problem ofachieving VSR at random scales by taking advantages of the high temporalresolution property of events. This is hampered by the difficulties ofrepresenting the spatial-temporal information of events when guiding VSR. Tothis end, we propose a novel framework that incorporates the spatial-temporalinterpolation of events to VSR in a unified framework. Our key idea is to learnimplicit neural representations from queried spatial-temporal coordinates andfeatures from both RGB frames and events. Our method contains three parts.Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3Dfeatures from events and RGB frames. Then, the Temporal Filter (TF) moduleunlocks more explicit motion information from the events near the queriedtimestamp and generates the 2D features. Lastly, the SpatialTemporal ImplicitRepresentation (STIR) module recovers the SR frame in arbitrary resolutionsfrom the outputs of these two modules. In addition, we collect a real-worlddataset with spatially aligned events and RGB frames. Extensive experimentsshow that our method significantly surpasses the prior-arts and achieves VSRwith random scales, e.g., 6.5. Code and dataset are available at ", "output": "Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the use of machine learning continues to expand, the importance ofensuring its safety cannot be overstated. A key concern in this regard is theability to identify whether a given sample is from the training distribution,or is an \"Out-Of-Distribution\" (OOD) sample. In addition, adversaries canmanipulate OOD samples in ways that lead a classifier to make a confidentprediction. In this study, we present a novel approach for certifying therobustness of OOD detection within a $ell_2$-norm around the input, regardlessof network architecture and without the need for specific components oradditional training. Further, we improve current techniques for detectingadversarial attacks on OOD samples, while providing high levels of certifiedand adversarial robustness on in-distribution samples. The average of all OODdetection metrics on CIFAR10/100 shows an increase of $sim 13 % / 5%$relative to previous approaches.", "output": "Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of machine learning has rapidly advanced the state of the art inmany fields of science and engineering, including experimental fluid dynamics,which is one of the original big-data disciplines. This perspective willhighlight several aspects of experimental fluid mechanics that stand to benefitfrom progress advances in machine learning, including: 1) augmenting thefidelity and quality of measurement techniques, 2) improving experimentaldesign and surrogate digital-twin models and 3) enabling real-time estimationand control. In each case, we discuss recent success stories and ongoingchallenges, along with caveats and limitations, and outline the potential fornew avenues of ML-augmented and ML-enabled experimental fluid mechanics.", "output": "The transformative potential of machine learning for experiments in fluid mechanics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite its pivotal role in research experiments, code correctness is oftenpresumed only on the basis of the perceived quality of the results. This comeswith the risk of erroneous outcomes and potentially misleading findings. Toaddress this issue, we posit that the current focus on result reproducibilityshould go hand in hand with the emphasis on coding best practices. We bolsterour call to the NLP community by presenting a case study, in which we identify(and correct) three bugs in widely used open-source implementations of thestate-of-the-art Conformer architecture. Through comparative experiments onautomatic speech recognition and translation in various language settings, wedemonstrate that the existence of bugs does not prevent the achievement of goodand reproducible results and can lead to incorrect conclusions that potentiallymisguide future research. In response to this, this study is a call to actiontoward the adoption of coding best practices aimed at fostering correctness andimproving the quality of the developed software.", "output": "Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent wave of large-scale text-to-image diffusion models hasdramatically increased our text-based image generation abilities. These modelscan generate realistic images for a staggering variety of prompts and exhibitimpressive compositional generalization abilities. Almost all use cases thusfar have solely focused on sampling; however, diffusion models can also provideconditional density estimates, which are useful for tasks beyond imagegeneration. In this paper, we show that the density estimates from large-scaletext-to-image diffusion models like Stable Diffusion can be leveraged toperform zero-shot classification without any additional training. Ourgenerative approach to classification, which we call Diffusion Classifier,attains strong results on a variety of benchmarks and outperforms alternativemethods of extracting knowledge from diffusion models. Although a gap remainsbetween generative and discriminative approaches on zero-shot recognitiontasks, we find that our diffusion-based approach has stronger multimodalrelational reasoning abilities than competing discriminative approaches.Finally, we use Diffusion Classifier to extract standard classifiers fromclass-conditional diffusion models trained on ImageNet. Even though thesemodels are trained with weak augmentations and no regularization, they approachthe performance of SOTA discriminative classifiers. Overall, our results are astep toward using generative over discriminative models for downstream tasks.Results and visualizations at ", "output": "Your Diffusion Model is Secretly a Zero-Shot Classifier."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing foundation models are trained on copyrighted material. Deployingthese models can pose both legal and ethical risks when data creators fail toreceive appropriate attribution or compensation. In the United States andseveral other countries, copyrighted content may be used to build foundationmodels without incurring liability due to the fair use doctrine. However, thereis a caveat: If the model produces output that is similar to copyrighted data,particularly in scenarios that affect the market of that data, fair use may nolonger apply to the output of the model. In this work, we emphasize that fairuse is not guaranteed, and additional work may be necessary to keep modeldevelopment and deployment squarely in the realm of fair use. First, we surveythe potential risks of developing and deploying foundation models based oncopyrighted content. We review relevant U.S. case law, drawing parallels toexisting and potential applications for generating text, source code, andvisual art. Experiments confirm that popular foundation models can generatecontent considerably similar to copyrighted material. Second, we discusstechnical mitigations that can help foundation models stay in line with fairuse. We argue that more research is needed to align mitigation strategies withthe current state of the law. Lastly, we suggest that the law and technicalmitigations should co-evolve. For example, coupled with other policymechanisms, the law could more explicitly consider safe harbors when strongtechnical tools are used to mitigate infringement harms. This co-evolution mayhelp strike a balance between intellectual property and innovation, whichspeaks to the original goal of fair use. But we emphasize that the strategieswe describe here are not a panacea and more work is needed to develop policiesthat address the potential harms of foundation models.", "output": "Foundation Models and Fair Use."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image steganography is the process of concealing secret information in imagesthrough imperceptible changes. Recent work has formulated this task as aclassic constrained optimization problem. In this paper, we argue that imagesteganography is inherently performed on the (elusive) manifold of naturalimages, and propose an iterative neural network trained to perform theoptimization steps. In contrast to classical optimization methods like L-BFGSor projected gradient descent, we train the neural network to also stay closeto the manifold of natural images throughout the optimization. We show that ourlearned neural optimization is faster and more reliable than classicaloptimization approaches. In comparison to previous state-of-the-artencoder-decoder-based steganography methods, it reduces the recovery error rateby multiple orders of magnitude and achieves zero error up to 3 bits per pixel(bpp) without the need for error-correcting codes.", "output": "Learning Iterative Neural Optimizers for Image Steganography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has the potential to benefit manyapplications, particularly those where manually annotating data is cumbersome.One such situation is the semantic segmentation of point clouds. In thiscontext, existing methods employ contrastive learning strategies and definepositive pairs by performing various augmentation of point clusters in a singleframe. As such, these methods do not exploit the temporal nature of LiDAR data.In this paper, we introduce an SSL strategy that leverages positive pairs inboth the spatial and temporal domain. To this end, we design (i) apoint-to-cluster learning strategy that aggregates spatial information todistinguish objects; and (ii) a cluster-to-cluster learning strategy based onunsupervised object tracking that exploits temporal correspondences. Wedemonstrate the benefits of our approach via extensive experiments performed byself-supervised training on two large-scale LiDAR datasets and transferring theresulting models to other point cloud segmentation benchmarks. Our resultsevidence that our method outperforms the state-of-the-art point cloud SSLmethods.", "output": "Spatiotemporal Self-supervised Learning for Point Clouds in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image arbitrary-scale super-resolution (MIASSR) has recently gainedwidespread attention, aiming to super sample medical volumes at arbitraryscales via a single model. However, existing MIASSR methods face two majorlimitations: (i) reliance on high-resolution (HR) volumes and (ii) limitedgeneralization ability, which restricts their application in various scenarios.To overcome these limitations, we propose Cube-based Neural Radiance Field(CuNeRF), a zero-shot MIASSR framework that can yield medical images atarbitrary scales and viewpoints in a continuous domain. Unlike existing MIASSRmethods that fit the mapping between low-resolution (LR) and HR volumes, CuNeRFfocuses on building a coordinate-intensity continuous representation from LRvolumes without the need for HR references. This is achieved by the proposeddifferentiable modules: including cube-based sampling, isotropic volumerendering, and cube-based hierarchical rendering. Through extensive experimentson magnetic resource imaging (MRI) and computed tomography (CT) modalities, wedemonstrate that CuNeRF outperforms state-of-the-art MIASSR methods. CuNeRFyields better visual verisimilitude and reduces aliasing artifacts at variousupsampling factors. Moreover, our CuNeRF does not need any LR-HR trainingpairs, which is more flexible and easier to be used than others. Our code willbe publicly available soon.", "output": "CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image Arbitrary-Scale Super Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Learning based diagnostics systems can provide accurate and robustquantitative analysis in digital pathology. Training these algorithms requireslarge amounts of annotated data which is impractical in pathology due to thehigh resolution of histopathological images. Hence, self-supervised methodshave been proposed to learn features using ad-hoc pretext tasks. Theself-supervised training process is time consuming and often leads to subparfeature representation due to a lack of constrain on the learnt feature space,particularly prominent under data imbalance. In this work, we propose toactively sample the training set using a handful of labels and a small proxynetwork, decreasing sample requirement by 93% and training time by 99%.", "output": "Data Efficient Contrastive Learning in Histopatholgy using Active Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-resolution heterogeneous reconstruction of 3D structures of proteins andother biomolecules using cryo-electron microscopy (cryo-EM) is essential forunderstanding fundamental processes of life. However, it is still challengingto reconstruct the continuous motions of 3D structures from hundreds ofthousands of noisy and randomly oriented 2D cryo-EM images. Existing methodsbased on coordinate-based neural networks show compelling results to modelcontinuous conformations of 3D structures in the Fourier domain, but theysuffer from a limited ability to model local flexible regions and lackinterpretability. We propose a novel approach, cryoFormer, that utilizes atransformer-based network architecture for continuous heterogeneous cryo-EMreconstruction. We for the first time directly reconstruct continuousconformations of 3D structures using an implicit feature volume in the 3Dspatial domain. A novel deformation transformer decoder further improvesreconstruction quality and, more importantly, locates and robustly tacklesflexible 3D regions caused by conformations. In experiments, our methodoutperforms current approaches on three public datasets (1 synthetic and 2experimental) and a new synthetic dataset of PEDV spike protein. The code andnew synthetic dataset will be released for better reproducibility of ourresults. Project page: ", "output": "CryoFormer: Continuous Reconstruction of 3D Structures from Cryo-EM Data using Transformer-based Neural Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The paper discusses an approach to decipher large collections of handwrittenindex cards of historical dictionaries. Our study provides a working solutionthat reads the cards, and links their lemmas to a searchable list of dictionaryentries, for a large historical dictionary entitled the Dictionary of the 17th-and 18th-century Polish, which comprizes 2.8 million index cards. We apply atailored handwritten text recognition (HTR) solution that involves (1) anoptimized detection model; (2) a recognition model to decipher the handwrittencontent, designed as a spatial transformer network (STN) followed byconvolutional neural network (RCNN) with a connectionist temporalclassification layer (CTC), trained using a synthetic set of 500,000 generatedPolish words of different length; (3) a post-processing step using constrainedWord Beam Search (WBC): the predictions were matched against a list ofdictionary entries known in advance. Our model achieved the accuracy of 0.881on the word level, which outperforms the base RCNN model. Within this study weproduced a set of 20,000 manually annotated index cards that can be used forfuture benchmarks and transfer learning HTR applications.", "output": "Scalable handwritten text recognition system for lexicographic sources of under-resourced languages and alphabets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-Supervised Learning can be more beneficial for the video domain comparedto images because of its higher annotation cost and dimensionality. Besides,any video understanding task requires reasoning over both spatial and temporaldimensions. In order to learn both the static and motion related features forthe semi-supervised action recognition task, existing methods rely on hardinput inductive biases like using two-modalities (RGB and Optical-flow) ortwo-stream of different playback rates. Instead of utilizing unlabeled videosthrough diverse input streams, we rely on self-supervised videorepresentations, particularly, we utilize temporally-invariant andtemporally-distinctive representations. We observe that these representationscomplement each other depending on the nature of the action. Based on thisobservation, we propose a student-teacher semi-supervised learning framework,TimeBalance, where we distill the knowledge from a temporally-invariant and atemporally-distinctive teacher. Depending on the nature of the unlabeled video,we dynamically combine the knowledge of these two teachers based on a noveltemporal similarity-based reweighting scheme. Our method achievesstate-of-the-art performance on three action recognition benchmarks: UCF101,HMDB51, and Kinetics400. Code: ", "output": "TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An unpaired image-to-image (I2I) translation technique seeks to find amapping between two domains of data in a fully unsupervised manner. While theinitial solutions to the I2I problem were provided by the generativeadversarial neural networks (GANs), currently, diffusion models (DM) hold thestate-of-the-art status on the I2I translation benchmarks in terms of FID. Yet,they suffer from some limitations, such as not using data from the sourcedomain during the training, or maintaining consistency of the source andtranslated images only via simple pixel-wise errors. This work revisits theclassic CycleGAN model and equips it with recent advancements in modelarchitectures and model training procedures. The revised model is shown tosignificantly outperform other advanced GAN- and DM-based competitors on avariety of benchmarks. In the case of Male2Female translation of CelebA, themodel achieves over 40% improvement in FID score compared to thestate-of-the-art results. This work also demonstrates the ineffectiveness ofthe pixel-wise I2I translation faithfulness metrics and suggests theirrevision. The code and trained models are available at", "output": "Rethinking CycleGAN: Improving Quality of GANs for Unpaired Image-to-Image Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning-based 3D object reconstruction has achieved unprecedentedresults. Among those, the transformer deep neural model showed outstandingperformance in many applications of computer vision. We introduceSnakeVoxFormer, a novel, 3D object reconstruction in voxel space from a singleimage using the transformer. The input to SnakeVoxFormer is a 2D image, and theresult is a 3D voxel model. The key novelty of our approach is in using therun-length encoding that traverses (like a snake) the voxel space and encodeswide spatial differences into a 1D structure that is suitable for transformerencoding. We then use dictionary encoding to convert the discovered RLE blocksinto tokens that are used for the transformer. The 1D representation is alossless 3D shape data compression method that converts to 1D data that useonly about 1% of the original data size. We show how different voxel traversingstrategies affect the effect of encoding and reconstruction. We compare ourmethod with the state-of-the-art for 3D voxel reconstruction from images andour method improves the state-of-the-art methods by at least 2.8% and up to19.8%.", "output": "SnakeVoxFormer: Transformer-based Single Image\\\\Voxel Reconstruction with Run Length Encoding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The soft Dice loss (SDL) has taken a pivotal role in many automatedsegmentation pipelines in the medical imaging community. Over the last years,some reasons behind its superior functioning have been uncovered and furtheroptimizations have been explored. However, there is currently no implementationthat supports its direct use in settings with soft labels. Hence, a synergybetween the use of SDL and research leveraging the use of soft labels, also inthe context of model calibration, is still missing. In this work, we introduceDice semimetric losses (DMLs), which (i) are by design identical to SDL in astandard setting with hard labels, but (ii) can be used in settings with softlabels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirmthe potential synergy of DMLs with soft labels (e.g. averaging, labelsmoothing, and knowledge distillation) over hard labels (e.g. majority votingand random selection). As a result, we obtain superior Dice scores and modelcalibration, which supports the wider adoption of DMLs in practice. Code isavailable athref{", "output": "Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting crime using machine learning and deep learning techniques hasgained considerable attention from researchers in recent years, focusing onidentifying patterns and trends in crime occurrences. This review paperexamines over 150 articles to explore the various machine learning and deeplearning algorithms applied to predict crime. The study provides access to thedatasets used for crime prediction by researchers and analyzes prominentapproaches applied in machine learning and deep learning algorithms to predictcrime, offering insights into different trends and factors related to criminalactivities. Additionally, the paper highlights potential gaps and futuredirections that can enhance the accuracy of crime prediction. Finally, thecomprehensive overview of research discussed in this paper on crime predictionusing machine learning and deep learning approaches serves as a valuablereference for researchers in this field. By gaining a deeper understanding ofcrime prediction techniques, law enforcement agencies can develop strategies toprevent and respond to criminal activities more effectively.", "output": "Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Purpose: Medical imaging has become increasingly important in diagnosing andtreating oncological patients, particularly in radiotherapy. Recent advances insynthetic computed tomography (sCT) generation have increased interest inpublic challenges to provide data and evaluation metrics for comparingdifferent approaches openly. This paper describes a dataset of brain and pelviscomputed tomography (CT) images with rigidly registered CBCT and MRI images tofacilitate the development and evaluation of sCT generation for radiotherapyplanning.Acquisition and validation methods: The dataset consists of CT, CBCT, and MRIof 540 brains and 540 pelvic radiotherapy patients from three Dutch universitymedical centers. Subjects' ages ranged from 3 to 93 years, with a mean age of60. Various scanner models and acquisition settings were used across patientsfrom the three data-providing centers. Details are available in CSV filesprovided with the datasets.Data format and usage notes: The data is available on Zenodo( under the SynthRAD2023 collection. Theimages for each subject are available in nifti format.Potential applications: This dataset will enable the evaluation anddevelopment of image synthesis algorithms for radiotherapy purposes on arealistic multi-center dataset with varying acquisition protocols. Synthetic CTgeneration has numerous applications in radiation therapy, including diagnosis,treatment planning, treatment monitoring, and surgical planning.", "output": "SynthRAD2023 Grand Challenge dataset: generating synthetic CT for radiotherapy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FMAS, a fast multi-objective neural architecture search frameworkfor semantic segmentation. FMAS subsamples the structure and pre-trainedparameters of DeepLabV3+, without fine-tuning, dramatically reducing trainingtime during search. To further reduce candidate evaluation time, we use asubset of the validation dataset during the search. Only the final, Paretonon-dominated, candidates are ultimately fine-tuned using the complete trainingset. We evaluate FMAS by searching for models that effectively trade accuracyand computational cost on the PASCAL VOC 2012 dataset. FMAS finds competitivedesigns quickly, e.g., taking just 0.5 GPU days to discover a DeepLabV3+variant that reduces FLOPs and parameters by 10$%$ and 20$%$ respectively,for less than 3$%$ increased error. We also search on an edge device calledGAP8 and use its latency as the metric. FMAS is capable of finding 2.2$times$faster network with 7.61$%$ MIoU loss.", "output": "FMAS: Fast Multi-Objective SuperNet Architecture Search for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we present a new method for deformable NeRF that can directlyuse optical flow as supervision. We overcome the major challenge with respectto the computationally inefficiency of enforcing the flow constraints to thebackward deformation field, used by deformable NeRFs. Specifically, we showthat inverting the backward deformation function is actually not needed forcomputing scene flows between frames. This insight dramatically simplifies theproblem, as one is no longer constrained to deformation functions that can beanalytically inverted. Instead, thanks to the weak assumptions required by ourderivation based on the inverse function theorem, our approach can be extendedto a broad class of commonly used backward deformation field. We presentresults on monocular novel view synthesis with rapid object motion, anddemonstrate significant improvements over baselines without flow supervision.", "output": "Flow supervision for Deformable NeRF."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most of existing video-language pre-training methods focus on instance-levelalignment between video clips and captions via global contrastive learning butneglect rich fine-grained local information, which is of importance todownstream tasks requiring temporal localization and semantic reasoning. Inthis work, we propose a simple yet effective video-language pre-trainingframework, namely G-ViLM, to learn discriminative spatiotemporal features. Twonovel designs involving spatiotemporal grounding and temporal grouping promotelearning local region-noun alignment and temporal-aware featuressimultaneously. Specifically, spatiotemporal grounding aggregates semanticallysimilar video tokens and aligns them with noun phrases extracted from thecaption to promote local region-noun correspondences. Moreover, temporalgrouping leverages cut-and-paste to manually create temporal scene changes andthen learns distinguishable features from different scenes. Comprehensiveevaluations demonstrate that G-ViLM performs favorably against existingapproaches on four representative downstream tasks, covering text-videoretrieval, video question answering, video action recognition and temporalaction localization. G-ViLM performs competitively on all evaluated tasks andin particular achieves R@10 of 65.1 on zero-shot MSR-VTT retrieval, over 9%higher than the state-of-the-art method.", "output": "Spatiotemporally Discriminative Video-Language Pre-Training with Text Grounding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a self-supervised approach for learning to perform audio sourceseparation in videos based on natural language queries, using only unlabeledvideo and audio pairs as training data. A key challenge in this task islearning to associate the linguistic description of a sound-emitting object toits visual features and the corresponding components of the audio waveform, allwithout access to annotations during training. To overcome this challenge, weadapt off-the-shelf vision-language foundation models to provide pseudo-targetsupervision via two novel loss functions and encourage a stronger alignmentbetween the audio, visual and natural language modalities. During inference,our approach can separate sounds given text, video and audio input, or giventext and audio input alone. We demonstrate the effectiveness of ourself-supervised approach on three audio-visual separation datasets, includingMUSIC, SOLOS and AudioSet, where we outperform state-of-the-art stronglysupervised approaches despite not using object detectors or text labels duringtraining.", "output": "Language-Guided Audio-Visual Source Separation via Trimodal Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A facial recognition algorithm was used to extract face descriptors fromcarefully standardized images of 591 neutral faces taken in the laboratorysetting. Face descriptors were entered into a cross-validated linear regressionto predict participants' scores on a political orientation scale (Cronbach'salpha=.94) while controlling for age, gender, and ethnicity. The model'sperformance exceeded r=.20: much better than that of human raters and on parwith how well job interviews predict job success, alcohol drivesaggressiveness, or psychological therapy improves mental health. Moreover, themodel derived from standardized images performed well (r=.12) in a sample ofnaturalistic images of 3,401 politicians from the U.S., UK, and Canada,suggesting that the associations between facial appearance and politicalorientation generalize beyond our sample. The analysis of facial featuresassociated with political orientation revealed that conservatives had largerlower faces, although political orientation was only weakly associated withbody mass index (BMI). The predictability of political orientation fromstandardized images has critical implications for privacy, regulation of facialrecognition technology, as well as the understanding the origins andconsequences of political orientation.", "output": "Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the record-breaking performance in Text-to-Image (T2I) generation byStable Diffusion, less research attention is paid to its adversarialrobustness. In this work, we study the problem of adversarial attack generationfor Stable Diffusion and ask if an adversarial text prompt can be obtained evenin the absence of end-to-end model queries. We call the resulting problem'query-free attack generation'. To resolve this problem, we show that thevulnerability of T2I models is rooted in the lack of robustness of textencoders, e.g., the CLIP text encoder used for attacking Stable Diffusion.Based on such insight, we propose both untargeted and targeted query-freeattacks, where the former is built on the most influential dimensions in thetext embedding space, which we call steerable key dimensions. By leveraging theproposed attacks, we empirically show that only a five-character perturbationto the text prompt is able to cause the significant content shift ofsynthesized images using Stable Diffusion. Moreover, we show that the proposedtarget attack can precisely steer the diffusion model to scrub the targetedimage content without causing much change in untargeted image content.", "output": "A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces Amazon Robotic Manipulation Benchmark (ARMBench), alarge-scale, object-centric benchmark dataset for robotic manipulation in thecontext of a warehouse. Automation of operations in modern warehouses requiresa robotic manipulator to deal with a wide variety of objects, unstructuredstorage, and dynamically changing inventory. Such settings pose challenges inperceiving the identity, physical characteristics, and state of objects duringmanipulation. Existing datasets for robotic manipulation consider a limited setof objects or utilize 3D models to generate synthetic scenes with limitation incapturing the variety of object properties, clutter, and interactions. Wepresent a large-scale dataset collected in an Amazon warehouse using a roboticmanipulator performing object singulation from containers with heterogeneouscontents. ARMBench contains images, videos, and metadata that corresponds to235K+ pick-and-place activities on 190K+ unique objects. The data is capturedat different stages of manipulation, i.e., pre-pick, during transfer, and afterplacement. Benchmark tasks are proposed by virtue of high-quality annotationsand baseline performance evaluation are presented on three visual perceptionchallenges, namely 1) object segmentation in clutter, 2) object identification,and 3) defect detection. ARMBench can be accessed at <a href=\" http URL</a>", "output": "ARMBench: An Object-centric Benchmark Dataset for Robotic Manipulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is growing interest in searching for information from large videocorpora. Prior works have studied relevant tasks, such as text-based videoretrieval, moment retrieval, video summarization, and video captioning inisolation, without an end-to-end setup that can jointly search from videocorpora and generate summaries. Such an end-to-end setup would allow for manyinteresting applications, e.g., a text-based search that finds a relevant videofrom a video corpus, extracts the most relevant moment from that video, andsegments the moment into important steps with captions. To address this, wepresent the HiREST (HIerarchical REtrieval and STep-captioning) dataset andpropose a new benchmark that covers hierarchical information retrieval andvisual/textual stepwise summarization from an instructional video corpus.HiREST consists of 3.4K text-video pairs from an instructional video dataset,where 1.1K videos have annotations of moment spans relevant to text query andbreakdown of each moment into key instruction steps with caption and timestamps(totaling 8.6K step captions). Our hierarchical benchmark consists of videoretrieval, moment retrieval, and two novel moment segmentation and stepcaptioning tasks. In moment segmentation, models break down a video moment intoinstruction steps and identify start-end boundaries. In step captioning, modelsgenerate a textual summary for each step. We also present starting pointtask-specific and end-to-end joint baseline models for our new benchmark. Whilethe baseline models show some promising results, there still exists large roomfor future improvement by the community. Project website:", "output": "Hierarchical Video-Moment Retrieval and Step-Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision is a popular and effective sensor for robotics from which we canderive rich information about the environment: the geometry and semantics ofthe scene, as well as the age, gender, identity, activity and even emotionalstate of humans within that scene. This raises important questions about thereach, lifespan, and potential misuse of this information. This paper is a callto action to consider privacy in the context of robotic vision. We propose aspecific form privacy preservation in which no images are captured or could bereconstructed by an attacker even with full remote access. We present a set ofprinciples by which such systems can be designed, and through a case study inlocalisation demonstrate in simulation a specific implementation that deliversan important robotic capability in an inherently privacy-preserving manner.This is a first step, and we hope to inspire future works that expand the rangeof applications open to sighted robotic systems.", "output": "The Need for Inherently Privacy-Preserving Vision in Trustworthy Autonomous Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image and video restoration has achieved a remarkable leap with the advent ofdeep learning. The success of deep learning paradigm lies in three keycomponents: data, model, and loss. Currently, many efforts have been devoted tothe first two while seldom study focuses on loss function. With the question``are the de facto optimization functions e.g., $L_1$, $L_2$, and perceptuallosses optimal?'', we explore the potential of loss and raise our belief``learned loss function empowers the learning capability of neural networks forimage and video restoration''.Concretely, we stand on the shoulders of the masked Autoencoders (MAE) andformulate it as a `learned loss function', owing to the fact the pre-trainedMAE innately inherits the prior of image reasoning. We investigate the efficacyof our belief from three perspectives: 1) from task-customized MAE to nativeMAE, 2) from image task to video task, and 3) from transformer structure toconvolution neural network structure. Extensive experiments across multipleimage and video tasks, including image denoising, image super-resolution, imageenhancement, guided image super-resolution, video denoising, and videoenhancement, demonstrate the consistent performance improvements introduced bythe learned loss function. Besides, the learned loss function is preferable asit can be directly plugged into existing networks during training withoutinvolving computations in the inference stage. Code will be publicly available.", "output": "Unlocking Masked Autoencoders as Loss Function for Image and Video Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work reveals undiscovered challenges in the performance andgeneralizability of deep learning models. We (1) identify spurious shortcutsand evaluation issues that can inflate performance and (2) propose training andanalysis methods to address them.We trained an AI model to classify cancer on a retrospective dataset of120,112 US exams (3,467 cancers) acquired from 2008 to 2017 and 16,693 UK exams(5,655 cancers) acquired from 2011 to 2015.We evaluated on a screening mammography test set of 11,593 US exams (102cancers; 7,594 women; age 57.1 pm 11.0) and 1,880 UK exams (590 cancers; 1,745women; age 63.3 pm 7.2). A model trained on images of only view markers (nobreast) achieved a 0.691 AUC. The original model trained on both datasetsachieved a 0.945 AUC on the combined US+UK dataset but paradoxically only 0.838and 0.892 on the US and UK datasets, respectively. Sampling cancers equallyfrom both datasets during training mitigated this shortcut. A similar AUCparadox (0.903) occurred when evaluating diagnostic exams vs screening exams(0.862 vs 0.861, respectively). Removing diagnostic exams during trainingalleviated this bias. Finally, the model did not exhibit the AUC paradox overscanner models but still exhibited a bias toward Selenia Dimension (SD) overHologic Selenia (HS) exams. Analysis showed that this AUC paradox occurred whena dataset attribute had values with a higher cancer prevalence (dataset bias)and the model consequently assigned a higher probability to these attributevalues (model bias). Stratification and balancing cancer prevalence canmitigate shortcuts during evaluation.Dataset and model bias can introduce shortcuts and the AUC paradox,potentially pervasive issues within the healthcare AI space. Our methods canverify and mitigate shortcuts while providing a clear understanding ofperformance.", "output": "Problems and shortcuts in deep learning for screening mammography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Controllable image denoising aims to generate clean samples with humanperceptual priors and balance sharpness and smoothness. In traditionalfilter-based denoising methods, this can be easily achieved by adjusting thefiltering strength. However, for NN (Neural Network)-based models, adjustingthe final denoising strength requires performing network inference each time,making it almost impossible for real-time user interaction. In this paper, weintroduce Real-time Controllable Denoising (RCD), the first deep image andvideo denoising pipeline that provides a fully controllable user interface toedit arbitrary denoising levels in real-time with only one-time networkinference. Unlike existing controllable denoising methods that require multipledenoisers and training stages, RCD replaces the last output layer (whichusually outputs a single noise map) of an existing CNN-based model with alightweight module that outputs multiple noise maps. We propose a novel NoiseDecorrelation process to enforce the orthogonality of the noise feature maps,allowing arbitrary noise level control through noise map interpolation. Thisprocess is network-free and does not require network inference. Our experimentsshow that RCD can enable real-time editable image and video denoising forvarious existing heavy-weight models without sacrificing their originalperformance.", "output": "Real-time Controllable Denoising for Image and Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene segmentation is widely used in the field of autonomous driving forenvironment perception, and semantic scene segmentation (3S) has received agreat deal of attention due to the richness of the semantic information itcontains. It aims to assign labels to pixels in an image, thus enablingautomatic image labeling. Current approaches are mainly based on convolutionalneural networks (CNN), but they rely on a large number of labels. Therefore,how to use a small size of labeled data to achieve semantic segmentationbecomes more and more important. In this paper, we propose a domain adaptation(DA) framework based on optimal transport (OT) and attention mechanism toaddress this issue. Concretely, first we generate the output space via CNN dueto its superiority of feature representation. Second, we utilize OT to achievea more robust alignment of source and target domains in output space, where theOT plan defines a well attention mechanism to improve the adaptation of themodel. In particular, with OT, the number of network parameters has beenreduced and the network has been better interpretable. Third, to betterdescribe the multi-scale property of features, we construct a multi-scalesegmentation network to perform domain adaptation. Finally, in order to verifythe performance of our proposed method, we conduct experimental comparison withthree benchmark and four SOTA methods on three scene datasets, and the meanintersection-over-union (mIOU) has been significant improved, and visualizationresults under multiple domain adaptation scenarios also show that our proposedmethod has better performance than compared semantic segmentation methods.", "output": "Domain Adaptive Semantic Segmentation by Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, orthogonal to the existing data and model studies, we insteadresort our efforts to investigate the potential of loss function in a newperspective and present our belief ``Random Weights Networks can Be Acted asLoss Prior Constraint for Image Restoration''. Inspired by Functional theory,we provide several alternative solutions to implement our belief in the strictmathematical manifolds including Taylor's Unfolding Network, Invertible NeuralNetwork, Central Difference Convolution and Zero-order Filtering as ``randomweights network prototype'' with respect of the following four levels: 1) thedifferent random weights strategies; 2) the different network architectures,emph{eg,} pure convolution layer or transformer; 3) the different networkarchitecture depths; 4) the different numbers of random weights networkcombination. Furthermore, to enlarge the capability of the randomly initializedmanifolds, we devise the manner of random weights in the following twovariants: 1) the weights are randomly initialized only once during the wholetraining procedure; 2) the weights are randomly initialized at each trainingiteration epoch. Our propose belief can be directly inserted into existingnetworks without any training and testing computational cost. Extensiveexperiments across multiple image restoration tasks, including imagede-noising, low-light image enhancement, guided image super-resolutiondemonstrate the consistent performance gains obtained by introducing ourbelief. To emphasize, our main focus is to spark the realms of loss functionand save their current neglected status. Code will be publicly available.", "output": "Random Weights Networks Work as Loss Prior Constraint for Image Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a method for 3D reconstruction only using calibrated multi-viewsurface azimuth maps. Our method, multi-view azimuth stereo, is effective fortextureless or specular surfaces, which are difficult for conventionalmulti-view stereo methods. We introduce the concept of tangent spaceconsistency: Multi-view azimuth observations of a surface point should belifted to the same tangent space. Leveraging this consistency, we recover theshape by optimizing a neural implicit surface representation. Our methodharnesses the robust azimuth estimation capabilities of photometric stereomethods or polarization imaging while bypassing potentially complex zenithangle estimation. Experiments using azimuth maps from various sources validatethe accurate shape recovery with our method, even without zenith angles.", "output": "Multi-View Azimuth Stereo via Tangent Space Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformers have shown superior performance on various computer vision taskswith their capabilities to capture long-range dependencies. Despite thesuccess, it is challenging to directly apply Transformers on point clouds dueto their quadratic cost in the number of points. In this paper, we present aSelf-Positioning point-based Transformer (SPoTr), which is designed to captureboth local and global shape contexts with reduced complexity. Specifically,this architecture consists of local self-attention and self-positioningpoint-based global cross-attention. The self-positioning points, adaptivelylocated based on the input shape, consider both spatial and semanticinformation with disentangled attention to improve expressive power. With theself-positioning points, we propose a novel global cross-attention mechanismfor point clouds, which improves the scalability of global self-attention byallowing the attention module to compute attention weights with only a smallset of self-positioning points. Experiments show the effectiveness of SPoTr onthree point cloud tasks such as shape classification, part segmentation, andscene segmentation. In particular, our proposed model achieves an accuracy gainof 2.6% over the previous best models on shape classification withScanObjectNN. We also provide qualitative analyses to demonstrate theinterpretability of self-positioning points. The code of SPoTr is available at", "output": "Self-positioning Point-based Transformer for Point Cloud Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When applying a pre-trained 2D-to-3D human pose lifting model to a targetunseen dataset, large performance degradation is commonly encountered due todomain shift issues. We observe that the degradation is caused by two factors:1) the large distribution gap over global positions of poses between the sourceand target datasets due to variant camera parameters and settings, and 2) thedeficient diversity of local structures of poses in training. To this end, wecombine textbf{global adaptation} and textbf{local generalization} intextit{PoseDA}, a simple yet effective framework of unsupervised domainadaptation for 3D human pose estimation. Specifically, global adaptation aimsto align global positions of poses from the source domain to the target domainwith a proposed global position alignment (GPA) module. And localgeneralization is designed to enhance the diversity of 2D-3D pose mapping witha local pose augmentation (LPA) module. These modules bring significantperformance improvement without introducing additional learnable parameters. Inaddition, we propose local pose augmentation (LPA) to enhance the diversity of3D poses following an adversarial training scheme consisting of 1) aaugmentation generator that generates the parameters of pre-defined posetransformations and 2) an anchor discriminator to ensure the reality andquality of the augmented data. Our approach can be applicable to almost all2D-3D lifting models. textit{PoseDA} achieves 61.3 mm of MPJPE on MPI-INF-3DHPunder a cross-dataset evaluation setup, improving upon the previousstate-of-the-art method by 10.2%.", "output": "Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extracting parametric edge curves from point clouds is a fundamental problemin 3D vision and geometry processing. Existing approaches mainly rely onkeypoint detection, a challenging procedure that tends to generate noisyoutput, making the subsequent edge extraction error-prone. To address thisissue, we propose to directly detect structured edges to circumvent thelimitations of the previous point-wise methods. We achieve this goal bypresenting NerVE, a novel neural volumetric edge representation that can beeasily learned through a volumetric learning framework. NerVE can be seamlesslyconverted to a versatile piece-wise linear (PWL) curve representation, enablinga unified strategy for learning all types of free-form curves. Furthermore, asNerVE encodes rich structural information, we show that edge extraction basedon NerVE can be reduced to a simple graph search problem. After convertingNerVE to the PWL representation, parametric curves can be obtained viaoff-the-shelf spline fitting algorithms. We evaluate our method on thechallenging ABC dataset. We show that a simple network based on NerVE canalready outperform the previous state-of-the-art methods by a great margin.Project page: ", "output": "NerVE: Neural Volumetric Edges for Parametric Curve Extraction from Point Cloud."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Capturing the interactions between humans and their environment in 3D isimportant for many applications in robotics, graphics, and vision. Recent worksto reconstruct the 3D human and object from a single RGB image do not haveconsistent relative translation across frames because they assume a fixeddepth. Moreover, their performance drops significantly when the object isoccluded. In this work, we propose a novel method to track the 3D human,object, contacts between them, and their relative translation across framesfrom a single RGB camera, while being robust to heavy occlusions. Our method isbuilt on two key insights. First, we condition our neural field reconstructionsfor human and object on per-frame SMPL model estimates obtained by pre-fittingSMPL to a video sequence. This improves neural reconstruction accuracy andproduces coherent relative translation across frames. Second, human and objectmotion from visible frames provides valuable information to infer the occludedobject. We propose a novel transformer-based neural network that explicitlyuses object visibility and human motion to leverage neighbouring frames to makepredictions for the occluded frames. Building on these insights, our method isable to track both human and object robustly even under occlusions. Experimentson two datasets show that our method significantly improves over thestate-of-the-art methods. Our code and pretrained models are available at:", "output": "Visibility Aware Human-Object Interaction Tracking from Single RGB Camera."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Synthesizing photo-realistic images from a point cloud is challenging becauseof the sparsity of point cloud representation. Recent Neural Radiance Fieldsand extensions are proposed to synthesize realistic images from 2D input. Inthis paper, we present Point2Pix as a novel point renderer to link the 3Dsparse point clouds with 2D dense image pixels. Taking advantage of the pointcloud 3D prior and NeRF rendering pipeline, our method can synthesizehigh-quality images from colored point clouds, generally for novel indoorscenes. To improve the efficiency of ray sampling, we propose point-guidedsampling, which focuses on valid samples. Also, we present Point Encoding tobuild Multi-scale Radiance Fields that provide discriminative 3D pointfeatures. Finally, we propose Fusion Encoding to efficiently synthesizehigh-quality images. Extensive experiments on the ScanNet and ArkitScenesdatasets demonstrate the effectiveness and generalization.", "output": "Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing learning-based methods for point cloud rendering adopt various 3Drepresentations and feature querying mechanisms to alleviate the sparsityproblem of point clouds. However, artifacts still appear in rendered images,due to the challenges in extracting continuous and discriminative 3D featuresfrom point clouds. In this paper, we present a dense while lightweight 3Drepresentation, named TriVol, that can be combined with NeRF to renderphoto-realistic images from point clouds. Our TriVol consists of triple slimvolumes, each of which is encoded from the point cloud. TriVol has twoadvantages. First, it fuses respective fields at different scales and thusextracts local and non-local features for discriminative representation.Second, since the volume size is greatly reduced, our 3D decoder can beefficiently inferred, allowing us to increase the resolution of the 3D space torender more point details. Extensive experiments on different benchmarks withvarying kinds of scenes/objects demonstrate our framework's effectivenesscompared with current approaches. Moreover, our framework has excellentgeneralization ability to render a category of scenes/objects withoutfine-tuning.", "output": "TriVol: Point Cloud Rendering via Triple Volumes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image super-resolution (SR) has attracted increasing attention due to itswide applications. However, current SR methods generally suffer fromover-smoothing and artifacts, and most work only with fixed magnifications.This paper introduces an Implicit Diffusion Model (IDM) for high-fidelitycontinuous image super-resolution. IDM integrates an implicit neuralrepresentation and a denoising diffusion model in a unified end-to-endframework, where the implicit neural representation is adopted in the decodingprocess to learn continuous-resolution representation. Furthermore, we design ascale-controllable conditioning mechanism that consists of a low-resolution(LR) conditioning network and a scaling factor. The scaling factor regulatesthe resolution and accordingly modulates the proportion of the LR informationand generated features in the final output, which enables the model toaccommodate the continuous-resolution requirement. Extensive experimentsvalidate the effectiveness of our IDM and demonstrate its superior performanceover prior arts.", "output": "Implicit Diffusion Models for Continuous Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To apply optical flow in practice, it is often necessary to resize the inputto smaller dimensions in order to reduce computational costs. However,downsizing inputs makes the estimation more challenging because objects andmotion ranges become smaller. Even though recent approaches have demonstratedhigh-quality flow estimation, they tend to fail to accurately model smallobjects and precise boundaries when the input resolution is lowered,restricting their applicability to high-resolution inputs. In this paper, weintroduce AnyFlow, a robust network that estimates accurate flow from images ofvarious resolutions. By representing optical flow as a continuouscoordinate-based representation, AnyFlow generates outputs at arbitrary scalesfrom low-resolution inputs, demonstrating superior performance over prior worksin capturing tiny objects with detail preservation on a wide range of scenes.We establish a new state-of-the-art performance of cross-dataset generalizationon the KITTI dataset, while achieving comparable accuracy on the onlinebenchmarks to other SOTA methods.", "output": "AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Audiovisual automatic speech recognition (AV-ASR) aims to improve therobustness of a speech recognition system by incorporating visual information.Training fully supervised multimodal models for this task from scratch, howeveris limited by the need for large labelled audiovisual datasets (in eachdownstream domain of interest). We present AVFormer, a simple method foraugmenting audio-only models with visual information, at the same timeperforming lightweight domain adaptation. We do this by (i) injecting visualembeddings into a frozen ASR model using lightweight trainable adaptors. Weshow that these can be trained on a small amount of weakly labelled video datawith minimum additional training time and parameters. (ii) We also introduce asimple curriculum scheme during training which we show is crucial to enable themodel to jointly process audio and visual information effectively; and finally(iii) we show that our model achieves state of the art zero-shot results onthree different AV-ASR benchmarks (How2, VisSpeech and Ego4D), while alsocrucially preserving decent performance on traditional audio-only speechrecognition benchmarks (LibriSpeech). Qualitative results show that our modeleffectively leverages visual information for robust speech recognition.", "output": "AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The work discusses the use of machine learning algorithms for anomalydetection in medical image analysis and how the performance of these algorithmsdepends on the number of annotators and the quality of labels. To address theissue of subjectivity in labeling with a single annotator, we introduce asimple and effective approach that aggregates annotations from multipleannotators with varying levels of expertise. We then aim to improve theefficiency of predictive models in abnormal detection tasks by estimatinghidden labels from multiple annotations and using a re-weighted loss functionto improve detection performance. Our method is evaluated on a real-worldmedical imaging dataset and outperforms relevant baselines that do not considerdisagreements among annotators.", "output": "Improving Object Detection in Medical Image Analysis through Multiple Expert Annotators: An Empirical Investigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have emerged as the best approach for generative modeling of2D images. Part of their success is due to the possibility of training them onmillions if not billions of images with a stable learning objective. However,extending these models to 3D remains difficult for two reasons. First, findinga large quantity of 3D training data is much more complex than for 2D images.Second, while it is conceptually trivial to extend the models to operate on 3Drather than 2D grids, the associated cubic growth in memory and computecomplexity makes this infeasible. We address the first challenge by introducinga new diffusion setup that can be trained, end-to-end, with only posed 2Dimages for supervision; and the second challenge by proposing an imageformation model that decouples model memory from spatial memory. We evaluateour method on real-world data, using the CO3D dataset which has not been usedto train 3D generative models before. We show that our diffusion models arescalable, train robustly, and are competitive in terms of sample quality andfidelity to existing approaches for 3D generative modeling.", "output": "HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Implicit neural representation has recently shown a promising ability inrepresenting images with arbitrary resolutions. In this paper, we present aLocal Implicit Transformer (LIT), which integrates the attention mechanism andfrequency encoding technique into a local implicit image function. We design across-scale local attention block to effectively aggregate local features. Tofurther improve representative power, we propose a Cascaded LIT (CLIT) thatexploits multi-scale features, along with a cumulative training strategy thatgradually increases the upsampling scales during training. We have conductedextensive experiments to validate the effectiveness of these components andanalyze various training strategies. The qualitative and quantitative resultsdemonstrate that LIT and CLIT achieve favorable results and outperform theprior works in arbitrary super-resolution tasks.", "output": "Cascaded Local Implicit Transformer for Arbitrary-Scale Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How to ensure fairness is an important topic in federated learning (FL).Recent studies have investigated how to reward clients based on theircontribution (collaboration fairness), and how to achieve uniformity ofperformance across clients (performance fairness). Despite achieving progresson either one, we argue that it is critical to consider them together, in orderto engage and motivate more diverse clients joining FL to derive a high-qualityglobal model. In this work, we propose a novel method to optimize both types offairness simultaneously. Specifically, we propose to estimate clientcontribution in gradient and data space. In gradient space, we monitor thegradient direction differences of each client with respect to others. And indata space, we measure the prediction error on client data using an auxiliarymodel. Based on this contribution estimation, we propose a FL method, federatedtraining via contribution estimation (FedCE), i.e., using estimation as globalmodel aggregation weights. We have theoretically analyzed our method andempirically evaluated it on two real-world medical datasets. The effectivenessof our approach has been validated with significant performance improvements,better collaboration fairness, better performance fairness, and comprehensiveanalytical studies.", "output": "Fair Federated Medical Image Segmentation via Client Contribution Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a deep learning-based wound classification tool that canassist medical personnel in non-wound care specialization to classify five keywound conditions, namely deep wound, infected wound, arterial wound, venouswound, and pressure wound, given color images captured using readily availablecameras. The accuracy of the classification is vital for appropriate woundmanagement. The proposed wound classification method adopts a multi-task deeplearning framework that leverages the relationships among the five key woundconditions for a unified wound classification architecture. With differences inCohen's kappa coefficients as the metrics to compare our proposed model withhumans, the performance of our model was better or non-inferior to those of allhuman medical personnel. Our convolutional neural network-based model is thefirst to classify five tasks of deep, infected, arterial, venous, and pressurewounds simultaneously with good accuracy. The proposed model is compact andmatches or exceeds the performance of human doctors and nurses. Medicalpersonnel who do not specialize in wound care can potentially benefit from anapp equipped with the proposed deep learning model.", "output": "Development of a deep learning-based tool to assist wound classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Patch-to-point matching has become a robust way of point cloud registration.However, previous patch-matching methods employ superpoints with poorlocalization precision as nodes, which may lead to ambiguous patch partitions.In this paper, we propose a HybridPoint-based network to find more robust andaccurate correspondences. Firstly, we propose to use salient points withprominent local features as nodes to increase patch repeatability, andintroduce some uniformly distributed points to complete the point cloud, thusconstituting hybrid points. Hybrid points not only have better localizationprecision but also give a complete picture of the whole point cloud.Furthermore, based on the characteristic of hybrid points, we propose adual-classes patch matching module, which leverages the matching results ofsalient points and filters the matching noise of non-salient points.Experiments show that our model achieves state-of-the-art performance on3DMatch, 3DLoMatch, and KITTI odometry, especially with 93.0% RegistrationRecall on the 3DMatch dataset. Our code and models are available at", "output": "HybridPoint: Point Cloud Registration Based on Hybrid Point Sampling and Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep functional maps have recently emerged as a successful paradigm fornon-rigid 3D shape correspondence tasks. An essential step in this pipelineconsists in learning feature functions that are used as constraints to solvefor a functional map inside the network. However, the precise nature of theinformation learned and stored in these functions is not yet well understood.Specifically, a major question is whether these features can be used for anyother objective, apart from their purely algebraic role in solving forfunctional map matrices. In this paper, we show that under some mildconditions, the features learned within deep functional map approaches can beused as point-wise descriptors and thus are directly comparable acrossdifferent shapes, even without the necessity of solving for a functional map attest time. Furthermore, informed by our analysis, we propose effectivemodifications to the standard deep functional map pipeline, which promotestructural properties of learned features, significantly improving the matchingresults. Finally, we demonstrate that previously unsuccessful attempts at usingextrinsic architectures for deep functional map feature extraction can beremedied via simple architectural changes, which encourage the theoreticalproperties suggested by our analysis. We thus bridge the gap between intrinsicand extrinsic surface-based learning, suggesting the necessary and sufficientconditions for successful shape matching. Our code is available at", "output": "Understanding and Improving Features Learned in Deep Functional Maps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Information surrounds people in modern life. Text is a very efficient type ofinformation that people use for communication for centuries. However, automatedtext-in-the-wild recognition remains a challenging problem. The majorlimitation for a DL system is the lack of training data. For the competitiveperformance, training set must contain many samples that replicate thereal-world cases. While there are many high-quality datasets for English textrecognition; there are no available datasets for Russian language. In thispaper, we present a large-scale human-labeled dataset for Russian textrecognition in-the-wild. We also publish a synthetic dataset and code toreproduce the generation process", "output": "RusTitW: Russian Language Text Dataset for Visual Text in-the-Wild Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cancer detection and classification from gigapixel whole slide images ofstained tissue specimens has recently experienced enormous progress incomputational histopathology. The limitation of available pixel-wise annotatedscans shifted the focus from tumor localization to global slide-levelclassification on the basis of (weakly-supervised) multiple-instance learningdespite the clinical importance of local cancer detection. However, the worseperformance of these techniques in comparison to fully supervised methods haslimited their usage until now for diagnostic interventions in domains oflife-threatening diseases such as cancer. In this work, we put the focus backon tumor localization in form of a patch-level classification task and take upthe setting of so-called coarse annotations, which provide greater trainingsupervision while remaining feasible from a clinical standpoint. To this end,we present a novel ensemble method that not only significantly improves thedetection accuracy of metastasis on the open CAMELYON16 data set of sentinellymph nodes of breast cancer patients, but also considerably increases itsrobustness against noise while training on coarse annotations. Our experimentsshow that better results can be achieved with our technique making itclinically feasible to use for cancer diagnosis and opening a new avenue fortranslational and clinical research.", "output": "Robust Tumor Detection from Coarse Annotations via Multi-Magnification Ensembles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a combination of visual and audio signals, video is inherentlymulti-modal. However, existing video generation methods are primarily intendedfor the synthesis of visual frames, whereas audio signals in realistic videosare disregarded. In this work, we concentrate on a rarely investigated problemof text guided sounding video generation and propose the Sounding VideoGenerator (SVG), a unified framework for generating realistic videos along withaudio signals. Specifically, we present the SVG-VQGAN to transform visualframes and audio melspectrograms into discrete tokens. SVG-VQGAN applies anovel hybrid contrastive learning method to model inter-modal and intra-modalconsistency and improve the quantized representations. A cross-modal attentionmodule is employed to extract associated features of visual frames and audiosignals for contrastive learning. Then, a Transformer-based decoder is used tomodel associations between texts, visual frames, and audio signals at tokenlevel for auto-regressive sounding video generation. AudioSetCap, a humanannotated text-video-audio paired dataset, is produced for training SVG.Experimental results demonstrate the superiority of our method when comparedwith existing textto-video generation methods as well as audio generationmethods on Kinetics and VAS datasets.", "output": "Sounding Video Generator: A Unified Framework for Text-guided Sounding Video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study presents a novel approach to bone age assessment (BAA) using amulti-view, multi-task classification model based on the Sauvegrain method. Astraightforward solution to automating the Sauvegrain method, which assesses amaturity score for each landmark in the elbow and predicts the bone age, is totrain classifiers independently to score each region of interest (RoI), butthis approach limits the accessible information to local morphologies andincreases computational costs. As a result, this work proposes aself-accumulative vision transformer (SAT) that mitigates anisotropic behavior,which usually occurs in multi-view, multi-task problems and limits theeffectiveness of a vision transformer, by applying token replay and regionalattention bias. A number of experiments show that SAT successfully exploits therelationships between landmarks and learns global morphological features,resulting in a mean absolute error of BAA that is 0.11 lower than that of theprevious work. Additionally, the proposed SAT has four times reduced parametersthan an ensemble of individual classifiers of the previous work. Lastly, thiswork also provides informative implications for clinical practice, improvingthe accuracy and efficiency of BAA in diagnosing abnormal growth inadolescents.", "output": "Self-accumulative Vision Transformer for Bone Age Assessment Using the Sauvegrain Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The fairness of a deep neural network is strongly affected by dataset biasand spurious correlations, both of which are usually present in modernfeature-rich and complex visual datasets. Due to the difficulty and variabilityof the task, no single de-biasing method has been universally successful. Inparticular, implicit methods not requiring explicit knowledge of bias variablesare especially relevant for real-world applications. We propose a novelimplicit mitigation method using a Bayesian neural network, allowing us toleverage the relationship between epistemic uncertainties and the presence ofbias or spurious correlations in a sample. Our proposed posterior estimatesharpening procedure encourages the network to focus on core features that donot contribute to high uncertainties. Experimental results on three benchmarkdatasets demonstrate that Bayesian networks with sharpened posterior estimatesperform comparably to prior existing methods and show potential worthy offurther exploration.", "output": "Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a Bayesian Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Satellite imagery analysis plays a vital role in remote sensing, but theinformation loss caused by cloud cover seriously hinders its application. Thisstudy presents a high-performance cloud removal architecture called ProgressiveMulti-scale Attention Autoencoder (PMAA), which simultaneously leverages globaland local information. It mainly consists of a cloud detection backbone and acloud removal module. The cloud detection backbone uses cloud masks toreinforce cloudy areas to prompt the cloud removal module. The cloud removalmodule mainly comprises a novel Multi-scale Attention Module (MAM) and a LocalInteraction Module (LIM). PMAA establishes the long-range dependency ofmulti-scale features using MAM and modulates the reconstruction of thefine-grained details using LIM, allowing for the simultaneous representation offine- and coarse-grained features at the same level. With the help of diverseand multi-scale feature representation, PMAA outperforms the previousstate-of-the-art model CTGAN consistently on the Sen2_MTC_Old and Sen2_MTC_Newdatasets. Furthermore, PMAA has a considerable efficiency advantage, with only0.5% and 14.6% of the parameters and computational complexity of CTGAN,respectively. These extensive results highlight the potential of PMAA as alightweight cloud removal network suitable for deployment on edge devices. Wewill release the code and trained models to facilitate the study in thisdirection.", "output": "PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-Performance Cloud Removal from Multi-temporal Satellite Imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, the self-supervised learning framework data2vec has shown inspiringperformance for various modalities using a masked student-teacher approach.However, it remains open whether such a framework generalizes to the uniquechallenges of 3D point clouds. To answer this question, we extend data2vec tothe point cloud domain and report encouraging results on several downstreamtasks. In an in-depth analysis, we discover that the leakage of positionalinformation reveals the overall object shape to the student even under heavymasking and thus hampers data2vec to learn strong representations for pointclouds. We address this 3D-specific shortcoming by proposing point2vec, whichunleashes the full potential of data2vec-like pre-training on point clouds. Ourexperiments show that point2vec outperforms other self-supervised methods onshape classification and few-shot learning on ModelNet40 and ScanObjectNN,while achieving competitive results on part segmentation on ShapeNetParts.These results suggest that the learned representations are strong andtransferable, highlighting point2vec as a promising direction forself-supervised learning of point cloud representations.", "output": "Point2Vec for Self-Supervised Representation Learning on Point Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting the future trajectories of the traffic agents is a gordiantechnique in autonomous driving. However, trajectory prediction suffers fromdata imbalance in the prevalent datasets, and the tailed data is often morecomplicated and safety-critical. In this paper, we focus on dealing with thelong-tail phenomenon in trajectory prediction. Previous methods dealing withlong-tail data did not take into account the variety of motion patterns in thetailed data. In this paper, we put forward a future enhanced contrastivelearning framework to recognize tail trajectory patterns and form a featurespace with separate pattern clusters. Furthermore, a distribution aware hyperpredictor is brought up to better utilize the shaped feature space. Our methodis a model-agnostic framework and can be plugged into many well-knownbaselines. Experimental results show that our framework outperforms thestate-of-the-art long-tail prediction method on tailed samples by 9.5% on ADEand 8.5% on FDE, while maintaining or slightly improving the averagedperformance. Our method also surpasses many long-tail techniques on trajectoryprediction task.", "output": "FEND: A Future Enhanced Distribution-Aware Contrastive Learning Framework for Long-tail Trajectory Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-Image synthesis is the task of generating an image according to aspecific text description. Generative Adversarial Networks have been consideredthe standard method for image synthesis virtually since their introduction;today, Denoising Diffusion Probabilistic Models are recently setting a newbaseline, with remarkable results in Text-to-Image synthesis, among otherfields. Aside its usefulness per se, it can also be particularly relevant as atool for data augmentation to aid training models for other document imageprocessing tasks. In this work, we present a latent diffusion-based method forstyled text-to-text-content-image generation on word-level. Our proposed methodmanages to generate realistic word image samples from different writer styles,by using class index styles and text content prompts without the need ofadversarial training, writer recognition, or text recognition. We gauge systemperformance with Frechet Inception Distance, writer recognition accuracy, andwriter retrieval. We show that the proposed model produces samples that areaesthetically pleasing, help boosting text recognition performance, and getssimilar writer retrieval score as real data.", "output": "WordStylist: Styled Verbatim Handwritten Text Generation with Latent Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Compared with previous two-stream trackers, the recent one-stream trackingpipeline, which allows earlier interaction between the template and searchregion, has achieved a remarkable performance gain. However, existingone-stream trackers always let the template interact with all parts inside thesearch region throughout all the encoder layers. This could potentially lead totarget-background confusion when the extracted feature representations are notsufficiently discriminative. To alleviate this issue, we propose a generalizedrelation modeling method based on adaptive token division. The proposed methodis a generalized formulation of attention-based relation modeling forTransformer tracking, which inherits the merits of both previous two-stream andone-stream pipelines whilst enabling more flexible relation modeling byselecting appropriate search tokens to interact with template tokens. Anattention masking strategy and the Gumbel-Softmax technique are introduced tofacilitate the parallel computation and end-to-end learning of the tokendivision module. Extensive experiments show that our method is superior to thetwo-stream and one-stream pipelines and achieves state-of-the-art performanceon six challenging benchmarks with a real-time running speed.", "output": "Generalized Relation Modeling for Transformer Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Composed image retrieval searches for a target image based on a multi-modaluser query comprised of a reference image and modification text describing thedesired changes. Existing approaches to solving this challenging task learn amapping from the (reference image, modification text)-pair to an imageembedding that is then matched against a large image corpus. One area that hasnot yet been explored is the reverse direction, which asks the question, whatreference image when modified as describe by the text would produce the giventarget image? In this work we propose a bi-directional training scheme thatleverages such reversed queries and can be applied to existing composed imageretrieval architectures. To encode the bi-directional query we prepend alearnable token to the modification text that designates the direction of thequery and then finetune the parameters of the text embedding module. We make noother changes to the network architecture. Experiments on two standard datasetsshow that our novel approach achieves improved performance over a baselineBLIP-based model that itself already achieves state-of-the-art performance.", "output": "Bi-directional Training for Composed Image Retrieval via Text Prompt Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Watershed segmentation method has been used in various applications. But manya times, due to its over-segmentation attributes, it underperforms in severaltasks where noise is a dominant source. In this study, Optical CoherenceTomography images have been acquired, and segmentation has been performed toanalyse the different regions of fluid filled sacs in a lemon. A modifiedwatershed algorithm has been proposed which gives promising results forsegmentation of internal lemon structures.", "output": "Modified watershed approach for segmentation of complex optical coherence tomographic images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Facial expression generation is one of the most challenging and long-soughtaspects of character animation, with many interesting applications. Thechallenging task, traditionally having relied heavily on digital craftspersons,remains yet to be explored. In this paper, we introduce a generative frameworkfor generating 3D facial expression sequences (i.e. 4D faces) that can beconditioned on different inputs to animate an arbitrary 3D face mesh. It iscomposed of two tasks: (1) Learning the generative model that is trained over aset of 3D landmark sequences, and (2) Generating 3D mesh sequences of an inputfacial mesh driven by the generated landmark sequences. The generative model isbased on a Denoising Diffusion Probabilistic Model (DDPM), which has achievedremarkable success in generative tasks of other domains. While it can betrained unconditionally, its reverse process can still be conditioned byvarious condition signals. This allows us to efficiently develop severaldownstream tasks involving various conditional generation, by using expressionlabels, text, partial sequences, or simply a facial geometry. To obtain thefull mesh deformation, we then develop a landmark-guided encoder-decoder toapply the geometrical deformation embedded in landmarks on a given facial mesh.Experiments show that our model has learned to generate realistic, qualityexpressions solely from the dataset of relatively small size, improving overthe state-of-the-art methods. Videos and qualitative comparisons with othermethods can be found at  Code and modelswill be made available upon acceptance.", "output": "4D Facial Expression Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models for image classification are typically trained under the\"closed-world\" assumption with a predefined set of image classes. However, whenthe models are deployed they may be faced with input images not belonging tothe classes encountered during training. This type of scenario is common inremote sensing image classification where images come from different geographicareas, sensors, and imaging conditions. In this paper we deal with the problemof detecting remote sensing images coming from a different distributioncompared to the training data - out of distribution images. We propose abenchmark for out of distribution detection in remote sensing sceneclassification and evaluate detectors based on maximum softmax probability andnearest neighbors. The experimental results show convincing advantages of themethod based on nearest neighbors.", "output": "Nearest Neighbor Based Out-of-Distribution Detection in Remote Sensing Scene Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inverse rendering methods aim to estimate geometry, materials andillumination from multi-view RGB images. In order to achieve betterdecomposition, recent approaches attempt to model indirect illuminationsreflected from different materials via Spherical Gaussians (SG), which,however, tends to blur the high-frequency reflection details. In this paper, wepropose an end-to-end inverse rendering pipeline that decomposes materials andillumination from multi-view images, while considering near-field indirectillumination. In a nutshell, we introduce the Monte Carlo sampling based pathtracing and cache the indirect illumination as neural radiance, enabling aphysics-faithful and easy-to-optimize inverse rendering method. To enhanceefficiency and practicality, we leverage SG to represent the smooth environmentilluminations and apply importance sampling techniques. To supervise indirectilluminations from unobserved directions, we develop a novel radianceconsistency constraint between implicit neural radiance and path tracingresults of unobserved rays along with the joint optimization of materials andilluminations, thus significantly improving the decomposition performance.Extensive experiments demonstrate that our method outperforms thestate-of-the-art on multiple synthetic and real datasets, especially in termsof inter-reflection decomposition.", "output": "NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field Indirect Illumination."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Local feature matching aims at finding correspondences between a pair ofimages. Although current detector-free methods leverage Transformerarchitecture to obtain an impressive performance, few works considermaintaining local consistency. Meanwhile, most methods struggle with largescale variations. To deal with the above issues, we propose AdaptiveSpot-Guided Transformer (ASTR) for local feature matching, which jointly modelsthe local consistency and scale variations in a unified coarse-to-finearchitecture. The proposed ASTR enjoys several merits. First, we design aspot-guided aggregation module to avoid interfering with irrelevant areasduring feature aggregation. Second, we design an adaptive scaling module toadjust the size of grids according to the calculated depth information at finestage. Extensive experimental results on five standard benchmarks demonstratethat our ASTR performs favorably against state-of-the-art methods. Our codewill be released on ", "output": "Adaptive Spot-Guided Transformer for Consistent Local Feature Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent multi-camera 3D object detectors usually leverage temporal informationto construct multi-view stereo that alleviates the ill-posed depth estimation.However, they typically assume all the objects are static and directlyaggregate features across frames. This work begins with a theoretical andempirical analysis to reveal that ignoring the motion of moving objects canresult in serious localization bias. Therefore, we propose to model DynamicObjects in RecurrenT (DORT) to tackle this problem. In contrast to previousglobal Bird-Eye-View (BEV) methods, DORT extracts object-wise local volumes formotion estimation that also alleviates the heavy computational burden. Byiteratively refining the estimated object motion and location, the precedingfeatures can be precisely aggregated to the current frame to mitigate theaforementioned adverse effects. The simple framework has two significantappealing properties. It is flexible and practical that can be plugged intomost camera-based 3D object detectors. As there are predictions of objectmotion in the loop, it can easily track objects across frames according totheir nearest center distances. Without bells and whistles, DORT outperformsall the previous methods on the nuScenes detection and tracking benchmarks with62.5% NDS and 57.6% AMOTA, respectively. The source code will be released.", "output": "DORT: Modeling Dynamic Objects in Recurrent for Multi-Camera 3D Object Detection and Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Obtaining large-scale labeled object detection dataset can be costly andtime-consuming, as it involves annotating images with bounding boxes and classlabels. Thus, some specialized active learning methods have been proposed toreduce the cost by selecting either coarse-grained samples or fine-grainedinstances from unlabeled data for labeling. However, the former approachessuffer from redundant labeling, while the latter methods generally lead totraining instability and sampling bias. To address these challenges, we proposea novel approach called Multi-scale Region-based Active Learning (MuRAL) forobject detection. MuRAL identifies informative regions of various scales toreduce annotation costs for well-learned objects and improve trainingperformance. The informative region score is designed to consider both thepredicted confidence of instances and the distribution of each object category,enabling our method to focus more on difficult-to-detect classes. Moreover,MuRAL employs a scale-aware selection strategy that ensures diverse regions areselected from different scales for labeling and downstream finetuning, whichenhances training stability. Our proposed method surpasses all existingcoarse-grained and fine-grained baselines on Cityscapes and MS COCO datasets,and demonstrates significant improvement in difficult category performance.", "output": "MuRAL: Multi-Scale Region-based Active Learning for Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Local feature matching is challenging due to the textureless and repetitivepattern. Existing methods foucs on using appearance features and globalinteraction and matching, while the importance of geometry prior in localfeature matching has not been fully exploited. Different from these methods, inthis paper, we delve into the importance of geometry prior and proposeStructured Epipolar Matcher (SEM) for local feature matching, which canleverage the geometric information in a iterative matching way. The proposedmodel enjoys several merits. First, our proposed Structured Feature Extractorcan model the relative positional relationship between pixels andhigh-confidence anchor points. Second, our proposed Epipolar Attention andMatching can filter out irrelevant areas by utilizing the epipolar constraint.Extensive experimental results on five standard benchmarks demonstrate thesuperior performance of our SEM compared to state-of-the-art methods.", "output": "Structured Epipolar Matcher for Local Feature Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning rich data representations from unlabeled data is a key challengetowards applying deep learning algorithms in downstream supervised tasks.Several variants of variational autoencoders have been proposed to learncompact data representaitons by encoding high-dimensional data in a lowerdimensional space. Two main classes of VAEs methods may be distinguisheddepending on the characteristics of the meta-priors that are enforced in therepresentation learning step. The first class of methods derives a continuousencoding by assuming a static prior distribution in the latent space. Thesecond class of methods learns instead a discrete latent representation usingvector quantization (VQ) along with a codebook. However, both classes ofmethods suffer from certain challenges, which may lead to suboptimal imagereconstruction results. The first class of methods suffers from posteriorcollapse, whereas the second class of methods suffers from codebook collapse.To address these challenges, we introduce a new VAE variant, termed SC-VAE(sparse coding-based VAE), which integrates sparse coding within variationalautoencoder framework. Instead of learning a continuous or discrete latentrepresentation, the proposed method learns a sparse data representation thatconsists of a linear combination of a small number of learned atoms. The sparsecoding problem is solved using a learnable version of the iterative shrinkagethresholding algorithm (ISTA). Experiments on two image datasets demonstratethat our model can achieve improved image reconstruction results compared tostate-of-the-art methods. Moreover, the use of learned sparse code vectorsallows us to perform downstream task like coarse image segmentation throughclustering image patches.", "output": "SC-VAE: Sparse Coding-based Variational Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks have been applied in many computer vision tasks andachieved state-of-the-art performance. However, misclassification will occurwhen DNN predicts adversarial examples which add human-imperceptibleadversarial noise to natural examples. This limits the application of DNN insecurity-critical fields. To alleviate this problem, we first conducted anempirical analysis of the latent features of both adversarial and naturalexamples and found the similarity matrix of natural examples is more compactthan those of adversarial examples. Motivated by this observation, we proposetextbf{L}atent textbf{F}eature textbf{R}elation textbf{C}onsistency(textbf{LFRC}), which constrains the relation of adversarial examples inlatent space to be consistent with the natural examples. Importantly, our LFRCis orthogonal to the previous method and can be easily combined with them toachieve further improvement. To demonstrate the effectiveness of LFRC, weconduct extensive experiments using different neural networks on benchmarkdatasets. For instance, LFRC can bring 0.78% further improvement compared toAT, and 1.09% improvement compared to TRADES, against AutoAttack on CIFAR10.Code is available at ", "output": "Latent Feature Relation Consistency for Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A significant portion of driving hazards is caused by human error anddisregard for local driving regulations; Consequently, an intelligentassistance system can be beneficial. This paper proposes a novel vision-basedmodular package to ensure drivers' safety by perceiving the environment. Eachmodule is designed based on accuracy and inference time to deliver real-timeperformance. As a result, the proposed system can be implemented on a widerange of vehicles with minimum hardware requirements. Our modular packagecomprises four main sections: lane detection, object detection, segmentation,and monocular depth estimation. Each section is accompanied by novel techniquesto improve the accuracy of others along with the entire system. Furthermore, aGUI is developed to display perceived information to the driver. In addition tousing public datasets, like BDD100K, we have also collected and annotated alocal dataset that we utilize to fine-tune and evaluate our system. We showthat the accuracy of our system is above 80% in all the sections. Our code anddata are available at", "output": "An intelligent modular real-time vision-based system for environment perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scale is the primary factor for building a powerful foundation model thatcould well generalize to a variety of downstream tasks. However, it is stillchallenging to train video foundation models with billions of parameters. Thispaper shows that video masked autoencoder (VideoMAE) is a scalable and generalself-supervised pre-trainer for building video foundation models. We scale theVideoMAE in both model and data with a core design. Specifically, we present adual masking strategy for efficient pre-training, with an encoder operating ona subset of video tokens and a decoder processing another subset of videotokens. Although VideoMAE is very efficient due to high masking ratio inencoder, masking decoder can still further reduce the overall computationalcost. This enables the efficient pre-training of billion-level models in video.We also use a progressive training paradigm that involves an initialpre-training on a diverse multi-sourced unlabeled dataset, followed by apost-pre-training on a mixed labeled dataset. Finally, we successfully train avideo ViT model with a billion parameters, which achieves a newstate-of-the-art performance on the datasets of Kinetics (90.0% on K400 and89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). Inaddition, we extensively verify the pre-trained video ViT models on a varietyof downstream tasks, demonstrating its effectiveness as a general videorepresentation learner.", "output": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Test-time adaptation methods have been gaining attention recently as apractical solution for addressing source-to-target domain gaps by graduallyupdating the model without requiring labels on the target data. In this paper,we propose a method of test-time adaptation for category-level object poseestimation called TTA-COPE. We design a pose ensemble approach with aself-training loss using pose-aware confidence. Unlike previous unsuperviseddomain adaptation methods for category-level object pose estimation, ourapproach processes the test data in a sequential, online manner, and it doesnot require access to the source domain at runtime. Extensive experimentalresults demonstrate that the proposed pose ensemble and the self-training lossimprove category-level object pose performance during test time under bothsemi-supervised and unsupervised settings. Project page:", "output": "TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Actively planning sensor views during object reconstruction is essential toautonomous mobile robots. This task is usually performed by evaluatinginformation gain from an explicit uncertainty map. Existing algorithms compareoptions among a set of preset candidate views and select the next-best-viewfrom them. In contrast to these, we take the emerging implicit representationas the object model and seamlessly combine it with the active reconstructiontask. To fully integrate observation information into the model, we propose asupervision method specifically for object-level reconstruction that considersboth valid and free space. Additionally, to directly evaluate view informationfrom the implicit object model, we introduce a sample-based uncertaintyevaluation method. It samples points on rays directly from the object model anduses variations of implicit function inferences as the uncertainty metrics,with no need for voxel traversal or an additional information map. Leveragingthe differentiability of our metrics, it is possible to optimize thenext-best-view by maximizing the uncertainty continuously. This does away withthe traditionally-used candidate views setting, which may provide sub-optimalresults. Experiments in simulations and real-world scenes show that our methodeffectively improves the reconstruction accuracy and the view-planningefficiency of active reconstruction tasks. The proposed system is going to opensource at ", "output": "Active Implicit Object Reconstruction using Uncertainty-guided Next-Best-View Optimziation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image generation using diffusion can be controlled in multiple ways. In thispaper, we systematically analyze the equations of modern generative diffusionnetworks to propose a framework, called MDP, that explains the design space ofsuitable manipulations. We identify 5 different manipulations, includingintermediate latent, conditional embedding, cross attention maps, guidance, andpredicted noise. We analyze the corresponding parameters of these manipulationsand the manipulation schedule. We show that some previous editing methods fitnicely into our framework. Particularly, we identified one specificconfiguration as a new type of control by manipulating the predicted noise,which can perform higher-quality edits than previous work for a variety oflocal and global edits.", "output": "MDP: A Generalized Framework for Text-Guided Image Editing by Manipulating the Diffusion Path."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sketch-an-Anchor is a novel method to train state-of-the-art Zero-shotSketch-based Image Retrieval (ZSSBIR) models in under an epoch. Most studiesbreak down the problem of ZSSBIR into two parts: domain alignment betweenimages and sketches, inherited from SBIR, and generalization to unseen data,inherent to the zero-shot protocol. We argue one of these problems can beconsiderably simplified and re-frame the ZSSBIR problem around thealready-stellar yet underexplored Zero-shot Image-based Retrieval performanceof off-the-shelf models. Our fast-converging model keeps the single-domainperformance while learning to extract similar representations from sketches. Tothis end we introduce our Semantic Anchors -- guiding embeddings learned fromword-based semantic spaces and features from off-the-shelf models -- andcombine them with our novel Anchored Contrastive Loss. Empirical evidence showswe can achieve state-of-the-art performance on all benchmark datasets whiletraining for 100x less iterations than other methods.", "output": "Sketch-an-Anchor: Sub-epoch Fast Model Adaptation for Zero-shot Sketch-based Image Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised denoising has attracted widespread attention due to itsability to train without clean images. However, noise in real-world scenariosis often spatially correlated, which causes many self-supervised algorithmsbased on the pixel-wise independent noise assumption to perform poorly onreal-world images. Recently, asymmetric pixel-shuffle downsampling (AP) hasbeen proposed to disrupt the spatial correlation of noise. However,downsampling introduces aliasing effects, and the post-processing to eliminatethese effects can destroy the spatial structure and high-frequency details ofthe image, in addition to being time-consuming. In this paper, wesystematically analyze downsampling-based methods and propose an AsymmetricTunable Blind-Spot Network (AT-BSN) to address these issues. We design ablind-spot network with a freely tunable blind-spot size, using a largeblind-spot during training to suppress local spatially correlated noise whileminimizing damage to the global structure, and a small blind-spot duringinference to minimize information loss. Moreover, we propose blind-spotself-ensemble and distillation of non-blind-spot network to further improveperformance and reduce computational complexity. Experimental resultsdemonstrate that our method achieves state-of-the-art results whilecomprehensively outperforming other self-supervised methods in terms of imagetexture maintaining, parameter count, computation cost, and inference time.", "output": "Exploring Asymmetric Tunable Blind-Spots for Self-supervised Denoising in Real-World Scenarios."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning semantic segmentation requires pixel-wise annotations, which can betime-consuming and expensive. To reduce the annotation cost, we propose asuperpixel-based active learning (AL) framework, which collects a dominantlabel per superpixel instead. To be specific, it consists of adaptivesuperpixel and sieving mechanisms, fully dedicated to AL. At each round of AL,we adaptively merge neighboring pixels of similar learned features intosuperpixels. We then query a selected subset of these superpixels using anacquisition function assuming no uniform superpixel size. This approach is moreefficient than existing methods, which rely only on innate features such as RGBcolor and assume uniform superpixel sizes. Obtaining a dominant label persuperpixel drastically reduces annotators' burden as it requires fewer clicks.However, it inevitably introduces noisy annotations due to mismatches betweensuperpixel and ground truth segmentation. To address this issue, we furtherdevise a sieving mechanism that identifies and excludes potentially noisyannotations from learning. Our experiments on both Cityscapes and PASCAL VOCdatasets demonstrate the efficacy of adaptive superpixel and sievingmechanisms.", "output": "Adaptive Superpixel for Active Learning in Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-view camera-based 3D object detection has gained popularity due to itslow cost. But accurately inferring 3D geometry solely from camera data remainschallenging, which impacts model performance. One promising approach to addressthis issue is to distill precise 3D geometry knowledge from LiDAR data.However, transferring knowledge between different sensor modalities is hinderedby the significant modality gap. In this paper, we approach this challenge fromthe perspective of both architecture design and knowledge distillation andpresent a new simulated multi-modal 3D object detection method named BEVSimDet.We first introduce a novel framework that includes a LiDAR and camerafusion-based teacher and a simulated multi-modal student, where the studentsimulates multi-modal features with image-only input. To facilitate effectivedistillation, we propose a simulated multi-modal distillation scheme thatsupports intra-modal, cross-modal, and multi-modal distillation simultaneously.By combining them together, BEVSimDet can learn better feature representationsfor 3D object detection while enjoying cost-effective camera-only deployment.Experimental results on the challenging nuScenes benchmark demonstrate theeffectiveness and superiority of BEVSimDet over recent representative methods.The source code will be released.", "output": "BEVSimDet: Simulated Multi-modal Distillation in Bird's-Eye View for Multi-view 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "6D Object pose estimation is a fundamental component in robotics enablingefficient interaction with the environment. It is particularly challenging inbin-picking applications, where many objects are low-feature and reflective,and self-occlusion between objects of the same type is common. We propose anovel multi-view approach leveraging known camera transformations from aneye-in-hand setup to combine heatmap and keypoint estimates into a probabilitydensity map over 3D space. The result is a robust approach that is scalable inthe number of views. It relies on a confidence score composed of keypointprobabilities and point-cloud alignment error, which allows reliable rejectionof false positives. We demonstrate an average pose estimation error ofapproximately 0.5mm and 2 degrees across a variety of difficult low-feature andreflective objects in the ROBI dataset, while also surpassing the state-of-artcorrect detection rate, measured using the 10% object diameter threshold on ADDerror.", "output": "Multi-View Keypoints for Reliable 6D Object Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of language models have moved from encoder-decoder todecoder-only designs. In addition, the common knowledge has it that the twomost popular multimodal tasks, the generative and contrastive tasks, tend toconflict with one another, are hard to accommodate in one architecture, andfurther need complex adaptations for downstream tasks. We propose a novelparadigm of training with a decoder-only model for multimodal tasks, which issurprisingly effective in jointly learning of these disparate vision-languagetasks. This is done with a simple model, called MaMMUT. It consists of a singlevision encoder and a text decoder, and is able to accommodate contrastive andgenerative learning by a novel two-pass approach on the text decoder. Wedemonstrate that joint training of these diverse-objective tasks is simple,effective, and maximizes the weight-sharing of the model. Furthermore, the samearchitecture enables straightforward extensions to open-vocabulary objectdetection and video-language tasks. The model tackles a diverse range of tasks,while being modest in capacity. Our model achieves the SOTA on image-text andtext-image retrieval, video question answering and open-vocabulary detectiontasks, outperforming much larger and more extensively trained foundationalmodels. It shows competitive results on VQA and Video Captioning, especiallyconsidering its size. Ablations confirm the flexibility and advantages of ourapproach.", "output": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How to automatically synthesize natural-looking dance movements based on apiece of music is an incrementally popular yet challenging task. Most existingdata-driven approaches require hard-to-get paired training data and fail togenerate long sequences of motion due to error accumulation of autoregressivestructure. We present a novel 3D dance synthesis system that only needsunpaired data for training and could generate realistic long-term motions atthe same time. For the unpaired data training, we explore the disentanglementof beat and style, and propose a Transformer-based model free of reliance uponpaired data. For the synthesis of long-term motions, we devise a newlong-history attention strategy. It first queries the long-history embeddingthrough an attention computation and then explicitly fuses this embedding intothe generation pipeline via multimodal adaptation gate (MAG). Objective andsubjective evaluations show that our results are comparable to strong baselinemethods, despite not requiring paired training data, and are robust wheninferring long-term music. To our best knowledge, we are the first to achieveunpaired data training - an ability that enables to alleviate data limitationseffectively. Our code is released on ", "output": "Robust Dancer: Long-term 3D Dance Synthesis Using Unpaired Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is broadly known that deep neural networks are susceptible to being fooledby adversarial examples with perturbations imperceptible by humans. Variousdefenses have been proposed to improve adversarial robustness, among whichadversarial training methods are most effective. However, most of these methodstreat the training samples independently and demand a tremendous amount ofsamples to train a robust network, while ignoring the latent structuralinformation among these samples. In this work, we propose a novel LocalStructure Preserving (LSP) regularization, which aims to preserve the localstructure of the input space in the learned embedding space. In this manner,the attacking effect of adversarial samples lying in the vicinity of cleansamples can be alleviated. We show strong empirical evidence that with orwithout adversarial training, our method consistently improves the performanceof adversarial robustness on several image classification datasets compared tothe baselines and some state-of-the-art approaches, thus providing promisingdirection for future research.", "output": "Beyond Empirical Risk Minimization: Local Structure Preserving Regularization for Improving Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is critical that the models pay attention not only to accuracy but also tothe certainty of prediction. Uncertain predictions of deep models caused bynoisy data raise significant concerns in trustworthy AI areas. To explore andhandle uncertainty due to intrinsic data noise, we propose a novel methodcalled ALUM to simultaneously handle the model uncertainty and data uncertaintyin a unified scheme. Rather than solely modeling data uncertainty in theultimate layer of a deep model based on randomly selected training data, wepropose to explore mined adversarial triplets to facilitate data uncertaintymodeling and non-parametric uncertainty estimations to compensate for theinsufficiently trained latent model layers. Thus, the critical data uncertaintyand model uncertainty caused by noisy data can be readily quantified forimproving model robustness. Our proposed ALUM is model-agnostic which can beeasily implemented into any existing deep model with little extra computationoverhead. Extensive experiments on various noisy learning tasks validate thesuperior robustness and generalization ability of our method. The code isreleased at ", "output": "ALUM: Adversarial Data Uncertainty Modeling from Latent Model Uncertainty Compensation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an end-to-end computer vision pipeline to detect non-nutritivesucking (NNS) -- an infant sucking pattern with no nutrition delivered -- as apotential biomarker for developmental delays, using off-the-shelf baby monitorvideo footage. One barrier to clinical (or algorithmic) assessment of NNS stemsfrom its sparsity, requiring experts to wade through hours of footage to findminutes of relevant activity. Our NNS activity segmentation algorithm solvesthis problem by identifying periods of NNS with high certainty -- up to 94.0%average precision and 84.9% average recall across 30 heterogeneous 60 s clips,drawn from our manually annotated NNS clinical in-crib dataset of 183 hours ofovernight baby monitor footage from 19 infants. Our method is based on anunderlying NNS action recognition algorithm, which uses spatiotemporal deeplearning networks and infant-specific pose estimation, achieving 94.9%accuracy in binary classification of 960 2.5 s balanced NNS vs. non-NNS clips.Tested on our second, independent, and public NNS in-the-wild dataset, NNSrecognition classification reaches 92.3% accuracy, and NNS segmentationachieves 90.8% precision and 84.2% recall.", "output": "A Video-based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating the 6-DoF pose of a rigid object from a single RGB image is acrucial yet challenging task. Recent studies have shown the great potential ofdense correspondence-based solutions, yet improvements are still needed toreach practical deployment. In this paper, we propose a novel pose estimationalgorithm named CheckerPose, which improves on three main aspects. Firstly,CheckerPose densely samples 3D keypoints from the surface of the 3D object andfinds their 2D correspondences progressively in the 2D image. Compared toprevious solutions that conduct dense sampling in the image space, our strategyenables the correspondence searching in a 2D grid (i.e., pixel coordinate).Secondly, for our 3D-to-2D correspondence, we design a compact binary coderepresentation for 2D image locations. This representation not only allows forprogressive correspondence refinement but also converts the correspondenceregression to a more efficient classification problem. Thirdly, we adopt agraph neural network to explicitly model the interactions among the sampled 3Dkeypoints, further boosting the reliability and accuracy of thecorrespondences. Together, these novel components make our CheckerPose a strongpose estimation algorithm. When evaluated on the popular Linemod, Linemod-O,and YCB-V object pose estimation benchmarks, CheckerPose clearly boosts theaccuracy of correspondence-based methods and achieves state-of-the-artperformances.", "output": "CheckerPose: Progressive Dense Keypoint Localization for Object Pose Estimation with Graph Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The joint optimization of the sensor trajectory and 3D map is a crucialcharacteristic of Simultaneous Localization and Mapping (SLAM) systems. Toachieve this, the gold standard is Bundle Adjustment (BA). Modern 3D LiDARs nowretain higher resolutions that enable the creation of point cloud imagesresembling those taken by conventional cameras. Nevertheless, the typicaleffective global refinement techniques employed for RGB-D sensors are notwidely applied to LiDARs. This paper presents a novel BA photometric strategythat accounts for both RGB-D and LiDAR in the same way. Our work can be used ontop of any SLAM/GNSS estimate to improve and refine the initial trajectory. Weconducted different experiments using these two depth sensors on publicbenchmarks. Our results show that our system performs on par or better comparedto other state-of-the-art ad-hoc SLAM/BA strategies, free from data associationand without making assumptions about the environment. In addition, we presentthe benefit of jointly using RGB-D and LiDAR within our unified method. Wefinally release an open-source CUDA/C++ implementation.", "output": "Photometric LiDAR and RGB-D Bundle Adjustment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Instant Neural Radiance Fields Stylization, a novel approach formulti-view image stylization for the 3D scene. Our approach models a neuralradiance field based on neural graphics primitives, which use a hashtable-based position encoder for position embedding. We split the positionencoder into two parts, the content and style sub-branches, and train thenetwork for normal novel view image synthesis with the content and styletargets. In the inference stage, we execute AdaIN to the output features of theposition encoder, with content and style voxel grid features as reference. Withthe adjusted features, the stylization of novel view images could be obtained.Our method extends the style target from style images to image sets of scenesand does not require additional network training for stylization. Given a setof images of 3D scenes and a style target(a style image or another set of 3Dscenes), our method can generate stylized novel views with a consistentappearance at various view angles in less than 10 minutes on modern GPUhardware. Extensive experimental results demonstrate the validity andsuperiority of our method.", "output": "Instant Neural Radiance Fields Stylization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study how pretraining label granularity affects thegeneralization of deep neural networks in image classification tasks. We focuson the \"fine-to-coarse\" transfer learning setting where the pretraining labelis more fine-grained than that of the target problem. We experiment with thismethod using the label hierarchy of iNaturalist 2021, and observe a 8.76%relative improvement of the error rate over the baseline. We find the followingconditions are key for the improvement: 1) the pretraining dataset has a strongand meaningful label hierarchy, 2) its label function strongly aligns with thatof the target task, and most importantly, 3) an appropriate level ofpretraining label granularity is chosen. The importance of pretraining labelgranularity is further corroborated by our transfer learning experiments onImageNet. Most notably, we show that pretraining at the leaf labels ofImageNet21k produces better transfer results on ImageNet1k than pretraining atother coarser granularity levels, which supports the common practice.Theoretically, through an analysis on a two-layer convolutional ReLU network,we prove that: 1) models trained on coarse-grained labels only respond stronglyto the common or \"easy-to-learn\" features; 2) with the dataset satisfying theright conditions, fine-grained pretraining encourages the model to also learnrarer or \"harder-to-learn\" features well, thus improving the model'sgeneralization.", "output": "Towards Understanding the Effect of Pretraining Label Granularity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, many visual scene understanding problems are addressed by denseprediction networks. But pixel-wise dense annotations are very expensive (e.g.,for scene parsing) or impossible (e.g., for intrinsic image decomposition),motivating us to leverage cheap point-level weak supervision. However, existingpointly-supervised methods still use the same architecture designed for fullsupervision. In stark contrast to them, we propose a new paradigm that makespredictions for point coordinate queries, as inspired by the recent success ofimplicit representations, like distance or radiance fields. As such, the methodis named as dense prediction fields (DPFs). DPFs generate expressiveintermediate features for continuous sub-pixel locations, thus allowing outputsof an arbitrary resolution. DPFs are naturally compatible with point-levelsupervision. We showcase the effectiveness of DPFs using two substantiallydifferent tasks: high-level semantic parsing and low-level intrinsic imagedecomposition. In these two cases, supervision comes in the form ofsingle-point semantic category and two-point relative reflectance,respectively. As benchmarked by three large-scale public datasetsPASCALContext, ADE20K and IIW, DPFs set new state-of-the-art performance on allof them with significant margins.Code can be accessed at ", "output": "DPF: Learning Dense Prediction Fields with Weak Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing instance segmentation models learn task-specific information usingmanual mask annotations from base (training) categories. These mask annotationsrequire tremendous human effort, limiting the scalability to annotate novel(new) categories. To alleviate this problem, Open-Vocabulary (OV) methodsleverage large-scale image-caption pairs and vision-language models to learnnovel categories. In summary, an OV method learns task-specific informationusing strong supervision from base annotations and novel category informationusing weak supervision from image-captions pairs. This difference betweenstrong and weak supervision leads to overfitting on base categories, resultingin poor generalization towards novel categories. In this work, we overcome thisissue by learning both base and novel categories from pseudo-mask annotationsgenerated by the vision-language model in a weakly supervised manner using ourproposed Mask-free OVIS pipeline. Our method automatically generatespseudo-mask annotations by leveraging the localization ability of a pre-trainedvision-language model for objects present in image-caption pairs. The generatedpseudo-mask annotations are then used to supervise an instance segmentationmodel, freeing the entire pipeline from any labour-expensive instance-levelannotations and overfitting. Our extensive experiments show that our methodtrained with just pseudo-masks significantly improves the mAP scores on theMS-COCO dataset and OpenImages dataset compared to the recent state-of-the-artmethods trained with manual masks. Codes and models are provided in", "output": "Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformers have shown great success in medical image segmentation. However,transformers may exhibit a limited generalization ability due to the underlyingsingle-scale self-attention (SA) mechanism. In this paper, we address thisissue by introducing a Multi-scale hiERarchical vIsion Transformer (MERIT)backbone network, which improves the generalizability of the model by computingSA at multiple scales. We also incorporate an attention-based decoder, namelyCascaded Attention Decoding (CASCADE), for further refinement of multi-stagefeatures generated by MERIT. Finally, we introduce an effective multi-stagefeature mixing loss aggregation (MUTATION) method for better model training viaimplicit ensembling. Our experiments on two widely used medical imagesegmentation benchmarks (i.e., Synapse Multi-organ, ACDC) demonstrate thesuperior performance of MERIT over state-of-the-art methods. Our MERITarchitecture and MUTATION loss aggregation can be used with downstream medicalimage and semantic segmentation tasks.", "output": "Multi-scale Hierarchical Vision Transformer with Cascaded Attention Decoding for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding 3D scenes from multi-view inputs has been proven to alleviatethe view discrepancy issue in 3D visual grounding. However, existing methodsnormally neglect the view cues embedded in the text modality and fail to weighthe relative importance of different views. In this paper, we proposeViewRefer, a multi-view framework for 3D visual grounding exploring how tograsp the view knowledge from both text and 3D modalities. For the text branch,ViewRefer leverages the diverse linguistic knowledge of large-scale languagemodels, e.g., GPT, to expand a single grounding text to multiplegeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformerfusion module with inter-view attention is introduced to boost the interactionof objects across views. On top of that, we further present a set of learnablemulti-view prototypes, which memorize scene-agnostic knowledge for differentviews, and enhance the framework from two perspectives: a view-guided attentionmodule for more robust text features, and a view-guided scoring strategy duringthe final prediction. With our designed paradigm, ViewRefer achieves superiorperformance on three benchmarks and surpasses the second-best by +2.8%, +1.2%,and +0.73% on Sr3D, Nr3D, and ScanRefer. Code will be released at", "output": "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modeling sounds emitted from physical object interactions is critical forimmersive perceptual experiences in real and virtual worlds. Traditionalmethods of impact sound synthesis use physics simulation to obtain a set ofphysics parameters that could represent and synthesize the sound. However, theyrequire fine details of both the object geometries and impact locations, whichare rarely available in the real world and can not be applied to synthesizeimpact sounds from common videos. On the other hand, existing video-driven deeplearning-based approaches could only capture the weak correspondence betweenvisual content and impact sounds since they lack of physics knowledge. In thiswork, we propose a physics-driven diffusion model that can synthesizehigh-fidelity impact sound for a silent video clip. In addition to the videocontent, we propose to use additional physics priors to guide the impact soundsynthesis procedure. The physics priors include both physics parameters thatare directly estimated from noisy real-world impact sound examples withoutsophisticated setup and learned residual parameters that interpret the soundenvironment via neural networks. We further implement a novel diffusion modelwith specific training and inference strategies to combine physics priors andvisual information for impact sound synthesis. Experimental results show thatour model outperforms several existing systems in generating realistic impactsounds. More importantly, the physics-based representations are fullyinterpretable and transparent, thus enabling us to perform sound editingflexibly.", "output": "Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The objective of this paper is an automatic Audio Description (AD) model thatingests movies and outputs AD in text form. Generating high-quality movie AD ischallenging due to the dependency of the descriptions on context, and thelimited amount of training data available. In this work, we leverage the powerof pretrained foundation models, such as GPT and CLIP, and only train a mappingnetwork that bridges the two models for visually-conditioned text generation.In order to obtain high-quality AD, we make the following four contributions:(i) we incorporate context from the movie clip, AD from previous clips, as wellas the subtitles; (ii) we address the lack of training data by pretraining onlarge-scale datasets, where visual or contextual information is unavailable,e.g. text-only AD without movies or visual captioning datasets without context;(iii) we improve on the currently available AD datasets, by removing labelnoise in the MAD dataset, and adding character naming information; and (iv) weobtain strong results on the movie AD task compared with previous methods.", "output": "AutoAD: Movie Description in Context."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inspired by the long-range modeling ability of ViTs, large-kernelconvolutions are widely studied and adopted recently to enlarge the receptivefield and improve model performance, like the remarkable work ConvNeXt whichemploys 7x7 depthwise convolution. Although such depthwise operator onlyconsumes a few FLOPs, it largely harms the model efficiency on powerfulcomputing devices due to the high memory access costs. For example, ConvNeXt-Thas similar FLOPs with ResNet-50 but only achieves 60% throughputs when trainedon A100 GPUs with full precision. Although reducing the kernel size of ConvNeXtcan improve speed, it results in significant performance degradation. It isstill unclear how to speed up large-kernel-based CNN models while preservingtheir performance. To tackle this issue, inspired by Inceptions, we propose todecompose large-kernel depthwise convolution into four parallel branches alongchannel dimension, i.e. small square kernel, two orthogonal band kernels, andan identity mapping. With this new Inception depthwise convolution, we build aseries of networks, namely IncepitonNeXt, which not only enjoy high throughputsbut also maintain competitive performance. For instance, InceptionNeXt-Tachieves 1.6x higher training throughputs than ConvNeX-T, as well as attains0.2% top-1 accuracy improvement on ImageNet-1K. We anticipate InceptionNeXt canserve as an economical baseline for future architecture design to reduce carbonfootprint. Code is available at ", "output": "InceptionNeXt: When Inception Meets ConvNeXt."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Steerable properties dominate the design of traditional filters, e.g., Gaborfilters, and endow features the capability of dealing with spatialtransformations. However, such excellent properties have not been well exploredin the popular deep convolutional neural networks (DCNNs). In this paper, wepropose a new deep model, termed Gabor Convolutional Networks (GCNs or GaborCNNs), which incorporates Gabor filters into DCNNs to enhance the resistance ofdeep learned features to the orientation and scale changes. By onlymanipulating the basic element of DCNNs based on Gabor filters, i.e., theconvolution operator, GCNs can be easily implemented and are compatible withany popular deep learning architecture. Experimental results demonstrate thesuper capability of our algorithm in recognizing objects, where the scale androtation changes occur frequently. The proposed GCNs have much fewer learnablenetwork parameters, and thus is easier to train with an end-to-end pipeline.", "output": "Gabor Convolutional Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep convolutional neural networks (CNN) have massively influenced recentadvances in large-scale image classification. More recently, a dynamic routingalgorithm with capsules (groups of neurons) has shown state-of-the-artrecognition performance. However, the behavior of such networks in the presenceof a degrading signal (noise) is mostly unexplored. An analytical study ondifferent network architectures toward noise robustness is essential forselecting the appropriate model in a specific application scenario. This paperpresents an extensive performance analysis of six deep architectures for imageclassification on six most common image degradation models. In this study, wehave compared VGG-16, VGG-19, ResNet-50, Inception-v3, MobileNet and CapsuleNetarchitectures on Gaussian white, Gaussian color, salt-and-pepper, Gaussianblur, motion blur and JPEG compression noise models.", "output": "Effects of Degradations on Deep Neural Network Architectures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Textual information in a captured scene plays an important role in sceneinterpretation and decision making. Though there exist methods that cansuccessfully detect and interpret complex text regions present in a scene, tothe best of our knowledge, there is no significant prior work that aims tomodify the textual information in an image. The ability to edit text directlyon images has several advantages including error correction, text restorationand image reusability. In this paper, we propose a method to modify text in animage at character-level. We approach the problem in two stages. At first, theunobserved character (target) is generated from an observed character (source)being modified. We propose two different neural network architectures - (a)FANnet to achieve structural consistency with source font and (b) Colornet topreserve source color. Next, we replace the source character with the generatedcharacter maintaining both geometric and visual consistency with neighboringcharacters. Our method works as a unified platform for modifying text inimages. We present the effectiveness of our method on COCO-Text and ICDARdatasets both qualitatively and quantitatively.", "output": "STEFANN: Scene Text Editor using Font Adaptive Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Evaluating the robustness of a defense model is a challenging task inadversarial robustness research. Obfuscated gradients have previously beenfound to exist in many defense methods and cause a false signal of robustness.In this paper, we identify a more subtle situation called Imbalanced Gradientsthat can also cause overestimated adversarial robustness. The phenomenon ofimbalanced gradients occurs when the gradient of one term of the margin lossdominates and pushes the attack towards to a suboptimal direction. To exploitimbalanced gradients, we formulate a Margin Decomposition (MD) attack thatdecomposes a margin loss into individual terms and then explores theattackability of these terms separately via a two-stage process. We alsopropose a multi-targeted and ensemble version of our MD attack. Byinvestigating 24 defense models proposed since 2018, we find that 11 models aresusceptible to a certain degree of imbalanced gradients and our MD attack candecrease their robustness evaluated by the best standalone baseline attack bymore than 1%. We also provide an in-depth investigation on the likely causes ofimbalanced gradients and effective countermeasures. Our code is available at", "output": "Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we present a novel matrix-encoding method that is particularlyconvenient for neural networks to make predictions in a privacy-preservingmanner using homomorphic encryption. Based on this encoding method, weimplement a convolutional neural network for handwritten image classificationover encryption. For two matrices $A$ and $B$ to perform homomorphicmultiplication, the main idea behind it, in a simple version, is to encryptmatrix $A$ and the transpose of matrix $B$ into two ciphertexts respectively.With additional operations, the homomorphic matrix multiplication can becalculated over encrypted matrices efficiently. For the convolution operation,we in advance span each convolution kernel to a matrix space of the same sizeas the input image so as to generate several ciphertexts, each of which islater used together with the ciphertext encrypting input images for calculatingsome of the final convolution results. We accumulate all these intermediateresults and thus complete the convolution operation.In a public cloud with 40 vCPUs, our convolutional neural networkimplementation on the MNIST testing dataset takes $sim$ 287 seconds to computeten likelihoods of 32 encrypted images of size $28 times 28$ simultaneously.The data owner only needs to upload one ciphertext ($sim 19.8$ MB) encryptingthese 32 images to the public cloud.", "output": "Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving Neural Networks (Inference)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In some scenarios, classifier requires detecting out-of-distribution samplesfar from its training data. With desirable characteristics, reconstructionautoencoder-based methods deal with this problem by using input reconstructionerror as a metric of novelty vs. normality. We formulate the essence of suchapproach as a quadruplet domain translation with an intrinsic bias to onlyquery for a proxy of conditional data uncertainty. Accordingly, an improvementdirection is formalized as maximumly compressing the autoencoder's latent spacewhile ensuring its reconstructive power for acting as a described domaintranslator. From it, strategies are introduced including semanticreconstruction, data certainty decomposition and normalized L2 distance tosubstantially improve original methods, which together establishstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR ofCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our methodworks without any additional data, hard-to-implement structure, time-consumingpipeline, and even harming the classification accuracy of known classes.", "output": "Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel method to reconstruct the 3D shapes of transparent objectsusing hand-held captured images under natural light conditions. It combines theadvantage of explicit mesh and multi-layer perceptron (MLP) network, a hybridrepresentation, to simplify the capture setting used in recent contributions.After obtaining an initial shape through the multi-view silhouettes, weintroduce surface-based local MLPs to encode the vertex displacement field(VDF) for the reconstruction of surface details. The design of local MLPsallows to represent the VDF in a piece-wise manner using two layer MLPnetworks, which is beneficial to the optimization algorithm. Defining localMLPs on the surface instead of the volume also reduces the searching space.Such a hybrid representation enables us to relax the ray-pixel correspondencesthat represent the light path constraint to our designed ray-cellcorrespondences, which significantly simplifies the implementation ofsingle-image based environment matting algorithm. We evaluate ourrepresentation and reconstruction algorithm on several transparent objects withground truth models. Our experiments show that our method can producehigh-quality reconstruction results superior to state-of-the-art methods usinga simplified data acquisition setup.", "output": "Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) allows multiple medical institutions tocollaboratively learn a global model without centralizing client data. It isdifficult, if possible at all, for such a global model to commonly achieveoptimal performance for each individual client, due to the heterogeneity ofmedical images from various scanners and patient demographics. This problembecomes even more significant when deploying the global model to unseen clientsoutside the FL with unseen distributions not presented during federatedtraining. To optimize the prediction accuracy of each individual client formedical imaging tasks, we propose a novel unified framework for bothtextit{Inside and Outside model Personalization in FL} (IOP-FL). Our insidepersonalization uses a lightweight gradient-based approach that exploits thelocal adapted model for each client, by accumulating both the global gradientsfor common knowledge and the local gradients for client-specific optimization.Moreover, and importantly, the obtained local personalized models and theglobal model can form a diverse and informative routing space to personalize anadapted model for outside FL clients. Hence, we design a new test-time routingscheme using the consistency loss with a shape constraint to dynamicallyincorporate the models, given the distribution information conveyed by the testdata. Our extensive experimental results on two medical image segmentationtasks present significant improvements over SOTA methods on both inside andoutside personalization, demonstrating the potential of our IOP-FL scheme forclinical practice.", "output": "IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient neural network backbones for mobile devices are often optimized formetrics such as FLOPs or parameter count. However, these metrics may notcorrelate well with latency of the network when deployed on a mobile device.Therefore, we perform extensive analysis of different metrics by deployingseveral mobile-friendly networks on a mobile device. We identify and analyzearchitectural and optimization bottlenecks in recent efficient neural networksand provide ways to mitigate these bottlenecks. To this end, we design anefficient backbone MobileOne, with variants achieving an inference time under 1ms on an iPhone12 with 75.9% top-1 accuracy on ImageNet. We show that MobileOneachieves state-of-the-art performance within the efficient architectures whilebeing many times faster on mobile. Our best model obtains similar performanceon ImageNet as MobileFormer while being 38x faster. Our model obtains 2.3%better top-1 accuracy on ImageNet than EfficientNet at similar latency.Furthermore, we show that our model generalizes to multiple tasks - imageclassification, object detection, and semantic segmentation with significantimprovements in latency and accuracy as compared to existing efficientarchitectures when deployed on a mobile device. Code and models are availableat ", "output": "MobileOne: An Improved One millisecond Mobile Backbone."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In order for artificial agents to successfully perform tasks in changingenvironments, they must be able to both detect and adapt to novelty. However,visual novelty detection research often only evaluates on repurposed datasetssuch as CIFAR-10 originally intended for object classification, where imagesfocus on one distinct, well-centered object. New benchmarks are needed torepresent the challenges of navigating the complex scenes of an open world. Ournew NovelCraft dataset contains multimodal episodic data of the images andsymbolic world-states seen by an agent completing a pogo stick assembly taskwithin a modified Minecraft environment. In some episodes, we insert novelobjects of varying size within the complex 3D scene that may impact gameplay.Our visual novelty detection benchmark finds that methods that rank best onpopular area-under-the-curve metrics may be outperformed by simpleralternatives when controlling false positives matters most. Further multimodalnovelty detection experiments suggest that methods that fuse both visual andsymbolic information can improve time until detection as well as overalldiscrimination. Finally, our evaluation of recent generalized categorydiscovery methods suggests that adapting to new imbalanced categories incomplex scenes remains an exciting open problem.", "output": "NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The detector-free feature matching approaches are currently attracting greatattention thanks to their excellent performance. However, these methods stillstruggle at large-scale and viewpoint variations, due to the geometricinconsistency resulting from the application of the mutual nearest neighbourcriterion (ie, one-to-one assignment) in patch-level matching.Accordingly, weintroduce AdaMatcher, which first accomplishes the feature correlation andco-visible area estimation through an elaborate feature interaction module,then performs adaptive assignment on patch-level matching while estimating thescales between images, and finally refines the co-visible matches through scalealignment and sub-pixel regression module.Extensive experiments show thatAdaMatcher outperforms solid baselines and achieves state-of-the-art results onmany downstream tasks. Additionally, the adaptive assignment and sub-pixelrefinement module can be used as a refinement network for other matchingmethods, such as SuperGlue, to boost their performance further. The code willbe publicly available at ", "output": "Adaptive Assignment for Geometry Aware Local Feature Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Deepfake has drawn considerable public attention due to securityand privacy concerns in social media digital forensics. As the wildly spreadingDeepfake videos on the Internet become more realistic, traditional detectiontechniques have failed in distinguishing between real and fake. Most existingdeep learning methods mainly focus on local features and relations within theface image using convolutional neural networks as a backbone. However, localfeatures and relations are insufficient for model training to learn enoughgeneral information for Deepfake detection. Therefore, the existing Deepfakedetection methods have reached a bottleneck to further improve the detectionperformance. To address this issue, we propose a deep convolutional Transformerto incorporate the decisive image features both locally and globally.Specifically, we apply convolutional pooling and re-attention to enrich theextracted features and enhance efficacy. Moreover, we employ the barelydiscussed image keyframes in model training for performance improvement andvisualize the feature quantity gap between the key and normal image framescaused by video compression. We finally illustrate the transferability withextensive experiments on several Deepfake benchmark datasets. The proposedsolution consistently outperforms several state-of-the-art baselines on bothwithin- and cross-dataset experiments.", "output": "Deep Convolutional Pooling Transformer for Deepfake Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding dynamic hand motions and actions from egocentric RGB videos isa fundamental yet challenging task due to self-occlusion and ambiguity. Toaddress occlusion and ambiguity, we develop a transformer-based framework toexploit temporal information for robust estimation. Noticing the differenttemporal granularity of and the semantic correlation between hand poseestimation and action recognition, we build a network hierarchy with twocascaded transformer encoders, where the first one exploits the short-termtemporal cue for hand pose estimation, and the latter aggregates per-frame poseand object information over a longer time span to recognize the action. Ourapproach achieves competitive results on two first-person hand actionbenchmarks, namely FPHA and H2O. Extensive ablation studies verify our designchoices.", "output": "Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel method for exemplar-based image translation, calledmatching interleaved diffusion models (MIDMs). Most existing methods for thistask were formulated as GAN-based matching-then-generation framework. However,in this framework, matching errors induced by the difficulty of semanticmatching across cross-domain, e.g., sketch and photo, can be easily propagatedto the generation step, which in turn leads to degenerated results. Motivatedby the recent success of diffusion models overcoming the shortcomings of GANs,we incorporate the diffusion models to overcome these limitations.Specifically, we formulate a diffusion-based matching-and-generation frameworkthat interleaves cross-domain matching and diffusion steps in the latent spaceby iteratively feeding the intermediate warp into the noising process anddenoising it to generate a translated image. In addition, to improve thereliability of the diffusion process, we design a confidence-aware processusing cycle-consistency to consider only confident regions during translation.Experimental results show that our MIDMs generate more plausible images thanstate-of-the-art methods.", "output": "MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in image captioning have focused on scaling the data andmodel size, substantially increasing the cost of pre-training and finetuning.As an alternative to large models, we present SmallCap, which generates acaption conditioned on an input image and related captions retrieved from adatastore. Our model is lightweight and fast to train, as the only learnedparameters are in newly introduced cross-attention layers between a pre-trainedCLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains withoutadditional finetuning and can exploit large-scale data in a training-freefashion since the contents of the datastore can be readily replaced. Ourexperiments show that SmallCap, trained only on COCO, has competitiveperformance on this benchmark, and also transfers to other domains withoutretraining, solely through retrieval from target-domain data. Furtherimprovement is achieved through the training-free exploitation of diversehuman-labeled and web data, which proves to be effective for a range ofdomains, including the nocaps benchmark, designed to test generalization tounseen visual concepts.", "output": "SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The main purpose of this study is to develop a pipeline for COVID-19detection from a big and challenging database of Computed Tomography (CT)images. The proposed pipeline includes a segmentation part, a lung extractionpart, and a classifier part. Optional slice removal techniques after UNet-basedsegmentation of slices were also tried. The methodologies tried in thesegmentation part are traditional segmentation methods as well as UNet-basedmethods. In the classification part, a Convolutional Neural Network (CNN) wasused to take the final diagnosis decisions. In terms of the results: in thesegmentation part, the proposed segmentation methods show high dice scores on apublicly available dataset. In the classification part, the results werecompared at slice-level and at patient-level as well. At slice-level, methodswere compared and showed high validation accuracy indicating efficiency inpredicting 2D slices. At patient level, the proposed methods were also comparedin terms of validation accuracy and macro F1 score on the validation set. Thedataset used for classification is COV-19CT Database. The method proposed hereshowed improvement from our precious results on the same dataset. InConclusion, the improved work in this paper has potential clinical usages forCOVID-19 detection and diagnosis via CT images. The code is on github at", "output": "COVID-19 Detection Using Segmentation, Region Extraction and Classification Pipeline."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the success of pretraining techniques in representation learning, anumber of continual learning methods based on pretrained models have beenproposed. Some of these methods design continual learning mechanisms on thepre-trained representations and only allow minimum updates or even no updatesof the backbone models during the training of continual learning. In thispaper, we question whether the complexity of these models is needed to achievegood performance by comparing them to a simple baseline that we designed. Weargue that the pretrained feature extractor itself can be strong enough toachieve a competitive or even better continual learning performance onSplit-CIFAR100 and CoRe 50 benchmarks. To validate this, we conduct a verysimple baseline that 1) use the frozen pretrained model to extract imagefeatures for every class encountered during the continual learning stage andcompute their corresponding mean features on training data, and 2) predict theclass of the input based on the nearest neighbor distance between test samplesand mean features of the classes; i.e., Nearest Mean Classifier (NMC). Thisbaseline is single-headed, exemplar-free, and can be task-free (by updating themeans continually). This baseline achieved 88.53% on 10-Split-CIFAR-100,surpassing most state-of-the-art continual learning methods that are allinitialized using the same pretrained transformer model. We hope our baselinemay encourage future progress in designing learning systems that cancontinually add quality to the learning representations even if they startedfrom some pretrained weights.", "output": "A Simple Baseline that Questions the Use of Pretrained-Models in Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study the transferability of ImageNet spatial and Kineticsspatio-temporal representations to multi-label Movie Trailer GenreClassification (MTGC). In particular, we present an extensive evaluation of thetransferability of ConvNet and Transformer models pretrained on ImageNet andKinetics to Trailers12k, a new manually-curated movie trailer dataset composedof 12,000 videos labeled with 10 different genres and associated metadata. Weanalyze different aspects that can influence transferability, such as framerate, input video extension, and spatio-temporal modeling. In order to reducethe spatio-temporal structure gap between ImageNet/Kinetics and Trailers12k, wepropose Dual Image and Video Transformer Architecture (DIViTA), which performsshot detection so as to segment the trailer into highly correlated clips,providing a more cohesive input for pretrained backbones and improvingtransferability (a 1.83% increase for ImageNet and 3.75% for Kinetics). Ourresults demonstrate that representations learned on either ImageNet or Kineticsare comparatively transferable to Trailers12k. Moreover, both datasets providecomplementary information that can be combined to improve classificationperformance (a 2.91% gain compared to the top single pretraining).Interestingly, using lightweight ConvNets as pretrained backbones resulted inonly a 3.46% drop in classification performance compared with the topTransformer while requiring only 11.82% of its parameters and 0.81% of itsFLOPS.", "output": "Improving Transfer Learning with a Dual Image and Video Transformer for Multi-label Movie Trailer Genre Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models achieve outstanding generative performance in variousdomains. Despite their great success, they lack semantic latent space which isessential for controlling the generative process. To address the problem, wepropose asymmetric reverse process (Asyrp) which discovers the semantic latentspace in frozen pretrained diffusion models. Our semantic latent space, namedh-space, has nice properties for accommodating semantic image manipulation:homogeneity, linearity, robustness, and consistency across timesteps. Inaddition, we introduce a principled design of the generative process forversatile editing and quality boost ing by quantifiable measures: editingstrength of an interval and quality deficiency at a timestep. Our method isapplicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets(CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page:", "output": "Diffusion Models already have a Semantic Latent Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image completion is a task that aims to fill in the missing region of amasked image with plausible contents. However, existing image completionmethods tend to fill in the missing region with the surrounding texture insteadof hallucinating a visual instance that is suitable in accordance with thecontext of the scene. In this work, we propose a novel image completion model,dubbed ImComplete, that hallucinates the missing instance that harmonizes wellwith - and thus preserves - the original context. ImComplete first adopts atransformer architecture that considers the visible instances and the locationof the missing region. Then, ImComplete completes the semantic segmentationmasks within the missing region, providing pixel-level semantic and structuralguidance. Finally, the image synthesis blocks generate photo-realistic content.We perform a comprehensive evaluation of the results in terms of visual quality(LPIPS and FID) and contextual preservation scores (CLIPscore and objectdetection accuracy) with COCO-panoptic and Visual Genome datasets. Experimentalresults show the superiority of ImComplete on various natural images.", "output": "Instance-Aware Image Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face recognition (FR) models can be easily fooled by adversarial examples,which are crafted by adding imperceptible perturbations on benign face images.The existence of adversarial face examples poses a great threat to the securityof society. In order to build a more sustainable digital nation, in this paper,we improve the transferability of adversarial face examples to expose moreblind spots of existing FR models. Though generating hard samples has shown itseffectiveness in improving the generalization of models in training tasks, theeffectiveness of utilizing this idea to improve the transferability ofadversarial face examples remains unexplored. To this end, based on theproperty of hard samples and the symmetry between training tasks andadversarial attack tasks, we propose the concept of hard models, which havesimilar effects as hard samples for adversarial attack tasks. Utilizing theconcept of hard models, we propose a novel attack method called BeneficialPerturbation Feature Augmentation Attack (BPFA), which reduces the overfittingof adversarial examples to surrogate FR models by constantly generating newhard models to craft the adversarial examples. Specifically, in thebackpropagation, BPFA records the gradients on pre-selected feature maps anduses the gradient on the input image to craft the adversarial example. In thenext forward propagation, BPFA leverages the recorded gradients to addbeneficial perturbations on their corresponding feature maps to increase theloss. Extensive experiments demonstrate that BPFA can significantly boost thetransferability of adversarial attacks on FR.", "output": "Improving the Transferability of Adversarial Attacks on Face Recognition with Beneficial Perturbation Feature Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Decoding visual stimuli from brain recordings aims to deepen ourunderstanding of the human visual system and build a solid foundation forbridging human and computer vision through the Brain-Computer Interface.However, reconstructing high-quality images with correct semantics from brainrecordings is a challenging problem due to the complex underlyingrepresentations of brain signals and the scarcity of data annotations. In thiswork, we present MinD-Vis: Sparse Masked Brain Modeling with Double-ConditionedLatent Diffusion Model for Human Vision Decoding. Firstly, we learn aneffective self-supervised representation of fMRI data using mask modeling in alarge latent space inspired by the sparse coding of information in the primaryvisual cortex. Then by augmenting a latent diffusion model withdouble-conditioning, we show that MinD-Vis can reconstruct highly plausibleimages with semantically matching details from brain recordings using very fewpaired annotations. We benchmarked our model qualitatively and quantitatively;the experimental results indicate that our method outperformed state-of-the-artin both semantic mapping (100-way semantic classification) and generationquality (FID) by 66% and 41% respectively. An exhaustive ablation study wasalso conducted to analyze our framework.", "output": "Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusion models, a class of generative models, have garneredimmense interest lately in various deep-learning problems. A diffusionprobabilistic model defines a forward diffusion stage where the input data isgradually perturbed over several steps by adding Gaussian noise and then learnsto reverse the diffusion process to retrieve the desired noise-free data fromnoisy data samples. Diffusion models are widely appreciated for their strongmode coverage and quality of the generated samples despite their knowncomputational burdens. Capitalizing on the advances in computer vision, thefield of medical imaging has also observed a growing interest in diffusionmodels. To help the researcher navigate this profusion, this survey intends toprovide a comprehensive overview of diffusion models in the discipline ofmedical image analysis. Specifically, we introduce the solid theoreticalfoundation and fundamental concepts behind diffusion models and the threegeneric diffusion modelling frameworks: diffusion probabilistic models,noise-conditioned score networks, and stochastic differential equations. Then,we provide a systematic taxonomy of diffusion models in the medical domain andpropose a multi-perspective categorization based on their application, imagingmodality, organ of interest, and algorithms. To this end, we cover extensiveapplications of diffusion models in the medical domain. Furthermore, weemphasize the practical use case of some selected approaches, and then wediscuss the limitations of the diffusion models in the medical domain andpropose several directions to fulfill the demands of this field. Finally, wegather the overviewed studies with their available open-source implementationsat", "output": "Diffusion Models for Medical Image Analysis: A Comprehensive Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of human reposing involves generating a realistic image of a personstanding in an arbitrary conceivable pose. There are multiple difficulties ingenerating perceptually accurate images, and existing methods suffer fromlimitations in preserving texture, maintaining pattern coherence, respectingcloth boundaries, handling occlusions, manipulating skin generation, etc. Thesedifficulties are further exacerbated by the fact that the possible space ofpose orientation for humans is large and variable, the nature of clothing itemsis highly non-rigid, and the diversity in body shape differs largely among thepopulation. To alleviate these difficulties and synthesize perceptuallyaccurate images, we propose VGFlow. Our model uses a visibility-guided flowmodule to disentangle the flow into visible and invisible parts of the targetfor simultaneous texture preservation and style manipulation. Furthermore, totackle distinct body shapes and avoid network artifacts, we also incorporate aself-supervised patch-wise \"realness\" loss to improve the output. VGFlowachieves state-of-the-art results as observed qualitatively and quantitativelyon different image quality metrics (SSIM, LPIPS, FID).", "output": "VGFlow: Visibility guided Flow Network for Human Reposing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attribution methods, which employ heatmaps to identify the most influentialregions of an image that impact model decisions, have gained widespreadpopularity as a type of explainability method. However, recent research hasexposed the limited practical value of these methods, attributed in part totheir narrow focus on the most prominent regions of an image -- revealing\"where\" the model looks, but failing to elucidate \"what\" the model sees inthose areas. In this work, we try to fill in this gap with CRAFT -- a novelapproach to identify both \"what\" and \"where\" by generating concept-basedexplanations. We introduce 3 new ingredients to the automatic conceptextraction literature: (i) a recursive strategy to detect and decomposeconcepts across layers, (ii) a novel method for a more faithful estimation ofconcept importance using Sobol indices, and (iii) the use of implicitdifferentiation to unlock Concept Attribution Maps.We conduct both human and computer vision experiments to demonstrate thebenefits of the proposed approach. We show that the proposed concept importanceestimation technique is more faithful to the model than previous methods. Whenevaluating the usefulness of the method for human experimenters on ahuman-centered utility benchmark, we find that our approach significantlyimproves on two of the three test scenarios. Our code is freely available atgithub.com/deel-ai/Craft.", "output": "CRAFT: Concept Recursive Activation FacTorization for Explainability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Synthesizing 3D human avatars interacting realistically with a scene is animportant problem with applications in AR/VR, video games and robotics. Towardsthis goal, we address the task of generating a virtual human -- hands and fullbody -- grasping everyday objects. Existing methods approach this problem bycollecting a 3D dataset of humans interacting with objects and training on thisdata. However, 1) these methods do not generalize to different object positionsand orientations, or to the presence of furniture in the scene, and 2) thediversity of their generated full-body poses is very limited. In this work, weaddress all the above challenges to generate realistic, diverse full-bodygrasps in everyday scenes without requiring any 3D full-body grasping data. Ourkey insight is to leverage the existence of both full-body pose and handgrasping priors, composing them using 3D geometrical constraints to obtainfull-body grasps. We empirically validate that these constraints can generate avariety of feasible human grasps that are superior to baselines bothquantitatively and qualitatively. See our webpage for more details:", "output": "FLEX: Full-Body Grasping Without Full-Body Grasps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Standard semantic segmentation models owe their success to curated datasetswith a fixed set of semantic categories, without contemplating the possibilityof identifying unknown objects from novel categories. Existing methods inoutlier detection suffer from a lack of smoothness and objectness in theirpredictions, due to limitations of the per-pixel classification paradigm.Furthermore, additional training for detecting outliers harms the performanceof known classes. In this paper, we explore another paradigm with region-levelclassification to better segment unknown objects. We show that the objectqueries in mask classification tend to behave like one vs all classifiers.Based on this finding, we propose a novel outlier scoring function called RbAby defining the event of being an outlier as being rejected by all knownclasses. Our extensive experiments show that mask classification improves theperformance of the existing outlier detection methods, and the best results areachieved with the proposed RbA. We also propose an objective to optimize RbAusing minimal outlier supervision. Further fine-tuning with outliers improvesthe unknown performance, and unlike previous methods, it does not degrade theinlier performance.", "output": "RbA: Segmenting Unknown Regions Rejected by All."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work addresses cross-view camera pose estimation, i.e., determining the3-Degrees-of-Freedom camera pose of a given ground-level image w.r.t. an aerialimage of the local area. We propose SliceMatch, which consists of ground andaerial feature extractors, feature aggregators, and a pose predictor. Thefeature extractors extract dense features from the ground and aerial images.Given a set of candidate camera poses, the feature aggregators construct asingle ground descriptor and a set of pose-dependent aerial descriptors.Notably, our novel aerial feature aggregator has a cross-view attention modulefor ground-view guided aerial feature selection and utilizes the geometricprojection of the ground camera's viewing frustum on the aerial image to poolfeatures. The efficient construction of aerial descriptors is achieved usingprecomputed masks. SliceMatch is trained using contrastive learning and poseestimation is formulated as a similarity comparison between the grounddescriptor and the aerial descriptors. Compared to the state-of-the-art,SliceMatch achieves a 19% lower median localization error on the VIGORbenchmark using the same VGG16 backbone at 150 frames per second, and a 50%lower error when using a ResNet50 backbone.", "output": "SliceMatch: Geometry-guided Aggregation for Cross-View Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Changing how pre-trained models behave -- e.g., improving their performanceon a downstream task or mitigating biases learned during pre-training -- is acommon practice when developing machine learning systems. In this work, wepropose a new paradigm for steering the behavior of neural networks, centeredaround textit{task vectors}. A task vector specifies a direction in the weightspace of a pre-trained model, such that movement in that direction improvesperformance on the task. We build task vectors by subtracting the weights of apre-trained model from the weights of the same model after fine-tuning on atask. We show that these task vectors can be modified and combined togetherthrough arithmetic operations such as negation and addition, and the behaviorof the resulting model is steered accordingly. Negating a task vector decreasesperformance on the target task, with little change in model behavior on controltasks. Moreover, adding task vectors together can improve performance onmultiple tasks at once. Finally, when tasks are linked by an analogyrelationship of the form ``A is to B as C is to D\", combining task vectors fromthree of the tasks can improve performance on the fourth, even when no datafrom the fourth task is used for training. Overall, our experiments withseveral models, modalities and tasks show that task arithmetic is a simple,efficient and effective way of editing models.", "output": "Editing Models with Task Arithmetic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Natural language explanations promise to offer intuitively understandableexplanations of a neural network's decision process in complex vision-languagetasks, as pursued in recent VL-NLE models. While current models offerimpressive performance on task accuracy and explanation plausibility, theysuffer from a range of issues: Some models feature a modular design where theexplanation generation module is poorly integrated with a separate module fortask-answer prediction, employ backbone models trained on limited sets oftasks, or incorporate ad hoc solutions to increase performance on singledatasets. We propose to evade these limitations by applying recent advances inlarge-scale multi-task pretraining of generative Transformer models to theproblem of VL-NLE tasks. Our approach outperforms recent models by a largemargin, with human annotators preferring the generated explanations over theground truth in two out of three evaluated datasets. As a novel challenge inVL-NLE research, we propose the problem of multi-task VL-NLE and show thatjointly training on multiple tasks can increase the explanation quality. Wediscuss the ethical implications of high-quality NLE generation and otherissues in recent VL-NLE research.", "output": "Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose GazeNeRF, a 3D-aware method for the task of gaze redirection.Existing gaze redirection methods operate on 2D images and struggle to generate3D consistent results. Instead, we build on the intuition that the face regionand eyeballs are separate 3D structures that move in a coordinated yetindependent fashion. Our method leverages recent advancements in conditionalimage-based neural radiance fields and proposes a two-stream architecture thatpredicts volumetric features for the face and eye regions separately. Rigidlytransforming the eye features via a 3D rotation matrix provides fine-grainedcontrol over the desired gaze angle. The final, redirected image is thenattained via differentiable volume compositing. Our experiments show that thisarchitecture outperforms naively conditioned NeRF baselines as well as previousstate-of-the-art 2D gaze redirection methods in terms of redirection accuracyand identity preservation.", "output": "GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While deep learning-based methods for blind face restoration have achievedunprecedented success, they still suffer from two major limitations. First,most of them deteriorate when facing complex degradations out of their trainingdata. Second, these methods require multiple constraints, e.g., fidelity,perceptual, and adversarial losses, which require laborious hyper-parametertuning to stabilize and balance their influences. In this work, we propose anovel method named DifFace that is capable of coping with unseen and complexdegradations more gracefully without complicated loss designs. The key of ourmethod is to establish a posterior distribution from the observed low-quality(LQ) image to its high-quality (HQ) counterpart. In particular, we design atransition distribution from the LQ image to the intermediate state of apre-trained diffusion model and then gradually transmit from this intermediatestate to the HQ target by recursively applying a pre-trained diffusion model.The transition distribution only relies on a restoration backbone that istrained with $L_2$ loss on some synthetic data, which favorably avoids thecumbersome training process in existing methods. Moreover, the transitiondistribution can contract the error of the restoration backbone and thus makesour method more robust to unknown degradations. Comprehensive experiments showthat DifFace is superior to current state-of-the-art methods, especially incases with severe degradations. Code and model are available at", "output": "DifFace: Blind Face Restoration with Diffused Error Contraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in self-supervised visual representation learning have pavedthe way for unsupervised methods tackling tasks such as object discovery andinstance segmentation. However, discovering objects in an image with nosupervision is a very hard task; what are the desired objects, when to separatethem into parts, how many are there, and of what classes? The answers to thesequestions depend on the tasks and datasets of evaluation. In this work, we takea different approach and propose to look for the background instead. This way,the salient objects emerge as a by-product without any strong assumption onwhat an object should be. We propose FOUND, a simple model made of a single$conv1times1$ initialized with coarse background masks extracted fromself-supervised patch-based representations. After fast training and refiningthese seed masks, the model reaches state-of-the-art results on unsupervisedsaliency detection and object discovery benchmarks. Moreover, we show that ourapproach yields good results in the unsupervised semantic segmentationretrieval task. The code to reproduce our results is available at", "output": "Unsupervised Object Localization: Observing the Background to Discover Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale neural network models combining text and images have madeincredible progress in recent years. However, it remains an open question towhat extent such models encode compositional representations of the conceptsover which they operate, such as correctly identifying ''red cube'' byreasoning over the constituents ''red'' and ''cube''. In this work, we focus onthe ability of a large pretrained vision and language model (CLIP) to encodecompositional concepts and to bind variables in a structure-sensitive way(e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). Inorder to inspect the performance of CLIP, we compare several architectures fromresearch on compositional distributional semantics models (CDSMs), a line ofresearch that attempts to implement traditional compositional linguisticstructures within embedding spaces. We find that CLIP can compose concepts in asingle-object setting, but in situations where concept binding is needed,performance drops dramatically. At the same time, CDSMs also perform poorly,with best performance at chance level.", "output": "Does CLIP Bind Concepts? Probing Compositionality in Large Image Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a simple and effective visual prompting method foradapting pre-trained models to downstream recognition tasks. Our methodincludes two key designs. First, rather than directly adding together theprompt and the image, we treat the prompt as an extra and independent learnablecomponent. We show that the strategy of reconciling the prompt and the imagematters, and find that warping the prompt around a properly shrinked imageempirically works the best. Second, we re-introduce two \"old tricks\" commonlyused in building transferable adversarial examples, i.e., input diversity andgradient normalization, into visual prompting. These techniques improveoptimization and enable the prompt to generalize better. We provide extensiveexperimental results to demonstrate the effectiveness of our method. Using aCLIP model, our prompting method sets a new record of 82.8% average accuracyacross 12 popular classification datasets, substantially surpassing the priorart by +5.6%. It is worth noting that this prompting performance alreadyoutperforms linear probing by +2.1% and can even match fully fine-tuning incertain datasets. In addition, our prompting method shows competitiveperformance across different data scales and against distribution shifts. Thecode is publicly available at ", "output": "Unleashing the Power of Visual Prompting At the Pixel Level."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uniform-precision neural network quantization has gained popularity since itsimplifies densely packed arithmetic unit for high computing capability.However, it ignores heterogeneous sensitivity to the impact of quantizationerrors across the layers, resulting in sub-optimal inference accuracy. Thiswork proposes a novel neural architecture search called neural channelexpansion that adjusts the network structure to alleviate accuracy degradationfrom ultra-low uniform-precision quantization. The proposed method selectivelyexpands channels for the quantization sensitive layers while satisfyinghardware constraints (e.g., FLOPs, PARAMs). Based on in-depth analysis andexperiments, we demonstrate that the proposed method can adapt several popularnetworks channels to achieve superior 2-bit quantization accuracy on CIFAR10and ImageNet. In particular, we achieve the best-to-date Top-1/Top-5 accuracyfor 2-bit ResNet50 with smaller FLOPs and the parameter size.", "output": "Automatic Network Adaptation for Ultra-Low Uniform-Precision Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open vocabulary object detection has been greatly advanced by the recentdevelopment of vision-language pretrained model, which helps recognize novelobjects with only semantic categories. The prior works mainly focus onknowledge transferring to the object proposal classification and employclass-agnostic box and mask prediction. In this work, we propose CondHead, aprincipled dynamic network design to better generalize the box regression andmask segmentation for open vocabulary setting. The core idea is toconditionally parameterize the network heads on semantic embedding and thus themodel is guided with class-specific knowledge to better detect novelcategories. Specifically, CondHead is composed of two streams of network heads,the dynamically aggregated head and the dynamically generated head. The formeris instantiated with a set of static heads that are conditionally aggregated,these heads are optimized as experts and are expected to learn sophisticatedprediction. The latter is instantiated with dynamically generated parametersand encodes general class-specific information. With such a conditional design,the detection model is bridged by the semantic embedding to offer stronglygeneralizable class-wise box and mask prediction. Our method brings significantimprovement to the state-of-the-art open vocabulary object detection methodswith very minor overhead, e.g., it surpasses a RegionClip model by 3.0detection AP on novel categories, with only 1.1% more computation.", "output": "Learning to Detect and Segment for Open Vocabulary Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern mobile burst photography pipelines capture and merge a short sequenceof frames to recover an enhanced image, but often disregard the 3D nature ofthe scene they capture, treating pixel motion between images as a 2Daggregation problem. We show that in a ''long-burst'', forty-two 12-megapixelRAW frames captured in a two-second sequence, there is enough parallaxinformation from natural hand tremor alone to recover high-quality scene depth.To this end, we devise a test-time optimization approach that fits a neuralRGB-D representation to long-burst data and simultaneously estimates scenedepth and camera motion. Our plane plus depth model is trained end-to-end, andperforms coarse-to-fine refinement by controlling which multi-resolution volumefeatures the network has access to at what time during training. We validatethe method experimentally, and demonstrate geometrically accurate depthreconstructions with no additional hardware or separate data pre-processing andpose-estimation steps.", "output": "Shakes on a Plane: Unsupervised Depth Estimation from Unstabilized Photography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion probabilistic models have recently achieved remarkable success ingenerating high quality image and video data. In this work, we build on thisclass of generative models and introduce a method for lossy compression of highresolution images. The resulting codec, which we call DIffuson-based ResidualAugmentation Codec (DIRAC), is the first neural codec to allow smooth traversalof the rate-distortion-perception tradeoff at test time, while obtainingcompetitive performance with GAN-based methods in perceptual quality.Furthermore, while sampling from diffusion probabilistic models is notoriouslyexpensive, we show that in the compression setting the number of steps can bedrastically reduced.", "output": "A Residual Diffusion Model for High Perceptual Quality Codec Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "IoU losses are surrogates that directly optimize the Jaccard index. Insemantic segmentation, leveraging IoU losses as part of the loss function isshown to perform better with respect to the Jaccard index measure thanoptimizing pixel-wise losses such as the cross-entropy loss alone. The mostnotable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss.However, these losses are incompatible with soft labels which are ubiquitous inmachine learning. In this paper, we propose Jaccard metric losses (JMLs), whichare identical to the soft Jaccard loss in a standard setting with hard labels,but are compatible with soft labels. With JMLs, we study two of the mostpopular use cases of soft labels: label smoothing and knowledge distillation.With a variety of architectures, our experiments show significant improvementsover the cross-entropy loss on three semantic segmentation datasets(Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approachoutperforms state-of-the-art knowledge distillation methods by a large margin.Code is available at:href{", "output": "Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal interpolation often plays a crucial role to learn meaningfulrepresentations in dynamic scenes. In this paper, we propose a novel method totrain spatiotemporal neural radiance fields of dynamic scenes based on temporalinterpolation of feature vectors. Two feature interpolation methods aresuggested depending on underlying representations, neural networks or grids. Inthe neural representation, we extract features from space-time inputs viamultiple neural network modules and interpolate them based on time frames. Theproposed multi-level feature interpolation network effectively capturesfeatures of both short-term and long-term time ranges. In the gridrepresentation, space-time features are learned via four-dimensional hashgrids, which remarkably reduces training time. The grid representation showsmore than 100 times faster training speed than the previous neural-net-basedmethods while maintaining the rendering quality. Concatenating static anddynamic features and adding a simple smoothness term further improve theperformance of our proposed models. Despite the simplicity of the modelarchitectures, our method achieved state-of-the-art performance both inrendering quality for the neural representation and in training speed for thegrid representation.", "output": "Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Selective experience replay is a popular strategy for integrating lifelonglearning with deep reinforcement learning. Selective experience replay aims torecount selected experiences from previous tasks to avoid catastrophicforgetting. Furthermore, selective experience replay based techniques are modelagnostic and allow experiences to be shared across different models. However,storing experiences from all previous tasks make lifelong learning usingselective experience replay computationally very expensive and impractical asthe number of tasks increase. To that end, we propose a rewarddistribution-preserving coreset compression technique for compressingexperience replay buffers stored for selective experience replay.We evaluated the coreset compression technique on the brain tumorsegmentation (BRATS) dataset for the task of ventricle localization and on thewhole-body MRI for localization of left knee cap, left kidney, righttrochanter, left lung, and spleen. The coreset lifelong learning models trainedon a sequence of 10 different brain MR imaging environments demonstratedexcellent performance localizing the ventricle with a mean pixel error distanceof 12.93 for the compression ratio of 10x. In comparison, the conventionallifelong learning model localized the ventricle with a mean pixel distance of10.87. Similarly, the coreset lifelong learning models trained on whole-bodyMRI demonstrated no significant difference (p=0.28) between the 10x compressedcoreset lifelong learning models and conventional lifelong learning models forall the landmarks. The mean pixel distance for the 10x compressed models acrossall the landmarks was 25.30, compared to 19.24 for the conventional lifelonglearning models. Our results demonstrate that the potential of thecoreset-based ERB compression method for compressing experiences without asignificant drop in performance.", "output": "Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although an object may appear in numerous contexts, we often describe it in alimited number of ways. Language allows us to abstract away visual variation torepresent and communicate concepts. Building on this intuition, we propose analternative approach to visual representation learning: using languagesimilarity to sample semantically similar image pairs for contrastive learning.Our approach diverges from image-based contrastive learning by sampling viewpairs using language similarity instead of hand-crafted augmentations orlearned clusters. Our approach also differs from image-text contrastivelearning by relying on pre-trained language models to guide the learning ratherthan directly minimizing a cross-modal loss. Through a series of experiments,we show that language-guided learning yields better features than image-basedand image-text representation learning approaches.", "output": "Learning Visual Representations via Language-Guided Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a novel framework, named Global-Local CorrespondenceFramework (GLCF), for visual anomaly detection with logical constraints. Visualanomaly detection has become an active research area in various real-worldapplications, such as industrial anomaly detection and medical diseasediagnosis. However, most existing methods focus on identifying local structuraldegeneration anomalies and often fail to detect high-level functional anomaliesthat involve logical constraints. To address this issue, we propose atwo-branch approach that consists of a local branch for detecting structuralanomalies and a global branch for detecting logical anomalies. To facilitatelocal-global feature correspondence, we introduce a novel semantic bottleneckenabled by the visual Transformer. Moreover, we develop feature estimationnetworks for each branch separately to detect anomalies. Our proposed frameworkis validated using various benchmarks, including industrial datasets, Mvtec AD,Mvtec Loco AD, and the Retinal-OCT medical dataset. Experimental results showthat our method outperforms existing methods, particularly in detecting logicalanomalies.", "output": "Learning Global-Local Correspondence with Semantic Bottleneck for Logical Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we present I$^2$-SDF, a new method for intrinsic indoor scenereconstruction and editing using differentiable Monte Carlo raytracing onneural signed distance fields (SDFs). Our holistic neural SDF-based frameworkjointly recovers the underlying shapes, incident radiance and materials frommulti-view images. We introduce a novel bubble loss for fine-grained smallobjects and error-guided adaptive sampling scheme to largely improve thereconstruction quality on large-scale indoor scenes. Further, we propose todecompose the neural radiance field into spatially-varying material of thescene as a neural field through surface-based, differentiable Monte Carloraytracing and emitter semantic segmentations, which enables physically basedand photorealistic scene relighting and editing applications. Through a numberof qualitative and quantitative experiments, we demonstrate the superiorquality of our method on indoor scene reconstruction, novel view synthesis, andscene editing compared to state-of-the-art baselines.", "output": "I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image-based table recognition is a challenging task due to the diversity oftable styles and the complexity of table structures. Most of the previousmethods focus on a non-end-to-end approach which divides the problem into twoseparate sub-problems: table structure recognition; and cell-contentrecognition and then attempts to solve each sub-problem independently using twoseparate systems. In this paper, we propose an end-to-end multi-task learningmodel for image-based table recognition. The proposed model consists of oneshared encoder, one shared decoder, and three separate decoders which are usedfor learning three sub-tasks of table recognition: table structure recognition,cell detection, and cell-content recognition. The whole system can be easilytrained and inferred in an end-to-end approach. In the experiments, we evaluatethe performance of the proposed model on two large-scale datasets: FinTabNetand PubTabNet. The experiment results show that the proposed model outperformsthe state-of-the-art methods in all benchmark datasets.", "output": "An End-to-End Multi-Task Learning Model for Image-based Table Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image-to-image reconstruction problems with free or inexpensive metadata inthe form of class labels appear often in biological and medical image domains.Existing text-guided or style-transfer image-to-image approaches do nottranslate to datasets where additional information is provided as discreteclasses. We introduce and implement a model which combines image-to-image andclass-guided denoising diffusion probabilistic models. We train our model on areal-world dataset of microscopy images used for drug discovery, with andwithout incorporating metadata labels. By exploring the properties ofimage-to-image diffusion with relevant labels, we show that class-guidedimage-to-image diffusion can improve the meaningful content of thereconstructed images and outperform the unguided model in useful downstreamtasks.", "output": "Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep implicit functions (DIFs) have emerged as a powerful paradigm for manycomputer vision tasks such as 3D shape reconstruction, generation,registration, completion, editing, and understanding. However, given a set of3D shapes with associated covariates there is at present no shaperepresentation method which allows to precisely represent the shapes whilecapturing the individual dependencies on each covariate. Such a method would beof high utility to researchers to discover knowledge hidden in a population ofshapes. We propose a 3D Neural Additive Model for Interpretable ShapeRepresentation (NAISR) which describes individual shapes by deforming a shapeatlas in accordance to the effect of disentangled covariates. Our approachcaptures shape population trends and allows for patient-specific predictionsthrough shape transfer. NAISR is the first approach to combine the benefits ofdeep implicit shape representations with an atlas deforming according tospecified covariates. Although our driving problem is the construction of anairway atlas, NAISR is a general approach for modeling, representing, andinvestigating shape populations. We evaluate NAISR with respect to shapereconstruction, shape disentanglement, shape evolution, and shape transfer forthe pediatric upper airway. Our experiments demonstrate that NAISR achievescompetitive shape reconstruction performance while retaining interpretability.", "output": "NAISR: A 3D Neural Additive Model for Interpretable Shape Representation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The success of deep learning models has led to their adaptation and adoptionby prominent video understanding methods. The majority of these approachesencode features in a joint space-time modality for which the inner workings andlearned representations are difficult to visually interpret. We propose LEArnedPreconscious Synthesis (LEAPS), an architecture-agnostic method forsynthesizing videos from the internal spatiotemporal representations of models.Using a stimulus video and a target class, we prime a fixed space-time modeland iteratively optimize a video initialized with random noise. We incorporateadditional regularizers to improve the feature diversity of the synthesizedvideos as well as the cross-frame temporal coherence of motions. Wequantitatively and qualitatively evaluate the applicability of LEAPS byinverting a range of spatiotemporal convolutional and attention-basedarchitectures trained on Kinetics-400, which to the best of our knowledge hasnot been previously accomplished.", "output": "Leaping Into Memories: Space-Time Deep Feature Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inversion by Direct Iteration (InDI) is a new formulation for supervisedimage restoration that avoids the so-called ``regression to the mean'' effectand produces more realistic and detailed images than existing regression-basedmethods. It does this by gradually improving image quality in small steps,similar to generative denoising diffusion models.Image restoration is an ill-posed problem where multiple high-quality imagesare plausible reconstructions of a given low-quality input. Therefore, theoutcome of a single step regression model is typically an aggregate of allpossible explanations, therefore lacking details and realism. The mainadvantage of InDI is that it does not try to predict the clean target image ina single step but instead gradually improves the image in small steps,resulting in better perceptual quality.While generative denoising diffusion models also work in small steps, ourformulation is distinct in that it does not require knowledge of any analyticform of the degradation process. Instead, we directly learn an iterativerestoration process from low-quality and high-quality paired examples. InDI canbe applied to virtually any image degradation, given paired training data. Inconditional denoising diffusion image restoration the denoising networkgenerates the restored image by repeatedly denoising an initial image of purenoise, conditioned on the degraded input. Contrary to conditional denoisingformulations, InDI directly proceeds by iteratively restoring the inputlow-quality image, producing high-quality results on a variety of imagerestoration tasks, including motion and out-of-focus deblurring,super-resolution, compression artifact removal, and denoising.", "output": "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Visual Information Extraction (VIE) has been becoming increasinglyimportant in both the academia and industry, due to the wide range ofreal-world applications. Previously, numerous works have been proposed totackle this problem. However, the benchmarks used to assess these methods arerelatively plain, i.e., scenarios with real-world complexity are not fullyrepresented in these benchmarks. As the first contribution of this work, wecurate and release a new dataset for VIE, in which the document images are muchmore challenging in that they are taken from real applications, anddifficulties such as blur, partial occlusion, and printing shift are quitecommon. All these factors may lead to failures in information extraction.Therefore, as the second contribution, we explore an alternative approach toprecisely and robustly extract key information from document images under suchtough conditions. Specifically, in contrast to previous methods, which usuallyeither incorporate visual information into a multi-modal architecture or traintext spotting and information extraction in an end-to-end fashion, weexplicitly model entities as semantic points, i.e., center points of entitiesare enriched with semantic information describing the attributes andrelationships of different entities, which could largely benefit entitylabeling and linking. Extensive experiments on standard benchmarks in thisfield as well as the proposed dataset demonstrate that the proposed method canachieve significantly enhanced performance on entity labeling and linking,compared with previous state-of-the-art models. Dataset is available at", "output": "Modeling Entities as Semantic Points for Visual Information Extraction in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human mesh recovery (HMR) provides rich human body information for variousreal-world applications such as gaming, human-computer interaction, and virtualreality. Compared to single image-based methods, video-based methods canutilize temporal information to further improve performance by incorporatinghuman body motion priors. However, many-to-many approaches such as VIBE sufferfrom motion smoothness and temporal inconsistency. While many-to-one approachessuch as TCMR and MPS-Net rely on the future frames, which is non-causal andtime inefficient during inference. To address these challenges, a novelDiffusion-Driven Transformer-based framework (DDT) for video-based HMR ispresented. DDT is designed to decode specific motion patterns from the inputsequence, enhancing motion smoothness and temporal consistency. As amany-to-many approach, the decoder of our DDT outputs the human mesh of all theframes, making DDT more viable for real-world applications where timeefficiency is crucial and a causal model is desired. Extensive experiments areconducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),which demonstrated the effectiveness and efficiency of our DDT.", "output": "DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural field-based 3D representations have recently been adopted in manyareas including SLAM systems. Current neural SLAM or online mapping systemslead to impressive results in the presence of simple captures, but they rely ona world-centric map representation as only a single neural field model is used.To define such a world-centric representation, accurate and static priorinformation about the scene, such as its boundaries and initial camera poses,are required. However, in real-time and on-the-fly scene capture applications,this prior knowledge cannot be assumed as fixed or static, since it dynamicallychanges and it is subject to significant updates based on run-timeobservations. Particularly in the context of large-scale mapping, significantcamera pose drift is inevitable, necessitating the correction via loop closure.To overcome this limitation, we propose NEWTON, a view-centric mapping methodthat dynamically constructs neural fields based on run-time observation. Incontrast to prior works, our method enables camera pose updates using loopclosures and scene boundary updates by representing the scene with multipleneural fields, where each is defined in a local coordinate system of a selectedkeyframe. The experimental results demonstrate the superior performance of ourmethod over existing world-centric neural field-based SLAM systems, inparticular for large-scale scenes subject to camera pose updates.", "output": "NEWTON: Neural View-Centric Mapping for On-the-Fly Large-Scale SLAM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Event cameras sense the intensity changes asynchronously and produce eventstreams with high dynamic range and low latency. This has inspired researchendeavors utilizing events to guide the challenging video superresolution (VSR)task. In this paper, we make the first attempt to address a novel problem ofachieving VSR at random scales by taking advantages of the high temporalresolution property of events. This is hampered by the difficulties ofrepresenting the spatial-temporal information of events when guiding VSR. Tothis end, we propose a novel framework that incorporates the spatial-temporalinterpolation of events to VSR in a unified framework. Our key idea is to learnimplicit neural representations from queried spatial-temporal coordinates andfeatures from both RGB frames and events. Our method contains three parts.Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3Dfeatures from events and RGB frames. Then, the Temporal Filter (TF) moduleunlocks more explicit motion information from the events near the queriedtimestamp and generates the 2D features. Lastly, the SpatialTemporal ImplicitRepresentation (STIR) module recovers the SR frame in arbitrary resolutionsfrom the outputs of these two modules. In addition, we collect a real-worlddataset with spatially aligned events and RGB frames. Extensive experimentsshow that our method significantly surpasses the prior-arts and achieves VSRwith random scales, e.g., 6.5. Code and dataset are available at ", "output": "Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated shape repair approaches currently lack access to datasets thatdescribe real-world damaged geometry. We present Fantastic Breaks (and Where toFind Them: adataset containing scanned, waterproofed, and cleaned 3D meshes for 150 brokenobjects, paired and geometrically aligned with complete counterparts. FantasticBreaks contains class and material labels, proxy repair parts that join tobroken meshes to generate complete meshes, and manually annotated fractureboundaries. Through a detailed analysis of fracture geometry, we revealdifferences between Fantastic Breaks and synthetic fracture datasets generatedusing geometric and physics-based methods. We show experimental shape repairevaluation with Fantastic Breaks using multiple learning-based approachespre-trained with synthetic datasets and re-trained with subset of FantasticBreaks.", "output": "Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Any-scale image synthesis offers an efficient and scalable solution tosynthesize photo-realistic images at any scale, even going beyond 2Kresolution. However, existing GAN-based solutions depend excessively onconvolutions and a hierarchical architecture, which introduce inconsistency andthe $``$texture sticking$\"$ issue when scaling the output resolution. Fromanother perspective, INR-based generators are scale-equivariant by design, buttheir huge memory footprint and slow inference hinder these networks from beingadopted in large-scale or real-time systems. In this work, we propose$textbf{C}$olumn-$textbf{R}$ow $textbf{E}$ntangled $textbf{P}$ixel$textbf{S}$ynthesis ($textbf{CREPS}$), a new generative model that is bothefficient and scale-equivariant without using any spatial convolutions orcoarse-to-fine design. To save memory footprint and make the system scalable,we employ a novel bi-line representation that decomposes layer-wise featuremaps into separate $``$thick$\"$ column and row encodings. Experiments onvarious datasets, including FFHQ, LSUN-Church, MetFaces, and Flickr-Scenery,confirm CREPS' ability to synthesize scale-consistent and alias-free images atany arbitrary resolution with proper training and inference speed. Code isavailable at ", "output": "Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the use of machine learning continues to expand, the importance ofensuring its safety cannot be overstated. A key concern in this regard is theability to identify whether a given sample is from the training distribution,or is an \"Out-Of-Distribution\" (OOD) sample. In addition, adversaries canmanipulate OOD samples in ways that lead a classifier to make a confidentprediction. In this study, we present a novel approach for certifying therobustness of OOD detection within a $ell_2$-norm around the input, regardlessof network architecture and without the need for specific components oradditional training. Further, we improve current techniques for detectingadversarial attacks on OOD samples, while providing high levels of certifiedand adversarial robustness on in-distribution samples. The average of all OODdetection metrics on CIFAR10/100 shows an increase of $sim 13 % / 5%$relative to previous approaches.", "output": "Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Microscopy images often suffer from high levels of noise, which can hinderfurther analysis and interpretation. Content-aware image restoration (CARE)methods have been proposed to address this issue, but they often require largeamounts of training data and suffer from over-fitting. To overcome thesechallenges, we propose a novel framework for few-shot microscopy imagedenoising. Our approach combines a generative adversarial network (GAN) trainedvia contrastive learning (CL) with two structure preserving loss terms(Structural Similarity Index and Total Variation loss) to further improve thequality of the denoised images using little data. We demonstrate theeffectiveness of our method on three well-known microscopy imaging datasets,and show that we can drastically reduce the amount of training data whileretaining the quality of the denoising, thus alleviating the burden ofacquiring paired data and enabling few-shot learning. The proposed frameworkcan be easily extended to other image restoration tasks and has the potentialto significantly advance the field of microscopy image analysis.", "output": "Generalizable Denoising of Microscopy Images using Generative Adversarial Networks and Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Standard deep learning models such as convolutional neural networks (CNNs)lack the ability of generalizing to domains which have not been seen duringtraining. This problem is mainly due to the common but often wrong assumptionof such models that the source and target data come from the same i.i.d.distribution. Recently, Vision Transformers (ViTs) have shown outstandingperformance for a broad range of computer vision tasks. However, very fewstudies have investigated their ability to generalize to new domains. Thispaper presents a first Token-level Feature Stylization (TFS-ViT) approach fordomain generalization, which improves the performance of ViTs to unseen data bysynthesizing new domains. Our approach transforms token features by mixing thenormalization statistics of images from different domains. We further improvethis approach with a novel strategy for attention-aware stylization, which usesthe attention maps of class (CLS) tokens to compute and mix normalizationstatistics of tokens corresponding to different image regions. The proposedmethod is flexible to the choice of backbone model and can be easily applied toany ViT-based architecture with a negligible increase in computationalcomplexity. Comprehensive experiments show that our approach is able to achievestate-of-the-art performance on five challenging benchmarks for domaingeneralization, and demonstrate its ability to deal with different types ofdomain shifts. The implementation is available at:", "output": "TFS-ViT: Token-Level Feature Stylization for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human-Object Interaction (HOI) detection aims to localize human-object pairsand recognize their interactions. Recently, Contrastive Language-ImagePre-training (CLIP) has shown great potential in providing interaction priorfor HOI detectors via knowledge distillation. However, such approaches oftenrely on large-scale training data and suffer from inferior performance underfew/zero-shot scenarios. In this paper, we propose a novel HOI detectionframework that efficiently extracts prior knowledge from CLIP and achievesbetter generalization. In detail, we first introduce a novel interactiondecoder to extract informative regions in the visual feature map of CLIP via across-attention mechanism, which is then fused with the detection backbone by aknowledge integration block for more accurate human-object pair detection. Inaddition, prior knowledge in CLIP text encoder is leveraged to generate aclassifier by embedding HOI descriptions. To distinguish fine-grainedinteractions, we build a verb classifier from training data via visual semanticarithmetic and a lightweight verb representation adapter. Furthermore, wepropose a training-free enhancement to exploit global HOI predictions fromCLIP. Extensive experiments demonstrate that our method outperforms the stateof the art by a large margin on various settings, e.g. +4.04 mAP on HICO-Det.The source code is available in ", "output": "HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a novel method for depth completion, which leveragesmulti-view improved monitored distillation to generate more precise depth maps.Our approach builds upon the state-of-the-art ensemble distillation method, inwhich we introduce a stereo-based model as a teacher model to improve theaccuracy of the student model for depth completion. By minimizing thereconstruction error for a given image during ensemble distillation, we canavoid learning inherent error modes of completion-based teachers. To provideself-supervised information, we also employ multi-view depth consistency andmulti-scale minimum reprojection. These techniques utilize existing structuralconstraints to yield supervised signals for student model training, withoutrequiring costly ground truth depth information. Our extensive experimentalevaluation demonstrates that our proposed method significantly improves theaccuracy of the baseline monitored distillation method.", "output": "Enhancing Depth Completion with Multi-View Monitored Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic radiology report generation has attracted enormous researchinterest due to its practical value in reducing the workload of radiologists.However, simultaneously establishing global correspondences between the image(e.g., Chest X-ray) and its related report and local alignments between imagepatches and keywords remains challenging. To this end, we propose an Unify,Align and then Refine (UAR) approach to learn multi-level cross-modalalignments and introduce three novel modules: Latent Space Unifier (LSU),Cross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR).Specifically, LSU unifies multimodal data into discrete tokens, making itflexible to learn common knowledge among modalities with a shared network. Themodality-agnostic CRA learns discriminative features via a set of orthonormalbasis and a dual-gate mechanism first and then globally aligns visual andtextual representations under a triplet contrastive loss. TIR booststoken-level local alignment via calibrating text-to-image attention with alearnable mask. Additionally, we design a two-stage training procedure to makeUAR gradually grasp cross-modal alignments at different levels, which imitatesradiologists' workflow: writing sentence by sentence first and then checkingword by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXRbenchmark datasets demonstrate the superiority of our UAR against variedstate-of-the-art methods.", "output": "Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Discovering novel concepts from unlabelled data and in a continuous manner isan important desideratum of lifelong learners. In the literature such problemshave been partially addressed under very restricted settings, where eitheraccess to labelled data is provided for discovering novel concepts (e.g., NCD)or learning occurs for a limited number of incremental steps (e.g.,class-iNCD). In this work we challenge the status quo and propose a morechallenging and practical learning paradigm called MSc-iNCD, where learningoccurs continuously and unsupervisedly, while exploiting the rich priors fromlarge-scale pre-trained models. To this end, we propose simple baselines thatare not only resilient under longer learning scenarios, but are surprisinglystrong when compared with sophisticated state-of-the-art methods. We conductextensive empirical evaluation on a multitude of benchmarks and show theeffectiveness of our proposed baselines, which significantly raises the bar.", "output": "Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomaly detectors are widely used in industrial production to detect andlocalize unknown defects in query images. These detectors are trained onnominal images and have shown success in distinguishing anomalies from mostnormal samples. However, hard-nominal examples are scattered and far apart frommost normalities, they are often mistaken for anomalies by existing anomalydetectors. To address this problem, we propose a simple yet efficient method:textbf{H}ard Nominal textbf{E}xample-aware textbf{T}emplate textbf{M}utualtextbf{M}atching (HETMM). Specifically, textit{HETMM} aims to construct arobust prototype-based decision boundary, which can precisely distinguishbetween hard-nominal examples and anomalies, yielding fewer false-positive andmissed-detection rates. Moreover, textit{HETMM} mutually explores theanomalies in two directions between queries and the template set, and thus itis capable to capture the logical anomalies. This is a significant advantageover most anomaly detectors that frequently fail to detect logical anomalies.Additionally, to meet the speed-accuracy demands, we further proposetextbf{P}ixel-level textbf{T}emplate textbf{S}election (PTS) to streamlinethe original template set. textit{PTS} selects cluster centres andhard-nominal examples to form a tiny set, maintaining the original decisionboundaries. Comprehensive experiments on five real-world datasets demonstratethat our methods yield outperformance than existing advances under thereal-time inference speed. Furthermore, textit{HETMM} can be hot-updated byinserting novel samples, which may promptly address some incremental learningissues.", "output": "Hard Nominal Example-aware Template Mutual Matching for Industrial Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent wave of large-scale text-to-image diffusion models hasdramatically increased our text-based image generation abilities. These modelscan generate realistic images for a staggering variety of prompts and exhibitimpressive compositional generalization abilities. Almost all use cases thusfar have solely focused on sampling; however, diffusion models can also provideconditional density estimates, which are useful for tasks beyond imagegeneration. In this paper, we show that the density estimates from large-scaletext-to-image diffusion models like Stable Diffusion can be leveraged toperform zero-shot classification without any additional training. Ourgenerative approach to classification, which we call Diffusion Classifier,attains strong results on a variety of benchmarks and outperforms alternativemethods of extracting knowledge from diffusion models. Although a gap remainsbetween generative and discriminative approaches on zero-shot recognitiontasks, we find that our diffusion-based approach has stronger multimodalrelational reasoning abilities than competing discriminative approaches.Finally, we use Diffusion Classifier to extract standard classifiers fromclass-conditional diffusion models trained on ImageNet. Even though thesemodels are trained with weak augmentations and no regularization, they approachthe performance of SOTA discriminative classifiers. Overall, our results are astep toward using generative over discriminative models for downstream tasks.Results and visualizations at ", "output": "Your Diffusion Model is Secretly a Zero-Shot Classifier."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An estimated 253 million people have visual impairments. These visualimpairments affect everyday lives, and limit their understanding of the outsideworld. This can pose a risk to health from falling or collisions. We propose asolution to this through quick and detailed communication of environmentalspatial geometry through sound, providing the blind and visually impaired theability to understand their spatial environment through sound technology. Themodel consists of fast object detection and 3D environmental mapping, which iscommunicated through a series of quick sound notes. These sound notes are atdifferent frequencies, pitches, and arrangements in order to preciselycommunicate the depth and location of points within the environment. Sounds arecommunicated in the form of musical notes in order to be easily recognizableand distinguishable. A unique algorithm is used to segment objects, providingminimal accuracy loss and improvement from the normal O(n2 ) to O(n) (which issignificant, as N in point clouds can often be in the range of 105 ). Intesting, we achieved an R-value of 0.866 on detailed objects and an accuracy of87.5% on an outdoor scene at night with large amounts of noise. We also providea supplementary video demo of our system.", "output": "Help the Blind See: Assistance for the Visually Impaired through Augmented Acoustic Simulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Precise estimation of global orientation and location is critical to ensure acompelling outdoor Augmented Reality (AR) experience. We address the problem ofgeo-pose estimation by cross-view matching of query ground images to ageo-referenced aerial satellite image database. Recently, neural network-basedmethods have shown state-of-the-art performance in cross-view matching.However, most of the prior works focus only on location estimation, ignoringorientation, which cannot meet the requirements in outdoor AR applications. Wepropose a new transformer neural network-based model and a modified tripletranking loss for joint location and orientation estimation. Experiments onseveral benchmark cross-view geo-localization datasets show that our modelachieves state-of-the-art performance. Furthermore, we present an approach toextend the single image query-based geo-localization approach by utilizingtemporal information from a navigation pipeline for robust continuousgeo-localization. Experimentation on several large-scale real-world videosequences demonstrates that our approach enables high-precision and stable ARinsertion.", "output": "Cross-View Visual Geo-Localization for Outdoor Augmented Reality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperspectral imaging acquires data in both the spatial and frequency domainsto offer abundant physical or biological information. However, conventionalhyperspectral imaging has intrinsic limitations of bulky instruments, slow dataacquisition rate, and spatiospectral tradeoff. Here we introduce hyperspectrallearning for snapshot hyperspectral imaging in which sampled hyperspectral datain a small subarea are incorporated into a learning algorithm to recover thehypercube. Hyperspectral learning exploits the idea that a photograph is morethan merely a picture and contains detailed spectral information. A smallsampling of hyperspectral data enables spectrally informed learning to recovera hypercube from an RGB image. Hyperspectral learning is capable of recoveringfull spectroscopic resolution in the hypercube, comparable to high spectralresolutions of scientific spectrometers. Hyperspectral learning also enablesultrafast dynamic imaging, leveraging ultraslow video recording in anoff-the-shelf smartphone, given that a video comprises a time series ofmultiple RGB images. To demonstrate its versatility, an experimental model ofvascular development is used to extract hemodynamic parameters via statisticaland deep-learning approaches. Subsequently, the hemodynamics of peripheralmicrocirculation is assessed at an ultrafast temporal resolution up to amillisecond, using a conventional smartphone camera. This spectrally informedlearning method is analogous to compressed sensing; however, it further allowsfor reliable hypercube recovery and key feature extractions with a transparentlearning algorithm. This learning-powered snapshot hyperspectral imaging methodyields high spectral and temporal resolutions and eliminates the spatiospectraltradeoff, offering simple hardware requirements and potential applications ofvarious machine-learning techniques.", "output": "mHealth hyperspectral learning for instantaneous spatiospectral imaging of hemodynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We show how any PAC learning algorithm that works under the uniformdistribution can be transformed, in a blackbox fashion, into one that worksunder an arbitrary and unknown distribution $mathcal{D}$. The efficiency ofour transformation scales with the inherent complexity of $mathcal{D}$,running in $mathrm{poly}(n, (md)^d)$ time for distributions over ${pm 1}^n$whose pmfs are computed by depth-$d$ decision trees, where $m$ is the samplecomplexity of the original algorithm. For monotone distributions ourtransformation uses only samples from $mathcal{D}$, and for general ones ituses subcube conditioning samples.A key technical ingredient is an algorithm which, given the aforementionedaccess to $mathcal{D}$, produces an optimal decision tree decomposition of$mathcal{D}$: an approximation of $mathcal{D}$ as a mixture of uniformdistributions over disjoint subcubes. With this decomposition in hand, we runthe uniform-distribution learner on each subcube and combine the hypothesesusing the decision tree. This algorithmic decomposition lemma also yields newalgorithms for learning decision tree distributions with runtimes thatexponentially improve on the prior state of the art -- results of independentinterest in distribution learning.", "output": "Lifting uniform learners via distributional decomposition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As protein therapeutics play an important role in almost all medical fields,numerous studies have been conducted on proteins using artificial intelligence.Artificial intelligence has enabled data driven predictions without the needfor expensive experiments. Nevertheless, unlike the various molecularfingerprint algorithms that have been developed, protein fingerprint algorithmshave rarely been studied. In this study, we proposed the amino acid molecularfingerprints repurposing based protein (AmorProt) fingerprint, a proteinsequence representation method that effectively uses the molecular fingerprintscorresponding to 20 amino acids. Subsequently, the performances of the treebased machine learning and artificial neural network models were compared using(1) amyloid classification and (2) isoelectric point regression. Finally, theapplicability and advantages of the developed platform were demonstratedthrough a case study and the following experiments: (3) comparison of datasetdependence with feature based methods; (4) feature importance analysis; and (5)protein space analysis. Consequently, the significantly improved modelperformance and data set independent versatility of the AmorProt fingerprintwere verified. The results revealed that the current protein representationmethod can be applied to various fields related to proteins, such as predictingtheir fundamental properties or interaction with ligands.", "output": "AmorProt: Amino Acid Molecular Fingerprints Repurposing based Protein Fingerprint."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep ensemble is a simple and straightforward approach for approximatingBayesian inference and has been successfully applied to many classificationtasks. This study aims to comprehensively investigate this approach in themulti-output regression task to predict the aerodynamic performance of amissile configuration. By scrutinizing the effect of the number of neuralnetworks used in the ensemble, an obvious trend toward underconfidence inestimated uncertainty is observed. In this context, we propose the deepensemble framework that applies the post-hoc calibration method, and itsimproved uncertainty quantification performance is demonstrated. It is comparedwith Gaussian process regression, the most prevalent model for uncertaintyquantification in engineering, and is proven to have superior performance interms of regression accuracy, reliability of estimated uncertainty, andtraining efficiency. Finally, the impact of the suggested framework on theresults of Bayesian optimization is examined, showing that whether or not thedeep ensemble is calibrated can result in completely different explorationcharacteristics. This framework can be seamlessly applied and extended to anyregression task, as no special assumptions have been made for the specificproblem used in this study.", "output": "Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The paper discusses the limitations of deep learning models in identifyingand utilizing features that remain invariant under a bijective transformationon the data entries, which we refer to as combinatorial patterns. We argue thatthe identification of such patterns may be important for certain applicationsand suggest providing neural networks with information that fully describes thecombinatorial patterns of input entries and allows the network to determinewhat is relevant for prediction. To demonstrate the feasibility of thisapproach, we present a combinatorial convolutional neural network for wordclassification.", "output": "Combinatorial Convolutional Neural Networks for Words."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The network pruning algorithm based on evolutionary multi-objective (EMO) canbalance the pruning rate and performance of the network. However, itspopulation-based nature often suffers from the complex pruning optimizationspace and the highly resource-consuming pruning structure verification process,which limits its application. To this end, this paper proposes an EMO jointpruning with multiple sub-networks (EMO-PMS) to reduce space complexity andresource consumption. First, a divide-and-conquer EMO network pruning frameworkis proposed, which decomposes the complex EMO pruning task on the whole networkinto easier sub-tasks on multiple sub-networks. On the one hand, thisdecomposition reduces the pruning optimization space and decreases theoptimization difficulty; on the other hand, the smaller network structureconverges faster, so the computational resource consumption of the proposedalgorithm is lower. Secondly, a sub-network training method based oncross-network constraints is designed so that the sub-network can process thefeatures generated by the previous one through feature constraints. This methodallows sub-networks optimized independently to collaborate better and improvesthe overall performance of the pruned network. Finally, a multiple sub-networksjoint pruning method based on EMO is proposed. For one thing, it can accuratelymeasure the feature processing capability of the sub-networks with thepre-trained feature selector. For another, it can combine multi-objectivepruning results on multiple sub-networks through global performance impairmentranking to design a joint pruning scheme. The proposed algorithm is validatedon three datasets with different challenging. Compared with fifteen advancedpruning algorithms, the experiment results exhibit the effectiveness andefficiency of the proposed algorithm.", "output": "An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks have revolutionized many aspects of society but in the era ofhuge models with billions of parameters, optimizing and deploying them forcommercial applications can require significant computational and financialresources. To address these challenges, we introduce the Tetra-AML toolbox,which automates neural architecture search and hyperparameter optimization viaa custom-developed black-box Tensor train Optimization algorithm, TetraOpt. Thetoolbox also provides model compression through quantization and pruning,augmented by compression using tensor networks. Here, we analyze a unifiedbenchmark for optimizing neural networks in computer vision tasks and show thesuperior performance of our approach compared to Bayesian optimization on theCIFAR-10 dataset. We also demonstrate the compression of ResNet-18 neuralnetworks, where we use 14.5 times less memory while losing just 3.2% ofaccuracy. The presented framework is generic, not limited by computer visionproblems, supports hardware acceleration (such as with GPUs and TPUs) and canbe further extended to quantum hardware and to hybrid quantum machine learningmodels.", "output": "Tetra-AML: Automatic Machine Learning via Tensor Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As we enter the exascale computing era, efficiently utilizing power andoptimizing the performance of scientific applications under power and energyconstraints has become critical and challenging. We propose a low-overheadautotuning framework to autotune performance and energy for various hybridMPI/OpenMP scientific applications at large scales and to explore the tradeoffsbetween application runtime and power/energy for energy efficient applicationexecution, then use this framework to autotune four ECP proxy applications --XSBench, AMG, SWFFT, and SW4lite. Our approach uses Bayesian optimization witha Random Forest surrogate model to effectively search parameter spaces with upto 6 million different configurations on two large-scale production systems,Theta at Argonne National Laboratory and Summit at Oak Ridge NationalLaboratory. The experimental results show that our autotuning framework atlarge scales has low overhead and achieves good scalability. Using the proposedautotuning framework to identify the best configurations, we achieve up to91.59% performance improvement, up to 21.2% energy savings, and up to 37.84%EDP improvement on up to 4,096 nodes.", "output": "ytopt: Autotuning Scientific Applications for Energy Efficiency at Large Scales."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classical results in neural network approximation theory show how arbitrarycontinuous functions can be approximated by networks with a single hiddenlayer, under mild assumptions on the activation function. However, theclassical theory does not give a constructive means to generate the networkparameters that achieve a desired accuracy. Recent results have demonstratedthat for specialized activation functions, such as ReLUs and some classes ofanalytic functions, high accuracy can be achieved via linear combinations ofrandomly initialized activations. These recent works utilize specializedintegral representations of target functions that depend on the specificactivation functions used. This paper defines mollified integralrepresentations, which provide a means to form integral representations oftarget functions using activations for which no direct integral representationis currently known. The new construction enables approximation guarantees forrandomly initialized networks for a variety of widely used activationfunctions.", "output": "Function Approximation with Randomly Initialized Neural Networks for Approximate Model Reference Adaptive Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Task-oriented dialog systems empower users to accomplish their goals byfacilitating intuitive and expressive natural language interactions.State-of-the-art approaches in task-oriented dialog systems formulate theproblem as a conditional sequence generation task and fine-tune pre-trainedcausal language models in the supervised setting. This requires labeledtraining data for each new domain or task, and acquiring such data isprohibitively laborious and expensive, thus making it a bottleneck for scalingsystems to a wide range of domains. To overcome this challenge, we introduce anovel Zero-Shot generalizable end-to-end Task-oriented Dialog system, ZS-ToD,that leverages domain schemas to allow for robust generalization to unseendomains and exploits effective summarization of the dialog history. We employGPT-2 as a backbone model and introduce a two-step training process where thegoal of the first step is to learn the general structure of the dialog data andthe second step optimizes the response generation as well as intermediateoutputs, such as dialog state and system actions. As opposed tostate-of-the-art systems that are trained to fulfill certain intents in thegiven domains and memorize task-specific conversational patterns, ZS-ToD learnsgeneric task-completion skills by comprehending domain semantics via domainschemas and generalizing to unseen domains seamlessly. We conduct an extensiveexperimental evaluation on SGD and SGD-X datasets that span up to 20 uniquedomains and ZS-ToD outperforms state-of-the-art systems on key metrics, with animprovement of +17% on joint goal accuracy and +5 on inform. Additionally, wepresent a detailed ablation study to demonstrate the effectiveness of theproposed components and training mechanism", "output": "Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An increasing part of energy is produced from renewable sources by a largenumber of small producers. The efficiency of these sources is volatile and, tosome extent, random, exacerbating the energy market balance problem. In manycountries, that balancing is performed on day-ahead (DA) energy markets. Inthis paper, we consider automated trading on a DA energy market by a mediumsize prosumer. We model this activity as a Markov Decision Process andformalize a framework in which a ready-to-use strategy can be optimized withreal-life data. We synthesize parametric trading strategies and optimize themwith an evolutionary algorithm. We also use state-of-the-art reinforcementlearning algorithms to optimize a black-box trading strategy fed with availableinformation from the environment that can impact future prices.", "output": "Reinforcement learning for optimization of energy trading strategy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-Supervised Learning can be more beneficial for the video domain comparedto images because of its higher annotation cost and dimensionality. Besides,any video understanding task requires reasoning over both spatial and temporaldimensions. In order to learn both the static and motion related features forthe semi-supervised action recognition task, existing methods rely on hardinput inductive biases like using two-modalities (RGB and Optical-flow) ortwo-stream of different playback rates. Instead of utilizing unlabeled videosthrough diverse input streams, we rely on self-supervised videorepresentations, particularly, we utilize temporally-invariant andtemporally-distinctive representations. We observe that these representationscomplement each other depending on the nature of the action. Based on thisobservation, we propose a student-teacher semi-supervised learning framework,TimeBalance, where we distill the knowledge from a temporally-invariant and atemporally-distinctive teacher. Depending on the nature of the unlabeled video,we dynamically combine the knowledge of these two teachers based on a noveltemporal similarity-based reweighting scheme. Our method achievesstate-of-the-art performance on three action recognition benchmarks: UCF101,HMDB51, and Kinetics400. Code: ", "output": "TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning is a popular collaborative learning approach that enablesclients to train a global model without sharing their local data. Verticalfederated learning (VFL) deals with scenarios in which the data on clients havedifferent feature spaces but share some overlapping samples. Existing VFLapproaches suffer from high communication costs and cannot deal efficientlywith limited overlapping samples commonly seen in the real world. We propose apractical vertical federated learning (VFL) framework called textbf{one-shotVFL} that can solve the communication bottleneck and the problem of limitedoverlapping samples simultaneously based on semi-supervised learning. We alsopropose textbf{few-shot VFL} to improve the accuracy further with just onemore communication round between the server and the clients. In our proposedframework, the clients only need to communicate with the server once or only afew times. We evaluate the proposed VFL framework on both image and tabulardatasets. Our methods can improve the accuracy by more than 46.5% and reducethe communication cost by more than 330$times$ compared with state-of-the-artVFL methods when evaluated on CIFAR-10. Our code will be made publiclyavailable at url{", "output": "Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wind farm modelling has been an area of rapidly increasing interest withnumerous analytical as well as computational-based approaches developed toextend the margins of wind farm efficiency and maximise power production. Inthis work, we present the novel ML framework WakeNet, which can reproducegeneralised 2D turbine wake velocity fields at hub-height over a wide range ofyaw angles, wind speeds and turbulence intensities (TIs), with a mean accuracyof 99.8% compared to the solution calculated using the state-of-the-art windfarm modelling software FLORIS. As the generation of sufficient high-fidelitydata for network training purposes can be cost-prohibitive, the utility ofmulti-fidelity transfer learning has also been investigated. Specifically, anetwork pre-trained on the low-fidelity Gaussian wake model is fine-tuned inorder to obtain accurate wake results for the mid-fidelity Curl wake model. Therobustness and overall performance of WakeNet on various wake steering controland layout optimisation scenarios has been validated through power-gainheatmaps, obtaining at least 90% of the power gained through optimisationperformed with FLORIS directly. We also demonstrate that when utilising theCurl model, WakeNet is able to provide similar power gains to FLORIS, twoorders of magnitude faster (e.g. 10 minutes vs 36 hours per optimisation case).The wake evaluation time of wakeNet when trained on a high-fidelity CFD datasetis expected to be similar, thus further increasing computational time gains.These promising results show that generalised wake modelling with ML tools canbe accurate enough to contribute towards active yaw and layout optimisation,while producing realistic optimised configurations at a fraction of thecomputational cost, hence making it feasible to perform real-time active yawcontrol as well as robust optimisation under uncertainty.", "output": "Accelerated wind farm yaw and layout optimisation with multi-fidelity deep transfer learning wake models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exploiting partial first-order information in a cyclic way is arguably themost natural strategy to obtain scalable first-order methods. However, despitetheir wide use in practice, cyclic schemes are far less understood from atheoretical perspective than their randomized counterparts. Motivated by arecent success in analyzing an extrapolated cyclic scheme for generalizedvariational inequalities, we propose an Accelerated Cyclic Coordinate DualAveraging with Extrapolation (A-CODER) method for composite convexoptimization, where the objective function can be expressed as the sum of asmooth convex function accessible via a gradient oracle and a convex, possiblynonsmooth, function accessible via a proximal oracle. We show that A-CODERattains the optimal convergence rate with improved dependence on the number ofblocks compared to prior work. Furthermore, for the setting where the smoothcomponent of the objective function is expressible in a finite sum form, weintroduce a variance-reduced variant of A-CODER, VR-A-CODER, withstate-of-the-art complexity guarantees. Finally, we demonstrate theeffectiveness of our algorithms through numerical experiments.", "output": "Accelerated Cyclic Coordinate Dual Averaging with Extrapolation for Composite Convex Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrary to Google Search's mission of delivering information from \"manyangles so you can form your own understanding of the world,\" we find thatGoogle and its most prominent returned results -- Wikipedia and YouTube, simplyreflect the narrow set of cultural stereotypes tied to the search language forcomplex topics like \"Buddhism,\" \"Liberalism,\" \"colonization,\" \"Iran\" and\"America.\" Simply stated, they present, to varying degrees, distinctinformation across the same search in different languages (we call it 'languagebias'). Instead of presenting a global picture of a complex topic, our onlinesearches turn us into the proverbial blind person touching a small portion ofan elephant, ignorant of the existence of other cultural perspectives. Thelanguage we use to search ends up as a cultural filter to promote ethnocentricviews, where a person evaluates other people or ideas based on their ownculture. We also find that language bias is deeply embedded in ChatGPT. As itis primarily trained on English language data, it presents the Anglo-Americanperspective as the normative view, reducing the complexity of a multifacetedissue to the single Anglo-American standard. In this paper, we present evidenceand analysis of language bias and discuss its larger social implications.Toward the end of the paper, we propose a potential framework of usingautomatic translation to leverage language bias and argue that the task ofpiecing together a genuine depiction of the elephant is a challenging andimportant endeavor that deserves a new area of research in NLP and requirescollaboration with scholars from the humanities to create ethically sound andsocially responsible technology together.", "output": "A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable AI (XAI) has established itself as an important component ofAI-driven interactive systems. With Augmented Reality (AR) becoming moreintegrated in daily lives, the role of XAI also becomes essential in AR becauseend-users will frequently interact with intelligent services. However, it isunclear how to design effective XAI experiences for AR. We propose XAIR, adesign framework that addresses \"when\", \"what\", and \"how\" to provideexplanations of AI output in AR. The framework was based on amulti-disciplinary literature review of XAI and HCI research, a large-scalesurvey probing 500+ end-users' preferences for AR-based explanations, and threeworkshops with 12 experts collecting their insights about XAI design in AR.XAIR's utility and effectiveness was verified via a study with 10 designers andanother study with 12 end-users. XAIR can provide guidelines for designers,inspiring them to identify new design opportunities and achieve effective XAIdesigns in AR.", "output": "XAIR: A Framework of Explainable AI in Augmented Reality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The soft Dice loss (SDL) has taken a pivotal role in many automatedsegmentation pipelines in the medical imaging community. Over the last years,some reasons behind its superior functioning have been uncovered and furtheroptimizations have been explored. However, there is currently no implementationthat supports its direct use in settings with soft labels. Hence, a synergybetween the use of SDL and research leveraging the use of soft labels, also inthe context of model calibration, is still missing. In this work, we introduceDice semimetric losses (DMLs), which (i) are by design identical to SDL in astandard setting with hard labels, but (ii) can be used in settings with softlabels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirmthe potential synergy of DMLs with soft labels (e.g. averaging, labelsmoothing, and knowledge distillation) over hard labels (e.g. majority votingand random selection). As a result, we obtain superior Dice scores and modelcalibration, which supports the wider adoption of DMLs in practice. Code isavailable athref{", "output": "Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Post-processing typically takes the outputs of a Numerical Weather Prediction(NWP) model and applies linear statistical techniques to produce improvelocalized forecasts, by including additional observations, or determiningsystematic errors at a finer scale. In this pilot study, we investigate thebenefits and challenges of using non-linear neural network (NN) based methodsto post-process multiple weather features -- temperature, moisture, wind,geopotential height, precipitable water -- at 30 vertical levels, globally andat lead times up to 7 days. We show that we can achieve accuracy improvementsof up to 12% (RMSE) in a field such as temperature at 850hPa for a 7 dayforecast. However, we recognize the need to strengthen foundational work onobjectively measuring a sharp and correct forecast. We discuss the challengesof using standard metrics such as root mean squared error (RMSE) or anomalycorrelation coefficient (ACC) as we move from linear statistical models to morecomplex non-linear machine learning approaches for post-processing globalweather forecasts.", "output": "A Machine Learning Outlook: Post-processing of Global Medium-range Forecasts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The literature on provable robustness in machine learning has primarilyfocused on static prediction problems, such as image classification, in whichinput samples are assumed to be independent and model performance is measuredas an expectation over the input distribution. Robustness certificates arederived for individual input instances with the assumption that the model isevaluated on each instance separately. However, in many deep learningapplications such as online content recommendation and stock market analysis,models use historical data to make predictions. Robustness certificates basedon the assumption of independent input samples are not directly applicable insuch scenarios. In this work, we focus on the provable robustness of machinelearning models in the context of data streams, where inputs are presented as asequence of potentially correlated items. We derive robustness certificates formodels that use a fixed-size sliding window over the input stream. Ourguarantees hold for the average model performance across the entire stream andare independent of stream size, making them suitable for large data streams. Weperform experiments on speech detection and human activity recognition tasksand show that our certificates can produce meaningful performance guaranteesagainst adversarial perturbations.", "output": "Provable Robustness for Streaming Models with a Sliding Window."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting crime using machine learning and deep learning techniques hasgained considerable attention from researchers in recent years, focusing onidentifying patterns and trends in crime occurrences. This review paperexamines over 150 articles to explore the various machine learning and deeplearning algorithms applied to predict crime. The study provides access to thedatasets used for crime prediction by researchers and analyzes prominentapproaches applied in machine learning and deep learning algorithms to predictcrime, offering insights into different trends and factors related to criminalactivities. Additionally, the paper highlights potential gaps and futuredirections that can enhance the accuracy of crime prediction. Finally, thecomprehensive overview of research discussed in this paper on crime predictionusing machine learning and deep learning approaches serves as a valuablereference for researchers in this field. By gaining a deeper understanding ofcrime prediction techniques, law enforcement agencies can develop strategies toprevent and respond to criminal activities more effectively.", "output": "Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural operators are gaining attention in computational science andengineering. PCA-Net is a recently proposed neural operator architecture whichcombines principal component analysis (PCA) with neural networks to approximatean underlying operator. The present work develops approximation theory for thisapproach, improving and significantly extending previous work in thisdirection. In terms of qualitative bounds, this paper derives a novel universalapproximation result, under minimal assumptions on the underlying operator andthe data-generating distribution. In terms of quantitative bounds, twopotential obstacles to efficient operator learning with PCA-Net are identified,and made rigorous through the derivation of lower complexity bounds; the firstrelates to the complexity of the output distribution, measured by a slow decayof the PCA eigenvalues. The other obstacle relates the inherent complexity ofthe space of operators between infinite-dimensional input and output spaces,resulting in a rigorous and quantifiable statement of the curse ofdimensionality. In addition to these lower bounds, upper complexity bounds arederived; first, a suitable smoothness criterion is shown to ensure a algebraicdecay of the PCA eigenvalues. Then, it is shown that PCA-Net can overcome thegeneral curse of dimensionality for specific operators of interest, arisingfrom the Darcy flow and Navier-Stokes equations.", "output": "Operator learning with PCA-Net: upper and lower complexity bounds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we address the emerging field of Streaming Federated Learning(SFL) and propose local cache update rules to manage dynamic data distributionsand limited cache capacity. Traditional federated learning relies on fixed datasets, whereas in SFL, data is streamed, and its distribution changes over time,leading to discrepancies between the local training dataset and long-termdistribution. To mitigate this problem, we propose three local cache updaterules - First-In-First-Out (FIFO), Static Ratio Selective Replacement (SRSR),and Dynamic Ratio Selective Replacement (DRSR) - that update the local cache ofeach client while considering the limited cache capacity. Furthermore, wederive a convergence bound for our proposed SFL algorithm as a function of thedistribution discrepancy between the long-term data distribution and theclient's local training dataset. We then evaluate our proposed algorithm on twodatasets: a network traffic classification dataset and an image classificationdataset. Our experimental results demonstrate that our proposed local cacheupdate rules significantly reduce the distribution discrepancy and outperformthe baseline methods. Our study advances the field of SFL and providespractical cache management solutions in federated learning.", "output": "On the Local Cache Update Rules in Streaming Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A facial recognition algorithm was used to extract face descriptors fromcarefully standardized images of 591 neutral faces taken in the laboratorysetting. Face descriptors were entered into a cross-validated linear regressionto predict participants' scores on a political orientation scale (Cronbach'salpha=.94) while controlling for age, gender, and ethnicity. The model'sperformance exceeded r=.20: much better than that of human raters and on parwith how well job interviews predict job success, alcohol drivesaggressiveness, or psychological therapy improves mental health. Moreover, themodel derived from standardized images performed well (r=.12) in a sample ofnaturalistic images of 3,401 politicians from the U.S., UK, and Canada,suggesting that the associations between facial appearance and politicalorientation generalize beyond our sample. The analysis of facial featuresassociated with political orientation revealed that conservatives had largerlower faces, although political orientation was only weakly associated withbody mass index (BMI). The predictability of political orientation fromstandardized images has critical implications for privacy, regulation of facialrecognition technology, as well as the understanding the origins andconsequences of political orientation.", "output": "Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT has enabled access to AI-generated writing for the masses, and withinjust a few months, this product has disrupted the knowledge economy, initiatinga culture shift in the way people work, learn, and write. The need todiscriminate human writing from AI is now both critical and urgent,particularly in domains like higher education and academic writing, where AIhad not been a significant threat or contributor to authorship. Addressing thisneed, we developed a method for discriminating text generated by ChatGPT from(human) academic scientists, relying on prevalent and accessible supervisedclassification methods. We focused on how a particular group of humans,academic scientists, write differently than ChatGPT, and this targeted approachled to the discovery of new features for discriminating (these) humans from AI;as examples, scientists write long paragraphs and have a penchant for equivocallanguage, frequently using words like but, however, and although. With a set of20 features, including the aforementioned ones and others, we built a modelthat assigned the author, as human or AI, at well over 99% accuracy, resultingin 20 times fewer misclassified documents compared to the field-leadingapproach. This strategy for discriminating a particular set of humans writingfrom AI could be further adapted and developed by others with basic skills insupervised classification, enabling access to many highly accurate and targetedmodels for detecting AI usage in academic writing and beyond.", "output": "ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper revisits classical works of Rauch (1963, et al. 1965) and developsa novel method for maximum likelihood (ML) smoothing estimation from incompleteinformation/data of stochastic state-space systems. Score function andconditional observed information matrices of incomplete data are introduced andtheir distributional identities are established. Using these identities, the MLsmoother $widehat{x}_{kvert n}^s =argmax_{x_k} logf(x_k,widehat{x}_{k+1vert n}^s, y_{0:n}verttheta)$, $kleq n-1$, ispresented. The result shows that the ML smoother gives an estimate of state$x_k$ with more adherence of loglikehood having less standard errors than thatof the ML state estimator $widehat{x}_k=argmax_{x_k} logf(x_k,y_{0:k}verttheta)$, with $widehat{x}_{nvert n}^s=widehat{x}_n$.Recursive estimation is given in terms of an EM-gradient-particle algorithmwhich extends the work of cite{Lange} for ML smoothing estimation. Thealgorithm has an explicit iteration update which lacks in (cite{Ramadan})EM-algorithm for smoothing. A sequential Monte Carlo method is developed forvaluation of the score function and observed information matrices. A recursiveequation for the covariance matrix of estimation error is developed tocalculate the standard errors. In the case of linear systems, the method showsthat the Rauch-Tung-Striebel (RTS) smoother is a fully efficient smoothingstate-estimator whose covariance matrix coincides with the Cram'er-Rao lowerbound, the inverse of expected information matrix. Furthermore, the RTSsmoother coincides with the Kalman filter having less covariance matrix.Numerical studies are performed, confirming the accuracy of the main results.", "output": "Maximum likelihood smoothing estimation in state-space models: An incomplete-information based approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate semantic guarantees of private learning algorithms for theirresilience to training Data Reconstruction Attacks (DRAs) by informedadversaries. To this end, we derive non-asymptotic minimax lower bounds on theadversary's reconstruction error against learners that satisfy differentialprivacy (DP) and metric differential privacy (mDP). Furthermore, we demonstratethat our lower bound analysis for the latter also covers the high dimensionalregime, wherein, the input data dimensionality may be larger than theadversary's query budget. Motivated by the theoretical improvements conferredby metric DP, we extend the privacy analysis of popular deep learningalgorithms such as DP-SGD and Projected Noisy SGD to cover the broader notionof metric differential privacy.", "output": "Non-Asymptotic Lower Bounds For Training Data Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion-weighted (DW) MRI measures the direction and scale of the localdiffusion process in every voxel through its spectrum in q-space, typicallyacquired in one or more shells. Recent developments in micro-structure imagingand multi-tissue decomposition have sparked renewed attention to the radialb-value dependence of the signal. Applications in tissue classification andmicro-architecture estimation, therefore, require a signal representation thatextends over the radial as well as angular domain. Multiple approaches havebeen proposed that can model the non-linear relationship between the DW-MRIsignal and biological microstructure. In the past few years, many deeplearning-based methods have been developed towards faster inference speed andhigher inter-scan consistency compared with traditional model-based methods(e.g., multi-shell multi-tissue constrained spherical deconvolution). However,a multi-stage learning strategy is typically required since the learningprocess relied on various middle representations, such as simple harmonicoscillator reconstruction (SHORE) representation. In this work, we present aunified dynamic network with a single-stage spherical convolutional neuralnetwork, which allows efficient fiber orientation distribution function (fODF)estimation through heterogeneous multi-shell diffusion MRI sequences. We studythe Human Connectome Project (HCP) young adults with test-retest scans. Fromthe experimental results, the proposed single-stage method outperforms priormulti-stage approaches in repeated fODF estimation with shell dropoff andsingle-shell DW-MRI sequences.", "output": "A Unified Single-stage Learning Model for Estimating Fiber Orientation Distribution Functions on Heterogeneous Multi-shell Diffusion-weighted MRI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As black-box models increasingly power high-stakes applications, a variety ofdata-driven explanation methods have been introduced. Meanwhile, machinelearning models are constantly challenged by distributional shifts. A questionnaturally arises: Are data-driven explanations robust againstout-of-distribution data? Our empirical results show that even though predictcorrectly, the model might still yield unreliable explanations underdistributional shifts. How to develop robust explanations againstout-of-distribution data? To address this problem, we propose an end-to-endmodel-agnostic learning framework Distributionally Robust Explanations (DRE).The key idea is, inspired by self-supervised learning, to fully utilizes theinter-distribution information to provide supervisory signals for the learningof explanations without human annotation. Can robust explanations benefit themodel's generalization capability? We conduct extensive experiments on a widerange of tasks and data types, including classification and regression on imageand scientific tabular data. Our results demonstrate that the proposed methodsignificantly improves the model's performance in terms of explanation andprediction robustness against distributional shifts.", "output": "Are Data-driven Explanations Robust against Out-of-distribution Data?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Speeding has been and continues to be a major contributing factor to trafficfatalities. Various transportation agencies have proposed speed managementstrategies to reduce the amount of speeding on arterials. While there have beenvarious studies done on the analysis of speeding proportions above the speedlimit, few studies have considered the effect on the individual's journey. Manystudies utilized speed data from detectors, which is limited in that there isno information of the route that the driver took. This study aims to explorethe effects of various roadway features an individual experiences for a givenjourney on speeding proportions. Connected vehicle trajectory data was utilizedto identify the path that a driver took, along with the vehicle relatedvariables. The level of speeding proportion is predicted using multiplelearning models. The model with the best performance, Extreme GradientBoosting, achieved an accuracy of 0.756. The proposed model can be used tounderstand how the environment and vehicle's path effects the drivers' speedingbehavior, as well as predict the areas with high levels of speedingproportions. The results suggested that features related to an individualdriver's trip, i.e., total travel time, has a significant contribution towardsspeeding. Features that are related to the environment of the individualdriver's trip, i.e., proportion of residential area, also had a significanteffect on reducing speeding proportions. It is expected that the findings couldhelp inform transportation agencies more on the factors related to speeding foran individual driver's trip.", "output": "Using Connected Vehicle Trajectory Data to Evaluate the Effects of Speeding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is growing interest in searching for information from large videocorpora. Prior works have studied relevant tasks, such as text-based videoretrieval, moment retrieval, video summarization, and video captioning inisolation, without an end-to-end setup that can jointly search from videocorpora and generate summaries. Such an end-to-end setup would allow for manyinteresting applications, e.g., a text-based search that finds a relevant videofrom a video corpus, extracts the most relevant moment from that video, andsegments the moment into important steps with captions. To address this, wepresent the HiREST (HIerarchical REtrieval and STep-captioning) dataset andpropose a new benchmark that covers hierarchical information retrieval andvisual/textual stepwise summarization from an instructional video corpus.HiREST consists of 3.4K text-video pairs from an instructional video dataset,where 1.1K videos have annotations of moment spans relevant to text query andbreakdown of each moment into key instruction steps with caption and timestamps(totaling 8.6K step captions). Our hierarchical benchmark consists of videoretrieval, moment retrieval, and two novel moment segmentation and stepcaptioning tasks. In moment segmentation, models break down a video moment intoinstruction steps and identify start-end boundaries. In step captioning, modelsgenerate a textual summary for each step. We also present starting pointtask-specific and end-to-end joint baseline models for our new benchmark. Whilethe baseline models show some promising results, there still exists large roomfor future improvement by the community. Project website:", "output": "Hierarchical Video-Moment Retrieval and Step-Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "EEG-based recognition of activities and states involves the use of priorneuroscience knowledge to generate quantitative EEG features, which may limitBCI performance. Although neural network-based methods can effectively extractfeatures, they often encounter issues such as poor generalization acrossdatasets, high predicting volatility, and low model interpretability. Hence, wepropose a novel lightweight multi-dimensional attention network, calledLMDA-Net. By incorporating two novel attention modules designed specificallyfor EEG signals, the channel attention module and the depth attention module,LMDA-Net can effectively integrate features from multiple dimensions, resultingin improved classification performance across various BCI tasks. LMDA-Net wasevaluated on four high-impact public datasets, including motor imagery (MI) andP300-Speller paradigms, and was compared with other representative models. Theexperimental results demonstrate that LMDA-Net outperforms other representativemethods in terms of classification accuracy and predicting volatility,achieving the highest accuracy in all datasets within 300 training epochs.Ablation experiments further confirm the effectiveness of the channel attentionmodule and the depth attention module. To facilitate an in-depth understandingof the features extracted by LMDA-Net, we propose class-specific neural networkfeature interpretability algorithms that are suitable for event-relatedpotentials (ERPs) and event-related desynchronization/synchronization(ERD/ERS). By mapping the output of the specific layer of LMDA-Net to the timeor spatial domain through class activation maps, the resulting featurevisualizations can provide interpretable analysis and establish connectionswith EEG time-spatial analysis in neuroscience. In summary, LMDA-Net showsgreat potential as a general online decoding model for various EEG tasks.", "output": "LMDA-Net:A lightweight multi-dimensional attention network for general EEG-based brain-computer interface paradigms and interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a multimodal deep learning (MDL) framework for predicting physicalproperties of a 10-dimensional acrylic polymer composite material by mergingphysical attributes and chemical data. Our MDL model comprises four modules,including three generative deep learning models for material structurecharacterization and a fourth model for property prediction. Our approachhandles an 18-dimensional complexity, with 10 compositional inputs and 8property outputs, successfully predicting 913,680 property data points across114,210 composition conditions. This level of complexity is unprecedented incomputational materials science, particularly for materials with undefinedstructures. We propose a framework to analyze the high-dimensional informationspace for inverse material design, demonstrating flexibility and adaptabilityto various materials and scales, provided sufficient data is available. Thisstudy advances future research on different materials and the development ofmore sophisticated models, drawing us closer to the ultimate goal of predictingall properties of all materials.", "output": "A Comprehensive and Versatile Multimodal Deep Learning Approach for Predicting Diverse Properties of Advanced Materials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work reveals undiscovered challenges in the performance andgeneralizability of deep learning models. We (1) identify spurious shortcutsand evaluation issues that can inflate performance and (2) propose training andanalysis methods to address them.We trained an AI model to classify cancer on a retrospective dataset of120,112 US exams (3,467 cancers) acquired from 2008 to 2017 and 16,693 UK exams(5,655 cancers) acquired from 2011 to 2015.We evaluated on a screening mammography test set of 11,593 US exams (102cancers; 7,594 women; age 57.1 pm 11.0) and 1,880 UK exams (590 cancers; 1,745women; age 63.3 pm 7.2). A model trained on images of only view markers (nobreast) achieved a 0.691 AUC. The original model trained on both datasetsachieved a 0.945 AUC on the combined US+UK dataset but paradoxically only 0.838and 0.892 on the US and UK datasets, respectively. Sampling cancers equallyfrom both datasets during training mitigated this shortcut. A similar AUCparadox (0.903) occurred when evaluating diagnostic exams vs screening exams(0.862 vs 0.861, respectively). Removing diagnostic exams during trainingalleviated this bias. Finally, the model did not exhibit the AUC paradox overscanner models but still exhibited a bias toward Selenia Dimension (SD) overHologic Selenia (HS) exams. Analysis showed that this AUC paradox occurred whena dataset attribute had values with a higher cancer prevalence (dataset bias)and the model consequently assigned a higher probability to these attributevalues (model bias). Stratification and balancing cancer prevalence canmitigate shortcuts during evaluation.Dataset and model bias can introduce shortcuts and the AUC paradox,potentially pervasive issues within the healthcare AI space. Our methods canverify and mitigate shortcuts while providing a clear understanding ofperformance.", "output": "Problems and shortcuts in deep learning for screening mammography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While decades of theoretical research have led to the invention of severalclasses of error-correction codes, the design of such codes is an extremelychallenging task, mostly driven by human ingenuity. Recent studies demonstratethat such designs can be effectively automated and accelerated via tools frommachine learning (ML), thus enabling ML-driven classes of error-correctioncodes with promising performance gains compared to classical designs. Afundamental challenge, however, is that it is prohibitively complex, if notimpossible, to design and train fully ML-driven encoder and decoder pairs forlarge code dimensions. In this paper, we propose Product Autoencoder(ProductAE) -- a computationally-efficient family of deep learning driven(encoder, decoder) pairs -- aimed at enabling the training of relatively largecodes (both encoder and decoder) with a manageable training complexity. Webuild upon ideas from classical product codes and propose constructing largeneural codes using smaller code components. ProductAE boils down the complexproblem of training the encoder and decoder for a large code dimension $k$ andblocklength $n$ to less-complex sub-problems of training encoders and decodersfor smaller dimensions and blocklengths. Our training results show successfultraining of ProductAEs of dimensions as large as $k = 300$ bits with meaningfulperformance gains compared to state-of-the-art classical and neural designs.Moreover, we demonstrate excellent robustness and adaptivity of ProductAEs tochannel models different than the ones used for training.", "output": "ProductAE: Toward Deep Learning Driven Error-Correction Codes of Large Dimensions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Protein language models (pLMs), pre-trained via causal language modeling onprotein sequences, have been a promising tool for protein sequence design. Inreal-world protein engineering, there are many cases where the amino acids inthe middle of a protein sequence are optimized while maintaining otherresidues. Unfortunately, because of the left-to-right nature of pLMs, existingpLMs modify suffix residues by prompting prefix residues, which areinsufficient for the infilling task that considers the whole surroundingcontext. To find the more effective pLMs for protein engineering, we design anew benchmark, Secondary structureE InFilling rEcoveRy, SEIFER, whichapproximates infilling sequence design scenarios. With the evaluation ofexisting models on the benchmark, we reveal the weakness of existing languagemodels and show that language models trained via fill-in-middle transformation,called ProtFIM, are more appropriate for protein engineering. Also, we provethat ProtFIM generates protein sequences with decent protein representationsthrough exhaustive experiments and visualizations.", "output": "ProtFIM: Fill-in-Middle Protein Sequence Design via Protein Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work we develop a novel approach using deep neural networks toreconstruct the conductivity distribution in elliptic problems from oneinternal measurement. The approach is based on a mixed reformulation of thegoverning equation and utilizes the standard least-squares objective toapproximate the conductivity and flux simultaneously, with deep neural networksas ansatz functions. We provide a thorough analysis of the neural networkapproximations for both continuous and empirical losses, including rigorouserror estimates that are explicit in terms of the noise level, various penaltyparameters and neural network architectural parameters (depth, width andparameter bound). We also provide extensive numerical experiments in two- andmulti-dimensions to illustrate distinct features of the approach, e.g.,excellent stability with respect to data noise and capability of solvinghigh-dimensional problems.", "output": "Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, graph pre-training has attracted wide research attention, whichaims to learn transferable knowledge from unlabeled graph data so as to improvedownstream performance. Despite these recent attempts, the negative transfer isa major issue when applying graph pre-trained models to downstream tasks.Existing works made great efforts on the issue of what to pre-train and how topre-train by designing a number of graph pre-training and fine-tuningstrategies. However, there are indeed cases where no matter how advanced thestrategy is, the \"pre-train and fine-tune\" paradigm still cannot achieve clearbenefits. This paper introduces a generic framework W2PGNN to answer thecrucial question of when to pre-train (i.e., in what situations could we takeadvantage of graph pre-training) before performing effortful pre-training orfine-tuning. We start from a new perspective to explore the complex generativemechanisms from the pre-training data to downstream data. In particular, W2PGNNfirst fits the pre-training data into graphon bases, each element of graphonbasis (i.e., a graphon) identifies a fundamental transferable pattern shared bya collection of pre-training graphs. All convex combinations of graphon basesgive rise to a generator space, from which graphs generated form the solutionspace for those downstream data that can benefit from pre-training. In thismanner, the feasibility of pre-training can be quantified as the generationprobability of the downstream data from any generator in the generator space.W2PGNN provides three broad applications, including providing the applicationscope of graph pre-trained models, quantifying the feasibility of performingpre-training, and helping select pre-training data to enhance downstreamperformance. We give a theoretically sound solution for the first applicationand extensive empirical justifications for the latter two applications.", "output": "When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There are plenty of graph neural network (GNN) accelerators being proposed.However, they highly rely on users' hardware expertise and are usuallyoptimized for one specific GNN model, making them challenging for practical use. Therefore, in this work, we propose GNNBuilder, the first automated, generic,end-to-end GNN accelerator generation framework. It features four advantages:(1) GNNBuilder can automatically generate GNN accelerators for a wide range ofGNN models arbitrarily defined by users; (2) GNNBuilder takes standard PyTorchprogramming interface, introducing zero overhead for algorithm developers; (3)GNNBuilder supports end-to-end code generation, simulation, acceleratoroptimization, and hardware deployment, realizing a push-button fashion for GNNaccelerator design; (4) GNNBuilder is equipped with accurate performance modelsof its generated accelerator, enabling fast and flexible design spaceexploration (DSE). In the experiments, first, we show that our acceleratorperformance model has errors within $36%$ for latency prediction and $18%$for BRAM count prediction. Second, we show that our generated accelerators canoutperform CPU by $6.33times$ and GPU by $6.87times$. This framework isopen-source, and the code is available at", "output": "GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The generalization performance of deep neural networks with regard to theoptimization algorithm is one of the major concerns in machine learning. Thisperformance can be affected by various factors. In this paper, we theoreticallyprove that the Lipschitz constant of a loss function is an important factor todiminish the generalization error of the output model obtained by Adam orAdamW. The results can be used as a guideline for choosing the loss functionwhen the optimization algorithm is Adam or AdamW. In addition, to evaluate thetheoretical bound in a practical setting, we choose the human age estimationproblem in computer vision. For assessing the generalization better, thetraining and test datasets are drawn from different distributions. Ourexperimental evaluation shows that the loss function with lower Lipschitzconstant and maximum value improves the generalization of the model trained byAdam or AdamW.", "output": "Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This note focuses on a simple approach to the unified analysis of SGD-typemethods from (Gorbunov et al., 2020) for strongly convex smooth optimizationproblems. The similarities in the analyses of different stochastic first-ordermethods are discussed along with the existing extensions of the framework. Thelimitations of the analysis and several alternative approaches are mentioned aswell.", "output": "Unified analysis of SGD-type methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past few years, there has been a significant amount of researchfocused on studying the ReLU activation function, with the aim of achievingneural network convergence through over-parametrization. However, recentdevelopments in the field of Large Language Models (LLMs) have sparked interestin the use of exponential activation functions, specifically in the attentionmechanism.Mathematically, we define the neural function $F: mathbb{R}^{d times m}times mathbb{R}^d rightarrow mathbb{R}$ using an exponential activationfunction. Given a set of data points with labels ${(x_1, y_1), (x_2, y_2),dots, (x_n, y_n)} subset mathbb{R}^d times mathbb{R}$ where $n$ denotesthe number of the data. Here $F(W(t),x)$ can be expressed as $F(W(t),x) :=sum_{r=1}^m a_r exp(langle w_r, x rangle)$, where $m$ represents the numberof neurons, and $w_r(t)$ are weights at time $t$. It's standard in literaturethat $a_r$ are the fixed weights and it's never changed during the training. Weinitialize the weights $W(0) in mathbb{R}^{d times m}$ with random Gaussiandistributions, such that $w_r(0) sim mathcal{N}(0, I_d)$ and initialize $a_r$from random sign distribution for each $r in [m]$.Using the gradient descent algorithm, we can find a weight $W(T)$ such that$| F(W(T), X) - y |_2 leq epsilon$ holds with probability $1-delta$, where$epsilon in (0,0.1)$ and $m = Omega(n^{2+o(1)}log(n/delta))$. To optimizethe over-parameterization bound $m$, we employ several tight analysistechniques from previous studies [Song and Yang arXiv 2019, Munteanu, Omlor,Song and Woodruff ICML 2022].", "output": "An Over-parameterized Exponential Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-target regression is useful in a plethora of applications. Althoughrandom forest models perform well in these tasks, they are often difficult tointerpret. Interpretability is crucial in machine learning, especially when itcan directly impact human well-being. Although model-agnostic techniques existfor multi-target regression, specific techniques tailored to random forestmodels are not available. To address this issue, we propose a technique thatprovides rule-based interpretations for instances made by a random forest modelfor multi-target regression, influenced by a recent model-specific techniquefor random forest interpretability. The proposed technique was evaluatedthrough extensive experiments and shown to offer competitive interpretationscompared to state-of-the-art techniques.", "output": "Local Interpretability of Random Forests for Multi-Target Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Orthogonality constraints naturally appear in many machine learning problems,from Principal Components Analysis to robust neural network training. They areusually solved using Riemannian optimization algorithms, which minimize theobjective function while enforcing the constraint. However, enforcing theorthogonality constraint can be the most time-consuming operation in suchalgorithms. Recently, Ablin &amp; Peyr'e (2022) proposed the Landing algorithm, amethod with cheap iterations that does not enforce the orthogonality constraintbut is attracted towards the manifold in a smooth manner. In this article, weprovide new practical and theoretical developments for the landing algorithm.First, the method is extended to the Stiefel manifold, the set of rectangularorthogonal matrices. We also consider stochastic and variance reductionalgorithms when the cost function is an average of many functions. Wedemonstrate that all these methods have the same rate of convergence as theirRiemannian counterparts that exactly enforce the constraint. Finally, ourexperiments demonstrate the promise of our approach to an array ofmachine-learning problems that involve orthogonality constraints.", "output": "Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How to ensure fairness is an important topic in federated learning (FL).Recent studies have investigated how to reward clients based on theircontribution (collaboration fairness), and how to achieve uniformity ofperformance across clients (performance fairness). Despite achieving progresson either one, we argue that it is critical to consider them together, in orderto engage and motivate more diverse clients joining FL to derive a high-qualityglobal model. In this work, we propose a novel method to optimize both types offairness simultaneously. Specifically, we propose to estimate clientcontribution in gradient and data space. In gradient space, we monitor thegradient direction differences of each client with respect to others. And indata space, we measure the prediction error on client data using an auxiliarymodel. Based on this contribution estimation, we propose a FL method, federatedtraining via contribution estimation (FedCE), i.e., using estimation as globalmodel aggregation weights. We have theoretically analyzed our method andempirically evaluated it on two real-world medical datasets. The effectivenessof our approach has been validated with significant performance improvements,better collaboration fairness, better performance fairness, and comprehensiveanalytical studies.", "output": "Fair Federated Medical Image Segmentation via Client Contribution Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online deep clustering refers to the joint use of a feature extractionnetwork and a clustering model to assign cluster labels to each new data pointor batch as it is processed. While faster and more versatile than offlinemethods, online clustering can easily reach the collapsed solution where theencoder maps all inputs to the same point and all are put into a singlecluster. Successful existing models have employed various techniques to avoidthis problem, most of which require data augmentation or which aim to make theaverage soft assignment across the dataset the same for each cluster. Wepropose a method that does not require data augmentation, and that, differentlyfrom existing methods, regularizes the hard assignments. Using a Bayesianframework, we derive an intuitive optimization objective that can bestraightforwardly included in the training of the encoder network. Tested onfour image datasets, we show that it consistently avoids collapse more robustlythan other methods and that it leads to more accurate clustering. We alsoconduct further experiments and analyses justifying our choice to regularizethe hard cluster assignments.", "output": "Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pillars are important structural units used to ensure mining safety inunderground hard rock mines. Therefore, precise predictions regarding thestability of underground pillars are required. One common index that is oftenused to assess pillar stability is the Safety Factor (SF). Unfortunately, suchcrisp boundaries in pillar stability assessment using SF are unreliable. Thispaper presents a novel application of Artificial Neural Network-Backpropagation(ANN-BP) and Deep Ensemble Learning for pillar stability classification. Thereare three types of ANN-BP used for the classification of pillar stabilitydistinguished by their activation functions: ANN-BP ReLU, ANN-BP ELU, andANN-BP GELU. This research also presents a new labeling alternative for pillarstability by considering its suitability with the SF. Thus, pillar stability isexpanded into four categories: failed with a suitable safety factor, intactwith a suitable safety factor, failed without a suitable safety factor, andintact without a suitable safety factor. There are five inputs used for eachmodel: pillar width, mining height, bord width, depth to floor, and ratio. Theresults showed that the ANN-BP model with Ensemble Learning could improveANN-BP performance with an average accuracy of 86.48% and an F_2-score of96.35% for the category of failed with a suitable safety factor.", "output": "Ensemble Learning Model on Artificial Neural Network-Backpropagation (ANN-BP) Architecture for Coal Pillar Stability Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stochastic gradient descent samples uniformly the training set to build anunbiased gradient estimate with a limited number of samples. However, at agiven step of the training process, some data are more helpful than others tocontinue learning. Importance sampling for training deep neural networks hasbeen widely studied to propose sampling schemes yielding better performancethan the uniform sampling scheme. After recalling the theory of importancesampling for deep learning, this paper reviews the challenges inherent to thisresearch area. In particular, we propose a metric allowing the assessment ofthe quality of a given sampling scheme; and we study the interplay between thesampling scheme and the optimizer used.", "output": "Importance Sampling for Stochastic Gradient Descent in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is a challenging problem to predict trends of futures prices withtraditional econometric models as one needs to consider not only futures'historical data but also correlations among different futures. Spatial-temporalgraph neural networks (STGNNs) have great advantages in dealing with such kindof spatial-temporal data. However, we cannot directly apply STGNNs tohigh-frequency future data because future investors have to consider both thelong-term and short-term characteristics when doing decision-making. To captureboth the long-term and short-term features, we exploit more label informationby designing four heterogeneous tasks: price regression, price moving averageregression, price gap regression (within a short interval), and change-pointdetection, which involve both long-term and short-term scenes. To make full useof these labels, we train our model in a continual manner. Traditionalcontinual GNNs define the gradient of prices as the parameter important toovercome catastrophic forgetting (CF). Unfortunately, the losses of the fourheterogeneous tasks lie in different spaces. Hence it is improper to calculatethe parameter importance with their losses. We propose to calculate parameterimportance with mutual information between original observations and theextracted features. The empirical results based on 49 commodity futuresdemonstrate that our model has higher prediction performance on capturinglong-term or short-term dynamic change.", "output": "Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A central problem in unsupervised deep learning is how to find usefulrepresentations of high-dimensional data, sometimes called \"disentanglement\".Most approaches are heuristic and lack a proper theoretical foundation. Inlinear representation learning, independent component analysis (ICA) has beensuccessful in many applications areas, and it is principled, i.e. based on awell-defined probabilistic model. However, extension of ICA to the nonlinearcase has been problematic due to the lack of identifiability, i.e. uniquenessof the representation. Recently, nonlinear extensions that utilize temporalstructure or some auxiliary information have been proposed. Such models are infact identifiable, and consequently, an increasing number of algorithms havebeen developed. In particular, some self-supervised algorithms can be shown toestimate nonlinear ICA, even though they have initially been proposed fromheuristic perspectives. This paper reviews the state-of-the-art of nonlinearICA theory and algorithms.", "output": "Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies an infinite horizon optimal control problem fordiscrete-time linear system and quadratic criteria, both with random parameterswhich are independent and identically distributed with respect to time. In thisgeneral setting, we apply the policy gradient method, a reinforcement learningtechnique, to search for the optimal control without requiring knowledge ofstatistical information of the parameters. We investigate the sub-Gaussianityof the state process and establish global linear convergence guarantee for thisapproach based on assumptions that are weaker and easier to verify compared toexisting results. Numerical experiments are presented to illustrate our result.", "output": "Policy Gradient Methods for Discrete Time Linear Quadratic Regulator With Random Parameters."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study building a multi-task agent in Minecraft. Without humandemonstrations, solving long-horizon tasks in this open-ended environment withreinforcement learning (RL) is extremely sample inefficient. To tackle thechallenge, we decompose solving Minecraft tasks into learning basic skills andplanning over the skills. We propose three types of fine-grained basic skillsin Minecraft, and use RL with intrinsic rewards to accomplish basic skills withhigh success rates. For skill planning, we use Large Language Models to findthe relationships between skills and build a skill graph in advance. When theagent is solving a task, our skill search algorithm walks on the skill graphand generates the proper skill plans for the agent. In experiments, our methodaccomplishes 24 diverse Minecraft tasks, where many tasks require sequentiallyexecuting for more than 10 skills. Our method outperforms baselines in mosttasks by a large margin. The project's website and code can be found at", "output": "Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Satellite imagery analysis plays a vital role in remote sensing, but theinformation loss caused by cloud cover seriously hinders its application. Thisstudy presents a high-performance cloud removal architecture called ProgressiveMulti-scale Attention Autoencoder (PMAA), which simultaneously leverages globaland local information. It mainly consists of a cloud detection backbone and acloud removal module. The cloud detection backbone uses cloud masks toreinforce cloudy areas to prompt the cloud removal module. The cloud removalmodule mainly comprises a novel Multi-scale Attention Module (MAM) and a LocalInteraction Module (LIM). PMAA establishes the long-range dependency ofmulti-scale features using MAM and modulates the reconstruction of thefine-grained details using LIM, allowing for the simultaneous representation offine- and coarse-grained features at the same level. With the help of diverseand multi-scale feature representation, PMAA outperforms the previousstate-of-the-art model CTGAN consistently on the Sen2_MTC_Old and Sen2_MTC_Newdatasets. Furthermore, PMAA has a considerable efficiency advantage, with only0.5% and 14.6% of the parameters and computational complexity of CTGAN,respectively. These extensive results highlight the potential of PMAA as alightweight cloud removal network suitable for deployment on edge devices. Wewill release the code and trained models to facilitate the study in thisdirection.", "output": "PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-Performance Cloud Removal from Multi-temporal Satellite Imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum machine learning has the potential for a transformative impact acrossindustry sectors and in particular in finance. In our work we look at theproblem of hedging where deep reinforcement learning offers a powerfulframework for real markets. We develop quantum reinforcement learning methodsbased on policy-search and distributional actor-critic algorithms that usequantum neural network architectures with orthogonal and compound layers forthe policy and value functions. We prove that the quantum neural networks weuse are trainable, and we perform extensive simulations that show that quantummodels can reduce the number of trainable parameters while achieving comparableperformance and that the distributional approach obtains better performancethan other standard approaches, both classical and quantum. We successfullyimplement the proposed models on a trapped-ion quantum processor, utilizingcircuits with up to $16$ qubits, and observe performance that agrees well withnoiseless simulation. Our quantum techniques are general and can be applied toother reinforcement learning problems beyond hedging.", "output": "Quantum Deep Hedging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Owing to their remarkable learning (and relearning) capabilities, deep neuralnetworks (DNNs) find use in numerous real-world applications. However, thelearning of these data-driven machine learning models is generally as good asthe data available to them for training. Hence, training datasets withlong-tail distribution pose a challenge for DNNs, since the DNNs trained onthem may provide a varying degree of classification performance acrossdifferent output classes. While the overall bias of such networks is alreadyhighlighted in existing works, this work identifies the node bias that leads toa varying sensitivity of the nodes for different output classes. To the best ofour knowledge, this is the first work highlighting this unique challenge inDNNs, discussing its probable causes, and providing open challenges for thisnew research direction. We support our reasoning using an empirical case studyof the networks trained on a real-world dataset.", "output": "Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution in trained DNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) is a type of distributed machine learning at thewireless edge that preserves the privacy of clients' data from adversaries andeven the central server. Existing federated learning approaches either use (i)secure multiparty computation (SMC) which is vulnerable to inference or (ii)differential privacy which may decrease the test accuracy given a large numberof parties with relatively small amounts of data each. To tackle the problemwith the existing methods in the literature, In this paper, we introduceincorporate federated learning in the inner-working of MIMO systems.", "output": "Federated Learning in MIMO Satellite Broadcast System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Composed image retrieval searches for a target image based on a multi-modaluser query comprised of a reference image and modification text describing thedesired changes. Existing approaches to solving this challenging task learn amapping from the (reference image, modification text)-pair to an imageembedding that is then matched against a large image corpus. One area that hasnot yet been explored is the reverse direction, which asks the question, whatreference image when modified as describe by the text would produce the giventarget image? In this work we propose a bi-directional training scheme thatleverages such reversed queries and can be applied to existing composed imageretrieval architectures. To encode the bi-directional query we prepend alearnable token to the modification text that designates the direction of thequery and then finetune the parameters of the text embedding module. We make noother changes to the network architecture. Experiments on two standard datasetsshow that our novel approach achieves improved performance over a baselineBLIP-based model that itself already achieves state-of-the-art performance.", "output": "Bi-directional Training for Composed Image Retrieval via Text Prompt Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Personalisation of language models for dialogue sensitises them to bettercapture the speaking patterns of people of specific characteristics, and/or inspecific environments. However, rich character annotations are difficult tocome by and to successfully leverage. In this work, we release and describe anovel set of manual annotations for 863 speakers from the popular Cornell MovieDialog Corpus, including features like characteristic quotes and characterdescriptions, and a set of six automatically extracted metadata for over 95% ofthe featured films. We perform extensive experiments on two corpora and showthat such annotations can be effectively used to personalise language models,reducing perplexity by up to 8.5%. Our method can be applied even zero-shot forspeakers for whom no prior training data is available, by relying oncombinations of characters' demographic characteristics. Since collecting suchmetadata is costly, we also contribute a cost-benefit analysis to highlightwhich annotations were most cost-effective relative to the reduction inperplexity.", "output": "Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fairlearn is an open source project to help practitioners assess and improvefairness of artificial intelligence (AI) systems. The associated Pythonlibrary, also named fairlearn, supports evaluation of a model's output acrossaffected populations and includes several algorithms for mitigating fairnessissues. Grounded in the understanding that fairness is a sociotechnicalchallenge, the project integrates learning resources that aid practitioners inconsidering a system's broader societal context.", "output": "Fairlearn: Assessing and Improving Fairness of AI Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, researchers proposed a variety of deep learning models forwind power forecasting. These models predict the wind power generation of windfarms or entire regions more accurately than traditional machine learningalgorithms or physical models. However, latest research has shown that deeplearning models can often be manipulated by adversarial attacks. Since windpower forecasts are essential for the stability of modern power systems, it isimportant to protect them from this threat. In this work, we investigate thevulnerability of two different forecasting models to targeted, semitargeted,and untargeted adversarial attacks. We consider a Long Short-Term Memory (LSTM)network for predicting the power generation of a wind farm and a ConvolutionalNeural Network (CNN) for forecasting the wind power generation throughoutGermany. Moreover, we propose the Total Adversarial Robustness Score (TARS), anevaluation metric for quantifying the robustness of regression models totargeted and semi-targeted adversarial attacks. It assesses the impact ofattacks on the model's performance, as well as the extent to which theattacker's goal was achieved, by assigning a score between 0 (very vulnerable)and 1 (very robust). In our experiments, the LSTM forecasting model was fairlyrobust and achieved a TARS value of over 0.81 for all adversarial attacksinvestigated. The CNN forecasting model only achieved TARS values below 0.06when trained ordinarily, and was thus very vulnerable. Yet, its robustnesscould be significantly improved by adversarial training, which always resultedin a TARS above 0.46.", "output": "Targeted Adversarial Attacks on Wind Power Forecasts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Underlying relationships among Multi-Agent Systems (MAS) in hazardousscenarios can be represented as Game-theoretic models. This paper proposes anew hierarchical network-based model called Game-theoretic Utility Tree (GUT),which decomposes high-level strategies into executable low-level actions forcooperative MAS decisions. It combines with a new payoff measure based on agentneeds for real-time strategy games. We present an Explore game domain, where wemeasure the performance of MAS achieving tasks from the perspective ofbalancing the success probability and system costs. We evaluate the GUTapproach against state-of-the-art methods that greedily rely on rewards of thecomposite actions. Conclusive results on extensive numerical simulationsindicate that GUT can organize more complex relationships among MAScooperation, helping the group achieve challenging tasks with lower costs andhigher winning rates. Furthermore, we demonstrated the applicability of the GUTusing the simulator-hardware testbed - Robotarium. The performances verifiedthe effectiveness of the GUT in the real robot application and validated thatthe GUT could effectively organize MAS cooperation strategies, helping thegroup with fewer advantages achieve higher performance.", "output": "A Hierarchical Game-Theoretic Decision-Making for Cooperative Multi-Agent Systems Under the Presence of Adversarial Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We describe a recurrent neural network (RNN) based architecture to learn theflow function of a causal, time-invariant and continuous-time control systemfrom trajectory data. By restricting the class of control inputs to piecewiseconstant functions, we show that learning the flow function is equivalent tolearning the input-to-state map of a discrete-time dynamical system. Thismotivates the use of an RNN together with encoder and decoder networks whichmap the state of the system to the hidden state of the RNN and back. We showthat the proposed architecture is able to approximate the flow function byexploiting the system's causality and time-invariance. The output of thelearned flow function model can be queried at any time instant. Weexperimentally validate the proposed method using models of the Van der Pol andFitzHugh Nagumo oscillators. In both cases, the results demonstrate that thearchitecture is able to closely reproduce the trajectories of these twosystems. For the Van der Pol oscillator, we further show that the trained modelgeneralises to the system's response with a prolonged prediction time horizonas well as control inputs outside the training distribution. For theFitzHugh-Nagumo oscillator, we show that the model accurately captures theinput-dependent phenomena of excitability.", "output": "Learning Flow Functions from Data with Applications to Nonlinear Oscillators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose FLANDERS, a novel federated learning (FL)aggregation scheme robust to Byzantine attacks. FLANDERS considers the localmodel updates sent by clients at each FL round as a matrix-valued time series.Then, it identifies malicious clients as outliers of this time series bycomparing actual observations with those estimated by a matrix autoregressiveforecasting model. Experiments conducted on several datasets under different FLsettings demonstrate that FLANDERS matches the robustness of the most powerfulbaselines against Byzantine clients. Furthermore, FLANDERS remains highlyeffective even under extremely severe attack scenarios, as opposed to existingdefense strategies.", "output": "A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neuro-symbolic rule learning has attracted lots of attention as it offersbetter interpretability than pure neural models and scales better than symbolicrule learning. A recent approach named pix2rule proposes a neural DisjunctiveNormal Form (neural DNF) module to learn symbolic rules with feed-forwardlayers. Although proved to be effective in synthetic binary classification,pix2rule has not been applied to more challenging tasks such as multi-label andmulti-class classifications over real-world data. In this paper, we addressthis limitation by extending the neural DNF module to (i) support rule learningin real-world multi-class and multi-label classification tasks, (ii) enforcethe symbolic property of mutual exclusivity (i.e. predicting exactly one class)in multi-class classification, and (iii) explore its scalability over largeinputs and outputs. We train a vanilla neural DNF model similar to pix2rule'sneural DNF module for multi-label classification, and we propose a novelextended model called neural DNF-EO (Exactly One) which enforces mutualexclusivity in multi-class classification. We evaluate the classificationperformance, scalability and interpretability of our neural DNF-based models,and compare them against pure neural models and a state-of-the-art symbolicrule learner named FastLAS. We demonstrate that our neural DNF-based modelsperform similarly to neural networks, but provide better interpretability byenabling the extraction of logical rules. Our models also scale well when therule search space grows in size, in contrast to FastLAS, which fails to learnin multi-class classification tasks with 200 classes and in all multi-labelsettings.", "output": "Neuro-symbolic Rule Learning in Real-world Classification Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the continuous growth in communication network complexity and trafficvolume, communication load balancing solutions are receiving increasingattention. Specifically, reinforcement learning (RL)-based methods have shownimpressive performance compared with traditional rule-based methods. However,standard RL methods generally require an enormous amount of data to train, andgeneralize poorly to scenarios that are not encountered during training. Wepropose a policy reuse framework in which a policy selector chooses the mostsuitable pre-trained RL policy to execute based on the current trafficcondition. Our method hinges on a policy bank composed of policies trained on adiverse set of traffic scenarios. When deploying to an unknown trafficscenario, we select a policy from the policy bank based on the similaritybetween the previous-day traffic of the current scenario and the trafficobserved during training. Experiments demonstrate that this framework canoutperform classical and adaptive rule-based methods by a large margin.", "output": "Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Communication load balancing aims to balance the load between differentavailable resources, and thus improve the quality of service for networksystems. After formulating the load balancing (LB) as a Markov decision processproblem, reinforcement learning (RL) has recently proven effective inaddressing the LB problem. To leverage the benefits of classical RL for loadbalancing, however, we need an explicit reward definition. Engineering thisreward function is challenging, because it involves the need for expertknowledge and there lacks a general consensus on the form of an optimal rewardfunction. In this work, we tackle the communication load balancing problem froman inverse reinforcement learning (IRL) approach. To the best of our knowledge,this is the first time IRL has been successfully applied in the field ofcommunication load balancing. Specifically, first, we infer a reward functionfrom a set of demonstrations, and then learn a reinforcement learning loadbalancing policy with the inferred reward function. Compared to classicalRL-based solution, the proposed solution can be more general and more suitablefor real-world scenarios. Experimental evaluations implemented on differentsimulated traffic scenarios have shown our method to be effective and betterthan other baselines by a considerable margin.", "output": "Communication Load Balancing via Efficient Inverse Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inverse optimal control methods can be used to characterize behavior insequential decision-making tasks. Most existing work, however, requires thecontrol signals to be known, or is limited to fully-observable or linearsystems. This paper introduces a probabilistic approach to inverse optimalcontrol for stochastic non-linear systems with missing control signals andpartial observability that unifies existing approaches. By using an explicitmodel of the noise characteristics of the sensory and control systems of theagent in conjunction with local linearization techniques, we derive anapproximate likelihood for the model parameters, which can be computed within asingle forward pass. We evaluate our proposed method on stochastic andpartially observable version of classic control tasks, a navigation task, and amanual reaching task. The proposed method has broad applicability, ranging fromimitation learning to sensorimotor neuroscience.", "output": "Probabilistic inverse optimal control with local linearization for non-linear partially observable systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Process mining is rapidly growing in the industry. Consequently, privacyconcerns regarding sensitive and private information included in event data,used by process mining algorithms, are becoming increasingly relevant.State-of-the-art research mainly focuses on providing privacy guarantees, e.g.,differential privacy, for trace variants that are used by the main processmining techniques, e.g., process discovery. However, privacy preservationtechniques for releasing trace variants still do not fulfill all therequirements of industry-scale usage. Moreover, providing privacy guaranteeswhen there exists a high rate of infrequent trace variants is still achallenge. In this paper, we introduce TraVaG as a new approach for releasingdifferentially private trace variants based on text{Generative AdversarialNetworks} (GANs) that provides industry-scale benefits and enhances the levelof privacy guarantees when there exists a high ratio of infrequent variants.Moreover, TraVaG overcomes shortcomings of conventional privacy preservationtechniques such as bounding the length of variants and introducing fakevariants. Experimental results on real-life event data show that our approachoutperforms state-of-the-art techniques in terms of privacy guarantees, plaindata utility preservation, and result utility preservation.", "output": "TraVaG: Differentially Private Trace Variant Generation Using GANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Topological Point Cloud Clustering (TPCC), a new method to clusterpoints in an arbitrary point cloud based on their contribution to globaltopological features. TPCC synthesizes desirable features from spectralclustering and topological data analysis and is based on considering thespectral properties of a simplicial complex associated to the considered pointcloud. As it is based on considering sparse eigenvector computations, TPCC issimilarly easy to interpret and implement as spectral clustering. However, byfocusing not just on a single matrix associated to a graph created from thepoint cloud data, but on a whole set of Hodge-Laplacians associated to anappropriately constructed simplicial complex, we can leverage a far richer setof topological features to characterize the data points within the point cloudand benefit from the relative robustness of topological techniques againstnoise. We test the performance of TPCC on both synthetic and real-world dataand compare it with classical spectral clustering.", "output": "Topological Point Cloud Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The maximum likelihood method is the best-known method for estimating theprobabilities behind the data. However, the conventional method obtains theprobability model closest to the empirical distribution, resulting inoverfitting. Then regularization methods prevent the model from beingexcessively close to the wrong probability, but little is known systematicallyabout their performance. The idea of regularization is similar toerror-correcting codes, which obtain optimal decoding by mixing suboptimalsolutions with an incorrectly received code. The optimal decoding inerror-correcting codes is achieved based on gauge symmetry. We propose atheoretically guaranteed regularization in the maximum likelihood method byfocusing on a gauge symmetry in Kullback -- Leibler divergence. In ourapproach, we obtain the optimal model without the need to search forhyperparameters frequently appearing in regularization.", "output": "Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Development and homeostasis in multicellular systems both require exquisitecontrol over spatial molecular pattern formation and maintenance. Advances inspatially-resolved and high-throughput molecular imaging methods such asmultiplexed immunofluorescence and spatial transcriptomics (ST) provideexciting new opportunities to augment our fundamental understanding of theseprocesses in health and disease. The large and complex datasets resulting fromthese techniques, particularly ST, have led to rapid development of innovativemachine learning (ML) tools primarily based on deep learning techniques. TheseML tools are now increasingly featured in integrated experimental andcomputational workflows to disentangle signals from noise in complex biologicalsystems. However, it can be difficult to understand and balance the differentimplicit assumptions and methodologies of a rapidly expanding toolbox ofanalytical tools in ST. To address this, we summarize major ST analysis goalsthat ML can help address and current analysis trends. We also describe fourmajor data science concepts and related heuristics that can help guidepractitioners in their choices of the right tools for the right biologicalquestions.", "output": "Machine Learning for Uncovering Biological Insights in Spatial Transcriptomics Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scale is the primary factor for building a powerful foundation model thatcould well generalize to a variety of downstream tasks. However, it is stillchallenging to train video foundation models with billions of parameters. Thispaper shows that video masked autoencoder (VideoMAE) is a scalable and generalself-supervised pre-trainer for building video foundation models. We scale theVideoMAE in both model and data with a core design. Specifically, we present adual masking strategy for efficient pre-training, with an encoder operating ona subset of video tokens and a decoder processing another subset of videotokens. Although VideoMAE is very efficient due to high masking ratio inencoder, masking decoder can still further reduce the overall computationalcost. This enables the efficient pre-training of billion-level models in video.We also use a progressive training paradigm that involves an initialpre-training on a diverse multi-sourced unlabeled dataset, followed by apost-pre-training on a mixed labeled dataset. Finally, we successfully train avideo ViT model with a billion parameters, which achieves a newstate-of-the-art performance on the datasets of Kinetics (90.0% on K400 and89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). Inaddition, we extensively verify the pre-trained video ViT models on a varietyof downstream tasks, demonstrating its effectiveness as a general videorepresentation learner.", "output": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unmanned Aerial Vehicles (UAVs) are increasingly used as aerial base stationsto provide ad hoc communications infrastructure. Building upon prior researchefforts which consider either static nodes, 2D trajectories or single UAVsystems, this paper focuses on the use of multiple UAVs for providing wirelesscommunication to mobile users in the absence of terrestrial communicationsinfrastructure. In particular, we jointly optimize UAV 3D trajectory and NOMApower allocation to maximize system throughput. Firstly, a weightedK-means-based clustering algorithm establishes UAV-user associations at regularintervals. The efficacy of training a novel Shared Deep Q-Network (SDQN) withaction masking is then explored. Unlike training each UAV separately using DQN,the SDQN reduces training time by using the experiences of multiple UAVsinstead of a single agent. We also show that SDQN can be used to train amulti-agent system with differing action spaces. Simulation results confirmthat: 1) training a shared DQN outperforms a conventional DQN in terms ofmaximum system throughput (+20%) and training time (-10%); 2) it can convergefor agents with different action spaces, yielding a 9% increase in throughputcompared to mutual learning algorithms; and 3) combining NOMA with an SDQNarchitecture enables the network to achieve a better sum rate compared withexisting baseline schemes.", "output": "Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled Mobile Communications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study presents a novel deep learning method, called GATv2-GCN, forpredicting player performance in sports. To construct a dynamic playerinteraction graph, we leverage player statistics and their interactions duringgameplay. We use a graph attention network to capture the attention that eachplayer pays to each other, allowing for more accurate modeling of the dynamicplayer interactions. To handle the multivariate player statistics time series,we incorporate a temporal convolution layer, which provides the model withtemporal predictive power. We evaluate the performance of our model usingreal-world sports data, demonstrating its effectiveness in predicting playerperformance. Furthermore, we explore the potential use of our model in a sportsbetting context, providing insights into profitable strategies that leverageour predictive power. The proposed method has the potential to advance thestate-of-the-art in player performance prediction and to provide valuableinsights for sports analytics and betting industries.", "output": "Who You Play Affects How You Play: Predicting Sports Performance Using Graph Attention Networks With Temporal Convolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-view clustering (MVC) based on non-negative matrix factorization (NMF)and its variants have received a huge amount of attention in recent years dueto their advantages in clustering interpretability. However, existing NMF-basedmulti-view clustering methods perform NMF on each view data respectively andignore the impact of between-view. Thus, they can't well exploit thewithin-view spatial structure and between-view complementary information. Toresolve this issue, we present semi-non-negative tensor factorization(Semi-NTF) and develop a novel multi-view clustering based on Semi-NTF withone-side orthogonal constraint. Our model directly performs Semi-NTF on the3rd-order tensor which is composed of anchor graphs of views. Thus, our modeldirectly considers the between-view relationship. Moreover, we use the tensorSchatten p-norm regularization as a rank approximation of the 3rd-order tensorwhich characterizes the cluster structure of multi-view data and exploits thebetween-view complementary information. In addition, we provide an optimizationalgorithm for the proposed method and prove mathematically that the algorithmalways converges to the stationary KKT point. Extensive experiments on variousbenchmark datasets indicate that our proposed method is able to achievesatisfactory clustering performance.", "output": "Multi-View Clustering via Semi-non-negative Tensor Factorization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The potential for pre-trained large language models (LLMs) to use naturallanguage feedback at inference time has been an exciting recent development. Webuild upon this observation by formalizing an algorithm for learning fromnatural language feedback at training time instead, which we call Imitationlearning from Language Feedback (ILF). ILF requires only a small amount ofhuman-written feedback during training and does not require the same feedbackat test time, making it both user-friendly and sample-efficient. We furthershow that ILF can be seen as a form of minimizing the KL divergence to theground truth distribution and demonstrate a proof-of-concept on a neuralprogram synthesis task. We use ILF to improve a Codegen-Mono 6.1B model'spass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic PythonProblems (MBPP) benchmark, outperforming both fine-tuning on MBPP andfine-tuning on repaired programs written by humans. Overall, our resultssuggest that learning from human-written natural language feedback is both moreeffective and sample-efficient than training exclusively on demonstrations forimproving an LLM's performance on code generation tasks.", "output": "Improving Code Generation by Training with Natural Language Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many peer-review venues are either using or looking to use algorithms toassign submissions to reviewers. The crux of such automated approaches is thenotion of the \"similarity score\"--a numerical estimate of the expertise of areviewer in reviewing a paper--and many algorithms have been proposed tocompute these scores. However, these algorithms have not been subjected to aprincipled comparison, making it difficult for stakeholders to choose thealgorithm in an evidence-based manner. The key challenge in comparing existingalgorithms and developing better algorithms is the lack of the publiclyavailable gold-standard data that would be needed to perform reproducibleresearch. We address this challenge by collecting a novel dataset of similarityscores that we release to the research community. Our dataset consists of 477self-reported expertise scores provided by 58 researchers who evaluated theirexpertise in reviewing papers they have read previously.We use this data to compare several popular algorithms employed in computerscience conferences and come up with recommendations for stakeholders. Our mainfindings are as follows. First, all algorithms make a non-trivial amount oferror. For the task of ordering two papers in terms of their relevance for areviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hardcases, highlighting the vital need for more research on thesimilarity-computation problem. Second, most existing algorithms are designedto work with titles and abstracts of papers, and in this regime the Specter+MFRalgorithm performs best. Third, to improve performance, it may be important todevelop modern deep-learning based algorithms that can make use of the fulltexts of papers: the classical TD-IDF algorithm enhanced with full texts ofpapers is on par with the deep-learning based Specter+MFR that cannot make useof this information.", "output": "A Gold Standard Dataset for the Reviewer Assignment Problem."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Chest X-rays remains to be the most common imaging modality used to diagnoselung diseases. However, they necessitate the interpretation of experts(radiologists and pulmonologists), who are few. This review paper investigatesthe use of deep transfer learning techniques to detect COVID-19, pneumonia, andtuberculosis in chest X-ray (CXR) images. It provides an overview of currentstate-of-the-art CXR image classification techniques and discusses thechallenges and opportunities in applying transfer learning to this domain. Thepaper provides a thorough examination of recent research studies that used deeptransfer learning algorithms for COVID-19, pneumonia, and tuberculosisdetection, highlighting the advantages and disadvantages of these approaches.Finally, the review paper discusses future research directions in the field ofdeep transfer learning for CXR image classification, as well as the potentialfor these techniques to aid in the diagnosis and treatment of lung diseases.", "output": "Deep transfer learning for detecting Covid-19, Pneumonia and Tuberculosis using CXR images -- A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pretrained language models often generate outputs that are not in line withhuman preferences, such as harmful text or factually incorrect summaries.Recent work approaches the above issues by learning from a simple form of humanfeedback: comparisons between pairs of model-generated outputs. However,comparison feedback only conveys limited information about human preferences.In this paper, we introduce Imitation learning from Language Feedback (ILF), anew approach that utilizes more informative language feedback. ILF consists ofthree steps that are applied iteratively: first, conditioning the languagemodel on the input, an initial LM output, and feedback to generate refinements.Second, selecting the refinement incorporating the most feedback. Third,finetuning the language model to maximize the likelihood of the chosenrefinement given the input. We show theoretically that ILF can be viewed asBayesian Inference, similar to Reinforcement Learning from human feedback. Weevaluate ILF's effectiveness on a carefully-controlled toy task and a realisticsummarization task. Our experiments demonstrate that large language modelsaccurately incorporate feedback and that finetuning with ILF scales well withthe dataset size, even outperforming finetuning on human summaries. Learningfrom both language and comparison feedback outperforms learning from eachalone, achieving human-level summarization performance.", "output": "Training Language Models with Language Feedback at Scale."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The COVID-19 pandemic has introduced new opportunities for healthcommunication, including an increase in the public use of online outlets forhealth-related emotions. People have turned to social media networks to sharesentiments related to the impacts of the COVID-19 pandemic. In this paper weexamine the role of social messaging shared by Persons in the Public Eye (i.e.athletes, politicians, news personnel) in determining overall public discoursedirection. We harvested approximately 13 million tweets ranging from 1 January2020 to 1 March 2022. The sentiment was calculated for each tweet using afine-tuned DistilRoBERTa model, which was used to compare COVID-19vaccine-related Twitter posts (tweets) that co-occurred with mentions of Peoplein the Public Eye. Our findings suggest the presence of consistent patterns ofemotional content co-occurring with messaging shared by Persons in the PublicEye for the first two years of the COVID-19 pandemic influenced public opinionand largely stimulated online public discourse. We demonstrate that as thepandemic progressed, public sentiment shared on social networks was shaped byrisk perceptions, political ideologies and health-protective behaviours sharedby Persons in the Public Eye, often in a negative light.", "output": "Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Swarm Intelligence algorithms have gained significant attention in recentyears as a means of solving complex and non-deterministic problems. Thesealgorithms are inspired by the collective behavior of natural creatures, andthey simulate this behavior to develop intelligent agents for computationaltasks. One such algorithm is Ant Colony Optimization (ACO), which is inspiredby the foraging behavior of ants and their pheromone laying mechanism. ACO isused for solving difficult problems that are discrete and combinatorial innature. Part-of-Speech (POS) tagging is a fundamental task in natural languageprocessing that aims to assign a part-of-speech role to each word in asentence. In this research paper, proposed a high-performance POS-taggingmethod based on ACO called ACO-tagger. This method achieved a high accuracyrate of 96.867%, outperforming several state-of-the-art methods. The proposedmethod is fast and efficient, making it a viable option for practicalapplications.", "output": "ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An abundance of information about cancer exists online, but categorizing andextracting useful information from it is difficult. Almost all research withinhealthcare data processing is concerned with formal clinical data, but there isvaluable information in non-clinical data too. The present study combinesmethods within distributed computing, text retrieval, clustering, andclassification into a coherent and computationally efficient system, that canclarify cancer patient trajectories based on non-clinical and freely availableinformation. We produce a fully-functional prototype that can retrieve, clusterand present information about cancer trajectories from non-clinical forumposts. We evaluate three clustering algorithms (MR-DBSCAN, DBSCAN, and HDBSCAN)and compare them in terms of Adjusted Rand Index and total run time as afunction of the number of posts retrieved and the neighborhood radius.Clustering results show that neighborhood radius has the most significantimpact on clustering performance. For small values, the data set is splitaccordingly, but high values produce a large number of possible partitions andsearching for the best partition is hereby time-consuming. With a properestimated radius, MR-DBSCAN can cluster 50000 forum posts in 46.1 seconds,compared to DBSCAN (143.4) and HDBSCAN (282.3). We conduct an interview withthe Danish Cancer Society and present our software prototype. The organizationsees a potential in software that can democratize online information aboutcancer and foresee that such systems will be required in the future.", "output": "Computationally Efficient Labeling of Cancer Related Forum Posts by Non-Clinical Text Information Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning, classification and prediction models have applicationsacross a range of fields. Sport analytics is an increasingly popularapplication, but most existing work is focused on automated refereeing inmainstream sports and injury prevention. Research on other sports, such astable tennis, has only recently started gaining more traction. This paperproposes the use of machine learning to predict the outcome of table tennissingle matches. We use player and match statistics as features and evaluatetheir relative importance in an ablation study. In terms of models, a number ofpopular models were explored. We found that 5-fold cross-validation andhyperparameter tuning was crucial to improve model performance. We investigateddifferent feature aggregation strategies in our ablation study to demonstratethe robustness of the models. Different models performed comparably, with theaccuracy of the results (61-70%) matching state-of-the-art models in comparablesports, such as tennis. The results can serve as a baseline for future tabletennis prediction models, and can feed back to prediction research in similarball sports.", "output": "Supervised Learning for Table Tennis Match Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "COVID-19 misinformation on social media platforms such as twitter is a threatto effective pandemic management. Prior works on tweet COVID-19 misinformationnegates the role of semantic features common to twitter such as chargedemotions. Thus, we present a novel COVID-19 misinformation model, which usesboth a tweet emotion encoder and COVID-19 misinformation encoder to predictwhether a tweet contains COVID-19 misinformation. Our emotion encoder wasfine-tuned on a novel annotated dataset and our COVID-19 misinformation encoderwas fine-tuned on a subset of the COVID-HeRA dataset. Experimental results showsuperior results using the combination of emotion and misinformation encodersas opposed to a misinformation classifier alone. Furthermore, extensive resultanalysis was conducted, highlighting low quality labels and mismatched labeldistributions as key limitations to our study.", "output": "Not cool, calm or collected: Using emotional language to detect COVID-19 misinformation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cooking recipes allow individuals to exchange culinary ideas and provide foodpreparation instructions. Due to a lack of adequate labeled data, categorizingraw recipes found online to the appropriate food genres is a challenging taskin this domain. Utilizing the knowledge of domain experts to categorize recipescould be a solution. In this study, we present a novel dataset of two millionculinary recipes labeled in respective categories leveraging the knowledge offood experts and an active learning technique. To construct the dataset, wecollect the recipes from the RecipeNLG dataset. Then, we employ three humanexperts whose trustworthiness score is higher than 86.667% to categorize 300Krecipe by their Named Entity Recognition (NER) and assign it to one of the ninecategories: bakery, drinks, non-veg, vegetables, fast food, cereals, meals,sides and fusion. Finally, we categorize the remaining 1900K recipes usingActive Learning method with a blend of Query-by-Committee and Human In The Loop(HITL) approaches. There are more than two million recipes in our dataset, eachof which is categorized and has a confidence score linked with it. For the 9genres, the Fleiss Kappa score of this massive dataset is roughly 0.56026. Webelieve that the research community can use this dataset to perform variousmachine learning tasks such as recipe genre classification, recipe generationof a specific genre, new recipe creation, etc. The dataset can also be used totrain and evaluate the performance of various NLP tasks such as named entityrecognition, part-of-speech tagging, semantic role labeling, and so on. Thedataset will be available upon publication: ", "output": "Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Public opinion reflects and shapes societal behavior, but the traditionalsurvey-based tools to measure it are limited. We introduce a novel approach toprobe media diet models -- language models adapted to online news, TVbroadcast, or radio show content -- that can emulate the opinions ofsubpopulations that have consumed a set of media. To validate this method, weuse as ground truth the opinions expressed in U.S. nationally representativesurveys on COVID-19 and consumer confidence. Our studies indicate that thisapproach is (1) predictive of human judgements found in survey responsedistributions and robust to phrasing and channels of media exposure, (2) moreaccurate at modeling people who follow media more closely, and (3) aligned withliterature on which types of opinions are affected by media consumption.Probing language models provides a powerful new method for investigating mediaeffects, has practical applications in supplementing polls and forecastingpublic opinion, and suggests a need for further study of the surprisingfidelity with which neural language models can predict human responses.", "output": "Language Models Trained on Media Diets Can Predict Public Opinion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A large number of real-world networks include multiple types of nodes andedges. Graph Neural Network (GNN) emerged as a deep learning framework toutilize node features on graph-structured data showing superior performance.However, popular GNN-based architectures operate on one homogeneous network.Enabling them to work on multiple networks brings additional challenges due tothe heterogeneity of the networks and the multiplicity of the existingassociations. In this study, we present a computational approach named GRAFutilizing GNN-based approaches on multiple networks with the help of attentionmechanisms and network fusion. Using attention-based neighborhood aggregation,GRAF learns the importance of each neighbor per node (called node-levelattention) followed by the importance of association (called association-levelattention) in a hierarchical way. Then, GRAF processes a network fusion stepweighing each edge according to learned node- and association-level attention,which results in a fused enriched network. Considering that the fused networkcould be a highly dense network with many weak edges depending on the giveninput networks, we included an edge elimination step with respect to edges'weights. Finally, GRAF utilizes Graph Convolutional Network (GCN) on the fusednetwork and incorporates the node features on the graph-structured data for theprediction task or any other downstream analysis. Our extensive evaluations ofprediction tasks from different domains showed that GRAF outperformed thestate-of-the-art methods. Utilization of learned node-level andassociation-level attention allowed us to prioritize the edges properly. Thesource code for our tool is publicly available at", "output": "GRAF: Graph Attention-aware Fusion Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Researchers often represent relations in multi-variate correlational datausing Gaussian graphical models, which require regularization to sparsify themodels. Acknowledging that they often study the modular structure of theinferred network, we suggest integrating it in the cross-validation of theregularization strength to balance under- and overfitting. Using synthetic andreal data, we show that this approach allows us to better recover and infermodular structure in noisy data compared with the graphical lasso, a standardapproach using the Gaussian log-likelihood when cross-validating theregularization strength.", "output": "Module-based regularization improves Gaussian graphical models when observing noisy data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We prove a quantitative result for the approximation of functions ofregularity $C^k$ (in the sense of real variables) defined on the complex cube$Omega_n := [-1,1]^n +i[-1,1]^nsubseteq mathbb{C}^n$ using shallowcomplex-valued neural networks. Precisely, we consider neural networks with asingle hidden layer and $m$ neurons, i.e., networks of the form $z mapstosum_{j=1}^m sigma_j cdot phibig(rho_j^T z + b_jbig)$ and show that onecan approximate every function in $C^k left( Omega_n; mathbb{C}right)$using a function of that form with error of the order $m^{-k/(2n)}$ as $m toinfty$, provided that the activation function $phi: mathbb{C} tomathbb{C}$ is smooth but not polyharmonic on some non-empty open set.Furthermore, we show that the selection of the weights $sigma_j, b_j inmathbb{C}$ and $rho_j in mathbb{C}^n$ is continuous with respect to $f$ andprove that the derived rate of approximation is optimal under this continuityassumption. We also discuss the optimality of the result for a possiblydiscontinuous choice of the weights.", "output": "Optimal approximation of $C^k$-functions using shallow complex-valued neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we derive a Probably Approxilmately Correct(PAC)-Bayesian errorbound for linear time-invariant (LTI) stochastic dynamical systems with inputs.Such bounds are widespread in machine learning, and they are useful forcharacterizing the predictive power of models learned from finitely many datapoints. In particular, with the bound derived in this paper relates futureaverage prediction errors with the prediction error generated by the model onthe data used for learning. In turn, this allows us to provide finite-sampleerror bounds for a wide class of learning/system identification algorithms.Furthermore, as LTI systems are a sub-class of recurrent neural networks(RNNs), these error bounds could be a first step towards PAC-Bayesian boundsfor RNNs.", "output": "PAC-Bayesian bounds for learning LTI-ss systems with input from empirical loss."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous agents that drive on roads shared with human drivers must reasonabout the nuanced interactions among traffic participants. This poses a highlychallenging decision making problem since human behavior is influenced by amultitude of factors (e.g., human intentions and emotions) that are hard tomodel. This paper presents a decision making approach for autonomous driving,focusing on the complex task of merging into moving traffic where uncertaintyemanates from the behavior of other drivers and imperfect sensor measurements.We frame the problem as a partially observable Markov decision process (POMDP)and solve it online with Monte Carlo tree search. The solution to the POMDP isa policy that performs high-level driving maneuvers, such as giving way to anapproaching car, keeping a safe distance from the vehicle in front or merginginto traffic. Our method leverages a model learned from data to predict thefuture states of traffic while explicitly accounting for interactions among thesurrounding agents. From these predictions, the autonomous vehicle cananticipate the future consequences of its actions on the environment andoptimize its trajectory accordingly. We thoroughly test our approach insimulation, showing that the autonomous vehicle can adapt its behavior todifferent situations. We also compare against other methods, demonstrating animprovement with respect to the considered performance metrics.", "output": "Decision Making for Autonomous Driving in Interactive Merge Scenarios via Learning-based Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a large-scale empirical investigation of the zero-shot learningphenomena in a specific recognizing textual entailment (RTE) task category,i.e. the automated mining of leaderboards for Empirical AI Research. The priorreported state-of-the-art models for leaderboards extraction formulated as anRTE task, in a non-zero-shot setting, are promising with above 90% reportedperformances. However, a central research question remains unexamined: did themodels actually learn entailment? Thus, for the experiments in this paper, twoprior reported state-of-the-art models are tested out-of-the-box for theirability to generalize or their capacity for entailment, given leaderboardlabels that were unseen during training. We hypothesize that if the modelslearned entailment, their zero-shot performances can be expected to bemoderately high as well--perhaps, concretely, better than chance. As a resultof this work, a zero-shot labeled dataset is created via distant labelingformulating the leaderboard extraction RTE task.", "output": "Zero-shot Entailment of Leaderboards for Empirical AI Research."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of language models have moved from encoder-decoder todecoder-only designs. In addition, the common knowledge has it that the twomost popular multimodal tasks, the generative and contrastive tasks, tend toconflict with one another, are hard to accommodate in one architecture, andfurther need complex adaptations for downstream tasks. We propose a novelparadigm of training with a decoder-only model for multimodal tasks, which issurprisingly effective in jointly learning of these disparate vision-languagetasks. This is done with a simple model, called MaMMUT. It consists of a singlevision encoder and a text decoder, and is able to accommodate contrastive andgenerative learning by a novel two-pass approach on the text decoder. Wedemonstrate that joint training of these diverse-objective tasks is simple,effective, and maximizes the weight-sharing of the model. Furthermore, the samearchitecture enables straightforward extensions to open-vocabulary objectdetection and video-language tasks. The model tackles a diverse range of tasks,while being modest in capacity. Our model achieves the SOTA on image-text andtext-image retrieval, video question answering and open-vocabulary detectiontasks, outperforming much larger and more extensively trained foundationalmodels. It shows competitive results on VQA and Video Captioning, especiallyconsidering its size. Ablations confirm the flexibility and advantages of ourapproach.", "output": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a randomly projected convex clustering model forclustering a collection of $n$ high dimensional data points in $mathbb{R}^d$with $K$ hidden clusters. Compared to the convex clustering model forclustering original data with dimension $d$, we prove that, under some mildconditions, the perfect recovery of the cluster membership assignments of theconvex clustering model, if exists, can be preserved by the randomly projectedconvex clustering model with embedding dimension $m = O(epsilon^{-2}log(n))$,where $0 &lt; epsilon &lt; 1$ is some given parameter. We further prove that theembedding dimension can be improved to be $O(epsilon^{-2}log(K))$, which isindependent of the number of data points. Extensive numerical experimentresults will be presented in this paper to demonstrate the robustness andsuperior performance of the randomly projected convex clustering model. Thenumerical results presented in this paper also demonstrate that the randomlyprojected convex clustering model can outperform the randomly projected K-meansmodel in practice.", "output": "Randomly Projected Convex Clustering Model: Motivation, Realization, and Cluster Recovery Guarantees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solving transport problems, i.e. finding a map transporting one givendistribution to another, has numerous applications in machine learning. Novelmass transport methods motivated by generative modeling have recently beenproposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models(FMMs) implement such a transport through a Stochastic Differential Equation(SDE) or an Ordinary Differential Equation (ODE). However, while it isdesirable in many applications to approximate the deterministic dynamic OptimalTransport (OT) map which admits attractive properties, DDMs and FMMs are notguaranteed to provide transports close to the OT map. In contrast,Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recoverentropy-regularized versions of OT. Unfortunately, existing numerical methodsapproximating SBs either scale poorly with dimension or accumulate errorsacross iterations. In this work, we introduce Iterative Markovian Fitting, anew methodology for solving SB problems, and Diffusion Schr\"odinger BridgeMatching (DSBM), a novel numerical algorithm for computing IMF iterates. DSBMsignificantly improves over previous SB numerics and recovers asspecial/limiting cases various recent transport methods. We demonstrate theperformance of DSBM on a variety of problems.", "output": "Diffusion Schr\\\"odinger Bridge Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep reinforcement learning (DRL) has achieved tremendous success in manycomplex decision-making tasks of autonomous systems with high-dimensional stateand/or action spaces. However, the safety and stability still remain majorconcerns that hinder the applications of DRL to safety-critical autonomoussystems. To address the concerns, we proposed the Phy-DRL: a physical deepreinforcement learning framework. The Phy-DRL is novel in two architecturaldesigns: i) Lyapunov-like reward, and ii) residual control (i.e., integrationof physics-model-based control and data-driven control). The concurrentphysical reward and residual control empower the Phy-DRL the (mathematically)provable safety and stability guarantees. Through experiments on the invertedpendulum, we show that the Phy-DRL features guaranteed safety and stability andenhanced robustness, while offering remarkably accelerated training andenlarged reward.", "output": "Physical Deep Reinforcement Learning Towards Safety Guarantee."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is broadly known that deep neural networks are susceptible to being fooledby adversarial examples with perturbations imperceptible by humans. Variousdefenses have been proposed to improve adversarial robustness, among whichadversarial training methods are most effective. However, most of these methodstreat the training samples independently and demand a tremendous amount ofsamples to train a robust network, while ignoring the latent structuralinformation among these samples. In this work, we propose a novel LocalStructure Preserving (LSP) regularization, which aims to preserve the localstructure of the input space in the learned embedding space. In this manner,the attacking effect of adversarial samples lying in the vicinity of cleansamples can be alleviated. We show strong empirical evidence that with orwithout adversarial training, our method consistently improves the performanceof adversarial robustness on several image classification datasets compared tothe baselines and some state-of-the-art approaches, thus providing promisingdirection for future research.", "output": "Beyond Empirical Risk Minimization: Local Structure Preserving Regularization for Improving Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is critical that the models pay attention not only to accuracy but also tothe certainty of prediction. Uncertain predictions of deep models caused bynoisy data raise significant concerns in trustworthy AI areas. To explore andhandle uncertainty due to intrinsic data noise, we propose a novel methodcalled ALUM to simultaneously handle the model uncertainty and data uncertaintyin a unified scheme. Rather than solely modeling data uncertainty in theultimate layer of a deep model based on randomly selected training data, wepropose to explore mined adversarial triplets to facilitate data uncertaintymodeling and non-parametric uncertainty estimations to compensate for theinsufficiently trained latent model layers. Thus, the critical data uncertaintyand model uncertainty caused by noisy data can be readily quantified forimproving model robustness. Our proposed ALUM is model-agnostic which can beeasily implemented into any existing deep model with little extra computationoverhead. Extensive experiments on various noisy learning tasks validate thesuperior robustness and generalization ability of our method. The code isreleased at ", "output": "ALUM: Adversarial Data Uncertainty Modeling from Latent Model Uncertainty Compensation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern computational methods, involving highly sophisticated mathematicalformulations, enable several tasks like modeling complex physical phenomenon,predicting key properties and design optimization. The higher fidelity in thesecomputer models makes it computationally intensive to query them hundreds oftimes for optimization and one usually relies on a simplified model albeit atthe cost of losing predictive accuracy and precision. Towards this, data-drivensurrogate modeling methods have shown a lot of promise in emulating thebehavior of the expensive computer models. However, a major bottleneck in suchmethods is the inability to deal with high input dimensionality and the needfor relatively large datasets. With such problems, the input and outputquantity of interest are tensors of high dimensionality. Commonly usedsurrogate modeling methods for such problems, suffer from requirements likehigh number of computational evaluations that precludes one from performingother numerical tasks like uncertainty quantification and statistical analysis.In this work, we propose an end-to-end approach that maps a high-dimensionalimage like input to an output of high dimensionality or its key statistics. Ourapproach uses two main framework that perform three steps: a) reduce the inputand output from a high-dimensional space to a reduced or low-dimensional space,b) model the input-output relationship in the low-dimensional space, and c)enable the incorporation of domain-specific physical constraints as masks. Inorder to accomplish the task of reducing input dimensionality we leverageprincipal component analysis, that is coupled with two surrogate modelingmethods namely: a) Bayesian hybrid modeling, and b) DeepHyper's deep neuralnetworks. We demonstrate the applicability of the approach on a problem of alinear elastic stress field data.", "output": "Application of probabilistic modeling and automated machine learning framework for high-dimensional stress field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Hopfield model has a long-standing tradition in statistical physics,being one of the few neural networks for which a theory is available. Extendingthe theory of Hopfield models for correlated data could help understand thesuccess of deep neural networks, for instance describing how they extractfeatures from data. Motivated by this, we propose and investigate a generalizedHopfield model that we name Hidden-Manifold Hopfield Model: we generate thecouplings from $P=alpha N$ examples with the Hebb rule using a non-lineartransformation of $D=alpha_D N$ random vectors that we call factors, with $N$the number of neurons. Using the replica method, we obtain a phase diagram forthe model that shows a phase transition where the factors hidden in theexamples become attractors of the dynamics; this phase exists above a criticalvalue of $alpha$ and below a critical value of $alpha_D$. We call thisbehaviour learning transition.", "output": "The Hidden-Manifold Hopfield Model and a learning phase transition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study how pretraining label granularity affects thegeneralization of deep neural networks in image classification tasks. We focuson the \"fine-to-coarse\" transfer learning setting where the pretraining labelis more fine-grained than that of the target problem. We experiment with thismethod using the label hierarchy of iNaturalist 2021, and observe a 8.76%relative improvement of the error rate over the baseline. We find the followingconditions are key for the improvement: 1) the pretraining dataset has a strongand meaningful label hierarchy, 2) its label function strongly aligns with thatof the target task, and most importantly, 3) an appropriate level ofpretraining label granularity is chosen. The importance of pretraining labelgranularity is further corroborated by our transfer learning experiments onImageNet. Most notably, we show that pretraining at the leaf labels ofImageNet21k produces better transfer results on ImageNet1k than pretraining atother coarser granularity levels, which supports the common practice.Theoretically, through an analysis on a two-layer convolutional ReLU network,we prove that: 1) models trained on coarse-grained labels only respond stronglyto the common or \"easy-to-learn\" features; 2) with the dataset satisfying theright conditions, fine-grained pretraining encourages the model to also learnrarer or \"harder-to-learn\" features well, thus improving the model'sgeneralization.", "output": "Towards Understanding the Effect of Pretraining Label Granularity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modeling sounds emitted from physical object interactions is critical forimmersive perceptual experiences in real and virtual worlds. Traditionalmethods of impact sound synthesis use physics simulation to obtain a set ofphysics parameters that could represent and synthesize the sound. However, theyrequire fine details of both the object geometries and impact locations, whichare rarely available in the real world and can not be applied to synthesizeimpact sounds from common videos. On the other hand, existing video-driven deeplearning-based approaches could only capture the weak correspondence betweenvisual content and impact sounds since they lack of physics knowledge. In thiswork, we propose a physics-driven diffusion model that can synthesizehigh-fidelity impact sound for a silent video clip. In addition to the videocontent, we propose to use additional physics priors to guide the impact soundsynthesis procedure. The physics priors include both physics parameters thatare directly estimated from noisy real-world impact sound examples withoutsophisticated setup and learned residual parameters that interpret the soundenvironment via neural networks. We further implement a novel diffusion modelwith specific training and inference strategies to combine physics priors andvisual information for impact sound synthesis. Experimental results show thatour model outperforms several existing systems in generating realistic impactsounds. More importantly, the physics-based representations are fullyinterpretable and transparent, thus enabling us to perform sound editingflexibly.", "output": "Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inspired by the long-range modeling ability of ViTs, large-kernelconvolutions are widely studied and adopted recently to enlarge the receptivefield and improve model performance, like the remarkable work ConvNeXt whichemploys 7x7 depthwise convolution. Although such depthwise operator onlyconsumes a few FLOPs, it largely harms the model efficiency on powerfulcomputing devices due to the high memory access costs. For example, ConvNeXt-Thas similar FLOPs with ResNet-50 but only achieves 60% throughputs when trainedon A100 GPUs with full precision. Although reducing the kernel size of ConvNeXtcan improve speed, it results in significant performance degradation. It isstill unclear how to speed up large-kernel-based CNN models while preservingtheir performance. To tackle this issue, inspired by Inceptions, we propose todecompose large-kernel depthwise convolution into four parallel branches alongchannel dimension, i.e. small square kernel, two orthogonal band kernels, andan identity mapping. With this new Inception depthwise convolution, we build aseries of networks, namely IncepitonNeXt, which not only enjoy high throughputsbut also maintain competitive performance. For instance, InceptionNeXt-Tachieves 1.6x higher training throughputs than ConvNeX-T, as well as attains0.2% top-1 accuracy improvement on ImageNet-1K. We anticipate InceptionNeXt canserve as an economical baseline for future architecture design to reduce carbonfootprint. Code is available at ", "output": "InceptionNeXt: When Inception Meets ConvNeXt."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Evaluating the robustness of a defense model is a challenging task inadversarial robustness research. Obfuscated gradients have previously beenfound to exist in many defense methods and cause a false signal of robustness.In this paper, we identify a more subtle situation called Imbalanced Gradientsthat can also cause overestimated adversarial robustness. The phenomenon ofimbalanced gradients occurs when the gradient of one term of the margin lossdominates and pushes the attack towards to a suboptimal direction. To exploitimbalanced gradients, we formulate a Margin Decomposition (MD) attack thatdecomposes a margin loss into individual terms and then explores theattackability of these terms separately via a two-stage process. We alsopropose a multi-targeted and ensemble version of our MD attack. Byinvestigating 24 defense models proposed since 2018, we find that 11 models aresusceptible to a certain degree of imbalanced gradients and our MD attack candecrease their robustness evaluated by the best standalone baseline attack bymore than 1%. We also provide an in-depth investigation on the likely causes ofimbalanced gradients and effective countermeasures. Our code is available at", "output": "Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The capability of a reinforcement learning (RL) agent heavily depends on thediversity of the learning scenarios generated by the environment. Generation ofdiverse realistic scenarios is challenging for real-time strategy (RTS)environments. The RTS environments are characterized by intelligententities/non-RL agents cooperating and competing with the RL agents with largestate and action spaces over a long period of time, resulting in an infinitespace of feasible, but not necessarily realistic, scenarios involving complexinteraction among different RL and non-RL agents. Yet, most of the existingsimulators rely on randomly generating the environments based on predefinedsettings/layouts and offer limited flexibility and control over the environmentdynamics for researchers to generate diverse, realistic scenarios as per theirdemand. To address this issue, for the first time, we formally introduce thebenefits of adopting an existing formal scenario specification language,SCENIC, to assist researchers to model and generate diverse scenarios in an RTSenvironment in a flexible, systematic, and programmatic manner. To showcase thebenefits, we interfaced SCENIC to an existing RTS environment Google ResearchFootball(GRF) simulator and introduced a benchmark consisting of 32 realisticscenarios, encoded in SCENIC, to train RL agents and testing theirgeneralization capabilities. We also show how researchers/RL practitioners canincorporate their domain knowledge to expedite the training process byintuitively modeling stochastic programmatic policies with SCENIC.", "output": "Scenic4RL: Programmatic Modeling and Generation of Reinforcement Learning Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many machine learning techniques incorporate identity-preservingtransformations into their models to generalize their performance to previouslyunseen data. These transformations are typically selected from a set offunctions that are known to maintain the identity of an input when applied(e.g., rotation, translation, flipping, and scaling). However, there are manynatural variations that cannot be labeled for supervision or defined throughexamination of the data. As suggested by the manifold hypothesis, many of thesenatural variations live on or near a low-dimensional, nonlinear manifold.Several techniques represent manifold variations through a set of learned Liegroup operators that define directions of motion on the manifold. However,these approaches are limited because they require transformation labels whentraining their models and they lack a method for determining which regions ofthe manifold are appropriate for applying each specific operator. We addressthese limitations by introducing a learning strategy that does not requiretransformation labels and developing a method that learns the local regionswhere each operator is likely to be used while preserving the identity ofinputs. Experiments on MNIST and Fashion MNIST highlight our model's ability tolearn identity-preserving transformations on multi-class datasets.Additionally, we train on CelebA to showcase our model's ability to learnsemantically meaningful transformations on complex datasets in an unsupervisedmanner.", "output": "Learning Identity-Preserving Transformations on Data Manifolds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks have long been used to model human intelligence, capturingelements of behavior and cognition, and their neural basis. Recent advancementsin deep learning have enabled neural network models to reach and even surpasshuman levels of intelligence in many respects, yet unlike humans, their abilityto learn new tasks quickly remains a challenge. People can reason not only infamiliar domains, but can also rapidly learn to reason through novel problemsand situations, raising the question of how well modern neural network modelscapture human intelligence and in which ways they diverge. In this work, weexplore this gap by investigating human adults' ability to learn an abstractreasoning task based on Sudoku from a brief instructional tutorial withexplanatory feedback for incorrect responses using a narrow range of trainingexamples. We find that participants who master the task do so within a smallnumber of trials and generalize well to puzzles outside of the training range.We also find that most of those who master the task can describe a validsolution strategy, and such participants perform better on transfer puzzlesthan those whose strategy descriptions are vague or incomplete. Interestingly,fewer than half of our human participants were successful in acquiring a validsolution strategy, and this ability is associated with high school mathematicseducation. We consider the challenges these findings pose for buildingcomputational models that capture all aspects of our findings and point towarda possible role for learning to engage in explanation-based reasoning tosupport rapid learning and generalization.", "output": "Systematic human learning and generalization from a brief tutorial with explanatory feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The progress of some AI paradigms such as deep learning is said to be linkedto an exponential growth in the number of parameters. There are many studiescorroborating these trends, but does this translate into an exponentialincrease in energy consumption? In order to answer this question we focus oninference costs rather than training costs, as the former account for most ofthe computing effort, solely because of the multiplicative factors. Also, apartfrom algorithmic innovations, we account for more specific and powerfulhardware (leading to higher FLOPS) that is usually accompanied with importantenergy efficiency optimisations. We also move the focus from the firstimplementation of a breakthrough paper towards the consolidated version of thetechniques one or two year later. Under this distinctive and comprehensiveperspective, we study relevant models in the areas of computer vision andnatural language processing: for a sustained increase in performance we see amuch softer growth in energy consumption than previously anticipated. The onlycaveat is, yet again, the multiplicative factor, as future AI increasespenetration and becomes more pervasive.", "output": "Compute and Energy Consumption Trends in Deep Learning Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In December 2019, a novel virus called COVID-19 had caused an enormous numberof causalities to date. The battle with the novel Coronavirus is baffling andhorrifying after the Spanish Flu 2019. While the front-line doctors and medicalresearchers have made significant progress in controlling the spread of thehighly contiguous virus, technology has also proved its significance in thebattle. Moreover, Artificial Intelligence has been adopted in many medicalapplications to diagnose many diseases, even baffling experienced doctors.Therefore, this survey paper explores the methodologies proposed that can aiddoctors and researchers in early and inexpensive methods of diagnosis of thedisease. Most developing countries have difficulties carrying out tests usingthe conventional manner, but a significant way can be adopted with Machine andDeep Learning. On the other hand, the access to different types of medicalimages has motivated the researchers. As a result, a mammoth number oftechniques are proposed. This paper first details the background knowledge ofthe conventional methods in the Artificial Intelligence domain. Following that,we gather the commonly used datasets and their use cases to date. In addition,we also show the percentage of researchers adopting Machine Learning over DeepLearning. Thus we provide a thorough analysis of this scenario. Lastly, in theresearch challenges, we elaborate on the problems faced in COVID-19 research,and we address the issues with our understanding to build a bright and healthyenvironment.", "output": "The Prominence of Artificial Intelligence in COVID-19."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Logistic regression training over encrypted data has been an attractive ideato security concerns for years. In this paper, we propose a faster gradientvariant called $texttt{quadratic gradient}$ to implement logistic regressiontraining in a homomorphic encryption domain, the core of which can be seen asan extension of the simplified fixed Hessian.We enhance Nesterov's accelerated gradient (NAG) and Adaptive GradientAlgorithm (Adagrad) respectively with this gradient variant and evaluate theenhanced algorithms on several datasets. Experimental results show that theenhanced methods have a state-of-the-art performance in convergence speedcompared to the naive first-order gradient methods. We then adopt the enhancedNAG method to implement homomorphic logistic regression training and obtain acomparable result by only $3$ iterations.", "output": "Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In some scenarios, classifier requires detecting out-of-distribution samplesfar from its training data. With desirable characteristics, reconstructionautoencoder-based methods deal with this problem by using input reconstructionerror as a metric of novelty vs. normality. We formulate the essence of suchapproach as a quadruplet domain translation with an intrinsic bias to onlyquery for a proxy of conditional data uncertainty. Accordingly, an improvementdirection is formalized as maximumly compressing the autoencoder's latent spacewhile ensuring its reconstructive power for acting as a described domaintranslator. From it, strategies are introduced including semanticreconstruction, data certainty decomposition and normalized L2 distance tosubstantially improve original methods, which together establishstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR ofCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our methodworks without any additional data, hard-to-implement structure, time-consumingpipeline, and even harming the classification accuracy of known classes.", "output": "Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) allows multiple medical institutions tocollaboratively learn a global model without centralizing client data. It isdifficult, if possible at all, for such a global model to commonly achieveoptimal performance for each individual client, due to the heterogeneity ofmedical images from various scanners and patient demographics. This problembecomes even more significant when deploying the global model to unseen clientsoutside the FL with unseen distributions not presented during federatedtraining. To optimize the prediction accuracy of each individual client formedical imaging tasks, we propose a novel unified framework for bothtextit{Inside and Outside model Personalization in FL} (IOP-FL). Our insidepersonalization uses a lightweight gradient-based approach that exploits thelocal adapted model for each client, by accumulating both the global gradientsfor common knowledge and the local gradients for client-specific optimization.Moreover, and importantly, the obtained local personalized models and theglobal model can form a diverse and informative routing space to personalize anadapted model for outside FL clients. Hence, we design a new test-time routingscheme using the consistency loss with a shape constraint to dynamicallyincorporate the models, given the distribution information conveyed by the testdata. Our extensive experimental results on two medical image segmentationtasks present significant improvements over SOTA methods on both inside andoutside personalization, demonstrating the potential of our IOP-FL scheme forclinical practice.", "output": "IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Block-based visual programming environments are increasingly used tointroduce computing concepts to beginners. Given that programming tasks areopen-ended and conceptual, novice students often struggle when learning inthese environments. AI-driven programming tutors hold great promise inautomatically assisting struggling students, and need several components torealize this potential. We investigate the crucial component of studentmodeling, in particular, the ability to automatically infer students'misconceptions for predicting (synthesizing) their behavior. We introduce anovel benchmark, StudentSyn, centered around the following challenge: For agiven student, synthesize the student's attempt on a new target task afterobserving the student's attempt on a fixed reference task. This challenge isakin to that of program synthesis; however, instead of synthesizing a{solution} (i.e., program an expert would write), the goal here is tosynthesize a {student attempt} (i.e., program that a given student wouldwrite). We first show that human experts (TutorSS) can achieve high performanceon the benchmark, whereas simple baselines perform poorly. Then, we develop twoneuro/symbolic techniques (NeurSS and SymSS) in a quest to close this gap withTutorSS.", "output": "From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A deep equilibrium model (DEQ) is implicitly defined through an equilibriumpoint of an infinite-depth weight-tied model with an input-injection. Insteadof infinite computations, it solves an equilibrium point directly withroot-finding and computes gradients with implicit differentiation. The trainingdynamics of over-parameterized DEQs are investigated in this study. Bysupposing a condition on the initial equilibrium point, we show that the uniqueequilibrium point always exists during the training process, and the gradientdescent is proved to converge to a globally optimal solution at a linearconvergence rate for the quadratic loss function. In order to show that therequired initial condition is satisfied via mild over-parameterization, weperform a fine-grained analysis on random DEQs. We propose a novelprobabilistic framework to overcome the technical difficulty in thenon-asymptotic analysis of infinite-depth weight-tied models.", "output": "Global Convergence of Over-parameterized Deep Equilibrium Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a linear contextual bandit algorithm with $O(sqrt{dTlog T})$regret bound, where $d$ is the dimension of contexts and $T$ isthe timehorizon. Our proposed algorithm is equipped with a novel estimator in whichexploration is embedded through explicit randomization. Depending on therandomization, our proposed estimator takes contributions either from contextsof all arms or from selected contexts. We establish a self-normalized bound forour estimator, which allows a novel decomposition of the cumulative regret intotextit{additive} dimension-dependent terms instead of multiplicative terms. Wealso prove a novel lower bound of $Omega(sqrt{dT})$ under our problemsetting. Hence, the regret of our proposed algorithm matches the lower bound upto logarithmic factors. The numerical experiments support the theoreticalguarantees and show that our proposed method outperforms the existing linearbandit algorithms.", "output": "Squeeze All: Novel Estimator and Self-Normalized Bound for Linear Contextual Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Next generation cellular networks will implement radio sensing functionsalongside customary communications, thereby enabling unprecedented worldwidesensing coverage outdoors. Deep learning has revolutionised computer vision buthas had limited application to radio perception tasks, in part due to lack ofsystematic datasets and benchmarks dedicated to the study of the performanceand promise of radio sensing. To address this gap, we present MaxRay: asynthetic radio-visual dataset and benchmark that facilitate precise targetlocalisation in radio. We further propose to learn to localise targets in radiowithout supervision by extracting self-coordinates from radio-visualcorrespondence. We use such self-supervised coordinates to train a radiolocaliser network. We characterise our performance against a number ofstate-of-the-art baselines. Our results indicate that accurate radio targetlocalisation can be automatically learned from paired radio-visual data withoutlabels, which is important for empirical data. This opens the door for vastdata scalability and may prove key to realising the promise of robust radiosensing atop a unified communication-perception cellular infrastructure.Dataset will be hosted on IEEE DataPort.", "output": "Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Compressed Vertical Federated Learning (C-VFL) forcommunication-efficient training on vertically partitioned data. In C-VFL, aserver and multiple parties collaboratively train a model on their respectivefeatures utilizing several local iterations and sharing compressed intermediateresults periodically. Our work provides the first theoretical analysis of theeffect message compression has on distributed training over verticallypartitioned data. We prove convergence of non-convex objectives at a rate of$O(frac{1}{sqrt{T}})$ when the compression error is bounded over the courseof training. We provide specific requirements for convergence with commoncompression techniques, such as quantization and top-$k$ sparsification.Finally, we experimentally show compression can reduce communication by over$90%$ without a significant decrease in accuracy over VFL without compression.", "output": "Compressed-VFL: Communication-Efficient Learning with Vertically Partitioned Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous vehicles use a variety of sensors and machine-learned models topredict the behavior of surrounding road users. Most of the machine-learnedmodels in the literature focus on quantitative error metrics like the root meansquare error (RMSE) to learn and report their models' capabilities. This focuson quantitative error metrics tends to ignore the more important behavioralaspect of the models, raising the question of whether these models reallypredict human-like behavior. Thus, we propose to analyze the output ofmachine-learned models much like we would analyze human data in conventionalbehavioral research. We introduce quantitative metrics to demonstrate presenceof three different behavioral phenomena in a naturalistic highway drivingdataset: 1) The kinematics-dependence of who passes a merging point first 2)Lane change by an on-highway vehicle to accommodate an on-ramp vehicle 3) Lanechanges by vehicles on the highway to avoid lead vehicle conflicts. Then, weanalyze the behavior of three machine-learned models using the same metrics.Even though the models' RMSE value differed, all the models captured thekinematic-dependent merging behavior but struggled at varying degrees tocapture the more nuanced courtesy lane change and highway lane change behavior.Additionally, the collision aversion analysis during lane changes showed thatthe models struggled to capture the physical aspect of human driving: leavingadequate gap between the vehicles. Thus, our analysis highlighted theinadequacy of simple quantitative metrics and the need to take a broaderbehavioral perspective when analyzing machine-learned models of human drivingpredictions.", "output": "Beyond RMSE: Do machine-learned models of road user interaction produce human-like behavior?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In order for artificial agents to successfully perform tasks in changingenvironments, they must be able to both detect and adapt to novelty. However,visual novelty detection research often only evaluates on repurposed datasetssuch as CIFAR-10 originally intended for object classification, where imagesfocus on one distinct, well-centered object. New benchmarks are needed torepresent the challenges of navigating the complex scenes of an open world. Ournew NovelCraft dataset contains multimodal episodic data of the images andsymbolic world-states seen by an agent completing a pogo stick assembly taskwithin a modified Minecraft environment. In some episodes, we insert novelobjects of varying size within the complex 3D scene that may impact gameplay.Our visual novelty detection benchmark finds that methods that rank best onpopular area-under-the-curve metrics may be outperformed by simpleralternatives when controlling false positives matters most. Further multimodalnovelty detection experiments suggest that methods that fuse both visual andsymbolic information can improve time until detection as well as overalldiscrimination. Finally, our evaluation of recent generalized categorydiscovery methods suggests that adapting to new imbalanced categories incomplex scenes remains an exciting open problem.", "output": "NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep recommender systems (DRS) are intensively applied in modern webservices. To deal with the massive web contents, DRS employs a two-stageworkflow: retrieval and ranking, to generate its recommendation results. Theretriever aims to select a small set of relevant candidates from the entireitems with high efficiency; while the ranker, usually more precise buttime-consuming, is supposed to further refine the best items from the retrievedcandidates. Traditionally, the two components are trained either independentlyor within a simple cascading pipeline, which is prone to poor collaborationeffect. Though some latest works suggested to train retriever and rankerjointly, there still exist many severe limitations: item distribution shiftbetween training and inference, false negative, and misalignment of rankingorder. As such, it remains to explore effective collaborations betweenretriever and ranker.", "output": "Cooperative Retriever and Ranker in Deep Recommenders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper provides a finite-time analysis of linear stochastic approximation(LSA) algorithms with fixed step size, a core method in statistics and machinelearning. LSA is used to compute approximate solutions of a $d$-dimensionallinear system $bar{mathbf{A}} theta = bar{mathbf{b}}$ for which$(bar{mathbf{A}}, bar{mathbf{b}})$ can only be estimated by(asymptotically) unbiased observations${(mathbf{A}(Z_n),mathbf{b}(Z_n))}_{n in mathbb{N}}$. We consider herethe case where ${Z_n}_{n in mathbb{N}}$ is an i.i.d. sequence or auniformly geometrically ergodic Markov chain. We derive $p$-th moment andhigh-probability deviation bounds for the iterates defined by LSA and itsPolyak-Ruppert-averaged version. Our finite-time instance-dependent bounds forthe averaged LSA iterates are sharp in the sense that the leading term weobtain coincides with the local asymptotic minimax limit. Moreover, theremainder terms of our bounds admit a tight dependence on the mixing time$t_{operatorname{mix}}$ of the underlying chain and the norm of the noisevariables. We emphasize that our result requires the SA step size to scale onlywith logarithm of the problem dimension $d$.", "output": "Finite-time High-probability Bounds for Polyak-Ruppert Averaged Iterates of Linear Stochastic Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multinomial logistic regression, also known by other names such as multiclasslogistic regression and softmax regression, is a fundamental classificationmethod that generalizes binary logistic regression to multiclass problems. Arecently work proposed a faster gradient called $texttt{quadratic gradient}$that can accelerate the binary logistic regression training, and presented anenhanced Nesterov's accelerated gradient (NAG) method for binary logisticregression.In this paper, we extend this work to multiclass logistic regression andpropose an enhanced Adaptive Gradient Algorithm (Adagrad) that can acceleratethe original Adagrad method. We test the enhanced NAG method and the enhancedAdagrad method on some multiclass-problem datasets. Experimental results showthat both enhanced methods converge faster than their original onesrespectively.", "output": "Multinomial Logistic Regression Algorithms via Quadratic Gradient."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent research has revealed that Graph Neural Networks (GNNs) aresusceptible to adversarial attacks targeting the graph structure. A maliciousattacker can manipulate a limited number of edges, given the training labels,to impair the victim model's performance. Previous empirical studies indicatethat gradient-based attackers tend to add edges rather than remove them. Inthis paper, we present a theoretical demonstration revealing that attackerstend to increase inter-class edges due to the message passing mechanism ofGNNs, which explains some previous empirical observations. By connectingdissimilar nodes, attackers can more effectively corrupt node features, makingsuch attacks more advantageous. However, we demonstrate that the inherentsmoothness of GNN's message passing tends to blur node dissimilarity in thefeature space, leading to the loss of crucial information during the forwardprocess. To address this issue, we propose a novel surrogate model withmulti-level propagation that preserves the node dissimilarity information. Thismodel parallelizes the propagation of unaggregated raw features and multi-hopaggregated features, while introducing batch normalization to enhance thedissimilarity in node representations and counteract the smoothness resultingfrom topological aggregation. Our experiments show significant improvement withour approach.Furthermore, both theoretical and experimental evidence suggestthat adding inter-class edges constitutes an easily observable attack pattern.We propose an innovative attack loss that balances attack effectiveness andimperceptibility, sacrificing some attack effectiveness to attain greaterimperceptibility. We also provide experiments to validate the compromiseperformance achieved through this attack loss.", "output": "What Does the Gradient Tell When Attacking the Graph Structure."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It might be inadequate for the line search technique for Newton's method touse only one floating point number. A column vector of the same size as thegradient might be better than a mere float number to accelerate each of thegradient elements with different rates. Moreover, a square matrix of the sameorder as the Hessian matrix might be helpful to correct the Hessian matrix.Chiang applied something between a column vector and a square matrix, namely adiagonal matrix, to accelerate the gradient and further proposed a fastergradient variant called quadratic gradient. In this paper, we present a new wayto build a new version of the quadratic gradient. This new quadratic gradientdoesn't satisfy the convergence conditions of the fixed Hessian Newton'smethod. However, experimental results show that it sometimes has a betterperformance than the original one in convergence rate. Also, Chiang speculatesthat there might be a relation between the Hessian matrix and the learning ratefor the first-order gradient descent method. We prove that the floating number$frac{1}{epsilon + max {| lambda_i | }}$ can be a good learning rate ofthe gradient methods, where $epsilon$ is a number to avoid division by zeroand $lambda_i$ the eigenvalues of the Hessian matrix.", "output": "Quadratic Gradient: Combining Gradient Algorithms and Newton's Method as One."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning from Demonstration (LfD) approaches empower end-users to teachrobots novel tasks via demonstrations of the desired behaviors, democratizingaccess to robotics. However, current LfD frameworks are not capable of fastadaptation to heterogeneous human demonstrations nor the large-scale deploymentin ubiquitous robotics applications. In this paper, we propose a novel LfDframework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Ourapproach (1) leverages learned strategies to construct policy mixtures for fastadaptation to new demonstrations, allowing for quick end-user personalization,(2) distills common knowledge across demonstrations, achieving accurate taskinference; and (3) expands its model only when needed in lifelong deployments,maintaining a concise set of prototypical strategies that can approximate allbehaviors via policy mixtures. We empirically validate that FLAIR achievesadaptability (i.e., the robot adapts to heterogeneous, user-specific taskpreferences), efficiency (i.e., the robot achieves sample-efficientadaptation), and scalability (i.e., the model grows sublinearly with the numberof demonstrations while maintaining high performance). FLAIR surpassesbenchmarks across three control tasks with an average 57% improvement in policyreturns and an average 78% fewer episodes required for demonstration modelingusing policy mixtures. Finally, we demonstrate the success of FLAIR in a tabletennis task and find users rate FLAIR as having higher task (p&lt;.05) andpersonalization (p&lt;.05) performance.", "output": "Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The main purpose of this study is to develop a pipeline for COVID-19detection from a big and challenging database of Computed Tomography (CT)images. The proposed pipeline includes a segmentation part, a lung extractionpart, and a classifier part. Optional slice removal techniques after UNet-basedsegmentation of slices were also tried. The methodologies tried in thesegmentation part are traditional segmentation methods as well as UNet-basedmethods. In the classification part, a Convolutional Neural Network (CNN) wasused to take the final diagnosis decisions. In terms of the results: in thesegmentation part, the proposed segmentation methods show high dice scores on apublicly available dataset. In the classification part, the results werecompared at slice-level and at patient-level as well. At slice-level, methodswere compared and showed high validation accuracy indicating efficiency inpredicting 2D slices. At patient level, the proposed methods were also comparedin terms of validation accuracy and macro F1 score on the validation set. Thedataset used for classification is COV-19CT Database. The method proposed hereshowed improvement from our precious results on the same dataset. InConclusion, the improved work in this paper has potential clinical usages forCOVID-19 detection and diagnosis via CT images. The code is on github at", "output": "COVID-19 Detection Using Segmentation, Region Extraction and Classification Pipeline."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the success of pretraining techniques in representation learning, anumber of continual learning methods based on pretrained models have beenproposed. Some of these methods design continual learning mechanisms on thepre-trained representations and only allow minimum updates or even no updatesof the backbone models during the training of continual learning. In thispaper, we question whether the complexity of these models is needed to achievegood performance by comparing them to a simple baseline that we designed. Weargue that the pretrained feature extractor itself can be strong enough toachieve a competitive or even better continual learning performance onSplit-CIFAR100 and CoRe 50 benchmarks. To validate this, we conduct a verysimple baseline that 1) use the frozen pretrained model to extract imagefeatures for every class encountered during the continual learning stage andcompute their corresponding mean features on training data, and 2) predict theclass of the input based on the nearest neighbor distance between test samplesand mean features of the classes; i.e., Nearest Mean Classifier (NMC). Thisbaseline is single-headed, exemplar-free, and can be task-free (by updating themeans continually). This baseline achieved 88.53% on 10-Split-CIFAR-100,surpassing most state-of-the-art continual learning methods that are allinitialized using the same pretrained transformer model. We hope our baselinemay encourage future progress in designing learning systems that cancontinually add quality to the learning representations even if they startedfrom some pretrained weights.", "output": "A Simple Baseline that Questions the Use of Pretrained-Models in Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The mixed multinomial logit model assumes constant preference parameters of adecision-maker throughout different choice situations, which may be consideredtoo strong for certain choice modelling applications. This paper proposes aneffective approach to model context-dependent intra-respondent heterogeneity,thereby introducing the concept of the Context-aware Bayesian mixed multinomiallogit model, where a neural network maps contextual information tointerpretable shifts in the preference parameters of each individual in eachchoice occasion. The proposed model offers several key advantages. First, itsupports both continuous and discrete variables, as well as complex non-linearinteractions between both types of variables. Secondly, each contextspecification is considered jointly as a whole by the neural network ratherthan each variable being considered independently. Finally, since the neuralnetwork parameters are shared across all decision-makers, it can leverageinformation from other decision-makers to infer the effect of a particularcontext on a particular decision-maker. Even though the context-aware Bayesianmixed multinomial logit model allows for flexible interactions betweenattributes, the increase in computational complexity is minor, compared to themixed multinomial logit model. We illustrate the concept and interpretation ofthe proposed model in a simulation study. We furthermore present a real-worldcase study from the travel behaviour domain - a bicycle route choice model,based on a large-scale, crowdsourced dataset of GPS trajectories including119,448 trips made by 8,555 cyclists.", "output": "Context-aware Bayesian Mixed Multinomial Logit Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study the transferability of ImageNet spatial and Kineticsspatio-temporal representations to multi-label Movie Trailer GenreClassification (MTGC). In particular, we present an extensive evaluation of thetransferability of ConvNet and Transformer models pretrained on ImageNet andKinetics to Trailers12k, a new manually-curated movie trailer dataset composedof 12,000 videos labeled with 10 different genres and associated metadata. Weanalyze different aspects that can influence transferability, such as framerate, input video extension, and spatio-temporal modeling. In order to reducethe spatio-temporal structure gap between ImageNet/Kinetics and Trailers12k, wepropose Dual Image and Video Transformer Architecture (DIViTA), which performsshot detection so as to segment the trailer into highly correlated clips,providing a more cohesive input for pretrained backbones and improvingtransferability (a 1.83% increase for ImageNet and 3.75% for Kinetics). Ourresults demonstrate that representations learned on either ImageNet or Kineticsare comparatively transferable to Trailers12k. Moreover, both datasets providecomplementary information that can be combined to improve classificationperformance (a 2.91% gain compared to the top single pretraining).Interestingly, using lightweight ConvNets as pretrained backbones resulted inonly a 3.46% drop in classification performance compared with the topTransformer while requiring only 11.82% of its parameters and 0.81% of itsFLOPS.", "output": "Improving Transfer Learning with a Dual Image and Video Transformer for Multi-label Movie Trailer Genre Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image completion is a task that aims to fill in the missing region of amasked image with plausible contents. However, existing image completionmethods tend to fill in the missing region with the surrounding texture insteadof hallucinating a visual instance that is suitable in accordance with thecontext of the scene. In this work, we propose a novel image completion model,dubbed ImComplete, that hallucinates the missing instance that harmonizes wellwith - and thus preserves - the original context. ImComplete first adopts atransformer architecture that considers the visible instances and the locationof the missing region. Then, ImComplete completes the semantic segmentationmasks within the missing region, providing pixel-level semantic and structuralguidance. Finally, the image synthesis blocks generate photo-realistic content.We perform a comprehensive evaluation of the results in terms of visual quality(LPIPS and FID) and contextual preservation scores (CLIPscore and objectdetection accuracy) with COCO-panoptic and Visual Genome datasets. Experimentalresults show the superiority of ImComplete on various natural images.", "output": "Instance-Aware Image Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper considers binary classification of high-dimensional features undera postulated model with a low-dimensional latent Gaussian mixture structure andnon-vanishing noise. A generalized least squares estimator is used to estimatethe direction of the optimal separating hyperplane. The estimated hyperplane isshown to interpolate on the training data. While the direction vector can beconsistently estimated as could be expected from recent results in linearregression, a naive plug-in estimate fails to consistently estimate theintercept. A simple correction, that requires an independent hold-out sample,renders the procedure minimax optimal in many scenarios. The interpolationproperty of the latter procedure can be retained, but surprisingly depends onthe way the labels are encoded.", "output": "Interpolating Discriminant Functions in High-Dimensional Gaussian Latent Mixtures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present new benchmarks on evaluation code generation models: MBXP andMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programminglanguages and are generated using a scalable conversion framework thattranspiles prompts and test cases from the original Python datasets into thecorresponding data in the target language. Using these benchmarks, we are ableto assess the performance of code generation models in a multi-lingual fashion,and discovered generalization ability of language models on out-of-domainlanguages, advantages of multi-lingual models over mono-lingual, the ability offew-shot prompting to teach the model new languages, and zero-shot translationabilities even on mono-lingual settings. Furthermore, we use our codegeneration model to perform large-scale bootstrapping to obtain syntheticcanonical solutions in several languages, which can be used for othercode-related evaluations such as code insertion, robustness, or summarizationtasks. Overall, our benchmarks represents a significant step towards a deeperunderstanding of language models' code generation abilities. We publiclyrelease our code and datasets at ", "output": "Multi-lingual Evaluation of Code Generation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Great endeavors have been made to study AI's ability in abstract reasoning,along with which different versions of RAVEN's progressive matrices (RPM) areproposed as benchmarks. Previous works give inkling that without sophisticateddesign or extra meta-data containing semantic information, neural networks maystill be indecisive in making decisions regarding RPM problems, afterrelentless training. Evidenced by thorough experiments and ablation studies, weshowcase that end-to-end neural networks embodied with felicitous inductivebias, intentionally design or serendipitously match, can solve RPM problemselegantly, without the augment of any extra meta-data or preferences of anyspecific backbone. Our work also reveals that multi-viewpoint withmulti-evaluation is a key learning strategy for successful reasoning. Finally,potential explanations for the failure of connectionist models ingeneralization are provided. We hope that these results will serve asinspections of AI's ability beyond perception and toward abstract reasoning.Source code can be found in ", "output": "Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Yang et al. (2023) recently addressed the open problem of solving VariationalInequalities (VIs) with equality and inequality constraints through afirst-order gradient method. However, the proposed primal-dual method calledACVI is applicable when we can compute analytic solutions of its subproblems;thus, the general case remains an open problem. In this paper, we adopt awarm-starting technique where we solve the subproblems approximately at eachiteration and initialize the variables with the approximate solution found atthe previous iteration. We prove its convergence and show that the gap functionof the last iterate of this inexact-ACVI method decreases at a rate of$mathcal{O}(frac{1}{sqrt{K}})$ when the operator is $L$-Lipschitz andmonotone, provided that the errors decrease at appropriate rates.Interestingly, we show that often in numerical experiments, this techniqueconverges faster than its exact counterpart. Furthermore, for the cases whenthe inequality constraints are simple, we propose a variant of ACVI namedP-ACVI and prove its convergence for the same setting. We further demonstratethe efficacy of the proposed methods through numerous experiments. We alsorelax the assumptions in Yang et al., yielding, to our knowledge, the firstconvergence result that does not rely on the assumption that the operator is$L$-Lipschitz. Our source code is provided at$texttt{", "output": "A Primal-dual Approach for Solving Variational Inequalities with General-form Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have been reported to have strong performance onnatural language processing tasks. However, performance metrics such asaccuracy do not measure the quality of the model in terms of its ability torobustly represent complex linguistic structure. In this paper, focusing on theability of language models to represent syntax, we propose a framework toassess the consistency and robustness of linguistic representations. To thisend, we introduce measures of robustness of neural network models that leveragerecent advances in extracting linguistic constructs from LLMs via probingtasks, i.e., simple tasks used to extract meaningful information about a singlefacet of a language model, such as syntax reconstruction and rootidentification. Empirically, we study the performance of four LLMs across sixdifferent corpora on the proposed robustness measures by analysing theirperformance and robustness with respect to syntax-preserving perturbations. Weprovide evidence that context-free representation (e.g., GloVe) are in somecases competitive with context-dependent representations from modern LLMs(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our keyobservation is that emergent syntactic representations in neural networks arebrittle. We make the code, trained models and logs available to the communityas a contribution to the debate about the capabilities of LLMs.", "output": "Emergent Linguistic Structures in Neural Networks are Fragile."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a novel deep reinforcement learning (DRL) approach to jointlyoptimize transmit beamforming and reconfigurable intelligent surface (RIS)phase shifts in a multiuser multiple input single output (MU-MISO) system tomaximize the sum downlink rate under the phase-dependent reflection amplitudemodel. Our approach addresses the challenge of imperfect channel stateinformation (CSI) and hardware impairments by considering a practical RISamplitude model. We compare the performance of our approach against a vanillaDRL agent in two scenarios: perfect CSI and phase-dependent RIS amplitudes, andmismatched CSI and ideal RIS reflections. The results demonstrate that theproposed framework significantly outperforms the vanilla DRL agent undermismatch and approaches the golden standard. Our contributions includemodifications to the DRL approach to address the joint design of transmitbeamforming and phase shifts and the phase-dependent amplitude model. To thebest of our knowledge, our method is the first DRL-based approach for thephase-dependent reflection amplitude model in RIS-aided MU-MISO systems. Ourfindings in this study highlight the potential of our approach as a promisingsolution to overcome hardware impairments in RIS-aided wireless communicationsystems.", "output": "Deep Reinforcement Learning Based Joint Downlink Beamforming and RIS Configuration in RIS-aided MU-MISO Systems Under Hardware Impairments and Imperfect CSI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Environmental sensors are crucial for monitoring weather conditions and theimpacts of climate change. However, it is challenging to maximise measurementinformativeness and place sensors efficiently, particularly in remote regionslike Antarctica. Probabilistic machine learning models can evaluate placementinformativeness by predicting the uncertainty reduction provided by a newsensor. Gaussian process (GP) models are widely used for this purpose, but theystruggle with capturing complex non-stationary behaviour and scaling to largedatasets. This paper proposes using a convolutional Gaussian neural process(ConvGNP) to address these issues. A ConvGNP uses neural networks toparameterise a joint Gaussian distribution at arbitrary target locations,enabling flexibility and scalability. Using simulated surface air temperatureanomaly over Antarctica as ground truth, the ConvGNP learns spatial andseasonal non-stationarities, outperforming a non-stationary GP baseline. In asimulated sensor placement experiment, the ConvGNP better predicts theperformance boost obtained from new observations than GP baselines, leading tomore informative sensor placements. We contrast our approach with physics-basedsensor placement methods and propose future work towards an operational sensorplacement recommendation system. This system could help to realiseenvironmental digital twins that actively direct measurement sampling toimprove the digital representation of reality.", "output": "Environmental Sensor Placement with Convolutional Gaussian Neural Processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing graph contrastive learning (GCL) techniques typically require twoforward passes for a single instance to construct the contrastive loss, whichis effective for capturing the low-frequency signals of node features. Such adual-pass design has shown empirical success on homophilic graphs, but itseffectiveness on heterophilic graphs, where directly connected nodes typicallyhave different labels, is unknown. In addition, existing GCL approaches fail toprovide strong performance guarantees. Coupled with the unpredictability of GCLapproaches on heterophilic graphs, their applicability in real-world contextsis limited. Then, a natural question arises: Can we design a GCL method thatworks for both homophilic and heterophilic graphs with a performance guarantee?To answer this question, we theoretically study the concentration property offeatures obtained by neighborhood aggregation on homophilic and heterophilicgraphs, introduce the single-pass graph contrastive learning loss based on theproperty, and provide performance guarantees for the minimizer of the loss ondownstream tasks. As a direct consequence of our analysis, we implement theSingle-Pass Graph Contrastive Learning method (SP-GCL). Empirically, on 14benchmark datasets with varying degrees of homophily, the features learned bythe SP-GCL can match or outperform existing strong baselines with significantlyless computational overhead, which demonstrates the usefulness of our findingsin real-world cases.", "output": "Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph convolutional networks (GCNs) is a class of artificial neural networksfor processing data that can be represented as graphs. Since financialtransactions can naturally be constructed as graphs, GCNs are widely applied inthe financial industry, especially for financial fraud detection. In thispaper, we focus on fraud detection on cryptocurrency truct networks. In theliterature, most works focus on static networks. Whereas in this study, weconsider the evolving nature of cryptocurrency networks, and use localstructural as well as the balance theory to guide the training process. Morespecifically, we compute motif matrices to capture the local topologicalinformation, then use them in the GCN aggregation process. The generatedembedding at each snapshot is a weighted average of embeddings within a timewindow, where the weights are learnable parameters. Since the trust networks issigned on each edge, balance theory is used to guide the training process.Experimental results on bitcoin-alpha and bitcoin-otc datasets show that theproposed model outperforms those in the literature.", "output": "Motif-aware temporal GCN for fraud detection in signed cryptocurrency trust networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A recurrent neural network (RNN) is a widely used deep-learning network fordealing with sequential data. Imitating a dynamical system, an infinite-widthRNN can approximate any open dynamical system in a compact domain. In general,deep networks with bounded widths are more effective than wide networks inpractice; however, the universal approximation theorem for deep narrowstructures has yet to be extensively studied. In this study, we prove theuniversality of deep narrow RNNs and show that the upper bound of the minimumwidth for universality can be independent of the length of the data.Specifically, we show that a deep RNN with ReLU activation can approximate anycontinuous function or $L^p$ function with the widths $d_x+d_y+2$ and$max{d_x+1,d_y}$, respectively, where the target function maps a finitesequence of vectors in $mathbb{R}^{d_x}$ to a finite sequence of vectors in$mathbb{R}^{d_y}$. We also compute the additional width required if theactivation function is $tanh$ or more. In addition, we prove the universalityof other recurrent networks, such as bidirectional RNNs. Bridging a multi-layerperceptron and an RNN, our theory and proof technique can be an initial steptoward further research on deep RNNs.", "output": "Minimal Width for Universal Property of Deep RNN."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A recent focal area in the space of graph neural networks (GNNs) is graphself-supervised learning (SSL), which aims to derive useful noderepresentations without labeled data. Notably, many state-of-the-art graph SSLmethods are contrastive methods, which use a combination of positive andnegative samples to learn node representations. Owing to challenges in negativesampling (slowness and model sensitivity), recent literature introducednon-contrastive methods, which instead only use positive samples. Though suchmethods have shown promising performance in node-level tasks, their suitabilityfor link prediction tasks, which are concerned with predicting link existencebetween pairs of nodes (and have broad applicability to recommendation systemscontexts) is yet unexplored. In this work, we extensively evaluate theperformance of existing non-contrastive methods for link prediction in bothtransductive and inductive settings. While most existing non-contrastivemethods perform poorly overall, we find that, surprisingly, BGRL generallyperforms well in transductive settings. However, it performs poorly in the morerealistic inductive settings where the model has to generalize to links to/fromunseen nodes. We find that non-contrastive models tend to overfit to thetraining graph and use this analysis to propose T-BGRL, a novel non-contrastiveframework that incorporates cheap corruptions to improve the generalizationability of the model. This simple modification strongly improves inductiveperformance in 5/6 of our datasets, with up to a 120% improvement inHits@50--all with comparable speed to other non-contrastive baselines and up to14x faster than the best-performing contrastive baseline. Our work impartsinteresting findings about non-contrastive learning for link prediction andpaves the way for future researchers to further expand upon this area.", "output": "Link Prediction with Non-Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Changing how pre-trained models behave -- e.g., improving their performanceon a downstream task or mitigating biases learned during pre-training -- is acommon practice when developing machine learning systems. In this work, wepropose a new paradigm for steering the behavior of neural networks, centeredaround textit{task vectors}. A task vector specifies a direction in the weightspace of a pre-trained model, such that movement in that direction improvesperformance on the task. We build task vectors by subtracting the weights of apre-trained model from the weights of the same model after fine-tuning on atask. We show that these task vectors can be modified and combined togetherthrough arithmetic operations such as negation and addition, and the behaviorof the resulting model is steered accordingly. Negating a task vector decreasesperformance on the target task, with little change in model behavior on controltasks. Moreover, adding task vectors together can improve performance onmultiple tasks at once. Finally, when tasks are linked by an analogyrelationship of the form ``A is to B as C is to D\", combining task vectors fromthree of the tasks can improve performance on the fourth, even when no datafrom the fourth task is used for training. Overall, our experiments withseveral models, modalities and tasks show that task arithmetic is a simple,efficient and effective way of editing models.", "output": "Editing Models with Task Arithmetic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hopfield networks and Boltzmann machines (BMs) are fundamental energy-basedneural network models. Recent studies on modern Hopfield networks have broadenthe class of energy functions and led to a unified perspective on generalHopfield networks including an attention module. In this letter, we considerthe BM counterparts of modern Hopfield networks using the associated energyfunctions, and study their salient properties from a trainability perspective.In particular, the energy function corresponding to the attention modulenaturally introduces a novel BM, which we refer to as the attentional BM(AttnBM). We verify that AttnBM has a tractable likelihood function andgradient for certain special cases and is easy to train. Moreover, we revealthe hidden connections between AttnBM and some single-layer models, namely theGaussian--Bernoulli restricted BM and the denoising autoencoder with softmaxunits coming from denoising score matching. We also investigate BMs introducedby other energy functions and show that the energy function of denseassociative memory models gives BMs belonging to Exponential Family Harmoniums.", "output": "Attention in a family of Boltzmann machines emerging from modern Hopfield networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Applying deep learning concepts from image detection and graph theory hasgreatly advanced protein-ligand binding affinity prediction, a challenge withenormous ramifications for both drug discovery and protein engineering. Webuild upon these advances by designing a novel deep learning architectureconsisting of a 3-dimensional convolutional neural network utilizingchannel-wise attention and two graph convolutional networks utilizingattention-based aggregation of node features. HAC-Net (Hybrid Attention-BasedConvolutional Neural Network) obtains state-of-the-art results on the PDBbindv.2016 core set, the most widely recognized benchmark in the field. Weextensively assess the generalizability of our model using multiple train-testsplits, each of which maximizes differences between either protein structures,protein sequences, or ligand extended-connectivity fingerprints of complexes inthe training and test sets. Furthermore, we perform 10-fold cross-validationwith a similarity cutoff between SMILES strings of ligands in the training andtest sets, and also evaluate the performance of HAC-Net on lower-quality data.We envision that this model can be extended to a broad range of supervisedlearning problems related to structure-based biomolecular property prediction.All of our software is available as open source at and the HACNet Python package isavailable through PyPI.", "output": "HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for Highly Accurate Protein-Ligand Binding Affinity Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we improve the kernel alignment regret bound for online kernellearning in the regime of the Hinge loss function. Previous algorithm achievesa regret of $O((mathcal{A}_TTln{T})^{frac{1}{4}})$ at a computationalcomplexity (space and per-round time) of $O(sqrt{mathcal{A}_TTln{T}})$,where $mathcal{A}_T$ is called textit{kernel alignment}. We propose analgorithm whose regret bound and computational complexity are better thanprevious results. Our results depend on the decay rate of eigenvalues of thekernel matrix. If the eigenvalues of the kernel matrix decay exponentially,then our algorithm enjoys a regret of $O(sqrt{mathcal{A}_T})$ at acomputational complexity of $O(ln^2{T})$. Otherwise, our algorithm enjoys aregret of $O((mathcal{A}_TT)^{frac{1}{4}})$ at a computational complexity of$O(sqrt{mathcal{A}_TT})$. We extend our algorithm to batch learning andobtain a $O(frac{1}{T}sqrt{mathbb{E}[mathcal{A}_T]})$ excess risk boundwhich improves the previous $O(1/sqrt{T})$ bound.", "output": "Improved Kernel Alignment Regret Bound for Online Kernel Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic Time Warping (DTW) is a popular time series distance measure thataligns the points in two series with one another. These alignments supportwarping of the time dimension to allow for processes that unfold at differingrates. The distance is the minimum sum of costs of the resulting alignmentsover any allowable warping of the time dimension. The cost of an alignment oftwo points is a function of the difference in the values of those points. Theoriginal cost function was the absolute value of this difference. Other costfunctions have been proposed. A popular alternative is the square of thedifference. However, to our knowledge, this is the first investigation of boththe relative impacts of using different cost functions and the potential totune cost functions to different tasks. We do so in this paper by using atunable cost function {lambda}{gamma} with parameter {gamma}. We show thathigher values of {gamma} place greater weight on larger pairwise differences,while lower values place greater weight on smaller pairwise differences. Wedemonstrate that training {gamma} significantly improves the accuracy of boththe DTW nearest neighbor and Proximity Forest classifiers.", "output": "Parameterizing the cost function of Dynamic Time Warping with application to time series classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data plays a crucial role in machine learning. However, in real-worldapplications, there are several problems with data, e.g., data are of lowquality; a limited number of data points lead to under-fitting of the machinelearning model; it is hard to access the data due to privacy, safety andregulatory concerns. Synthetic data generation offers a promising new avenue,as it can be shared and used in ways that real-world data cannot. This papersystematically reviews the existing works that leverage machine learning modelsfor synthetic data generation. Specifically, we discuss the synthetic datageneration works from several perspectives: (i) applications, includingcomputer vision, speech, natural language, healthcare, and business; (ii)machine learning methods, particularly neural network architectures and deepgenerative models; (iii) privacy and fairness issue. In addition, we identifythe challenges and opportunities in this emerging field and suggest futureresearch directions.", "output": "Machine Learning for Synthetic Data Generation: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "IoU losses are surrogates that directly optimize the Jaccard index. Insemantic segmentation, leveraging IoU losses as part of the loss function isshown to perform better with respect to the Jaccard index measure thanoptimizing pixel-wise losses such as the cross-entropy loss alone. The mostnotable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss.However, these losses are incompatible with soft labels which are ubiquitous inmachine learning. In this paper, we propose Jaccard metric losses (JMLs), whichare identical to the soft Jaccard loss in a standard setting with hard labels,but are compatible with soft labels. With JMLs, we study two of the mostpopular use cases of soft labels: label smoothing and knowledge distillation.With a variety of architectures, our experiments show significant improvementsover the cross-entropy loss on three semantic segmentation datasets(Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approachoutperforms state-of-the-art knowledge distillation methods by a large margin.Code is available at:href{", "output": "Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, subgraph GNNs have emerged as an important direction for developingexpressive graph neural networks (GNNs). While numerous architectures have beenproposed, so far there is still a limited understanding of how various designparadigms differ in terms of expressive power, nor is it clear what designprinciple achieves maximal expressiveness with minimal architecturalcomplexity. To address these fundamental questions, this paper conducts asystematic study of general node-based subgraph GNNs through the lens ofSubgraph Weisfeiler-Lehman Tests (SWL). Our central result is to build acomplete hierarchy of SWL with strictly growing expressivity. Concretely, weprove that any node-based subgraph GNN falls into one of the six SWLequivalence classes, among which $mathsf{SSWL}$ achieves the maximalexpressive power. We also study how these equivalence classes differ in termsof their practical expressiveness such as encoding graph distance andbiconnectivity. Furthermore, we give a tight expressivity upper bound of allSWL algorithms by establishing a close relation with localized versions of WLand Folklore WL (FWL) tests. Our results provide insights into the power ofexisting subgraph GNNs, guide the design of new architectures, and point outtheir limitations by revealing an inherent gap with the 2-FWL test. Finally,experiments demonstrate that $mathsf{SSWL}$-inspired subgraph GNNs cansignificantly outperform prior architectures on multiple benchmarks despitegreat simplicity.", "output": "A Complete Expressiveness Hierarchy for Subgraph GNNs via Subgraph Weisfeiler-Lehman Tests."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Selective experience replay is a popular strategy for integrating lifelonglearning with deep reinforcement learning. Selective experience replay aims torecount selected experiences from previous tasks to avoid catastrophicforgetting. Furthermore, selective experience replay based techniques are modelagnostic and allow experiences to be shared across different models. However,storing experiences from all previous tasks make lifelong learning usingselective experience replay computationally very expensive and impractical asthe number of tasks increase. To that end, we propose a rewarddistribution-preserving coreset compression technique for compressingexperience replay buffers stored for selective experience replay.We evaluated the coreset compression technique on the brain tumorsegmentation (BRATS) dataset for the task of ventricle localization and on thewhole-body MRI for localization of left knee cap, left kidney, righttrochanter, left lung, and spleen. The coreset lifelong learning models trainedon a sequence of 10 different brain MR imaging environments demonstratedexcellent performance localizing the ventricle with a mean pixel error distanceof 12.93 for the compression ratio of 10x. In comparison, the conventionallifelong learning model localized the ventricle with a mean pixel distance of10.87. Similarly, the coreset lifelong learning models trained on whole-bodyMRI demonstrated no significant difference (p=0.28) between the 10x compressedcoreset lifelong learning models and conventional lifelong learning models forall the landmarks. The mean pixel distance for the 10x compressed models acrossall the landmarks was 25.30, compared to 19.24 for the conventional lifelonglearning models. Our results demonstrate that the potential of thecoreset-based ERB compression method for compressing experiences without asignificant drop in performance.", "output": "Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT is a recent chatbot service released by OpenAI and is receivingincreasing attention over the past few months. While evaluations of variousaspects of ChatGPT have been done, its robustness, i.e., the performance tounexpected inputs, is still unclear to the public. Robustness is of particularconcern in responsible AI, especially for safety-critical applications. In thispaper, we conduct a thorough evaluation of the robustness of ChatGPT from theadversarial and out-of-distribution (OOD) perspective. To do so, we employ theAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkartreview and DDXPlus medical diagnosis datasets for OOD evaluation. We selectseveral popular foundation models as baselines. Results show that ChatGPT showsconsistent advantages on most adversarial and OOD classification andtranslation tasks. However, the absolute performance is far from perfection,which suggests that adversarial and OOD robustness remains a significant threatto foundation models. Moreover, ChatGPT shows astounding performance inunderstanding dialogue-related texts and we find that it tends to provideinformal suggestions for medical tasks instead of definitive answers. Finally,we present in-depth discussions of possible research directions.", "output": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tree-shaped graphical models are widely used for their tractability. However,they unfortunately lack expressive power as they require committing to aparticular sparse dependency structure. We propose a novel class of generativemodels called mixtures of all trees: that is, a mixture over all possible($n^{n-2}$) tree-shaped graphical models over $n$ variables. We show that it ispossible to parameterize this Mixture of All Trees (MoAT) model compactly(using a polynomial-size representation) in a way that allows for tractablelikelihood computation and optimization via stochastic gradient descent.Furthermore, by leveraging the tractability of tree-shaped models, we devisefast-converging conditional sampling algorithms for approximate inference, eventhough our theoretical analysis suggests that exact computation of marginals inthe MoAT model is NP-hard. Empirically, MoAT achieves state-of-the-artperformance on density estimation benchmarks when compared against powerfulprobabilistic models including hidden Chow-Liu Trees.", "output": "Mixtures of All Trees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Here, we introduce a new approach for generating sequences of impliedvolatility (IV) surfaces across multiple assets that is faithful to historicalprices. We do so using a combination of functional data analysis and neuralstochastic differential equations (SDEs) combined with a probability integraltransform penalty to reduce model misspecification. We demonstrate thatlearning the joint dynamics of IV surfaces and prices produces market scenariosthat are consistent with historical features and lie within the sub-manifold ofsurfaces that are essentially free of static arbitrage. Finally, we demonstratethat delta hedging using the simulated surfaces generates profit and loss (P&amp;L)distributions that are consistent with realised P&amp;Ls.", "output": "FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most of the existing clustering methods are based on a single granularity ofinformation, such as the distance and density of each data. This mostfine-grained based approach is usually inefficient and susceptible to noise.Therefore, we propose a clustering algorithm that combines multi-granularityGranular-Ball and minimum spanning tree (MST). We construct coarsegrainedgranular-balls, and then use granular-balls and MST to implement the clusteringmethod based on \"large-scale priority\", which can greatly avoid the influenceof outliers and accelerate the construction process of MST. Experimentalresults on several data sets demonstrate the power of the algorithm. All codeshave been released at ", "output": "GBMST: An Efficient Minimum Spanning Tree Clustering Based on Granular-Ball Computing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Coupled partial differential equations (PDEs) are key tasks in modeling thecomplex dynamics of many physical processes. Recently, neural operators haveshown the ability to solve PDEs by learning the integral kernel directly inFourier/Wavelet space, so the difficulty for solving the coupled PDEs dependson dealing with the coupled mappings between the functions. Towards this end,we propose a textit{coupled multiwavelets neural operator} (CMWNO) learningscheme by decoupling the coupled integral kernels during the multiwaveletdecomposition and reconstruction procedures in the Wavelet space. The proposedmodel achieves significantly higher accuracy compared to previouslearning-based solvers in solving the coupled PDEs including Gray-Scott (GS)equations and the non-local mean field game (MFG) problem. According to ourexperimental results, the proposed model exhibits a $2times sim 4times$improvement relative $L$2 error compared to the best results from thestate-of-the-art models.", "output": "Coupled Multiwavelet Neural Operator Learning for Coupled Partial Differential Equations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fall accidents are critical issues in an aging and aged society. Recently,many researchers developed pre-impact fall detection systems using deeplearning to support wearable-based fall protection systems for preventingsevere injuries. However, most works only employed simple neural network modelsinstead of complex models considering the usability in resource-constrainedmobile devices and strict latency requirements. In this work, we propose anovel pre-impact fall detection via CNN-ViT knowledge distillation, namelyPreFallKD, to strike a balance between detection performance and computationalcomplexity. The proposed PreFallKD transfers the detection knowledge from thepre-trained teacher model (vision transformer) to the student model(lightweight convolutional neural networks). Additionally, we apply dataaugmentation techniques to tackle issues of data imbalance. We conduct theexperiment on the KFall public dataset and compare PreFallKD with otherstate-of-the-art models. The experiment results show that PreFallKD could boostthe student model during the testing phase and achieves reliable F1-score(92.66%) and lead time (551.3 ms).", "output": "PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate and efficient uncertainty estimation is crucial to build reliableMachine Learning (ML) models capable to provide calibrated uncertaintyestimates, generalize and detect Out-Of-Distribution (OOD) datasets. To thisend, Deterministic Uncertainty Methods (DUMs) is a promising model familycapable to perform uncertainty estimation in a single forward pass. This workinvestigates important design choices in DUMs: (1) we show that trainingschemes decoupling the core architecture and the uncertainty head schemes cansignificantly improve uncertainty performances. (2) we demonstrate that thecore architecture expressiveness is crucial for uncertainty performance andthat additional architecture constraints to avoid feature collapse candeteriorate the trade-off between OOD generalization and detection. (3)Contrary to other Bayesian models, we show that the prior defined by DUMs donot have a strong effect on the final performances.", "output": "Training, Architecture, and Prior for Deterministic Uncertainty Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image-to-image reconstruction problems with free or inexpensive metadata inthe form of class labels appear often in biological and medical image domains.Existing text-guided or style-transfer image-to-image approaches do nottranslate to datasets where additional information is provided as discreteclasses. We introduce and implement a model which combines image-to-image andclass-guided denoising diffusion probabilistic models. We train our model on areal-world dataset of microscopy images used for drug discovery, with andwithout incorporating metadata labels. By exploring the properties ofimage-to-image diffusion with relevant labels, we show that class-guidedimage-to-image diffusion can improve the meaningful content of thereconstructed images and outperform the unguided model in useful downstreamtasks.", "output": "Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inversion by Direct Iteration (InDI) is a new formulation for supervisedimage restoration that avoids the so-called ``regression to the mean'' effectand produces more realistic and detailed images than existing regression-basedmethods. It does this by gradually improving image quality in small steps,similar to generative denoising diffusion models.Image restoration is an ill-posed problem where multiple high-quality imagesare plausible reconstructions of a given low-quality input. Therefore, theoutcome of a single step regression model is typically an aggregate of allpossible explanations, therefore lacking details and realism. The mainadvantage of InDI is that it does not try to predict the clean target image ina single step but instead gradually improves the image in small steps,resulting in better perceptual quality.While generative denoising diffusion models also work in small steps, ourformulation is distinct in that it does not require knowledge of any analyticform of the degradation process. Instead, we directly learn an iterativerestoration process from low-quality and high-quality paired examples. InDI canbe applied to virtually any image degradation, given paired training data. Inconditional denoising diffusion image restoration the denoising networkgenerates the restored image by repeatedly denoising an initial image of purenoise, conditioned on the degraded input. Contrary to conditional denoisingformulations, InDI directly proceeds by iteratively restoring the inputlow-quality image, producing high-quality results on a variety of imagerestoration tasks, including motion and out-of-focus deblurring,super-resolution, compression artifact removal, and denoising.", "output": "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Any-scale image synthesis offers an efficient and scalable solution tosynthesize photo-realistic images at any scale, even going beyond 2Kresolution. However, existing GAN-based solutions depend excessively onconvolutions and a hierarchical architecture, which introduce inconsistency andthe $``$texture sticking$\"$ issue when scaling the output resolution. Fromanother perspective, INR-based generators are scale-equivariant by design, buttheir huge memory footprint and slow inference hinder these networks from beingadopted in large-scale or real-time systems. In this work, we propose$textbf{C}$olumn-$textbf{R}$ow $textbf{E}$ntangled $textbf{P}$ixel$textbf{S}$ynthesis ($textbf{CREPS}$), a new generative model that is bothefficient and scale-equivariant without using any spatial convolutions orcoarse-to-fine design. To save memory footprint and make the system scalable,we employ a novel bi-line representation that decomposes layer-wise featuremaps into separate $``$thick$\"$ column and row encodings. Experiments onvarious datasets, including FFHQ, LSUN-Church, MetFaces, and Flickr-Scenery,confirm CREPS' ability to synthesize scale-consistent and alias-free images atany arbitrary resolution with proper training and inference speed. Code isavailable at ", "output": "Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the use of machine learning continues to expand, the importance ofensuring its safety cannot be overstated. A key concern in this regard is theability to identify whether a given sample is from the training distribution,or is an \"Out-Of-Distribution\" (OOD) sample. In addition, adversaries canmanipulate OOD samples in ways that lead a classifier to make a confidentprediction. In this study, we present a novel approach for certifying therobustness of OOD detection within a $ell_2$-norm around the input, regardlessof network architecture and without the need for specific components oradditional training. Further, we improve current techniques for detectingadversarial attacks on OOD samples, while providing high levels of certifiedand adversarial robustness on in-distribution samples. The average of all OODdetection metrics on CIFAR10/100 shows an increase of $sim 13 % / 5%$relative to previous approaches.", "output": "Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Microscopy images often suffer from high levels of noise, which can hinderfurther analysis and interpretation. Content-aware image restoration (CARE)methods have been proposed to address this issue, but they often require largeamounts of training data and suffer from over-fitting. To overcome thesechallenges, we propose a novel framework for few-shot microscopy imagedenoising. Our approach combines a generative adversarial network (GAN) trainedvia contrastive learning (CL) with two structure preserving loss terms(Structural Similarity Index and Total Variation loss) to further improve thequality of the denoised images using little data. We demonstrate theeffectiveness of our method on three well-known microscopy imaging datasets,and show that we can drastically reduce the amount of training data whileretaining the quality of the denoising, thus alleviating the burden ofacquiring paired data and enabling few-shot learning. The proposed frameworkcan be easily extended to other image restoration tasks and has the potentialto significantly advance the field of microscopy image analysis.", "output": "Generalizable Denoising of Microscopy Images using Generative Adversarial Networks and Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inflation is a major determinant for allocation decisions and its forecast isa fundamental aim of governments and central banks. However, forecastinginflation is not a trivial task, as its prediction relies on low frequency,highly fluctuating data with unclear explanatory variables. While classicalmodels show some possibility of predicting inflation, reliably beating therandom walk benchmark remains difficult. Recently, (deep) neural networks haveshown impressive results in a multitude of applications, increasingly settingthe new state-of-the-art. This paper investigates the potential of thetransformer deep neural network architecture to forecast different inflationrates. The results are compared to a study on classical time series and machinelearning models. We show that our adapted transformer, on average, outperformsthe baseline in 6 out of 16 experiments, showing best scores in two out of fourinvestigated inflation rates. Our results demonstrate that a transformer basedneural network can outperform classical regression and machine learning modelsin certain inflation rates and forecasting horizons.", "output": "Inflation forecasting with attention based transformer neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pre-trained large language models have recently achieved ground-breakingperformance in a wide variety of language understanding tasks. However, thesame model can not be applied to multimodal behavior understanding tasks (e.g.,video sentiment/humor detection) unless non-verbal features (e.g., acoustic andvisual) can be integrated with language. Jointly modeling multiple modalitiessignificantly increases the model complexity, and makes the training processdata-hungry. While an enormous amount of text data is available via the web,collecting large-scale multimodal behavioral video datasets is extremelyexpensive, both in terms of time and money. In this paper, we investigatewhether large language models alone can successfully incorporate non-verbalinformation when they are presented in textual form. We present a way toconvert the acoustic and visual information into corresponding textualdescriptions and concatenate them with the spoken text. We feed this augmentedinput to a pre-trained BERT model and fine-tune it on three downstreammultimodal tasks: sentiment, humor, and sarcasm detection. Our approach,TextMI, significantly reduces model complexity, adds interpretability to themodel's decision, and can be applied for a diverse set of tasks while achievingsuperior (multimodal sarcasm detection) or near SOTA (multimodal sentimentanalysis and multimodal humor detection) performance. We propose TextMI as ageneral, competitive baseline for multimodal behavioral analysis tasks,particularly in a low-resource setting.", "output": "TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Discovering novel concepts from unlabelled data and in a continuous manner isan important desideratum of lifelong learners. In the literature such problemshave been partially addressed under very restricted settings, where eitheraccess to labelled data is provided for discovering novel concepts (e.g., NCD)or learning occurs for a limited number of incremental steps (e.g.,class-iNCD). In this work we challenge the status quo and propose a morechallenging and practical learning paradigm called MSc-iNCD, where learningoccurs continuously and unsupervisedly, while exploiting the rich priors fromlarge-scale pre-trained models. To this end, we propose simple baselines thatare not only resilient under longer learning scenarios, but are surprisinglystrong when compared with sophisticated state-of-the-art methods. We conductextensive empirical evaluation on a multitude of benchmarks and show theeffectiveness of our proposed baselines, which significantly raises the bar.", "output": "Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learned partial differential equation (PDE) solvers trade thereliability of standard numerical methods for potential gains in accuracyand/or speed. The only way for a solver to guarantee that it outputs the exactsolution is to use a convergent method in the limit that the grid spacing$Delta x$ and timestep $Delta t$ approach zero. Machine learned solvers,which learn to update the solution at large $Delta x$ and/or $Delta t$, cannever guarantee perfect accuracy. Some amount of error is inevitable, so thequestion becomes: how do we constrain machine learned solvers to give us thesorts of errors that we are willing to tolerate? In this paper, we design morereliable machine learned PDE solvers by preserving discrete analogues of thecontinuous invariants of the underlying PDE. Examples of such invariantsinclude conservation of mass, conservation of energy, the second law ofthermodynamics, and/or non-negative density. Our key insight is simple: topreserve invariants, at each timestep apply an error-correcting algorithm tothe update rule. Though this strategy is different from how standard solverspreserve invariants, it is necessary to retain the flexibility that allowsmachine learned solvers to be accurate at large $Delta x$ and/or $Delta t$.This strategy can be applied to any autoregressive solver for anytime-dependent PDE in arbitrary geometries with arbitrary boundary conditions.Although this strategy is very general, the specific error-correctingalgorithms need to be tailored to the invariants of the underlying equations aswell as to the solution representation and time-stepping scheme of the solver.The error-correcting algorithms we introduce have two key properties. First, bypreserving the right invariants they guarantee numerical stability. Second, inclosed or periodic systems they do so without degrading the accuracy of analready-accurate solver.", "output": "Invariant preservation in machine learned PDE solvers via error correction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent wave of large-scale text-to-image diffusion models hasdramatically increased our text-based image generation abilities. These modelscan generate realistic images for a staggering variety of prompts and exhibitimpressive compositional generalization abilities. Almost all use cases thusfar have solely focused on sampling; however, diffusion models can also provideconditional density estimates, which are useful for tasks beyond imagegeneration. In this paper, we show that the density estimates from large-scaletext-to-image diffusion models like Stable Diffusion can be leveraged toperform zero-shot classification without any additional training. Ourgenerative approach to classification, which we call Diffusion Classifier,attains strong results on a variety of benchmarks and outperforms alternativemethods of extracting knowledge from diffusion models. Although a gap remainsbetween generative and discriminative approaches on zero-shot recognitiontasks, we find that our diffusion-based approach has stronger multimodalrelational reasoning abilities than competing discriminative approaches.Finally, we use Diffusion Classifier to extract standard classifiers fromclass-conditional diffusion models trained on ImageNet. Even though thesemodels are trained with weak augmentations and no regularization, they approachthe performance of SOTA discriminative classifiers. Overall, our results are astep toward using generative over discriminative models for downstream tasks.Results and visualizations at ", "output": "Your Diffusion Model is Secretly a Zero-Shot Classifier."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing foundation models are trained on copyrighted material. Deployingthese models can pose both legal and ethical risks when data creators fail toreceive appropriate attribution or compensation. In the United States andseveral other countries, copyrighted content may be used to build foundationmodels without incurring liability due to the fair use doctrine. However, thereis a caveat: If the model produces output that is similar to copyrighted data,particularly in scenarios that affect the market of that data, fair use may nolonger apply to the output of the model. In this work, we emphasize that fairuse is not guaranteed, and additional work may be necessary to keep modeldevelopment and deployment squarely in the realm of fair use. First, we surveythe potential risks of developing and deploying foundation models based oncopyrighted content. We review relevant U.S. case law, drawing parallels toexisting and potential applications for generating text, source code, andvisual art. Experiments confirm that popular foundation models can generatecontent considerably similar to copyrighted material. Second, we discusstechnical mitigations that can help foundation models stay in line with fairuse. We argue that more research is needed to align mitigation strategies withthe current state of the law. Lastly, we suggest that the law and technicalmitigations should co-evolve. For example, coupled with other policymechanisms, the law could more explicitly consider safe harbors when strongtechnical tools are used to mitigate infringement harms. This co-evolution mayhelp strike a balance between intellectual property and innovation, whichspeaks to the original goal of fair use. But we emphasize that the strategieswe describe here are not a panacea and more work is needed to develop policiesthat address the potential harms of foundation models.", "output": "Foundation Models and Fair Use."}]