[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A microscopic particle obeys the principles of quantum mechanics -- so whereis the sharp boundary between the macroscopic and microscopic worlds? It wasthis \"interpretation problem\" that prompted Schr\"odinger to propose his famousthought experiment (a cat that is simultaneously both dead and alive) andsparked a great debate about the quantum measurement problem, and there isstill no satisfactory answer yet. This is precisely the inadequacy of rigorousmathematical models in describing the laws of nature. We propose acomputational model to describe and understand the laws of nature based onDarwin's natural selection. In fact, whether it's a macro particle, a microelectron or a security, they can all be considered as an entity, the change ofthis entity over time can be described by a data series composed of states andvalues. An observer can learn from this data series to construct theories(usually consisting of functions and differential equations). We don't modelwith the usual functions or differential equations, but with a state DecisionTree (determines the state of an entity) and a value Function Tree (determinesthe distance between two points of an entity). A state Decision Tree and avalue Function Tree together can reconstruct an entity's trajectory and makepredictions about its future trajectory. Our proposed algorithmic modeldiscovers laws of nature by only learning observed historical data (sequentialmeasurement of observables) based on maximizing the observer's expected value.There is no differential equation in our model; our model has an emphasis onmachine learning, where the observer builds up his/her experience by beingrewarded or punished for each decision he/she makes, and eventually leads torediscovering Newton's law, the Born rule (quantum mechanics) and the efficientmarket hypothesis (financial market).", "output": "Machine learning for discovering laws of nature."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes an interactive art installation \"Mood spRing\" designed toreflect the mood of the environment through interpretation of language andtone. Mood spRing consists of an AI program that controls an immersive 3Danimation of the seasons. If the AI program perceives the language and tone ofthe users as pleasant, the animation progresses through idealized renditions ofseasons. Otherwise, it slips into unpleasant weather and natural disasters ofthe season. To interpret the language and tone of the user interaction, hybridstate-of-the-art emotion detection methods are applied to the user audio andtext inputs. The emotional states detected separately from tone and languageare fused by a novel approach that aims at minimizing the possible modeldisparity across diverse demographic groups.", "output": "Visual Response to Emotional State of User Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, wearable emotion recognition based on peripheral physiologicalsignals has drawn massive attention due to its less invasive nature and itsapplicability in real-life scenarios. However, how to effectively fusemultimodal data remains a challenging problem. Moreover, traditionalfully-supervised based approaches suffer from overfitting given limited labeleddata. To address the above issues, we propose a novel self-supervised learning(SSL) framework for wearable emotion recognition, where efficient multimodalfusion is realized with temporal convolution-based modality-specific encodersand a transformer-based shared encoder, capturing both intra-modal andinter-modal correlations. Extensive unlabeled data is automatically assignedlabels by five signal transforms, and the proposed SSL model is pre-trainedwith signal transformation recognition as a pretext task, allowing theextraction of generalized multimodal representations for emotion-relateddownstream tasks. For evaluation, the proposed SSL model was first pre-trainedon a large-scale self-collected physiological dataset and the resulting encoderwas subsequently frozen or fine-tuned on three public supervised emotionrecognition datasets. Ultimately, our SSL-based method achievedstate-of-the-art results in various emotion classification tasks. Meanwhile,the proposed model proved to be more accurate and robust compared tofully-supervised methods on low data regimes.", "output": "Transformer-based Self-supervised Multimodal Representation Learning for Wearable Emotion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce the range of oBERTa language models, aneasy-to-use set of language models, which allows Natural Language Processing(NLP) practitioners to obtain between 3.8 and 24.3 times faster models withoutexpertise in model compression. Specifically, oBERTa extends existing work onpruning, knowledge distillation, and quantization and leverages frozenembeddings to improve knowledge distillation, and improved model initializationto deliver higher accuracy on a a broad range of transfer tasks. In generatingoBERTa, we explore how the highly optimized RoBERTa differs from the BERT withrespect to pruning during pre-training and fine-tuning and find it lessamenable to compression during fine-tuning. We explore the use of oBERTa on abroad seven representative NLP tasks and find that the improved compressiontechniques allow a pruned oBERTa model to match the performance of BERTBASE andexceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answeringdataset, despite being 8x and 2x, respectively, faster in inference. We releaseour code, training regimes, and associated model for broad usage to encourageusage and experimentation.", "output": "oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Assessing the progression of muscle fatigue for daily exercises providesvital indicators for precise rehabilitation, personalized training dose,especially under the context of Metaverse. Assessing fatigue of multi-musclecoordination-involved daily exercises requires the neuromuscular features thatrepresent the fatigue-induced characteristics of spatiotemporal adaptions ofmultiple muscles and the estimator that captures the time-evolving progressionof fatigue. In this paper, we propose to depict fatigue by the features ofmuscle compensation and spinal module activation changes and estimatecontinuous fatigue by a physiological rationale model. First, we extract musclesynergy fractionation and the variance of spinal module spikings as featuresinspired by the prior of fatigue-induced neuromuscular adaptations. Second, wetreat the features as observations and develop a Bayesian Gaussian process tocapture the time-evolving progression. Third, we solve the issue of lackingsupervision information by mathematically formulating the time-evolvingcharacteristics of fatigue as the loss function. Finally, we adapt the metricsthat follow the physiological principles of fatigue to quantitatively evaluatethe performance. Our extensive experiments present a 0.99 similarity betweendays, a over 0.7 similarity with other views of fatigue and a nearly 1 weakmonotonicity, which outperform other methods. This study would aim theobjective assessment of muscle fatigue.", "output": "Estimating Continuous Muscle Fatigue For Multi-Muscle Coordinated Exercise: A Pilot Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attention (and distraction) recognition is a key factor in improvinghuman-robot collaboration. We present an assembly scenario where a humanoperator and a cobot collaborate equally to piece together a gearbox. The setupprovides multiple opportunities for the cobot to adapt its behavior dependingon the operator's attention, which can improve the collaboration experience andreduce psychological strain. As a first step, we recognize the areas in theworkspace that the human operator is paying attention to, and consequently,detect when the operator is distracted. We propose a novel deep-learningapproach to develop an attention recognition model. First, we train aconvolutional neural network to estimate the gaze direction using a publiclyavailable image dataset. Then, we use transfer learning with a small dataset tomap the gaze direction onto pre-defined areas of interest. Models trained usingthis approach performed very well in leave-one-subject-out evaluation on thesmall dataset. We performed an additional validation of our models using thevideo snippets collected from participants working as an operator in thepresented assembly scenario. Although the recall for the Distracted class waslower in this case, the models performed well in recognizing the areas theoperator paid attention to. To the best of our knowledge, this is the firstwork that validated an attention recognition model using data from a settingthat mimics industrial human-robot collaboration. Our findings highlight theneed for validation of attention recognition solutions in such full-fledged,non-guided scenarios.", "output": "Gaze-based Attention Recognition for Human-Robot Collaboration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Like people, LLMs do not always generate the best text for a given generationproblem on their first try (e.g., summaries, answers, explanations). Just aspeople then refine their text, we introduce SELF-REFINE, a framework forsimilarly improving initial outputs from LLMs through iterative feedback andrefinement. The main idea is to generate an output using an LLM, then allow thesame model to provide multi-aspect feedback for its own output; finally, thesame model refines its previously generated output given its own feedback.Unlike earlier work, our iterative refinement framework does not requiresupervised training data or reinforcement learning, and works with a singleLLM. We experiment with 7 diverse tasks, ranging from review rewriting to mathreasoning, demonstrating that our approach outperforms direct generation. Inall tasks, outputs generated with SELF-REFINE are preferred by humans and byautomated metrics over those generated directly with GPT-3.5 and GPT-4,improving on average by absolute 20% across tasks.", "output": "Self-Refine: Iterative Refinement with Self-Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Path Planning methods for autonomous control of Unmanned Aerial Vehicle (UAV)swarms are on the rise because of all the advantages they bring. There are moreand more scenarios where autonomous control of multiple UAVs is required. Mostof these scenarios present a large number of obstacles, such as power lines ortrees. If all UAVs can be operated autonomously, personnel expenses can bedecreased. In addition, if their flight paths are optimal, energy consumptionis reduced. This ensures that more battery time is left for other operations.In this paper, a Reinforcement Learning based system is proposed for solvingthis problem in environments with obstacles by making use of Q-Learning. Thismethod allows a model, in this particular case an Artificial Neural Network, toself-adjust by learning from its mistakes and achievements. Regardless of thesize of the map or the number of UAVs in the swarm, the goal of these paths isto ensure complete coverage of an area with fixed obstacles for tasks, likefield prospecting. Setting goals or having any prior information aside from theprovided map is not required. For experimentation, five maps of different sizeswith different obstacles were used. The experiments were performed withdifferent number of UAVs. For the calculation of the results, the number ofactions taken by all UAVs to complete the task in each experiment is taken intoaccount. The lower the number of actions, the shorter the path and the lowerthe energy consumption. The results are satisfactory, showing that the systemobtains solutions in fewer movements the more UAVs there are. For a betterpresentation, these results have been compared to another state-of-the-artapproach.", "output": "Q-learning Based System for Path Planning with UAV Swarms in Obstacle Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Metadata quality is crucial for digital objects to be discovered throughdigital library interfaces. However, due to various reasons, the metadata ofdigital objects often exhibits incomplete, inconsistent, and incorrect values.We investigate methods to automatically detect, correct, and canonicalizescholarly metadata, using seven key fields of electronic theses anddissertations (ETDs) as a case study. We propose MetaEnhance, a framework thatutilizes state-of-the-art artificial intelligence methods to improve thequality of these fields. To evaluate MetaEnhance, we compiled a metadataquality evaluation benchmark containing 500 ETDs, by combining subsets sampledusing multiple criteria. We tested MetaEnhance on this benchmark and found thatthe proposed methods achieved nearly perfect F1-scores in detecting errors andF1-scores in correcting errors ranging from 0.85 to 1.00 for five of sevenfields.", "output": "MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The cooperative management of rice terraces in Bali reveals an interestingphenomenon that stems from the feedback loop between human decisions and theecosystem process. In particular, spatial patterning is observed, which isheavily reliant on the farmer's decision to plant crops as well as the responsefrom the physical environment like pest damage and water shortage. A recentstudy proposed an evolutionary game theoretic model to infer particular powerlaws governing this spatial patterning along the Bali region. In this paper, weshow a snapshot of rice patches in Bali with colors to indicate the differentstages of rice growth", "output": "Utilizing Remote Sensing to Analyze Land Usage and Rice Planting Patterns."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we induce character-level noise in various forms whenfine-tuning BERT to enable zero-shot cross-lingual transfer to unseen dialectsand languages. We fine-tune BERT on three sentence-level classification tasksand evaluate our approach on an assortment of unseen dialects and languages. Wefind that character-level noise can be an extremely effective agent ofcross-lingual transfer under certain conditions, while it is not as helpful inothers. Specifically, we explore these differences in terms of the nature ofthe task and the relationships between source and target languages, findingthat introduction of character-level noise during fine-tuning is particularlyhelpful when a task draws on surface level cues and the source-targetcross-lingual pair has a relatively high lexical overlap with shorter (i.e.,less meaningful) unseen tokens on average.", "output": "Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to Dialects and Closely-Related Languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Setting proper evaluation objectives for explainable artificial intelligence(XAI) is vital for making XAI algorithms follow human communication norms,support human reasoning processes, and fulfill human needs for AI explanations.In this article, we examine explanation plausibility, which is the mostpervasive human-grounded concept in XAI evaluation. Plausibility measures howreasonable the machine explanation is compared to the human explanation.Plausibility has been conventionally formulated as an important evaluationobjective for AI explainability tasks. We argue against this idea, and show howoptimizing and evaluating XAI for plausibility is sometimes harmful, and alwaysineffective to achieve model understandability, transparency, andtrustworthiness. Specifically, evaluating XAI algorithms for plausibilityregularizes the machine explanation to express exactly the same content ashuman explanation, which deviates from the fundamental motivation for humans toexplain: expressing similar or alternative reasoning trajectories whileconforming to understandable forms or language. Optimizing XAI for plausibilityregardless of the model decision correctness also jeopardizes modeltrustworthiness, as doing so breaks an important assumption in human-humanexplanation namely that plausible explanations typically imply correctdecisions, and violating this assumption eventually leads to either undertrustor overtrust of AI models. Instead of being the end goal in XAI evaluation,plausibility can serve as an intermediate computational proxy for the humanprocess of interpreting explanations to optimize the utility of XAI. We furtherhighlight the importance of explainability-specific evaluation objectives bydifferentiating the AI explanation task from the object localization task.", "output": "Rethinking AI Explainability and Plausibility."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Detecting protein-protein interactions (PPIs) is crucial for understandinggenetic mechanisms, disease pathogenesis, and drug design. However, with thefast-paced growth of biomedical literature, there is a growing need forautomated and accurate extraction of PPIs to facilitate scientific knowledgediscovery. Pre-trained language models, such as generative pre-trainedtransformer (GPT) and bidirectional encoder representations from transformers(BERT), have shown promising results in natural language processing (NLP)tasks. We evaluated the PPI identification performance of various GPT and BERTmodels using a manually curated benchmark corpus of 164 PPIs in 77 sentencesfrom learning language in logic (LLL). BERT-based models achieved the bestoverall performance, with PubMedBERT achieving the highest precision (85.17%)and F1-score (86.47%) and BioM-ALBERT achieving the highest recall (93.83%).Despite not being explicitly trained for biomedical texts, GPT-4 achievedcomparable performance to the best BERT models with 83.34% precision, 76.57%recall, and 79.18% F1-score. These findings suggest that GPT models caneffectively detect PPIs from text data and have the potential for use inbiomedical literature mining tasks.", "output": "Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Linear transformation of the inputs alters the training performance offeed-forward networks that are otherwise equivalent. However, most lineartransforms are viewed as a pre-processing operation separate from the actualtraining. Starting from equivalent networks, it is shown that pre-processinginputs using linear transformation are equivalent to multiplying the negativegradient matrix with an autocorrelation matrix per training iteration. Secondorder method is proposed to find the autocorrelation matrix that maximizeslearning in a given iteration. When the autocorrelation matrix is diagonal, themethod optimizes input gains. This optimal input gain (OIG) approach is used toimprove two first-order two-stage training algorithms, namely back-propagation(BP) and hidden weight optimization (HWO), which alternately update the inputweights and solve linear equations for output weights. Results show that theproposed OIG approach greatly enhances the performance of the first-orderalgorithms, often allowing them to rival the popular Levenberg-Marquardtapproach with far less computation. It is shown that HWO is equivalent to BPwith Whitening transformation applied to the inputs. HWO effectively combinesWhitening transformation with learning. Thus, OIG improved HWO could be asignificant building block to more complex deep learning architectures.", "output": "Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapid advancement of conversational and chat-based language models hasled to remarkable progress in complex task-solving. However, their successheavily relies on human input to guide the conversation, which can bechallenging and time-consuming. This paper explores the potential of buildingscalable techniques to facilitate autonomous cooperation among communicativeagents and provide insight into their \"cognitive\" processes. To address thechallenges of achieving autonomous cooperation, we propose a novelcommunicative agent framework named role-playing. Our approach involves usinginception prompting to guide chat agents toward task completion whilemaintaining consistency with human intentions. We showcase how role-playing canbe used to generate conversational data for studying the behaviors andcapabilities of chat agents, providing a valuable resource for investigatingconversational language models. Our contributions include introducing a novelcommunicative agent framework, offering a scalable approach for studying thecooperative behaviors and capabilities of multi-agent systems, andopen-sourcing our library to support research on communicative agents andbeyond. The GitHub repository of this project is made publicly available on:", "output": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies show that models trained by continual learning can achieve thecomparable performances as the standard supervised learning and the learningflexibility of continual learning models enables their wide applications in thereal world. Deep learning models, however, are shown to be vulnerable toadversarial attacks. Though there are many studies on the model robustness inthe context of standard supervised learning, protecting continual learning fromadversarial attacks has not yet been investigated. To fill in this researchgap, we are the first to study adversarial robustness in continual learning andpropose a novel method called textbf{T}ask-textbf{A}ware textbf{B}oundarytextbf{A}ugmentation (TABA) to boost the robustness of continual learningmodels. With extensive experiments on CIFAR-10 and CIFAR-100, we show theefficacy of adversarial training and TABA in defending adversarial attacks.", "output": "Towards Adversarially Robust Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Blast furnace modeling and control is one of the important problems in theindustrial field, and the black-box model is an effective mean to describe thecomplex blast furnace system. In practice, there are often different learningtargets, such as safety and energy saving in industrial applications, dependingon the application. For this reason, this paper proposes a framework to designa domain knowledge integrated classification model that yields a classifier forindustrial application. Our knowledge incorporated learning scheme allows theusers to create a classifier that identifies \"important samples\" (whosemisclassifications can lead to severe consequences) more correctly, whilekeeping the proper precision of classifying the remaining samples. Theeffectiveness of the proposed method has been verified by two real blastfurnace datasets, which guides the operators to utilize their prior experiencefor controlling the blast furnace systems better.", "output": "Domain Knowledge integrated for Blast Furnace Classifier Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a 3D object, kinematic motion prediction aims to identify the mobileparts as well as the corresponding motion parameters. Due to the largevariations in both topological structure and geometric details of 3D objects,this remains a challenging task and the lack of large scale labeled data alsoconstrain the performance of deep learning based approaches. In this paper, wetackle the task of object kinematic motion prediction problem in a semi-weaklysupervised manner. Our key observations are two-fold. First, although 3Ddataset with fully annotated motion labels is limited, there are existingdatasets and methods for object part semantic segmentation at large scale.Second, semantic part segmentation and mobile part segmentation is not alwaysconsistent but it is possible to detect the mobile parts from the underlying 3Dstructure. Towards this end, we propose a graph neural network to learn the mapbetween hierarchical part-level segmentation and mobile parts parameters, whichare further refined based on geometric alignment. This network can be firsttrained on PartNet-Mobility dataset with fully labeled mobility information andthen applied on PartNet dataset with fine-grained and hierarchical part-levelsegmentation. The network predictions yield a large scale of 3D objects withpseudo labeled mobility information and can further be used forweakly-supervised learning with pre-existing segmentation. Our experiments showthere are significant performance boosts with the augmented data for previousmethod designed for kinematic motion prediction on 3D partial scans.", "output": "Semi-Weakly Supervised Object Kinematic Motion Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In-context learning (ICL) with pre-trained language models (PTLMs) has showngreat success in code generation. ICL does not require training. PTLMs take asthe input a prompt consisting of a few requirement-code examples and a newrequirement, and output a new program. However, existing studies simply reuseICL techniques for natural language generation and ignore unique features ofcode generation. We refer to these studies as standard ICL.Inspired by observations of the human coding process, we propose a novel ICLapproach for code generation named AceCoder. Compared to standard ICL, AceCoderhas two novelties. (1) Example retrieval. It retrieves similar programs asexamples and learns programming skills (e.g., algorithms, APIs) from them. (2)Guided Code Generation. It encourages PTLMs to output an intermediatepreliminary (e.g., test cases, APIs) before generating programs. Thepreliminary can help PTLMs understand requirements and guide the next codegeneration. We apply AceCoder to six PTLMs (e.g., Codex) and evaluate it onthree public benchmarks using the Pass@k. Results show that AceCoder cansignificantly improve the performance of PTLMs on code generation. (1) In termsof Pass@1, AceCoder outperforms standard ICL by up to 79.7% and fine-tunedmodels by up to 171%. (2) AceCoder is effective in PTLMs with different sizes(e.g., 1B to 175B) and different languages (e.g., Python, Java, andJavaScript). (3) We investigate multiple choices of the intermediatepreliminary. (4) We manually evaluate generated programs in three aspects andprove the superiority of AceCoder. (5) Finally, we discuss some insights aboutICL for practitioners.", "output": "Towards Enhancing In-Context Learning for Code Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Referring image segmentation (RIS) aims to find a segmentation mask given areferring expression grounded to a region of the input image. Collectinglabelled datasets for this task, however, is notoriously costly andlabor-intensive. To overcome this issue, we propose a simple yet effectivezero-shot referring image segmentation method by leveraging the pre-trainedcross-modal knowledge from CLIP. In order to obtain segmentation masks groundedto the input text, we propose a mask-guided visual encoder that captures globaland local contextual information of an input image. By utilizing instance masksobtained from off-the-shelf mask proposal techniques, our method is able tosegment fine-detailed Istance-level groundings. We also introduce aglobal-local text encoder where the global feature captures complexsentence-level semantics of the entire input expression while the local featurefocuses on the target noun phrase extracted by a dependency parser. In ourexperiments, the proposed method outperforms several zero-shot baselines of thetask and even the weakly supervised referring expression segmentation methodwith substantial margins. Our code is available at", "output": "Zero-shot Referring Image Segmentation with Global-Local Context Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Saliency methods provide post-hoc model interpretation by attributing inputfeatures to the model outputs. Current methods mainly achieve this using asingle input sample, thereby failing to answer input-independent inquiriesabout the model. We also show that input-specific saliency mapping isintrinsically susceptible to misleading feature attribution. Current attemptsto use 'general' input features for model interpretation assume access to adataset containing those features, which biases the interpretation. Addressingthe gap, we introduce a new perspective of input-agnostic saliency mapping thatcomputationally estimates the high-level features attributed by the model toits outputs. These features are geometrically correlated, and are computed byaccumulating model's gradient information with respect to an unrestricted datadistribution. To compute these features, we nudge independent data points overthe model loss surface towards the local minima associated by ahuman-understandable concept, e.g., class label for classifiers. With asystematic projection, scaling and refinement process, this information istransformed into an interpretable visualization without compromising itsmodel-fidelity. The visualization serves as a stand-alone qualitativeinterpretation. With an extensive evaluation, we not only demonstratesuccessful visualizations for a variety of concepts for large-scale models, butalso showcase an interesting utility of this new form of saliency mapping byidentifying backdoor signatures in compromised classifiers.", "output": "Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The abundance of instructional videos and their narrations over the Internetoffers an exciting avenue for understanding procedural activities. In thiswork, we propose to learn video representation that encodes both action stepsand their temporal ordering, based on a large-scale dataset of webinstructional videos and their narrations, without using human annotations. Ourmethod jointly learns a video representation to encode individual stepconcepts, and a deep probabilistic model to capture both temporal dependenciesand immense individual variations in the step ordering. We empiricallydemonstrate that learning temporal ordering not only enables new capabilitiesfor procedure reasoning, but also reinforces the recognition of individualsteps. Our model significantly advances the state-of-the-art results on stepclassification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting(+7.4% on COIN). Moreover, our model attains promising results in zero-shotinference for step classification and forecasting, as well as in predictingdiverse and plausible steps for incomplete procedures. Our code is available at", "output": "Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Finding relevant and high-quality datasets to train machine learning modelsis a major bottleneck for practitioners. Furthermore, to address ambitiousreal-world use-cases there is usually the requirement that the data comelabelled with high-quality annotations that can facilitate the training of asupervised model. Manually labelling data with high-quality labels is generallya time-consuming and challenging task and often this turns out to be thebottleneck in a machine learning project. Weak Supervised Learning (WSL)approaches have been developed to alleviate the annotation burden by offeringan automatic way of assigning approximate labels (pseudo-labels) to unlabelleddata based on heuristics, distant supervision and knowledge bases. We applyprobabilistic generative latent variable models (PLVMs), trained on heuristiclabelling representations of the original dataset, as an accurate, fast andcost-effective way to generate pseudo-labels. We show that the PLVMs achievestate-of-the-art performance across four datasets. For example, they achieve22% points higher F1 score than Snorkel in the class-imbalanced Spouse dataset.PLVMs are plug-and-playable and are a drop-in replacement to existing WSLframeworks (e.g. Snorkel) or they can be used as benchmark models for morecomplicated algorithms, giving practitioners a compelling accuracy boost.", "output": "A Benchmark Generative Probabilistic Model for Weak Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Process simulation is an analysis tool in process mining that allows users tomeasure the impact of changes, prevent losses, and update the process withoutrisks or costs. In the literature, several process simulation techniques areavailable and they are usually built upon process models discovered from agiven event log or learned via deep learning. Each group of approaches has itsown strengths and limitations. The former is usually restricted to thecontrol-flow but it is more interpretable, whereas the latter is notinterpretable by nature but has a greater generalization capability on largeevent logs. Despite the great performance achieved by deep learning approaches,they are still not suitable to be applied to real scenarios and generate valuefor users. This issue is mainly due to fact their stochasticity is hard tocontrol. To address this problem, we propose the CoSMo framework forimplementing process simulation models fully based on deep learning. Thisframework enables simulating event logs that satisfy a constraint byconditioning the learning phase of a deep neural network. Throughoutexperiments, the simulation is validated from both control-flow and data-flowperspectives, demonstrating the proposed framework's capability of simulatingcases while satisfying imposed conditions.", "output": "CoSMo: a Framework for Implementing Conditioned Process Simulation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce Interval Real Logic (IRL), a two-sorted logicthat interprets knowledge such as sequential properties (traces) and eventproperties using sequences of real-featured data. We interpret connectivesusing fuzzy logic, event durations using trapezoidal fuzzy intervals, and fuzzytemporal relations using relationships between the intervals' areas. We proposeInterval Logic Tensor Networks (ILTN), a neuro-symbolic system that learns bypropagating gradients through IRL. In order to support effective learning, ILTNdefines smoothened versions of the fuzzy intervals and temporal relations ofIRL using softplus activations. We show that ILTN can successfully leverageknowledge expressed in IRL in synthetic tasks that require reasoning aboutevents to predict their fuzzy durations. Our results show that the system iscapable of making events compliant with background temporal knowledge.", "output": "Interval Logic Tensor Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a general methodology that learns to classify images withoutlabels by leveraging pretrained feature extractors. Our approach involvesself-distillation training of clustering heads, based on the fact that nearestneighbors in the pretrained feature space are likely to share the same label.We propose a novel objective to learn associations between images byintroducing a variant of pointwise mutual information together with instanceweighting. We demonstrate that the proposed objective is able to attenuate theeffect of false positive pairs while efficiently exploiting the structure inthe pretrained feature space. As a result, we improve the clustering accuracyover $k$-means on $17$ different pretrained models by $6.1$% and $12.2$% onImageNet and CIFAR100, respectively. Finally, using self-supervised pretrainedvision transformers we push the clustering accuracy on ImageNet to $61.6$%.The code will be open-sourced.", "output": "Exploring the Limits of Deep Image Clustering using Pretrained Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-definition roads are an essential component of realistic drivingscenario simulation for autonomous vehicle testing. Roundabouts are one of thekey road segments that have not been thoroughly investigated. Based on thegeometric constraints of the nearby road structure, this work presents a novelmethod for procedurally building roundabouts. The suggested method can resultin roundabout lanes that are not perfectly circular and resemble real-worldroundabouts by allowing approaching roadways to be connected to a roundabout atany angle. One can easily incorporate the roundabout in their HD roadgeneration process or use the standalone roundabouts in scenario-based testingof autonomous driving.", "output": "Procedural Generation of Complex Roundabouts for Autonomous Vehicle Testing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Social Honeypots are tools deployed in Online Social Networks (OSN) toattract malevolent activities performed by spammers and bots. To this end,their content is designed to be of maximum interest to malicious users.However, by choosing an appropriate content topic, this attractive mechanismcould be extended to any OSN users, rather than only luring malicious actors.As a result, honeypots can be used to attract individuals interested in a widerange of topics, from sports and hobbies to more sensitive subjects likepolitical views and conspiracies. With all these individuals gathered in oneplace, honeypot owners can conduct many analyses, from social to marketingstudies.In this work, we introduce a novel concept of social honeypot for attractingOSN users interested in a generic target topic. We propose a framework based onfully-automated content generation strategies and engagement plans to mimiclegit Instagram pages. To validate our framework, we created 21 self-managedsocial honeypots (i.e., pages) on Instagram, covering three topics, fourcontent generation strategies, and three engaging plans. In nine weeks, ourhoneypots gathered a total of 753 followers, 5387 comments, and 15739 likes.These results demonstrate the validity of our approach, and through statisticalanalysis, we examine the characteristics of effective social honeypots.", "output": "Social Honeypot for Humans: Luring People through Self-managed Instagram Pages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The metaverse, which is at the stage of innovation and exploration, faces thedilemma of data collection and the problem of private data leakage in theprocess of development. This can seriously hinder the widespread deployment ofthe metaverse. Fortunately, federated learning (FL) is a solution to the aboveproblems. FL is a distributed machine learning paradigm with privacy-preservingfeatures designed for a large number of edge devices. Federated learning formetaverse (FL4M) will be a powerful tool. Because FL allows edge devices toparticipate in training tasks locally using their own data, computationalpower, and model-building capabilities. Applying FL to the metaverse not onlyprotects the data privacy of participants but also reduces the need for highcomputing power and high memory on servers. Until now, there have been manystudies about FL and the metaverse, respectively. In this paper, we review someof the early advances of FL4M, which will be a research direction withunlimited development potential. We first introduce the concepts of metaverseand FL, respectively. Besides, we discuss the convergence of key metaversetechnologies and FL in detail, such as big data, communication technology, theInternet of Things, edge computing, blockchain, and extended reality. Finally,we discuss some key challenges and promising directions of FL4M in detail. Insummary, we hope that our up-to-date brief survey can help people betterunderstand FL4M and build a fair, open, and secure metaverse.", "output": "Federated Learning for Metaverse: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Entropy measures are effective features for time series classificationproblems. Traditional entropy measures, such as Shannon entropy, useprobability distribution function. However, for the effective separation oftime series, new entropy estimation methods are required to characterize thechaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn)is based on the classification of special datasets (MNIST-10 andSARS-CoV-2-RBV1) in relation to the entropy of the time series recorded in thereservoir of the LogNNet neural network. NNetEn estimates the chaotic dynamicsof time series in an original way. Based on the NNetEn algorithm, we proposetwo new classification metrics: R2 Efficiency and Pearson Efficiency. Theefficiency of NNetEn is verified on separation of two chaotic time series ofsine mapping using dispersion analysis (ANOVA). For two close dynamic timeseries (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124and reflects high efficiency of the introduced method in classificationproblems. The EEG signal classification for healthy persons and patients withAlzheimer disease illustrates the practical application of the NNetEn features.Our computations demonstrate the synergistic effect of increasingclassification accuracy when applying traditional entropy measures and theNNetEn concept conjointly. An implementation of the algorithms in Python ispresented.", "output": "Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Purpose - To characterise and assess the quality of published researchevaluating artificial intelligence (AI) methods for ovarian cancer diagnosis orprognosis using histopathology data. Methods - A search of 5 sources wasconducted up to 01/12/2022. The inclusion criteria required that researchevaluated AI on histopathology images for diagnostic or prognostic inferencesin ovarian cancer, including tubo-ovarian and peritoneal tumours. Reviews andnon-English language articles were excluded. The risk of bias was assessed forevery included model using PROBAST. Results - A total of 1434 research articleswere identified, of which 36 were eligible for inclusion. These studiesreported 62 models of interest, including 35 classifiers, 14 survivalprediction models, 7 segmentation models, and 6 regression models. Models weredeveloped using 1-1375 slides from 1-664 ovarian cancer patients. A wide arrayof outcomes were predicted, including overall survival (9/62), histologicalsubtypes (7/62), stain quantity (6/62) and malignancy (5/62). Older studiesused traditional machine learning (ML) models with hand-crafted features, whilenewer studies typically employed deep learning (DL) to automatically learnfeatures and predict the outcome(s) of interest. All models were found to be athigh or unclear risk of bias overall. Research was frequently limited byinsufficient reporting, small sample sizes, and insufficient validation.Conclusion - Limited research has been conducted and none of the associatedmodels have been demonstrated to be ready for real-world implementation.Recommendations are provided addressing underlying biases and flaws in studydesign, which should help inform higher-quality reproducible future research.Key aspects include more transparent and comprehensive reporting, and improvedperformance evaluation using cross-validation and external validations.", "output": "Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Without the ability to estimate and benchmark AI capability advancements,organizations are left to respond to each change reactively, impeding theirability to build viable mid and long-term strategies. This paper explores therecent growth of forecasting, a political science tool that uses explicitassumptions and quantitative estimation that leads to improved predictionaccuracy. Done at the collective level, forecasting can identify and verifytalent, enable leaders to build better models of AI advancements and improveinputs into design policy. Successful approaches to forecasting and casestudies are examined, revealing a subclass of \"superforecasters\" who outperform98% of the population and whose insights will be most reliable. Finally,techniques behind successful forecasting are outlined, including PhillipTetlock's \"Ten Commandments.\" To adapt to a quickly changing technologylandscape, designers and policymakers should consider forecasting as a firstline of defense.", "output": "Asking Better Questions -- The Art and Science of Forecasting: A mechanism for truer answers to high-stakes questions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "AI systems may be better thought of as peers than as tools. This paperexplores applications of augmented collective intelligence (ACI) beneficial tocollaborative ideation. Design considerations are offered for an experimentthat evaluates the performance of hybrid human- AI collectives. Theinvestigation described combines humans and large language models (LLMs) toideate on increasingly complex topics. A promising real-time collection toolcalled Polis is examined to facilitate ACI, including case studies from citizenengagement projects in Taiwan and Bowling Green, Kentucky. The authors discussthree challenges to consider when designing an ACI experiment: topic selection,participant selection, and evaluation of results. The paper concludes thatresearchers should address these challenges to conduct empirical studies of ACIin collaborative ideation.", "output": "Augmented Collective Intelligence in Collaborative Ideation: Agenda and Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers have been incredibly effective when tackling computervision tasks due to their ability to model long feature dependencies. By usinglarge-scale training data and various self-supervised signals (e.g., maskedrandom patches), vision transformers provide state-of-the-art performance onseveral benchmarking datasets, such as ImageNet-1k and CIFAR-10. However, thesevision transformers pretrained over general large-scale image corpora couldonly produce an anisotropic representation space, limiting theirgeneralizability and transferability to the target downstream tasks. In thispaper, we propose a simple and effective Label-aware Contrastive Trainingframework LaCViT, which improves the isotropy of the pretrained representationspace for vision transformers, thereby enabling more effective transferlearning amongst a wide range of image classification tasks. Throughexperimentation over five standard image classification datasets, wedemonstrate that LaCViT-trained models outperform the original pretrainedbaselines by around 9% absolute Accuracy@1, and consistent improvements can beobserved when applying LaCViT to our three evaluated vision transformers.", "output": "LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In real-world applications, a machine learning model is required to handle anopen-set recognition (OSR), where unknown classes appear during the inference,in addition to a domain shift, where the distribution of data differs betweenthe training and inference phases. Domain generalization (DG) aims to handlethe domain shift situation where the target domain of the inference phase isinaccessible during model training. Open domain generalization (ODG) takes intoaccount both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a methodtargeting ODG but has a complicated learning process. On the other hand,although various DG methods have been proposed, they have not been evaluated inODG situations. This work comprehensively evaluates existing DG methods in ODGand shows that two simple DG methods, CORrelation ALignment (CORAL) and MaximumMean Discrepancy (MMD), are competitive with DAML in several cases. Inaddition, we propose simple extensions of CORAL and MMD by introducing thetechniques used in DAML, such as ensemble learning and Dirichlet mixup dataaugmentation. The experimental evaluation demonstrates that the extended CORALand MMD can perform comparably to DAML with lower computational costs. Thissuggests that the simple DG methods and their simple extensions are strongbaselines for ODG. The code used in the experiments is available at", "output": "Simple Domain Generalization Methods are Strong Baselines for Open Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This White Paper sets out to explain the value that metamodelling can bringto air traffic management (ATM) research. It will define metamodelling andexplore what it can, and cannot, do. The reader is assumed to have basicknowledge of SESAR: the Single European Sky ATM Research project. An importantelement of SESAR, as the technological pillar of the Single European Skyinitiative, is to bring about improvements, as measured through specific keyperformance indicators (KPIs), and as implemented by a series of so-calledSESAR 'Solutions'. These 'Solutions' are new or improved operational proceduresor technologies, designed to meet operational and performance improvementsdescribed in the European ATM Master Plan.", "output": "NOSTROMO: Lessons learned, conclusions and way forward."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Analogical inference is a remarkable capability of human reasoning, and hasbeen used to solve hard reasoning tasks. Analogy based reasoning (AR) hasgained increasing interest from the artificial intelligence community and hasshown its potential in multiple machine learning tasks such as classification,decision making and recommendation with competitive results. We propose a deeplearning (DL) framework to address and tackle two key tasks in AR: analogydetection and solving. The framework is thoroughly tested on the Siganalogiesdataset of morphological analogical proportions (APs) between words, and shownto outperform symbolic approaches in many languages. Previous work haveexplored the behavior of the Analogy Neural Network for classification (ANNc)on analogy detection and of the Analogy Neural Network for retrieval (ANNr) onanalogy solving by retrieval, as well as the potential of an autoencoder (AE)for analogy solving by generating the solution word. In this article wesummarize these findings and we extend them by combining ANNr and the AEembedding model, and checking the performance of ANNc as an retrieval method.The combination of ANNr and AE outperforms the other approaches in almost allcases, and ANNc as a retrieval method achieves competitive or betterperformance than 3CosMul. We conclude with general guidelines on using ourframework to tackle APs with DL.", "output": "Solving morphological analogies: from retrieval to generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Perception datasets for agriculture are limited both in quantity anddiversity which hinders effective training of supervised learning approaches.Self-supervised learning techniques alleviate this problem, however, existingmethods are not optimized for dense prediction tasks in agriculture domainswhich results in degraded performance. In this work, we address this limitationwith our proposed Injected Noise Discriminator (INoD) which exploits principlesof feature replacement and dataset discrimination for self-supervisedrepresentation learning. INoD interleaves feature maps from two disjointdatasets during their convolutional encoding and predicts the datasetaffiliation of the resultant feature map as a pretext task. Our approachenables the network to learn unequivocal representations of objects seen in onedataset while observing them in conjunction with similar features from thedisjoint dataset. This allows the network to reason about higher-levelsemantics of the entailed objects, thus improving its performance on variousdownstream tasks. Additionally, we introduce the novel Fraunhofer Potato 2022dataset consisting of over 16,800 images for object detection in potato fields.Extensive evaluations of our proposed INoD pretraining strategy for the tasksof object detection, semantic segmentation, and instance segmentation on theSugar Beets 2016 and our potato dataset demonstrate that it achievesstate-of-the-art performance.", "output": "INoD: Injected Noise Discriminator for Self-Supervised Representation Learning in Agricultural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal and numerical expression understanding is of great importance inmany downstream Natural Language Processing (NLP) and Information Retrieval(IR) tasks. However, much previous work covers only a few sub-types and focusesonly on entity extraction, which severely limits the usability of identifiedmentions. In order for such entities to be useful in downstream scenarios,coverage and granularity of sub-types are important; and, even more so,providing resolution into concrete values that can be manipulated. Furthermore,most previous work addresses only a handful of languages. Here we describe amulti-lingual evaluation dataset - NTX - covering diverse temporal andnumerical expressions across 14 languages and covering extraction,normalization, and resolution. Along with the dataset we provide a robustrule-based system as a strong baseline for comparisons against other models tobe evaluated in this dataset. Data and code are available aturl{", "output": "Dataset and Baseline System for Multi-lingual Extraction and Normalization of Temporal and Numerical Expressions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Endoscopic video recordings are widely used in minimally invasiverobot-assisted surgery, but when the endoscope is outside the patient's body,it can capture irrelevant segments that may contain sensitive information. Toaddress this, we propose a framework that accurately detects out-of-body framesin surgical videos by leveraging self-supervision with minimal data labels. Weuse a massive amount of unlabeled endoscopic images to learn meaningfulrepresentations in a self-supervised manner. Our approach, which involvespre-training on an auxiliary task and fine-tuning with limited supervision,outperforms previous methods for detecting out-of-body frames in surgicalvideos captured from da Vinci X and Xi surgical systems. The average F1 scoresrange from 96.00 to 98.02. Remarkably, using only 5% of the training labels,our approach still maintains an average F1 score performance above 97,outperforming fully-supervised methods with 95% fewer labels. These resultsdemonstrate the potential of our framework to facilitate the safe handling ofsurgical video recordings and enhance data privacy protection in minimallyinvasive surgery.", "output": "Automatic Detection of Out-of-body Frames in Surgical Videos for Privacy Protection Using Self-supervised Learning and Minimal Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) are vulnerable to adversarial examples, which maylead to catastrophe in security-critical domains. Numerous detection methodsare proposed to characterize the feature uniqueness of adversarial examples, orto distinguish DNN's behavior activated by the adversarial examples. Detectionsbased on features cannot handle adversarial examples with large perturbations.Besides, they require a large amount of specific adversarial examples. Anothermainstream, model-based detections, which characterize input properties bymodel behaviors, suffer from heavy computation cost. To address the issues, weintroduce the concept of local gradient, and reveal that adversarial exampleshave a quite larger bound of local gradient than the benign ones. Inspired bythe observation, we leverage local gradient for detecting adversarial examples,and propose a general framework AdvCheck. Specifically, by calculating thelocal gradient from a few benign examples and noise-added misclassifiedexamples to train a detector, adversarial examples and even misclassifiednatural inputs can be precisely distinguished from benign ones. Throughextensive experiments, we have validated the AdvCheck's superior performance tothe state-of-the-art (SOTA) baselines, with detection rate ($sim times 1.2$)on general adversarial attacks and ($sim times 1.4$) on misclassified naturalinputs on average, with average 1/500 time cost. We also provide interpretableresults for successful detection.", "output": "AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In smart electrical grids, fault detection tasks may have a high impact onsociety due to their economic and critical implications. In the recent years,numerous smart grid applications, such as defect detection and loadforecasting, have embraced data-driven methodologies. The purpose of this studyis to investigate the challenges associated with the security of machinelearning (ML) applications in the smart grid scenario. Indeed, the robustnessand security of these data-driven algorithms have not been extensively studiedin relation to all power grid applications. We demonstrate first that the deepneural network method used in the smart grid is susceptible to adversarialperturbation. Then, we highlight how studies on fault localization and typeclassification illustrate the weaknesses of present ML algorithms in smartgrids to various adversarial attacks", "output": "Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimization problems involving minimization of a rank-one convex functionover constraints modeling restrictions on the support of the decision variablesemerge in various machine learning applications. These problems are oftenmodeled with indicator variables for identifying the support of the continuousvariables. In this paper we investigate compact extended formulations for suchproblems through perspective reformulation techniques. In contrast to themajority of previous work that relies on support function arguments anddisjunctive programming techniques to provide convex hull results, we propose aconstructive approach that exploits a hidden conic structure induced byperspective functions. To this end, we first establish a convex hull result fora general conic mixed-binary set in which each conic constraint involves alinear function of independent continuous variables and a set of binaryvariables. We then demonstrate that extended representations of sets associatedwith epigraphs of rank-one convex functions over constraints modeling indicatorrelations naturally admit such a conic representation. This enables us tosystematically give perspective formulations for the convex hull descriptionsof these sets with nonlinear separable or non-separable objective functions,sign constraints on continuous variables, and combinatorial constraints onindicator variables. We illustrate the efficacy of our results on sparsenonnegative logistic regression problems.", "output": "Constrained Optimization of Rank-One Functions with Indicator Variables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised Continual learning involves updating a deep neural network (DNN)from an ever-growing stream of labeled data. While most work has focused onovercoming catastrophic forgetting, one of the major motivations behindcontinual learning is being able to efficiently update a network with newinformation, rather than retraining from scratch on the training dataset as itgrows over time. Despite recent continual learning methods largely solving thecatastrophic forgetting problem, there has been little attention paid to theefficiency of these algorithms. Here, we study recent methods for incrementalclass learning and illustrate that many are highly inefficient in terms ofcompute, memory, and storage. Some methods even require more compute thantraining from scratch! We argue that for continual learning to have real-worldapplicability, the research community cannot ignore the resources used by thesealgorithms. There is more to continual learning than mitigating catastrophicforgetting.", "output": "How Efficient Are Today's Continual Learning Algorithms?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are proven to be vulnerable to backdoor attacks.Detecting the trigger samples during the inference stage, i.e., the test-timetrigger sample detection, can prevent the backdoor from being triggered.However, existing detection methods often require the defenders to have highaccessibility to victim models, extra clean data, or knowledge about theappearance of backdoor triggers, limiting their practicality. In this paper, wepropose the test-time corruption robustness consistency evaluation (TeCo), anovel test-time trigger sample detection method that only needs the hard-labeloutputs of the victim models without any extra information. Our journey beginswith the intriguing observation that the backdoor-infected models have similarperformance across different image corruptions for the clean images, butperform discrepantly for the trigger samples. Based on this phenomenon, wedesign TeCo to evaluate test-time robustness consistency by calculating thedeviation of severity that leads to predictions' transition across differentcorruptions. Extensive experiments demonstrate that compared withstate-of-the-art defenses, which even require either certain information aboutthe trigger types or accessibility of clean data, TeCo outperforms them ondifferent backdoor attacks, datasets, and model architectures, enjoying ahigher AUROC by 10% and 5 times of stability.", "output": "Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, with the rapid deployment of service APIs, personalized servicerecommendations have played a paramount role in the growth of the e-commerceindustry. Quality-of-Service (QoS) parameters determining the serviceperformance, often used for recommendation, fluctuate over time. Thus, the QoSprediction is essential to identify a suitable service among functionallyequivalent services over time. The contemporary temporal QoS prediction methodshardly achieved the desired accuracy due to various limitations, such as theinability to handle data sparsity and outliers and capture higher-ordertemporal relationships among user-service interactions. Even though some recentrecurrent neural-network-based architectures can model temporal relationshipsamong QoS data, prediction accuracy degrades due to the absence of otherfeatures (e.g., collaborative features) to comprehend the relationship amongthe user-service interactions. This paper addresses the above challenges andproposes a scalable strategy for Temporal QoS Prediction using Multi-sourceCollaborative-Features (TPMCF), achieving high prediction accuracy and fasterresponsiveness. TPMCF combines the collaborative-features of users/services byexploiting user-service relationship with the spatio-temporal auto-extractedfeatures by employing graph convolution and transformer encoder with multi-headself-attention. We validated our proposed method on WS-DREAM-2 datasets.Extensive experiments showed TPMCF outperformed major state-of-the-artapproaches regarding prediction accuracy while ensuring high scalability andreasonably faster responsiveness.", "output": "TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language is essentially a complex, intricate system of human expressionsgoverned by grammatical rules. It poses a significant challenge to developcapable AI algorithms for comprehending and grasping a language. As a majorapproach, language modeling has been widely studied for language understandingand generation in the past two decades, evolving from statistical languagemodels to neural language models. Recently, pre-trained language models (PLMs)have been proposed by pre-training Transformer models over large-scale corpora,showing strong capabilities in solving various NLP tasks. Since researchershave found that model scaling can lead to performance improvement, they furtherstudy the scaling effect by increasing the model size to an even larger size.Interestingly, when the parameter scale exceeds a certain level, these enlargedlanguage models not only achieve a significant performance improvement but alsoshow some special abilities that are not present in small-scale languagemodels. To discriminate the difference in parameter scale, the researchcommunity has coined the term large language models (LLM) for the PLMs ofsignificant size. Recently, the research on LLMs has been largely advanced byboth academia and industry, and a remarkable progress is the launch of ChatGPT,which has attracted widespread attention from society. The technical evolutionof LLMs has been making an important impact on the entire AI community, whichwould revolutionize the way how we develop and use AI algorithms. In thissurvey, we review the recent advances of LLMs by introducing the background,key findings, and mainstream techniques. In particular, we focus on four majoraspects of LLMs, namely pre-training, adaptation tuning, utilization, andcapacity evaluation. Besides, we also summarize the available resources fordeveloping LLMs and discuss the remaining issues for future directions.", "output": "A Survey of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the largest and most comprehensive empirical study of pre-trainedvisual representations (PVRs) or visual 'foundation models' for Embodied AI.First, we curate CortexBench, consisting of 17 different tasks spanninglocomotion, navigation, dexterous, and mobile manipulation. Next, wesystematically evaluate existing PVRs and find that none are universallydominant.To study the effect of pre-training data scale and diversity, we combine over4,000 hours of egocentric videos from 7 different sources (over 5.6M images)and ImageNet to train different-sized vision transformers using MaskedAuto-Encoding (MAE) on slices of this data. Contrary to inferences from priorwork, we find that scaling dataset size and diversity does not improveperformance universally (but does so on average).Our largest model, named VC-1, outperforms all prior PVRs on average but doesnot universally dominate either. Finally, we show that task or domain-specificadaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achievingcompetitive or superior performance than the best known results on all of thebenchmarks in CortexBench. These models required over 10,000 GPU-hours to trainand can be found on our website for the benefit of the research community.", "output": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating 3D humans from images often produces implausible bodies that lean,float, or penetrate the floor. Such methods ignore the fact that bodies aretypically supported by the scene. A physics engine can be used to enforcephysical plausibility, but these are not differentiable, rely on unrealisticproxy bodies, and are difficult to integrate into existing optimization andlearning frameworks. In contrast, we exploit novel intuitive-physics (IP) termsthat can be inferred from a 3D SMPL body interacting with the scene. Inspiredby biomechanics, we infer the pressure heatmap on the body, the Center ofPressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). Withthese, we develop IPMAN, to estimate a 3D body from a color image in a \"stable\"configuration by encouraging plausible floor contact and overlapping CoP andCoM. Our IP terms are intuitive, easy to implement, fast to compute,differentiable, and can be integrated into existing optimization and regressionmethods. We evaluate IPMAN on standard datasets and MoYo, a new dataset withsynchronized multi-view images, ground-truth 3D bodies with complex poses,body-floor contact, CoM and pressure. IPMAN produces more plausible resultsthan the state of the art, improving accuracy for static poses, while nothurting dynamic ones. Code and data are available for research at", "output": "3D Human Pose Estimation via Intuitive Physics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have demonstrated the effectiveness of the combination ofmachine learning and logical reasoning, including data-driven logicalreasoning, knowledge driven machine learning and abductive learning, ininventing advanced technologies for different artificial intelligenceapplications. One-step abductive multi-target learning (OSAMTL), an approachinspired by abductive learning, via simply combining machine learning andlogical reasoning in a one-step balanced multi-target learning way, has as wellshown its effectiveness in handling complex noisy labels of a single noisysample in medical histopathology whole slide image analysis (MHWSIA). However,OSAMTL is not suitable for the situation where diverse noisy samples (DiNS) areprovided for a learning task. In this paper, giving definition of DiNS, wepropose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) toexpand the original OSAMTL to handle complex noisy labels of DiNS. ApplyingOSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show thatOSAMTL-DiNS is able to enable various state-of-the-art approaches for learningfrom noisy labels to achieve more rational predictions. We released a modelpretrained with OSAMTL-DiNS for tumour segmentation in HE-stained pre-treatmentbiopsy images in breast cancer, which has been successfully applied as apre-processing tool to extract tumour-associated stroma compartment forpredicting the pathological complete response to neoadjuvant chemotherapy inbreast cancer.", "output": "One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Peer review is a widely accepted mechanism for research evaluation, playing apivotal role in academic publishing. However, criticisms have long been leveledon this mechanism, mostly because of its poor efficiency and lowreproducibility. Recent years have seen the application of artificialintelligence (AI) in assisting the peer review process. Nonetheless, with theinvolvement of humans, such limitations remain inevitable. In this paper, wepropose the concept and pipeline of automated scholarly paper review (ASPR) andreview the relevant literature and technologies of achieving a full-scalecomputerized review process. On the basis of the review and discussion, weconclude that there is already corresponding research and preliminaryimplementation at each stage of ASPR. We further look into the challenges inASPR with the existing technologies. The major difficulties lie in imperfectdocument parsing and representation, inadequate data, defective human-computerinteraction, and flawed deep logical reasoning. Moreover, we discuss thepossible moral and ethical issues and point out the future directions of ASPR.In the foreseeable future, ASPR and peer review will coexist in a reinforcingmanner before ASPR is able to fully undertake the reviewing workload fromhumans.", "output": "Automated scholarly paper review: Concepts, technologies, and challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper resolves the open question of designing near-optimal algorithmsfor learning imperfect-information extensive-form games from bandit feedback.We present the first line of algorithms that require only$widetilde{mathcal{O}}((XA+YB)/varepsilon^2)$ episodes of play to find an$varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where$X,Y$ are the number of information sets and $A,B$ are the number of actionsfor the two players. This improves upon the best known sample complexity of$widetilde{mathcal{O}}((X^2A+Y^2B)/varepsilon^2)$ by a factor of$widetilde{mathcal{O}}(max{X, Y})$, and matches the information-theoreticlower bound up to logarithmic factors. We achieve this sample complexity by twonew algorithms: Balanced Online Mirror Descent, and Balanced CounterfactualRegret Minimization. Both algorithms rely on novel approaches of integratingemph{balanced exploration policies} into their classical counterparts. We alsoextend our results to learning Coarse Correlated Equilibria in multi-playergeneral-sum games.", "output": "Near-Optimal Learning of Extensive-Form Games with Imperfect Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose VRL3, a powerful data-driven framework with a simple design forsolving challenging visual deep reinforcement learning (DRL) tasks. We analyzea number of major obstacles in taking a data-driven approach, and present asuite of design principles, novel findings, and critical insights aboutdata-driven visual DRL. Our framework has three stages: in stage 1, we leveragenon-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations;in stage 2, we use offline RL data (e.g. a limited number of expertdemonstrations) to convert the task-agnostic representations into more powerfultask-specific representations; in stage 3, we fine-tune the agent with onlineRL. On a set of challenging hand manipulation tasks with sparse reward andrealistic visual inputs, compared to the previous SOTA, VRL3 achieves anaverage of 780% better sample efficiency. And on the hardest task, VRL3 is1220% more sample efficient (2440% when using a wider encoder) and solves thetask with only 10% of the computation. These significant results clearlydemonstrate the great potential of data-driven deep reinforcement learning.", "output": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Our work is the first attempt to apply Natural Language Processing toautomate the development of simulation models of systems vitally important forlogistics. We demonstrated that the framework built on top of the fine-tunedGPT-3 Codex, a Transformer-based language model, could produce functionallyvalid simulations of queuing and inventory control systems given the verbaldescription. In conducted experiments, GPT-3 Codex demonstrated convincingexpertise in Python as well as an understanding of the domain-specificvocabulary. As a result, the language model could produce simulations of asingle-product inventory-control system and single-server queuing system giventhe domain-specific context, a detailed description of the process, and a listof variables with the corresponding values. The demonstrated results, alongwith the rapid improvement of language models, open the door for significantsimplification of the workflow behind the simulation model development, whichwill allow experts to focus on the high-level consideration of the problem andholistic thinking.", "output": "From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the HIT-UAV dataset, a high-altitude infrared thermal dataset forobject detection applications on Unmanned Aerial Vehicles (UAVs). The datasetcomprises 2,898 infrared thermal images extracted from 43,470 frames inhundreds of videos captured by UAVs in various scenarios including schools,parking lots, roads, and playgrounds. Moreover, the HIT-UAV provides essentialflight data for each image, such as flight altitude, camera perspective, date,and daylight intensity. For each image, we have manually annotated objectinstances with bounding boxes of two types (oriented and standard) to tacklethe challenge of significant overlap of object instances in aerial images. Tothe best of our knowledge, the HIT-UAV is the first publicly availablehigh-altitude UAV-based infrared thermal dataset for detecting persons andvehicles. We have trained and evaluated well-established object detectionalgorithms on the HIT-UAV. Our results demonstrate that the detectionalgorithms perform exceptionally well on the HIT-UAV compared to visual lightdatasets since infrared thermal images do not contain significant irrelevantinformation about objects. We believe that the HIT-UAV will contribute tovarious UAV-based applications and researches. The dataset is freely availableat ", "output": "HIT-UAV: A high-altitude infrared thermal dataset for Unmanned Aerial Vehicle-based object detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time-dependent data-generating distributions have proven to be difficult forgradient-based training of neural networks, as the greedy updates result incatastrophic forgetting of previously learned knowledge. Despite the progressin the field of continual learning to overcome this forgetting, we show that aset of common state-of-the-art methods still suffers from substantialforgetting upon starting to learn new tasks, except that this forgetting istemporary and followed by a phase of performance recovery. We refer to thisintriguing but potentially problematic phenomenon as the stability gap. Thestability gap had likely remained under the radar due to standard practice inthe field of evaluating continual learning models only after each task.Instead, we establish a framework for continual evaluation that usesper-iteration evaluation and we define a new set of metrics to quantifyworst-case performance. Empirically we show that experience replay,constraint-based replay, knowledge-distillation, and parameter regularizationmethods are all prone to the stability gap; and that the stability gap can beobserved in class-, task-, and domain-incremental learning benchmarks.Additionally, a controlled experiment shows that the stability gap increaseswhen tasks are more dissimilar. Finally, by disentangling gradients intoplasticity and stability components, we propose a conceptual explanation forthe stability gap.", "output": "Continual evaluation for lifelong learning: Identifying the stability gap."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Freely available and easy-to-use audio editing tools make it straightforwardto perform audio splicing. Convincing forgeries can be created by combiningvarious speech samples from the same person. Detection of such splices isimportant both in the public sector when considering misinformation, and in alegal context to verify the integrity of evidence. Unfortunately, most existingdetection algorithms for audio splicing use handcrafted features and makespecific assumptions. However, criminal investigators are often faced withaudio samples from unconstrained sources with unknown characteristics, whichraises the need for more generally applicable methods.With this work, we aim to take a first step towards unconstrained audiosplicing detection to address this need. We simulate various attack scenariosin the form of post-processing operations that may disguise splicing. Wepropose a Transformer sequence-to-sequence (seq2seq) network for splicingdetection and localization. Our extensive evaluation shows that the proposedmethod outperforms existing dedicated approaches for splicing detection [3, 10]as well as the general-purpose networks EfficientNet [28] and RegNet [25].", "output": "Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning, especially deep neural networks (DNNs), has been widely andsuccessfully adopted in many critical applications for its high effectivenessand efficiency. The rapid development of DNNs has benefited from the existenceof some high-quality datasets ($e.g.$, ImageNet), which allow researchers anddevelopers to easily verify the performance of their methods. Currently, almostall existing released datasets require that they can only be adopted foracademic or educational purposes rather than commercial purposes withoutpermission. However, there is still no good way to ensure that. In this paper,we formulate the protection of released datasets as verifying whether they areadopted for training a (suspicious) third-party model, where defenders can onlyquery the model while having no information about its parameters and trainingdetails. Based on this formulation, we propose to embed external patterns viabackdoor watermarking for the ownership verification to protect them. Ourmethod contains two main parts, including dataset watermarking and datasetverification. Specifically, we exploit poison-only backdoor attacks ($e.g.$,BadNets) for dataset watermarking and design a hypothesis-test-guided methodfor dataset verification. We also provide some theoretical analyses of ourmethods. Experiments on multiple benchmark datasets of different tasks areconducted, which verify the effectiveness of our method. The code forreproducing main experiments is available aturl{", "output": "Black-box Dataset Ownership Verification via Backdoor Watermarking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusion models (DDMs) have attracted attention for theirexceptional generation quality and diversity. This success is largelyattributed to the use of class- or text-conditional diffusion guidance methods,such as classifier and classifier-free guidance. In this paper, we present amore comprehensive perspective that goes beyond the traditional guidancemethods. From this generalized perspective, we introduce novel condition- andtraining-free strategies to enhance the quality of generated images. As asimple solution, blur guidance improves the suitability of intermediate samplesfor their fine-scale information and structures, enabling diffusion models togenerate higher quality samples with a moderate guidance scale. Improving uponthis, Self-Attention Guidance (SAG) uses the intermediate self-attention mapsof diffusion models to enhance their stability and efficacy. Specifically, SAGadversarially blurs only the regions that diffusion models attend to at eachiteration and guides them accordingly. Our experimental results show that ourSAG improves the performance of various diffusion models, including ADM, IDDPM,Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidancemethods leads to further improvement.", "output": "Improving Sample Quality of Diffusion Models Using Self-Attention Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Reinforcement Learning (RL) has emerged as a powerful paradigm fortraining neural policies to solve complex control tasks. However, thesepolicies tend to be overfit to the exact specifications of the task andenvironment they were trained on, and thus do not perform well when conditionsdeviate slightly or when composed hierarchically to solve even more complextasks. Recent work has shown that training a mixture of policies, as opposed toa single one, that are driven to explore different regions of the state-actionspace can address this shortcoming by generating a diverse set of behaviors,referred to as skills, that can be collectively used to great effect inadaptation tasks or for hierarchical planning. This is typically realized byincluding a diversity term - often derived from information theory - in theobjective function optimized by RL. However these approaches often requirecareful hyperparameter tuning to be effective. In this work, we demonstratethat less widely-used neuroevolution methods, specifically Quality Diversity(QD), are a competitive alternative to information-theory-augmented RL forskill discovery. Through an extensive empirical evaluation comparing eightstate-of-the-art algorithms (four flagship algorithms from each line of work)on the basis of (i) metrics directly evaluating the skills' diversity, (ii) theskills' performance on adaptation tasks, and (iii) the skills' performance whenused as primitives for hierarchical planning; QD methods are found to provideequal, and sometimes improved, performance whilst being less sensitive tohyperparameters and more scalable. As no single method is found to providenear-optimal performance across all environments, there is a rich scope forfurther research which we support by proposing future directions and providingoptimized open-source implementations.", "output": "Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent 3D generative models have achieved remarkable performance insynthesizing high resolution photorealistic images with view consistency anddetailed 3D shapes, but training them for diverse domains is challenging sinceit requires massive training images and their camera distribution information.Text-guided domain adaptation methods have shown impressive performance onconverting the 2D generative model on one domain into the models on otherdomains with different styles by leveraging the CLIP (ContrastiveLanguage-Image Pre-training), rather than collecting massive datasets for thosedomains. However, one drawback of them is that the sample diversity in theoriginal generative model is not well-preserved in the domain-adaptedgenerative models due to the deterministic nature of the CLIP text encoder.Text-guided domain adaptation will be even more challenging for 3D generativemodels not only because of catastrophic diversity loss, but also because ofinferior text-image correspondence and poor image quality. Here we proposeDATID-3D, a domain adaptation method tailored for 3D generative models usingtext-to-image diffusion models that can synthesize diverse images per textprompt without collecting additional images and camera information for thetarget domain. Unlike 3D extensions of prior text-guided domain adaptationmethods, our novel pipeline was able to fine-tune the state-of-the-art 3Dgenerator of the source domain to synthesize high resolution, multi-viewconsistent images in text-guided targeted domains without additional data,outperforming the existing text-guided domain adaptation methods in diversityand text-image correspondence. Furthermore, we propose and demonstrate diverse3D image manipulations such as one-shot instance-selected adaptation andsingle-view manipulated 3D reconstruction to fully enjoy diversity in text.", "output": "DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To simulate bosons on a qubit- or qudit-based quantum computer, one has toregularize the theory by truncating infinite-dimensional local Hilbert spacesto finite dimensions. In the search for practical quantum applications, it isimportant to know how big the truncation errors can be. In general, it is noteasy to estimate errors unless we have a good quantum computer. In this paperwe show that traditional sampling methods on classical devices, specificallyMarkov Chain Monte Carlo, can address this issue with a reasonable amount ofcomputational resources available today. As a demonstration, we apply this ideato the scalar field theory on a two-dimensional lattice, with a size that goesbeyond what is achievable using exact diagonalization methods. This method canbe used to estimate the resources needed for realistic quantum simulations ofbosonic theories, and also, to check the validity of the results of thecorresponding quantum simulations.", "output": "Estimating truncation effects of quantum bosonic systems using sampling algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper demonstrates an approach for learning highly semantic imagerepresentations without relying on hand-crafted data-augmentations. Weintroduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), anon-generative approach for self-supervised learning from images. The ideabehind I-JEPA is simple: from a single context block, predict therepresentations of various target blocks in the same image. A core designchoice to guide I-JEPA towards producing semantic representations is themasking strategy; specifically, it is crucial to (a) sample target blocks withsufficiently large scale (semantic), and to (b) use a sufficiently informative(spatially distributed) context block. Empirically, when combined with VisionTransformers, we find I-JEPA to be highly scalable. For instance, we train aViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strongdownstream performance across a wide range of tasks, from linear classificationto object counting and depth prediction.", "output": "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) are known to be vulnerable to adversarialgeometric transformation. This paper aims to verify the robustness oflarge-scale DNNs against the combination of multiple geometric transformationswith a provable guarantee. Given a set of transformations (e.g., rotation,scaling, etc.), we develop GeoRobust, a black-box robustness analyser builtupon a novel global optimisation strategy, for locating the worst-casecombination of transformations that affect and even alter a network's output.GeoRobust can provide provable guarantees on finding the worst-case combinationbased on recent advances in Lipschitzian theory. Due to its black-box nature,GeoRobust can be deployed on large-scale DNNs regardless of theirarchitectures, activation functions, and the number of neurons. In practice,GeoRobust can locate the worst-case geometric transformation with highprecision for the ResNet50 model on ImageNet in a few seconds on average. Weexamined 18 ImageNet classifiers, including the ResNet family and visiontransformers, and found a positive correlation between the geometric robustnessof the networks and the parameter numbers. We also observe that increasing thedepth of DNN is more beneficial than increasing its width in terms of improvingits geometric robustness. Our tool GeoRobust is available at", "output": "Towards Verifying the Geometric Robustness of Large-scale Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An increasing number of monitoring systems have been developed in smartcities to ensure that the real-time operations of a city satisfy safety andperformance requirements. However, many existing city requirements are writtenin English with missing, inaccurate, or ambiguous information. There is a highdemand for assisting city policymakers in converting human-specifiedrequirements to machine-understandable formal specifications for monitoringsystems. To tackle this limitation, we build CitySpec, the first intelligentassistant system for requirement specification in smart cities. To createCitySpec, we first collect over 1,500 real-world city requirements acrossdifferent domains (e.g., transportation and energy) from over 100 cities andextract city-specific knowledge to generate a dataset of city vocabulary with3,061 words. We also build a translation model and enhance it throughrequirement synthesis and develop a novel online learning framework withshielded validation. The evaluation results on real-world city requirementsshow that CitySpec increases the sentence-level accuracy of requirementspecification from 59.02% to 86.64%, and has strong adaptability to a new cityand a new domain (e.g., the F1 score for requirements in Seattle increases from77.6% to 93.75% with online learning). After the enhancement from the shieldfunction, CitySpec is now immune to most known textual adversarial inputs(e.g., the attack success rate of DeepWordBug after the shield function isreduced to 0% from 82.73%). We test the CitySpec with 18 participants fromdifferent domains. CitySpec shows its strong usability and adaptability todifferent domains, and also its robustness to malicious inputs.", "output": "CitySpec with Shield: A Secure Intelligent Assistant for Requirement Formalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models in the literature are optimized with various objectives thatare special cases of a weighted loss, where the weighting function specifiesthe weight per noise level. Uniform weighting corresponds to maximizing theELBO, a principled approximation of maximum likelihood. In current practicediffusion models are optimized with non-uniform weighting due to better resultsin terms of sample quality. In this work we expose a direct relationshipbetween the weighted loss (with any weighting) and the ELBO objective.We show that the weighted loss can be written as a weighted integral ofELBOs, with one ELBO per noise level. If the weighting function is monotonic,then the weighted loss is a likelihood-based objective: it maximizes the ELBOunder simple data augmentation, namely Gaussian noise perturbation. Our maincontribution is a deeper theoretical understanding of the diffusion objective,but we also performed some experiments comparing monotonic with non-monotonicweightings, finding that monotonic weighting performs competitively with thebest published results.", "output": "Understanding the Diffusion Objective as a Weighted Integral of ELBOs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we introduce BO-Muse, a new approach to human-AI teaming forthe optimization of expensive black-box functions. Inspired by the intrinsicdifficulty of extracting expert knowledge and distilling it back into AI modelsand by observations of human behavior in real-world experimental design, ouralgorithm lets the human expert take the lead in the experimental process. Thehuman expert can use their domain expertise to its full potential, while the AIplays the role of a muse, injecting novelty and searching for areas of weaknessto break the human out of over-exploitation induced by cognitive entrenchment.With mild assumptions, we show that our algorithm converges sub-linearly, at arate faster than the AI or human alone. We validate our algorithm usingsynthetic data and with human experts performing real-world experiments.", "output": "BO-Muse: A human expert and AI teaming framework for accelerated experimental design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The emergence of pretrained large language models has led to the deploymentof a range of social chatbots for chitchat. Although these chatbots demonstratelanguage ability and fluency, they are not guaranteed to be engaging and canstruggle to retain users. This work investigates the development of socialchatbots that prioritize user engagement to enhance retention, specificallyexamining the use of human feedback to efficiently develop highly engagingchatbots. The proposed approach uses automatic pseudo-labels collected fromuser interactions to train a reward model that can be used to rejectlow-scoring sample responses generated by the chatbot model at inference time.Intuitive evaluation metrics, such as mean conversation length (MCL), areintroduced as proxies to measure the level of engagement of deployed chatbots.A/B testing on groups of 10,000 new daily chatbot users on the Chai Researchplatform shows that this approach increases the MCL by up to 70%, whichtranslates to a more than 30% increase in user retention for a GPT-J 6B model.Future work aims to use the reward model to realise a data fly-wheel, where thelatest user conversations can be used to alternately fine-tune the languagemodel and the reward model.", "output": "Rewarding Chatbots for Real-World Engagement with Millions of Users."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models have demonstrated surprising ability to performin-context learning, i.e., these models can be directly applied to solvenumerous downstream tasks by conditioning on a prompt constructed by a fewinput-output examples. However, prior research has shown that in-contextlearning can suffer from high instability due to variations in trainingexamples, example order, and prompt formats. Therefore, the construction of anappropriate prompt is essential for improving the performance of in-contextlearning. In this paper, we revisit this problem from the view of predictivebias. Specifically, we introduce a metric to evaluate the predictive bias of afixed prompt against labels or a given attributes. Then we empirically showthat prompts with higher bias always lead to unsatisfactory predictive quality.Based on this observation, we propose a novel search strategy based on thegreedy search to identify the near-optimal prompt for improving the performanceof in-context learning. We perform comprehensive experiments withstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.Our results indicate that our method can enhance the model's in-contextlearning performance in an effective and interpretable manner.", "output": "Fairness-guided Few-shot Prompting for Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While code-mixing is a common linguistic practice in many parts of the world,collecting high-quality and low-cost code-mixed data remains a challenge fornatural language processing (NLP) research. The proliferation of Large LanguageModels (LLMs) in recent times compels one to ask: can these systems be used fordata generation? In this article, we explore prompting multilingual LLMs in azero-shot manner to create code-mixed data for five languages in South EastAsia (SEA) -- Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as thecreole language Singlish. We find that ChatGPT shows the most potential,capable of producing code-mixed text 68% of the time when the term\"code-mixing\" is explicitly defined. Moreover, both ChatGPT's and InstructGPT's(davinci-003) performances in generating Singlish texts are noteworthy,averaging a 96% success rate across a variety of prompts. Their code-mixingproficiency, however, is dampened by word choice errors that lead to semanticinaccuracies. Other multilingual models such as BLOOMZ and Flan-T5-XXL areunable to produce code-mixed texts altogether. By highlighting the limitedpromises of LLMs in a specific form of low-resource data generation, we callfor a measured approach when applying similar techniques to other data-scarceNLP contexts.", "output": "Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a novel approach to Multi-Agent Reinforcement Learning(MARL) that combines cooperative task decomposition with the learning of rewardmachines (RMs) encoding the structure of the sub-tasks. The proposed methodhelps deal with the non-Markovian nature of the rewards in partially observableenvironments and improves the interpretability of the learnt policies requiredto complete the cooperative task. The RMs associated with each sub-task arelearnt in a decentralised manner and then used to guide the behaviour of eachagent. By doing so, the complexity of a cooperative multi-agent problem isreduced, allowing for more effective learning. The results suggest that ourapproach is a promising direction for future research in MARL, especially incomplex environments with large state spaces and multiple agents.", "output": "Learning Reward Machines in Cooperative Multi-Agent Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite its pivotal role in research experiments, code correctness is oftenpresumed only on the basis of the perceived quality of the results. This comeswith the risk of erroneous outcomes and potentially misleading findings. Toaddress this issue, we posit that the current focus on result reproducibilityshould go hand in hand with the emphasis on coding best practices. We bolsterour call to the NLP community by presenting a case study, in which we identify(and correct) three bugs in widely used open-source implementations of thestate-of-the-art Conformer architecture. Through comparative experiments onautomatic speech recognition and translation in various language settings, wedemonstrate that the existence of bugs does not prevent the achievement of goodand reproducible results and can lead to incorrect conclusions that potentiallymisguide future research. In response to this, this study is a call to actiontoward the adoption of coding best practices aimed at fostering correctness andimproving the quality of the developed software.", "output": "Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object-centric process discovery (OCPD) constitutes a paradigm shift inprocess mining. Instead of assuming a single case notion present in the eventlog, OCPD can handle events without a single case notion, but that are insteadrelated to a collection of objects each having a certain type. The object typesconstitute multiple, interacting case notions. The output of OCPD is anobject-centric Petri net, i.e. a Petri net with object-typed places, thatrepresents the parallel execution of multiple execution flows corresponding toobject types. Similar to classical process discovery, where we aim forbehaviorally sound process models as a result, in OCPD, we aim for soundness ofthe resulting object-centric Petri nets. However, the existing OCPD approachcan result in violations of soundness. As we will show, one violation arisesfor multiple interacting object types with loops that arise in collaborativesystems. This paper proposes an extended OCPD approach and proves that it doesnot suffer from this violation of soundness of the resulting object-centricPetri nets. We also show how we prevent the OCPD approach from introducingspurious interactions in the discovered object-centric Petri net. The proposedframework is prototypically implemented.", "output": "Preventing Object-centric Discovery of Unsound Process Models for Object Interactions with Loops in Collaborative Systems: Extended Version."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attention (and distraction) recognition is a key factor in improvinghuman-robot collaboration. We present an assembly scenario where a humanoperator and a cobot collaborate equally to piece together a gearbox. The setupprovides multiple opportunities for the cobot to adapt its behavior dependingon the operator's attention, which can improve the collaboration experience andreduce psychological strain. As a first step, we recognize the areas in theworkspace that the human operator is paying attention to, and consequently,detect when the operator is distracted. We propose a novel deep-learningapproach to develop an attention recognition model. First, we train aconvolutional neural network to estimate the gaze direction using a publiclyavailable image dataset. Then, we use transfer learning with a small dataset tomap the gaze direction onto pre-defined areas of interest. Models trained usingthis approach performed very well in leave-one-subject-out evaluation on thesmall dataset. We performed an additional validation of our models using thevideo snippets collected from participants working as an operator in thepresented assembly scenario. Although the recall for the Distracted class waslower in this case, the models performed well in recognizing the areas theoperator paid attention to. To the best of our knowledge, this is the firstwork that validated an attention recognition model using data from a settingthat mimics industrial human-robot collaboration. Our findings highlight theneed for validation of attention recognition solutions in such full-fledged,non-guided scenarios.", "output": "Gaze-based Attention Recognition for Human-Robot Collaboration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated endoscopy video analysis is a challenging task in medical computervision, with the primary objective of assisting surgeons during procedures. Thedifficulty arises from the complexity of surgical scenes and the lack of asufficient amount of annotated data. In recent years, large-scale pretraininghas shown great success in natural language processing and computer visioncommunities. These approaches reduce the need for annotated data, which isalways a concern in the medical domain. However, most works on endoscopic videounderstanding use models pretrained on natural images, creating a domain gapbetween pretraining and finetuning. In this work, we investigate the need forendoscopy domain-specific pretraining based on downstream objectives. To thisend, we first collect Endo700k, the largest publicly available corpus ofendoscopic images, extracted from nine public Minimally Invasive Surgery (MIS)datasets. Endo700k comprises more than 700,000 unannotated raw images. Next, weintroduce EndoViT, an endoscopy pretrained Vision Transformer (ViT). Throughablations, we demonstrate that domain-specific pretraining is particularlybeneficial for more complex downstream tasks, such as Action Triplet Detection,and less effective and even unnecessary for simpler tasks, such as SurgicalPhase Recognition. We will release both our code and pretrained models uponacceptance to facilitate further research in this direction.", "output": "Whether and When does Endoscopy Domain Pretraining Make Sense?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores training medical vision-language models (VLMs) -- wherethe visual and language inputs are embedded into a common space -- with aparticular focus on scenarios where training data is limited, as is often thecase in clinical datasets. We explore several candidate methods to improvelow-data performance, including: (i) adapting generic pre-trained models tonovel image and text domains (i.e. medical imaging and reports) via unimodalself-supervision; (ii) using local (e.g. GLoRIA) &amp; global (e.g. InfoNCE)contrastive loss functions as well as a combination of the two; (iii) extrasupervision during VLM training, via: (a) image- and text-onlyself-supervision, and (b) creating additional positive image-text pairs fortraining through augmentation and nearest-neighbour search.Using text-to-image retrieval as a benchmark, we evaluate the performance ofthese methods with variable sized training datasets of paired chest X-rays andradiological reports. Combined, they significantly improve retrieval comparedto fine-tuning CLIP, roughly equivalent to training with the data. A similarpattern is found in the downstream task classification of CXR-relatedconditions with our method outperforming CLIP and also BioVIL, a strong CXR VLMbenchmark, in the zero-shot and linear probing settings. We conclude with a setof recommendations for researchers aiming to train vision-language models onother medical imaging modalities when training data is scarce. To facilitatefurther research, we will make our code and models publicly available.", "output": "Vision-Language Modelling For Radiological Imaging and Reports In The Low Data Regime."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The hardware-efficiency and accuracy of Deep Neural Networks (DNNs)implemented on In-memory Computing (IMC) architectures primarily depend on theDNN architecture and the peripheral circuit parameters. It is thereforeessential to holistically co-search the network and peripheral parameters toachieve optimal performance. To this end, we propose XPert, which co-searchesnetwork architecture in tandem with peripheral parameters such as the type andprecision of analog-to-digital converters, crossbar column sharing and thelayer-specific input precision using an optimization-based design spaceexploration. Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lowerEDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%)accuracy for CIFAR10 (TinyImagenet) datasets. The code for this paper isavailable at ", "output": "XPert: Peripheral Circuit & Neural Architecture Co-search for Area and Energy-efficient Xbar-based Computing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Characters are essential to the plot of any story. Establishing thecharacters before writing a story can improve the clarity of the plot and theoverall flow of the narrative. However, previous work on visual storytellingtends to focus on detecting objects in images and discovering relationshipsbetween them. In this approach, characters are not distinguished from otherobjects when they are fed into the generation pipeline. The result is acoherent sequence of events rather than a character-centric story. In order toaddress this limitation, we introduce the VIST-Character dataset, whichprovides rich character-centric annotations, including visual and textualco-reference chains and importance ratings for characters. Based on thisdataset, we propose two new tasks: important character detection and charactergrounding in visual stories. For both tasks, we develop simple, unsupervisedmodels based on distributional similarity and pre-trained vision-and-languagemodels. Our new dataset, together with these models, can serve as thefoundation for subsequent work on analysing and generating stories from acharacter-centric perspective.", "output": "Detecting and Grounding Important Characters in Visual Stories."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning models deployed in the open world may encounter observationsthat they were not trained to recognize, and they risk misclassifying suchobservations with high confidence. Therefore, it is essential that these modelsare able to ascertain what is in-distribution (ID) and out-of-distribution(OOD), to avoid this misclassification. In recent years, huge strides have beenmade in creating models that are robust to this distinction. As a result, thecurrent state-of-the-art has reached near perfect performance on relativelycoarse-grained OOD detection tasks, such as distinguishing horses from trucks,while struggling with finer-grained classification, like differentiating modelsof commercial aircraft. In this paper, we describe a new theoretical frameworkfor understanding fine- and coarse-grained OOD detection, we re-conceptualizefine grained classification into a three part problem, and we propose a newbaseline task for OOD models on two fine-grained hierarchical data sets, twonew evaluation methods to differentiate fine- and coarse-grained OODperformance, along with a new loss function for models in this task.", "output": "Establishing baselines and introducing TernaryMixOE for fine-grained out-of-distribution detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Virtual try-on, i.e making people virtually try new garments, is an activeresearch area in computer vision with great commercial applications. Currentvirtual try-on methods usually work in a two-stage pipeline. First, the garmentimage is warped on the person's pose using a flow estimation network. Then inthe second stage, the warped garment is fused with the person image to render anew try-on image. Unfortunately, such methods are heavily dependent on thequality of the garment warping which often fails when dealing with hard poses(e.g., a person lifting or crossing arms). In this work, we propose a robustwarping method for virtual try-on based on a learned garment DensePose whichhas a direct correspondence with the person's DensePose. Due to the lack ofannotated data, we show how to leverage an off-the-shelf person DensePose modeland a pretrained flow model to learn the garment DensePose in a weaklysupervised manner. The garment DensePose allows a robust warping to anyperson's pose without any additional computation. Our method achieves thestate-of-the-art equivalent on virtual try-on benchmarks and shows warpingrobustness on in-the-wild person images with hard poses, making it more suitedfor real-world virtual try-on applications.", "output": "Learning Garment DensePose for Robust Warping in Virtual Try-On."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Dual Cross-Attention (DCA), a simple yet effective attentionmodule that is able to enhance skip-connections in U-Net-based architecturesfor medical image segmentation. DCA addresses the semantic gap between encoderand decoder features by sequentially capturing channel and spatial dependenciesacross multi-scale encoder features. First, the Channel Cross-Attention (CCA)extracts global channel-wise dependencies by utilizing cross-attention acrosschannel tokens of multi-scale encoder features. Then, the SpatialCross-Attention (SCA) module performs cross-attention to capture spatialdependencies across spatial tokens. Finally, these fine-grained encoderfeatures are up-sampled and connected to their corresponding decoder parts toform the skip-connection scheme. Our proposed DCA module can be integrated intoany encoder-decoder architecture with skip-connections such as U-Net and itsvariants. We test our DCA module by integrating it into six U-Net-basedarchitectures such as U-Net, V-Net, R2Unet, ResUnet++, DoubleUnet andMultiResUnet. Our DCA module shows Dice Score improvements up to 2.05% on GlaS,2.74% on MoNuSeg, 1.37% on CVC-ClinicDB, 1.12% on Kvasir-Seg and 1.44% onSynapse datasets. Our codes are available at:", "output": "Dual Cross-Attention for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we propose a novel method for zero-shot, cross-domain imageretrieval in which we make two key contributions. The first is a test-timere-ranking procedure that enables query-gallery pairs, without meaningfulshared visual features, to be matched by incorporating gallery-gallery ranksinto an iterative re-ranking process. The second is the use of cross-attentionat training time and knowledge distillation to encourage cross-attention-likefeatures to be extracted at test time from a single image. When combined withthe Vision Transformer architecture and zero-shot retrieval losses, ourapproach yields state-of-the-art results on the Sketchy and TU-Berlinsketch-based image retrieval benchmarks. However, unlike many previous methods,none of the components in our approach are engineered specifically towards thesketch-based image retrieval task - it can be generally applied to anycross-domain, zero-shot retrieval task. We therefore also show results onzero-shot cartoon-to-photo retrieval using the Office-Home dataset.", "output": "If At First You Don't Succeed: Test Time Re-ranking for Zero-shot, Cross-domain Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural rendering of implicit surfaces performs well in 3D visionapplications. However, it requires dense input views as supervision. When onlysparse input images are available, output quality drops significantly due tothe shape-radiance ambiguity problem. We note that this ambiguity can beconstrained when a 3D point is visible in multiple views, as is the case inmulti-view stereo (MVS). We thus propose to regularize neural renderingoptimization with an MVS solution. The use of an MVS probability volume and ageneralized cross entropy loss leads to a noise-tolerant optimization process.In addition, neural rendering provides global consistency constraints thatguide the MVS depth hypothesis sampling and thus improves MVS performance.Given only three sparse input views, experiments show that our method not onlyoutperforms generic neural rendering models by a large margin but alsosignificantly increases the reconstruction quality of MVS models. Projectwebpage: ", "output": "S-VolSDF: Sparse Multi-View Stereo Regularization of Neural Implicit Surfaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "International benchmarking competitions have become fundamental for thecomparative performance assessment of image analysis methods. However, littleattention has been given to investigating what can be learnt from thesecompetitions. Do they really generate scientific progress? What are common andsuccessful participation strategies? What makes a solution superior to acompeting method? To address this gap in the literature, we performed amulti-center study with all 80 competitions that were conducted in the scope ofIEEE ISBI 2021 and MICCAI 2021. Statistical analyses performed based oncomprehensive descriptions of the submitted algorithms linked to their rank aswell as the underlying participation strategies revealed common characteristicsof winning solutions. These typically include the use of multi-task learning(63%) and/or multi-stage pipelines (61%), and a focus on augmentation (100%),image preprocessing (97%), data curation (79%), and postprocessing (66%). The\"typical\" lead of a winning team is a computer scientist with a doctoraldegree, five years of experience in biomedical image analysis, and four yearsof experience in deep learning. Two core general development strategies stoodout for highly-ranked teams: the reflection of the metrics in the method designand the focus on analyzing and handling failure cases. According to theorganizers, 43% of the winning algorithms exceeded the state of the art butonly 11% completely solved the respective domain problem. The insights of ourstudy could help researchers (1) improve algorithm development strategies whenapproaching new problems, and (2) focus on open research questions revealed bythis work.", "output": "Why is the winner the best?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks have been proven to be both highly effective within computervision, and highly vulnerable to adversarial attacks. Consequently, as the useof neural networks increases due to their unrivaled performance, so too doesthe threat posed by adversarial attacks. In this work, we build towardsaddressing the challenge of adversarial robustness by exploring therelationship between the mini-batch size used during adversarial samplegeneration and the strength of the adversarial samples produced. We demonstratethat an increase in mini-batch size results in a decrease in the efficacy ofthe samples produced, and we draw connections between these observations andthe phenomenon of vanishing gradients. Next, we formulate loss functions suchthat adversarial sample strength is not degraded by mini-batch size. Ourfindings highlight a potential risk for underestimating the true (practical)strength of adversarial attacks, and a risk of overestimating a model'srobustness. We share our codes to let others replicate our experiments and tofacilitate further exploration of the connections between batch size andadversarial sample strength.", "output": "Generating Adversarial Samples in Mini-Batches May Be Detrimental To Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There have been tremendous efforts over the past decades dedicated to thegeneration of realistic graphs in a variety of domains, ranging from socialnetworks to computer networks, from gene regulatory networks to onlinetransaction networks. Despite the remarkable success, the vast majority ofthese works are unsupervised in nature and are typically trained to minimizethe expected graph reconstruction loss, which would result in therepresentation disparity issue in the generated graphs, i.e., the protectedgroups (often minorities) contribute less to the objective and thus suffer fromsystematically higher errors. In this paper, we aim to tailor graph generationto downstream mining tasks by leveraging label information and user-preferredparity constraint. In particular, we start from the investigation ofrepresentation disparity in the context of graph generative models. To mitigatethe disparity, we propose a fairness-aware graph generative model namedFairGen. Our model jointly trains a label-informed graph generation module anda fair representation learning module by progressively learning the behaviorsof the protected and unprotected groups, from the `easy' concepts to the `hard'ones. In addition, we propose a generic context sampling strategy for graphgenerative models, which is proven to be capable of fairly capturing thecontextual information of each group with a high probability. Experimentalresults on seven real-world data sets, including web-based graphs, demonstratethat FairGen (1) obtains performance on par with state-of-the-art graphgenerative models across six network properties, (2) mitigates therepresentation disparity issues in the generated graphs, and (3) substantiallyboosts the model performance by up to 17% in downstream tasks via dataaugmentation.", "output": "FairGen: Towards Fair Graph Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The analysis of 3D point clouds has diverse applications in robotics, visionand graphics. Processing them presents specific challenges since they arenaturally sparse, can vary in spatial resolution and are typically unordered.Graph-based networks to abstract features have emerged as a promisingalternative to convolutional neural networks for their analysis, but these canbe computationally heavy as well as memory inefficient. To address theselimitations we introduce a novel Multi-level Graph Convolution Neural (MLGCN)model, which uses Graph Neural Networks (GNN) blocks to extract features from3D point clouds at specific locality levels. Our approach employs precomputedgraph KNNs, where each KNN graph is shared between GCN blocks inside a GNNblock, making it both efficient and effective compared to present models. Wedemonstrate the efficacy of our approach on point cloud based objectclassification and part segmentation tasks on benchmark datasets, showing thatit produces comparable results to those of state-of-the-art models whilerequiring up to a thousand times fewer floating-point operations (FLOPs) andhaving significantly reduced storage requirements. Thus, our MLGCN model couldbe particular relevant to point cloud based 3D shape analysis in industrialapplications when computing resources are scarce.", "output": "MLGCN: An Ultra Efficient Graph Convolution Neural Model For 3D Point Cloud Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rainy weather significantly deteriorates the visibility of scene objects,particularly when images are captured through outdoor camera lenses orwindshields. Through careful observation of numerous rainy photos, we havefound that the images are generally affected by various rainwater artifactssuch as raindrops, rain streaks, and rainy haze, which impact the image qualityfrom both near and far distances, resulting in a complex and intertwinedprocess of image degradation. However, current deraining techniques are limitedin their ability to address only one or two types of rainwater, which poses achallenge in removing the mixture of rain (MOR). In this study, we propose aneffective image deraining paradigm for Mixture of rain REmoval, calledDEMore-Net, which takes full account of the MOR effect. Going beyond theexisting deraining wisdom, DEMore-Net is a joint learning paradigm thatintegrates depth estimation and MOR removal tasks to achieve superior rainremoval. The depth information can offer additional meaningful guidanceinformation based on distance, thus better helping DEMore-Net remove differenttypes of rainwater. Moreover, this study explores normalization approaches inimage deraining tasks and introduces a new Hybrid Normalization Block (HNB) toenhance the deraining performance of DEMore-Net. Extensive experimentsconducted on synthetic datasets and real-world MOR photos fully validate thesuperiority of the proposed DEMore-Net. Code is available at", "output": "Joint Depth Estimation and Mixture of Rain Removal From a Single Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a 3D object, kinematic motion prediction aims to identify the mobileparts as well as the corresponding motion parameters. Due to the largevariations in both topological structure and geometric details of 3D objects,this remains a challenging task and the lack of large scale labeled data alsoconstrain the performance of deep learning based approaches. In this paper, wetackle the task of object kinematic motion prediction problem in a semi-weaklysupervised manner. Our key observations are two-fold. First, although 3Ddataset with fully annotated motion labels is limited, there are existingdatasets and methods for object part semantic segmentation at large scale.Second, semantic part segmentation and mobile part segmentation is not alwaysconsistent but it is possible to detect the mobile parts from the underlying 3Dstructure. Towards this end, we propose a graph neural network to learn the mapbetween hierarchical part-level segmentation and mobile parts parameters, whichare further refined based on geometric alignment. This network can be firsttrained on PartNet-Mobility dataset with fully labeled mobility information andthen applied on PartNet dataset with fine-grained and hierarchical part-levelsegmentation. The network predictions yield a large scale of 3D objects withpseudo labeled mobility information and can further be used forweakly-supervised learning with pre-existing segmentation. Our experiments showthere are significant performance boosts with the augmented data for previousmethod designed for kinematic motion prediction on 3D partial scans.", "output": "Semi-Weakly Supervised Object Kinematic Motion Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When interacting in a three dimensional world, humans must estimate 3Dstructure from visual inputs projected down to two dimensional retinal images.It has been shown that humans use the persistence of object shape overmotion-induced transformations as a cue to resolve depth ambiguity when solvingthis underconstrained problem. With the aim of understanding how biologicalvision systems may internally represent 3D transformations, we propose acomputational model, based on a generative manifold model, which can be used toinfer 3D structure from the motion of 2D points. Our model can also learnrepresentations of the transformations with minimal supervision, providing aproof of concept for how humans may develop internal representations on adevelopmental or evolutionary time scale. Focused on rotational motion, we showhow our model infers depth from moving 2D projected points, learns 3Drotational transformations from 2D training stimuli, and compares to humanperformance on psychophysical structure-from-motion experiments.", "output": "Learning Internal Representations of 3D Transformations from 2D Projected Inputs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present CrossLoc3D, a novel 3D place recognition method that solves alarge-scale point matching problem in a cross-source setting. Cross-sourcepoint cloud data corresponds to point sets captured by depth sensors withdifferent accuracies or from different distances and perspectives. We addressthe challenges in terms of developing 3D place recognition methods that accountfor the representation gap between points captured by different sources. Ourmethod handles cross-source data by utilizing multi-grained features andselecting convolution kernel sizes that correspond to most prominent features.Inspired by the diffusion models, our method uses a novel iterative refinementprocess that gradually shifts the embedding spaces from different sources to asingle canonical space for better metric learning. In addition, we presentCS-Campus3D, the first 3D aerial-ground cross-source dataset consisting ofpoint cloud data from both aerial and ground LiDAR scans. The point clouds inCS-Campus3D have representation gaps and other features like different views,point densities, and noise patterns. We show that our CrossLoc3D algorithm canachieve an improvement of 4.74% - 15.37% in terms of the top 1 average recallon our CS-Campus3D benchmark and achieves performance comparable tostate-of-the-art 3D place recognition method on the Oxford RobotCar. We willrelease the code and CS-Campus3D benchmark.", "output": "CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world images taken by different cameras with different degradationkernels often result in a cross-device domain gap in image super-resolution. Aprevalent attempt to this issue is unsupervised domain adaptation (UDA) thatneeds to access source data. Considering privacy policies or transmissionrestrictions of data in many practical applications, we propose a SOurce-freeimage Super-Resolution framework (SOSR) to address this issue, i.e., adapt amodel pre-trained on labeled source data to a target domain with only unlabeledtarget data. SOSR leverages the source model to generate refined pseudo-labelsfor teacher-student learning. To better utilize the pseudo-labels, this paperproposes a novel wavelet-based augmentation method, named Wavelet AugmentationTransformer (WAT), which can be flexibly incorporated with existing networks,to implicitly produce useful augmented data. WAT learns low-frequencyinformation of varying levels across diverse samples, which is aggregatedefficiently via deformable attention. Furthermore, an uncertainty-awareself-training mechanism is proposed to improve the accuracy of pseudo-labels,with inaccurate predictions being rectified by uncertainty estimation. Toacquire better SR results and avoid overfitting pseudo-labels, severalregularization losses are proposed to constrain the frequency informationbetween target LR and SR images. Experiments show that without accessing sourcedata, SOSR achieves superior results to the state-of-the-art UDA methods.", "output": "SOSR: Source-Free Image Super-Resolution with Wavelet Augmentation Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One-shot talking head generation has received growing attention in recentyears, with various creative and practical applications. An ideal natural andvivid generated talking head video should contain natural head pose changes.However, it is challenging to map head pose sequences from driving audio sincethere exists a natural gap between audio-visual modalities. In this work, wepropose a Flow-guided One-shot model that achieves NaTural head motions(FONT)over generated talking heads. Specifically, the head pose prediction module isdesigned to generate head pose sequences from the source face and drivingaudio. We add the random sampling operation and the structural similarityconstraint to model the diversity in the one-to-many mapping betweenaudio-visual modality, thus predicting natural head poses. Then we develop akeypoint predictor that produces unsupervised keypoints from the source face,driving audio and pose sequences to describe the facial structure information.Finally, a flow-guided occlusion-aware generator is employed to producephoto-realistic talking head videos from the estimated keypoints and sourceface. Extensive experimental results prove that FONT generates talking headswith natural head poses and synchronized mouth shapes, outperforming othercompared methods.", "output": "FONT: Flow-guided One-shot Talking Head Generation with Natural Head Motions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a new method for anomaly detection in time-series data byincorporating the concept of difference subspace into the singular spectrumanalysis (SSA). The key idea is to monitor slight temporal variations of thedifference subspace between two signal subspaces corresponding to the past andpresent time-series data, as anomaly score. It is a natural generalization ofthe conventional SSA-based method which measures the minimum angle between thetwo signal subspaces as the degree of changes. By replacing the minimum anglewith the difference subspace, our method boosts the performance while using theSSA-based framework as it can capture the whole structural difference betweenthe two subspaces in its magnitude and direction. We demonstrate our method'seffectiveness through performance evaluations on public time-series datasets.", "output": "Time-series Anomaly Detection based on Difference Subspace between Signal Subspaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) have been shown to be effective in various visiontasks. However, resizing them to a mobile-friendly size leads to significantperformance degradation. Therefore, developing lightweight vision transformershas become a crucial area of research. This paper introduces CloFormer, alightweight vision transformer that leverages context-aware local enhancement.CloFormer explores the relationship between globally shared weights often usedin vanilla convolutional operators and token-specific context-aware weightsappearing in attention, then proposes an effective and straightforward moduleto capture high-frequency local information. In CloFormer, we introduceAttnConv, a convolution operator in attention's style. The proposed AttnConvuses shared weights to aggregate local information and deploys carefullydesigned context-aware weights to enhance local features. The combination ofthe AttnConv and vanilla attention which uses pooling to reduce FLOPs inCloFormer enables the model to perceive high-frequency and low-frequencyinformation. Extensive experiments were conducted in image classification,object detection, and semantic segmentation, demonstrating the superiority ofCloFormer.", "output": "Rethinking Local Perception in Lightweight Vision Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Neural Microfacet Fields, a method for recovering materials,geometry, and environment illumination from images of a scene. Our method usesa microfacet reflectance model within a volumetric setting by treating eachsample along the ray as a (potentially non-opaque) surface. Using surface-basedMonte Carlo rendering in a volumetric setting enables our method to performinverse rendering efficiently by combining decades of research in surface-basedlight transport with recent advances in volume rendering for view synthesis.Our approach outperforms prior work in inverse rendering, capturing highfidelity geometry and high frequency illumination details; its novel viewsynthesis results are on par with state-of-the-art methods that do not recoverillumination or materials.", "output": "Neural Microfacet Fields for Inverse Rendering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Referring image segmentation (RIS) aims to find a segmentation mask given areferring expression grounded to a region of the input image. Collectinglabelled datasets for this task, however, is notoriously costly andlabor-intensive. To overcome this issue, we propose a simple yet effectivezero-shot referring image segmentation method by leveraging the pre-trainedcross-modal knowledge from CLIP. In order to obtain segmentation masks groundedto the input text, we propose a mask-guided visual encoder that captures globaland local contextual information of an input image. By utilizing instance masksobtained from off-the-shelf mask proposal techniques, our method is able tosegment fine-detailed Istance-level groundings. We also introduce aglobal-local text encoder where the global feature captures complexsentence-level semantics of the entire input expression while the local featurefocuses on the target noun phrase extracted by a dependency parser. In ourexperiments, the proposed method outperforms several zero-shot baselines of thetask and even the weakly supervised referring expression segmentation methodwith substantial margins. Our code is available at", "output": "Zero-shot Referring Image Segmentation with Global-Local Context Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer-based networks have achieved impressive performance in 3D pointcloud understanding. However, most of them concentrate on aggregating localfeatures, but neglect to directly model global dependencies, which results in alimited effective receptive field. Besides, how to effectively incorporatelocal and global components also remains challenging. To tackle these problems,we propose Asymmetric Parallel Point Transformer (APPT). Specifically, weintroduce Global Pivot Attention to extract global features and enlarge theeffective receptive field. Moreover, we design the Asymmetric Parallelstructure to effectively integrate local and global information. Combined withthese designs, APPT is able to capture features globally throughout the entirenetwork while focusing on local-detailed features. Extensive experiments showthat our method outperforms the priors and achieves state-of-the-art on severalbenchmarks for 3D point cloud understanding, such as 3D semantic segmentationon S3DIS, 3D shape classification on ModelNet40, and 3D part segmentation onShapeNet.", "output": "APPT : Asymmetric Parallel Point Transformer for 3D Point Cloud Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Satellite-based Synthetic Aperture Radar (SAR) images can be used as a sourceof remote sensed imagery regardless of cloud cover and day-night cycle.However, the speckle noise and varying image acquisition conditions pose achallenge for change detection classifiers. This paper proposes a new method ofimproving SAR image processing to produce higher quality difference images forthe classification algorithms. The method is built on a neural network-basedmapping transformation function that produces artificial SAR images from alocation in the requested acquisition conditions. The inputs for the model are:previous SAR images from the location, imaging angle information from the SARimages, digital elevation model, and weather conditions. The method was testedwith data from a location in North-East Finland by using Sentinel-1 SAR imagesfrom European Space Agency, weather data from Finnish Meteorological Institute,and a digital elevation model from National Land Survey of Finland. In order toverify the method, changes to the SAR images were simulated, and theperformance of the proposed method was measured using experimentation where itgave substantial improvements to performance when compared to a moreconventional method of creating difference images.", "output": "Improved Difference Images for Change Detection Classifiers in SAR Imagery Using Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Saliency methods provide post-hoc model interpretation by attributing inputfeatures to the model outputs. Current methods mainly achieve this using asingle input sample, thereby failing to answer input-independent inquiriesabout the model. We also show that input-specific saliency mapping isintrinsically susceptible to misleading feature attribution. Current attemptsto use 'general' input features for model interpretation assume access to adataset containing those features, which biases the interpretation. Addressingthe gap, we introduce a new perspective of input-agnostic saliency mapping thatcomputationally estimates the high-level features attributed by the model toits outputs. These features are geometrically correlated, and are computed byaccumulating model's gradient information with respect to an unrestricted datadistribution. To compute these features, we nudge independent data points overthe model loss surface towards the local minima associated by ahuman-understandable concept, e.g., class label for classifiers. With asystematic projection, scaling and refinement process, this information istransformed into an interpretable visualization without compromising itsmodel-fidelity. The visualization serves as a stand-alone qualitativeinterpretation. With an extensive evaluation, we not only demonstratesuccessful visualizations for a variety of concepts for large-scale models, butalso showcase an interesting utility of this new form of saliency mapping byidentifying backdoor signatures in compromised classifiers.", "output": "Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The abundance of instructional videos and their narrations over the Internetoffers an exciting avenue for understanding procedural activities. In thiswork, we propose to learn video representation that encodes both action stepsand their temporal ordering, based on a large-scale dataset of webinstructional videos and their narrations, without using human annotations. Ourmethod jointly learns a video representation to encode individual stepconcepts, and a deep probabilistic model to capture both temporal dependenciesand immense individual variations in the step ordering. We empiricallydemonstrate that learning temporal ordering not only enables new capabilitiesfor procedure reasoning, but also reinforces the recognition of individualsteps. Our model significantly advances the state-of-the-art results on stepclassification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting(+7.4% on COIN). Moreover, our model attains promising results in zero-shotinference for step classification and forecasting, as well as in predictingdiverse and plausible steps for incomplete procedures. Our code is available at", "output": "Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object-centric learning (OCL) aspires general and compositional understandingof scenes by representing a scene as a collection of object-centricrepresentations. OCL has also been extended to multi-view image and videodatasets to apply various data-driven inductive biases by utilizing geometricor temporal information in the multi-image data. Single-view images carry lessinformation about how to disentangle a given scene than videos or multi-viewimages do. Hence, owing to the difficulty of applying inductive biases, OCL forsingle-view images remains challenging, resulting in inconsistent learning ofobject-centric representation. To this end, we introduce a novel OCL frameworkfor single-view images, SLot Attention via SHepherding (SLASH), which consistsof two simple-yet-effective modules on top of Slot Attention. The new modules,Attention Refining Kernel (ARK) and Intermediate Point Predictor and Encoder(IPPE), respectively, prevent slots from being distracted by the backgroundnoise and indicate locations for slots to focus on to facilitate learning ofobject-centric representation. We also propose a weak semi-supervision approachfor OCL, whilst our proposed framework can be used without any assistantannotation during the inference. Experiments show that our proposed methodenables consistent learning of object-centric representation and achievesstrong performance across four datasets. Code is available aturl{", "output": "Shepherding Slots to Objects: Towards Stable and Robust Object-Centric Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent times, various modules such as squeeze-and-excitation, and othershave been proposed to improve the quality of features learned from wearablesensor signals. However, these modules often cause the number of parameters tobe large, which is not suitable for building lightweight human activityrecognition models which can be easily deployed on end devices. In thisresearch, we propose a feature learning module, termed WSense, which uses two1D CNN and global max pooling layers to extract similar quality features fromwearable sensor data while ignoring the difference in activity recognitionmodels caused by the size of the sliding window. Experiments were carried outusing CNN and ConvLSTM feature learning pipelines on a dataset obtained with asingle accelerometer (WISDM) and another obtained using the fusion ofaccelerometers, gyroscopes, and magnetometers (PAMAP2) under various slidingwindow sizes. A total of nine hundred sixty (960) experiments were conducted tovalidate the WSense module against baselines and existing methods on the twodatasets. The results showed that the WSense module aided pipelines in learningsimilar quality features and outperformed the baselines and existing modelswith a minimal and uniform model size across all sliding window segmentations.The code is available at ", "output": "WSense: A Robust Feature Learning Module for Lightweight Human Activity Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point cloud data are widely used in manufacturing applications for processinspection, modeling, monitoring and optimization. The state-of-art tensorregression techniques have effectively been used for analysis of structuredpoint cloud data, where the measurements on a uniform grid can be formed into atensor. However, these techniques are not capable of handling unstructuredpoint cloud data that are often in the form of manifolds. In this paper, wepropose a nonlinear dimension reduction approach named Maximum CovarianceUnfolding Regression that is able to learn the low-dimensional (LD) manifold ofpoint clouds with the highest correlation with explanatory covariates. This LDmanifold is then used for regression modeling and process optimization based onprocess variables. The performance of the proposed method is subsequentlyevaluated and compared with benchmark methods through simulations and a casestudy of steel bracket manufacturing.", "output": "Maximum Covariance Unfolding Regression: A Novel Covariate-based Manifold Learning Approach for Point Cloud Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Change detection in remote sensing imagery is essential for a variety ofapplications such as urban planning, disaster management, and climate research.However, existing methods for identifying semantically changed areas overlookthe availability of semantic information in the form of existing mapsdescribing features of the earth's surface. In this paper, we leverage thisinformation for change detection in bi-temporal images. We show that the simpleintegration of the additional information via concatenation of latentrepresentations suffices to significantly outperform state-of-the-art changedetection methods. Motivated by this observation, we propose the new task ofConditional Change Detection, where pre-change semantic information is used asinput next to bi-temporal images. To fully exploit the extra information, wepropose MapFormer, a novel architecture based on a multi-modal feature fusionmodule that allows for feature processing conditioned on the available semanticinformation. We further employ a supervised, cross-modal contrastive loss toguide the learning of visual representations. Our approach outperforms existingchange detection methods by an absolute 11.7% and 18.4% in terms of binarychange IoU on DynamicEarthNet and HRSCD, respectively. Furthermore, wedemonstrate the robustness of our approach to the quality of the pre-changesemantic information and the absence pre-change imagery. The code will be madepublicly available.", "output": "MapFormer: Boosting Change Detection by Using Pre-change Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Content affinity loss including feature and pixel affinity is a main problemwhich leads to artifacts in photorealistic and video style transfer. This paperproposes a new framework named CAP-VSTNet, which consists of a new reversibleresidual network and an unbiased linear transform module, for versatile styletransfer. This reversible residual network can not only preserve contentaffinity but not introduce redundant information as traditional reversiblenetworks, and hence facilitate better stylization. Empowered by MattingLaplacian training loss which can address the pixel affinity loss problem ledby the linear transform, the proposed framework is applicable and effective onversatile style transfer. Extensive experiments show that CAP-VSTNet canproduce better qualitative and quantitative results in comparison with thestate-of-the-art methods.", "output": "CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent breakthroughs in the field of language-guided image generation haveyielded impressive achievements, enabling the creation of high-quality anddiverse images based on user instructions. Although the synthesis performanceis fascinating, one significant limitation of current image generation modelsis their insufficient ability to generate coherent text within images,particularly for complex glyph structures like Chinese characters. To addressthis problem, we introduce GlyphDraw, a general learning framework aiming atendowing image generation models with the capacity to generate images embeddedwith coherent text. To the best of our knowledge, this is the first work in thefield of image synthesis to address the generation of Chinese characters. % wefirst adopt the OCR technique to collect images with Chinese characters astraining samples, and extract the text and locations as auxiliary information.We first sophisticatedly design the image-text dataset's construction strategy,then build our model specifically on a diffusion-based image generator andcarefully modify the network structure to allow the model to learn drawingChinese characters with the help of glyph and position information.Furthermore, we maintain the model's open-domain image synthesis capability bypreventing catastrophic forgetting by using a variety of training techniques.Extensive qualitative and quantitative experiments demonstrate that our methodnot only produces accurate Chinese characters as in prompts, but also naturallyblends the generated text into the background. Please refer to", "output": "GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce the novel state-of-the-art Dual-attentionTransformer and Discriminative Flow (DADF) framework for visual anomalydetection. Based on only normal knowledge, visual anomaly detection has wideapplications in industrial scenarios and has attracted significant attention.However, most existing methods fail to meet the requirements. In contrast, theproposed DTDF presents a new paradigm: it firstly leverages a pre-trainednetwork to acquire multi-scale prior embeddings, followed by the development ofa vision Transformer with dual attention mechanisms, namely self-attention andmemorial-attention, to achieve two-level reconstruction for prior embeddingswith the sequential and normality association. Additionally, we propose usingnormalizing flow to establish discriminative likelihood for the jointdistribution of prior and reconstructions at each scale. The DADF achieves98.3/98.4 of image/pixel AUROC on Mvtec AD; 83.7 of image AUROC and 67.4 ofpixel sPRO on Mvtec LOCO AD benchmarks, demonstrating the effectiveness of ourproposed approach.", "output": "Visual Anomaly Detection via Dual-Attention Transformer and Discriminative Flow."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Polarization is a fundamental property of light that encodes abundantinformation regarding surface shape, material, illumination and viewinggeometry. The computer vision community has witnessed a blossom ofpolarization-based vision applications, such as reflection removal,shape-from-polarization, transparent object segmentation and color constancy,partially due to the emergence of single-chip mono/color polarization sensorsthat make polarization data acquisition easier than ever. However, ispolarization-based vision vulnerable to adversarial attacks? If so, is thatpossible to realize these adversarial attacks in the physical world, withoutbeing perceived by human eyes? In this paper, we warn the community of thevulnerability of polarization-based vision, which can be more serious thanRGB-based vision. By adapting a commercial LCD projector, we achieve locallycontrollable polarizing projection, which is successfully utilized to foolstate-of-the-art polarization-based vision algorithms for glass segmentationand color constancy. Compared with existing physical attacks on RGB-basedvision, which always suffer from the trade-off between attack efficacy and eyeconceivability, the adversarial attackers based on polarizing projection arecontact-free and visually imperceptible, since naked human eyes can rarelyperceive the difference of viciously manipulated polarizing light and ordinaryillumination. This poses unprecedented risks on polarization-based vision, bothin the monochromatic and trichromatic domain, for which due attentions shouldbe paid and counter measures be considered.", "output": "Fooling Polarization-based Vision using Locally Controllable Polarizing Projection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, great progress has been made in the Lift-Splat-Shot-based(LSS-based) 3D object detection method, which converts features of 2D cameraview and 3D lidar view to Bird's-Eye-View (BEV) for feature fusion. However,inaccurate depth estimation (e.g. the 'depth jump' problem) is an obstacle todevelop LSS-based methods. To alleviate the 'depth jump' problem, we proposedEdge-Aware Bird's-Eye-View (EA-BEV) projector. By coupling proposed edge-awaredepth fusion module and depth estimate module, the proposed EA-BEV projectorsolves the problem and enforces refined supervision on depth. Besides, wepropose sparse depth supervision and gradient edge depth supervision, forconstraining learning on global depth and local marginal depth information. OurEA-BEV projector is a plug-and-play module for any LSS-based 3D objectdetection models, and effectively improves the baseline performance. Wedemonstrate the effectiveness on the nuScenes benchmark. On the nuScenes 3Dobject detection validation dataset, our proposed EA-BEV projector can boostseveral state-of-the-art LLS-based baselines on nuScenes 3D object detectionbenchmark and nuScenes BEV map segmentation benchmark with negligible incrementof inference time.", "output": "EA-BEV: Edge-aware Bird' s-Eye-View Projector for 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a general methodology that learns to classify images withoutlabels by leveraging pretrained feature extractors. Our approach involvesself-distillation training of clustering heads, based on the fact that nearestneighbors in the pretrained feature space are likely to share the same label.We propose a novel objective to learn associations between images byintroducing a variant of pointwise mutual information together with instanceweighting. We demonstrate that the proposed objective is able to attenuate theeffect of false positive pairs while efficiently exploiting the structure inthe pretrained feature space. As a result, we improve the clustering accuracyover $k$-means on $17$ different pretrained models by $6.1$% and $12.2$% onImageNet and CIFAR100, respectively. Finally, using self-supervised pretrainedvision transformers we push the clustering accuracy on ImageNet to $61.6$%.The code will be open-sourced.", "output": "Exploring the Limits of Deep Image Clustering using Pretrained Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce a novel 3D-aware image generation method thatleverages 2D diffusion models. We formulate the 3D-aware image generation taskas multiview 2D image set generation, and further to a sequentialunconditional-conditional multiview image generation process. This allows us toutilize 2D diffusion models to boost the generative modeling power of themethod. Additionally, we incorporate depth information from monocular depthestimators to construct the training data for the conditional diffusion modelusing only still images. We train our method on a large-scale dataset, i.e.,ImageNet, which is not addressed by previous methods. It produces high-qualityimages that significantly outperform prior methods. Furthermore, our approachshowcases its capability to generate instances with large view angles, eventhough the training images are diverse and unaligned, gathered from\"in-the-wild\" real-world environments.", "output": "3D-aware Image Generation using 2D Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent progress of diffusion models in terms of image quality has led toa major shift in research related to generative models. Current approachesoften fine-tune pre-trained foundation models using domain-specifictext-to-image pairs. This approach is straightforward for X-ray imagegeneration due to the high availability of radiology reports linked to specificimages. However, current approaches hardly ever look at attention layers toverify whether the models understand what they are generating. In this paper,we discover an important trade-off between image fidelity and interpretabilityin generative diffusion models. In particular, we show that fine-tuningtext-to-image models with learnable text encoder leads to a lack ofinterpretability of diffusion models. Finally, we demonstrate theinterpretability of diffusion models by showing that keeping the languageencoder frozen, enables diffusion models to achieve state-of-the-art phrasegrounding performance on certain diseases for a challenging multi-labelsegmentation task, without any additional training. Code and models will beavailable at ", "output": "Pay Attention: Accuracy Versus Interpretability Trade-off in Fine-tuned Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Synthesizing 3D human motion in a contextual, ecological environment isimportant for simulating realistic activities people perform in the real world.However, conventional optics-based motion capture systems are not suited forsimultaneously capturing human movements and complex scenes. The lack of richcontextual 3D human motion datasets presents a roadblock to creatinghigh-quality generative human motion models. We propose a novel motionacquisition system in which the actor perceives and operates in a highlycontextual virtual world while being motion captured in the real world. Oursystem enables rapid collection of high-quality human motion in highly diversescenes, without the concern of occlusion or the need for physical sceneconstruction in the real world. We present CIRCLE, a dataset containing 10hours of full-body reaching motion from 5 subjects across nine scenes, pairedwith ego-centric information of the environment represented in various forms,such as RGBD videos. We use this dataset to train a model that generates humanmotion conditioned on scene information. Leveraging our dataset, the modellearns to use ego-centric scene information to achieve nontrivial reachingtasks in the context of complex 3D scenes. To download the data please visit", "output": "CIRCLE: Capture In Rich Contextual Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Paranasal anomalies are commonly discovered during routine radiologicalscreenings and can present with a wide range of morphological features. Thisdiversity can make it difficult for convolutional neural networks (CNNs) toaccurately classify these anomalies, especially when working with limiteddatasets. Additionally, current approaches to paranasal anomaly classificationare constrained to identifying a single anomaly at a time. These challengesnecessitate the need for further research and development in this area.In this study, we investigate the feasibility of using a 3D convolutionalneural network (CNN) to classify healthy maxillary sinuses (MS) and MS withpolyps or cysts. The task of accurately identifying the relevant MS volumewithin larger head and neck Magnetic Resonance Imaging (MRI) scans can bedifficult, but we develop a straightforward strategy to tackle this challenge.Our end-to-end solution includes the use of a novel sampling technique that notonly effectively localizes the relevant MS volume, but also increases the sizeof the training dataset and improves classification results. Additionally, weemploy a multiple instance ensemble prediction method to further boostclassification performance. Finally, we identify the optimal size of MS volumesto achieve the highest possible classification performance on our dataset.With our multiple instance ensemble prediction strategy and samplingstrategy, our 3D CNNs achieve an F1 of 0.85 whereas without it, they achieve anF1 of 0.70.We demonstrate the feasibility of classifying anomalies in the MS. We proposea data enlarging strategy alongside a novel ensembling strategy that proves tobe beneficial for paranasal anomaly classification in the MS.", "output": "Multiple Instance Ensembling For Paranasal Anomaly Classification In The Maxillary Sinus."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D object detection is one of the most important tasks in autonomous drivingand robotics. Our research focuses on tackling low efficiency issue ofpoint-based methods on large-scale point clouds. Existing point-based methodsadopt farthest point sampling (FPS) strategy for downsampling, which iscomputationally expensive in terms of inference time and memory consumptionwhen the number of point cloud increases. In order to improve efficiency, wepropose a novel Instance-Centroid Faster Point Sampling Module (IC-FPS) , whicheffectively replaces the first Set Abstraction (SA) layer that is extremelytedious. IC-FPS module is comprised of two methods, local feature diffusionbased background point filter (LFDBF) and Centroid-Instance Sampling Strategy(CISS). LFDBF is constructed to exclude most invalid background points, whileCISS substitutes FPS strategy by fast sampling centroids and instance points.IC-FPS module can be inserted to almost every point-based models. Extensiveexperiments on multiple public benchmarks have demonstrated the superiority ofIC-FPS. On Waymo dataset, the proposed module significantly improvesperformance of baseline model and accelerates inference speed by 3.8 times. Forthe first time, real-time detection of point-based models in large-scale pointcloud scenario is realized.", "output": "IC-FPS: Instance-Centroid Faster Point Sampling Module for 3D Point-base Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain adaptation helps generalizing object detection models to target domaindata with distribution shift. It is often achieved by adapting with access tothe whole target domain data. In a more realistic scenario, target distributionis often unpredictable until inference stage. This motivates us to exploreadapting an object detection model at test-time, a.k.a. test-time adaptation(TTA). In this work, we approach test-time adaptive object detection (TTAOD)from two perspective. First, we adopt a self-training paradigm to generatepseudo labeled objects with an exponential moving average model. The pseudolabels are further used to supervise adapting source domain model. Asself-training is prone to incorrect pseudo labels, we further incorporatealigning feature distributions at two output levels as regularizations toself-training. To validate the performance on TTAOD, we create benchmarks basedon three standard object detection datasets and adapt generic TTA methods toobject detection task. Extensive evaluations suggest our proposed method setsthe state-of-the-art on test-time adaptive object detection task.", "output": "STFAR: Improving Object Detection Robustness at Test-Time by Self-Training with Feature Alignment Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Organ at Risk (OAR) segmentation from CT scans is a key component of theradiotherapy treatment workflow. In recent years, deep learning techniques haveshown remarkable potential in automating this process. In this paper, weinvestigate the performance of Generative Adversarial Networks (GANs) comparedto supervised learning approaches for segmenting OARs from CT images. Wepropose three GAN-based models with identical generator architectures butdifferent discriminator networks. These models are compared withwell-established CNN models, such as SE-ResUnet and DeepLabV3, using theStructSeg dataset, which consists of 50 annotated CT scans containing contoursof six OARs. Our work aims to provide insight into the advantages anddisadvantages of adversarial training in the context of OAR segmentation. Theresults are very promising and show that the proposed GAN-based approaches aresimilar or superior to their CNN-based counterparts, particularly whensegmenting more challenging target organs.", "output": "Comparing Adversarial and Supervised Learning for Organs at Risk Segmentation in CT images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classic Machine Learning techniques require training on data available in asingle data lake. However, aggregating data from different owners is not alwaysconvenient for different reasons, including security, privacy and secrecy. Datacarry a value that might vanish when shared with others; the ability to avoidsharing the data enables industrial applications where security and privacy areof paramount importance, making it possible to train global models byimplementing only local policies which can be run independently and even onair-gapped data centres. Federated Learning (FL) is a distributed machinelearning approach which has emerged as an effective way to address privacyconcerns by only sharing local AI models while keeping the data decentralized.Two critical challenges of Federated Learning are managing the heterogeneoussystems in the same federated network and dealing with real data, which areoften not independently and identically distributed (non-IID) among theclients. In this paper, we focus on the second problem, i.e., the problem ofstatistical heterogeneity of the data in the same federated network. In thissetting, local models might be strayed far from the local optimum of thecomplete dataset, thus possibly hindering the convergence of the federatedmodel. Several Federated Learning algorithms, such as FedAvg, FedProx andFederated Curvature (FedCurv), aiming at tackling the non-IID setting, havealready been proposed. This work provides an empirical assessment of thebehaviour of FedAvg and FedCurv in common non-IID scenarios. Results show thatthe number of epochs per round is an important hyper-parameter that, when tunedappropriately, can lead to significant performance gains while reducing thecommunication cost. As a side product of this work, we release the non-IIDversion of the datasets we used so to facilitate further comparisons from theFL community.", "output": "Benchmarking FedAvg and FedCurv for Image Classification Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Motion capture is a long-standing research problem. Although it has beenstudied for decades, the majority of research focus on ground-based movementssuch as walking, sitting, dancing, etc. Off-grounded actions such as climbingare largely overlooked. As an important type of action in sports andfirefighting field, the climbing movements is challenging to capture because ofits complex back poses, intricate human-scene interactions, and difficultglobal localization. The research community does not have an in-depthunderstanding of the climbing action due to the lack of specific datasets. Toaddress this limitation, we collect CIMI4D, a large rocktextbf{C}ltextbf{I}mbing textbf{M}ottextbf{I}on dataset from 12 personsclimbing 13 different climbing walls. The dataset consists of around 180,000frames of pose inertial measurements, LiDAR point clouds, RGB videos,high-precision static point cloud scenes, and reconstructed scene meshes.Moreover, we frame-wise annotate touch rock holds to facilitate a detailedexploration of human-scene interaction. The core of this dataset is a blendingoptimization process, which corrects for the pose as it drifts and is affectedby the magnetic conditions. To evaluate the merit of CIMI4D, we perform fourtasks which include human pose estimations (with/without scene constraints),pose prediction, and pose generation. The experimental results demonstrate thatCIMI4D presents great challenges to existing methods and enables extensiveresearch opportunities. We share the dataset with the research community in<a href=\" http URL</a>", "output": "CIMI4D: A Large Multimodal Climbing Motion Dataset under Human-scene Interactions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the medical images field, semantic segmentation is one of the mostimportant, yet difficult and time-consuming tasks to be performed byphysicians. Thanks to the recent advancement in the Deep Learning modelsregarding Computer Vision, the promise to automate this kind of task is gettingmore and more realistic. However, many problems are still to be solved, likethe scarce availability of data and the difficulty to extend the efficiency ofhighly specialised models to general scenarios. Organs at risk segmentation forradiotherapy treatment planning falls in this category, as the limited dataavailable negatively affects the possibility to develop general-purpose models;in this work, we focus on the possibility to solve this problem by presentingthree types of ensembles of single-organ models able to produce multi-organmasks exploiting the different specialisations of their components. The resultsobtained are promising and prove that this is a possible solution to findingefficient multi-organ segmentation methods.", "output": "Ensemble Methods for Multi-Organ Segmentation in CT Series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal action segmentation is crucial for understanding long-form videos.Previous works on this task commonly adopt an iterative refinement paradigm byusing multi-stage models. Our paper proposes an essentially different frameworkvia denoising diffusion models, which nonetheless shares the same inherentspirit of such iterative refinement. In this framework, action predictions areprogressively generated from random noise with input video features asconditions. To enhance the modeling of three striking characteristics of humanactions, including the position prior, the boundary ambiguity, and therelational dependency, we devise a unified masking strategy for theconditioning inputs in our framework. Extensive experiments on three benchmarkdatasets, i.e., GTEA, 50Salads, and Breakfast, are performed and the proposedmethod achieves superior or comparable results to state-of-the-art methods,showing the effectiveness of a generative approach for action segmentation. Ourcodes will be made available.", "output": "Diffusion Action Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image segmentation is considered as the basic step for medical imageanalysis and surgical intervention. And many previous works attempted toincorporate shape priors for designing segmentation models, which is beneficialto attain finer masks with anatomical shape information. Here in our work, wedetailedly discuss three types of segmentation models with shape priors, whichconsist of atlas-based models, statistical-based models and UNet-based models.On the ground that the former two kinds of methods show a poor generalizationability, UNet-based models have dominated the field of medical imagesegmentation in recent years. However, existing UNet-based models tend toemploy implicit shape priors, which do not have a good interpretability andgeneralization ability on different organs with distinctive shapes. Thus, weproposed a novel shape prior module (SPM), which could explicitly introduceshape priors to promote the segmentation performance of UNet-based models. Toevaluate the effectiveness of SPM, we conduct experiments on three challengingpublic datasets. And our proposed model achieves state-of-the-art performance.Furthermore, SPM shows an outstanding generalization ability on differentclassic convolution-neural-networks (CNNs) and recent Transformer-basedbackbones, which can serve as a plug-and-play structure for the segmentationtask of different datasets.", "output": "Learning with Explicit Shape Priors for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) forbetter geometry under non-Lambertian surface and dynamic lighting conditionsthat cause significant variation in the radiance of a point when viewed fromdifferent angles. Instead of explicitly modeling the underlying factors thatresult in the view-dependent phenomenon, which could be complex yet notinclusive, we develop a simple and effective technique that normalizes theview-dependence by distilling invariant information already encoded in thelearned NeRFs. We then jointly train NeRFs for view synthesis withview-dependence normalization to attain quality geometry. Our experiments showthat even though shape-radiance ambiguity is inevitable, the proposednormalization can minimize its effect on geometry, which essentially aligns theoptimal capacity needed for explaining view-dependent variations. Our methodapplies to various baselines and significantly improves geometry withoutchanging the volume rendering pipeline, even if the data is captured under amoving light source. Code is available at: ", "output": "VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence Normalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, learning-based feature detection and matching haveoutperformed manually-designed methods in in-air cases. However, it ischallenging to learn the features in the underwater scenario due to the absenceof annotated underwater datasets. This paper proposes a cross-modal knowledgedistillation framework for training an underwater feature detection andmatching network (UFEN). In particular, we use in-air RGBD data to generatesynthetic underwater images based on a physical underwater imaging formationmodel and employ these as the medium to distil knowledge from a teacher modelSuperPoint pretrained on in-air images. We embed UFEN into the ORB-SLAM3framework to replace the ORB feature by introducing an additional binarizationlayer. To test the effectiveness of our method, we built a new underwaterdataset with groundtruth measurements named EASI( recorded in an indoor water tank fordifferent turbidity levels. The experimental results on the existing datasetand our new dataset demonstrate the effectiveness of our method.", "output": "Knowledge Distillation for Feature Extraction in Underwater VSLAM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computer vision for detecting building pathologies has interested researchersfor quite some time. Vision-based crack detection is a non-destructiveassessment technique, which can be useful especially for Cultural Heritage (CH)where strict regulations apply and, even simple, interventions are notpermitted. Recently, shallow and deep machine learning architectures applied onvarious types of imagery are gaining ground. In this article a crack detectionmethodology for stone masonry walls is presented. In the proposed approach,crack detection is approached as an unsupervised anomaly detection problem onRGB (Red Green Blue) image patches. Towards this direction, some of the mostpopular state of the art CNN (Convolutional Neural Network) architectures aredeployed and modified to binary classify the images or image patches bypredicting a specific class for the tested imagery; 'Crack' or 'No crack', anddetect and localize those cracks on the RGB imagery with high accuracy. Testingof the model was performed on various test sites and random images retrievedfrom the internet and collected by the authors and results suggested the highperformance of specific networks compared to the rest, considering also thesmall numbers of epochs required for training. Those results met the accuracydelivered by more complex and computationally heavy approaches, requiring alarge amount of data for training. Source code is available on GitHub while datasets are available onZenodo  .", "output": "Unsupervised crack detection on complex stone masonry surfaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce a new approach to address the challenge ofgeneralization in hyperspectral anomaly detection (AD). Our method eliminatesthe need for adjusting parameters or retraining on new test scenes as requiredby most existing methods. Employing an image-level training paradigm, weachieve a general anomaly enhancement network for hyperspectral AD that onlyneeds to be trained once. Trained on a set of anomaly-free hyperspectral imageswith random masks, our network can learn the spatial context characteristicsbetween anomalies and background in an unsupervised way. Additionally, aplug-and-play model selection module is proposed to search for aspatial-spectral transform domain that is more suitable for AD task than theoriginal data. To establish a unified benchmark to comprehensively evaluate ourmethod and existing methods, we develop a large-scale hyperspectral AD dataset(HAD100) that includes 100 real test scenes with diverse anomaly targets. Incomparison experiments, we combine our network with a parameter-free detectorand achieve the optimal balance between detection accuracy and inference speedamong state-of-the-art AD methods. Experimental results also show that ourmethod still achieves competitive performance when the training and test setare captured by different sensor devices. Our code is available at", "output": "You Only Train Once: Learning a General Anomaly Enhancement Network with Random Masks for Hyperspectral Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers have been incredibly effective when tackling computervision tasks due to their ability to model long feature dependencies. By usinglarge-scale training data and various self-supervised signals (e.g., maskedrandom patches), vision transformers provide state-of-the-art performance onseveral benchmarking datasets, such as ImageNet-1k and CIFAR-10. However, thesevision transformers pretrained over general large-scale image corpora couldonly produce an anisotropic representation space, limiting theirgeneralizability and transferability to the target downstream tasks. In thispaper, we propose a simple and effective Label-aware Contrastive Trainingframework LaCViT, which improves the isotropy of the pretrained representationspace for vision transformers, thereby enabling more effective transferlearning amongst a wide range of image classification tasks. Throughexperimentation over five standard image classification datasets, wedemonstrate that LaCViT-trained models outperform the original pretrainedbaselines by around 9% absolute Accuracy@1, and consistent improvements can beobserved when applying LaCViT to our three evaluated vision transformers.", "output": "LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advanced minimally invasive neurosurgery navigation relies mainly on MagneticResonance Imaging (MRI) guidance. MRI guidance, however, only providespre-operative information in the majority of the cases. Once the surgerybegins, the value of this guidance diminishes to some extent because of theanatomical changes due to surgery. Guidance with live image feedback comingdirectly from the surgical device, e.g., endoscope, can complement MRI-basednavigation or be an alternative if MRI guidance is not feasible. With thismotivation, we present a method for live image-only guidance leveraging a largedata set of annotated neurosurgical videos.First, we report the performance ofa deep learning-based object detection method, YOLO, on detecting anatomicalstructures in neurosurgical images. Second, we present a method for generatingneurosurgical roadmaps using unsupervised embedding without assuming exactanatomical matches between patients, presence of an extensive anatomical atlas,or the need for simultaneous localization and mapping. A generated roadmapencodes the common anatomical paths taken in surgeries in the training set. Atinference, the roadmap can be used to map a surgeon's current location usinglive image feedback on the path to provide guidance by being able to predictwhich structures should appear going forward or backward, much like a mappingapplication. Even though the embedding is not supervised by positioninformation, we show that it is correlated to the location inside the brain andon the surgical path. We trained and evaluated the proposed method with a dataset of 166 transsphenoidal adenomectomy procedures.", "output": "Live image-based neurosurgical guidance and roadmap generation using unsupervised embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Retinal fundus images can be an invaluable diagnosis tool for screeningepidemic diseases like hypertension or diabetes. And they become especiallyuseful when the arterioles and venules they depict are clearly identified andannotated. However, manual annotation of these vessels is extremely timedemanding and taxing, which calls for automatic segmentation. Althoughconvolutional neural networks can achieve high overlap between predictions andexpert annotations, they often fail to produce topologically correctpredictions of tubular structures. This situation is exacerbated by thebifurcation versus crossing ambiguity which causes classification mistakes.This paper shows that including a topology preserving term in the loss functionimproves the continuity of the segmented vessels, although at the expense ofartery-vein misclassification and overall lower overlap metrics. However, weshow that by including an orientation score guided convolutional module, basedon the anisotropic single sided cake wavelet, we reduce such misclassificationand further increase the topology correctness of the results. We evaluate ourmodel on public datasets with conveniently chosen metrics to assess bothoverlap and topology correctness, showing that our model is able to produceresults on par with state-of-the-art from the point of view of overlap, whileincreasing topological accuracy.", "output": "The Topology-Overlap Trade-Off in Retinal Arteriole-Venule Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In real-world applications, a machine learning model is required to handle anopen-set recognition (OSR), where unknown classes appear during the inference,in addition to a domain shift, where the distribution of data differs betweenthe training and inference phases. Domain generalization (DG) aims to handlethe domain shift situation where the target domain of the inference phase isinaccessible during model training. Open domain generalization (ODG) takes intoaccount both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a methodtargeting ODG but has a complicated learning process. On the other hand,although various DG methods have been proposed, they have not been evaluated inODG situations. This work comprehensively evaluates existing DG methods in ODGand shows that two simple DG methods, CORrelation ALignment (CORAL) and MaximumMean Discrepancy (MMD), are competitive with DAML in several cases. Inaddition, we propose simple extensions of CORAL and MMD by introducing thetechniques used in DAML, such as ensemble learning and Dirichlet mixup dataaugmentation. The experimental evaluation demonstrates that the extended CORALand MMD can perform comparably to DAML with lower computational costs. Thissuggests that the simple DG methods and their simple extensions are strongbaselines for ODG. The code used in the experiments is available at", "output": "Simple Domain Generalization Methods are Strong Baselines for Open Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although there are many datasets for traffic sign classification, there arefew datasets collected for traffic sign recognition and few of them obtainenough instances especially for training a model with the deep learning method.The deep learning method is almost the only way to train a model for real-worldusage that covers various highly similar classes compared with the traditionalway such as through color, shape, etc. Also, for some certain sign classes,their sign meanings were destined to can't get enough instances in the dataset.To solve this problem, we purpose a unique data augmentation method for thetraffic sign recognition dataset that takes advantage of the standard of thetraffic sign. We called it TSR dataset augmentation. We based on the benchmarkTsinghua-Tencent 100K (TT100K) dataset to verify the unique data augmentationmethod. we performed the method on four main iteration version datasets basedon the TT100K dataset and the experimental results showed our method isefficacious. The iteration version datasets based on TT100K, data augmentationmethod source code and the training results introduced in this paper arepublicly available.", "output": "Traffic Sign Recognition Dataset and Data Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weakly supervised video anomaly detection (WS-VAD) is a challenging problemthat aims to learn VAD models only with video-level annotations. In this work,we propose a Long-Short Temporal Co-teaching (LSTC) method to address theWS-VAD problem. It constructs two tubelet-based spatio-temporal transformernetworks to learn from short- and long-term video clips respectively. Eachnetwork is trained with respect to a multiple instance learning (MIL)-basedranking loss, together with a cross-entropy loss when clip-level pseudo labelsare available. A co-teaching strategy is adopted to train the two networks.That is, clip-level pseudo labels generated from each network are used tosupervise the other one at the next training round, and the two networks arelearned alternatively and iteratively. Our proposed method is able to betterdeal with the anomalies with varying durations as well as subtle anomalies.Extensive experiments on three public datasets demonstrate that our methodoutperforms state-of-the-art WS-VAD methods.", "output": "Long-Short Temporal Co-Teaching for Weakly Supervised Video Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adapting a segmentation model from a labeled source domain to a targetdomain, where a single unlabeled datum is available, is one the mostchallenging problems in domain adaptation and is otherwise known as one-shotunsupervised domain adaptation (OSUDA). Most of the prior works have addressedthe problem by relying on style transfer techniques, where the source imagesare stylized to have the appearance of the target domain. Departing from thecommon notion of transferring only the target ``texture'' information, weleverage text-to-image diffusion models (e.g., Stable Diffusion) to generate asynthetic target dataset with photo-realistic images that not only faithfullydepict the style of the target domain, but are also characterized by novelscenes in diverse contexts. The text interface in our method Data AugmenTationwith diffUsion Models (DATUM) endows us with the possibility of guiding thegeneration of images towards desired semantic concepts while respecting theoriginal spatial context of a single training image, which is not possible inexisting OSUDA methods. Extensive experiments on standard benchmarks show thatour DATUM surpasses the state-of-the-art OSUDA methods by up to +7.1%. Theimplementation is available at ", "output": "One-shot Unsupervised Domain Adaptation with Personalized Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point cloud registration is an important task in robotics and autonomousdriving to estimate the ego-motion of the vehicle. Recent advances followingthe coarse-to-fine manner show promising potential in point cloud registration.However, existing methods rely on good superpoint correspondences, which arehard to be obtained reliably and efficiently, thus resulting in less robust andaccurate point cloud registration. In this paper, we propose a novel network,named RDMNet, to find dense point correspondences coarse-to-fine and improvefinal pose estimation based on such reliable correspondences. Our RDMNet uses adevised 3D-RoFormer mechanism to first extract distinctive superpoints andgenerates reliable superpoints matches between two point clouds. The proposed3D-RoFormer fuses 3D position information into the transformer network,efficiently exploiting point clouds' contextual and geometric information togenerate robust superpoint correspondences. RDMNet then propagates the sparsesuperpoints matches to dense point matches using the neighborhood informationfor accurate point cloud registration. We extensively evaluate our method onmultiple datasets from different environments. The experimental resultsdemonstrate that our method outperforms existing state-of-the-art approaches inall tested datasets with a strong generalization ability.", "output": "RDMNet: Reliable Dense Matching Based Point Cloud Registration for Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Perception datasets for agriculture are limited both in quantity anddiversity which hinders effective training of supervised learning approaches.Self-supervised learning techniques alleviate this problem, however, existingmethods are not optimized for dense prediction tasks in agriculture domainswhich results in degraded performance. In this work, we address this limitationwith our proposed Injected Noise Discriminator (INoD) which exploits principlesof feature replacement and dataset discrimination for self-supervisedrepresentation learning. INoD interleaves feature maps from two disjointdatasets during their convolutional encoding and predicts the datasetaffiliation of the resultant feature map as a pretext task. Our approachenables the network to learn unequivocal representations of objects seen in onedataset while observing them in conjunction with similar features from thedisjoint dataset. This allows the network to reason about higher-levelsemantics of the entailed objects, thus improving its performance on variousdownstream tasks. Additionally, we introduce the novel Fraunhofer Potato 2022dataset consisting of over 16,800 images for object detection in potato fields.Extensive evaluations of our proposed INoD pretraining strategy for the tasksof object detection, semantic segmentation, and instance segmentation on theSugar Beets 2016 and our potato dataset demonstrate that it achievesstate-of-the-art performance.", "output": "INoD: Injected Noise Discriminator for Self-Supervised Representation Learning in Agricultural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Endoscopic video recordings are widely used in minimally invasiverobot-assisted surgery, but when the endoscope is outside the patient's body,it can capture irrelevant segments that may contain sensitive information. Toaddress this, we propose a framework that accurately detects out-of-body framesin surgical videos by leveraging self-supervision with minimal data labels. Weuse a massive amount of unlabeled endoscopic images to learn meaningfulrepresentations in a self-supervised manner. Our approach, which involvespre-training on an auxiliary task and fine-tuning with limited supervision,outperforms previous methods for detecting out-of-body frames in surgicalvideos captured from da Vinci X and Xi surgical systems. The average F1 scoresrange from 96.00 to 98.02. Remarkably, using only 5% of the training labels,our approach still maintains an average F1 score performance above 97,outperforming fully-supervised methods with 95% fewer labels. These resultsdemonstrate the potential of our framework to facilitate the safe handling ofsurgical video recordings and enhance data privacy protection in minimallyinvasive surgery.", "output": "Automatic Detection of Out-of-body Frames in Surgical Videos for Privacy Protection Using Self-supervised Learning and Minimal Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Average-K classification is an alternative to top-K classification in whichthe number of labels returned varies with the ambiguity of the input image butmust average to K over all the samples. A simple method to solve this task isto threshold the softmax output of a model trained with the cross-entropy loss.This approach is theoretically proven to be asymptotically consistent, but itis not guaranteed to be optimal for a finite set of samples. In this paper, wepropose a new loss function based on a multi-label classification head inaddition to the classical softmax. This second head is trained usingpseudo-labels generated by thresholding the softmax head while guaranteeingthat K classes are returned on average. We show that this approach allows themodel to better capture ambiguities between classes and, as a result, to returnmore consistent sets of possible classes. Experiments on two datasets from theliterature demonstrate that our approach outperforms the softmax baseline, aswell as several other loss functions more generally designed for weaklysupervised multi-label classification. The gains are larger the higher theuncertainty, especially for classes with few samples.", "output": "A two-head loss function for deep Average-K classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tracking 3D human motion in real-time is crucial for numerous applicationsacross many fields. Traditional approaches involve attaching artificialfiducial objects or sensors to the body, limiting their usability andcomfort-of-use and consequently narrowing their application fields. Recentadvances in Artificial Intelligence (AI) have allowed for markerless solutions.However, most of these methods operate in 2D, while those providing 3Dsolutions compromise accuracy and real-time performance. To address thischallenge and unlock the potential of visual pose estimation methods inreal-world scenarios, we propose a markerless framework that combinesmulti-camera views and 2D AI-based pose estimation methods to track 3D humanmotion. Our approach integrates a Weighted Least Square (WLS) algorithm thatcomputes 3D human motion from multiple 2D pose estimations provided by anAI-driven method. The method is integrated within the Open-VICO frameworkallowing simulation and real-world execution. Several experiments have beenconducted, which have shown high accuracy and real-time performance,demonstrating the high level of readiness for real-world applications and thepotential to revolutionize human motion capture.", "output": "Markerless 3D human pose tracking through multiple cameras and AI: Enabling high accuracy, robustness, and real-time performance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper addresses the problem of rolling shutter correction in complexnonlinear and dynamic scenes with extreme occlusion. Existing methods sufferfrom two main drawbacks. Firstly, they face challenges in estimating theaccurate correction field due to the uniform velocity assumption, leading tosignificant image correction errors under complex motion. Secondly, the drasticocclusion in dynamic scenes prevents current solutions from achieving betterimage quality because of the inherent difficulties in aligning and aggregatingmultiple frames. To tackle these challenges, we model the curvilineartrajectory of pixels analytically and propose a geometry-based QuadraticRolling Shutter (QRS) motion solver, which precisely estimates the high-ordercorrection field of individual pixel. Besides, to reconstruct high-qualityocclusion frames in dynamic scenes, we present a 3D video architecture thateffectively Aligns and Aggregates multi-frame context, namely, RSA^2-Net. Weevaluate our method across a broad range of cameras and video sequences,demonstrating its significant superiority. Specifically, our method surpassesthe state-of-the-arts by +4.98, +0.77, and +4.33 of PSNR on Carla-RS,Fastec-RS, and BS-RSC datasets, respectively.", "output": "Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) are vulnerable to adversarial examples, which maylead to catastrophe in security-critical domains. Numerous detection methodsare proposed to characterize the feature uniqueness of adversarial examples, orto distinguish DNN's behavior activated by the adversarial examples. Detectionsbased on features cannot handle adversarial examples with large perturbations.Besides, they require a large amount of specific adversarial examples. Anothermainstream, model-based detections, which characterize input properties bymodel behaviors, suffer from heavy computation cost. To address the issues, weintroduce the concept of local gradient, and reveal that adversarial exampleshave a quite larger bound of local gradient than the benign ones. Inspired bythe observation, we leverage local gradient for detecting adversarial examples,and propose a general framework AdvCheck. Specifically, by calculating thelocal gradient from a few benign examples and noise-added misclassifiedexamples to train a detector, adversarial examples and even misclassifiednatural inputs can be precisely distinguished from benign ones. Throughextensive experiments, we have validated the AdvCheck's superior performance tothe state-of-the-art (SOTA) baselines, with detection rate ($sim times 1.2$)on general adversarial attacks and ($sim times 1.4$) on misclassified naturalinputs on average, with average 1/500 time cost. We also provide interpretableresults for successful detection.", "output": "AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While current multi-frame restoration methods combine information frommultiple input images using 2D alignment techniques, recent advances in novelview synthesis are paving the way for a new paradigm relying on volumetricscene representations. In this work, we introduce the first 3D-basedmulti-frame denoising method that significantly outperforms its 2D-basedcounterparts with lower computational requirements. Our method extends themultiplane image (MPI) framework for novel view synthesis by introducing alearnable encoder-renderer pair manipulating multiplane representations infeature space. The encoder fuses information across views and operates in adepth-wise manner while the renderer fuses information across depths andoperates in a view-wise manner. The two modules are trained end-to-end andlearn to separate depths in an unsupervised way, giving rise to MultiplaneFeature (MPF) representations. Experiments on the Spaces and RealForward-Facing datasets as well as on raw burst data validate our approach forview synthesis, multi-frame denoising, and view synthesis under noisyconditions.", "output": "Efficient View Synthesis and 3D-based Multi-Frame Denoising with Multiplane Feature Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent self-supervised methods are mainly designed for representationlearning with the base model, e.g., ResNets or ViTs. They cannot be easilytransferred to DETR, with task-specific Transformer modules. In this work, wepresent Siamese DETR, a Siamese self-supervised pretraining approach for theTransformer architecture in DETR. We consider learning view-invariant anddetection-oriented representations simultaneously through two complementarytasks, i.e., localization and discrimination, in a novel multi-view learningframework. Two self-supervised pretext tasks are designed: (i) Multi-ViewRegion Detection aims at learning to localize regions-of-interest betweenaugmented views of the input, and (ii) Multi-View Semantic Discriminationattempts to improve object-level discrimination for each region. The proposedSiamese DETR achieves state-of-the-art transfer performance on COCO and PASCALVOC detection using different DETR variants in all setups. Code is available at", "output": "Siamese DETR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural-network-based single image depth prediction (SIDP) is a challengingtask where the goal is to predict the scene's per-pixel depth at test time.Since the problem, by definition, is ill-posed, the fundamental goal is to comeup with an approach that can reliably model the scene depth from a set oftraining examples. In the pursuit of perfect depth estimation, most existingstate-of-the-art learning techniques predict a single scalar depth valueper-pixel. Yet, it is well-known that the trained model has accuracy limits andcan predict imprecise depth. Therefore, an SIDP approach must be mindful of theexpected depth variations in the model's prediction at test time. Accordingly,we introduce an approach that performs continuous modeling of per-pixel depth,where we can predict and reason about the per-pixel depth and its distribution.To this end, we model per-pixel scene depth using a multivariate Gaussiandistribution. Moreover, contrary to the existing uncertainty modeling methods-- in the same spirit, where per-pixel depth is assumed to be independent, weintroduce per-pixel covariance modeling that encodes its depth dependency w.r.tall the scene points. Unfortunately, per-pixel depth covariance modeling leadsto a computationally expensive continuous loss function, which we solveefficiently using the learned low-rank approximation of the overall covariancematrix. Notably, when tested on benchmark datasets such as KITTI, NYU, andSUN-RGB-D, the SIDP model obtained by optimizing our loss function showsstate-of-the-art results. Our method's accuracy (named MG) is among the top onthe KITTI depth-prediction benchmark leaderboard.", "output": "Single Image Depth Prediction Made Better: A Multivariate Gaussian Take."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised Continual learning involves updating a deep neural network (DNN)from an ever-growing stream of labeled data. While most work has focused onovercoming catastrophic forgetting, one of the major motivations behindcontinual learning is being able to efficiently update a network with newinformation, rather than retraining from scratch on the training dataset as itgrows over time. Despite recent continual learning methods largely solving thecatastrophic forgetting problem, there has been little attention paid to theefficiency of these algorithms. Here, we study recent methods for incrementalclass learning and illustrate that many are highly inefficient in terms ofcompute, memory, and storage. Some methods even require more compute thantraining from scratch! We argue that for continual learning to have real-worldapplicability, the research community cannot ignore the resources used by thesealgorithms. There is more to continual learning than mitigating catastrophicforgetting.", "output": "How Efficient Are Today's Continual Learning Algorithms?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the fact that DeepFake forgery detection algorithms have achievedimpressive performance on known manipulations, they often face disastrousperformance degradation when generalized to an unseen manipulation. Some recentworks show improvement in generalization but rely on features fragile to imagedistortions such as compression. To this end, we propose Diff-ID, a concise andeffective approach that explains and measures the identity loss induced byfacial manipulations. When testing on an image of a specific person, Diff-IDutilizes an authentic image of that person as a reference and aligns them tothe same identity-insensitive attribute feature space by applying aface-swapping generator. We then visualize the identity loss between the testand the reference image from the image differences of the aligned pairs, anddesign a custom metric to quantify the identity loss. The metric is then provedto be effective in distinguishing the forgery images from the real ones.Extensive experiments show that our approach achieves high detectionperformance on DeepFake images and state-of-the-art generalization ability tounknown forgery methods, while also being robust to image distortions.", "output": "Diff-ID: An Explainable Identity Difference Quantification Framework for DeepFake Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of human action recognition using motion capture (MoCap)sequences. Unlike existing techniques that take multiple manual steps to derivestandardized skeleton representations as model input, we propose a novelSpatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences.The model uses a hierarchical transformer with intra-frame off-set attentionand inter-frame self-attention. The attention mechanism allows the model tofreely attend between any two vertex patches to learn non-local relationshipsin the spatial-temporal domain. Masked vertex modeling and future frameprediction are used as two self-supervised tasks to fully activate thebi-directional and auto-regressive attention in our hierarchical transformer.The proposed method achieves state-of-the-art performance compared toskeleton-based and point-cloud-based models on common MoCap benchmarks. Code isavailable at ", "output": "STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale diffusion models like Stable Diffusion are powerful and findvarious real-world applications while customizing such models by fine-tuning isboth memory and time inefficient. Motivated by the recent progress in naturallanguage processing, we investigate parameter-efficient tuning in largediffusion models by inserting small learnable modules (termed adapters). Inparticular, we decompose the design space of adapters into orthogonal factors-- the input position, the output position as well as the function form, andperform Analysis of Variance (ANOVA), a classical statistical approach foranalyzing the correlation between discrete (design options) and continuousvariables (evaluation metrics). Our analysis suggests that the input positionof adapters is the critical factor influencing the performance of downstreamtasks. Then, we carefully study the choice of the input position, and we findthat putting the input position after the cross-attention block can lead to thebest performance, validated by additional visualization analyses. Finally, weprovide a recipe for parameter-efficient tuning in diffusion models, which iscomparable if not superior to the fully fine-tuned baseline (e.g., DreamBooth)with only 0.75 % extra parameters, across various customized tasks.", "output": "A Closer Look at Parameter-Efficient Tuning in Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advances in 3D-aware generative models have pushed the boundary of imagesynthesis with explicit camera control. To achieve high-resolution imagesynthesis, several attempts have been made to design efficient generators, suchas hybrid architectures with both 3D and 2D components. However, such a designcompromises multiview consistency, and the design of a pure 3D generator withhigh resolution is still an open problem. In this work, we present GenerativeVolumetric Primitives (GVP), the first pure 3D generative model that can sampleand render 512-resolution images in real-time. GVP jointly models a number ofvolumetric primitives and their spatial information, both of which can beefficiently generated via a 2D convolutional network. The mixture of theseprimitives naturally captures the sparsity and correspondence in the 3D volume.The training of such a generator with a high degree of freedom is made possiblethrough a knowledge distillation technique. Experiments on several datasetsdemonstrate superior efficiency and 3D consistency of GVP over thestate-of-the-art.", "output": "GVP: Generative Volumetric Primitives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, research on few-shot learning (FSL) has been fast-growing inthe 2D image domain due to the less requirement for labeled training data andgreater generalization for novel classes. However, its application in 3D pointcloud data is relatively under-explored. Not only need to distinguish unseenclasses as in the 2D domain, 3D FSL is more challenging in terms of irregularstructures, subtle inter-class differences, and high intra-class variances{when trained on a low number of data.} Moreover, different architectures andlearning algorithms make it difficult to study the effectiveness of existing 2DFSL algorithms when migrating to the 3D domain. In this work, for the firsttime, we perform systematic and extensive investigations of directly applyingrecent 2D FSL works to 3D point cloud related backbone networks and thussuggest a strong learning baseline for few-shot 3D point cloud classification.Furthermore, we propose a new network, Point-cloud Correlation Interaction(PCIA), with three novel plug-and-play components called Salient-Part Fusion(SPF) module, Self-Channel Interaction Plus (SCI+) module, and Cross-InstanceFusion Plus (CIF+) module to obtain more representative embeddings and improvethe feature distinction. These modules can be inserted into most FSL algorithmswith minor changes and significantly improve the performance. Experimentalresults on three benchmark datasets, ModelNet40-FS, ShapeNet70-FS, andScanObjectNN-FS, demonstrate that our method achieves state-of-the-artperformance for the 3D FSL task. Code and datasets are available at", "output": "A Closer Look at Few-Shot 3D Point Cloud Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Without ground truth supervision, self-supervised depth estimation can betrapped in a local minimum due to the gradient-locality issue of thephotometric loss. In this paper, we present a framework to enhance depth byleveraging semantic segmentation to guide the network to jump out of the localminimum. Prior works have proposed to share encoders between these two tasks orexplicitly align them based on priors like the consistency between edges in thedepth and segmentation maps. Yet, these methods usually require ground truth orhigh-quality pseudo labels, which may not be easily accessible in real-worldapplications. In contrast, we investigate self-supervised depth estimationalong with a segmentation branch that is supervised with noisy labels providedby models pre-trained with limited data. We extend parameter sharing from theencoder to the decoder and study the influence of different numbers of shareddecoder parameters on model performance. Also, we propose to use cross-taskinformation to refine current depth and segmentation predictions to generatepseudo-depth and semantic labels for training. The advantages of the proposedmethod are demonstrated through extensive experiments on the KITTI benchmarkand a downstream task for endoscopic tissue deformation tracking.", "output": "SemHint-MD: Learning from Noisy Semantic Labels for Self-Supervised Monocular Depth Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Our goal is to learn a video representation that is useful for downstreamprocedure understanding tasks in instructional videos. Due to the small amountof available annotations, a key challenge in procedure understanding is to beable to extract from unlabeled videos the procedural knowledge such as theidentity of the task (e.g., 'make latte'), its steps (e.g., 'pour milk'), orthe potential next steps given partial progress in its execution. Our maininsight is that instructional videos depict sequences of steps that repeatbetween instances of the same or different tasks, and that this structure canbe well represented by a Procedural Knowledge Graph (PKG), where nodes arediscrete steps and edges connect steps that occur sequentially in theinstructional activities. This graph can then be used to generate pseudo labelsto train a video representation that encodes the procedural knowledge in a moreaccessible form to generalize to multiple procedure understanding tasks. Webuild a PKG by combining information from a text-based procedural knowledgedatabase and an unlabeled instructional video corpus and then use it togenerate training pseudo labels with four novel pre-training objectives. Wecall this PKG-based pre-training procedure and the resulting model Paprika,Procedure-Aware PRe-training for Instructional Knowledge Acquisition. Weevaluate Paprika on COIN and CrossTask for procedure understanding tasks suchas task recognition, step recognition, and step forecasting. Paprika yields avideo representation that improves over the state of the art: up to 11.23%gains in accuracy in 12 evaluation settings. Implementation is available at", "output": "Procedure-Aware Pretraining for Instructional Video Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Vision-Language Foundation Models (VLFM), such as CLIP, ALIGN andFlorence, are trained on large-scale datasets of image-caption pairs andachieve superior transferability and robustness on downstream tasks, but theyare difficult to use in many practical applications due to their large size,high latency and fixed architectures. Unfortunately, recent work shows traininga small custom VLFM for resource-limited applications is currently verydifficult using public and smaller-scale data. In this paper, we introduce anew distillation mechanism (DIME-FM) that allows us to transfer the knowledgecontained in large VLFMs to smaller, customized foundation models using arelatively small amount of inexpensive, unpaired images and sentences. Wetransfer the knowledge from the pre-trained CLIP-ViTL/14 model to a ViT-B/32model, with only 40M public images and 28.4M unpaired public sentences. Theresulting model \"Distill-ViT-B/32\" rivals the CLIP-ViT-B/32 model pre-trainedon its private WiT dataset (400M image-text pairs): Distill-ViT-B/32 achievessimilar results in terms of zero-shot and linear-probing performance on bothImageNet and the ELEVATER (20 image classification tasks) benchmarks. It alsodisplays comparable robustness when evaluated on five datasets with naturaldistribution shifts from ImageNet.", "output": "DIME-FM: DIstilling Multimodal and Efficient Foundation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the largest and most comprehensive empirical study of pre-trainedvisual representations (PVRs) or visual 'foundation models' for Embodied AI.First, we curate CortexBench, consisting of 17 different tasks spanninglocomotion, navigation, dexterous, and mobile manipulation. Next, wesystematically evaluate existing PVRs and find that none are universallydominant.To study the effect of pre-training data scale and diversity, we combine over4,000 hours of egocentric videos from 7 different sources (over 5.6M images)and ImageNet to train different-sized vision transformers using MaskedAuto-Encoding (MAE) on slices of this data. Contrary to inferences from priorwork, we find that scaling dataset size and diversity does not improveperformance universally (but does so on average).Our largest model, named VC-1, outperforms all prior PVRs on average but doesnot universally dominate either. Finally, we show that task or domain-specificadaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achievingcompetitive or superior performance than the best known results on all of thebenchmarks in CortexBench. These models required over 10,000 GPU-hours to trainand can be found on our website for the benefit of the research community.", "output": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce $infty$-Diff, a generative diffusion model which directlyoperates on infinite resolution data. By randomly sampling subsets ofcoordinates during training and learning to denoise the content at thosecoordinates, a continuous function is learned that allows sampling at arbitraryresolutions. In contrast to other recent infinite resolution generative models,our approach operates directly on the raw data, not requiring latent vectorcompression for context, using hypernetworks, nor relying on discretecomponents. As such, our approach achieves significantly higher sample quality,as evidenced by lower FID scores, as well as being able to effectively scale tohigher resolutions than the training data while retaining detail.", "output": "$\\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating 3D humans from images often produces implausible bodies that lean,float, or penetrate the floor. Such methods ignore the fact that bodies aretypically supported by the scene. A physics engine can be used to enforcephysical plausibility, but these are not differentiable, rely on unrealisticproxy bodies, and are difficult to integrate into existing optimization andlearning frameworks. In contrast, we exploit novel intuitive-physics (IP) termsthat can be inferred from a 3D SMPL body interacting with the scene. Inspiredby biomechanics, we infer the pressure heatmap on the body, the Center ofPressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). Withthese, we develop IPMAN, to estimate a 3D body from a color image in a \"stable\"configuration by encouraging plausible floor contact and overlapping CoP andCoM. Our IP terms are intuitive, easy to implement, fast to compute,differentiable, and can be integrated into existing optimization and regressionmethods. We evaluate IPMAN on standard datasets and MoYo, a new dataset withsynchronized multi-view images, ground-truth 3D bodies with complex poses,body-floor contact, CoM and pressure. IPMAN produces more plausible resultsthan the state of the art, improving accuracy for static poses, while nothurting dynamic ones. Code and data are available for research at", "output": "3D Human Pose Estimation via Intuitive Physics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object re-identification (ReID) aims to find instances with the same identityas the given probe from a large gallery. Pairwise losses play an important rolein training a strong ReID network. Existing pairwise losses densely exploiteach instance as an anchor and sample its triplets in a mini-batch. This densesampling mechanism inevitably introduces positive pairs that share few visualsimilarities, which can be harmful to the training. To address this problem, wepropose a novel loss paradigm termed Sparse Pairwise (SP) loss that onlyleverages few appropriate pairs for each class in a mini-batch, and empiricallydemonstrate that it is sufficient for the ReID tasks. Based on the proposedloss framework, we propose an adaptive positive mining strategy that candynamically adapt to diverse intra-class variations. Extensive experiments showthat SP loss and its adaptive variant AdaSP loss outperform other pairwiselosses, and achieve state-of-the-art performance across several ReIDbenchmarks. Code is available at ", "output": "Adaptive Sparse Pairwise Loss for Object Re-Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Creative workflows for generating graphical documents involve complexinter-related tasks, such as aligning elements, choosing appropriate fonts, oremploying aesthetically harmonious colors. In this work, we attempt at buildinga holistic model that can jointly solve many different design tasks. Our model,which we denote by FlexDM, treats vector graphic documents as a set ofmulti-modal elements, and learns to predict masked fields such as element type,position, styling attributes, image, or text, using a unified architecture.Through the use of explicit multi-task learning and in-domain pre-training, ourmodel can better capture the multi-modal relationships among the differentdocument fields. Experimental results corroborate that our single FlexDM isable to successfully solve a multitude of different design tasks, whileachieving performance that is competitive with task-specific and costlybaselines.", "output": "Towards Flexible Multi-modal Document Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In deep metric learning (DML), high-level input data are represented in alower-level representation (embedding) space, such that samples from the sameclass are mapped close together, while samples from disparate classes aremapped further apart. In this lower-level representation, only a singleinference sample from each known class is required to discriminate betweenclasses accurately. The features a DML model uses to discriminate betweenclasses and the importance of each feature in the training process are unknown.To investigate this, this study takes embeddings trained to discriminate faces(identities) and uses unsupervised clustering to identify the features involvedin facial identity discrimination by examining their representation within theembedded space. This study is split into two cases; intra classsub-discrimination, where attributes that differ between a single identity areconsidered; such as beards and emotions; and extra class sub-discrimination,where attributes which differ between different identities/people, areconsidered; such as gender, skin tone and age. In the intra class scenario, theinference process distinguishes common attributes between single identities,achieving 90.0% and 76.0% accuracy for beards and glasses, respectively. Thesystem can also perform extra class sub-discrimination with a high accuracyrate, notably 99.3%, 99.3% and 94.1% for gender, skin tone, and age,respectively.", "output": "Feature Representation in Deep Metric Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to domain shift, a large performance drop is usually observed when atrained crowd counting model is deployed in the wild. While existingdomain-adaptive crowd counting methods achieve promising results, theytypically regard each crowd image as a whole and reduce domain discrepancies ina holistic manner, thus limiting further improvement of domain adaptationperformance. To this end, we propose to untangle emph{domain-invariant} crowdand emph{domain-specific} background from crowd images and design afine-grained domain adaption method for crowd counting. Specifically, todisentangle crowd from background, we propose to learn crowd segmentation frompoint-level crowd counting annotations in a weakly-supervised manner. Based onthe derived segmentation, we design a crowd-aware domain adaptation mechanismconsisting of two crowd-aware adaptation modules, i.e., Crowd Region Transfer(CRT) and Crowd Density Alignment (CDA). The CRT module is designed to guidecrowd features transfer across domains beyond background distractions. The CDAmodule dedicates to regularising target-domain crowd density generation by itsown crowd density distribution. Our method outperforms previous approachesconsistently in the widely-used adaptation scenarios.", "output": "Fine-grained Domain Adaptive Crowd Counting via Point-derived Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep generative models (DGMs) have the potential to revolutionize diagnosticimaging. Generative adversarial networks (GANs) are one kind of DGM which arewidely employed. The overarching problem with deploying GANs, and other DGMs,in any application that requires domain expertise in order to actually use thegenerated images is that there generally is not adequate or automatic means ofassessing the domain-relevant quality of generated images. In this work, wedemonstrate several objective tests of images output by two popular GANarchitectures. We designed several stochastic context models (SCMs) of distinctimage features that can be recovered after generation by a trained GAN. Severalof these features are high-order, algorithmic pixel-arrangement rules which arenot readily expressed in covariance matrices. We designed and validatedstatistical classifiers to detect specific effects of the known arrangementrules. We then tested the rates at which two different GANs correctlyreproduced the feature context under a variety of training scenarios, anddegrees of feature-class similarity. We found that ensembles of generatedimages can appear largely accurate visually, and show high accuracy in ensemblemeasures, while not exhibiting the known spatial arrangements. Furthermore,GANs trained on a spectrum of distinct spatial orders did not respect the givenprevalence of those orders in the training data. The main conclusion is thatSCMs can be engineered to quantify numerous errors, per image, that may not becaptured in ensemble statistics but plausibly can affect subsequent use of theGAN-generated images.", "output": "A Method for Evaluating Deep Generative Models of Images via Assessing the Reproduction of High-order Spatial Context."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose VRL3, a powerful data-driven framework with a simple design forsolving challenging visual deep reinforcement learning (DRL) tasks. We analyzea number of major obstacles in taking a data-driven approach, and present asuite of design principles, novel findings, and critical insights aboutdata-driven visual DRL. Our framework has three stages: in stage 1, we leveragenon-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations;in stage 2, we use offline RL data (e.g. a limited number of expertdemonstrations) to convert the task-agnostic representations into more powerfultask-specific representations; in stage 3, we fine-tune the agent with onlineRL. On a set of challenging hand manipulation tasks with sparse reward andrealistic visual inputs, compared to the previous SOTA, VRL3 achieves anaverage of 780% better sample efficiency. And on the hardest task, VRL3 is1220% more sample efficient (2440% when using a wider encoder) and solves thetask with only 10% of the computation. These significant results clearlydemonstrate the great potential of data-driven deep reinforcement learning.", "output": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In previous deep-learning-based methods, semantic segmentation has beenregarded as a static or dynamic per-pixel classification task, textit{i.e.,}classify each pixel representation to a specific category. However, thesemethods only focus on learning better pixel representations or classificationkernels while ignoring the structural information of objects, which is criticalto human decision-making mechanism. In this paper, we present a new paradigmfor semantic segmentation, named structure-aware extraction. Specifically, itgenerates the segmentation results via the interactions between a set oflearned structure tokens and the image feature, which aims to progressivelyextract the structural information of each category from the feature. Extensiveexperiments show that our StructToken outperforms the state-of-the-art on threewidely-used benchmarks, including ADE20K, Cityscapes, and COCO-Stuff-10K.", "output": "StructToken : Rethinking Semantic Segmentation with Structural Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the HIT-UAV dataset, a high-altitude infrared thermal dataset forobject detection applications on Unmanned Aerial Vehicles (UAVs). The datasetcomprises 2,898 infrared thermal images extracted from 43,470 frames inhundreds of videos captured by UAVs in various scenarios including schools,parking lots, roads, and playgrounds. Moreover, the HIT-UAV provides essentialflight data for each image, such as flight altitude, camera perspective, date,and daylight intensity. For each image, we have manually annotated objectinstances with bounding boxes of two types (oriented and standard) to tacklethe challenge of significant overlap of object instances in aerial images. Tothe best of our knowledge, the HIT-UAV is the first publicly availablehigh-altitude UAV-based infrared thermal dataset for detecting persons andvehicles. We have trained and evaluated well-established object detectionalgorithms on the HIT-UAV. Our results demonstrate that the detectionalgorithms perform exceptionally well on the HIT-UAV compared to visual lightdatasets since infrared thermal images do not contain significant irrelevantinformation about objects. We believe that the HIT-UAV will contribute tovarious UAV-based applications and researches. The dataset is freely availableat ", "output": "HIT-UAV: A high-altitude infrared thermal dataset for Unmanned Aerial Vehicle-based object detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans naturally change their environment through interactions, e.g., byopening doors or moving furniture. To reproduce such interactions in virtualspaces (e.g., metaverse), we need to capture and model them, including changesin the scene geometry, ideally from egocentric input alone (head camera andbody-worn inertial sensors). While the head camera can be used to localize theperson in the scene, estimating dynamic object pose is much more challenging.As the object is often not visible from the head camera (e.g., a human notlooking at a chair while sitting down), we can not rely on visual object poseestimation. Instead, our key observation is that human motion tells us a lotabout scene changes. Motivated by this, we present iReplica, the firsthuman-object interaction reasoning method which can track objects and scenechanges based solely on human motion. iReplica is an essential first steptowards advanced AR/VR applications in immersive virtual universes and canprovide human-centric training data to teach machines to interact with theirsurroundings. Our code, data and model will be available on our project page at<a href=\" http URL</a>", "output": "Interaction Replica: Tracking human-object interaction and scene changes from human motion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose Mixed and Masked AutoEncoder (MixMAE), a simple butefficient pretraining method that is applicable to various hierarchical VisionTransformers. Existing masked image modeling (MIM) methods for hierarchicalVision Transformers replace a random subset of input tokens with a special[MASK] symbol and aim at reconstructing original image tokens from thecorrupted image. However, we find that using the [MASK] symbol greatly slowsdown the training and causes pretraining-finetuning inconsistency, due to thelarge masking ratio (e.g., 60% in SimMIM). On the other hand, MAE does notintroduce [MASK] tokens at its encoder at all but is not applicable forhierarchical Vision Transformers. To solve the issue and accelerate thepretraining of hierarchical models, we replace the masked tokens of one imagewith visible tokens of another image, i.e., creating a mixed image. We thenconduct dual reconstruction to reconstruct the two original images from themixed input, which significantly improves efficiency. While MixMAE can beapplied to various hierarchical Transformers, this paper explores using SwinTransformer with a large window size and scales up to huge model size (to reach600M parameters). Empirical results demonstrate that MixMAE can learnhigh-quality visual representations efficiently. Notably, MixMAE withSwin-B/W14 achieves 85.1% top-1 accuracy on ImageNet-1K by pretraining for 600epochs. Besides, its transfer performances on the other 6 datasets show thatMixMAE has better FLOPs / performance tradeoff than previous popular MIMmethods. Code is available at ", "output": "MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time-dependent data-generating distributions have proven to be difficult forgradient-based training of neural networks, as the greedy updates result incatastrophic forgetting of previously learned knowledge. Despite the progressin the field of continual learning to overcome this forgetting, we show that aset of common state-of-the-art methods still suffers from substantialforgetting upon starting to learn new tasks, except that this forgetting istemporary and followed by a phase of performance recovery. We refer to thisintriguing but potentially problematic phenomenon as the stability gap. Thestability gap had likely remained under the radar due to standard practice inthe field of evaluating continual learning models only after each task.Instead, we establish a framework for continual evaluation that usesper-iteration evaluation and we define a new set of metrics to quantifyworst-case performance. Empirically we show that experience replay,constraint-based replay, knowledge-distillation, and parameter regularizationmethods are all prone to the stability gap; and that the stability gap can beobserved in class-, task-, and domain-incremental learning benchmarks.Additionally, a controlled experiment shows that the stability gap increaseswhen tasks are more dissimilar. Finally, by disentangling gradients intoplasticity and stability components, we propose a conceptual explanation forthe stability gap.", "output": "Continual evaluation for lifelong learning: Identifying the stability gap."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Measuring perceptual color differences (CDs) is of great importance in modernsmartphone photography. Despite the long history, most CD measures have beenconstrained by psychophysical data of homogeneous color patches or a limitednumber of simplistic natural photographic images. It is thus questionablewhether existing CD measures generalize in the age of smartphone photographycharacterized by greater content complexities and learning-based image signalprocessors. In this paper, we put together so far the largest image dataset forperceptual CD assessment, in which the photographic images are 1) captured bysix flagship smartphones, 2) altered by Photoshop, 3) post-processed bybuilt-in filters of the smartphones, and 4) reproduced with incorrect colorprofiles. We then conduct a large-scale psychophysical experiment to gatherperceptual CDs of 30,000 image pairs in a carefully controlled laboratoryenvironment. Based on the newly established dataset, we make one of the firstattempts to construct an end-to-end learnable CD formula based on a lightweightneural network, as a generalization of several previous metrics. Extensiveexperiments demonstrate that the optimized formula outperforms 33 existing CDmeasures by a large margin, offers reasonable local CD maps without the use ofdense supervision, generalizes well to homogeneous color patch data, andempirically behaves as a proper metric in the mathematical sense. Our datasetand code are publicly available at ", "output": "Measuring Perceptual Color Differences of Smartphone Photographs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study two of the most popular performance metrics in medical imagesegmentation, Accuracy and Dice, when the target labels are noisy. For bothmetrics, several statements related to characterization and volume propertiesof the set of optimal segmentations are proved, and associated experiments areprovided. Our main insights are: (i) the volume of the solutions to bothmetrics may deviate significantly from the expected volume of the target, (ii)the volume of a solution to Accuracy is always less than or equal to the volumeof a solution to Dice and (iii) the optimal solutions to both of these metricscoincide when the set of feasible segmentations is constrained to the set ofsegmentations with the volume equal to the expected volume of the target.", "output": "On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Imputation of missing images via source-to-target modality translation canimprove diversity in medical imaging protocols. A pervasive approach forsynthesizing target images involves one-shot mapping through generativeadversarial networks (GAN). Yet, GAN models that implicitly characterize theimage distribution can suffer from limited sample fidelity. Here, we propose anovel method based on adversarial diffusion modeling, SynDiff, for improvedperformance in medical image translation. To capture a direct correlate of theimage distribution, SynDiff leverages a conditional diffusion process thatprogressively maps noise and source images onto the target image. For fast andaccurate image sampling during inference, large diffusion steps are taken withadversarial projections in the reverse diffusion direction. To enable trainingon unpaired datasets, a cycle-consistent architecture is devised with coupleddiffusive and non-diffusive modules that bilaterally translate between twomodalities. Extensive assessments are reported on the utility of SynDiffagainst competing GAN and diffusion models in multi-contrast MRI and MRI-CTtranslation. Our demonstrations indicate that SynDiff offers quantitatively andqualitatively superior performance against competing baselines.", "output": "Unsupervised Medical Image Translation with Adversarial Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trajectory prediction has been widely pursued in many fields, and manymodel-based and model-free methods have been explored. The former includerule-based, geometric or optimization-based models, and the latter are mainlycomprised of deep learning approaches. In this paper, we propose a new methodcombining both methodologies based on a new Neural Differential Equation model.Our new model (Neural Social Physics or NSP) is a deep neural network withinwhich we use an explicit physics model with learnable parameters. The explicitphysics model serves as a strong inductive bias in modeling pedestrianbehaviors, while the rest of the network provides a strong data-fittingcapability in terms of system parameter estimation and dynamics stochasticitymodeling. We compare NSP with 15 recent deep learning methods on 6 datasets andimprove the state-of-the-art performance by 5.56%-70%. Besides, we show thatNSP has better generalizability in predicting plausible trajectories indrastically different scenarios where the density is 2-5 times as high as thetesting data. Finally, we show that the physics model in NSP can provideplausible explanations for pedestrian behaviors, as opposed to black-box deeplearning. Code is available:", "output": "Human Trajectory Prediction via Neural Social Physics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conventional de-noising methods rely on the assumption that all samples areindependent and identically distributed, so the resultant classifier, thoughdisturbed by noise, can still easily identify the noises as the outliers oftraining distribution. However, the assumption is unrealistic in large-scaledata that is inevitably long-tailed. Such imbalanced training data makes aclassifier less discriminative for the tail classes, whose previously \"easy\"noises are now turned into \"hard\" ones -- they are almost as outliers as theclean tail samples. We introduce this new challenge as Noisy Long-TailedClassification (NLT). Not surprisingly, we find that most de-noising methodsfail to identify the hard noises, resulting in significant performance drop onthe three proposed NLT benchmarks: ImageNet-NLT, Animal10-NLT, and Food101-NLT.To this end, we design an iterative noisy learning framework calledHard-to-Easy (H2E). Our bootstrapping philosophy is to first learn a classifieras noise identifier invariant to the class and context distributional changes,reducing \"hard\" noises to \"easy\" ones, whose removal further improves theinvariance. Experimental results show that our H2E outperforms state-of-the-artde-noising methods and their ablations on long-tailed settings whilemaintaining a stable performance on the conventional balanced settings.Datasets and codes are available at ", "output": "Identifying Hard Noise in Long-Tailed Sample Distribution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Freely available and easy-to-use audio editing tools make it straightforwardto perform audio splicing. Convincing forgeries can be created by combiningvarious speech samples from the same person. Detection of such splices isimportant both in the public sector when considering misinformation, and in alegal context to verify the integrity of evidence. Unfortunately, most existingdetection algorithms for audio splicing use handcrafted features and makespecific assumptions. However, criminal investigators are often faced withaudio samples from unconstrained sources with unknown characteristics, whichraises the need for more generally applicable methods.With this work, we aim to take a first step towards unconstrained audiosplicing detection to address this need. We simulate various attack scenariosin the form of post-processing operations that may disguise splicing. Wepropose a Transformer sequence-to-sequence (seq2seq) network for splicingdetection and localization. Our extensive evaluation shows that the proposedmethod outperforms existing dedicated approaches for splicing detection [3, 10]as well as the general-purpose networks EfficientNet [28] and RegNet [25].", "output": "Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Out-Of-Distribution generalization (OOD) is all about learning invarianceagainst environmental changes. If the context in every class is evenlydistributed, OOD would be trivial because the context can be easily removed dueto an underlying principle: class is invariant to context. However, collectingsuch a balanced dataset is impractical. Learning on imbalanced data makes themodel bias to context and thus hurts OOD. Therefore, the key to OOD is contextbalance. We argue that the widely adopted assumption in prior work, the contextbias can be directly annotated or estimated from biased class prediction,renders the context incomplete or even incorrect. In contrast, we point out theeveroverlooked other side of the above principle: context is also invariant toclass, which motivates us to consider the classes (which are already labeled)as the varying environments to resolve context bias (without context labels).We implement this idea by minimizing the contrastive loss of intra-class samplesimilarity while assuring this similarity to be invariant across all classes.On benchmarks with various context biases and domain gaps, we show that asimple re-weighting based classifier equipped with our context estimationachieves state-of-the-art performance. We provide the theoreticaljustifications in Appendix and codes on", "output": "Class Is Invariant to Context and Vice Versa: On Learning Invariance for Out-Of-Distribution Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in neural radiance fields (NeRFs) achieve state-of-the-artnovel view synthesis and facilitate dense estimation of scene properties.However, NeRFs often fail for large, unbounded scenes that are captured undervery sparse views with the scene content concentrated far away from the camera,as is typical for field robotics applications. In particular, NeRF-stylealgorithms perform poorly: (1) when there are insufficient views with littlepose diversity, (2) when scenes contain saturation and shadows, and (3) whenfinely sampling large unbounded scenes with fine structures becomescomputationally intensive.This paper proposes CLONeR, which significantly improves upon NeRF byallowing it to model large outdoor driving scenes that are observed from sparseinput sensor views. This is achieved by decoupling occupancy and color learningwithin the NeRF framework into separate Multi-Layer Perceptrons (MLPs) trainedusing LiDAR and camera data, respectively. In addition, this paper proposes anovel method to build differentiable 3D Occupancy Grid Maps (OGM) alongside theNeRF model, and leverage this occupancy grid for improved sampling of pointsalong a ray for volumetric rendering in metric space.Through extensive quantitative and qualitative experiments on scenes from theKITTI dataset, this paper demonstrates that the proposed method outperformsstate-of-the-art NeRF models on both novel view synthesis and dense depthprediction tasks when trained on sparse input data.", "output": "CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning, especially deep neural networks (DNNs), has been widely andsuccessfully adopted in many critical applications for its high effectivenessand efficiency. The rapid development of DNNs has benefited from the existenceof some high-quality datasets ($e.g.$, ImageNet), which allow researchers anddevelopers to easily verify the performance of their methods. Currently, almostall existing released datasets require that they can only be adopted foracademic or educational purposes rather than commercial purposes withoutpermission. However, there is still no good way to ensure that. In this paper,we formulate the protection of released datasets as verifying whether they areadopted for training a (suspicious) third-party model, where defenders can onlyquery the model while having no information about its parameters and trainingdetails. Based on this formulation, we propose to embed external patterns viabackdoor watermarking for the ownership verification to protect them. Ourmethod contains two main parts, including dataset watermarking and datasetverification. Specifically, we exploit poison-only backdoor attacks ($e.g.$,BadNets) for dataset watermarking and design a hypothesis-test-guided methodfor dataset verification. We also provide some theoretical analyses of ourmethods. Experiments on multiple benchmark datasets of different tasks areconducted, which verify the effectiveness of our method. The code forreproducing main experiments is available aturl{", "output": "Black-box Dataset Ownership Verification via Backdoor Watermarking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusion models (DDMs) have attracted attention for theirexceptional generation quality and diversity. This success is largelyattributed to the use of class- or text-conditional diffusion guidance methods,such as classifier and classifier-free guidance. In this paper, we present amore comprehensive perspective that goes beyond the traditional guidancemethods. From this generalized perspective, we introduce novel condition- andtraining-free strategies to enhance the quality of generated images. As asimple solution, blur guidance improves the suitability of intermediate samplesfor their fine-scale information and structures, enabling diffusion models togenerate higher quality samples with a moderate guidance scale. Improving uponthis, Self-Attention Guidance (SAG) uses the intermediate self-attention mapsof diffusion models to enhance their stability and efficacy. Specifically, SAGadversarially blurs only the regions that diffusion models attend to at eachiteration and guides them accordingly. Our experimental results show that ourSAG improves the performance of various diffusion models, including ADM, IDDPM,Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidancemethods leads to further improvement.", "output": "Improving Sample Quality of Diffusion Models Using Self-Attention Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the clear performance benefits of data augmentations, little is knownabout why they are so effective. In this paper, we disentangle several keymechanisms through which data augmentations operate. Establishing an exchangerate between augmented and additional real data, we find that inout-of-distribution testing scenarios, augmentations which yield samples thatare diverse, but inconsistent with the data distribution can be even morevaluable than additional training data. Moreover, we find that dataaugmentations which encourage invariances can be more valuable than invariancealone, especially on small and medium sized training sets. Following thisobservation, we show that augmentations induce additional stochasticity duringtraining, effectively flattening the loss landscape.", "output": "How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the ever-increasing interest in applying deep learning (DL) models tomedical imaging, the typical scarcity and imbalance of medical datasets canseverely impact the performance of DL models. The generation of synthetic datathat might be freely shared without compromising patient privacy is awell-known technique for addressing these difficulties. Inpainting algorithmsare a subset of DL generative models that can alter one or more regions of aninput image while matching its surrounding context and, in certain cases,non-imaging input conditions. Although the majority of inpainting techniquesfor medical imaging data use generative adversarial networks (GANs), theperformance of these algorithms is frequently suboptimal due to their limitedoutput variety, a problem that is already well-known for GANs. Denoisingdiffusion probabilistic models (DDPMs) are a recently introduced family ofgenerative networks that can generate results of comparable quality to GANs,but with diverse outputs. In this paper, we describe a DDPM to execute multipleinpainting tasks on 2D axial slices of brain MRI with various sequences, andpresent proof-of-concept examples of its performance in a variety of evaluationscenarios. Our model and a public online interface to try our tool areavailable at: ", "output": "Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generic motion understanding from video involves not only tracking objects,but also perceiving how their surfaces deform and move. This information isuseful to make inferences about 3D shape, physical properties and objectinteractions. While the problem of tracking arbitrary physical points onsurfaces over longer video clips has received some attention, no dataset orbenchmark for evaluation existed, until now. In this paper, we first formalizethe problem, naming it tracking any point (TAP). We introduce a companionbenchmark, TAP-Vid, which is composed of both real-world videos with accuratehuman annotations of point tracks, and synthetic videos with perfectground-truth point tracks. Central to the construction of our benchmark is anovel semi-automatic crowdsourced pipeline which uses optical flow estimates tocompensate for easier, short-term motion like camera shake, allowing annotatorsto focus on harder sections of video. We validate our pipeline on syntheticdata and propose a simple end-to-end point tracking model TAP-Net, showing thatit outperforms all prior methods on our benchmark when trained on syntheticdata.", "output": "TAP-Vid: A Benchmark for Tracking Any Point in a Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Purpose: This research aims to facilitate the use of state-of-the-artcomputer vision algorithms for the automated training of surgeons and theanalysis of surgical footage. By estimating 2D hand poses, we model themovement of the practitioner's hands, and their interaction with surgicalinstruments, to study their potential benefit for surgical training.Methods: We leverage pre-trained models on a publicly-available hands datasetto create our own in-house dataset of 100 open surgery simulation videos with2D hand poses. We also assess the ability of pose estimations to segmentsurgical videos into gestures and tool-usage segments and compare them tokinematic sensors and I3D features. Furthermore, we introduce 6 novel surgicaldexterity proxies stemming from domain experts' training advice, all of whichour framework can automatically detect given raw video footage.Results: State-of-the-art gesture segmentation accuracy of 88.35% on theOpen Surgery Simulation dataset is achieved with the fusion of 2D poses and I3Dfeatures from multiple angles. The introduced surgical skill proxies presentedsignificant differences for novices compared to experts and produced actionablefeedback for improvement.Conclusion: This research demonstrates the benefit of pose estimations foropen surgery by analyzing their effectiveness in gesture segmentation and skillassessment. Gesture segmentation using pose estimations achieved comparableresults to physical sensors while being remote and markerless. Surgicaldexterity proxies that rely on pose estimation proved they can be used to worktowards automated training feedback. We hope our findings encourage additionalcollaboration on novel skill proxies to make surgical training more efficient.", "output": "Using Hand Pose Estimation To Automate Open Surgery Training Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the design of a productionized end-to-end stereo depth sensingsystem that does pre-processing, online stereo rectification, and stereo depthestimation with a fallback to monocular depth estimation when rectification isunreliable. The output of our depth sensing system is then used in a novel viewgeneration pipeline to create 3D computational photography effects usingpoint-of-view images captured by smart glasses. All these steps are executedon-device on the stringent compute budget of a mobile phone, and because weexpect the users can use a wide range of smartphones, our design needs to begeneral and cannot be dependent on a particular hardware or ML accelerator suchas a smartphone GPU. Although each of these steps is well studied, adescription of a practical system is still lacking. For such a system, allthese steps need to work in tandem with one another and fallback gracefully onfailures within the system or less than ideal input data. We show how we handleunforeseen changes to calibration, e.g., due to heat, robustly support depthestimation in the wild, and still abide by the memory and latency constraintsrequired for a smooth user experience. We show that our trained models arefast, and run in less than 1s on a six-year-old Samsung Galaxy S8 phone's CPU.Our models generalize well to unseen data and achieve good results onMiddlebury and in-the-wild images captured from the smart glasses.", "output": "A Practical Stereo Depth System for Smart Glasses."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current Dynamic Texture Synthesis (DyTS) models can synthesize realisticvideos. However, they require a slow iterative optimization process tosynthesize a single fixed-size short video, and they do not offer anypost-training control over the synthesis process. We propose Dynamic NeuralCellular Automata (DyNCA), a framework for real-time and controllable dynamictexture synthesis. Our method is built upon the recently introduced NCA modelsand can synthesize infinitely long and arbitrary-sized realistic video texturesin real time. We quantitatively and qualitatively evaluate our model and showthat our synthesized videos appear more realistic than the existing results. Weimprove the SOTA DyTS performance by $2sim 4$ orders of magnitude. Moreover,our model offers several real-time video controls including motion speed,motion direction, and an editing brush tool. We exhibit our trained models inan online interactive demo that runs on local hardware and is accessible onpersonal computers and smartphones.", "output": "DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hierarchical classification (HC) assigns each object with multiple labelsorganized into a hierarchical structure. The existing deep learning based HCmethods usually predict an instance starting from the root node until a leafnode is reached. However, in the real world, images interfered by noise,occlusion, blur, or low resolution may not provide sufficient information forthe classification at subordinate levels. To address this issue, we propose anovel semantic guided level-category hybrid prediction network (SGLCHPN) thatcan jointly perform the level and category prediction in an end-to-end manner.SGLCHPN comprises two modules: a visual transformer that extracts featurevectors from the input images, and a semantic guided cross-attention modulethat uses categories word embeddings as queries to guide learningcategory-specific representations. In order to evaluate the proposed method, weconstruct two new datasets in which images are at a broad range of quality andthus are labeled to different levels (depths) in the hierarchy according totheir individual quality. Experimental results demonstrate the effectiveness ofour proposed HC method.", "output": "Semantic Guided Level-Category Hybrid Prediction Network for Hierarchical Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of predicting the 3D shape, articulation, viewpoint,texture, and lighting of an articulated animal like a horse given a single testimage as input. We present a new method, dubbed MagicPony, that learns thispredictor purely from in-the-wild single-view images of the object category,with minimal assumptions about the topology of deformation. At its core is animplicit-explicit representation of articulated shape and appearance, combiningthe strengths of neural fields and meshes. In order to help the modelunderstand an object's shape and pose, we distil the knowledge captured by anoff-the-shelf self-supervised vision transformer and fuse it into the 3D model.To overcome local optima in viewpoint estimation, we further introduce a newviewpoint sampling scheme that comes at no additional training cost. MagicPonyoutperforms prior work on this challenging task and demonstrates excellentgeneralisation in reconstructing art, despite the fact that it is only trainedon real images.", "output": "MagicPony: Learning Articulated 3D Animals in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating a dense depth map from a single view is geometrically ill-posed,and state-of-the-art methods rely on learning depth's relation with visualappearance using deep neural networks. On the other hand, Structure from Motion(SfM) leverages multi-view constraints to produce very accurate but sparsemaps, as matching across images is typically limited by locally discriminativetexture. In this work, we combine the strengths of both approaches by proposinga novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts theperformance of single-view depth networks at test time using SfM multi-viewcues. Specifically, and differently from the state of the art, we use sparseSfM point clouds as test-time self-supervisory signal, fine-tuning the networkencoder to learn a better representation of the test scene. Our results showhow the addition of SfM-TTR to several state-of-the-art self-supervised andsupervised networks improves significantly their performance, outperformingprevious TTR baselines mainly based on photometric multi-view consistency. Thecode is available at ", "output": "SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent 3D generative models have achieved remarkable performance insynthesizing high resolution photorealistic images with view consistency anddetailed 3D shapes, but training them for diverse domains is challenging sinceit requires massive training images and their camera distribution information.Text-guided domain adaptation methods have shown impressive performance onconverting the 2D generative model on one domain into the models on otherdomains with different styles by leveraging the CLIP (ContrastiveLanguage-Image Pre-training), rather than collecting massive datasets for thosedomains. However, one drawback of them is that the sample diversity in theoriginal generative model is not well-preserved in the domain-adaptedgenerative models due to the deterministic nature of the CLIP text encoder.Text-guided domain adaptation will be even more challenging for 3D generativemodels not only because of catastrophic diversity loss, but also because ofinferior text-image correspondence and poor image quality. Here we proposeDATID-3D, a domain adaptation method tailored for 3D generative models usingtext-to-image diffusion models that can synthesize diverse images per textprompt without collecting additional images and camera information for thetarget domain. Unlike 3D extensions of prior text-guided domain adaptationmethods, our novel pipeline was able to fine-tune the state-of-the-art 3Dgenerator of the source domain to synthesize high resolution, multi-viewconsistent images in text-guided targeted domains without additional data,outperforming the existing text-guided domain adaptation methods in diversityand text-image correspondence. Furthermore, we propose and demonstrate diverse3D image manipulations such as one-shot instance-selected adaptation andsingle-view manipulated 3D reconstruction to fully enjoy diversity in text.", "output": "DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Depth-aware Image-based NEural Radiance fields (DINER). Given asparse set of RGB input views, we predict depth and feature maps to guide thereconstruction of a volumetric scene representation that allows us to render 3Dobjects under novel views. Specifically, we propose novel techniques toincorporate depth information into feature fusion and efficient scene sampling.In comparison to the previous state of the art, DINER achieves higher synthesisquality and can process input views with greater disparity. This allows us tocapture scenes more completely without changing capturing hardware requirementsand ultimately enables larger viewpoint changes during novel view synthesis. Weevaluate our method by synthesizing novel views, both for human heads and forgeneral objects, and observe significantly improved qualitative results andincreased perceptual metrics compared to the previous state of the art. Thecode is publicly available for research purposes.", "output": "DINER: Depth-aware Image-based NEural Radiance fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object goal navigation (ObjectNav) in unseen environments is a fundamentaltask for Embodied AI. Agents in existing works learn ObjectNav policies basedon 2D maps, scene graphs, or image sequences. Considering this task happens in3D space, a 3D-aware agent can advance its ObjectNav capability via learningfrom fine-grained spatial information. However, leveraging 3D scenerepresentation can be prohibitively unpractical for policy learning in thisfloor-level task, due to low sample efficiency and expensive computationalcost. In this work, we propose a framework for the challenging 3D-awareObjectNav based on two straightforward sub-policies. The two sub-polices,namely corner-guided exploration policy and category-aware identificationpolicy, simultaneously perform by utilizing online fused 3D points asobservation. Through extensive experiments, we show that this framework candramatically improve the performance in ObjectNav through learning from 3Dscene representation. Our framework achieves the best performance among allmodular-based methods on the Matterport3D and Gibson datasets, while requiring(up to 30x) less computational cost for training.", "output": "3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Changing how pre-trained models behave -- e.g., improving their performanceon a downstream task or mitigating biases learned during pre-training -- is acommon practice when developing machine learning systems. In this work, wepropose a new paradigm for steering the behavior of neural networks, centeredaround textit{task vectors}. A task vector specifies a direction in the weightspace of a pre-trained model, such that movement in that direction improvesperformance on the task. We build task vectors by subtracting the weights of apre-trained model from the weights of the same model after fine-tuning on atask. We show that these task vectors can be modified and combined togetherthrough arithmetic operations such as negation and addition, and the behaviorof the resulting model is steered accordingly. Negating a task vector decreasesperformance on the target task, with little change in model behavior on controltasks. Moreover, adding task vectors together can improve performance onmultiple tasks at once. Finally, when tasks are linked by an analogyrelationship of the form ``A is to B as C is to D\", combining task vectors fromthree of the tasks can improve performance on the fourth, even when no datafrom the fourth task is used for training. Overall, our experiments withseveral models, modalities and tasks show that task arithmetic is a simple,efficient and effective way of editing models.", "output": "Editing Models with Task Arithmetic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prompt learning is one of the most effective and trending ways to adaptpowerful vision-language foundation models like CLIP to downstream datasets bytuning learnable prompt vectors with very few samples. However, although promptlearning achieves excellent performance over in-domain data, it still faces themajor challenge of generalizing to unseen classes and domains. Some existingprompt learning methods tackle this issue by adaptively generating differentprompts for different tokens or domains but neglecting the ability of learnedprompts to generalize to unseen domains. In this paper, we propose a novelprompt learning paradigm that directly generates emph{domain invariant} promptthat can be generalized to unseen domains, called MetaPrompt. Specifically, adual-modality prompt tuning network is proposed to generate prompts for inputfrom both image and text modalities. With a novel asymmetric contrastive loss,the representation from the original pre-trained vision-language model acts assupervision to enhance the generalization ability of the learned prompt. Moreimportantly, we propose a meta-learning-based prompt tuning algorithm thatexplicitly constrains the task-specific prompt tuned for one domain or class toalso achieve good performance in another domain or class. Extensive experimentson 11 datasets for base-to-new generalization and 4 datasets for domaingeneralization demonstrate that our method consistently and significantlyoutperforms existing methods.", "output": "Learning Domain Invariant Prompt for Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An insight into the architecture of the Encoder-Decoder Network with GuidedTransmission Map (EDN-GTM), a novel and effective single image dehazing scheme,is presented in this paper. The EDN-GTM takes a conventional RGB hazy image inconjunction with the corresponding transmission map estimated by the darkchannel prior (DCP) approach as inputs of the network. The EDN-GTM adopts anenhanced structure of U-Net developed for dehazing tasks and the resultingEDN-GDM has shown state-of-the-art performances on benchmark dehazing datasetsin terms of PSNR and SSIM metrics. In order to give an in-depth understandingof the well-designed architecture which largely contributes to the success ofthe EDN-GTM, extensive experiments and analysis from selecting the corestructure of the scheme to investigating advanced network designs are presentedin this paper.", "output": "Encoder-Decoder Network with Guided Transmission Map: Architecture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain adaptation of GANs is a problem of fine-tuning the state-of-the-artGAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domainwith few samples (e.g. painting faces, sketches, etc.). While there are a greatnumber of methods that tackle this problem in different ways, there are stillmany important questions that remain unanswered.In this paper, we provide a systematic and in-depth analysis of the domainadaptation problem of GANs, focusing on the StyleGAN model. First, we perform adetailed exploration of the most important parts of StyleGAN that areresponsible for adapting the generator to a new domain depending on thesimilarity between the source and target domains. As a result of this in-depthstudy, we propose new efficient and lightweight parameterizations of StyleGANfor domain adaptation. Particularly, we show there exist directions inStyleSpace (StyleDomain directions) that are sufficient for adapting to similardomains and they can be reduced further. For dissimilar domains, we proposeAffine$+$ and AffineLight$+$ parameterizations that allows us to outperformexisting baselines in few-shot adaptation with low data regime. Finally, weexamine StyleDomain directions and discover their many surprising propertiesthat we apply for domain mixing and cross-domain image morphing.", "output": "StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent work leverages the expressive power of generative adversarial networks(GANs) to generate labeled synthetic datasets. These dataset generation methodsoften require new annotations of synthetic images, which forces practitionersto seek out annotators, curate a set of synthetic images, and ensure thequality of generated labels. We introduce the HandsOff framework, a techniquecapable of producing an unlimited number of synthetic images and correspondinglabels after being trained on less than 50 pre-existing labeled images. Ourframework avoids the practical drawbacks of prior work by unifying the field ofGAN inversion with dataset generation. We generate datasets with richpixel-wise labels in multiple challenging domains such as faces, cars,full-body human poses, and urban driving scenes. Our method achievesstate-of-the-art performance in semantic segmentation, keypoint detection, anddepth estimation compared to prior dataset generation approaches and transferlearning baselines. We additionally showcase its ability to address broadchallenges in model development which stem from fixed, hand-annotated datasets,such as the long-tail problem in semantic segmentation. Project page:austinxu87.github.io/handsoff.", "output": "HandsOff: Labeled Dataset Generation With No Additional Human Annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the problem of extracting key steps from unlabeled proceduralvideos, motivated by the potential of Augmented Reality (AR) headsets torevolutionize job training and performance. We decompose the problem into twosteps: representation learning and key steps extraction. We propose a trainingobjective, Bootstrapped Multi-Cue Contrastive (BMC2) loss to learndisciriminative representations for various steps without any labels. Differentfrom prior works, we develop techniques to train a light-weight temporal modulewhich uses off-the-shelf features for self supervision. Our approach canseamlessly leverage information from multiple cues like optical flow, depth orgaze to learn discriminative features for key-steps making it amenable for ARapplications. We finally extract key steps via a tunable algorithm thatclusters the representations and samples. We show significant improvements overprior works for the task of key step localization and phase classification.Qualitative results demonstrate that the extracted key steps are meaningful tosuccinctly represent various steps of the procedural tasks.", "output": "STEPs: Self-Supervised Key Step Extraction from Unlabeled Procedural Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper demonstrates an approach for learning highly semantic imagerepresentations without relying on hand-crafted data-augmentations. Weintroduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), anon-generative approach for self-supervised learning from images. The ideabehind I-JEPA is simple: from a single context block, predict therepresentations of various target blocks in the same image. A core designchoice to guide I-JEPA towards producing semantic representations is themasking strategy; specifically, it is crucial to (a) sample target blocks withsufficiently large scale (semantic), and to (b) use a sufficiently informative(spatially distributed) context block. Empirically, when combined with VisionTransformers, we find I-JEPA to be highly scalable. For instance, we train aViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strongdownstream performance across a wide range of tasks, from linear classificationto object counting and depth prediction.", "output": "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new paradigm for fine-tuning large-scale visionlanguagepre-trained models on downstream task, dubbed Prompt Regularization (ProReg).Different from traditional fine-tuning which easily overfits to the downstreamtask data, ProReg uses the prediction by prompting the pretrained model toregularize the fine-tuning. The motivation is: by prompting the large model \"aphoto of a [CLASS]\", the fil-lin answer is only dependent on the pretrainingencyclopedic knowledge while independent of the task data distribution, whichis usually biased. Specifically, given a training sample prediction duringfine-tuning, we first calculate its KullbackLeibler loss of the promptprediction and Cross-Entropy loss of the ground-truth label, and then combinethem with a proposed sample-wise adaptive trade-off weight, which automaticallyadjusts the transfer between the pretrained and downstream domains. On variousout-of-distribution benchmarks, we show the consistently strong performance ofProReg compared with conventional fine-tuning, zero-shot prompt, prompt tuning,and other state-of-the-art methods.", "output": "Debiased Fine-Tuning for Vision-language Models by Prompt Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) are known to be vulnerable to adversarialgeometric transformation. This paper aims to verify the robustness oflarge-scale DNNs against the combination of multiple geometric transformationswith a provable guarantee. Given a set of transformations (e.g., rotation,scaling, etc.), we develop GeoRobust, a black-box robustness analyser builtupon a novel global optimisation strategy, for locating the worst-casecombination of transformations that affect and even alter a network's output.GeoRobust can provide provable guarantees on finding the worst-case combinationbased on recent advances in Lipschitzian theory. Due to its black-box nature,GeoRobust can be deployed on large-scale DNNs regardless of theirarchitectures, activation functions, and the number of neurons. In practice,GeoRobust can locate the worst-case geometric transformation with highprecision for the ResNet50 model on ImageNet in a few seconds on average. Weexamined 18 ImageNet classifiers, including the ResNet family and visiontransformers, and found a positive correlation between the geometric robustnessof the networks and the parameter numbers. We also observe that increasing thedepth of DNN is more beneficial than increasing its width in terms of improvingits geometric robustness. Our tool GeoRobust is available at", "output": "Towards Verifying the Geometric Robustness of Large-scale Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNN) which are employed in perception systems forautonomous driving require a huge amount of data to train on, as they mustreliably achieve high performance in all kinds of situations. However, theseDNN are usually restricted to a closed set of semantic classes available intheir training data, and are therefore unreliable when confronted withpreviously unseen instances. Thus, multiple perception datasets have beencreated for the evaluation of anomaly detection methods, which can becategorized into three groups: real anomalies in real-world, syntheticanomalies augmented into real-world and completely synthetic scenes. Thissurvey provides a structured and, to the best of our knowledge, completeoverview and comparison of perception datasets for anomaly detection inautonomous driving. Each chapter provides information about tasks and groundtruth, context information, and licenses. Additionally, we discuss currentweaknesses and gaps in existing datasets to underline the importance ofdeveloping further data.", "output": "Perception Datasets for Anomaly Detection in Autonomous Driving: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Geospatial technologies are becoming increasingly essential in our world fora wide range of applications, including agriculture, urban planning, anddisaster response. To help improve the applicability and performance of deeplearning models on these geospatial tasks, various works have beguninvestigating foundation models for this domain. Researchers have explored twoprominent approaches for introducing such models in geospatial applications,but both have drawbacks in terms of limited performance benefit or prohibitivetraining cost. Therefore, in this work, we propose a novel paradigm forbuilding highly effective geospatial foundation models with minimal resourcecost and carbon impact. We first construct a compact yet diverse dataset frommultiple sources to promote feature diversity, which we term GeoPile. Then, weinvestigate the potential of continual pretraining from large-scaleImageNet-22k models and propose a multi-objective continual pretrainingparadigm, which leverages the strong representations of ImageNet whilesimultaneously providing the freedom to learn valuable in-domain features. Ourapproach outperforms previous state-of-the-art geospatial pretraining methodsin an extensive evaluation on seven downstream datasets covering various taskssuch as change detection, classification, multi-label classification, semanticsegmentation, and super-resolution.", "output": "GFM: Building Geospatial Foundation Models via Continual Pretraining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The higher-order correlation clustering problem is an expressive model, andrecently, local search heuristics have been proposed for several applications.Certifying optimality, however, is NP-hard and practically hampered already bythe complexity of the problem statement. Here, we focus on establishing partialoptimality conditions for the special case of complete graphs and cubicobjective functions. In addition, we define and implement algorithms fortesting these conditions and examine their effect numerically, on two datasets.", "output": "Partial Optimality in Cubic Correlation Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Featured by a bottleneck structure, autoencoder (AE) and its variants havebeen largely applied in various medical image analysis tasks, such assegmentation, reconstruction and de-noising. Despite of their promisingperformances in aforementioned tasks, in this paper, we claim that AE modelsare not applicable to single image super-resolution (SISR) for 3D CT data. Ourhypothesis is that the bottleneck architecture that resizes feature maps in AEmodels degrades the details of input images, thus can sabotage the performanceof super-resolution. Although U-Net proposed skip connections that mergeinformation from different levels, we claim that the degrading impact offeature resizing operations could hardly be removed by skip connections. Byconducting large-scale ablation experiments and comparing the performancebetween models with and without the bottleneck design on a public CT lungdataset , we have discovered that AE models, including U-Net, have failed toachieve a compatible SISR result ($p&lt;0.05$ by Student's t-test) compared to thebaseline model. Our work is the first comparative study investigating thesuitability of AE architecture for 3D CT SISR tasks and brings a rationale forresearchers to re-think the choice of model architectures especially for 3D CTSISR tasks. The full implementation and trained models can be found at:", "output": "Is Autoencoder Truly Applicable for 3D CT Super-Resolution?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative adversarial networks (GANs) have achieved remarkable progress inthe natural image field. However, when applying GANs in the remote sensing (RS)image generation task, an extraordinary phenomenon is observed: the GAN modelis more sensitive to the size of training data for RS image generation than fornatural image generation. In other words, the generation quality of RS imageswill change significantly with the number of training categories or samples percategory. In this paper, we first analyze this phenomenon from two kinds of toyexperiments and conclude that the amount of feature information contained inthe GAN model decreases with reduced training data. Then we establish astructural causal model (SCM) of the data generation process and interpret thegenerated data as the counterfactuals. Based on this SCM, we theoreticallyprove that the quality of generated images is positively correlated with theamount of feature information. This provides insights for enriching the featureinformation learned by the GAN model during training. Consequently, we proposetwo innovative adjustment schemes, namely Uniformity Regularization (UR) andEntropy Regularization (ER), to increase the information learned by the GANmodel at the distributional and sample levels, respectively. We theoreticallyand empirically demonstrate the effectiveness and versatility of our methods.Extensive experiments on three RS datasets and two natural datasets show thatour methods outperform the well-established models on RS image generationtasks. The source code is available at ", "output": "Intriguing Property and Counterfactual Explanation of GAN for Remote Sensing Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, we have witnessed the great advancement of Deep neuralnetworks (DNNs) in image restoration. However, a critical limitation is thatthey cannot generalize well to real-world degradations with different degreesor types. In this paper, we are the first to propose a novel training strategyfor image restoration from the causality perspective, to improve thegeneralization ability of DNNs for unknown degradations. Our method, termedDistortion Invariant representation Learning (DIL), treats each distortion typeand degree as one specific confounder, and learns the distortion-invariantrepresentation by eliminating the harmful confounding effect of eachdegradation. We derive our DIL with the back-door criterion in causality bymodeling the interventions of different distortions from the optimizationperspective. Particularly, we introduce counterfactual distortion augmentationto simulate the virtual distortion types and degrees as the confounders. Then,we instantiate the intervention of each distortion with a virtual modelupdating based on corresponding distorted images, and eliminate them from themeta-learning perspective. Extensive experiments demonstrate the effectivenessof our DIL on the generalization capability for unseen distortion types anddegrees. Our code will be available at", "output": "Learning Distortion Invariant Representation for Image Restoration from A Causality Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In contrast to the natural capabilities of humans to learn new tasks in asequential fashion, neural networks are known to suffer from catastrophicforgetting, where the model's performances on old tasks drop dramatically afterbeing optimized for a new task. Since then, the continual learning (CL)community has proposed several solutions aiming to equip the neural networkwith the ability to learn the current task (plasticity) while still achievinghigh accuracy on the previous tasks (stability). Despite remarkableimprovements, the plasticity-stability trade-off is still far from being solvedand its underlying mechanism is poorly understood. In this work, we proposeAuxiliary Network Continual Learning (ANCL), a novel method that applies anadditional auxiliary network which promotes plasticity to the continuallylearned model which mainly focuses on stability. More concretely, the proposedframework materializes in a regularizer that naturally interpolates betweenplasticity and stability, surpassing strong baselines on task incremental andclass incremental scenarios. Through extensive analyses on ANCL solutions, weidentify some essential principles beneath the stability-plasticity trade-off.", "output": "Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available at and dataset will be releasedupon paper acceptance.", "output": "Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization (DG) aims to learn a model that generalizes well tounseen target domains utilizing multiple source domains without re-training.Most existing DG works are based on convolutional neural networks (CNNs).However, the local operation of the convolution kernel makes the model focustoo much on local representations (e.g., texture), which inherently causes themodel more prone to overfit to the source domains and hampers itsgeneralization ability. Recently, several MLP-based methods have achievedpromising results in supervised learning tasks by learning global interactionsamong different patches of the image. Inspired by this, in this paper, we firstanalyze the difference between CNN and MLP methods in DG and find that MLPmethods exhibit a better generalization ability because they can better capturethe global representations (e.g., structure) than CNN methods. Then, based on arecent lightweight MLP method, we obtain a strong baseline that outperformsmost state-of-the-art CNN-based methods. The baseline can learn globalstructure representations with a filter to suppress structure irrelevantinformation in the frequency space. Moreover, we propose a dynAmicLOw-Frequency spectrum Transform (ALOFT) that can perturb local texturefeatures while preserving global structure features, thus enabling the filterto remove structure-irrelevant information sufficiently. Extensive experimentson four benchmarks have demonstrated that our method can achieve greatperformance improvement with a small number of parameters compared to SOTACNN-based DG methods. Our code is available at", "output": "ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robust real-time perception of 3D world is essential to the autonomousvehicle. We introduce an end-to-end surround camera perception system forself-driving. Our perception system is a novel multi-task, multi-camera networkwhich takes a variable set of time-synced camera images as input and produces arich collection of 3D signals such as sizes, orientations, locations ofobstacles, parking spaces and free-spaces, etc. Our perception network ismodular and end-to-end: 1) the outputs can be consumed directly by downstreammodules without any post-processing such as clustering and fusion -- improvingspeed of model deployment and in-car testing 2) the whole network training isdone in one single stage -- improving speed of model improvement anditerations. The network is well designed to have high accuracy while running at53 fps on NVIDIA Orin SoC (system-on-a-chip). The network is robust to sensormounting variations (within some tolerances) and can be quickly customized fordifferent vehicle types via efficient model fine-tuning thanks of itscapability of taking calibration parameters as additional inputs duringtraining and testing. Most importantly, our network has been successfullydeployed and being tested on real roads.", "output": "NVAutoNet: Fast and Accurate 360$^{\\circ}$ 3D Visual Perception For Self Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel technique to estimate the 6D pose of objects from singleimages where the 3D geometry of the object is only given approximately and notas a precise 3D model. To achieve this, we employ a dense 2D-to-3Dcorrespondence predictor that regresses 3D model coordinates for every pixel.In addition to the 3D coordinates, our model also estimates the pixel-wisecoordinate error to discard correspondences that are likely wrong. This allowsus to generate multiple 6D pose hypotheses of the object, which we then refineiteratively using a highly efficient region-based approach. We also introduce anovel pixel-wise posterior formulation by which we can estimate the probabilityfor each hypothesis and select the most likely one. As we show in experiments,our approach is capable of dealing with extreme visual conditions includingoverexposure, high contrast, or low signal-to-noise ratio. This makes it apowerful technique for the particularly challenging task of estimating the poseof tumbling satellites for in-orbit robotic applications. Our method achievesstate-of-the-art performance on the SPEED+ dataset and has won the SPEC2021post-mortem competition.", "output": "6D Object Pose Estimation from Approximate 3D Models for Orbital Robotics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated shape repair approaches currently lack access to datasets thatdescribe real-world damaged geometry. We present Fantastic Breaks (and Where toFind Them: adataset containing scanned, waterproofed, and cleaned 3D meshes for 150 brokenobjects, paired and geometrically aligned with complete counterparts. FantasticBreaks contains class and material labels, proxy repair parts that join tobroken meshes to generate complete meshes, and manually annotated fractureboundaries. Through a detailed analysis of fracture geometry, we revealdifferences between Fantastic Breaks and synthetic fracture datasets generatedusing geometric and physics-based methods. We show experimental shape repairevaluation with Fantastic Breaks using multiple learning-based approachespre-trained with synthetic datasets and re-trained with subset of FantasticBreaks.", "output": "Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training machines to synthesize diverse handwritings is an intriguing task.Recently, RNN-based methods have been proposed to generate stylized onlineChinese characters. However, these methods mainly focus on capturing a person'soverall writing style, neglecting subtle style inconsistencies betweencharacters written by the same person. For example, while a person'shandwriting typically exhibits general uniformity (e.g., glyph slant and aspectratios), there are still small style variations in finer details (e.g., strokelength and curvature) of characters. In light of this, we propose todisentangle the style representations at both writer and character levels fromindividual handwritings to synthesize realistic stylized online handwrittencharacters. Specifically, we present the style-disentangled Transformer (SDT),which employs two complementary contrastive objectives to extract the stylecommonalities of reference samples and capture the detailed style patterns ofeach sample, respectively. Extensive experiments on various language scriptsdemonstrate the effectiveness of SDT. Notably, our empirical findings revealthat the two learned style representations provide information at differentfrequency magnitudes, underscoring the importance of separate style extraction.Our source code is public at: ", "output": "Disentangling Writer and Character Styles for Handwriting Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated chromosome instance segmentation from metaphase cell microscopicimages is critical for the diagnosis of chromosomal disorders (i.e., karyotypeanalysis). However, it is still a challenging task due to lacking of denselyannotated datasets and the complicated morphologies of chromosomes, e.g., densedistribution, arbitrary orientations, and wide range of lengths. To facilitatethe development of this area, we take a big step forward and manually constructa large-scale densely annotated dataset named AutoKary2022, which contains over27,000 chromosome instances in 612 microscopic images from 50 patients.Specifically, each instance is annotated with a polygonal mask and a classlabel to assist in precise chromosome detection and segmentation. On top of it,we systematically investigate representative methods on this dataset and obtaina number of interesting findings, which helps us have a deeper understanding ofthe fundamental problems in chromosome instance segmentation. We hope thisdataset could advance research towards medical understanding. The dataset canbe available at:", "output": "AutoKary2022: A Large-Scale Densely Annotated Dateset for Chromosome Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, the authors develop regression approaches based on deeplearning to perform thread density estimation for plain weave canvas analysis.Previous approaches were based on Fourier analysis, which is quite robust forsome scenarios but fails in some others, in machine learning tools, thatinvolve pre-labeling of the painting at hand, or the segmentation of threadcrossing points, that provides good estimations in all scenarios with no needof pre-labeling. The segmentation approach is time-consuming as the estimationof the densities is performed after locating the crossing points. In this novelproposal, we avoid this step by computing the density of threads directly fromthe image with a regression deep learning model. We also incorporate someimprovements in the initial preprocessing of the input image with an impact onthe final error. Several models are proposed and analyzed to retain the bestone. Furthermore, we further reduce the density estimation error by introducinga semi-supervised approach. The performance of our novel algorithm is analyzedwith works by Ribera, Vel'azquez, and Poussin where we compare our results tothe ones of previous approaches. Finally, the method is put into practice tosupport the change of authorship or a masterpiece at the Museo del Prado.", "output": "Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the challenges in federated learning is the non-independent andidentically distributed (non-iid) characteristics between heterogeneousdevices, which cause significant differences in local updates and affect theperformance of the central server. Although many studies have been proposed toaddress this challenge, they only focus on local training and aggregationprocesses to smooth the changes and fail to achieve high performance with deeplearning models. Inspired by the phenomenon of neural collapse, we force eachclient to be optimized toward an optimal global structure for classification.Specifically, we initialize it as a random simplex Equiangular Tight Frame(ETF) and fix it as the unit optimization target of all clients during thelocal updating. After guaranteeing all clients are learning to converge to theglobal optimum, we propose to add a global memory vector for each category toremedy the parameter fluctuation caused by the bias of the intra-classcondition distribution among clients. Our experimental results show that ourmethod can improve the performance with faster convergence speed ondifferent-size datasets.", "output": "Neural Collapse Inspired Federated Learning with Non-iid Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ground-glass opacity is a hallmark of numerous lung diseases, includingpatients with COVID19 and pneumonia, pulmonary fibrosis, and tuberculosis. Thisbrief note presents experimental results of a proof-of-concept framework thatgot implemented and tested over three days as driven by the third challengeentitled \"COVID-19 Competition\", hosted at the AI-Enabled Medical ImageAnalysis Workshop of the 2023 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP 2023). Using a newly built virtualenvironment (created on March 17, 2023), we investigated various pre-trainedtwo-dimensional convolutional neural networks (CNN) such as Dense NeuralNetwork, Residual Neural Networks (ResNet), and Vision Transformers, as well asthe extent of fine-tuning. Based on empirical experiments, we opted tofine-tune them using ADAM's optimization algorithm with a standard learningrate of 0.001 for all CNN architectures and apply early-stopping whenever thevalidation loss reached a plateau. For each trained CNN, the model state withthe best validation accuracy achieved during training was stored and laterreloaded for new classifications of unseen samples drawn from the validationset provided by the challenge organizers. According to the organizers, few ofthese 2D CNNs yielded performance comparable to an architecture that combinedResNet and Recurrent Neural Network (Gated Recurrent Units). As part of thechallenge requirement, the source code produced during the course of thisexercise is posted at  We also hope thatother researchers may find this light prototype consisting of few Python filesbased on PyTorch 1.13.1 and TorchVision 0.14.1 approachable.", "output": "Severity classification of ground-glass opacity via 2-D convolutional neural network and lung CT scans: a 3-day exploration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online camera-to-ground calibration is to generate a non-rigid bodytransformation between the camera and the road surface in a real-time manner.Existing solutions utilize static calibration, suffering from environmentalvariations such as tire pressure changes, vehicle loading volume variations,and road surface diversity. Other online solutions exploit the usage of roadelements or photometric consistency between overlapping views across images,which require continuous detection of specific targets on the road orassistance with multiple cameras to facilitate calibration. In our work, wepropose an online monocular camera-to-ground calibration solution that does notutilize any specific targets while driving. We perform a coarse-to-fineapproach for ground feature extraction through wheel odometry and estimate thecamera-to-ground calibration parameters through a sliding-window-based factorgraph optimization. Considering the non-rigid transformation ofcamera-to-ground while driving, we provide metrics to quantify calibrationperformance and stopping criteria to report/broadcast our satisfyingcalibration results. Extensive experiments using real-world data demonstratethat our algorithm is effective and outperforms state-of-the-art techniques.", "output": "Online Camera-to-ground Calibration for Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The robustness of 3D perception systems under natural corruptions fromenvironments and sensors is pivotal for safety-critical applications. Existinglarge-scale 3D perception datasets often contain data that are meticulouslycleaned. Such configurations, however, cannot reflect the reliability ofperception models during the deployment stage. In this work, we present Robo3D,the first comprehensive benchmark heading toward probing the robustness of 3Ddetectors and segmentors under out-of-distribution scenarios against naturalcorruptions that occur in real-world environments. Specifically, we considereight corruption types stemming from adversarial weather conditions, externaldisturbances, and internal sensor failure. We uncover that, although promisingresults have been progressively achieved on standard benchmarks,state-of-the-art 3D perception models are at risk of being vulnerable tocorruptions. We draw key observations on the use of data representations,augmentation schemes, and training strategies, that could severely affect themodel's performance. To pursue better robustness, we propose adensity-insensitive training framework along with a simple flexiblevoxelization strategy to enhance the model resiliency. We hope our benchmarkand approach could inspire future research in designing more robust andreliable 3D perception models. Our robustness benchmark suite is publiclyavailable.", "output": "Robo3D: Towards Robust and Reliable 3D Perception against Corruptions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A microscopic particle obeys the principles of quantum mechanics -- so whereis the sharp boundary between the macroscopic and microscopic worlds? It wasthis \"interpretation problem\" that prompted Schr\"odinger to propose his famousthought experiment (a cat that is simultaneously both dead and alive) andsparked a great debate about the quantum measurement problem, and there isstill no satisfactory answer yet. This is precisely the inadequacy of rigorousmathematical models in describing the laws of nature. We propose acomputational model to describe and understand the laws of nature based onDarwin's natural selection. In fact, whether it's a macro particle, a microelectron or a security, they can all be considered as an entity, the change ofthis entity over time can be described by a data series composed of states andvalues. An observer can learn from this data series to construct theories(usually consisting of functions and differential equations). We don't modelwith the usual functions or differential equations, but with a state DecisionTree (determines the state of an entity) and a value Function Tree (determinesthe distance between two points of an entity). A state Decision Tree and avalue Function Tree together can reconstruct an entity's trajectory and makepredictions about its future trajectory. Our proposed algorithmic modeldiscovers laws of nature by only learning observed historical data (sequentialmeasurement of observables) based on maximizing the observer's expected value.There is no differential equation in our model; our model has an emphasis onmachine learning, where the observer builds up his/her experience by beingrewarded or punished for each decision he/she makes, and eventually leads torediscovering Newton's law, the Born rule (quantum mechanics) and the efficientmarket hypothesis (financial market).", "output": "Machine learning for discovering laws of nature."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ensemble forecast post-processing is a necessary step in producing accurateprobabilistic forecasts. Conventional post-processing methods operate byestimating the parameters of a parametric distribution, frequently on aper-location or per-lead-time basis. We propose a novel, neural network-basedmethod, which produces forecasts for all locations and lead times, jointly. Torelax the distributional assumption of many post-processing methods, ourapproach incorporates normalizing flows as flexible parametric distributionestimators. This enables us to model varying forecast distributions in amathematically exact way. We demonstrate the effectiveness of our method in thecontext of the EUPPBench benchmark, where we conduct temperature forecastpost-processing for stations in a sub-region of western Europe. We show thatour novel method exhibits state-of-the-art performance on the benchmark,outclassing our previous, well-performing entry. Additionally, by providing adetailed comparison of three variants of our novel post-processing method, weelucidate the reasons why our method outperforms per-lead-time-based approachesand approaches with distributional assumptions.", "output": "Ensemble weather forecast post-processing with a flexible probabilistic neural network approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce the range of oBERTa language models, aneasy-to-use set of language models, which allows Natural Language Processing(NLP) practitioners to obtain between 3.8 and 24.3 times faster models withoutexpertise in model compression. Specifically, oBERTa extends existing work onpruning, knowledge distillation, and quantization and leverages frozenembeddings to improve knowledge distillation, and improved model initializationto deliver higher accuracy on a a broad range of transfer tasks. In generatingoBERTa, we explore how the highly optimized RoBERTa differs from the BERT withrespect to pruning during pre-training and fine-tuning and find it lessamenable to compression during fine-tuning. We explore the use of oBERTa on abroad seven representative NLP tasks and find that the improved compressiontechniques allow a pruned oBERTa model to match the performance of BERTBASE andexceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answeringdataset, despite being 8x and 2x, respectively, faster in inference. We releaseour code, training regimes, and associated model for broad usage to encourageusage and experimentation.", "output": "oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning-based approaches for generating novel drug molecules withspecific properties have gained a lot of interest in the last years. Recentstudies have demonstrated promising performance for string-based generation ofnovel molecules utilizing reinforcement learning. In this paper, we develop aunified framework for using reinforcement learning for de novo drug design,wherein we systematically study various on- and off-policy reinforcementlearning algorithms and replay buffers to learn an RNN-based policy to generatenovel molecules predicted to be active against the dopamine receptor DRD2. Ourfindings suggest that it is advantageous to use at least both top-scoring andlow-scoring molecules for updating the policy when structural diversity isessential. Using all generated molecules at an iteration seems to enhanceperformance stability for on-policy algorithms. In addition, when replayinghigh, intermediate, and low-scoring molecules, off-policy algorithms displaythe potential of improving the structural diversity and number of activemolecules generated, but possibly at the cost of a longer exploration phase.Our work provides an open-source framework enabling researchers to investigatevarious reinforcement learning methods for de novo drug design.", "output": "Utilizing Reinforcement Learning for de novo Drug Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "People with diabetes have to manage their blood glucose level to keep itwithin an appropriate range. Predicting whether future glucose values will beoutside the healthy threshold is of vital importance in order to takecorrective actions to avoid potential health damage. In this paper we describeour research with the aim of predicting the future behavior of blood glucoselevels, so that hypoglycemic events may be anticipated. The approach of thiswork is the application of transformation functions on glucose time series, andtheir use in convolutional neural networks. We have tested our proposed methodusing real data from 4 different diabetes patients with promising results.", "output": "Patterns Detection in Glucose Time Series by Domain Transformations and Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, the increased urbanization and industrialization has led toa rising water demand and resources, thus increasing the gap between demand andsupply. Proper water distribution and forecasting of water consumption are keyfactors in mitigating the imbalance of supply and demand by improvingoperations, planning and management of water resources. To this end, in thispaper, several well-known forecasting algorithms are evaluated over timeseries, water consumption data from Greece, a country with diversesocio-economic and urbanization issues. The forecasting algorithms areevaluated on a real-world dataset provided by the Water Supply and SewerageCompany of Greece revealing key insights about each algorithm and its use.", "output": "An evaluation of time series forecasting models on water consumption data: A case study of Greece."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce an adaptive refinement procedure for smart, and scalableabstraction of dynamical systems. Our technique relies on partitioning thestate space depending on the observation of future outputs. However, thisknowledge is dynamically constructed in an adaptive, asymmetric way. In orderto learn the optimal structure, we define a Kantorovich-inspired metric betweenMarkov chains, and we use it as a loss function. Our technique is prone todata-driven frameworks, but not restricted to.We also study properties of the above mentioned metric between Markov chains,which we believe could be of application for wider purpose. We propose analgorithm to approximate it, and we show that our method yields a much bettercomputational complexity than using classical linear programming techniques.", "output": "Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version]."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many organizations measure treatment effects via an experimentation platformto evaluate the casual effect of product variations prior to full-scaledeployment. However, standard experimentation platforms do not performoptimally for end user populations that exhibit heterogeneous treatment effects(HTEs). Here we present a personalized experimentation framework, PersonalizedExperiments (PEX), which optimizes treatment group assignment at the user levelvia HTE modeling and sequential decision policy optimization to optimizemultiple short-term and long-term outcomes simultaneously. We describe anend-to-end workflow that has proven to be successful in practice and can bereadily implemented using open-source software.", "output": "Practical Policy Optimization with Personalized Experimentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a methodology to align a medium-sized GPT model,originally trained in English for an open domain, to a small closed domain inSpanish. The application for which the model is finely tuned is the questionanswering task. To achieve this we also needed to train and implement anotherneural network (which we called the reward model) that could score anddetermine whether an answer is appropriate for a given question. This componentserved to improve the decoding and generation of the answers of the system.Numerical metrics such as BLEU and perplexity were used to evaluate the model,and human judgment was also used to compare the decoding technique with others.Finally, the results favored the proposed method, and it was determined that itis feasible to use a reward model to align the generation of responses.", "output": "Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Like people, LLMs do not always generate the best text for a given generationproblem on their first try (e.g., summaries, answers, explanations). Just aspeople then refine their text, we introduce SELF-REFINE, a framework forsimilarly improving initial outputs from LLMs through iterative feedback andrefinement. The main idea is to generate an output using an LLM, then allow thesame model to provide multi-aspect feedback for its own output; finally, thesame model refines its previously generated output given its own feedback.Unlike earlier work, our iterative refinement framework does not requiresupervised training data or reinforcement learning, and works with a singleLLM. We experiment with 7 diverse tasks, ranging from review rewriting to mathreasoning, demonstrating that our approach outperforms direct generation. Inall tasks, outputs generated with SELF-REFINE are preferred by humans and byautomated metrics over those generated directly with GPT-3.5 and GPT-4,improving on average by absolute 20% across tasks.", "output": "Self-Refine: Iterative Refinement with Self-Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The particle-flow (PF) algorithm, which infers particles based on tracks andcalorimeter clusters, is of central importance to event reconstruction in theCMS experiment at the CERN LHC, and has been a focus of development in light ofplanned Phase-2 running conditions with an increased pileup and detectorgranularity. In recent years, the machine learned particle-flow (MLPF)algorithm, a graph neural network that performs PF reconstruction, has beenexplored in CMS, with the possible advantages of directly optimizing for thephysical quantities of interest, being highly reconfigurable to new conditions,and being a natural fit for deployment to heterogeneous accelerators. Wediscuss progress in CMS towards an improved implementation of the MLPFreconstruction, now optimized using generator/simulation-level particleinformation as the target for the first time. This paves the way to potentiallyimproving the detector response in terms of physical quantities of interest. Wedescribe the simulation-based training target, progress and studies onevent-based loss terms, details on the model hyperparameter tuning, as well asphysics validation with respect to the current PF algorithm in terms ofhigh-level physical quantities such as the jet and missing transverse momentumresolutions. We find that the MLPF algorithm, trained on a generator/simulatorlevel particle information for the first time, results in broadly compatibleparticle and jet reconstruction performance with the baseline PF, setting thestage for improving the physics performance by additional training statisticsand model tuning.", "output": "Progress towards an improved particle flow algorithm at CMS with machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Metadata quality is crucial for digital objects to be discovered throughdigital library interfaces. However, due to various reasons, the metadata ofdigital objects often exhibits incomplete, inconsistent, and incorrect values.We investigate methods to automatically detect, correct, and canonicalizescholarly metadata, using seven key fields of electronic theses anddissertations (ETDs) as a case study. We propose MetaEnhance, a framework thatutilizes state-of-the-art artificial intelligence methods to improve thequality of these fields. To evaluate MetaEnhance, we compiled a metadataquality evaluation benchmark containing 500 ETDs, by combining subsets sampledusing multiple criteria. We tested MetaEnhance on this benchmark and found thatthe proposed methods achieved nearly perfect F1-scores in detecting errors andF1-scores in correcting errors ranging from 0.85 to 1.00 for five of sevenfields.", "output": "MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Motivated by the paradigm of reservoir computing, we consider randomlyinitialized controlled ResNets defined as Euler-discretizations of neuralcontrolled differential equations (Neural CDEs). We show that in theinfinite-width-then-depth limit and under proper scaling, these architecturesconverge weakly to Gaussian processes indexed on some spaces of continuouspaths and with kernels satisfying certain partial differential equations (PDEs)varying according to the choice of activation function. In the special casewhere the activation is the identity, we show that the equation reduces to alinear PDE and the limiting kernel agrees with the signature kernel of Salvi etal. (2021). In this setting, we also show that the width-depth limits commute.We name this new family of limiting kernels neural signature kernels. Finally,we show that in the infinite-depth regime, finite-width controlled ResNetsconverge in distribution to Neural CDEs with random vector fields which,depending on whether the weights are shared across layers, are eithertime-independent and Gaussian or behave like a matrix-valued Brownian motion.", "output": "Neural signature kernels as infinite-width-depth-limits of controlled ResNets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the convex hulls of reachable sets of nonlinear systems with boundeddisturbances. Reachable sets play a critical role in control, but remainnotoriously challenging to compute, and existing over-approximation tools tendto be conservative or computationally expensive. In this work, we exactlycharacterize the convex hulls of reachable sets as the convex hulls ofsolutions of an ordinary differential equation from all possible initial valuesof the disturbances. This finite-dimensional characterization unlocks a tightestimation algorithm to over-approximate reachable sets that is significantlyfaster and more accurate than existing methods. We present applications toneural feedback loop analysis and robust model predictive control.", "output": "Exact Characterization of the Convex Hulls of Reachable Sets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing conversational models are handled by a database(DB) and API basedsystems. However, very often users' questions require information that cannotbe handled by such systems. Nonetheless, answers to these questions areavailable in the form of customer reviews and FAQs. DSTC-11 proposes a threestage pipeline consisting of knowledge seeking turn detection, knowledgeselection and response generation to create a conversational model grounded onthis subjective knowledge. In this paper, we focus on improving the knowledgeselection module to enhance the overall system performance. In particular, wepropose entity retrieval methods which result in an accurate and fasterknowledge search. Our proposed Named Entity Recognition (NER) based entityretrieval method results in 7X faster search compared to the baseline model.Additionally, we also explore a potential keyword extraction method which canimprove the accuracy of knowledge selection. Preliminary results show a 4 %improvement in exact match score on knowledge selection task. The code isavailable ", "output": "Task Oriented Conversational Modelling With Subjective Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Dual Cross-Attention (DCA), a simple yet effective attentionmodule that is able to enhance skip-connections in U-Net-based architecturesfor medical image segmentation. DCA addresses the semantic gap between encoderand decoder features by sequentially capturing channel and spatial dependenciesacross multi-scale encoder features. First, the Channel Cross-Attention (CCA)extracts global channel-wise dependencies by utilizing cross-attention acrosschannel tokens of multi-scale encoder features. Then, the SpatialCross-Attention (SCA) module performs cross-attention to capture spatialdependencies across spatial tokens. Finally, these fine-grained encoderfeatures are up-sampled and connected to their corresponding decoder parts toform the skip-connection scheme. Our proposed DCA module can be integrated intoany encoder-decoder architecture with skip-connections such as U-Net and itsvariants. We test our DCA module by integrating it into six U-Net-basedarchitectures such as U-Net, V-Net, R2Unet, ResUnet++, DoubleUnet andMultiResUnet. Our DCA module shows Dice Score improvements up to 2.05% on GlaS,2.74% on MoNuSeg, 1.37% on CVC-ClinicDB, 1.12% on Kvasir-Seg and 1.44% onSynapse datasets. Our codes are available at:", "output": "Dual Cross-Attention for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Software engineers develop, fine-tune, and deploy deep learning (DL) models.They use and re-use models in a variety of development frameworks and deploythem on a range of runtime environments. In this diverse ecosystem, engineersuse DL model converters to move models from frameworks to runtime environments.However, errors in converters can compromise model quality and disruptdeployment. The failure frequency and failure modes of DL model converters areunknown.In this paper, we conduct the first failure analysis on DL model converters.Specifically, we characterize failures in model converters associated with ONNX(Open Neural Network eXchange). We analyze past failures in the ONNX convertersin two major DL frameworks, PyTorch and TensorFlow. The symptoms, causes, andlocations of failures (for N=200 issues), and trends over time are alsoreported. We also evaluate present-day failures by converting 8,797 models,both real-world and synthetically generated instances. The consistent resultfrom both parts of the study is that DL model converters commonly fail byproducing models that exhibit incorrect behavior: 33% of past failures and 8%of converted models fell into this category. Our results motivate futureresearch on making DL software simpler to maintain, extend, and validate.", "output": "Analysis of Failures and Risks in Deep Learning Model Converters: A Case Study in the ONNX Ecosystem."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weak supervision overcomes the label bottleneck, enabling efficientdevelopment of training sets. Millions of models trained on such datasets havebeen deployed in the real world and interact with users on a daily basis.However, the techniques that make weak supervision attractive -- such asintegrating any source of signal to estimate unknown labels -- also ensure thatthe pseudolabels it produces are highly biased. Surprisingly, given everydayuse and the potential for increased bias, weak supervision has not been studiedfrom the point of view of fairness. This work begins such a study. Ourdeparture point is the observation that even when a fair model can be builtfrom a dataset with access to ground-truth labels, the corresponding datasetlabeled via weak supervision can be arbitrarily unfair. Fortunately, not all islost: we propose and empirically validate a model for source unfairness in weaksupervision, then introduce a simple counterfactual fairness-based techniquethat can mitigate these biases. Theoretically, we show that it is possible forour approach to simultaneously improve both accuracy and fairness metrics -- incontrast to standard fairness approaches that suffer from tradeoffs.Empirically, we show that our technique improves accuracy on weak supervisionbaselines by as much as 32% while reducing demographic parity gap by 82.5%.", "output": "Mitigating Source Bias for Fairer Weak Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of online multiclass learning when the number oflabels is unbounded. We show that the Multiclass Littlestone dimension, firstintroduced in cite{DanielyERMprinciple}, continues to characterize onlinelearnability in this setting. Our result complements the recent work bycite{Brukhimetal2022} who give a characterization of batch multiclasslearnability when the label space is unbounded.", "output": "A Characterization of Online Multiclass Learnability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "International benchmarking competitions have become fundamental for thecomparative performance assessment of image analysis methods. However, littleattention has been given to investigating what can be learnt from thesecompetitions. Do they really generate scientific progress? What are common andsuccessful participation strategies? What makes a solution superior to acompeting method? To address this gap in the literature, we performed amulti-center study with all 80 competitions that were conducted in the scope ofIEEE ISBI 2021 and MICCAI 2021. Statistical analyses performed based oncomprehensive descriptions of the submitted algorithms linked to their rank aswell as the underlying participation strategies revealed common characteristicsof winning solutions. These typically include the use of multi-task learning(63%) and/or multi-stage pipelines (61%), and a focus on augmentation (100%),image preprocessing (97%), data curation (79%), and postprocessing (66%). The\"typical\" lead of a winning team is a computer scientist with a doctoraldegree, five years of experience in biomedical image analysis, and four yearsof experience in deep learning. Two core general development strategies stoodout for highly-ranked teams: the reflection of the metrics in the method designand the focus on analyzing and handling failure cases. According to theorganizers, 43% of the winning algorithms exceeded the state of the art butonly 11% completely solved the respective domain problem. The insights of ourstudy could help researchers (1) improve algorithm development strategies whenapproaching new problems, and (2) focus on open research questions revealed bythis work.", "output": "Why is the winner the best?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks have been proven to be both highly effective within computervision, and highly vulnerable to adversarial attacks. Consequently, as the useof neural networks increases due to their unrivaled performance, so too doesthe threat posed by adversarial attacks. In this work, we build towardsaddressing the challenge of adversarial robustness by exploring therelationship between the mini-batch size used during adversarial samplegeneration and the strength of the adversarial samples produced. We demonstratethat an increase in mini-batch size results in a decrease in the efficacy ofthe samples produced, and we draw connections between these observations andthe phenomenon of vanishing gradients. Next, we formulate loss functions suchthat adversarial sample strength is not degraded by mini-batch size. Ourfindings highlight a potential risk for underestimating the true (practical)strength of adversarial attacks, and a risk of overestimating a model'srobustness. We share our codes to let others replicate our experiments and tofacilitate further exploration of the connections between batch size andadversarial sample strength.", "output": "Generating Adversarial Samples in Mini-Batches May Be Detrimental To Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient large-scale neural network training and inference on commodity CPUhardware is of immense practical significance in democratizing deep learning(DL) capabilities. Presently, the process of training massive models consistingof hundreds of millions to billions of parameters requires the extensive use ofspecialized hardware accelerators, such as GPUs, which are only accessible to alimited number of institutions with considerable financial resources. Moreover,there is often an alarming carbon footprint associated with training anddeploying these models. In this paper, we address these challenges byintroducing BOLT, a sparse deep learning library for training massive neuralnetwork models on standard CPU hardware. BOLT provides a flexible, high-levelAPI for constructing models that will be familiar to users of existing popularDL frameworks. By automatically tuning specialized hyperparameters, BOLT alsoabstracts away the algorithmic details of sparse network training. We evaluateBOLT on a number of machine learning tasks drawn from recommendations, search,natural language processing, and personalization. We find that our proposedsystem achieves competitive performance with state-of-the-art techniques at afraction of the cost and energy consumption and an order-of-magnitude fasterinference time. BOLT has also been successfully deployed by multiple businessesto address critical problems, and we highlight one customer deployment casestudy in the field of e-commerce.", "output": "BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Item response theory aims to estimate respondent's latent skills from theirresponses in tests composed of items with different levels of difficulty.Several models of item response theory have been proposed for different typesof tasks, such as binary or probabilistic responses, response time, multipleresponses, among others. In this paper, we propose a new version of$beta^3$-IRT, called $beta^{4}$-IRT, which uses the gradient descent methodto estimate the model parameters. In $beta^3$-IRT, abilities and difficultiesare bounded, thus we employ link functions in order to turn $beta^{4}$-IRTinto an unconstrained gradient descent process. The original $beta^3$-IRT hada symmetry problem, meaning that, if an item was initialised with adiscrimination value with the wrong sign, e.g. negative when the actualdiscrimination should be positive, the fitting process could be unable torecover the correct discrimination and difficulty values for the item. In orderto tackle this limitation, we modelled the discrimination parameter as theproduct of two new parameters, one corresponding to the sign and the secondassociated to the magnitude. We also proposed sensible priors for allparameters. We performed experiments to compare $beta^{4}$-IRT and$beta^3$-IRT regarding parameter recovery and our new version outperformed theoriginal $beta^3$-IRT. Finally, we made $beta^{4}$-IRT publicly available asa Python package, along with the implementation of $beta^3$-IRT used in ourexperiments.", "output": "$\\beta^{4}$-IRT: A New $\\beta^{3}$-IRT with Enhanced Discrimination Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Linear transformation of the inputs alters the training performance offeed-forward networks that are otherwise equivalent. However, most lineartransforms are viewed as a pre-processing operation separate from the actualtraining. Starting from equivalent networks, it is shown that pre-processinginputs using linear transformation are equivalent to multiplying the negativegradient matrix with an autocorrelation matrix per training iteration. Secondorder method is proposed to find the autocorrelation matrix that maximizeslearning in a given iteration. When the autocorrelation matrix is diagonal, themethod optimizes input gains. This optimal input gain (OIG) approach is used toimprove two first-order two-stage training algorithms, namely back-propagation(BP) and hidden weight optimization (HWO), which alternately update the inputweights and solve linear equations for output weights. Results show that theproposed OIG approach greatly enhances the performance of the first-orderalgorithms, often allowing them to rival the popular Levenberg-Marquardtapproach with far less computation. It is shown that HWO is equivalent to BPwith Whitening transformation applied to the inputs. HWO effectively combinesWhitening transformation with learning. Thus, OIG improved HWO could be asignificant building block to more complex deep learning architectures.", "output": "Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There have been tremendous efforts over the past decades dedicated to thegeneration of realistic graphs in a variety of domains, ranging from socialnetworks to computer networks, from gene regulatory networks to onlinetransaction networks. Despite the remarkable success, the vast majority ofthese works are unsupervised in nature and are typically trained to minimizethe expected graph reconstruction loss, which would result in therepresentation disparity issue in the generated graphs, i.e., the protectedgroups (often minorities) contribute less to the objective and thus suffer fromsystematically higher errors. In this paper, we aim to tailor graph generationto downstream mining tasks by leveraging label information and user-preferredparity constraint. In particular, we start from the investigation ofrepresentation disparity in the context of graph generative models. To mitigatethe disparity, we propose a fairness-aware graph generative model namedFairGen. Our model jointly trains a label-informed graph generation module anda fair representation learning module by progressively learning the behaviorsof the protected and unprotected groups, from the `easy' concepts to the `hard'ones. In addition, we propose a generic context sampling strategy for graphgenerative models, which is proven to be capable of fairly capturing thecontextual information of each group with a high probability. Experimentalresults on seven real-world data sets, including web-based graphs, demonstratethat FairGen (1) obtains performance on par with state-of-the-art graphgenerative models across six network properties, (2) mitigates therepresentation disparity issues in the generated graphs, and (3) substantiallyboosts the model performance by up to 17% in downstream tasks via dataaugmentation.", "output": "FairGen: Towards Fair Graph Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the nonlinear regression problem under L2 loss (square loss)functions. Traditional nonlinear regression models often result in non-convexoptimization problems with respect to the parameter set. We show that a convexnonlinear regression model exists for the traditional least squares problem,which can be a promising towards designing more complex systems with easier totrain models.", "output": "A Note On Nonlinear Regression Under L2 Loss."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapid advancement of conversational and chat-based language models hasled to remarkable progress in complex task-solving. However, their successheavily relies on human input to guide the conversation, which can bechallenging and time-consuming. This paper explores the potential of buildingscalable techniques to facilitate autonomous cooperation among communicativeagents and provide insight into their \"cognitive\" processes. To address thechallenges of achieving autonomous cooperation, we propose a novelcommunicative agent framework named role-playing. Our approach involves usinginception prompting to guide chat agents toward task completion whilemaintaining consistency with human intentions. We showcase how role-playing canbe used to generate conversational data for studying the behaviors andcapabilities of chat agents, providing a valuable resource for investigatingconversational language models. Our contributions include introducing a novelcommunicative agent framework, offering a scalable approach for studying thecooperative behaviors and capabilities of multi-agent systems, andopen-sourcing our library to support research on communicative agents andbeyond. The GitHub repository of this project is made publicly available on:", "output": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The information bottleneck (IB) method offers an attractive framework forunderstanding representation learning, however its applications are oftenlimited by its computational intractability. Analytical characterization of theIB method is not only of practical interest, but it can also lead to newinsights into learning phenomena. Here we consider a generalized IB problem, inwhich the mutual information in the original IB method is replaced bycorrelation measures based on Renyi and Jeffreys divergences. We derive anexact analytical IB solution for the case of Gaussian correlated variables. Ouranalysis reveals a series of structural transitions, similar to thosepreviously observed in the original IB case. We find further that althoughsolving the original, Renyi and Jeffreys IB problems yields differentrepresentations in general, the structural transitions occur at the samecritical tradeoff parameters, and the Renyi and Jeffreys IB solutions performwell under the original IB objective. Our results suggest that formulating theIB method with alternative correlation measures could offer a strategy forobtaining an approximate solution to the original IB problem.", "output": "Generalized Information Bottleneck for Gaussian Variables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies show that models trained by continual learning can achieve thecomparable performances as the standard supervised learning and the learningflexibility of continual learning models enables their wide applications in thereal world. Deep learning models, however, are shown to be vulnerable toadversarial attacks. Though there are many studies on the model robustness inthe context of standard supervised learning, protecting continual learning fromadversarial attacks has not yet been investigated. To fill in this researchgap, we are the first to study adversarial robustness in continual learning andpropose a novel method called textbf{T}ask-textbf{A}ware textbf{B}oundarytextbf{A}ugmentation (TABA) to boost the robustness of continual learningmodels. With extensive experiments on CIFAR-10 and CIFAR-100, we show theefficacy of adversarial training and TABA in defending adversarial attacks.", "output": "Towards Adversarially Robust Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Representation multi-task learning (MTL) and transfer learning (TL) haveachieved tremendous success in practice. However, the theoretical understandingof these methods is still lacking. Most existing theoretical works focus oncases where all tasks share the same representation, and claim that MTL and TLalmost always improve performance. However, as the number of tasks grow,assuming all tasks share the same representation is unrealistic. Also, thisdoes not always match empirical findings, which suggest that a sharedrepresentation may not necessarily improve single-task or target-only learningperformance. In this paper, we aim to understand how to learn from tasks withtextit{similar but not exactly the same} linear representations, while dealingwith outlier tasks. We propose two algorithms that are textit{adaptive} to thesimilarity structure and textit{robust} to outlier tasks under both MTL and TLsettings. Our algorithms outperform single-task or target-only learning whenrepresentations across tasks are sufficiently similar and the fraction ofoutlier tasks is small. Furthermore, they always perform no worse thansingle-task learning or target-only learning, even when the representations aredissimilar. We provide information-theoretic lower bounds to show that ouralgorithms are nearly textit{minimax} optimal in a large regime.", "output": "Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Meta-learning owns unique effectiveness and swiftness in tackling emergingtasks with limited data. Its broad applicability is revealed by viewing it as abi-level optimization problem. The resultant algorithmic viewpoint however,faces scalability issues when the inner-level optimization relies ongradient-based iterations. Implicit differentiation has been considered toalleviate this challenge, but it is restricted to an isotropic Gaussian prior,and only favors deterministic meta-learning approaches. This work markedlymitigates the scalability bottleneck by cross-fertilizing the benefits ofimplicit differentiation to probabilistic Bayesian meta-learning. The novelimplicit Bayesian meta-learning (iBaML) method not only broadens the scope oflearnable priors, but also quantifies the associated uncertainty. Furthermore,the ultimate complexity is well controlled regardless of the inner-leveloptimization trajectory. Analytical error bounds are established to demonstratethe precision and efficiency of the generalized implicit gradient over theexplicit one. Extensive numerical tests are also carried out to empiricallyvalidate the performance of the proposed method.", "output": "Scalable Bayesian Meta-Learning through Generalized Implicit Gradients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Blast furnace modeling and control is one of the important problems in theindustrial field, and the black-box model is an effective mean to describe thecomplex blast furnace system. In practice, there are often different learningtargets, such as safety and energy saving in industrial applications, dependingon the application. For this reason, this paper proposes a framework to designa domain knowledge integrated classification model that yields a classifier forindustrial application. Our knowledge incorporated learning scheme allows theusers to create a classifier that identifies \"important samples\" (whosemisclassifications can lead to severe consequences) more correctly, whilekeeping the proper precision of classifying the remaining samples. Theeffectiveness of the proposed method has been verified by two real blastfurnace datasets, which guides the operators to utilize their prior experiencefor controlling the blast furnace systems better.", "output": "Domain Knowledge integrated for Blast Furnace Classifier Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When interacting in a three dimensional world, humans must estimate 3Dstructure from visual inputs projected down to two dimensional retinal images.It has been shown that humans use the persistence of object shape overmotion-induced transformations as a cue to resolve depth ambiguity when solvingthis underconstrained problem. With the aim of understanding how biologicalvision systems may internally represent 3D transformations, we propose acomputational model, based on a generative manifold model, which can be used toinfer 3D structure from the motion of 2D points. Our model can also learnrepresentations of the transformations with minimal supervision, providing aproof of concept for how humans may develop internal representations on adevelopmental or evolutionary time scale. Focused on rotational motion, we showhow our model infers depth from moving 2D projected points, learns 3Drotational transformations from 2D training stimuli, and compares to humanperformance on psychophysical structure-from-motion experiments.", "output": "Learning Internal Representations of 3D Transformations from 2D Projected Inputs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We focus on a class of non-smooth optimization problems over the Stiefelmanifold in the decentralized setting, where a connected network of $n$ agentscooperatively minimize a finite-sum objective function with each componentbeing weakly convex in the ambient Euclidean space. Such optimization problems,albeit frequently encountered in applications, are quite challenging due totheir non-smoothness and non-convexity. To tackle them, we propose an iterativemethod called the decentralized Riemannian subgradient method (DRSM). Theglobal convergence and an iteration complexity of $mathcal{O}(varepsilon^{-2}log^2(varepsilon^{-1}))$ for forcing a natural stationarity measure below$varepsilon$ are established via the powerful tool of proximal smoothness fromvariational analysis, which could be of independent interest. Besides, we showthe local linear convergence of the DRSM using geometrically diminishingstepsizes when the problem at hand further possesses a sharpness property.Numerical experiments are conducted to corroborate our theoretical findings.", "output": "Decentralized Weakly Convex Optimization Over the Stiefel Manifold."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to predict traffic flow over time for crowded areas during rushhours is increasingly important as it can help authorities make informeddecisions for congestion mitigation or scheduling of infrastructure developmentin an area. However, a crucial challenge in traffic flow forecasting is theslow shifting in temporal peaks between daily and weekly cycles, resulting inthe nonstationarity of the traffic flow signal and leading to difficulty inaccurate forecasting. To address this challenge, we propose a slow shiftingconcerned machine learning method for traffic flow forecasting, which includestwo parts. First, we take advantage of Empirical Mode Decomposition as thefeature engineering to alleviate the nonstationarity of traffic flow data,yielding a series of stationary components. Second, due to the superiority ofLong-Short-Term-Memory networks in capturing temporal features, an advancedtraffic flow forecasting model is developed by taking the stationary componentsas inputs. Finally, we apply this method on a benchmark of real-world data andprovide a comparison with other existing methods. Our proposed methodoutperforms the state-of-art results by 14.55% and 62.56% using the metrics ofroot mean squared error and mean absolute percentage error, respectively.", "output": "A Slow-Shifting Concerned Machine Learning Method for Short-term Traffic Flow Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For large-scale IT corpora with hundreds of classes organized in a hierarchy,the task of accurate classification of classes at the higher level in thehierarchies is crucial to avoid errors propagating to the lower levels. In thebusiness world, an efficient and explainable ML model is preferred over anexpensive black-box model, especially if the performance increase is marginal.A current trend in the Natural Language Processing (NLP) community is towardsemploying huge pre-trained language models (PLMs) or what is known asself-attention models (e.g., BERT) for almost any kind of NLP task (e.g.,question-answering, sentiment analysis, text classification). Despite thewidespread use of PLMs and the impressive performance in a broad range of NLPtasks, there is a lack of a clear and well-justified need to as why thesemodels are being employed for domain-specific text classification (TC) tasks,given the monosemic nature of specialized words (i.e., jargon) found indomain-specific text which renders the purpose of contextualized embeddings(e.g., PLMs) futile. In this paper, we compare the accuracies of somestate-of-the-art (SOTA) models reported in the literature against a Linear SVMclassifier and TFIDF vectorization model on three TC datasets. Results show acomparable performance for the LinearSVM. The findings of this study show thatfor domain-specific TC tasks, a linear model can provide a comparable, cheap,reproducible, and interpretable alternative to attention-based models.", "output": "Attention is Not Always What You Need: Towards Efficient Classification of Domain-Specific Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a new method for anomaly detection in time-series data byincorporating the concept of difference subspace into the singular spectrumanalysis (SSA). The key idea is to monitor slight temporal variations of thedifference subspace between two signal subspaces corresponding to the past andpresent time-series data, as anomaly score. It is a natural generalization ofthe conventional SSA-based method which measures the minimum angle between thetwo signal subspaces as the degree of changes. By replacing the minimum anglewith the difference subspace, our method boosts the performance while using theSSA-based framework as it can capture the whole structural difference betweenthe two subspaces in its magnitude and direction. We demonstrate our method'seffectiveness through performance evaluations on public time-series datasets.", "output": "Time-series Anomaly Detection based on Difference Subspace between Signal Subspaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In supervised learning, the regularization path is sometimes used as aconvenient theoretical proxy for the optimization path of gradient descentinitialized with zero. In this paper, we study a modification of theregularization path for infinite-width 2-layer ReLU neural networks withnon-zero initial distribution of the weights at different scales. By exploitinga link with unbalanced optimal transport theory, we show that, despite thenon-convexity of the 2-layer network training, this problem admits an infinitedimensional convex counterpart. We formulate the corresponding functionaloptimization problem and investigate its main properties. In particular, weshow that as the scale of the initialization ranges between $0$ and $+infty$,the associated path interpolates continuously between the so-called kernel andrich regimes. The numerical experiments confirm that, in our setting, thescaling path and the final states of the optimization path behave similarlyeven beyond these extreme points.", "output": "On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Introduction: Traditional Korean medicine (TKM) emphasizes individualizeddiagnosis and treatment, making AI modeling difficult due to limited data andimplicit processes. GPT-3.5 and GPT-4, large language models, have shownimpressive medical knowledge despite lacking medicine-specific training. Thisstudy aimed to assess the capabilities of GPT-3.5 and GPT-4 for TKM using theKorean National Licensing Examination for Korean Medicine Doctors. Methods:GPT-3.5 (February 2023) and GPT-4 (March 2023) models answered 340 questionsfrom the 2022 examination across 12 subjects. Each question was independentlyevaluated five times in an initialized session. Results: GPT-3.5 and GPT-4achieved 42.06% and 57.29% accuracy, respectively, with GPT-4 nearing passingperformance. There were significant differences in accuracy by subjects, with83.75% accuracy for neuropsychiatry compared to 28.75% for internal medicine(2). Both models showed high accuracy in recall-based and diagnosis-basedquestions but struggled with intervention-based ones. The accuracy forquestions that require TKM-specialized knowledge was relatively lower than theaccuracy for questions that do not GPT-4 showed high accuracy for table-basedquestions, and both models demonstrated consistent responses. A positivecorrelation between consistency and accuracy was observed. Conclusion: Modelsin this study showed near-passing performance in decision-making for TKMwithout domain-specific training. However, limits were also observed that werebelieved to be caused by culturally-biased learning. Our study suggests thatfoundation models have potential in culturally-adapted medicine, specificallyTKM, for clinical assistance, medical education, and medical research.", "output": "Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The variety of complex algorithmic approaches for tackling time-seriesclassification problems has grown considerably over the past decades, includingthe development of sophisticated but challenging-to-interpretdeep-learning-based methods. But without comparison to simpler methods it canbe difficult to determine when such complexity is required to obtain strongperformance on a given problem. Here we evaluate the performance of anextremely simple classification approach -- a linear classifier in the space oftwo simple features that ignore the sequential ordering of the data: the meanand standard deviation of time-series values. Across a large repository of 128univariate time-series classification problems, this simple distributionalmoment-based approach outperformed chance on 69 problems, and reached 100%accuracy on two problems. With a neuroimaging time-series case study, we findthat a simple linear model based on the mean and standard deviation performsbetter at classifying individuals with schizophrenia than a model thatadditionally includes features of the time-series dynamics. Comparing theperformance of simple distributional features of a time series providesimportant context for interpreting the performance of complex time-seriesclassification models, which may not always be required to obtain highaccuracy.", "output": "Never a Dull Moment: Distributional Properties as a Baseline for Time-Series Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, an off-policy reinforcement learning algorithm is designed tosolve the continuous-time LQR problem using only input-state data measured fromthe system. Different from other algorithms in the literature, we propose theuse of a specific persistently exciting input as the exploration signal duringthe data collection step. We then show that, using this persistently exciteddata, the solution of the matrix equation in our algorithm is guaranteed toexist and to be unique at every iteration. Convergence of the algorithm to theoptimal control input is also proven. Moreover, we formulate the policyevaluation step as the solution of a Sylvester-transpose equation, whichincreases the efficiency of its solution. Finally, a method to determine astabilizing policy to initialize the algorithm using only measured data isproposed.", "output": "An Efficient Off-Policy Reinforcement Learning Algorithm for the Continuous-Time LQR Problem."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes an interpretable neural network-based non-proportionalodds model (N$^3$POM) for ordinal regression, where the response variable cantake not only discrete but also continuous values, and the regressioncoefficients vary depending on the predicting ordinal response. In contrast toconventional approaches estimating the linear coefficients of regressiondirectly from the discrete response, we train a non-linear neural network thatoutputs the linear coefficients by taking the response as its input. By virtueof the neural network, N$^3$POM may have flexibility while preserving theinterpretability of the conventional ordinal regression. We show a sufficientcondition so that the predicted conditional cumulative probability~(CCP)satisfies the monotonicity constraint locally over a user-specified region inthe covariate space; we also provide a monotonicity-preserving stochastic (MPS)algorithm for training the neural network adequately.", "output": "An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We focus on learning hidden dynamics from data using ODE-nets templated onimplicit numerical initial value problem solvers. First, we perform InverseModified error analysis of the ODE-nets using unrolled implicit schemes forease of interpretation. It is shown that training an ODE-net using an unrolledimplicit scheme returns a close approximation of an Inverse ModifiedDifferential Equation (IMDE). In addition, we establish a theoretical basis forhyper-parameter selection when training such ODE-nets, whereas currentstrategies usually treat numerical integration of ODE-nets as a black box. Wethus formulate an adaptive algorithm which monitors the level of error andadapts the number of (unrolled) implicit solution iterations during thetraining process, so that the error of the unrolled approximation is less thanthe current learning loss. This helps accelerate training, while maintainingaccuracy. Several numerical experiments are performed to demonstrate theadvantages of the proposed algorithm compared to nonadaptive unrollings, andvalidate the theoretical analysis. We also note that this approach naturallyallows for incorporating partially known physical terms in the equations,giving rise to what is termed ``gray box\" identification.", "output": "Implementation and (Inverse Modified) Error Analysis for implicitly-templated ODE-nets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Saliency methods provide post-hoc model interpretation by attributing inputfeatures to the model outputs. Current methods mainly achieve this using asingle input sample, thereby failing to answer input-independent inquiriesabout the model. We also show that input-specific saliency mapping isintrinsically susceptible to misleading feature attribution. Current attemptsto use 'general' input features for model interpretation assume access to adataset containing those features, which biases the interpretation. Addressingthe gap, we introduce a new perspective of input-agnostic saliency mapping thatcomputationally estimates the high-level features attributed by the model toits outputs. These features are geometrically correlated, and are computed byaccumulating model's gradient information with respect to an unrestricted datadistribution. To compute these features, we nudge independent data points overthe model loss surface towards the local minima associated by ahuman-understandable concept, e.g., class label for classifiers. With asystematic projection, scaling and refinement process, this information istransformed into an interpretable visualization without compromising itsmodel-fidelity. The visualization serves as a stand-alone qualitativeinterpretation. With an extensive evaluation, we not only demonstratesuccessful visualizations for a variety of concepts for large-scale models, butalso showcase an interesting utility of this new form of saliency mapping byidentifying backdoor signatures in compromised classifiers.", "output": "Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The abundance of instructional videos and their narrations over the Internetoffers an exciting avenue for understanding procedural activities. In thiswork, we propose to learn video representation that encodes both action stepsand their temporal ordering, based on a large-scale dataset of webinstructional videos and their narrations, without using human annotations. Ourmethod jointly learns a video representation to encode individual stepconcepts, and a deep probabilistic model to capture both temporal dependenciesand immense individual variations in the step ordering. We empiricallydemonstrate that learning temporal ordering not only enables new capabilitiesfor procedure reasoning, but also reinforces the recognition of individualsteps. Our model significantly advances the state-of-the-art results on stepclassification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting(+7.4% on COIN). Moreover, our model attains promising results in zero-shotinference for step classification and forecasting, as well as in predictingdiverse and plausible steps for incomplete procedures. Our code is available at", "output": "Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Finding relevant and high-quality datasets to train machine learning modelsis a major bottleneck for practitioners. Furthermore, to address ambitiousreal-world use-cases there is usually the requirement that the data comelabelled with high-quality annotations that can facilitate the training of asupervised model. Manually labelling data with high-quality labels is generallya time-consuming and challenging task and often this turns out to be thebottleneck in a machine learning project. Weak Supervised Learning (WSL)approaches have been developed to alleviate the annotation burden by offeringan automatic way of assigning approximate labels (pseudo-labels) to unlabelleddata based on heuristics, distant supervision and knowledge bases. We applyprobabilistic generative latent variable models (PLVMs), trained on heuristiclabelling representations of the original dataset, as an accurate, fast andcost-effective way to generate pseudo-labels. We show that the PLVMs achievestate-of-the-art performance across four datasets. For example, they achieve22% points higher F1 score than Snorkel in the class-imbalanced Spouse dataset.PLVMs are plug-and-playable and are a drop-in replacement to existing WSLframeworks (e.g. Snorkel) or they can be used as benchmark models for morecomplicated algorithms, giving practitioners a compelling accuracy boost.", "output": "A Benchmark Generative Probabilistic Model for Weak Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point cloud data are widely used in manufacturing applications for processinspection, modeling, monitoring and optimization. The state-of-art tensorregression techniques have effectively been used for analysis of structuredpoint cloud data, where the measurements on a uniform grid can be formed into atensor. However, these techniques are not capable of handling unstructuredpoint cloud data that are often in the form of manifolds. In this paper, wepropose a nonlinear dimension reduction approach named Maximum CovarianceUnfolding Regression that is able to learn the low-dimensional (LD) manifold ofpoint clouds with the highest correlation with explanatory covariates. This LDmanifold is then used for regression modeling and process optimization based onprocess variables. The performance of the proposed method is subsequentlyevaluated and compared with benchmark methods through simulations and a casestudy of steel bracket manufacturing.", "output": "Maximum Covariance Unfolding Regression: A Novel Covariate-based Manifold Learning Approach for Point Cloud Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Content affinity loss including feature and pixel affinity is a main problemwhich leads to artifacts in photorealistic and video style transfer. This paperproposes a new framework named CAP-VSTNet, which consists of a new reversibleresidual network and an unbiased linear transform module, for versatile styletransfer. This reversible residual network can not only preserve contentaffinity but not introduce redundant information as traditional reversiblenetworks, and hence facilitate better stylization. Empowered by MattingLaplacian training loss which can address the pixel affinity loss problem ledby the linear transform, the proposed framework is applicable and effective onversatile style transfer. Extensive experiments show that CAP-VSTNet canproduce better qualitative and quantitative results in comparison with thestate-of-the-art methods.", "output": "CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Memory optimization for deep neural network (DNN) inference gains highrelevance with the emergence of TinyML, which refers to the deployment of DNNinference tasks on tiny, low-power microcontrollers. Applications such as audiokeyword detection or radar-based gesture recognition are heavily constrained bythe limited memory on such tiny devices because DNN inference requires largeintermediate run-time buffers to store activations and other intermediate data,which leads to high memory usage. In this paper, we propose a new FusedDepthwise Tiling (FDT) method for the memory optimization of DNNs, which,compared to existing tiling methods, reduces memory usage without inducing anyrun time overhead. FDT applies to a larger variety of network layers thanexisting tiling methods that focus on convolutions. It improves TinyML memoryoptimization significantly by reducing memory of models where this was notpossible before and additionally providing alternative design points for modelsthat show high run time overhead with existing methods. In order to identifythe best tiling configuration, an end-to-end flow with a new path discoverymethod is proposed, which applies FDT and existing tiling methods in a fullyautomated way, including the scheduling of the operations and planning of thelayout of buffers in memory. Out of seven evaluated models, FDT achievedsignificant memory reduction for two models by 76.2% and 18.1% where existingtiling methods could not be applied. Two other models showed a significant runtime overhead with existing methods and FDT provided alternative design pointswith no overhead but reduced memory savings.", "output": "Fused Depthwise Tiling for Memory Optimization in TinyML Deep Neural Network Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Process simulation is an analysis tool in process mining that allows users tomeasure the impact of changes, prevent losses, and update the process withoutrisks or costs. In the literature, several process simulation techniques areavailable and they are usually built upon process models discovered from agiven event log or learned via deep learning. Each group of approaches has itsown strengths and limitations. The former is usually restricted to thecontrol-flow but it is more interpretable, whereas the latter is notinterpretable by nature but has a greater generalization capability on largeevent logs. Despite the great performance achieved by deep learning approaches,they are still not suitable to be applied to real scenarios and generate valuefor users. This issue is mainly due to fact their stochasticity is hard tocontrol. To address this problem, we propose the CoSMo framework forimplementing process simulation models fully based on deep learning. Thisframework enables simulating event logs that satisfy a constraint byconditioning the learning phase of a deep neural network. Throughoutexperiments, the simulation is validated from both control-flow and data-flowperspectives, demonstrating the proposed framework's capability of simulatingcases while satisfying imposed conditions.", "output": "CoSMo: a Framework for Implementing Conditioned Process Simulation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A wireless federated learning system is investigated by allowing a server andworkers to exchange uncoded information via orthogonal wireless channels. Sincethe workers frequently upload local gradients to the server viabandwidth-limited channels, the uplink transmission from the workers to theserver becomes a communication bottleneck. Therefore, a one-shot distributedprinciple component analysis (PCA) is leveraged to reduce the dimension ofuploaded gradients such that the communication bottleneck is relieved. APCA-based wireless federated learning (PCA-WFL) algorithm and its acceleratedversion (i.e., PCA-AWFL) are proposed based on the low-dimensional gradientsand the Nesterov's momentum. For the non-convex loss functions, a finite-timeanalysis is performed to quantify the impacts of system hyper-parameters on theconvergence of the PCA-WFL and PCA-AWFL algorithms. The PCA-AWFL algorithm istheoretically certified to converge faster than the PCA-WFL algorithm. Besides,the convergence rates of PCA-WFL and PCA-AWFL algorithms quantitatively revealthe linear speedup with respect to the number of workers over the vanillagradient descent algorithm. Numerical results are used to demonstrate theimproved convergence rates of the proposed PCA-WFL and PCA-AWFL algorithms overthe benchmarks.", "output": "Accelerating Wireless Federated Learning via Nesterov's Momentum and Distributed Principle Component Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Virtual Reality (VR) technology is being advanced along the lines ofenhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), andsupporting unconstrained mobility of the users in their VEs, while constrainingthem within specialized VR setups through Redirected Walking (RDW). For meetingthe extreme data-rate and latency requirements of future VR systems, supportingwireless networking infrastructures will operate in millimeter Wave (mmWave)frequencies and leverage highly directional communication in both transmissionand reception through beamforming and beamsteering. We propose to leveragepredictive context-awareness for optimizing transmitter and receiver-sidebeamforming and beamsteering. In particular, we argue that short-termprediction of users' lateral movements in multiuser VR setups with RDW can beutilized for optimizing transmitter-side beamforming and beamsteering throughLine-of-Sight (LoS) \"tracking\" in the users' directions. At the same time,short-term prediction of orientational movements can be used for receiver-sidebeamforming for coverage flexibility enhancements. We target two open problemsin predicting these two context information instances: i) lateral movementprediction in multiuser VR settings with RDW and ii) generation of synthetichead rotation datasets to be utilized in the training of existing orientationalmovements predictors. We follow by experimentally showing that Long Short-TermMemory (LSTM) networks feature promising accuracy in predicting lateralmovements, as well as that context-awareness stemming from VEs further benefitsthis accuracy. Second, we show that a TimeGAN-based approach for orientationaldata generation can generate synthetic samples closely matching theexperimentally obtained ones.", "output": "Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we explore the impact of network topology on the approximationcapabilities of artificial neural networks (ANNs), with a particular focus oncomplex topologies. We propose a novel methodology for constructing complexANNs based on various topologies, including Barab'asi-Albert,ErdH{o}s-R'enyi, Watts-Strogatz, and multilayer perceptrons (MLPs). Theconstructed networks are evaluated on synthetic datasets generated frommanifold learning generators, with varying levels of task difficulty and noise.Our findings reveal that complex topologies lead to superior performance inhigh-difficulty regimes compared to traditional MLPs. This performanceadvantage is attributed to the ability of complex networks to exploit thecompositionality of the underlying target function. However, this benefit comesat the cost of increased forward-pass computation time and reduced robustnessto graph damage. Additionally, we investigate the relationship between varioustopological attributes and model performance. Our analysis shows that no singleattribute can account for the observed performance differences, suggesting thatthe influence of network topology on approximation capabilities may be moreintricate than a simple correlation with individual topological attributes. Ourstudy sheds light on the potential of complex topologies for enhancing theperformance of ANNs and provides a foundation for future research exploring theinterplay between multiple topological attributes and their impact on modelperformance.", "output": "Beyond Multilayer Perceptrons: Investigating Complex Topologies in Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The knowledge of the states of a vehicle is a necessity to perform properplanning and control. These quantities are usually accessible throughmeasurements. Control theory brings extremely useful methods -- observers -- todeal with quantities that cannot be directly measured or with noisymeasurements. Classical observers are mathematically derived from models. Inspite of their success, such as the Kalman filter, they show their limits whensystems display high non-linearities, modeling errors, high uncertainties ordifficult interactions with the environment (e.g. road contact). In this work,we present a method to build a learning-based observer able to outperformclassical observing methods. We compare several neural network architecturesand define the data generation procedure used to train them. The method isevaluated on a kinematic bicycle model which allows to easily generate data fortraining and testing. This model is also used in an Extended Kalman Filter(EKF) for comparison of the learning-based observer with a state of the artmodel-based observer. The results prove the interest of our approach and pavethe way for future improvements of the technique.", "output": "Learning-based Observer Evaluated on the Kinematic Bicycle Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven offline model-based optimization (MBO) is an establishedpractical approach to black-box computational design problems for which thetrue objective function is unknown and expensive to query. However, thestandard approach which optimizes designs against a learned proxy model of theground truth objective can suffer from distributional shift. Specifically, inhigh-dimensional design spaces where valid designs lie on a narrow manifold,the standard approach is susceptible to producing out-of-distribution, invaliddesigns that \"fool\" the learned proxy model into outputting a high value. Usingan ensemble rather than a single model as the learned proxy can help mitigatedistribution shift, but naive formulations for combining gradient informationfrom the ensemble, such as minimum or mean gradient, are still suboptimal andoften hampered by non-convergent behavior.In this work, we explore alternate approaches for combining gradientinformation from the ensemble that are robust to distribution shift withoutcompromising optimality of the produced designs. More specifically, we exploretwo functions, formulated as convex optimization problems, for combininggradient information: multiple gradient descent algorithm (MGDA) andconflict-averse gradient descent (CAGrad). We evaluate these algorithms on adiverse set of five computational design tasks. We compare performance ofensemble MBO with MGDA and ensemble MBO with CAGrad with three naive baselinealgorithms: (a) standard single-model MBO, (b) ensemble MBO with mean gradient,and (c) ensemble MBO with minimum gradient.Our results suggest that MGDA and CAGrad strike a desirable balance betweenconservatism and optimality and can help robustify data-driven offline MBOwithout compromising optimality of designs.", "output": "Conflict-Averse Gradient Optimization of Ensembles for Effective Offline Model-Based Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gradient regularization, as described in citet{barrett2021implicit}, is ahighly effective technique for promoting flat minima during gradient descent.Empirical evidence suggests that this regularization technique cansignificantly enhance the robustness of deep learning models against noisyperturbations, while also reducing test error. In this paper, we explore theper-example gradient regularization (PEGR) and present a theoretical analysisthat demonstrates its effectiveness in improving both test error and robustnessagainst noise perturbations. Specifically, we adopt a signal-noise data modelfrom citet{cao2022benign} and show that PEGR can learn signals effectivelywhile suppressing noise. In contrast, standard gradient descent struggles todistinguish the signal from the noise, leading to suboptimal generalizationperformance. Our analysis reveals that PEGR penalizes the variance of patternlearning, thus effectively suppressing the memorization of noises from thetraining data. These findings underscore the importance of variance control indeep learning training and offer useful insights for developing more effectivetraining approaches.", "output": "Per-Example Gradient Regularization Improves Learning Signals from Noisy Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Organ at Risk (OAR) segmentation from CT scans is a key component of theradiotherapy treatment workflow. In recent years, deep learning techniques haveshown remarkable potential in automating this process. In this paper, weinvestigate the performance of Generative Adversarial Networks (GANs) comparedto supervised learning approaches for segmenting OARs from CT images. Wepropose three GAN-based models with identical generator architectures butdifferent discriminator networks. These models are compared withwell-established CNN models, such as SE-ResUnet and DeepLabV3, using theStructSeg dataset, which consists of 50 annotated CT scans containing contoursof six OARs. Our work aims to provide insight into the advantages anddisadvantages of adversarial training in the context of OAR segmentation. Theresults are very promising and show that the proposed GAN-based approaches aresimilar or superior to their CNN-based counterparts, particularly whensegmenting more challenging target organs.", "output": "Comparing Adversarial and Supervised Learning for Organs at Risk Segmentation in CT images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classic Machine Learning techniques require training on data available in asingle data lake. However, aggregating data from different owners is not alwaysconvenient for different reasons, including security, privacy and secrecy. Datacarry a value that might vanish when shared with others; the ability to avoidsharing the data enables industrial applications where security and privacy areof paramount importance, making it possible to train global models byimplementing only local policies which can be run independently and even onair-gapped data centres. Federated Learning (FL) is a distributed machinelearning approach which has emerged as an effective way to address privacyconcerns by only sharing local AI models while keeping the data decentralized.Two critical challenges of Federated Learning are managing the heterogeneoussystems in the same federated network and dealing with real data, which areoften not independently and identically distributed (non-IID) among theclients. In this paper, we focus on the second problem, i.e., the problem ofstatistical heterogeneity of the data in the same federated network. In thissetting, local models might be strayed far from the local optimum of thecomplete dataset, thus possibly hindering the convergence of the federatedmodel. Several Federated Learning algorithms, such as FedAvg, FedProx andFederated Curvature (FedCurv), aiming at tackling the non-IID setting, havealready been proposed. This work provides an empirical assessment of thebehaviour of FedAvg and FedCurv in common non-IID scenarios. Results show thatthe number of epochs per round is an important hyper-parameter that, when tunedappropriately, can lead to significant performance gains while reducing thecommunication cost. As a side product of this work, we release the non-IIDversion of the datasets we used so to facilitate further comparisons from theFL community.", "output": "Benchmarking FedAvg and FedCurv for Image Classification Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic detection of machine anomaly remains challenging for machinelearning. We believe the capability of generative adversarial network (GAN)suits the need of machine audio anomaly detection, yet rarely has this beeninvestigated by previous work. In this paper, we propose AEGAN-AD, a totallyunsupervised approach in which the generator (also an autoencoder) is trainedto reconstruct input spectrograms. It is pointed out that the denoising natureof reconstruction deprecates its capacity. Thus, the discriminator isredesigned to aid the generator during both training stage and detection stage.The performance of AEGAN-AD on the dataset of DCASE 2022 Challenge TASK 2demonstrates the state-of-the-art result on five machine types. A novel anomalylocalization method is also investigated. Source code available at:www.github.com/jianganbai/AEGAN-AD", "output": "Unsupervised Anomaly Detection and Localization of Machine Audio: A GAN-based Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, the idea of using FP8 as a number format for neural networktraining has been floating around the deep learning world. Given that mosttraining is currently conducted with entire networks in FP32, or sometimes FP16with mixed-precision, the step to having some parts of a network run in FP8with 8-bit weights is an appealing potential speed-up for the generally costlyand time-intensive training procedures in deep learning. A natural questionarises regarding what this development means for efficient inference on edgedevices. In the efficient inference device world, workloads are frequentlyexecuted in INT8. Sometimes going even as low as INT4 when efficiency calls forit. In this whitepaper, we compare the performance for both the FP8 and INTformats for efficient on-device inference. We theoretically show the differencebetween the INT and FP formats for neural networks and present a plethora ofpost-training quantization and quantization-aware-training results to show howthis theory translates to practice. We also provide a hardware analysis showingthat the FP formats are somewhere between 50-180% less efficient in terms ofcompute in dedicated hardware than the INT format. Based on our research and aread of the research field, we conclude that although the proposed FP8 formatcould be good for training, the results for inference do not warrant adedicated implementation of FP8 in favor of INT8 for efficient inference. Weshow that our results are mostly consistent with previous findings but thatimportant comparisons between the formats have thus far been lacking. Finally,we discuss what happens when FP8-trained networks are converted to INT8 andconclude with a brief discussion on the most efficient way for on-devicedeployment and an extensive suite of INT8 results for many models.", "output": "FP8 versus INT8 for efficient deep learning inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the medical images field, semantic segmentation is one of the mostimportant, yet difficult and time-consuming tasks to be performed byphysicians. Thanks to the recent advancement in the Deep Learning modelsregarding Computer Vision, the promise to automate this kind of task is gettingmore and more realistic. However, many problems are still to be solved, likethe scarce availability of data and the difficulty to extend the efficiency ofhighly specialised models to general scenarios. Organs at risk segmentation forradiotherapy treatment planning falls in this category, as the limited dataavailable negatively affects the possibility to develop general-purpose models;in this work, we focus on the possibility to solve this problem by presentingthree types of ensembles of single-organ models able to produce multi-organmasks exploiting the different specialisations of their components. The resultsobtained are promising and prove that this is a possible solution to findingefficient multi-organ segmentation methods.", "output": "Ensemble Methods for Multi-Organ Segmentation in CT Series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As control engineering methods are applied to increasingly complex systems,data-driven approaches for system identification appear as a promisingalternative to physics-based modeling. While many of these approaches rely onthe availability of state measurements, the states of a complex system areoften not directly measurable. It may then be necessary to jointly estimate thedynamics and a latent state, making it considerably more challenging to designcontrollers with performance guarantees. This paper proposes a novel method forthe computation of an optimal input trajectory for unknown nonlinear systemswith latent states. Probabilistic performance guarantees are derived for theresulting input trajectory, and an approach to validate the performance ofarbitrary control laws is presented. The effectiveness of the proposed methodis demonstrated in a numerical simulation.", "output": "Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The information diffusion performance of GCN and its variant models islimited by the adjacency matrix, which can lower their performance. Therefore,we introduce a new framework for graph convolutional networks called HybridDiffusion-based Graph Convolutional Network (HD-GCN) to address the limitationsof information diffusion caused by the adjacency matrix. In the HD-GCNframework, we initially utilize diffusion maps to facilitate the diffusion ofinformation among nodes that are adjacent to each other in the feature space.This allows for the diffusion of information between similar points that maynot have an adjacent relationship. Next, we utilize graph convolution tofurther propagate information among adjacent nodes after the diffusion maps,thereby enabling the spread of information among similar nodes that areadjacent in the graph. Finally, we employ the diffusion distances obtainedthrough the use of diffusion maps to regularize and constrain the predictedlabels of training nodes. This regularization method is then applied to theHD-GCN training, resulting in a smoother classification surface. The modelproposed in this paper effectively overcomes the limitations of informationdiffusion imposed only by the adjacency matrix. HD-GCN utilizes hybriddiffusion by combining information diffusion between neighborhood nodes in thefeature space and adjacent nodes in the adjacency matrix. This method allowsfor more comprehensive information propagation among nodes, resulting inimproved model performance. We evaluated the performance of DM-GCN on threewell-known citation network datasets and the results showed that the proposedframework is more effective than several graph-based semi-supervised learningmethods.", "output": "HD-GCN:A Hybrid Diffusion Graph Convolutional Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many real world situations, like minor traffic offenses in big cities, acentral authority is tasked with periodic administering punishments to a largenumber of individuals. Common practice is to give each individual a chance tosuffer a smaller fine and be guaranteed to avoid the legal process withprobable considerably larger punishment. However, thanks to the large number ofoffenders and a limited capacity of the central authority, the individual riskis typically small and a rational individual will not choose to pay the fine.Here we show that if the central authority processes the offenders in apublicly known order, it properly incentives the offenders to pay the fine. Weshow analytically and on realistic experiments that our mechanism promotesnon-cooperation and incentives individuals to pay. Moreover, the same holds foran arbitrary coalition. We quantify the expected total payment the centralauthority receives, and show it increases considerably.", "output": "Promoting Non-Cooperation Through Ordering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The metaverse, which is at the stage of innovation and exploration, faces thedilemma of data collection and the problem of private data leakage in theprocess of development. This can seriously hinder the widespread deployment ofthe metaverse. Fortunately, federated learning (FL) is a solution to the aboveproblems. FL is a distributed machine learning paradigm with privacy-preservingfeatures designed for a large number of edge devices. Federated learning formetaverse (FL4M) will be a powerful tool. Because FL allows edge devices toparticipate in training tasks locally using their own data, computationalpower, and model-building capabilities. Applying FL to the metaverse not onlyprotects the data privacy of participants but also reduces the need for highcomputing power and high memory on servers. Until now, there have been manystudies about FL and the metaverse, respectively. In this paper, we review someof the early advances of FL4M, which will be a research direction withunlimited development potential. We first introduce the concepts of metaverseand FL, respectively. Besides, we discuss the convergence of key metaversetechnologies and FL in detail, such as big data, communication technology, theInternet of Things, edge computing, blockchain, and extended reality. Finally,we discuss some key challenges and promising directions of FL4M in detail. Insummary, we hope that our up-to-date brief survey can help people betterunderstand FL4M and build a fair, open, and secure metaverse.", "output": "Federated Learning for Metaverse: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nonnegative Matrix Factorization is an important tool in unsupervised machinelearning to decompose a data matrix into a product of parts that are ofteninterpretable. Many algorithms have been proposed during the last threedecades. A well-known method is the Multiplicative Updates algorithm proposedby Lee and Seung in 2002. Multiplicative updates have many interestingfeatures: they are simple to implement and can be adapted to popular variantssuch as sparse Nonnegative Matrix Factorization, and, according to recentbenchmarks, is state-of-the-art for many problems where the loss function isnot the Frobenius norm. In this manuscript, we propose to improve theMultiplicative Updates algorithm seen as an alternating majorizationminimization algorithm by crafting a tighter upper bound of the Hessian matrixfor each alternate subproblem. Convergence is still ensured and we observe inpractice on both synthetic and real world dataset that the proposed fastMUalgorithm is often several orders of magnitude faster than the regularMultiplicative Updates algorithm, and can even be competitive withstate-of-the-art methods for the Frobenius loss.", "output": "A fast Multiplicative Updates algorithm for Non-negative Matrix Factorization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Entropy measures are effective features for time series classificationproblems. Traditional entropy measures, such as Shannon entropy, useprobability distribution function. However, for the effective separation oftime series, new entropy estimation methods are required to characterize thechaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn)is based on the classification of special datasets (MNIST-10 andSARS-CoV-2-RBV1) in relation to the entropy of the time series recorded in thereservoir of the LogNNet neural network. NNetEn estimates the chaotic dynamicsof time series in an original way. Based on the NNetEn algorithm, we proposetwo new classification metrics: R2 Efficiency and Pearson Efficiency. Theefficiency of NNetEn is verified on separation of two chaotic time series ofsine mapping using dispersion analysis (ANOVA). For two close dynamic timeseries (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124and reflects high efficiency of the introduced method in classificationproblems. The EEG signal classification for healthy persons and patients withAlzheimer disease illustrates the practical application of the NNetEn features.Our computations demonstrate the synergistic effect of increasingclassification accuracy when applying traditional entropy measures and theNNetEn concept conjointly. An implementation of the algorithms in Python ispresented.", "output": "Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Purpose - To characterise and assess the quality of published researchevaluating artificial intelligence (AI) methods for ovarian cancer diagnosis orprognosis using histopathology data. Methods - A search of 5 sources wasconducted up to 01/12/2022. The inclusion criteria required that researchevaluated AI on histopathology images for diagnostic or prognostic inferencesin ovarian cancer, including tubo-ovarian and peritoneal tumours. Reviews andnon-English language articles were excluded. The risk of bias was assessed forevery included model using PROBAST. Results - A total of 1434 research articleswere identified, of which 36 were eligible for inclusion. These studiesreported 62 models of interest, including 35 classifiers, 14 survivalprediction models, 7 segmentation models, and 6 regression models. Models weredeveloped using 1-1375 slides from 1-664 ovarian cancer patients. A wide arrayof outcomes were predicted, including overall survival (9/62), histologicalsubtypes (7/62), stain quantity (6/62) and malignancy (5/62). Older studiesused traditional machine learning (ML) models with hand-crafted features, whilenewer studies typically employed deep learning (DL) to automatically learnfeatures and predict the outcome(s) of interest. All models were found to be athigh or unclear risk of bias overall. Research was frequently limited byinsufficient reporting, small sample sizes, and insufficient validation.Conclusion - Limited research has been conducted and none of the associatedmodels have been demonstrated to be ready for real-world implementation.Recommendations are provided addressing underlying biases and flaws in studydesign, which should help inform higher-quality reproducible future research.Key aspects include more transparent and comprehensive reporting, and improvedperformance evaluation using cross-validation and external validations.", "output": "Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The interactions between cells and the extracellular matrix are vital for theself-organisation of tissues. In this paper we present proof-of-concept to usemachine learning tools to predict the role of this mechanobiology in theself-organisation of cell-laden hydrogels grown in tethered moulds. We developa process for the automated generation of mould designs with and without keysymmetries. We create a large training set with $N=6500$ cases by runningdetailed biophysical simulations of cell-matrix interactions using thecontractile network dipole orientation (CONDOR) model for the self-organisationof cellular hydrogels within these moulds. These are used to train animplementation of the texttt{pix2pix} deep learning model, reserving $740$cases that were unseen in the training of the neural network for training andvalidation. Comparison between the predictions of the machine learningtechnique and the reserved predictions from the biophysical algorithm show thatthe machine learning algorithm makes excellent predictions. The machinelearning algorithm is significantly faster than the biophysical method, openingthe possibility of very high throughput rational design of moulds forpharmaceutical testing, regenerative medicine and fundamental studies ofbiology. Future extensions for scaffolds and 3D bioprinting will openadditional applications.", "output": "Rapid prediction of lab-grown tissue properties using deep learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In real-world applications, a machine learning model is required to handle anopen-set recognition (OSR), where unknown classes appear during the inference,in addition to a domain shift, where the distribution of data differs betweenthe training and inference phases. Domain generalization (DG) aims to handlethe domain shift situation where the target domain of the inference phase isinaccessible during model training. Open domain generalization (ODG) takes intoaccount both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a methodtargeting ODG but has a complicated learning process. On the other hand,although various DG methods have been proposed, they have not been evaluated inODG situations. This work comprehensively evaluates existing DG methods in ODGand shows that two simple DG methods, CORrelation ALignment (CORAL) and MaximumMean Discrepancy (MMD), are competitive with DAML in several cases. Inaddition, we propose simple extensions of CORAL and MMD by introducing thetechniques used in DAML, such as ensemble learning and Dirichlet mixup dataaugmentation. The experimental evaluation demonstrates that the extended CORALand MMD can perform comparably to DAML with lower computational costs. Thissuggests that the simple DG methods and their simple extensions are strongbaselines for ODG. The code used in the experiments is available at", "output": "Simple Domain Generalization Methods are Strong Baselines for Open Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although there are many datasets for traffic sign classification, there arefew datasets collected for traffic sign recognition and few of them obtainenough instances especially for training a model with the deep learning method.The deep learning method is almost the only way to train a model for real-worldusage that covers various highly similar classes compared with the traditionalway such as through color, shape, etc. Also, for some certain sign classes,their sign meanings were destined to can't get enough instances in the dataset.To solve this problem, we purpose a unique data augmentation method for thetraffic sign recognition dataset that takes advantage of the standard of thetraffic sign. We called it TSR dataset augmentation. We based on the benchmarkTsinghua-Tencent 100K (TT100K) dataset to verify the unique data augmentationmethod. we performed the method on four main iteration version datasets basedon the TT100K dataset and the experimental results showed our method isefficacious. The iteration version datasets based on TT100K, data augmentationmethod source code and the training results introduced in this paper arepublicly available.", "output": "Traffic Sign Recognition Dataset and Data Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, machine learning-based cardinality estimation methods arereplacing traditional methods. This change is expected to contribute to one ofthe most important applications of cardinality estimation, the query optimizer,to speed up query processing. However, none of the existing methods do notprecisely estimate cardinalities when relational schemas consist of many tableswith strong correlations between tables/attributes. This paper describes thatmultiple density estimators can be combined to effectively target thecardinality estimation of data with large and complex schemas having strongcorrelations. We propose Scardina, a new join cardinality estimation methodusing multiple partitioned models based on the schema structure.", "output": "Scardina: Scalable Join Cardinality Estimation by Multiple Density Estimators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we revisit the problem of Differentially Private StochasticConvex Optimization (DP-SCO) in Euclidean and general $ell_p^d$ spaces.Specifically, we focus on three settings that are still far from wellunderstood: (1) DP-SCO over a constrained and bounded (convex) set in Euclideanspace; (2) unconstrained DP-SCO in $ell_p^d$ space; (3) DP-SCO withheavy-tailed data over a constrained and bounded set in $ell_p^d$ space. Forproblem (1), for both convex and strongly convex loss functions, we proposemethods whose outputs could achieve (expected) excess population risks that areonly dependent on the Gaussian width of the constraint set rather than thedimension of the space. Moreover, we also show the bound for strongly convexfunctions is optimal up to a logarithmic factor. For problems (2) and (3), wepropose several novel algorithms and provide the first theoretical results forboth cases when $1&lt;p&lt;2$ and $2leq pleq infty$.", "output": "Differentially Private Stochastic Convex Optimization in (Non)-Euclidean Space Revisited."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Additive manufacturing has been recognized as an industrial technologicalrevolution for manufacturing, which allows fabrication of materials withcomplex three-dimensional (3D) structures directly from computer-aided designmodels. The mechanical properties of interpenetrating phase composites (IPCs),especially response to dynamic loading, highly depend on their 3D structures.In general, for each specified structural design, it could take hours or daysto perform either finite element analysis (FEA) or experiments to test themechanical response of IPCs to a given dynamic load. To accelerate thephysics-based prediction of mechanical properties of IPCs for variousstructural designs, we employ a deep neural operator (DNO) to learn thetransient response of IPCs under dynamic loading as surrogate of physics-basedFEA models. We consider a 3D IPC beam formed by two metals with a ratio ofYoung's modulus of 2.7, wherein random blocks of constituent materials are usedto demonstrate the generality and robustness of the DNO model. To obtain FEAresults of IPC properties, 5,000 random time-dependent strain loads generatedby a Gaussian process kennel are applied to the 3D IPC beam, and the reactionforces and stress fields inside the IPC beam under various loading arecollected. Subsequently, the DNO model is trained using an incremental learningmethod with sequence-to-sequence training implemented in JAX, leading to a 100Xspeedup compared to widely used vanilla deep operator network models. After anoffline training, the DNO model can act as surrogate of physics-based FEA topredict the transient mechanical response in terms of reaction force and stressdistribution of the IPCs to various strain loads in one second at an accuracyof 98%. Also, the learned operator is able to provide extended prediction ofthe IPC beam subject to longer random strain loads at a reasonably wellaccuracy.", "output": "Deep neural operator for learning transient response of interpenetrating phase composites subject to dynamic loading."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Network structures underlie the dynamics of many complex phenomena, from generegulation and foodwebs to power grids and social media. Yet, as they oftencannot be observed directly, their connectivities must be inferred fromobservations of their emergent dynamics. In this work we present a powerful andfast computational method to infer large network adjacency matrices from timeseries data using a neural network. Using a neural network provides uncertaintyquantification on the prediction in a manner that reflects both thenon-convexity of the inference problem as well as the noise on the data. Thisis useful since network inference problems are typically underdetermined, and afeature that has hitherto been lacking from network inference methods. Wedemonstrate our method's capabilities by inferring line failure locations inthe British power grid from observations of its response to a power cut. Sincethe problem is underdetermined, many classical statistical tools (e.g.regression) will not be straightforwardly applicable. Our method, in contrast,provides probability densities on each edge, allowing the use of hypothesistesting to make meaningful probabilistic statements about the location of thepower cut. We also demonstrate our method's ability to learn an entire costmatrix for a non-linear model from a dataset of economic activity in GreaterLondon. Our method outperforms OLS regression on noisy data in terms of bothspeed and prediction accuracy, and scales as $N^2$ where OLS is cubic. Sinceour technique is not specifically engineered for network inference, itrepresents a general parameter estimation scheme that is applicable to anyparameter dimension.", "output": "Inferring networks from time series: a neural approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This White Paper sets out to explain the value that metamodelling can bringto air traffic management (ATM) research. It will define metamodelling andexplore what it can, and cannot, do. The reader is assumed to have basicknowledge of SESAR: the Single European Sky ATM Research project. An importantelement of SESAR, as the technological pillar of the Single European Skyinitiative, is to bring about improvements, as measured through specific keyperformance indicators (KPIs), and as implemented by a series of so-calledSESAR 'Solutions'. These 'Solutions' are new or improved operational proceduresor technologies, designed to meet operational and performance improvementsdescribed in the European ATM Master Plan.", "output": "NOSTROMO: Lessons learned, conclusions and way forward."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Analogical inference is a remarkable capability of human reasoning, and hasbeen used to solve hard reasoning tasks. Analogy based reasoning (AR) hasgained increasing interest from the artificial intelligence community and hasshown its potential in multiple machine learning tasks such as classification,decision making and recommendation with competitive results. We propose a deeplearning (DL) framework to address and tackle two key tasks in AR: analogydetection and solving. The framework is thoroughly tested on the Siganalogiesdataset of morphological analogical proportions (APs) between words, and shownto outperform symbolic approaches in many languages. Previous work haveexplored the behavior of the Analogy Neural Network for classification (ANNc)on analogy detection and of the Analogy Neural Network for retrieval (ANNr) onanalogy solving by retrieval, as well as the potential of an autoencoder (AE)for analogy solving by generating the solution word. In this article wesummarize these findings and we extend them by combining ANNr and the AEembedding model, and checking the performance of ANNc as an retrieval method.The combination of ANNr and AE outperforms the other approaches in almost allcases, and ANNc as a retrieval method achieves competitive or betterperformance than 3CosMul. We conclude with general guidelines on using ourframework to tackle APs with DL.", "output": "Solving morphological analogies: from retrieval to generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a second-order method, the Natural Gradient Descent (NGD) has the abilityto accelerate training of neural networks. However, due to the prohibitivecomputational and memory costs of computing and inverting the FisherInformation Matrix (FIM), efficient approximations are necessary to make NGDscalable to Deep Neural Networks (DNNs). Many such approximations have beenattempted. The most sophisticated of these is KFAC, which approximates the FIMas a block-diagonal matrix, where each block corresponds to a layer of theneural network. By doing so, KFAC ignores the interactions between differentlayers. In this work, we investigate the interest of restoring somelow-frequency interactions between the layers by means of two-level methods.Inspired from domain decomposition, several two-level corrections to KFAC usingdifferent coarse spaces are proposed and assessed. The obtained results showthat incorporating the layer interactions in this fashion does not reallyimprove the performance of KFAC. This suggests that it is safe to discard theoff-diagonal blocks of the FIM, since the block-diagonal approach issufficiently robust, accurate and economical in computation time.", "output": "Analysis and Comparison of Two-Level KFAC Methods for Training Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As geospatial machine learning models and maps derived from their predictionsare increasingly used for downstream analyses in science and policy, it isimperative to evaluate their accuracy and applicability. Geospatial machinelearning has key distinctions from other learning paradigms, and as such, thecorrect way to measure performance of spatial machine learning outputs has beena topic of debate. In this paper, I delineate unique challenges of modelevaluation for geospatial machine learning with global or remotely senseddatasets, culminating in concrete takeaways to improve evaluations ofgeospatial model performance.", "output": "Evaluation Challenges for Geospatial ML."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "English is the most widely spoken language in the world, used daily bymillions of people as a first or second language in many different contexts. Asa result, there are many varieties of English. Although the great many advancesin English automatic speech recognition (ASR) over the past decades, resultsare usually reported based on test datasets which fail to represent thediversity of English as spoken today around the globe. We present the firstrelease of The Edinburgh International Accents of English Corpus (EdAcc). Thisdataset attempts to better represent the wide diversity of English,encompassing almost 40 hours of dyadic video call conversations betweenfriends. Unlike other datasets, EdAcc includes a wide range of first andsecond-language varieties of English and a linguistic background profile ofeach speaker. Results on latest public, and commercial models show that EdAcchighlights shortcomings of current English ASR models. The best performingmodel, trained on 680 thousand hours of transcribed data, obtains an average of19.7% word error rate (WER) -- in contrast to the 2.7% WER obtained whenevaluated on US English clean read speech. Across all models, we observe a dropin performance on Indian, Jamaican, and Nigerian English speakers. Recordings,linguistic backgrounds, data statement, and evaluation scripts are released onour website ( under CC-BY-SA license.", "output": "The Edinburgh International Accents of English Corpus: Towards the Democratization of English ASR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Average-K classification is an alternative to top-K classification in whichthe number of labels returned varies with the ambiguity of the input image butmust average to K over all the samples. A simple method to solve this task isto threshold the softmax output of a model trained with the cross-entropy loss.This approach is theoretically proven to be asymptotically consistent, but itis not guaranteed to be optimal for a finite set of samples. In this paper, wepropose a new loss function based on a multi-label classification head inaddition to the classical softmax. This second head is trained usingpseudo-labels generated by thresholding the softmax head while guaranteeingthat K classes are returned on average. We show that this approach allows themodel to better capture ambiguities between classes and, as a result, to returnmore consistent sets of possible classes. Experiments on two datasets from theliterature demonstrate that our approach outperforms the softmax baseline, aswell as several other loss functions more generally designed for weaklysupervised multi-label classification. The gains are larger the higher theuncertainty, especially for classes with few samples.", "output": "A two-head loss function for deep Average-K classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) are vulnerable to adversarial examples, which maylead to catastrophe in security-critical domains. Numerous detection methodsare proposed to characterize the feature uniqueness of adversarial examples, orto distinguish DNN's behavior activated by the adversarial examples. Detectionsbased on features cannot handle adversarial examples with large perturbations.Besides, they require a large amount of specific adversarial examples. Anothermainstream, model-based detections, which characterize input properties bymodel behaviors, suffer from heavy computation cost. To address the issues, weintroduce the concept of local gradient, and reveal that adversarial exampleshave a quite larger bound of local gradient than the benign ones. Inspired bythe observation, we leverage local gradient for detecting adversarial examples,and propose a general framework AdvCheck. Specifically, by calculating thelocal gradient from a few benign examples and noise-added misclassifiedexamples to train a detector, adversarial examples and even misclassifiednatural inputs can be precisely distinguished from benign ones. Throughextensive experiments, we have validated the AdvCheck's superior performance tothe state-of-the-art (SOTA) baselines, with detection rate ($sim times 1.2$)on general adversarial attacks and ($sim times 1.4$) on misclassified naturalinputs on average, with average 1/500 time cost. We also provide interpretableresults for successful detection.", "output": "AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model extraction attacks have been widely applied, which can normally be usedto recover confidential parameters of neural networks for multiple layers.Recently, side-channel analysis of neural networks allows parameter extractioneven for networks with several multiple deep layers with high effectiveness. Itis therefore of interest to implement a certain level of protection againstthese attacks. In this paper, we propose a desynchronization-basedcountermeasure that makes the timing analysis of activation functions harder.We analyze the timing properties of several activation functions and design thedesynchronization in a way that the dependency on the input and the activationtype is hidden. We experimentally verify the effectiveness of thecountermeasure on a 32-bit ARM Cortex-M4 microcontroller and employ a t-test toshow the side-channel information leakage. The overhead ultimately depends onthe number of neurons in the fully-connected layer, for example, in the case of4096 neurons in VGG-19, the overheads are between 2.8% and 11%.", "output": "A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In smart electrical grids, fault detection tasks may have a high impact onsociety due to their economic and critical implications. In the recent years,numerous smart grid applications, such as defect detection and loadforecasting, have embraced data-driven methodologies. The purpose of this studyis to investigate the challenges associated with the security of machinelearning (ML) applications in the smart grid scenario. Indeed, the robustnessand security of these data-driven algorithms have not been extensively studiedin relation to all power grid applications. We demonstrate first that the deepneural network method used in the smart grid is susceptible to adversarialperturbation. Then, we highlight how studies on fault localization and typeclassification illustrate the weaknesses of present ML algorithms in smartgrids to various adversarial attacks", "output": "Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As various forms of fraud proliferate on Ethereum, it is imperative tosafeguard against these malicious activities to protect susceptible users frombeing victimized. While current studies solely rely on graph-based frauddetection approaches, it is argued that they may not be well-suited for dealingwith highly repetitive, skew-distributed and heterogeneous Ethereumtransactions. To address these challenges, we propose BERT4ETH, a universalpre-trained Transformer encoder that serves as an account representationextractor for detecting various fraud behaviors on Ethereum. BERT4ETH featuresthe superior modeling capability of Transformer to capture the dynamicsequential patterns inherent in Ethereum transactions, and addresses thechallenges of pre-training a BERT model for Ethereum with three practical andeffective strategies, namely repetitiveness reduction, skew alleviation andheterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETHoutperforms state-of-the-art methods with significant enhancements in terms ofthe phishing account detection and de-anonymization tasks. The code forBERT4ETH is available at: ", "output": "BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current trends in networking propose the use of Machine Learning (ML) for awide variety of network optimization tasks. As such, many efforts have beenmade to produce ML-based solutions for Traffic Engineering (TE), which is afundamental problem in ISP networks. Nowadays, state-of-the-art TE optimizersrely on traditional optimization techniques, such as Local search, ConstraintProgramming, or Linear programming. In this paper, we present MAGNNETO, adistributed ML-based framework that leverages Multi-Agent ReinforcementLearning and Graph Neural Networks for distributed TE optimization. MAGNNETOdeploys a set of agents across the network that learn and communicate in adistributed fashion via message exchanges between neighboring agents.Particularly, we apply this framework to optimize link weights in OSPF, withthe goal of minimizing network congestion. In our evaluation, we compareMAGNNETO against several state-of-the-art TE optimizers in more than 75topologies (up to 153 nodes and 354 links), including realistic traffic loads.Our experimental results show that, thanks to its distributed nature, MAGNNETOachieves comparable performance to state-of-the-art TE optimizers withsignificantly lower execution times. Moreover, our ML-based solutiondemonstrates a strong generalization capability to successfully operate in newnetworks unseen during training.", "output": "MAGNNETO: A Graph Neural Network-based Multi-Agent system for Traffic Engineering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimization problems involving minimization of a rank-one convex functionover constraints modeling restrictions on the support of the decision variablesemerge in various machine learning applications. These problems are oftenmodeled with indicator variables for identifying the support of the continuousvariables. In this paper we investigate compact extended formulations for suchproblems through perspective reformulation techniques. In contrast to themajority of previous work that relies on support function arguments anddisjunctive programming techniques to provide convex hull results, we propose aconstructive approach that exploits a hidden conic structure induced byperspective functions. To this end, we first establish a convex hull result fora general conic mixed-binary set in which each conic constraint involves alinear function of independent continuous variables and a set of binaryvariables. We then demonstrate that extended representations of sets associatedwith epigraphs of rank-one convex functions over constraints modeling indicatorrelations naturally admit such a conic representation. This enables us tosystematically give perspective formulations for the convex hull descriptionsof these sets with nonlinear separable or non-separable objective functions,sign constraints on continuous variables, and combinatorial constraints onindicator variables. We illustrate the efficacy of our results on sparsenonnegative logistic regression problems.", "output": "Constrained Optimization of Rank-One Functions with Indicator Variables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised Continual learning involves updating a deep neural network (DNN)from an ever-growing stream of labeled data. While most work has focused onovercoming catastrophic forgetting, one of the major motivations behindcontinual learning is being able to efficiently update a network with newinformation, rather than retraining from scratch on the training dataset as itgrows over time. Despite recent continual learning methods largely solving thecatastrophic forgetting problem, there has been little attention paid to theefficiency of these algorithms. Here, we study recent methods for incrementalclass learning and illustrate that many are highly inefficient in terms ofcompute, memory, and storage. Some methods even require more compute thantraining from scratch! We argue that for continual learning to have real-worldapplicability, the research community cannot ignore the resources used by thesealgorithms. There is more to continual learning than mitigating catastrophicforgetting.", "output": "How Efficient Are Today's Continual Learning Algorithms?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vertical federated learning (VFL) enables a service provider (i.e., activeparty) who owns labeled features to collaborate with passive parties whopossess auxiliary features to improve model performance. Existing VFLapproaches, however, have two major vulnerabilities when passive partiesunexpectedly quit in the deployment phase of VFL - severe performancedegradation and intellectual property (IP) leakage of the active party'slabels. In this paper, we propose textbf{Party-wise Dropout} to improve theVFL model's robustness against the unexpected exit of passive parties and adefense method called textbf{DIMIP} to protect the active party's IP in thedeployment phase. We evaluate our proposed methods on multiple datasets againstdifferent inference attacks. The results show that Party-wise Dropouteffectively maintains model performance after the passive party quits, andDIMIP successfully disguises label information from the passive party's featureextractor, thereby mitigating IP leakage.", "output": "Robust and IP-Protecting Vertical Federated Learning against Unexpected Quitting of Parties."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale diffusion models like Stable Diffusion are powerful and findvarious real-world applications while customizing such models by fine-tuning isboth memory and time inefficient. Motivated by the recent progress in naturallanguage processing, we investigate parameter-efficient tuning in largediffusion models by inserting small learnable modules (termed adapters). Inparticular, we decompose the design space of adapters into orthogonal factors-- the input position, the output position as well as the function form, andperform Analysis of Variance (ANOVA), a classical statistical approach foranalyzing the correlation between discrete (design options) and continuousvariables (evaluation metrics). Our analysis suggests that the input positionof adapters is the critical factor influencing the performance of downstreamtasks. Then, we carefully study the choice of the input position, and we findthat putting the input position after the cross-attention block can lead to thebest performance, validated by additional visualization analyses. Finally, weprovide a recipe for parameter-efficient tuning in diffusion models, which iscomparable if not superior to the fully fine-tuned baseline (e.g., DreamBooth)with only 0.75 % extra parameters, across various customized tasks.", "output": "A Closer Look at Parameter-Efficient Tuning in Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop a novel credit assignment algorithm for information processingwith spiking neurons without requiring feedback synapses. Specifically, wepropose an event-driven generalization of the forward-forward and thepredictive forward-forward learning processes for a spiking neural system thatiteratively processes sensory input over a stimulus window. As a result, therecurrent circuit computes the membrane potential of each neuron in each layeras a function of local bottom-up, top-down, and lateral signals, facilitating adynamic, layer-wise parallel form of neural computation. Unlike spiking neuralcoding, which relies on feedback synapses to adjust neural electrical activity,our model operates purely online and forward in time, offering a promising wayto learn distributed representations of sensory data patterns with temporalspike signals. Notably, our experimental results on several pattern datasetsdemonstrate that the even-driven forward-forward (ED-FF) framework works wellfor training a dynamic recurrent spiking system capable of both classificationand reconstruction.", "output": "Learning Spiking Neural Systems with the Event-Driven Forward-Forward Process."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are proven to be vulnerable to backdoor attacks.Detecting the trigger samples during the inference stage, i.e., the test-timetrigger sample detection, can prevent the backdoor from being triggered.However, existing detection methods often require the defenders to have highaccessibility to victim models, extra clean data, or knowledge about theappearance of backdoor triggers, limiting their practicality. In this paper, wepropose the test-time corruption robustness consistency evaluation (TeCo), anovel test-time trigger sample detection method that only needs the hard-labeloutputs of the victim models without any extra information. Our journey beginswith the intriguing observation that the backdoor-infected models have similarperformance across different image corruptions for the clean images, butperform discrepantly for the trigger samples. Based on this phenomenon, wedesign TeCo to evaluate test-time robustness consistency by calculating thedeviation of severity that leads to predictions' transition across differentcorruptions. Extensive experiments demonstrate that compared withstate-of-the-art defenses, which even require either certain information aboutthe trigger types or accessibility of clean data, TeCo outperforms them ondifferent backdoor attacks, datasets, and model architectures, enjoying ahigher AUROC by 10% and 5 times of stability.", "output": "Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advances in 3D-aware generative models have pushed the boundary of imagesynthesis with explicit camera control. To achieve high-resolution imagesynthesis, several attempts have been made to design efficient generators, suchas hybrid architectures with both 3D and 2D components. However, such a designcompromises multiview consistency, and the design of a pure 3D generator withhigh resolution is still an open problem. In this work, we present GenerativeVolumetric Primitives (GVP), the first pure 3D generative model that can sampleand render 512-resolution images in real-time. GVP jointly models a number ofvolumetric primitives and their spatial information, both of which can beefficiently generated via a 2D convolutional network. The mixture of theseprimitives naturally captures the sparsity and correspondence in the 3D volume.The training of such a generator with a high degree of freedom is made possiblethrough a knowledge distillation technique. Experiments on several datasetsdemonstrate superior efficiency and 3D consistency of GVP over thestate-of-the-art.", "output": "GVP: Generative Volumetric Primitives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data privacy and ownership are significant in social data science, raisinglegal and ethical concerns. Sharing and analyzing data is difficult whendifferent parties own different parts of it. An approach to this challenge isto apply de-identification or anonymization techniques to the data beforecollecting it for analysis. However, this can reduce data utility and increasethe risk of re-identification. To address these limitations, we present PADME,a distributed analytics tool that federates model implementation and training.PADME uses a federated approach where the model is implemented and deployed byall parties and visits each data location incrementally for training. Thisenables the analysis of data across locations while still allowing the model tobe trained as if all data were in a single location. Training the model on datain its original location preserves data ownership. Furthermore, the results arenot provided until the analysis is completed on all data locations to ensureprivacy and avoid bias in the results.", "output": "PADME-SoSci: A Platform for Analytics and Distributed Machine Learning for the Social Sciences."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, with the rapid deployment of service APIs, personalized servicerecommendations have played a paramount role in the growth of the e-commerceindustry. Quality-of-Service (QoS) parameters determining the serviceperformance, often used for recommendation, fluctuate over time. Thus, the QoSprediction is essential to identify a suitable service among functionallyequivalent services over time. The contemporary temporal QoS prediction methodshardly achieved the desired accuracy due to various limitations, such as theinability to handle data sparsity and outliers and capture higher-ordertemporal relationships among user-service interactions. Even though some recentrecurrent neural-network-based architectures can model temporal relationshipsamong QoS data, prediction accuracy degrades due to the absence of otherfeatures (e.g., collaborative features) to comprehend the relationship amongthe user-service interactions. This paper addresses the above challenges andproposes a scalable strategy for Temporal QoS Prediction using Multi-sourceCollaborative-Features (TPMCF), achieving high prediction accuracy and fasterresponsiveness. TPMCF combines the collaborative-features of users/services byexploiting user-service relationship with the spatio-temporal auto-extractedfeatures by employing graph convolution and transformer encoder with multi-headself-attention. We validated our proposed method on WS-DREAM-2 datasets.Extensive experiments showed TPMCF outperformed major state-of-the-artapproaches regarding prediction accuracy while ensuring high scalability andreasonably faster responsiveness.", "output": "TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive learning methods have shown an impressive ability to learnmeaningful representations for image or time series classification. However,these methods are less effective for time series forecasting, as optimizationof instance discrimination is not directly applicable to predicting the futurestate from the history context. Moreover, the construction of positive andnegative pairs in current technologies strongly relies on specific time seriescharacteristics, restricting their generalization across diverse types of timeseries data. To address these limitations, we propose SimTS, a simplerepresentation learning approach for improving time series forecasting bylearning to predict the future from the past in the latent space. SimTS doesnot rely on negative pairs or specific assumptions about the characteristics ofthe particular time series. Our extensive experiments on several benchmark timeseries forecasting datasets show that SimTS achieves competitive performancecompared to existing contrastive learning methods. Furthermore, we show theshortcomings of the current contrastive learning framework used for time seriesforecasting through a detailed ablation study. Overall, our work suggests thatSimTS is a promising alternative to other contrastive learning approaches fortime series forecasting.", "output": "SimTS: Rethinking Contrastive Representation Learning for Time Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Additive Noise Models (ANM) encode a popular functional assumption thatenables learning causal structure from observational data. Due to a lack ofreal-world data meeting the assumptions, synthetic ANM data are often used toevaluate causal discovery algorithms. Reisach et al. (2021) show that, forcommon simulation parameters, a variable ordering by increasing variance isclosely aligned with a causal order and introduce var-sortability to quantifythe alignment. Here, we show that not only variance, but also the fraction of avariable's variance explained by all others, as captured by the coefficient ofdetermination $R^2$, tends to increase along the causal order. Simple baselinealgorithms can use $R^2$-sortability to match the performance of establishedmethods. Since $R^2$-sortability is invariant under data rescaling, thesealgorithms perform equally well on standardized or rescaled data, addressing akey limitation of algorithms exploiting var-sortability. We characterize andempirically assess $R^2$-sortability for different simulation parameters. Weshow that all simulation parameters can affect $R^2$-sortability and must bechosen deliberately to control the difficulty of the causal discovery task andthe real-world plausibility of the simulated data. We provide an implementationof the sortability measures and sortability-based algorithms in our libraryCausalDisco (", "output": "Simple Sorting Criteria Help Find the Causal Order in Additive Noise Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electron, optical, and scanning probe microscopy methods are generating everincreasing volume of image data containing information on atomic and mesoscalestructures and functionalities. This necessitates the development of themachine learning methods for discovery of physical and chemical phenomena fromthe data, such as manifestations of symmetry breaking in electron and scanningtunneling microscopy images, variability of the nanoparticles. Variationalautoencoders (VAEs) are emerging as a powerful paradigm for the unsuperviseddata analysis, allowing to disentangle the factors of variability and discoveroptimal parsimonious representation. Here, we summarize recent developments inVAEs, covering the basic principles and intuition behind the VAEs. Theinvariant VAEs are introduced as an approach to accommodate scale andtranslation invariances present in imaging data and separate known factors ofvariations from the ones to be discovered. We further describe theopportunities enabled by the control over VAE architecture, includingconditional, semi-supervised, and joint VAEs. Several case studies of VAEapplications for toy models and experimental data sets in Scanning TransmissionElectron Microscopy are discussed, emphasizing the deep connection between VAEand basic physical principles. All the codes used here are available at and this article can be used as anapplication guide when applying these to own data sets.", "output": "Physics and Chemistry from Parsimonious Representations: Image Analysis via Invariant Variational Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the largest and most comprehensive empirical study of pre-trainedvisual representations (PVRs) or visual 'foundation models' for Embodied AI.First, we curate CortexBench, consisting of 17 different tasks spanninglocomotion, navigation, dexterous, and mobile manipulation. Next, wesystematically evaluate existing PVRs and find that none are universallydominant.To study the effect of pre-training data scale and diversity, we combine over4,000 hours of egocentric videos from 7 different sources (over 5.6M images)and ImageNet to train different-sized vision transformers using MaskedAuto-Encoding (MAE) on slices of this data. Contrary to inferences from priorwork, we find that scaling dataset size and diversity does not improveperformance universally (but does so on average).Our largest model, named VC-1, outperforms all prior PVRs on average but doesnot universally dominate either. Finally, we show that task or domain-specificadaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achievingcompetitive or superior performance than the best known results on all of thebenchmarks in CortexBench. These models required over 10,000 GPU-hours to trainand can be found on our website for the benefit of the research community.", "output": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce $infty$-Diff, a generative diffusion model which directlyoperates on infinite resolution data. By randomly sampling subsets ofcoordinates during training and learning to denoise the content at thosecoordinates, a continuous function is learned that allows sampling at arbitraryresolutions. In contrast to other recent infinite resolution generative models,our approach operates directly on the raw data, not requiring latent vectorcompression for context, using hypernetworks, nor relying on discretecomponents. As such, our approach achieves significantly higher sample quality,as evidenced by lower FID scores, as well as being able to effectively scale tohigher resolutions than the training data while retaining detail.", "output": "$\\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To improve how neural networks function it is crucial to understand theirlearning process. The information bottleneck theory of deep learning proposesthat neural networks achieve good generalization by compressing theirrepresentations to disregard information that is not relevant to the task.However, empirical evidence for this theory is conflicting, as compression wasonly observed when networks used saturating activation functions. In contrast,networks with non-saturating activation functions achieved comparable levels oftask performance but did not show compression. In this paper we developed morerobust mutual information estimation techniques, that adapt to hidden activityof neural networks and produce more sensitive measurements of activations fromall functions, especially unbounded functions. Using these adaptive estimationtechniques, we explored compression in networks with a range of differentactivation functions. With two improved methods of estimation, firstly, we showthat saturation of the activation function is not required for compression, andthe amount of compression varies between different activation functions. Wealso find that there is a large amount of variation in compression betweendifferent network initializations. Secondary, we see that L2 regularizationleads to significantly increased compression, while preventing overfitting.Finally, we show that only compression of the last layer is positivelycorrelated with generalization.", "output": "Adaptive Estimators Show Information Compression in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent work has shown potential in using Mixed Integer Programming (MIP)solvers to optimize certain aspects of neural networks (NNs). However theintriguing approach of training NNs with MIP solvers is under-explored.State-of-the-art-methods to train NNs are typically gradient-based and requiresignificant data, computation on GPUs, and extensive hyper-parameter tuning. Incontrast, training with MIP solvers does not require GPUs or heavyhyper-parameter tuning, but currently cannot handle anything but small amountsof data. This article builds on recent advances that train binarized NNs usingMIP solvers. We go beyond current work by formulating new MIP models whichimprove training efficiency and which can train the important class ofinteger-valued neural networks (INNs). We provide two novel methods to furtherthe potential significance of using MIP to train NNs. The first methodoptimizes the number of neurons in the NN while training. This reduces the needfor deciding on network architecture before training. The second methodaddresses the amount of training data which MIP can feasibly handle: we providea batch training method that dramatically increases the amount of data that MIPsolvers can use to train. We thus provide a promising step towards using muchmore data than before when training NNs using MIP models. Experimental resultson two real-world data-limited datasets demonstrate that our approach stronglyoutperforms the previous state of the art in training NN with MIP, in terms ofaccuracy, training time and amount of data. Our methodology is proficient attraining NNs when minimal training data is available, and at training withminimal memory requirements -- which is potentially valuable for deploying tolow-memory devices.", "output": "Optimal training of integer-valued neural networks with mixed integer programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of causal inference with panel data is a central econometricquestion. The following is a fundamental version of this problem: Let $M^*$ bea low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix$Z$ with entries in ${0,1}$ we observe the matrix $O$ with entries $O_{ij} :=M^*_{ij} + E_{ij} + mathcal{T}_{ij} Z_{ij}$ where $mathcal{T}_{ij} $ areunknown, heterogenous treatment effects. The problem requires we estimate theaverage treatment effect $tau^* := sum_{ij} mathcal{T}_{ij} Z_{ij} /sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach toestimating $tau^*$ when $Z$ places support on a single row. This paper extendsthat framework to allow rate-optimal recovery of $tau^*$ for general $Z$, thusbroadly expanding its applicability. Our guarantees are the first of their typein this general setting. Computational experiments on synthetic and real-worlddata show a substantial advantage over competing estimators.", "output": "Learning Treatment Effects in Panels with General Intervention Patterns."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently published graph neural networks (GNNs) show promising performance atsocial event detection tasks. However, most studies are oriented towardmonolingual data in languages with abundant training samples. This has left themore common multilingual settings and lesser-spoken languages relativelyunexplored. Thus, we present a GNN that incorporates cross-lingual wordembeddings for detecting events in multilingual data streams. The first exploitis to make the GNN work with multilingual data. For this, we outline aconstruction strategy that aligns messages in different languages at both thenode and semantic levels. Relationships between messages are established bymerging entities that are the same but are referred to in different languages.Non-English message representations are converted into English semantic spacevia the cross-lingual word embeddings. The resulting message graph is thenuniformly encoded by a GNN model. In special cases where a lesser-spokenlanguage needs to be detected, a novel cross-lingual knowledge distillationframework, called CLKD, exploits prior knowledge learned from similar threadsin English to make up for the paucity of annotated data. Experiments on bothsynthetic and real-world datasets show the framework to be highly effective atdetection in both multilingual data and in languages where training samples arescarce.", "output": "Transferring Knowledge Distillation for Multilingual Social Event Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop a new framework for embedding joint probability distributions intensor product reproducing kernel Hilbert spaces (RKHS). Our frameworkaccommodates a low-dimensional, normalized and positive model of aRadon-Nikodym derivative, which we estimate from sample sizes of up to severalmillion data points, alleviating the inherent limitations of RKHS modeling.Well-defined normalized and positive conditional distributions are naturalby-products to our approach. The embedding is fast to compute and accommodateslearning problems ranging from prediction to classification. Our theoreticalfindings are supplemented by favorable numerical results.", "output": "Adaptive joint distribution learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have demonstrated the effectiveness of the combination ofmachine learning and logical reasoning, including data-driven logicalreasoning, knowledge driven machine learning and abductive learning, ininventing advanced technologies for different artificial intelligenceapplications. One-step abductive multi-target learning (OSAMTL), an approachinspired by abductive learning, via simply combining machine learning andlogical reasoning in a one-step balanced multi-target learning way, has as wellshown its effectiveness in handling complex noisy labels of a single noisysample in medical histopathology whole slide image analysis (MHWSIA). However,OSAMTL is not suitable for the situation where diverse noisy samples (DiNS) areprovided for a learning task. In this paper, giving definition of DiNS, wepropose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) toexpand the original OSAMTL to handle complex noisy labels of DiNS. ApplyingOSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show thatOSAMTL-DiNS is able to enable various state-of-the-art approaches for learningfrom noisy labels to achieve more rational predictions. We released a modelpretrained with OSAMTL-DiNS for tumour segmentation in HE-stained pre-treatmentbiopsy images in breast cancer, which has been successfully applied as apre-processing tool to extract tumour-associated stroma compartment forpredicting the pathological complete response to neoadjuvant chemotherapy inbreast cancer.", "output": "One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep generative models (DGMs) have the potential to revolutionize diagnosticimaging. Generative adversarial networks (GANs) are one kind of DGM which arewidely employed. The overarching problem with deploying GANs, and other DGMs,in any application that requires domain expertise in order to actually use thegenerated images is that there generally is not adequate or automatic means ofassessing the domain-relevant quality of generated images. In this work, wedemonstrate several objective tests of images output by two popular GANarchitectures. We designed several stochastic context models (SCMs) of distinctimage features that can be recovered after generation by a trained GAN. Severalof these features are high-order, algorithmic pixel-arrangement rules which arenot readily expressed in covariance matrices. We designed and validatedstatistical classifiers to detect specific effects of the known arrangementrules. We then tested the rates at which two different GANs correctlyreproduced the feature context under a variety of training scenarios, anddegrees of feature-class similarity. We found that ensembles of generatedimages can appear largely accurate visually, and show high accuracy in ensemblemeasures, while not exhibiting the known spatial arrangements. Furthermore,GANs trained on a spectrum of distinct spatial orders did not respect the givenprevalence of those orders in the training data. The main conclusion is thatSCMs can be engineered to quantify numerous errors, per image, that may not becaptured in ensemble statistics but plausibly can affect subsequent use of theGAN-generated images.", "output": "A Method for Evaluating Deep Generative Models of Images via Assessing the Reproduction of High-order Spatial Context."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This manuscript proposes to extend the information set of time-seriesregression trees with latent stationary factors extracted via state-spacemethods. In doing so, this approach generalises time-series regression trees ontwo dimensions. First, it allows to handle predictors that exhibit measurementerror, non-stationary trends, seasonality and/or irregularities such as missingobservations. Second, it gives a transparent way for using domain-specifictheory to inform time-series regression trees. Empirically, ensembles of thesefactor-augmented trees provide a reliable approach for macro-finance problems.This article highlights it focussing on the lead-lag effect between equityvolatility and the business cycle in the United States.", "output": "Factor-augmented tree ensembles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper resolves the open question of designing near-optimal algorithmsfor learning imperfect-information extensive-form games from bandit feedback.We present the first line of algorithms that require only$widetilde{mathcal{O}}((XA+YB)/varepsilon^2)$ episodes of play to find an$varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where$X,Y$ are the number of information sets and $A,B$ are the number of actionsfor the two players. This improves upon the best known sample complexity of$widetilde{mathcal{O}}((X^2A+Y^2B)/varepsilon^2)$ by a factor of$widetilde{mathcal{O}}(max{X, Y})$, and matches the information-theoreticlower bound up to logarithmic factors. We achieve this sample complexity by twonew algorithms: Balanced Online Mirror Descent, and Balanced CounterfactualRegret Minimization. Both algorithms rely on novel approaches of integratingemph{balanced exploration policies} into their classical counterparts. We alsoextend our results to learning Coarse Correlated Equilibria in multi-playergeneral-sum games.", "output": "Near-Optimal Learning of Extensive-Form Games with Imperfect Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The static optimal transport $(mathrm{OT})$ problem between Gaussians seeksto recover an optimal map, or more generally a coupling, to morph a Gaussianinto another. It has been well studied and applied to a wide variety of tasks.Here we focus on the dynamic formulation of OT, also known as the Schr\"odingerbridge (SB) problem, which has recently seen a surge of interest in machinelearning due to its connections with diffusion-based generative models. Incontrast to the static setting, much less is known about the dynamic setting,even for Gaussian distributions. In this paper, we provide closed-formexpressions for SBs between Gaussian measures. In contrast to the staticGaussian OT problem, which can be simply reduced to studying convex programs,our framework for solving SBs requires significantly more involved tools suchas Riemannian geometry and generator theory. Notably, we establish that thesolutions of SBs between Gaussian measures are themselves Gaussian processeswith explicit mean and covariance kernels, and thus are readily amenable formany downstream applications such as generative modeling or interpolation. Todemonstrate the utility, we devise a new method for modeling the evolution ofsingle-cell genomics data and report significantly improved numerical stabilitycompared to existing SB-based approaches.", "output": "The Schr\\\"odinger Bridge between Gaussian Measures has a Closed Form."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evaluation of explanation methods is a research topic that has not yetbeen explored deeply, however, since explainability is supposed to strengthentrust in artificial intelligence, it is necessary to systematically review andcompare explanation methods in order to confirm their correctness. Until now,no tool with focus on XAI evaluation exists that exhaustively and speedilyallows researchers to evaluate the performance of explanations of neuralnetwork predictions. To increase transparency and reproducibility in the field,we therefore built Quantus -- a comprehensive, evaluation toolkit in Pythonthat includes a growing, well-organised collection of evaluation metrics andtutorials for evaluating explainable methods. The toolkit has been thoroughlytested and is available under an open-source license on PyPi (or on", "output": "Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose VRL3, a powerful data-driven framework with a simple design forsolving challenging visual deep reinforcement learning (DRL) tasks. We analyzea number of major obstacles in taking a data-driven approach, and present asuite of design principles, novel findings, and critical insights aboutdata-driven visual DRL. Our framework has three stages: in stage 1, we leveragenon-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations;in stage 2, we use offline RL data (e.g. a limited number of expertdemonstrations) to convert the task-agnostic representations into more powerfultask-specific representations; in stage 3, we fine-tune the agent with onlineRL. On a set of challenging hand manipulation tasks with sparse reward andrealistic visual inputs, compared to the previous SOTA, VRL3 achieves anaverage of 780% better sample efficiency. And on the hardest task, VRL3 is1220% more sample efficient (2440% when using a wider encoder) and solves thetask with only 10% of the computation. These significant results clearlydemonstrate the great potential of data-driven deep reinforcement learning.", "output": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of fitting a model to a dynamical environment when newmodes of behavior emerge sequentially. The learning model is aware when a newmode appears, but it does not have access to the true modes of individualtraining sequences. The state-of-the-art continual learning approaches cannothandle this setup, because parameter transfer suffers from catastrophicinterference and episodic memory design requires the knowledge of theground-truth modes of sequences. We devise a novel continual learning methodthat overcomes both limitations by maintaining a descriptor of the mode of anencountered sequence in a neural episodic memory. We employ a Dirichlet Processprior on the attention weights of the memory to foster efficient storage of themode descriptors. Our method performs continual learning by transferringknowledge across tasks by retrieving the descriptors of similar modes of pasttasks to the mode of a current sequence and feeding this descriptor into itstransition kernel as control input. We observe the continual learningperformance of our method to compare favorably to the mainstream parametertransfer approach.", "output": "Continual Learning of Multi-modal Dynamics with External Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, the use of WiFi fingerprints for indoor positioning hasgrown in popularity, largely due to the widespread availability of WiFi and theproliferation of mobile communication devices. However, many existing methodsfor constructing fingerprint datasets rely on labor-intensive andtime-consuming processes of collecting large amounts of data. Additionally,these methods often focus on ideal laboratory environments, rather thanconsidering the practical challenges of large multi-floor buildings. To addressthese issues, we present a novel WiDAGCN model that can be trained using asmall number of labeled site survey data and large amounts of unlabeledcrowdsensed WiFi fingerprints. By constructing heterogeneous graphs based onreceived signal strength indicators (RSSIs) between waypoints and WiFi accesspoints (APs), our model is able to effectively capture the topologicalstructure of the data. We also incorporate graph convolutional networks (GCNs)to extract graph-level embeddings, a feature that has been largely overlookedin previous WiFi indoor localization studies. To deal with the challenges oflarge amounts of unlabeled data and multiple data domains, we employ asemi-supervised domain adversarial training scheme to effectively utilizeunlabeled data and align the data distributions across domains. Our system isevaluated using a public indoor localization dataset that includes multiplebuildings, and the results show that it performs competitively in terms oflocalization accuracy in large buildings.", "output": "Domain Adversarial Graph Convolutional Network Based on RSSI and Crowdsensing for Indoor Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time-dependent data-generating distributions have proven to be difficult forgradient-based training of neural networks, as the greedy updates result incatastrophic forgetting of previously learned knowledge. Despite the progressin the field of continual learning to overcome this forgetting, we show that aset of common state-of-the-art methods still suffers from substantialforgetting upon starting to learn new tasks, except that this forgetting istemporary and followed by a phase of performance recovery. We refer to thisintriguing but potentially problematic phenomenon as the stability gap. Thestability gap had likely remained under the radar due to standard practice inthe field of evaluating continual learning models only after each task.Instead, we establish a framework for continual evaluation that usesper-iteration evaluation and we define a new set of metrics to quantifyworst-case performance. Empirically we show that experience replay,constraint-based replay, knowledge-distillation, and parameter regularizationmethods are all prone to the stability gap; and that the stability gap can beobserved in class-, task-, and domain-incremental learning benchmarks.Additionally, a controlled experiment shows that the stability gap increaseswhen tasks are more dissimilar. Finally, by disentangling gradients intoplasticity and stability components, we propose a conceptual explanation forthe stability gap.", "output": "Continual evaluation for lifelong learning: Identifying the stability gap."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study two of the most popular performance metrics in medical imagesegmentation, Accuracy and Dice, when the target labels are noisy. For bothmetrics, several statements related to characterization and volume propertiesof the set of optimal segmentations are proved, and associated experiments areprovided. Our main insights are: (i) the volume of the solutions to bothmetrics may deviate significantly from the expected volume of the target, (ii)the volume of a solution to Accuracy is always less than or equal to the volumeof a solution to Dice and (iii) the optimal solutions to both of these metricscoincide when the set of feasible segmentations is constrained to the set ofsegmentations with the volume equal to the expected volume of the target.", "output": "On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimal transport (OT) theory describes general principles to define andselect, among many possible choices, the most efficient way to map aprobability measure onto another. That theory has been mostly used to estimate,given a pair of source and target probability measures $(mu, nu)$, aparameterized map $T_theta$ that can efficiently map $mu$ onto $nu$. In manyapplications, such as predicting cell responses to treatments, pairs ofinput/output data measures $(mu, nu)$ that define optimal transport problemsdo not arise in isolation but are associated with a context $c$, as forinstance a treatment when comparing populations of untreated and treated cells.To account for that context in OT estimation, we introduce CondOT, a multi-taskapproach to estimate a family of OT maps conditioned on a context variable,using several pairs of measures $left(mu_i, nu_iright)$ tagged with acontext label $c_i$. CondOT learns a global map $mathcal{T}_theta$conditioned on context that is not only expected to fit all labeled pairs inthe dataset $left{left(c_i,left(mu_i, nu_iright)right)right}$, i.e.,$mathcal{T}_thetaleft(c_iright) sharp mu_i approx nu_i$, but shouldalso generalize to produce meaningful maps $mathcal{T}_thetaleft(c_{text{new }}right)$ when conditioned on unseen contexts $c_{text {new }}$. Ourapproach harnesses and provides a novel usage for partially input convex neuralnetworks, for which we introduce a robust and efficient initialization strategyinspired by Gaussian approximations. We demonstrate the ability of CondOT toinfer the effect of an arbitrary combination of genetic or therapeuticperturbations on single cells, using only observations of the effects of saidperturbations separately.", "output": "Supervised Training of Conditional Monge Maps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many applications, multiple parties have private data regarding the sameset of users but on disjoint sets of attributes, and a server wants to leveragethe data to train a model. To enable model learning while protecting theprivacy of the data subjects, we need vertical federated learning (VFL)techniques, where the data parties share only information for training themodel, instead of the private data. However, it is challenging to ensure thatthe shared information maintains privacy while learning accurate models. To thebest of our knowledge, the algorithm proposed in this paper is the firstpractical solution for differentially private vertical federated k-meansclustering, where the server can obtain a set of global centers with a provabledifferential privacy guarantee. Our algorithm assumes an untrusted centralserver that aggregates differentially private local centers and membershipencodings from local data parties. It builds a weighted grid as the synopsis ofthe global dataset based on the received information. Final centers aregenerated by running any k-means algorithm on the weighted grid. Our approachfor grid weight estimation uses a novel, light-weight, and differentiallyprivate set intersection cardinality estimation algorithm based on theFlajolet-Martin sketch. To improve the estimation accuracy in the setting withmore than two data parties, we further propose a refined version of the weightsestimation algorithm and a parameter tuning strategy to reduce the finalk-means utility to be close to that in the central private setting. We providetheoretical utility analysis and experimental evaluation results for thecluster centers computed by our algorithm and show that our approach performsbetter both theoretically and empirically than the two baselines based onexisting techniques.", "output": "Differentially Private Vertical Federated Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Small neural networks (NNs) used for error correction were shown to improveon classic channel codes and to address channel model changes. We extend thecode dimension of any such structure by using the same NN under one-hotencoding multiple times, then serially-concatenated with an outer classic code.We design NNs with the same network parameters, where each Reed-Solomoncodeword symbol is an input to a different NN. Significant improvements inblock error probabilities for an additive Gaussian noise channel as compared tothe small neural code are illustrated, as well as robustness to channel modelchanges.", "output": "Concatenated Classic and Neural (CCN) Codes: ConcatenatedAE."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning, especially deep neural networks (DNNs), has been widely andsuccessfully adopted in many critical applications for its high effectivenessand efficiency. The rapid development of DNNs has benefited from the existenceof some high-quality datasets ($e.g.$, ImageNet), which allow researchers anddevelopers to easily verify the performance of their methods. Currently, almostall existing released datasets require that they can only be adopted foracademic or educational purposes rather than commercial purposes withoutpermission. However, there is still no good way to ensure that. In this paper,we formulate the protection of released datasets as verifying whether they areadopted for training a (suspicious) third-party model, where defenders can onlyquery the model while having no information about its parameters and trainingdetails. Based on this formulation, we propose to embed external patterns viabackdoor watermarking for the ownership verification to protect them. Ourmethod contains two main parts, including dataset watermarking and datasetverification. Specifically, we exploit poison-only backdoor attacks ($e.g.$,BadNets) for dataset watermarking and design a hypothesis-test-guided methodfor dataset verification. We also provide some theoretical analyses of ourmethods. Experiments on multiple benchmark datasets of different tasks areconducted, which verify the effectiveness of our method. The code forreproducing main experiments is available aturl{", "output": "Black-box Dataset Ownership Verification via Backdoor Watermarking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a new low-rank matrix factorization model dubbedbounded simplex-structured matrix factorization (BSSMF). Given an input matrix$X$ and a factorization rank $r$, BSSMF looks for a matrix $W$ with $r$ columnsand a matrix $H$ with $r$ rows such that $X approx WH$ where the entries ineach column of $W$ are bounded, that is, they belong to given intervals, andthe columns of $H$ belong to the probability simplex, that is, $H$ is columnstochastic. BSSMF generalizes nonnegative matrix factorization (NMF), andsimplex-structured matrix factorization (SSMF). BSSMF is particularly wellsuited when the entries of the input matrix $X$ belong to a given interval; forexample when the rows of $X$ represent images, or $X$ is a rating matrix suchas in the Netflix and MovieLens datasets where the entries of $X$ belong to theinterval $[1,5]$. The simplex-structured matrix $H$ not only leads to an easilyunderstandable decomposition providing a soft clustering of the columns of $X$,but implies that the entries of each column of $WH$ belong to the sameintervals as the columns of $W$. In this paper, we first propose a fastalgorithm for BSSMF, even in the presence of missing data in $X$. Then weprovide identifiability conditions for BSSMF, that is, we provide conditionsunder which BSSMF admits a unique decomposition, up to trivial ambiguities.Finally, we illustrate the effectiveness of BSSMF on two applications:extraction of features in a set of images, and the matrix completion problemfor recommender systems.", "output": "Bounded Simplex-Structured Matrix Factorization: Algorithms, Identifiability and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusion models (DDMs) have attracted attention for theirexceptional generation quality and diversity. This success is largelyattributed to the use of class- or text-conditional diffusion guidance methods,such as classifier and classifier-free guidance. In this paper, we present amore comprehensive perspective that goes beyond the traditional guidancemethods. From this generalized perspective, we introduce novel condition- andtraining-free strategies to enhance the quality of generated images. As asimple solution, blur guidance improves the suitability of intermediate samplesfor their fine-scale information and structures, enabling diffusion models togenerate higher quality samples with a moderate guidance scale. Improving uponthis, Self-Attention Guidance (SAG) uses the intermediate self-attention mapsof diffusion models to enhance their stability and efficacy. Specifically, SAGadversarially blurs only the regions that diffusion models attend to at eachiteration and guides them accordingly. Our experimental results show that ourSAG improves the performance of various diffusion models, including ADM, IDDPM,Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidancemethods leads to further improvement.", "output": "Improving Sample Quality of Diffusion Models Using Self-Attention Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Reinforcement Learning (RL) has emerged as a powerful paradigm fortraining neural policies to solve complex control tasks. However, thesepolicies tend to be overfit to the exact specifications of the task andenvironment they were trained on, and thus do not perform well when conditionsdeviate slightly or when composed hierarchically to solve even more complextasks. Recent work has shown that training a mixture of policies, as opposed toa single one, that are driven to explore different regions of the state-actionspace can address this shortcoming by generating a diverse set of behaviors,referred to as skills, that can be collectively used to great effect inadaptation tasks or for hierarchical planning. This is typically realized byincluding a diversity term - often derived from information theory - in theobjective function optimized by RL. However these approaches often requirecareful hyperparameter tuning to be effective. In this work, we demonstratethat less widely-used neuroevolution methods, specifically Quality Diversity(QD), are a competitive alternative to information-theory-augmented RL forskill discovery. Through an extensive empirical evaluation comparing eightstate-of-the-art algorithms (four flagship algorithms from each line of work)on the basis of (i) metrics directly evaluating the skills' diversity, (ii) theskills' performance on adaptation tasks, and (iii) the skills' performance whenused as primitives for hierarchical planning; QD methods are found to provideequal, and sometimes improved, performance whilst being less sensitive tohyperparameters and more scalable. As no single method is found to providenear-optimal performance across all environments, there is a rich scope forfurther research which we support by proposing future directions and providingoptimized open-source implementations.", "output": "Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the clear performance benefits of data augmentations, little is knownabout why they are so effective. In this paper, we disentangle several keymechanisms through which data augmentations operate. Establishing an exchangerate between augmented and additional real data, we find that inout-of-distribution testing scenarios, augmentations which yield samples thatare diverse, but inconsistent with the data distribution can be even morevaluable than additional training data. Moreover, we find that dataaugmentations which encourage invariances can be more valuable than invariancealone, especially on small and medium sized training sets. Following thisobservation, we show that augmentations induce additional stochasticity duringtraining, effectively flattening the loss landscape.", "output": "How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article presents a three-step framework for learning and solving partialdifferential equations (PDEs) using kernel methods. Given a training setconsisting of pairs of noisy PDE solutions and source/boundary terms on a mesh,kernel smoothing is utilized to denoise the data and approximate derivatives ofthe solution. This information is then used in a kernel regression model tolearn the algebraic form of the PDE. The learned PDE is then used within akernel based solver to approximate the solution of the PDE with a newsource/boundary term, thereby constituting an operator learning framework.Numerical experiments compare the method to state-of-the-art algorithms anddemonstrate its competitive performance.", "output": "A Kernel Approach for PDE Discovery and Operator Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks are a powerful class of non-linear functions. However, theirblack-box nature makes it difficult to explain their behaviour and certifytheir safety. Abstraction techniques address this challenge by transforming theneural network into a simpler, over-approximated function. Unfortunately,existing abstraction techniques are slack, which limits their applicability tosmall local regions of the input domain. In this paper, we propose GlobalInterval Neural Network Abstractions with Center-Exact Reconstruction(GINNACER). Our novel abstraction technique produces sound over-approximationbounds over the whole input domain while guaranteeing exact reconstructions forany given local input. Our experiments show that GINNACER is several orders ofmagnitude tighter than state-of-the-art global abstraction techniques, whilebeing competitive with local ones.", "output": "Towards Global Neural Network Abstractions with Locally-Exact Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the ever-increasing interest in applying deep learning (DL) models tomedical imaging, the typical scarcity and imbalance of medical datasets canseverely impact the performance of DL models. The generation of synthetic datathat might be freely shared without compromising patient privacy is awell-known technique for addressing these difficulties. Inpainting algorithmsare a subset of DL generative models that can alter one or more regions of aninput image while matching its surrounding context and, in certain cases,non-imaging input conditions. Although the majority of inpainting techniquesfor medical imaging data use generative adversarial networks (GANs), theperformance of these algorithms is frequently suboptimal due to their limitedoutput variety, a problem that is already well-known for GANs. Denoisingdiffusion probabilistic models (DDPMs) are a recently introduced family ofgenerative networks that can generate results of comparable quality to GANs,but with diverse outputs. In this paper, we describe a DDPM to execute multipleinpainting tasks on 2D axial slices of brain MRI with various sequences, andpresent proof-of-concept examples of its performance in a variety of evaluationscenarios. Our model and a public online interface to try our tool areavailable at: ", "output": "Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitatingpreventive care and to delay further progression. Speech based automatic ADscreening systems provide a non-intrusive and more scalable alternative toother clinical screening techniques. Textual embedding features produced bypre-trained language models (PLMs) such as BERT are widely used in suchsystems. However, PLM domain fine-tuning is commonly based on the masked wordor sentence prediction costs that are inconsistent with the back-end ADdetection task. To this end, this paper investigates the use of prompt-basedfine-tuning of PLMs that consistently uses AD classification errors as thetraining objective function. Disfluency features based on hesitation or pausefiller token frequencies are further incorporated into prompt phrases duringPLM fine-tuning. The decision voting based combination among systems usingdifferent PLMs (BERT and RoBERTa) or systems with different fine-tuningparadigms (conventional masked-language modelling fine-tuning and prompt-basedfine-tuning) is further applied. Mean, standard deviation and the maximum amongaccuracy scores over 15 experiment runs are adopted as performance measurementsfor the AD detection system. Mean detection accuracy of 84.20% (with std 2.09%,best 87.5%) and 82.64% (with std 4.0%, best 89.58%) were obtained using manualand ASR speech transcripts respectively on the ADReSS20 test set consisting of48 elderly speakers.", "output": "Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-agent learning has gained increasing attention to tackle distributedmachine learning scenarios under constrictions of data exchanging. However,existing multi-agent learning models usually consider data fusion under fixedand compulsory collaborative relations among agents, which is not as flexibleand autonomous as human collaboration. To fill this gap, we propose adistributed multi-agent learning model inspired by human collaboration, inwhich the agents can autonomously detect suitable collaborators and refer tocollaborators' model for better performance. To implement such adaptivecollaboration, we use a collaboration graph to indicate the pairwisecollaborative relation. The collaboration graph can be obtained by graphlearning techniques based on model similarity between different agents. Sincemodel similarity can not be formulated by a fixed graphical optimization, wedesign a graph learning network by unrolling, which can learn underlyingsimilar features among potential collaborators. By testing on both regressionand classification tasks, we validate that our proposed collaboration model canfigure out accurate collaborative relationship and greatly improve agents'learning performance.", "output": "Unrolled Graph Learning for Multi-Agent Collaboration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graphons are general and powerful models for generating graphs of varyingsize. In this paper, we propose to directly model graphons using neuralnetworks, obtaining Implicit Graphon Neural Representation (IGNR). Existingwork in modeling and reconstructing graphons often approximates a targetgraphon by a fixed resolution piece-wise constant representation. Our IGNR hasthe benefit that it can represent graphons up to arbitrary resolutions, andenables natural and efficient generation of arbitrary sized graphs with desiredstructure once the model is learned. Furthermore, we allow the input graph datato be unaligned and have different sizes by leveraging the Gromov-Wassersteindistance. We first demonstrate the effectiveness of our model by showing itssuperior performance on a graphon learning task. We then propose an extensionof IGNR that can be incorporated into an auto-encoder framework, anddemonstrate its good performance under a more general setting of graphonlearning. We also show that our model is suitable for graph representationlearning and graph generation.", "output": "Implicit Graphon Neural Representation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current Dynamic Texture Synthesis (DyTS) models can synthesize realisticvideos. However, they require a slow iterative optimization process tosynthesize a single fixed-size short video, and they do not offer anypost-training control over the synthesis process. We propose Dynamic NeuralCellular Automata (DyNCA), a framework for real-time and controllable dynamictexture synthesis. Our method is built upon the recently introduced NCA modelsand can synthesize infinitely long and arbitrary-sized realistic video texturesin real time. We quantitatively and qualitatively evaluate our model and showthat our synthesized videos appear more realistic than the existing results. Weimprove the SOTA DyTS performance by $2sim 4$ orders of magnitude. Moreover,our model offers several real-time video controls including motion speed,motion direction, and an editing brush tool. We exhibit our trained models inan online interactive demo that runs on local hardware and is accessible onpersonal computers and smartphones.", "output": "DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing statistical methods can estimate a policy, or a mapping fromcovariates to decisions, which can then instruct decision makers (e.g., whetherto administer hypotension treatment based on covariates blood pressure andheart rate). There is great interest in using such data-driven policies inhealthcare. However, it is often important to explain to the healthcareprovider, and to the patient, how a new policy differs from the currentstandard of care. This end is facilitated if one can pinpoint the aspects ofthe policy (i.e., the parameters for blood pressure and heart rate) that changewhen moving from the standard of care to the new, suggested policy. To thisend, we adapt ideas from Trust Region Policy Optimization (TRPO). In our work,however, unlike in TRPO, the difference between the suggested policy andstandard of care is required to be sparse, aiding with interpretability. Thisyields ``relative sparsity,\" where, as a function of a tuning parameter,$lambda$, we can approximately control the number of parameters in oursuggested policy that differ from their counterparts in the standard of care(e.g., heart rate only). We propose a criterion for selecting $lambda$,perform simulations, and illustrate our method with a real, observationalhealthcare dataset, deriving a policy that is easy to explain in the context ofthe current standard of care. Our work promotes the adoption of data-drivendecision aids, which have great potential to improve health outcomes.", "output": "Relative Sparsity for Medical Decision Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the fundamental question of how to define and measure the distancefrom calibration for probabilistic predictors. While the notion of perfectcalibration is well-understood, there is no consensus on how to quantify thedistance from perfect calibration. Numerous calibration measures have beenproposed in the literature, but it is unclear how they compare to each other,and many popular measures such as Expected Calibration Error (ECE) fail tosatisfy basic properties like continuity.We present a rigorous framework for analyzing calibration measures, inspiredby the literature on property testing. We propose a ground-truth notion ofdistance from calibration: the $ell_1$ distance to the nearest perfectlycalibrated predictor. We define a consistent calibration measure as one that ispolynomially related to this distance. Applying our framework, we identifythree calibration measures that are consistent and can be estimatedefficiently: smooth calibration, interval calibration, and Laplace kernelcalibration. The former two give quadratic approximations to the ground truthdistance, which we show is information-theoretically optimal in a natural modelfor measuring calibration which we term the prediction-only access model. Ourwork thus establishes fundamental lower and upper bounds on measuring thedistance to calibration, and also provides theoretical justification forpreferring certain metrics (like Laplace kernel calibration) in practice.", "output": "A Unifying Theory of Distance from Calibration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Changing how pre-trained models behave -- e.g., improving their performanceon a downstream task or mitigating biases learned during pre-training -- is acommon practice when developing machine learning systems. In this work, wepropose a new paradigm for steering the behavior of neural networks, centeredaround textit{task vectors}. A task vector specifies a direction in the weightspace of a pre-trained model, such that movement in that direction improvesperformance on the task. We build task vectors by subtracting the weights of apre-trained model from the weights of the same model after fine-tuning on atask. We show that these task vectors can be modified and combined togetherthrough arithmetic operations such as negation and addition, and the behaviorof the resulting model is steered accordingly. Negating a task vector decreasesperformance on the target task, with little change in model behavior on controltasks. Moreover, adding task vectors together can improve performance onmultiple tasks at once. Finally, when tasks are linked by an analogyrelationship of the form ``A is to B as C is to D\", combining task vectors fromthree of the tasks can improve performance on the fourth, even when no datafrom the fourth task is used for training. Overall, our experiments withseveral models, modalities and tasks show that task arithmetic is a simple,efficient and effective way of editing models.", "output": "Editing Models with Task Arithmetic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate uncertainty quantification is necessary to enhance the reliabilityof deep learning models in real-world applications. In the case of regressiontasks, prediction intervals (PIs) should be provided along with thedeterministic predictions of deep learning models. Such PIs are useful or\"high-quality\" as long as they are sufficiently narrow and capture most of theprobability density. In this paper, we present a method to learn predictionintervals for regression-based neural networks automatically in addition to theconventional target predictions. In particular, we train two companion neuralnetworks: one that uses one output, the target estimate, and another that usestwo outputs, the upper and lower bounds of the corresponding PI. Our maincontribution is the design of a novel loss function for the PI-generationnetwork that takes into account the output of the target-estimation network andhas two optimization objectives: minimizing the mean prediction interval widthand ensuring the PI integrity using constraints that maximize the predictioninterval probability coverage implicitly. Furthermore, we introduce aself-adaptive coefficient that balances both objectives within the lossfunction, which alleviates the task of fine-tuning. Experiments using asynthetic dataset, eight benchmark datasets, and a real-world crop yieldprediction dataset showed that our method was able to maintain a nominalprobability coverage and produce significantly narrower PIs without detrimentto its target estimation accuracy when compared to those PIs generated by threestate-of-the-art neural-network-based methods. In other words, our method wasshown to produce higher-quality PIs.", "output": "Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To simulate bosons on a qubit- or qudit-based quantum computer, one has toregularize the theory by truncating infinite-dimensional local Hilbert spacesto finite dimensions. In the search for practical quantum applications, it isimportant to know how big the truncation errors can be. In general, it is noteasy to estimate errors unless we have a good quantum computer. In this paperwe show that traditional sampling methods on classical devices, specificallyMarkov Chain Monte Carlo, can address this issue with a reasonable amount ofcomputational resources available today. As a demonstration, we apply this ideato the scalar field theory on a two-dimensional lattice, with a size that goesbeyond what is achievable using exact diagonalization methods. This method canbe used to estimate the resources needed for realistic quantum simulations ofbosonic theories, and also, to check the validity of the results of thecorresponding quantum simulations.", "output": "Estimating truncation effects of quantum bosonic systems using sampling algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain adaptation of GANs is a problem of fine-tuning the state-of-the-artGAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domainwith few samples (e.g. painting faces, sketches, etc.). While there are a greatnumber of methods that tackle this problem in different ways, there are stillmany important questions that remain unanswered.In this paper, we provide a systematic and in-depth analysis of the domainadaptation problem of GANs, focusing on the StyleGAN model. First, we perform adetailed exploration of the most important parts of StyleGAN that areresponsible for adapting the generator to a new domain depending on thesimilarity between the source and target domains. As a result of this in-depthstudy, we propose new efficient and lightweight parameterizations of StyleGANfor domain adaptation. Particularly, we show there exist directions inStyleSpace (StyleDomain directions) that are sufficient for adapting to similardomains and they can be reduced further. For dissimilar domains, we proposeAffine$+$ and AffineLight$+$ parameterizations that allows us to outperformexisting baselines in few-shot adaptation with low data regime. Finally, weexamine StyleDomain directions and discover their many surprising propertiesthat we apply for domain mixing and cross-domain image morphing.", "output": "StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent work leverages the expressive power of generative adversarial networks(GANs) to generate labeled synthetic datasets. These dataset generation methodsoften require new annotations of synthetic images, which forces practitionersto seek out annotators, curate a set of synthetic images, and ensure thequality of generated labels. We introduce the HandsOff framework, a techniquecapable of producing an unlimited number of synthetic images and correspondinglabels after being trained on less than 50 pre-existing labeled images. Ourframework avoids the practical drawbacks of prior work by unifying the field ofGAN inversion with dataset generation. We generate datasets with richpixel-wise labels in multiple challenging domains such as faces, cars,full-body human poses, and urban driving scenes. Our method achievesstate-of-the-art performance in semantic segmentation, keypoint detection, anddepth estimation compared to prior dataset generation approaches and transferlearning baselines. We additionally showcase its ability to address broadchallenges in model development which stem from fixed, hand-annotated datasets,such as the long-tail problem in semantic segmentation. Project page:austinxu87.github.io/handsoff.", "output": "HandsOff: Labeled Dataset Generation With No Additional Human Annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern deep neural networks have achieved impressive performance on tasksfrom image classification to natural language processing. Surprisingly, thesecomplex systems with massive amounts of parameters exhibit the same structuralproperties in their last-layer features and classifiers across canonicaldatasets when training until convergence. In particular, it has been observedthat the last-layer features collapse to their class-means, and thoseclass-means are the vertices of a simplex Equiangular Tight Frame (ETF). Thisphenomenon is known as Neural Collapse ($mathcal{NC}$). Recent papers havetheoretically shown that $mathcal{NC}$ emerges in the global minimizers oftraining problems with the simplified ``unconstrained feature model''. In thiscontext, we take a step further and prove the $mathcal{NC}$ occurrences indeep linear networks for the popular mean squared error (MSE) and cross entropy(CE) losses, showing that global solutions exhibit $mathcal{NC}$ propertiesacross the linear layers. Furthermore, we extend our study to imbalanced datafor MSE loss and present the first geometric analysis of $mathcal{NC}$ underbias-free setting. Our results demonstrate the convergence of the last-layerfeatures and classifiers to a geometry consisting of orthogonal vectors, whoselengths depend on the amount of data in their corresponding classes. Finally,we empirically validate our theoretical analyses on synthetic and practicalnetwork architectures with both balanced and imbalanced scenarios.", "output": "Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Projection operations are a typical computation bottleneck in onlinelearning. In this paper, we enable projection-free online learning within theframework of Online Convex Optimization with Memory (OCO-M) -- OCO-M captureshow the history of decisions affects the current outcome by allowing the onlinelearning loss functions to depend on both current and past decisions.Particularly, we introduce the first projection-free meta-base learningalgorithm with memory that minimizes dynamic regret, i.e., that minimizes thesuboptimality against any sequence of time-varying decisions. We are motivatedby artificial intelligence applications where autonomous agents need to adaptto time-varying environments in real-time, accounting for how past decisionsaffect the present. Examples of such applications are: online control ofdynamical systems; statistical arbitrage; and time series prediction. Thealgorithm builds on the Online Frank-Wolfe (OFW) and Hedge algorithms. Wedemonstrate how our algorithm can be applied to the online control of lineartime-varying systems in the presence of unpredictable process noise. To thisend, we develop a controller with memory and bounded dynamic regret against anyoptimal time-varying linear feedback control policy. We validate our algorithmin simulated scenarios of online control of linear time-invariant systems.", "output": "Efficient Online Learning with Memory via Frank-Wolfe Optimization: Algorithms with Bounded Dynamic Regret and Applications to Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper demonstrates an approach for learning highly semantic imagerepresentations without relying on hand-crafted data-augmentations. Weintroduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), anon-generative approach for self-supervised learning from images. The ideabehind I-JEPA is simple: from a single context block, predict therepresentations of various target blocks in the same image. A core designchoice to guide I-JEPA towards producing semantic representations is themasking strategy; specifically, it is crucial to (a) sample target blocks withsufficiently large scale (semantic), and to (b) use a sufficiently informative(spatially distributed) context block. Empirically, when combined with VisionTransformers, we find I-JEPA to be highly scalable. For instance, we train aViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strongdownstream performance across a wide range of tasks, from linear classificationto object counting and depth prediction.", "output": "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Gaussian process state-space model (GPSSM) has attracted much attentionover the past decade. However, the model representation power of the GPSSM isfar from satisfactory. Most GPSSM studies rely on the standard Gaussian process(GP) with a preliminary kernel, such as the squared exponential (SE) kernel orMat'{e}rn kernel, which limits the model representation power and itsapplication in complex scenarios. To address this issue, this paper proposes anovel class of probabilistic state-space models, called TGPSSMs. By leveraginga parametric normalizing flow, the TGPSSMs enrich the GP priors in the standardGPSSM, rendering the state-space model more flexible and expressive.Additionally, we present a scalable variational inference algorithm forlearning and inference in TGPSSMs, which provides a flexible and optimalstructure for the variational distribution of latent states. The algorithm isinterpretable and computationally efficient owing to the sparse representationof GP and the bijective nature of normalizing flow. To further improve thelearning and inference performance of the proposed algorithm, we integrate aconstrained optimization framework to enhance the state-space representationcapabilities and optimize the hyperparameters. The experimental results basedon various synthetic and real datasets corroborate that the proposed TGPSSMyields superior learning and inference performance compared to severalstate-of-the-art methods. The accompanying source code is available aturl{", "output": "Towards Flexibility and Interpretability of Gaussian Process State-Space Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lotof attention to potentially support many applications. Given that KGs areusually incomplete, neural models are proposed to answer the logical queries byparameterizing set operators with complex neural networks. However, suchmethods usually train neural set operators with a large number of entity andrelation embeddings from the zero, where whether and how the embeddings or theneural set operators contribute to the performance remains not clear. In thispaper, we propose a simple framework for complex query answering thatdecomposes the KG embeddings from neural set operators. We propose to representthe complex queries into the query graph. On top of the query graph, we proposethe Logical Message Passing Neural Network (LMPNN) that connects the localone-hop inferences on atomic formulas to the global logical reasoning forcomplex query answering. We leverage existing effective KG embeddings toconduct one-hop inferences on atomic formulas, the results of which areregarded as the messages passed in LMPNN. The reasoning process over theoverall logical formulas is turned into the forward pass of LMPNN thatincrementally aggregates local information to finally predict the answers'embeddings. The complex logical inference across different types of querieswill then be learned from training examples based on the LMPNN architecture.Theoretically, our query-graph represenation is more general than theprevailing operator-tree formulation, so our approach applies to a broaderrange of complex KG queries. Empirically, our approach yields the newstate-of-the-art neural CQA model. Our research bridges the gap between complexKG query answering tasks and the long-standing achievements of knowledge graphrepresentation learning.", "output": "Logical Message Passing Networks with One-hop Inference on Atomic Formulas."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The performance of neural network-based speech enhancement systems isprimarily influenced by the model architecture, whereas training times andcomputational resource utilization are primarily affected by trainingparameters such as the batch size. Since noisy and reverberant speech mixturescan have different duration, a batching strategy is required to handle variablesize inputs during training, in particular for state-of-the-art end-to-endsystems. Such strategies usually strive for a compromise between zero-paddingand data randomization, and can be combined with a dynamic batch size for amore consistent amount of data in each batch. However, the effect of thesestrategies on resource utilization and more importantly network performance isnot well documented. This paper systematically investigates the effect ofdifferent batching strategies and batch sizes on the training statistics andspeech enhancement performance of a Conv-TasNet, evaluated in both matched andmismatched conditions. We find that using a small batch size during trainingimproves performance in both conditions for all batching strategies. Moreover,using sorted or bucket batching with a dynamic batch size allows for reducedtraining time and GPU memory usage while achieving similar performance comparedto random batching with a fixed batch size.", "output": "On Batching Variable Size Inputs for Training End-to-End Speech Enhancement Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) are known to be vulnerable to adversarialgeometric transformation. This paper aims to verify the robustness oflarge-scale DNNs against the combination of multiple geometric transformationswith a provable guarantee. Given a set of transformations (e.g., rotation,scaling, etc.), we develop GeoRobust, a black-box robustness analyser builtupon a novel global optimisation strategy, for locating the worst-casecombination of transformations that affect and even alter a network's output.GeoRobust can provide provable guarantees on finding the worst-case combinationbased on recent advances in Lipschitzian theory. Due to its black-box nature,GeoRobust can be deployed on large-scale DNNs regardless of theirarchitectures, activation functions, and the number of neurons. In practice,GeoRobust can locate the worst-case geometric transformation with highprecision for the ResNet50 model on ImageNet in a few seconds on average. Weexamined 18 ImageNet classifiers, including the ResNet family and visiontransformers, and found a positive correlation between the geometric robustnessof the networks and the parameter numbers. We also observe that increasing thedepth of DNN is more beneficial than increasing its width in terms of improvingits geometric robustness. Our tool GeoRobust is available at", "output": "Towards Verifying the Geometric Robustness of Large-scale Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a unified framework for deriving PAC-Bayesian generalizationbounds. Unlike most previous literature on this topic, our bounds areanytime-valid (i.e., time-uniform), meaning that they hold at all stoppingtimes, not only for a fixed sample size. Our approach combines four tools inthe following order: (a) nonnegative supermartingales or reversesubmartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula(or other convex duality principles), and (d) Ville's inequality. Our mainresult is a PAC-Bayes theorem which holds for a wide class of discretestochastic processes. We show how this result implies time-uniform versions ofwell-known classical PAC-Bayes bounds, such as those of Seeger, McAllester,Maurer, and Catoni, in addition to many recent bounds. We also present severalnovel bounds. Our framework also enables us to relax traditional assumptions;in particular, we consider nonstationary loss functions and non-i.i.d. data. Insum, we unify the derivation of past bounds and ease the search for futurebounds: one may simply check if our supermartingale or submartingale conditionsare met and, if so, be guaranteed a (time-uniform) PAC-Bayes bound.", "output": "A unified recipe for deriving (time-uniform) PAC-Bayes bounds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many real-world data are naturally represented as a sparse reorderablematrix, whose rows and columns can be arbitrarily ordered (e.g., the adjacencymatrix of a bipartite graph). Storing a sparse matrix in conventional waysrequires an amount of space linear in the number of non-zeros, and lossycompression of sparse matrices (e.g., Truncated SVD) typically requires anamount of space linear in the number of rows and columns. In this work, wepropose NeuKron for compressing a sparse reorderable matrix into aconstant-size space. NeuKron generalizes Kronecker products using a recurrentneural network with a constant number of parameters. NeuKron updates theparameters so that a given matrix is approximated by the product and reordersthe rows and columns of the matrix to facilitate the approximation. The updatestake time linear in the number of non-zeros in the input matrix, and theapproximation of each entry can be retrieved in logarithmic time. We alsoextend NeuKron to compress sparse reorderable tensors (e.g. multi-layergraphs), which generalize matrices. Through experiments on ten real-worlddatasets, we show that NeuKron is (a) Compact: requiring up to five orders ofmagnitude less space than its best competitor with similar approximationerrors, (b) Accurate: giving up to 10x smaller approximation error than itsbest competitors with similar size outputs, and (c) Scalable: successfullycompressing a matrix with over 230 million non-zero entries.", "output": "NeuKron: Constant-Size Lossy Compression of Sparse Reorderable Matrices and Tensors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The higher-order correlation clustering problem is an expressive model, andrecently, local search heuristics have been proposed for several applications.Certifying optimality, however, is NP-hard and practically hampered already bythe complexity of the problem statement. Here, we focus on establishing partialoptimality conditions for the special case of complete graphs and cubicobjective functions. In addition, we define and implement algorithms fortesting these conditions and examine their effect numerically, on two datasets.", "output": "Partial Optimality in Cubic Correlation Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models in the literature are optimized with various objectives thatare special cases of a weighted loss, where the weighting function specifiesthe weight per noise level. Uniform weighting corresponds to maximizing theELBO, a principled approximation of maximum likelihood. In current practicediffusion models are optimized with non-uniform weighting due to better resultsin terms of sample quality. In this work we expose a direct relationshipbetween the weighted loss (with any weighting) and the ELBO objective.We show that the weighted loss can be written as a weighted integral ofELBOs, with one ELBO per noise level. If the weighting function is monotonic,then the weighted loss is a likelihood-based objective: it maximizes the ELBOunder simple data augmentation, namely Gaussian noise perturbation. Our maincontribution is a deeper theoretical understanding of the diffusion objective,but we also performed some experiments comparing monotonic with non-monotonicweightings, finding that monotonic weighting performs competitively with thebest published results.", "output": "Understanding the Diffusion Objective as a Weighted Integral of ELBOs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we introduce BO-Muse, a new approach to human-AI teaming forthe optimization of expensive black-box functions. Inspired by the intrinsicdifficulty of extracting expert knowledge and distilling it back into AI modelsand by observations of human behavior in real-world experimental design, ouralgorithm lets the human expert take the lead in the experimental process. Thehuman expert can use their domain expertise to its full potential, while the AIplays the role of a muse, injecting novelty and searching for areas of weaknessto break the human out of over-exploitation induced by cognitive entrenchment.With mild assumptions, we show that our algorithm converges sub-linearly, at arate faster than the AI or human alone. We validate our algorithm usingsynthetic data and with human experts performing real-world experiments.", "output": "BO-Muse: A human expert and AI teaming framework for accelerated experimental design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We formulate the Multiple Kernel Learning (abbreviated as MKL) problem forthe support vector machine with the infamous $(0,1)$-loss function. Somefirst-order optimality conditions are given and then exploited to develop afast ADMM solver for the nonconvex and nonsmooth optimization problem. A simplenumerical experiment on synthetic planar data shows that our MKL-$L_{0/1}$-SVMframework could be promising.", "output": "An ADMM Solver for the MKL-$L_{0/1}$-SVM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The emergence of pretrained large language models has led to the deploymentof a range of social chatbots for chitchat. Although these chatbots demonstratelanguage ability and fluency, they are not guaranteed to be engaging and canstruggle to retain users. This work investigates the development of socialchatbots that prioritize user engagement to enhance retention, specificallyexamining the use of human feedback to efficiently develop highly engagingchatbots. The proposed approach uses automatic pseudo-labels collected fromuser interactions to train a reward model that can be used to rejectlow-scoring sample responses generated by the chatbot model at inference time.Intuitive evaluation metrics, such as mean conversation length (MCL), areintroduced as proxies to measure the level of engagement of deployed chatbots.A/B testing on groups of 10,000 new daily chatbot users on the Chai Researchplatform shows that this approach increases the MCL by up to 70%, whichtranslates to a more than 30% increase in user retention for a GPT-J 6B model.Future work aims to use the reward model to realise a data fly-wheel, where thelatest user conversations can be used to alternately fine-tune the languagemodel and the reward model.", "output": "Rewarding Chatbots for Real-World Engagement with Millions of Users."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In contrast to the natural capabilities of humans to learn new tasks in asequential fashion, neural networks are known to suffer from catastrophicforgetting, where the model's performances on old tasks drop dramatically afterbeing optimized for a new task. Since then, the continual learning (CL)community has proposed several solutions aiming to equip the neural networkwith the ability to learn the current task (plasticity) while still achievinghigh accuracy on the previous tasks (stability). Despite remarkableimprovements, the plasticity-stability trade-off is still far from being solvedand its underlying mechanism is poorly understood. In this work, we proposeAuxiliary Network Continual Learning (ANCL), a novel method that applies anadditional auxiliary network which promotes plasticity to the continuallylearned model which mainly focuses on stability. More concretely, the proposedframework materializes in a regularizer that naturally interpolates betweenplasticity and stability, surpassing strong baselines on task incremental andclass incremental scenarios. Through extensive analyses on ANCL solutions, weidentify some essential principles beneath the stability-plasticity trade-off.", "output": "Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization (DG) aims to learn a model that generalizes well tounseen target domains utilizing multiple source domains without re-training.Most existing DG works are based on convolutional neural networks (CNNs).However, the local operation of the convolution kernel makes the model focustoo much on local representations (e.g., texture), which inherently causes themodel more prone to overfit to the source domains and hampers itsgeneralization ability. Recently, several MLP-based methods have achievedpromising results in supervised learning tasks by learning global interactionsamong different patches of the image. Inspired by this, in this paper, we firstanalyze the difference between CNN and MLP methods in DG and find that MLPmethods exhibit a better generalization ability because they can better capturethe global representations (e.g., structure) than CNN methods. Then, based on arecent lightweight MLP method, we obtain a strong baseline that outperformsmost state-of-the-art CNN-based methods. The baseline can learn globalstructure representations with a filter to suppress structure irrelevantinformation in the frequency space. Moreover, we propose a dynAmicLOw-Frequency spectrum Transform (ALOFT) that can perturb local texturefeatures while preserving global structure features, thus enabling the filterto remove structure-irrelevant information sufficiently. Extensive experimentson four benchmarks have demonstrated that our method can achieve greatperformance improvement with a small number of parameters compared to SOTACNN-based DG methods. Our code is available at", "output": "ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study presents a systematic machine learning approach for creatingefficient hybrid models and discovering sorption uptake models in non-linearadvection-diffusion-sorption systems. It demonstrates an effective method totrain these complex systems using gradient based optimizers, adjointsensitivity analysis, and JIT-compiled vector Jacobian products, combined withspatial discretization and adaptive integrators. Sparse and symbolic regressionwere employed to identify missing functions in the artificial neural network.The robustness of the proposed method was tested on an in-silico data set ofnoisy breakthrough curve observations of fixed-bed adsorption, resulting in awell-fitted hybrid model. The study successfully reconstructed sorption uptakekinetics using sparse and symbolic regression, and accurately predictedbreakthrough curves using identified polynomials, highlighting the potential ofthe proposed framework for discovering sorption kinetic law structures.", "output": "Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, the authors develop regression approaches based on deeplearning to perform thread density estimation for plain weave canvas analysis.Previous approaches were based on Fourier analysis, which is quite robust forsome scenarios but fails in some others, in machine learning tools, thatinvolve pre-labeling of the painting at hand, or the segmentation of threadcrossing points, that provides good estimations in all scenarios with no needof pre-labeling. The segmentation approach is time-consuming as the estimationof the densities is performed after locating the crossing points. In this novelproposal, we avoid this step by computing the density of threads directly fromthe image with a regression deep learning model. We also incorporate someimprovements in the initial preprocessing of the input image with an impact onthe final error. Several models are proposed and analyzed to retain the bestone. Furthermore, we further reduce the density estimation error by introducinga semi-supervised approach. The performance of our novel algorithm is analyzedwith works by Ribera, Vel'azquez, and Poussin where we compare our results tothe ones of previous approaches. Finally, the method is put into practice tosupport the change of authorship or a masterpiece at the Museo del Prado.", "output": "Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the challenges in federated learning is the non-independent andidentically distributed (non-iid) characteristics between heterogeneousdevices, which cause significant differences in local updates and affect theperformance of the central server. Although many studies have been proposed toaddress this challenge, they only focus on local training and aggregationprocesses to smooth the changes and fail to achieve high performance with deeplearning models. Inspired by the phenomenon of neural collapse, we force eachclient to be optimized toward an optimal global structure for classification.Specifically, we initialize it as a random simplex Equiangular Tight Frame(ETF) and fix it as the unit optimization target of all clients during thelocal updating. After guaranteeing all clients are learning to converge to theglobal optimum, we propose to add a global memory vector for each category toremedy the parameter fluctuation caused by the bias of the intra-classcondition distribution among clients. Our experimental results show that ourmethod can improve the performance with faster convergence speed ondifferent-size datasets.", "output": "Neural Collapse Inspired Federated Learning with Non-iid Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ground-glass opacity is a hallmark of numerous lung diseases, includingpatients with COVID19 and pneumonia, pulmonary fibrosis, and tuberculosis. Thisbrief note presents experimental results of a proof-of-concept framework thatgot implemented and tested over three days as driven by the third challengeentitled \"COVID-19 Competition\", hosted at the AI-Enabled Medical ImageAnalysis Workshop of the 2023 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP 2023). Using a newly built virtualenvironment (created on March 17, 2023), we investigated various pre-trainedtwo-dimensional convolutional neural networks (CNN) such as Dense NeuralNetwork, Residual Neural Networks (ResNet), and Vision Transformers, as well asthe extent of fine-tuning. Based on empirical experiments, we opted tofine-tune them using ADAM's optimization algorithm with a standard learningrate of 0.001 for all CNN architectures and apply early-stopping whenever thevalidation loss reached a plateau. For each trained CNN, the model state withthe best validation accuracy achieved during training was stored and laterreloaded for new classifications of unseen samples drawn from the validationset provided by the challenge organizers. According to the organizers, few ofthese 2D CNNs yielded performance comparable to an architecture that combinedResNet and Recurrent Neural Network (Gated Recurrent Units). As part of thechallenge requirement, the source code produced during the course of thisexercise is posted at  We also hope thatother researchers may find this light prototype consisting of few Python filesbased on PyTorch 1.13.1 and TorchVision 0.14.1 approachable.", "output": "Severity classification of ground-glass opacity via 2-D convolutional neural network and lung CT scans: a 3-day exploration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Laplacian based algorithms for data lying on a manifold have beenproven effective for tasks such as dimensionality reduction, clustering, anddenoising. In this work, we consider data sets whose data point not only lie ona manifold, but are also closed under the action of a continuous group. Anexample of such data set is volumes that line on a low dimensional manifold,where each volume may be rotated in three-dimensional space. We introduce theG-invariant graph Laplacian that generalizes the graph Laplacian by accountingfor the action of the group on the data set. We show that like the standardgraph Laplacian, the G-invariant graph Laplacian converges to theLaplace-Beltrami operator on the data manifold, but with a significantlyimproved convergence rate. Furthermore, we show that the eigenfunctions of theG-invariant graph Laplacian admit the form of tensor products between the groupelements and eigenvectors of certain matrices, which can be computedefficiently using FFT-type algorithms. We demonstrate our construction and itsadvantages on the problem of filtering data on a noisy manifold closed underthe action of the special unitary group SU(2).", "output": "The G-invariant graph Laplacian."}]