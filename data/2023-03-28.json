[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The spread of many infectious diseases is modeled using variants of the SIRcompartmental model, which is a coupled differential equation. The coefficientsof the SIR model determine the spread trajectories of disease, on whose basisproactive measures can be taken. Hence, the coefficient estimates must be bothfast and accurate. Shaier et al. in the paper \"Disease Informed NeuralNetworks\" used Physics Informed Neural Networks (PINNs) to estimate theparameters of the SIR model. There are two drawbacks to this approach. First,the training time for PINNs is high, with certain diseases taking close to 90hrs to train. Second, PINNs don't generalize for a new SIDR trajectory, andlearning its corresponding SIR parameters requires retraining the PINN fromscratch. In this work, we aim to eliminate both of these drawbacks. We generatea dataset between the parameters of ODE and the spread trajectories by solvingthe forward problem for a large distribution of parameters using the LSODAalgorithm. We then use a neural network to learn the mapping between spreadtrajectories and coefficients of SIDR in an offline manner. This allows us tolearn the parameters of a new spread trajectory without having to retrain,enabling generalization at test time. We observe a speed-up of 3-4 orders ofmagnitude with accuracy comparable to that of PINNs for 11 highly infectiousdiseases. Further finetuning of neural network inferred ODE coefficients usingPINN further leads to 2-3 orders improvement of estimated coefficients.", "output": "DeepEpiSolver: Unravelling Inverse problems in Covid, HIV, Ebola and Disease Transmission."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Reinforcement Learning (DRL) enhances the efficiency of AutonomousVehicles (AV), but also makes them susceptible to backdoor attacks that canresult in traffic congestion or collisions. Backdoor functionality is typicallyincorporated by contaminating training datasets with covert malicious data tomaintain high precision on genuine inputs while inducing the desired(malicious) outputs for specific inputs chosen by adversaries. Current defensesagainst backdoors mainly focus on image classification using image-basedfeatures, which cannot be readily transferred to the regression task ofDRL-based AV controllers since the inputs are continuous sensor data, i.e., thecombinations of velocity and distance of AV and its surrounding vehicles. Ourproposed method adds well-designed noise to the input to neutralize backdoors.The approach involves learning an optimal smoothing (noise) distribution topreserve the normal functionality of genuine inputs while neutralizingbackdoors. By doing so, the resulting model is expected to be more resilientagainst backdoor attacks while maintaining high accuracy on genuine inputs. Theeffectiveness of the proposed method is verified on a simulated traffic systembased on a microscopic traffic simulator, where experimental results showcasethat the smoothed traffic controller can neutralize all trigger samples andmaintain the performance of relieving traffic congestion", "output": "Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When learning a task as a team, some agents in Multi-Agent ReinforcementLearning (MARL) may fail to understand their true impact in the performance ofthe team. Such agents end up learning sub-optimal policies, demonstratingundesired lazy behaviours. To investigate this problem, we start by formalisingthe use of temporal causality applied to MARL problems. We then show howcausality can be used to penalise such lazy agents and improve theirbehaviours. By understanding how their local observations are causally relatedto the team reward, each agent in the team can adjust their individual creditbased on whether they helped to cause the reward or not. We show empiricallythat using causality estimations in MARL improves not only the holisticperformance of the team, but also the individual capabilities of each agent. Weobserve that the improvements are consistent in a set of differentenvironments.", "output": "Causality Detection for Efficient Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integrated Gradients (IG) as well as its variants are well-known techniquesfor interpreting the decisions of deep neural networks. While IG-basedapproaches attain state-of-the-art performance, they often integrate noise intotheir explanation saliency maps, which reduce their interpretability. Tominimize the noise, we examine the source of the noise analytically and proposea new approach to reduce the explanation noise based on our analyticalfindings. We propose the Important Direction Gradient Integration (IDGI)framework, which can be easily incorporated into any IG-based method that usesthe Reimann Integration for integrated gradient computation. Extensiveexperiments with three IG-based methods show that IDGI improves themdrastically on numerous interpretability metrics.", "output": "IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time-series data augmentation mitigates the issue of insufficient trainingdata for deep learning models. Yet, existing augmentation methods are mainlydesigned for classification, where class labels can be preserved even ifaugmentation alters the temporal dynamics. We note that augmentation designedfor forecasting requires diversity as well as coherence with the originaltemporal dynamics. As time-series data generated by real-life physicalprocesses exhibit characteristics in both the time and frequency domains, wepropose to combine Spectral and Time Augmentation (STAug) for generating morediverse and coherent samples. Specifically, in the frequency domain, we use theEmpirical Mode Decomposition to decompose a time series and reassemble thesubcomponents with random weights. This way, we generate diverse samples whilebeing coherent with the original temporal relationships as they contain thesame set of base components. In the time domain, we adapt a mix-up strategythat generates diverse as well as linearly in-between coherent samples.Experiments on five real-world time-series datasets demonstrate that STAugoutperforms the base models without data augmentation as well asstate-of-the-art augmentation methods.", "output": "Towards Diverse and Coherent Augmentation for Time-Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Planning agents are ill-equipped to act in novel situations in which theirdomain model no longer accurately represents the world. We introduce anapproach for such agents operating in open worlds that detects the presence ofnovelties and effectively adapts their domain models and consequent actionselection. It uses observations of action execution and measures theirdivergence from what is expected, according to the environment model, to inferexistence of a novelty. Then, it revises the model through a heuristics-guidedsearch over model changes. We report empirical evaluations on the CartPoleproblem, a standard Reinforcement Learning (RL) benchmark. The results showthat our approach can deal with a class of novelties very quickly and in aninterpretable fashion.", "output": "Learning to Operate in Open Worlds by Adapting Planning Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of International Classification of Diseases (ICD) codes in healthcarepresents a challenge in selecting relevant codes as features for machinelearning models due to this system's large number of codes. In this study, wecompared several unsupervised feature selection methods for an ICD codedatabase of 49,075 coronary artery disease patients in Alberta, Canada.Specifically, we employed Laplacian Score, Unsupervised Feature Selection forMulti-Cluster Data, Autoencoder Inspired Unsupervised Feature Selection,Principal Feature Analysis, and Concrete Autoencoders with and without ICD treeweight adjustment to select the 100 best features from over 9,000 codes. Weassessed the selected features based on their ability to reconstruct theinitial feature space and predict 90-day mortality following discharge. Ourfindings revealed that the Concrete Autoencoder methods outperformed all othermethods in both tasks. Furthermore, the weight adjustment in the ConcreteAutoencoder method decreased the complexity of features.", "output": "Unsupervised Feature Selection to Identify Important ICD-10 Codes for Machine Learning: A Case Study on a Coronary Artery Disease Patient Cohort."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art order dispatching algorithms for ridesharing batch passengerrequests and allocate them to a fleet of vehicles in a centralized manner,optimizing over the estimated values of each passenger-vehicle matching usinginteger linear programming (ILP). Using good estimates of future values, suchILP-based approaches are able to significantly increase the service rates(percentage of requests served) for a fixed fleet of vehicles. However, suchapproaches that focus solely on maximizing efficiency can lead to disparitiesfor both drivers (e.g., income inequality) and passengers (e.g., inequality ofservice for different groups). Existing approaches that consider fairness onlydo it for naive assignment policies, require extensive training, or look atonly single-sided fairness. We propose a simple incentive-based fairness schemethat can be implemented online as a part of this ILP formulation that allows usto improve fairness over a variety of fairness metrics. Deriving from a lens ofvariance minimization, we describe how these fairness incentives can beformulated for two distinct use cases for passenger groups and driver fairness.We show that under mild conditions, our approach can guarantee an improvementin the chosen metric for the worst-off individual. We also show empiricallythat our Simple Incentives approach significantly outperforms prior art,despite requiring no retraining; indeed, it often leads to a large improvementover the state-of-the-art fairness-aware approach in both overall service rateand fairness.", "output": "Using Simple Incentives to Improve Two-Sided Fairness in Ridesharing Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Train/Test-Time Adaptation with Retrieval (${\\rm T^3AR}$), amethod to adapt models both at train and test time by means of a retrievalmodule and a searchable pool of external samples. Before inference, ${\\rmT^3AR}$ adapts a given model to the downstream task using refined pseudo-labelsand a self-supervised contrastive objective function whose noise distributionleverages retrieved real samples to improve feature adaptation on the targetdata manifold. The retrieval of real images is key to ${\\rm T^3AR}$ since itdoes not rely solely on synthetic data augmentations to compensate for the lackof adaptation data, as typically done by other adaptation algorithms.Furthermore, thanks to the retrieval module, our method gives the user orservice provider the possibility to improve model adaptation on the downstreamtask by incorporating further relevant data or to fully remove samples that mayno longer be available due to changes in user preference after deployment.First, we show that ${\\rm T^3AR}$ can be used at training time to improvedownstream fine-grained classification over standard fine-tuning baselines, andthe fewer the adaptation data the higher the relative improvement (up to 13%).Second, we apply ${\\rm T^3AR}$ for test-time adaptation and show thatexploiting a pool of external images at test-time leads to more robustrepresentations over existing methods on DomainNet-126 and VISDA-C, especiallywhen few adaptation data are available (up to 8%).", "output": "Train/Test-Time Adaptation with Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scholarly publications are key to the transfer of knowledge from scholars toothers. However, research papers are information-dense, and as the volume ofthe scientific literature grows, the need for new technology to support thereading process grows. In contrast to the process of finding papers, which hasbeen transformed by Internet technology, the experience of reading researchpapers has changed little in decades. The PDF format for sharing researchpapers is widely used due to its portability, but it has significant downsidesincluding: static content, poor accessibility for low-vision readers, anddifficulty reading on mobile devices. This paper explores the question \"Canrecent advances in AI and HCI power intelligent, interactive, and accessiblereading interfaces -- even for legacy PDFs?\" We describe the Semantic ReaderProject, a collaborative effort across multiple institutions to exploreautomatic creation of dynamic reading interfaces for research papers. Throughthis project, we've developed ten research prototype interfaces and conductedusability studies with more than 300 participants and real-world users showingimproved reading experiences for scholars. We've also released a productionreading interface for research papers that will incorporate the best featuresas they mature. We structure this paper around challenges scholars and thepublic face when reading research papers -- Discovery, Efficiency,Comprehension, Synthesis, and Accessibility -- and present an overview of ourprogress and remaining open challenges.", "output": "The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multiple patterning lithography (MPL) is regarded as one of the mostpromising ways of overcoming the resolution limitations of conventional opticallithography due to the delay of next-generation lithography technology. As thefeature size continues to decrease, layout decomposition for multiplepatterning lithography (MPLD) technology is becoming increasingly crucial forimproving the manufacturability in advanced nodes. The decomposition processrefers to assigning the layout features to different mask layers according tothe design rules and density requirements. When the number of masks $k \\geq 3$,the MPLD problems are NP-hard and thus may suffer from runtime overhead forpractical designs. However, the number of layout patterns is increasingexponentially in industrial layouts, which hinders the runtime performance ofMPLD models. In this research, we substitute the CPU's dance link datastructure with parallel GPU matrix operations to accelerate the solution forexact cover-based MPLD algorithms. Experimental results demonstrate that oursystem is capable of full-scale, lightning-fast layout decomposition, which canachieve more than 10$\\times$ speed-up without quality degradation compared tostate-of-the-art layout decomposition methods.", "output": "GPU-accelerated Matrix Cover Algorithm for Multiple Patterning Layout Decomposition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hilbert and Ackermann asked for a method to consistently extend incompletetheories to complete theories. G\\\"odel essentially proved that any theorycapable of encoding its own statements and their proofs contains statementsthat are true but not provable. Hilbert did not accept that G\\\"odel'sconstruction answered his question, and in his late writings and lectures,G\\\"odel agreed that it did not, since theories can be completed incrementally,by adding axioms to prove ever more true statements, as science normally does,with completeness as the vanishing point. This pragmatic view of validity isfamiliar not only to scientists who conjecture test hypotheses but also to realestate agents and other dealers, who conjure claims, albeit invalid, asnecessary to close a deal, confident that they will be able to conjure otherclaims, albeit invalid, sufficient to make the first claims valid. We study theunderlying logical process and describe the trajectories leading to testablebut unfalsifiable theories to which bots and other automated learners arelikely to converge.", "output": "From G\\\"odel's Incompleteness Theorem to the completeness of bot religions (Extended abstract)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The open radio access network (O-RAN) architecture offers a cost-effectiveand scalable solution for internet service providers to optimize their networksusing machine learning algorithms. The architecture's open interfaces enablenetwork function virtualization, with the O-RAN serving as the primarycommunication device for users. However, the limited frequency resources andinformation explosion make it difficult to achieve an optimal networkexperience without effective traffic control or resource allocation. To addressthis, we consider mobility-aware load balancing to evenly distribute loadsacross the network, preventing network congestion and user outages caused byexcessive load concentration on open radio unit (O-RU) governed by a singleopen distributed unit (O-DU). We have proposed a multi-agent multi-armed banditfor load balancing and resource allocation (mmLBRA) scheme, designed to bothachieve load balancing and improve the effective sum-rate performance of theO-RAN network. We also present the mmLBRA-LB and mmLBRA-RA sub-schemes that canoperate independently in non-realtime RAN intelligent controller (Non-RT RIC)and near-RT RIC, respectively, providing a solution with moderate loads andhigh-rate in O-RUs. Simulation results show that the proposed mmLBRA schemesignificantly increases the effective network sum-rate while achieving betterload balancing across O-RUs compared to rule-based and other existing heuristicmethods in open literature.", "output": "Intelligent Load Balancing and Resource Allocation in O-RAN: A Multi-Agent Multi-Armed Bandit Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Heuristic search is a powerful approach that has successfully been applied toa broad class of planning problems, including classical planning,multi-objective planning, and probabilistic planning modelled as a stochasticshortest path (SSP) problem. Here, we extend the reach of heuristic search to amore expressive class of problems, namely multi-objective stochastic shortestpaths (MOSSPs), which require computing a coverage set of non-dominatedpolicies. We design new heuristic search algorithms MOLAO* and MOLRTDP, whichextend well-known SSP algorithms to the multi-objective case. We furtherconstruct a spectrum of domain-independent heuristic functions differing intheir ability to take into account the stochastic and multi-objective featuresof the problem to guide the search. Our experiments demonstrate the benefits ofthese algorithms and the relative merits of the heuristics.", "output": "Heuristic Search for Multi-Objective Probabilistic Planning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FlexNeRF, a method for photorealistic freeviewpoint rendering ofhumans in motion from monocular videos. Our approach works well with sparseviews, which is a challenging scenario when the subject is exhibitingfast/complex motions. We propose a novel approach which jointly optimizes acanonical time and pose configuration, with a pose-dependent motion field andpose-independent temporal deformations complementing each other. Thanks to ournovel temporal and cyclic consistency constraints along with additional losseson intermediate representation such as segmentation, our approach provides highquality outputs as the observed views become sparser. We empiricallydemonstrate that our method significantly outperforms the state-of-the-art onpublic benchmark datasets as well as a self-captured fashion dataset. Theproject page is available at: https://flex-nerf.github.io/", "output": "FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cell instance segmentation in cytology images has significant importance forbiology analysis and cancer screening, while remains challenging due to 1) theextensive overlapping translucent cell clusters that cause the ambiguousboundaries, and 2) the confusion of mimics and debris as nuclei. In this work,we proposed a De-overlapping Network (DoNet) in a decompose-and-recombinedstrategy. A Dual-path Region Segmentation Module (DRM) explicitly decomposesthe cell clusters into intersection and complement regions, followed by aSemantic Consistency-guided Recombination Module (CRM) for integration. Tofurther introduce the containment relationship of the nucleus in the cytoplasm,we design a Mask-guided Region Proposal Strategy (MRP) that integrates the cellattention maps for inner-cell instance prediction. We validate the proposedapproach on ISBI2014 and CPS datasets. Experiments show that our proposed DoNetsignificantly outperforms other state-of-the-art (SOTA) cell instancesegmentation methods. The code is available athttps://github.com/DeepDoNet/DoNet.", "output": "DoNet: Deep De-overlapping Network for Cytology Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-language (VL) pre-training has recently gained much attention for itstransferability and flexibility in novel concepts (e.g., cross-modalitytransfer) across various visual tasks. However, VL-driven segmentation has beenunder-explored, and the existing approaches still have the burden of acquiringadditional training images or even segmentation annotations to adapt a VL modelto downstream segmentation tasks. In this paper, we introduce a novelimage-free segmentation task where the goal is to perform semantic segmentationgiven only a set of the target semantic categories, but without anytask-specific images and annotations. To tackle this challenging task, ourproposed method, coined IFSeg, generates VL-driven artificialimage-segmentation pairs and updates a pre-trained VL model to a segmentationtask. We construct this artificial training data by creating a 2D map of randomsemantic categories and another map of their corresponding word tokens. Giventhat a pre-trained VL model projects visual and text tokens into a common spacewhere tokens that share the semantics are located closely, this artificiallygenerated word map can replace the real image inputs for such a VL model.Through an extensive set of experiments, our model not only establishes aneffective baseline for this novel task but also demonstrates strongperformances compared to existing methods that rely on stronger supervision,such as task-specific images and segmentation masks. Code is available athttps://github.com/alinlab/ifseg.", "output": "IFSeg: Image-free Semantic Segmentation via Vision-Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning-based recognition systems are deployed at scale for severalreal-world applications that inevitably involve our social life. Although beingof great support when making complex decisions, they might capture spuriousdata correlations and leverage sensitive attributes (e.g. age, gender,ethnicity). How to factor out this information while keeping a high predictionperformance is a task with still several open questions, many of which areshared with those of the domain adaptation and generalization literature whichfocuses on avoiding visual domain biases. In this work, we propose an in-depthstudy of the relationship between cross-domain learning (CD) and model fairnessby introducing a benchmark on face and medical images spanning severaldemographic groups as well as classification and localization tasks. Afterhaving highlighted the limits of the current evaluation metrics, we introduce anew Harmonic Fairness (HF) score to assess jointly how fair and accurate everymodel is with respect to a reference baseline. Our study covers 14 CDapproaches alongside three state-of-the-art fairness algorithms and shows howthe former can outperform the latter. Overall, our work paves the way for amore systematic analysis of fairness problems in computer vision. Codeavailable at: https://github.com/iurada/fairness_crossdomain", "output": "Fairness meets Cross-Domain Learning: a new perspective on Models and Metrics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have witnessed a rapid growth of deep generative models, withtext-to-image models gaining significant attention from the public. However,existing models often generate images that do not align well with humanaesthetic preferences, such as awkward combinations of limbs and facialexpressions. To address this issue, we collect a dataset of human choices ongenerated images from the Stable Foundation Discord channel. Our experimentsdemonstrate that current evaluation metrics for generative models do notcorrelate well with human choices. Thus, we train a human preference classifierwith the collected dataset and derive a Human Preference Score (HPS) based onthe classifier. Using the HPS, we propose a simple yet effective method toadapt Stable Diffusion to better align with human aesthetic preferences. Ourexperiments show that the HPS outperforms CLIP in predicting human choices andhas good generalization capability towards images generated from other models.By tuning Stable Diffusion with the guidance of the HPS, the adapted model isable to generate images that are more preferred by human users.", "output": "Better Aligning Text-to-Image Models with Human Preference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The model's ability to understand synonymous expression is crucial in manykinds of downstream tasks. It will make the model to better understand thesimilarity between context, and more robust to the synonym substitution attack.However, many Pretrained Language Model (PLM) lack synonym knowledge due tolimitation of small-scale synsets and PLM's pretraining objectives. In thispaper, we propose a framework called Sem4SAP to mine synsets from OpenKnowledge Graph (Open-KG) and using the mined synsets to do synonym-awarepretraining for language models. We propose to coarsly filter the content inOpen-KG and use the frequency information to better help the clustering processunder low-resource unsupervised conditions. We expand the mined synsets bymigrating core semantics between synonymous expressions.We also propose twonovel and effective synonym-aware pre-training methods for injecting synonymknowledge into PLMs.Extensive experiments demonstrate that Sem4SAP candramatically outperform the original PLMs and other baselines on ten differenttasks.", "output": "Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Beta-VAE is a very classical model for disentangled representation learning,the use of an expanding bottleneck that allow information into the decodergradually is key to representation disentanglement as well as high-qualityreconstruction. During recent experiments on such fascinating structure, wediscovered that the total amount of latent variables can affect therepresentation learnt by the network: with very few latent variables, thenetwork tend to learn the most important or principal variables, acting like aPCA; with very large numbers of latent variables, the variables tend to be moredisentangled, and act like an ICA. Our assumption is that the competitionbetween latent variables while trying to gain the most information bandwidthcan lead to this phenomenon.", "output": "Beta-VAE has 2 Behaviors: PCA or ICA?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Every human being in this world produces waste. South Africa is a developingcountry with many townships that have limited waste resources. Over-increasingpopulation growth overpowers the volume of most municipal authorities toprovide even the most essential services. Waste in townships is produced vialittering, dumping of bins, cutting of trees, dumping of waste near rivers, andoverrunning of waste bins. Waste increases diseases, air pollution, andenvironmental pollution, and lastly increases gas emissions that contribute tothe release of greenhouse gases. The ungathered waste is dumped widely in thestreets and drains contributing to flooding, breeding of insects, rodentvectors, and spreading of diseases. Therefore, the aim of this paper is todesign a smart waste management system for the city of Johannesburg. The cityof Johannesburg contains waste municipality workers and has provided some areaswith waste resources such as waste bins and trucks for collecting waste. Butthe problem is that the resources only are not enough to solve the problem ofwaste in the city. The waste municipality uses traditional ways of collectingwaste such as going to each street and picking up waste bins. The traditionalway has worked for years but as the population is increasing more waste isproduced which causes various problems for the waste municipalities and thepublic at large. The proposed system consists of sensors, user applications,and a real-time monitoring system. This paper adopts the experimentalmethodology.", "output": "Design of a Smart Waste Management System for the City of Johannesburg."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data privacy has become an increasingly important concern in real-world bigdata applications such as machine learning. To address the problem, federatedlearning (FL) has been a promising solution to building effective machinelearning models from decentralized and private data. Existing federatedlearning algorithms mainly tackle the supervised learning problem, where dataare assumed to be fully labeled. However, in practice, fully labeled data isoften hard to obtain, as the participants may not have sufficient domainexpertise, or they lack the motivation and tools to label data. Therefore, theproblem of federated learning without full labels is important in real-world FLapplications. In this paper, we discuss how the problem can be solved withmachine learning techniques that leverage unlabeled data. We present a surveyof methods that combine FL with semi-supervised learning, self-supervisedlearning, and transfer learning methods. We also summarize the datasets used toevaluate FL methods without full labels. Finally, we highlight futuredirections in the context of FL without full labels.", "output": "Federated Learning without Full Labels: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In-betweening is a technique for generating transitions given initial andtarget character states. The majority of existing works require multiple (often$&gt;$10) frames as input, which are not always accessible. Our work deals with afocused yet challenging problem: to generate the transition when given exactlytwo frames (only the first and last). To cope with this challenging scenario,we implement our bi-directional scheme which generates forward and backwardtransitions from the start and end frames with two adversarial autoregressivenetworks, and stitches them in the middle of the transition where there is nostrict ground truth. The autoregressive networks based on conditionalvariational autoencoders (CVAE) are optimized by searching for a pair ofoptimal latent codes that minimize a novel stitching loss between theiroutputs. Results show that our method achieves higher motion quality and morediverse results than existing methods on both the LaFAN1 and Human3.6mdatasets.", "output": "Diverse Motion In-betweening with Dual Posture Stitching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Densest Subgraph Problem requires to find, in a given graph, a subset ofvertices whose induced subgraph maximizes a measure of density. The problem hasreceived a great deal of attention in the algorithmic literature over the lastfive decades, with many variants proposed and many applications built on top ofthis basic definition. Recent years have witnessed a revival of researchinterest on this problem with several interesting contributions, including somegroundbreaking results, published in 2022 and 2023. This survey provides a deepoverview of the fundamental results and an exhaustive coverage of the manyvariants proposed in the literature, with a special attention on the mostrecent results. The survey also presents a comprehensive overview ofapplications and discusses some interesting open problems for this evergreenresearch topic.", "output": "A Survey on the Densest Subgraph Problem and its Variants."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many skeletal action recognition models use GCNs to represent the human bodyby 3D body joints connected body parts. GCNs aggregate one- or few-hop graphneighbourhoods, and ignore the dependency between not linked body joints. Wepropose to form hypergraph to model hyper-edges between graph nodes (e.g.,third- and fourth-order hyper-edges capture three and four nodes) which helpcapture higher-order motion patterns of groups of body joints. We split actionsequences into temporal blocks, Higher-order Transformer (HoT) producesembeddings of each temporal block based on (i) the body joints, (ii) pairwiselinks of body joints and (iii) higher-order hyper-edges of skeleton bodyjoints. We combine such HoT embeddings of hyper-edges of orders 1, ..., r by anovel Multi-order Multi-mode Transformer (3Mformer) with two modules whoseorder can be exchanged to achieve coupled-mode attention on coupled-mode tokensbased on 'channel-temporal block', 'order-channel-body joint','channel-hyper-edge (any order)' and 'channel-only' pairs. The first module,called Multi-order Pooling (MP), additionally learns weighted aggregation alongthe hyper-edge mode, whereas the second module, Temporal block Pooling (TP),aggregates along the temporal block mode. Our end-to-end trainable networkyields state-of-the-art results compared to GCN-, transformer- andhypergraph-based counterparts.", "output": "3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Among the pressing issues facing Australian and other First Nations peoplesis the repatriation of the bodily remains of their ancestors, which arecurrently held in Western scientific institutions. The success of securing thereturn of these remains to their communities for reburial depends largely onlocating information within scientific and other literature published between1790 and 1970 documenting their theft, donation, sale, or exchange betweeninstitutions. This article reports on collaborative research by data scientistsand social science researchers in the Research, Reconcile, Renew Network (RRR)to develop and apply text mining techniques to identify this vital information.We describe our work to date on developing a machine learning-based solution toautomate the process of finding and semantically analysing relevant texts.Classification models, particularly deep learning-based models, are known tohave low accuracy when trained with small amounts of labelled (i.e.relevant/non-relevant) documents. To improve the accuracy of our detectionmodel, we explore the use of an Informed Neural Network (INN) model thatdescribes documentary content using expert-informed contextual knowledge. Onlya few labelled documents are used to provide specificity to the model, usingconceptually related keywords identified by RRR experts in provenance research.The results confirm the value of using an INN network model for identifyingrelevant documents related to the investigation of the global commercial tradein Indigenous human remains. Empirical analysis suggests that this INN modelcan be generalized for use by other researchers in the social sciences andhumanities who want to extract relevant information from large textual corpora.", "output": "Informed Machine Learning, Centrality, CNN, Relevant Document Detection, Repatriation of Indigenous Human Remains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Taxonomy is formulated as directed acyclic concepts graphs or trees thatsupport many downstream tasks. Many new coming concepts need to be added to anexisting taxonomy. The traditional taxonomy expansion task aims only at findingthe best position for new coming concepts in the existing taxonomy. However,they have two drawbacks when being applied to the real-scenarios. The previousmethods suffer from low-efficiency since they waste much time when most of thenew coming concepts are indeed noisy concepts. They also suffer fromlow-effectiveness since they collect training samples only from the existingtaxonomy, which limits the ability of the model to mine more hypernym-hyponymrelationships among real concepts. This paper proposes a pluggable frameworkcalled Generative Adversarial Network for Taxonomy Entering Evaluation (GANTEE)to alleviate these drawbacks. A generative adversarial network is designed inthis framework by discriminative models to alleviate the first drawback and thegenerative model to alleviate the second drawback. Two discriminators are usedin GANTEE to provide long-term and short-term rewards, respectively. Moreover,to further improve the efficiency, pre-trained language models are used toretrieve the representation of the concepts quickly. The experiments on threereal-world large-scale datasets with two different languages show that GANTEEimproves the performance of the existing taxonomy expansion methods in botheffectiveness and efficiency.", "output": "GANTEE: Generative Adversatial Network for Taxonomy Entering Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While supervised learning assumes the presence of labeled data, we may haveprior information about how models should behave. In this paper, we formalizethis notion as learning from explanation constraints and provide a learningtheoretic framework to analyze how such explanations can improve the learningof our models. For what models would explanations be helpful? Our first keycontribution addresses this question via the definition of what we call EPACmodels (models that satisfy these constraints in expectation over new data),and we analyze this class of models using standard learning theoretic tools.Our second key contribution is to characterize these restrictions (in terms oftheir Rademacher complexities) for a canonical class of explanations given bygradient information for linear models and two layer neural networks. Finally,we provide an algorithmic solution for our framework, via a variationalapproximation that achieves better performance and satisfies these constraintsmore frequently, when compared to simpler augmented Lagrangian methods toincorporate these explanations. We demonstrate the benefits of our approachover a large array of synthetic and real-world experiments.", "output": "Learning with Explanation Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We compare the performance of two popular algorithms, fictitious play andcounterfactual regret minimization, in approximating Nash equilibrium inmultiplayer games. Despite recent success of counterfactual regret minimizationin multiplayer poker and conjectures of its superiority, we show thatfictitious play leads to improved Nash equilibrium approximation over a varietyof game classes and sizes.", "output": "Fictitious Play Outperforms Counterfactual Regret Minimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The theoretical understanding of MOEAs is lagging far behind their success inpractice. In particular, previous theory work considers mostly easy problemsthat are composed of unimodal objectives.As a first step towards a deeper understanding of how evolutionary algorithmssolve multimodal multiobjective problems, we propose the OJZJ problem, abi-objective problem composed of two objectives isomorphic to the classic jumpfunction benchmark. We prove that SEMO with probability one does not computethe full Pareto front, regardless of the runtime. In contrast, for all problemsizes $n$ and all jump sizes ${k \\in [4..\\frac n2 - 1]}$, the global SEMO(GSEMO) covers the Pareto front in an expected number of $\\Theta((n-2k)n^{k})$iterations. For $k = o(n)$, we also show the tighter bound $\\frac 32 e n^{k+1}\\pm o(n^{k+1})$, which might be the first runtime bound for an MOEA that istight apart from lower-order terms. We also combine the GSEMO with twoapproaches that showed advantages in single-objective multimodal problems. Whenusing the GSEMO with a heavy-tailed mutation operator, the expected runtimeimproves by a factor of at least $k^{\\Omega(k)}$. When adapting the recentstagnation-detection strategy of Rajabi and Witt (2022) to the GSEMO, theexpected runtime also improves by a factor of at least $k^{\\Omega(k)}$ andsurpasses the heavy-tailed GSEMO by a small polynomial factor in $k$. Via anexperimental analysis, we show that these asymptotic differences are visiblealready for small problem sizes: A factor-$5$ speed-up from heavy-tailedmutation and a factor-$10$ speed-up from stagnation detection can be observedalready for jump size~$4$ and problem sizes between $10$ and $50$. Overall, ourresults show that the ideas recently developed to aid single-objectiveevolutionary algorithms to cope with local optima can be effectively employedalso in multiobjective optimization.", "output": "Theoretical Analyses of Multiobjective Evolutionary Algorithms on Multimodal Objectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image registration is a crucial task in signal processing, but it oftenencounters issues with stability and efficiency. Non-learning registrationapproaches rely on optimizing similarity metrics between fixed and movingimages, which can be expensive in terms of time and space complexity. Thisproblem can be exacerbated when the images are large or there are significantdeformations between them. Recently, deep learning, specifically convolutionalneural network (CNN)-based methods, have been explored as an effective solutionto the weaknesses of non-learning approaches. To further advance learningapproaches in image registration, we introduce an attention mechanism in thedeformable image registration problem. Our proposed approach is based on aTransformer framework called AiR, which can be efficiently trained on GPGPUdevices. We treat the image registration problem as a language translation taskand use the Transformer to learn the deformation field. The method learns anunsupervised generated deformation map and is tested on two benchmark datasets.In summary, our approach shows promising effectiveness in addressing stabilityand efficiency issues in image registration tasks. The source code of AiR isavailable on Github.", "output": "Attention for Image Registration (AiR): an unsupervised Transformer approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization for semantic segmentation is highly demanded in realapplications, where a trained model is expected to work well in previouslyunseen domains. One challenge lies in the lack of data which could cover thediverse distributions of the possible unseen domains for training. In thispaper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme,which is the first to exploit the diversity of web-crawled images forgeneralizable semantic segmentation. To explore and exploit the real-world datadistributions, we collect a web-crawled dataset which presents large diversityin terms of weather conditions, sites, lighting, camera styles, etc. We alsopresent a method which injects the style representation of the web-crawled datainto the source domain on-the-fly during training, which enables the network toexperience images of diverse styles with reliable labels for effectivetraining. Moreover, we use the web-crawled dataset with predicted pseudo labelsfor training to further enhance the capability of the network. Extensiveexperiments demonstrate that our method clearly outperforms existing domaingeneralization techniques.", "output": "WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomaly detection on attributed networks is widely used in online shopping,financial transactions, communication networks, and so on. However, mostexisting works trying to detect anomalies on attributed networks only considera single kind of interaction, so they cannot deal with various kinds ofinteractions on multi-view attributed networks. It remains a challenging taskto jointly consider all different kinds of interactions and detect anomalousinstances on multi-view attributed networks. In this paper, we propose a graphconvolution-based framework, named AnomMAN, to detect Anomaly on Multi-viewAttributed Networks. To jointly consider attributes and all kinds ofinteractions on multi-view attributed networks, we use the attention mechanismto define the importance of all views in networks. Since the low-passcharacteristic of graph convolution operation filters out most high-frequencysignals (aonmaly signals), it cannot be directly applied to anomaly detectiontasks. AnomMAN introduces the graph auto-encoder module to turn thedisadvantage of low-pass features into an advantage. According to experimentson real-world datasets, AnomMAN outperforms the state-of-the-art models and twovariants of our proposed model.", "output": "AnomMAN: Detect Anomaly on Multi-view Attributed Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "PGMax is an open-source Python package for (a) easily specifying discreteProbabilistic Graphical Models (PGMs) as factor graphs; and (b) automaticallyrunning efficient and scalable loopy belief propagation (LBP) in JAX. PGMaxsupports general factor graphs with tractable factors, and leverages modernaccelerators like GPUs for inference. Compared with existing alternatives,PGMax obtains higher-quality inference results with up to threeorders-of-magnitude inference time speedups. PGMax additionally interactsseamlessly with the rapidly growing JAX ecosystem, opening up new researchpossibilities. Our source code, examples and documentation are available athttps://github.com/deepmind/PGMax.", "output": "PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point cloud data have been widely explored due to its superior accuracy androbustness under various adverse situations. Meanwhile, deep neural networks(DNNs) have achieved very impressive success in various applications such assurveillance and autonomous driving. The convergence of point cloud and DNNshas led to many deep point cloud models, largely trained under the supervisionof large-scale and densely-labelled point cloud data. Unsupervised point cloudrepresentation learning, which aims to learn general and useful point cloudrepresentations from unlabelled point cloud data, has recently attractedincreasing attention due to the constraint in large-scale point cloudlabelling. This paper provides a comprehensive review of unsupervised pointcloud representation learning using DNNs. It first describes the motivation,general pipelines as well as terminologies of the recent studies. Relevantbackground including widely adopted point cloud datasets and DNN architecturesis then briefly presented. This is followed by an extensive discussion ofexisting unsupervised point cloud representation learning methods according totheir technical approaches. We also quantitatively benchmark and discuss thereviewed methods over multiple widely adopted point cloud datasets. Finally, weshare our humble opinion about several challenges and problems that could bepursued in future research in unsupervised point cloud representation learning.A project associated with this survey has been built athttps://github.com/xiaoaoran/3d_url_survey.", "output": "Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Each year, deep learning demonstrates new and improved empirical results withdeeper and wider neural networks. Meanwhile, with existing theoreticalframeworks, it is difficult to analyze networks deeper than two layers withoutresorting to counting parameters or encountering sample complexity bounds thatare exponential in depth. Perhaps it may be fruitful to try to analyze modernmachine learning under a different lens. In this paper, we propose a novelinformation-theoretic framework with its own notions of regret and samplecomplexity for analyzing the data requirements of machine learning. With ourframework, we first work through some classical examples such as scalarestimation and linear regression to build intuition and introduce generaltechniques. Then, we use the framework to study the sample complexity oflearning from data generated by deep neural networks with ReLU activationunits. For a particular prior distribution on weights, we establish samplecomplexity bounds that are simultaneously width independent and linear indepth. This prior distribution gives rise to high-dimensional latentrepresentations that, with high probability, admit reasonably accuratelow-dimensional approximations. We conclude by corroborating our theoreticalresults with experimental analysis of random single-hidden-layer neuralnetworks.", "output": "An Information-Theoretic Framework for Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the last few years, there have been several revolutions in the field ofdeep learning, mainly headlined by the large impact of Generative AdversarialNetworks (GANs). GANs not only provide an unique architecture when definingtheir models, but also generate incredible results which have had a directimpact on society. Due to the significant improvements and new areas ofresearch that GANs have brought, the community is constantly coming up with newresearches that make it almost impossible to keep up with the times. Our surveyaims to provide a general overview of GANs, showing the latest architectures,optimizations of the loss functions, validation metrics and application areasof the most widely recognized variants. The efficiency of the differentvariants of the model architecture will be evaluated, as well as showing thebest application area; as a vital part of the process, the different metricsfor evaluating the performance of GANs and the frequently used loss functionswill be analyzed. The final objective of this survey is to provide a summary ofthe evolution and performance of the GANs which are having better results toguide future researchers in the field.", "output": "A survey on GANs for computer vision: Recent research, analysis and taxonomy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study multi-modal few-shot object detection (FSOD) in this paper, usingboth few-shot visual examples and class semantic information for detection,which are complementary to each other by definition. Most of the previous workson multi-modal FSOD are fine-tuning-based which are inefficient for onlineapplications. Moreover, these methods usually require expertise like classnames to extract class semantic embedding, which are hard to get for rareclasses. Our approach is motivated by the high-level conceptual similarity of(metric-based) meta-learning and prompt-based learning to learn generalizablefew-shot and zero-shot object detection models respectively withoutfine-tuning. Specifically, we combine the few-shot visual classifier and textclassifier learned via meta-learning and prompt-based learning respectively tobuild the multi-modal classifier and detection models. In addition, to fullyexploit the pre-trained language models, we propose meta-learning-basedcross-modal prompting to generate soft prompts for novel classes present infew-shot visual examples, which are then used to learn the text classifier.Knowledge distillation is introduced to learn the soft prompt generator withoutusing human prior knowledge of class names, which may not be available for rareclasses. Our insight is that the few-shot support images naturally includerelated context information and semantics of the class. We comprehensivelyevaluate the proposed multi-modal FSOD models on multiple few-shot objectdetection benchmarks, achieving promising results.", "output": "Multi-Modal Few-Shot Object Detection with Meta-Learning-Based Cross-Modal Prompting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given entities and their interactions in the web data, which may haveoccurred at different time, how can we find communities of entities and tracktheir evolution? In this paper, we approach this important task from graphclustering perspective. Recently, state-of-the-art clustering performance invarious domains has been achieved by deep clustering methods. Especially, deepgraph clustering (DGC) methods have successfully extended deep clustering tograph-structured data by learning node representations and cluster assignmentsin a joint optimization framework. Despite some differences in modeling choices(e.g., encoder architectures), existing DGC methods are mainly based onautoencoders and use the same clustering objective with relatively minoradaptations. Also, while many real-world graphs are dynamic, previous DGCmethods considered only static graphs. In this work, we develop CGC, a novelend-to-end framework for graph clustering, which fundamentally differs fromexisting methods. CGC learns node embeddings and cluster assignments in acontrastive graph learning framework, where positive and negative samples arecarefully selected in a multi-level scheme such that they reflect hierarchicalcommunity structures and network homophily. Also, we extend CGC fortime-evolving data, where temporal graph clustering is performed in anincremental learning fashion, with the ability to detect change points.Extensive evaluation on real-world graphs demonstrates that the proposed CGCconsistently outperforms existing methods.", "output": "CGC: Contrastive Graph Clustering for Community Detection and Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep hashing has been extensively utilized in massive image retrieval becauseof its efficiency and effectiveness. However, deep hashing models arevulnerable to adversarial examples, making it essential to develop adversarialdefense methods for image retrieval. Existing solutions achieved limiteddefense performance because of using weak adversarial samples for training andlacking discriminative optimization objectives to learn robust features. Inthis paper, we present a min-max based Center-guided Adversarial Training,namely CgAT, to improve the robustness of deep hashing networks through worstadversarial examples. Specifically, we first formulate the center code as asemantically-discriminative representative of the input image content, whichpreserves the semantic similarity with positive samples and dissimilarity withnegative examples. We prove that a mathematical formula can calculate thecenter code immediately. After obtaining the center codes in each optimizationiteration of the deep hashing network, they are adopted to guide theadversarial training process. On the one hand, CgAT generates the worstadversarial examples as augmented data by maximizing the Hamming distancebetween the hash codes of the adversarial examples and the center codes. On theother hand, CgAT learns to mitigate the effects of adversarial samples byminimizing the Hamming distance to the center codes. Extensive experiments onthe benchmark datasets demonstrate the effectiveness of our adversarialtraining algorithm in defending against adversarial attacks for deephashing-based retrieval. Compared with the current state-of-the-art defensemethod, we significantly improve the defense performance by an average of18.61\\%, 12.35\\%, and 11.56\\% on FLICKR-25K, NUS-WIDE, and MS-COCO,respectively. The code is available at https://github.com/xunguangwang/CgAT.", "output": "CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose CLIP-Dissect, a new technique to automaticallydescribe the function of individual hidden neurons inside vision networks.CLIP-Dissect leverages recent advances in multimodal vision/language models tolabel internal neurons with open-ended concepts without the need for anylabeled data or human examples. We show that CLIP-Dissect provides moreaccurate descriptions than existing methods for last layer neurons where theground-truth is available as well as qualitatively good descriptions for hiddenlayer neurons. In addition, our method is very flexible: it is model agnostic,can easily handle new concepts and can be extended to take advantage of bettermultimodal models in the future. Finally CLIP-Dissect is computationallyefficient and can label all neurons from five layers of ResNet-50 in just 4minutes, which is more than 10 times faster than existing methods. Our code isavailable at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.", "output": "CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We are concerned with retrieving a query person from multiple videos capturedby a non-overlapping camera network. Existing methods often rely on purelyvisual matching or consider temporal constraints but ignore the spatialinformation of the camera network. To address this issue, we propose apedestrian retrieval framework based on cross-camera trajectory generation,which integrates both temporal and spatial information. To obtain pedestriantrajectories, we propose a novel cross-camera spatio-temporal model thatintegrates pedestrians' walking habits and the path layout between cameras toform a joint probability distribution. Such a spatio-temporal model among acamera network can be specified using sparsely sampled pedestrian data. Basedon the spatio-temporal model, cross-camera trajectories can be extracted by theconditional random field model and further optimized by restricted non-negativematrix factorization. Finally, a trajectory re-ranking technique is proposed toimprove the pedestrian retrieval results. To verify the effectiveness of ourmethod, we construct the first cross-camera pedestrian trajectory dataset, thePerson Trajectory Dataset, in real surveillance scenarios. Extensiveexperiments verify the effectiveness and robustness of the proposed method.", "output": "Cross-Camera Trajectories Help Person Retrieval in a Camera Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The topic of comprehensibility of machine-learned theories has recently drawnincreasing attention. Inductive Logic Programming (ILP) uses logic programmingto derive logic theories from small data based on abduction and inductiontechniques. Learned theories are represented in the form of rules asdeclarative descriptions of obtained knowledge. In earlier work, the authorsprovided the first evidence of a measurable increase in human comprehensionbased on machine-learned logic rules for simple classification tasks. In alater study, it was found that the presentation of machine-learned explanationsto humans can produce both beneficial and harmful effects in the context ofgame learning. We continue our investigation of comprehensibility by examiningthe effects of the ordering of concept presentations on human comprehension. Inthis work, we examine the explanatory effects of curriculum order and thepresence of machine-learned explanations for sequential problem-solving. Weshow that 1) there exist tasks A and B such that learning A before B has abetter human comprehension with respect to learning B before A and 2) thereexist tasks A and B such that the presence of explanations when learning Acontributes to improved human comprehension when subsequently learning B. Wepropose a framework for the effects of sequential teaching on comprehensionbased on an existing definition of comprehensibility and provide evidence forsupport from data collected in human trials. Empirical results show thatsequential teaching of concepts with increasing complexity a) has a beneficialeffect on human comprehension and b) leads to human re-discovery ofdivide-and-conquer problem-solving strategies, and c) studying machine-learnedexplanations allows adaptations of human problem-solving strategy with betterperformance.", "output": "Explanatory machine learning for sequential human teaching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The asymptotic mean squared test error and sensitivity of the Random FeaturesRegression model (RFR) have been recently studied. We build on this work andidentify in closed-form the family of Activation Functions (AFs) that minimizea combination of the test error and sensitivity of the RFR under differentnotions of functional parsimony. We find scenarios under which the optimal AFsare linear, saturated linear functions, or expressible in terms of Hermitepolynomials. Finally, we show how using optimal AFs impacts well-establishedproperties of the RFR model, such as its double descent curve, and thedependency of its optimal regularization parameter on the observation noiselevel.", "output": "Optimal Activation Functions for the Random Features Regression Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the latest advances in Deep Learning-based} generative models, it hasnot taken long to take advantage of their remarkable performance in the area oftime series. Deep neural networks used to work with time series heavily dependon the size and consistency of the datasets used in training. These featuresare not usually abundant in the real world, where they are usually limited andoften have constraints that must be guaranteed. Therefore, an effective way toincrease the amount of data is by using Data Augmentation techniques, either byadding noise or permutations and by generating new synthetic data. This worksystematically reviews the current state-of-the-art in the area to provide anoverview of all available algorithms and proposes a taxonomy of the mostrelevant research. The efficiency of the different variants will be evaluatedas a central part of the process, as well as the different metrics to evaluatethe performance and the main problems concerning each model will be analysed.The ultimate aim of this study is to provide a summary of the evolution andperformance of areas that produce better results to guide future researchers inthis field.", "output": "Data Augmentation techniques in time series domain: A survey and taxonomy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An oft-cited challenge of federated learning is the presence ofheterogeneity. \\emph{Data heterogeneity} refers to the fact that data fromdifferent clients may follow very different distributions. \\emph{Systemheterogeneity} refers to client devices having different system capabilities. Aconsiderable number of federated optimization methods address this challenge.In the literature, empirical evaluations usually start federated training fromrandom initialization. However, in many practical applications of federatedlearning, the server has access to proxy data for the training task that can beused to pre-train a model before starting federated training. Using fourstandard federated learning benchmark datasets, we empirically study the impactof starting from a pre-trained model in federated learning. Unsurprisingly,starting from a pre-trained model reduces the training time required to reach atarget error rate and enables the training of more accurate models (up to 40\\%)than is possible when starting from random initialization. Surprisingly, wealso find that starting federated learning from a pre-trained initializationreduces the effect of both data and system heterogeneity. We recommend futurework proposing and evaluating federated optimization methods to evaluate theperformance when starting from random and pre-trained initializations. Thisstudy raises several questions for further work on understanding the role ofheterogeneity in federated optimization. \\footnote{Our code is available at:\\url{https://github.com/facebookresearch/where_to_begin}}", "output": "Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The teacher-free online Knowledge Distillation (KD) aims to train an ensembleof multiple student models collaboratively and distill knowledge from eachother. Although existing online KD methods achieve desirable performance, theyoften focus on class probabilities as the core knowledge type, ignoring thevaluable feature representational information. We present a Mutual ContrastiveLearning (MCL) framework for online KD. The core idea of MCL is to performmutual interaction and transfer of contrastive distributions among a cohort ofnetworks in an online manner. Our MCL can aggregate cross-network embeddinginformation and maximize the lower bound to the mutual information between twonetworks. This enables each network to learn extra contrastive knowledge fromothers, leading to better feature representations, thus improving theperformance of visual recognition tasks. Beyond the final layer, we extend MCLto intermediate layers and perform an adaptive layer-matching mechanism trainedby meta-optimization. Experiments on image classification and transfer learningto visual recognition tasks show that layer-wise MCL can lead to consistentperformance gains against state-of-the-art online KD approaches. Thesuperiority demonstrates that layer-wise MCL can guide the network to generatebetter feature representations. Our code is publicly avaliable athttps://github.com/winycg/L-MCL.", "output": "Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rapid discovery of new reactions and molecules in recent years has beenfacilitated by the advancements in high throughput screening, accessibility toa much more complex chemical design space, and the development of accuratemolecular modeling frameworks. A holistic study of the growing chemistryliterature is, therefore, required that focuses on understanding the recenttrends and extrapolating them into possible future trajectories. To this end,several network theory-based studies have been reported that use a directedgraph representation of chemical reactions. Here, we perform a study based onrepresenting chemical reactions as hypergraphs where the hyperedges representchemical reactions and nodes represent the participating molecules. We use astandard reactions dataset to construct a hypernetwork and report itsstatistics such as degree distributions, average path length, assortativity ordegree correlations, PageRank centrality, and graph-based clusters (orcommunities). We also compute each statistic for an equivalent directed graphrepresentation of reactions to draw parallels and highlight differences betweenthe two. To demonstrate the AI applicability of hypergraph reactionrepresentation, we generate dense hypergraph embeddings and use them in thereaction classification problem. We conclude that the hypernetworkrepresentation is flexible, preserves reaction context, and uncovers hiddeninsights that are otherwise not apparent in a traditional directed graphrepresentation of chemical reactions.", "output": "AI-driven Hypergraph Network of Organic Chemistry: Network Statistics and Applications in Reaction Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Metaverse encapsulates our expectations of the next-generation Internet,while bringing new key performance indicators (KPIs). Although conventionalultra-reliable and low-latency communications (URLLC) can satisfy objectiveKPIs, it is difficult to provide a personalized immersive experience that is adistinctive feature of the Metaverse. Since the quality of experience (QoE) canbe regarded as a comprehensive KPI, the URLLC is evolved towards the nextgeneration URLLC (xURLLC) with a personalized resource allocation scheme toachieve higher QoE. To deploy Metaverse xURLLC services, we study theinteraction between the Metaverse service provider (MSP) and the networkinfrastructure provider (InP), and provide an optimal contract designframework. Specifically, the utility of the MSP, defined as a function ofMetaverse users' QoE, is to be maximized, while ensuring the incentives of theInP. To model the QoE mathematically, we propose a novel metric namedMeta-Immersion that incorporates both the objective KPIs and subjectivefeelings of Metaverse users. Furthermore, we develop an attention-awarerendering capacity allocation scheme to improve QoE in xURLLC. Using auser-object-attention level dataset, we validate that the xURLLC can achieve anaverage of 20.1% QoE improvement compared to the conventional URLLC with auniform resource allocation scheme.", "output": "Attention-aware Resource Allocation and QoE Analysis for Metaverse xURLLC Services."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the most neglected sources of energy loss is streetlights whichgenerate too much light in areas where it is not required. Energy waste hasenormous economic and environmental effects. In addition, due to theconventional manual nature of the operation, streetlights are frequently seenbeing turned ON during the day and OFF in the evening, which is regrettableeven in the twenty-first century. These issues require automated streetlightcontrol in order to be resolved. This study aims to develop a novel streetlightcontrolling method by combining a smart transport monitoring system powered bycomputer vision technology with a closed circuit television (CCTV) camera thatallows the light-emitting diode (LED) streetlight to automatically light upwith the appropriate brightness by detecting the presence of pedestrians orvehicles and dimming the streetlight in their absence using semantic imagesegmentation from the CCTV video streaming. Consequently, our modeldistinguishes daylight and nighttime, which made it feasible to automate theprocess of turning the streetlight 'ON' and 'OFF' to save energy consumptioncosts. According to the aforementioned approach, geolocation sensor data couldbe utilized to make more informed streetlight management decisions. To completethe tasks, we consider training the U-net model with ResNet-34 as its backbone.The validity of the models is guaranteed with the use of assessment matrices.The suggested concept is straightforward, economical, energy-efficient,long-lasting, and more resilient than conventional alternatives.", "output": "CNN based Intelligent Streetlight Management Using Smart CCTV Camera and Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision transformers (ViT) have shown promise in various vision tasks whilethe U-Net based on a convolutional neural network (CNN) remains dominant indiffusion models. We design a simple and general ViT-based architecture (namedU-ViT) for image generation with diffusion models. U-ViT is characterized bytreating all inputs including the time, condition and noisy image patches astokens and employing long skip connections between shallow and deep layers. Weevaluate U-ViT in unconditional and class-conditional image generation, as wellas text-to-image generation tasks, where U-ViT is comparable if not superior toa CNN-based U-Net of a similar size. In particular, latent diffusion modelswith U-ViT achieve record-breaking FID scores of 2.29 in class-conditionalimage generation on ImageNet 256x256, and 5.48 in text-to-image generation onMS-COCO, among methods without accessing large external datasets during thetraining of generative models. Our results suggest that, for diffusion-basedimage modeling, the long skip connection is crucial while the down-sampling andup-sampling operators in CNN-based U-Net are not always necessary. We believethat U-ViT can provide insights for future research on backbones in diffusionmodels and benefit generative modeling on large scale cross-modality datasets.", "output": "All are Worth Words: A ViT Backbone for Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-domain dialogue systems aim to interact with humans through naturallanguage texts in an open-ended fashion. Despite the recent success of superlarge dialogue systems such as ChatGPT, using medium-to-small-sized dialoguesystems remains the common practice as they are more lightweight andaccessible; however, generating diverse dialogue responses is challenging,especially with smaller models. In this work, we propose an Equal-size HardExpectation--Maximization (EqHard-EM) algorithm to train a multi-decoder modelfor diverse dialogue generation. Our algorithm assigns a sample to a decoder ina hard manner and additionally imposes an equal-assignment constraint to ensurethat all decoders are well-trained. We provide detailed theoretical analysis tojustify our approach. Further, experiments on two large-scale open-domaindialogue datasets verify that our EqHard-EM algorithm generates high-qualitydiverse responses.", "output": "An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series anomaly detection is a challenging problem due to the complextemporal dependencies and the limited label data. Although some algorithmsincluding both traditional and deep models have been proposed, most of themmainly focus on time-domain modeling, and do not fully utilize the informationin the frequency domain of the time series data. In this paper, we propose aTime-Frequency analysis based time series Anomaly Detection model, or TFAD forshort, to exploit both time and frequency domains for performance improvement.Besides, we incorporate time series decomposition and data augmentationmechanisms in the designed time-frequency architecture to further boost theabilities of performance and interpretability. Empirical studies on widely usedbenchmark datasets show that our approach obtains state-of-the-art performancein univariate and multivariate time series anomaly detection tasks. Code isprovided at https://github.com/DAMO-DI-ML/CIKM22-TFAD.", "output": "TFAD: A Decomposition Time Series Anomaly Detection Architecture with Time-Frequency Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a smoothly broken power law functional form (referred to by us asa Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates thescaling behaviors of deep neural networks (i.e. how the evaluation metric ofinterest varies as the amount of compute used for training, number of modelparameters, training dataset size, model input size, number of training steps,or upstream performance varies) for various architectures and for each ofvarious tasks within a large and diverse set of upstream and downstream tasks,in zero-shot, prompted, and fine-tuned settings. This set includes large-scalevision, language, audio, video, diffusion, generative modeling, multimodallearning, contrastive learning, AI alignment, robotics, out-of-distribution(OOD) generalization, continual learning, transfer learning, uncertaintyestimation / calibration, out-of-distribution detection, adversarialrobustness, distillation, sparsity, retrieval, quantization, pruning,molecules, computer programming/coding, math word problems, \"emergent\" \"phasetransitions / changes\", arithmetic, unsupervised/self-supervised learning, andreinforcement learning (single agent and multi-agent). When compared to otherfunctional forms for neural scaling behavior, this functional form yieldsextrapolations of scaling behavior that are considerably more accurate on thisset. Moreover, this functional form accurately models and extrapolates scalingbehavior that other functional forms are incapable of expressing such as thenon-monotonic transitions present in the scaling behavior of phenomena such asdouble descent and the delayed, sharp inflection points present in the scalingbehavior of tasks such as arithmetic. Lastly, we use this functional form toglean insights about the limit of the predictability of scaling behavior. Codeis available at https://github.com/ethancaballero/broken_neural_scaling_laws", "output": "Broken Neural Scaling Laws."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing Visual Object Tracking (VOT) only takes the target area in the firstframe as a template. This causes tracking to inevitably fail in fast-changingand crowded scenes, as it cannot account for changes in object appearancebetween frames. To this end, we revamped the tracking framework withProgressive Context Encoding Transformer Tracker (ProContEXT), which coherentlyexploits spatial and temporal contexts to predict object motion trajectories.Specifically, ProContEXT leverages a context-aware self-attention module toencode the spatial and temporal context, refining and updating the multi-scalestatic and dynamic templates to progressively perform accurate tracking. Itexplores the complementary between spatial and temporal context, raising a newpathway to multi-context modeling for transformer-based trackers. In addition,ProContEXT revised the token pruning technique to reduce computationalcomplexity. Extensive experiments on popular benchmark datasets such as GOT-10kand TrackingNet demonstrate that the proposed ProContEXT achievesstate-of-the-art performance.", "output": "ProContEXT: Exploring Progressive Context Transformer for Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Streaming perception is a fundamental task in autonomous driving thatrequires a careful balance between the latency and accuracy of the autopilotsystem. However, current methods for streaming perception are limited as theyrely only on the current and adjacent two frames to learn movement patterns,which restricts their ability to model complex scenes, often leading to poordetection results. To address this limitation, we propose LongShortNet, a noveldual-path network that captures long-term temporal motion and integrates itwith short-term spatial semantics for real-time perception. Our proposedLongShortNet is notable as it is the first work to extend long-term temporalmodeling to streaming perception, enabling spatiotemporal feature fusion. Weevaluate LongShortNet on the challenging Argoverse-HD dataset and demonstratethat it outperforms existing state-of-the-art methods with almost no additionalcomputational cost.", "output": "LongShortNet: Exploring Temporal and Semantic Features Fusion in Streaming Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image Signal Processors (ISPs) play important roles in image recognitiontasks as well as in the perceptual quality of captured images. In most cases,experts make a lot of effort to manually tune many parameters of ISPs, but theparameters are sub-optimal. In the literature, two types of techniques havebeen actively studied: a machine learning-based parameter tuning technique anda DNN-based ISP technique. The former is lightweight but lacks expressivepower. The latter has expressive power, but the computational cost is too heavyon edge devices. To solve these problems, we propose \"DynamicISP,\" whichconsists of multiple classical ISP functions and dynamically controls theparameters of each frame according to the recognition result of the previousframe. We show our method successfully controls the parameters of multiple ISPfunctions and achieves state-of-the-art accuracy with low computational cost insingle and multi-category object detection tasks.", "output": "DynamicISP: Dynamically Controlled Image Signal Processor for Image Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Grammatical Error Correction (GEC) is the task of automatically detecting andcorrecting errors in text. The task not only includes the correction ofgrammatical errors, such as missing prepositions and mismatched subject-verbagreement, but also orthographic and semantic errors, such as misspellings andword choice errors respectively. The field has seen significant progress in thelast decade, motivated in part by a series of five shared tasks, which drovethe development of rule-based methods, statistical classifiers, statisticalmachine translation, and finally neural machine translation systems whichrepresent the current dominant state of the art. In this survey paper, wecondense the field into a single article and first outline some of thelinguistic challenges of the task, introduce the most popular datasets that areavailable to researchers (for both English and other languages), and summarisethe various methods and techniques that have been developed with a particularfocus on artificial error generation. We next describe the many differentapproaches to evaluation as well as concerns surrounding metric reliability,especially in relation to subjective human judgements, before concluding withan overview of recent progress and suggestions for future work and remainingchallenges. We hope that this survey will serve as comprehensive resource forresearchers who are new to the field or who want to be kept apprised of recentdevelopments.", "output": "Grammatical Error Correction: A Survey of the State of the Art."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Financial frauds cause billions of losses annually and yet it lacks efficientapproaches in detecting frauds considering user profile and their behaviorssimultaneously in social network . A social network forms a graph structurewhilst Graph neural networks (GNN), a promising research domain in DeepLearning, can seamlessly process non-Euclidean graph data . In financial frauddetection, the modus operandi of criminals can be identified by analyzing userprofile and their behaviors such as transaction, loaning etc. as well as theirsocial connectivity. Currently, most GNNs are incapable of selecting importantneighbors since the neighbors' edge attributes (i.e., behaviors) are ignored.In this paper, we propose a novel behavior information aggregation network(BIAN) to combine the user behaviors with other user features. Different fromits close \"relatives\" such as Graph Attention Networks (GAT) and GraphTransformer Networks (GTN), it aggregates neighbors based on neighboring edgeattribute distribution, namely, user behaviors in financial social network. Theexperimental results on a real-world large-scale financial social networkdataset, DGraph, show that BIAN obtains the 10.2% gain in AUROC comparing withthe State-Of-The-Art models.", "output": "Fraudulent User Detection Via Behavior Information Aggregation Network (BIAN) On Large-Scale Financial Social Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model-based deep learning has achieved astounding successes due in part tothe availability of large-scale real-world data. However, processing suchmassive amounts of data comes at a considerable cost in terms of computations,storage, training and the search for good neural architectures. Datasetdistillation has thus recently come to the fore. This paradigm involvesdistilling information from large real-world datasets into tiny and compactsynthetic datasets such that processing the latter ideally yields similarperformances as the former. State-of-the-art methods primarily rely on learningthe synthetic dataset by matching the gradients obtained during trainingbetween the real and synthetic data. However, these gradient-matching methodssuffer from the so-called accumulated trajectory error caused by thediscrepancy between the distillation and subsequent evaluation. To mitigate theadverse impact of this accumulated trajectory error, we propose a novelapproach that encourages the optimization algorithm to seek a flat trajectory.We show that the weights trained on synthetic data are robust against theaccumulated errors perturbations with the regularization towards the flattrajectory. Our method, called Flat Trajectory Distillation (FTD), is shown toboost the performance of gradient-matching methods by up to 4.7% on a subset ofimages of the ImageNet dataset with higher resolution images. We also validatethe effectiveness and generalizability of our method with datasets of differentresolutions and demonstrate its applicability to neural architecture search.Code is available at https://github.com/AngusDujw/FTD-distillation.", "output": "Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale commonsense knowledge bases empower a broad range of AIapplications, where the automatic extraction of commonsense knowledge (CKE) isa fundamental and challenging problem. CKE from text is known for sufferingfrom the inherent sparsity and reporting bias of commonsense in text. Visualperception, on the other hand, contains rich commonsense knowledge aboutreal-world entities, e.g., (person, can_hold, bottle), which can serve aspromising sources for acquiring grounded commonsense knowledge. In this work,we present CLEVER, which formulates CKE as a distantly supervisedmulti-instance learning problem, where models learn to summarize commonsenserelations from a bag of images about an entity pair without any humanannotation on image instances. To address the problem, CLEVER leveragesvision-language pre-training models for deep understanding of each image in thebag, and selects informative instances from the bag to summarize commonsenseentity relations via a novel contrastive attention mechanism. Comprehensiveexperimental results in held-out and human evaluation show that CLEVER canextract commonsense knowledge in promising quality, outperforming pre-trainedlanguage model-based methods by 3.9 AUC and 6.4 mAUC points. The predictedcommonsense scores show strong correlation with human judgment with a 0.78Spearman coefficient. Moreover, the extracted commonsense can also be groundedinto images with reasonable interpretability. The data and codes can beobtained at https://github.com/thunlp/CLEVER.", "output": "Visually Grounded Commonsense Knowledge Acquisition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object trackers deployed on low-power devices need to be light-weight,however, most of the current state-of-the-art (SOTA) methods rely on usingcompute-heavy backbones built using CNNs or transformers. Large sizes of suchmodels do not allow their deployment in low-power conditions and designingcompressed variants of large tracking models is of great importance. This paperdemonstrates how highly compressed light-weight object trackers can be designedusing neural architectural pruning of large CNN and transformer based trackers.Further, a comparative study on architectural choices best suited to designlight-weight trackers is provided. A comparison between SOTA trackers usingCNNs, transformers as well as the combination of the two is presented to studytheir stability at various compression ratios. Finally results for extremepruning scenarios going as low as 1% in some cases are shown to study thelimits of network pruning in object tracking. This work provides deeperinsights into designing highly efficient trackers from existing SOTA methods.", "output": "On Designing Light-Weight Object Trackers through Network Pruning: Use CNNs or Transformers?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot (FS) and zero-shot (ZS) learning are two different approaches forscaling temporal action detection (TAD) to new classes. The former adapts apretrained vision model to a new task represented by as few as a single videoper class, whilst the latter requires no training examples by exploiting asemantic description of the new class. In this work, we introduce a newmulti-modality few-shot (MMFS) TAD problem, which can be considered as amarriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and newclass names jointly. To tackle this problem, we further introduce a novelMUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled byefficiently bridging pretrained vision and language models whilst maximallyreusing already learned capacity. Concretely, we construct multi-modal promptsby mapping support videos into the textual token space of a vision-languagemodel using a meta-learned adapter-equipped visual semantics tokenizer. Totackle large intra-class variation, we further design a query featureregulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14demonstrate that our MUPPET outperforms state-of-the-art alternative methods,often by a large margin. We also show that our MUPPET can be easily extended totackle the few-shot object detection problem and again achieves thestate-of-the-art performance on MS-COCO dataset. The code will be available inhttps://github.com/sauradip/MUPPET", "output": "Multi-Modal Few-Shot Temporal Action Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Corgi, a novel method for text-to-image generation. Corgi is basedon our proposed shifted diffusion model, which achieves better image embeddinggeneration from input text. Unlike the baseline diffusion model used in DALL-E2, our method seamlessly encodes prior knowledge of the pre-trained CLIP modelin its diffusion process by designing a new initialization distribution and anew transition step of the diffusion. Compared to the strong DALL-E 2 baseline,our method performs better in generating image embedding from the text in termsof both efficiency and effectiveness, resulting in better text-to-imagegeneration. Extensive large-scale experiments are conducted and evaluated interms of both quantitative measures and human evaluation, indicating a strongergeneration ability of our method compared to existing ones. Furthermore, ourmodel enables semi-supervised and language-free training for text-to-imagegeneration, where only part or none of the images in the training dataset havean associated caption. Trained with only 1.7% of the images being captioned,our semi-supervised model obtains FID results comparable to DALL-E 2 onzero-shot text-to-image generation evaluated on MS-COCO. Corgi also achievesnew state-of-the-art results across different datasets on downstreamlanguage-free text-to-image generation tasks, outperforming the previousmethod, Lafite, by a large margin.", "output": "Shifted Diffusion for Text-to-image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable AI (XAI) is slowly becoming a key component for many AIapplications. Rule-based and modified backpropagation XAI approaches howeveroften face challenges when being applied to modern model architecturesincluding innovative layer building blocks, which is caused by two reasons.Firstly, the high flexibility of rule-based XAI methods leads to numerouspotential parameterizations. Secondly, many XAI methods break theimplementation-invariance axiom because they struggle with certain modelcomponents, e.g., BatchNorm layers. The latter can be addressed with modelcanonization, which is the process of re-structuring the model to disregardproblematic components without changing the underlying function. While modelcanonization is straightforward for simple architectures (e.g., VGG, ResNet),it can be challenging for more complex and highly interconnected models (e.g.,DenseNet). Moreover, there is only little quantifiable evidence that modelcanonization is beneficial for XAI. In this work, we propose canonizations forcurrently relevant model blocks applicable to popular deep neural networkarchitectures,including VGG, ResNet, EfficientNet, DenseNets, as well asRelation Networks. We further suggest a XAI evaluation framework with which wequantify and compare the effect sof model canonization for various XAI methodsin image classification tasks on the Pascal-VOC and ILSVRC2017 datasets, aswell as for Visual Question Answering using CLEVR-XAI. Moreover, addressing theformer issue outlined above, we demonstrate how our evaluation framework can beapplied to perform hyperparameter search for XAI methods to optimize thequality of explanations.", "output": "Optimizing Explanations by Network Canonization and Hyperparameter Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inspired by the impressive performance of recent face image editing methods,several studies have been naturally proposed to extend these methods to theface video editing task. One of the main challenges here is temporalconsistency among edited frames, which is still unresolved. To this end, wepropose a novel face video editing framework based on diffusion autoencodersthat can successfully extract the decomposed features - for the first time as aface video editing model - of identity and motion from a given video. Thismodeling allows us to edit the video by simply manipulating the temporallyinvariant feature to the desired direction for the consistency. Another uniquestrength of our model is that, since our model is based on diffusion models, itcan satisfy both reconstruction and edit capabilities at the same time, and isrobust to corner cases in wild face videos (e.g. occluded faces) unlike theexisting GAN-based methods.", "output": "Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale multi-modal training with image-text pairs imparts stronggeneralization to CLIP model. Since training on a similar scale for videos isinfeasible, recent approaches focus on the effective transfer of image-basedCLIP to the video domain. In this pursuit, new parametric modules are added tolearn temporal information and inter-frame relationships which requiremeticulous design efforts. Furthermore, when the resulting models are learnedon videos, they tend to overfit on the given task distribution and lack ingeneralization aspect. This begs the following question: How to effectivelytransfer image-level CLIP representations to videos? In this work, we show thata simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient tobridge the domain gap from images to videos. Our qualitative analysisillustrates that the frame-level processing from CLIP image-encoder followed byfeature pooling and similarity matching with corresponding text embeddingshelps in implicitly modeling the temporal cues within ViFi-CLIP. Suchfine-tuning helps the model to focus on scene dynamics, moving objects andinter-object relationships. For low-data regimes where full fine-tuning is notviable, we propose a `bridge and prompt' approach that first uses fine-tuningto bridge the domain gap and then learns prompts on language and vision side toadapt CLIP representations. We extensively evaluate this simple yet strongbaseline on zero-shot, base-to-novel generalization, few-shot and fullysupervised settings across five video benchmarks. Our code is available athttps://github.com/muzairkhattak/ViFi-CLIP.", "output": "Fine-tuned CLIP Models are Efficient Video Learners."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Curated knowledge graphs encode domain expertise and improve the performanceof recommendation, segmentation, ad targeting, and other machine learningsystems in several domains. As new concepts emerge in a domain, knowledgegraphs must be expanded to preserve machine learning performance. Manuallyexpanding knowledge graphs, however, is infeasible at scale. In this work, wepropose a method for knowledge graph expansion with humans-in-the-loop.Concretely, given a knowledge graph, our method predicts the \"parents\" of newconcepts to be added to this graph for further verification by human experts.We show that our method is both accurate and provably \"human-friendly\".Specifically, we prove that our method predicts parents that are \"near\"concepts' true parents in the knowledge graph, even when the predictions areincorrect. We then show, with a controlled experiment, that satisfying thisproperty increases both the speed and the accuracy of the human-algorithmcollaboration. We further evaluate our method on a knowledge graph fromPinterest and show that it outperforms competing methods on both accuracy andhuman-friendliness. Upon deployment in production at Pinterest, our methodreduced the time needed for knowledge graph expansion by ~400% (compared tomanual expansion), and contributed to a subsequent increase in ad revenue of20%.", "output": "Expanding Knowledge Graphs with Humans in the Loop."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In biological research, fluorescence staining is a key technique to revealthe locations and morphology of subcellular structures. However, it is slow,expensive, and harmful to cells. In this paper, we model it as a deep learningtask termed subcellular structure prediction (SSP), aiming to predict the 3Dfluorescent images of multiple subcellular structures from a 3Dtransmitted-light image. Unfortunately, due to the limitations of currentbiotechnology, each image is partially labeled in SSP. Besides, naturally,subcellular structures vary considerably in size, which causes the multi-scaleissue of SSP. To overcome these challenges, we propose Re-parameterizingMixture-of-Diverse-Experts (RepMode), a network that dynamically organizes itsparameters with task-aware priors to handle specified single-label predictiontasks. In RepMode, the Mixture-of-Diverse-Experts (MoDE) block is designed tolearn the generalized parameters for all tasks, and gating re-parameterization(GatRep) is performed to generate the specialized parameters for each task, bywhich RepMode can maintain a compact practical topology exactly like a plainnetwork, and meanwhile achieves a powerful theoretical topology. Comprehensiveexperiments show that RepMode can achieve state-of-the-art overall performancein SSP.", "output": "RepMode: Learning to Re-parameterize Diverse Experts for Subcellular Structure Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of semantic segmentation calibration. Lots of solutionshave been proposed to approach model miscalibration of confidence in imageclassification. However, to date, confidence calibration research on semanticsegmentation is still limited. We provide a systematic study on the calibrationof semantic segmentation models and propose a simple yet effective approach.First, we find that model capacity, crop size, multi-scale testing, andprediction correctness have impact on calibration. Among them, predictioncorrectness, especially misprediction, is more important to miscalibration dueto over-confidence. Next, we propose a simple, unifying, and effectiveapproach, namely selective scaling, by separating correct/incorrect predictionfor scaling and more focusing on misprediction logit smoothing. Then, we studypopular existing calibration methods and compare them with selective scaling onsemantic segmentation calibration. We conduct extensive experiments with avariety of benchmarks on both in-domain and domain-shift calibration and showthat selective scaling consistently outperforms other methods.", "output": "On Calibrating Semantic Segmentation Models: Analyses and An Algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With more and more data being collected, data-driven modeling methods havebeen gaining in popularity in recent years. While physically sound, classicalgray-box models are often cumbersome to identify and scale, and their accuracymight be hindered by their limited expressiveness. On the other hand, classicalblack-box methods, typically relying on Neural Networks (NNs) nowadays, oftenachieve impressive performance, even at scale, by deriving statistical patternsfrom data. However, they remain completely oblivious to the underlying physicallaws, which may lead to potentially catastrophic failures if decisions forreal-world physical systems are based on them. Physically Consistent NeuralNetworks (PCNNs) were recently developed to address these aforementionedissues, ensuring physical consistency while still leveraging NNs to attainstate-of-the-art accuracy.In this work, we scale PCNNs to model building temperature dynamics andpropose a thorough comparison with classical gray-box and black-box methods.More precisely, we design three distinct PCNN extensions, thereby exemplifyingthe modularity and flexibility of the architecture, and formally prove theirphysical consistency. In the presented case study, PCNNs are shown to achievestate-of-the-art accuracy, even outperforming classical NN-based models despitetheir constrained structure. Our investigations furthermore provide a clearillustration of NNs achieving seemingly good performance while remainingcompletely physics-agnostic, which can be misleading in practice. While thisperformance comes at the cost of computational complexity, PCNNs on the otherhand show accuracy improvements of 17-35% compared to all other physicallyconsistent methods, paving the way for scalable physically consistent modelswith state-of-the-art performance.", "output": "Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modelling and understanding time remains a challenge in contemporary videounderstanding models. With language emerging as a key driver towards powerfulgeneralization, it is imperative for foundational video-language models to havea sense of time. In this paper, we consider a specific aspect of temporalunderstanding: consistency of time order as elicited by before/after relations.We establish that seven existing video-language models struggle to understandeven such simple temporal relations. We then question whether it is feasible toequip these foundational models with temporal awareness without re-trainingthem from scratch. Towards this, we propose a temporal adaptation recipe on topof one such model, VideoCLIP, based on post-pretraining on a small amount ofvideo-text data. We conduct a zero-shot evaluation of the adapted models on sixdatasets for three downstream tasks which require varying degrees of timeawareness. We observe encouraging performance gains especially when the taskneeds higher time awareness. Our work serves as a first step towards probingand instilling a sense of time in existing video-language models without theneed for data and compute-intense training from scratch.", "output": "Test of Time: Instilling Video-Language Models with a Sense of Time."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study ObjectGoal Navigation -- where a virtual robot situated in a newenvironment is asked to navigate to an object. Prior work has shown thatimitation learning (IL) using behavior cloning (BC) on a dataset of humandemonstrations achieves promising results. However, this has limitations -- 1)BC policies generalize poorly to new states, since the training mimics actionsnot their consequences, and 2) collecting demonstrations is expensive. On theother hand, reinforcement learning (RL) is trivially scalable, but requirescareful reward engineering to achieve desirable behavior. We present PIRLNav, atwo-stage learning scheme for BC pretraining on human demonstrations followedby RL-finetuning. This leads to a policy that achieves a success rate of$65.0\\%$ on ObjectNav ($+5.0\\%$ absolute over previous state-of-the-art). Usingthis BC$\\rightarrow$RL training recipe, we present a rigorous empiricalanalysis of design choices. First, we investigate whether human demonstrationscan be replaced with `free' (automatically generated) sources ofdemonstrations, e.g. shortest paths (SP) or task-agnostic frontier exploration(FE) trajectories. We find that BC$\\rightarrow$RL on human demonstrationsoutperforms BC$\\rightarrow$RL on SP and FE trajectories, even when controlledfor same BC-pretraining success on train, and even on a subset of val episodeswhere BC-pretraining success favors the SP or FE policies. Next, we study howRL-finetuning performance scales with the size of the BC pretraining dataset.We find that as we increase the size of BC-pretraining dataset and get to highBC accuracies, improvements from RL-finetuning are smaller, and that $90\\%$ ofthe performance of our best BC$\\rightarrow$RL policy can be achieved with lessthan half the number of BC demonstrations. Finally, we analyze failure modes ofour ObjectNav policies, and present guidelines for further improving them.", "output": "PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of learning goal-conditioned policies in Minecraft, apopular, widely accessible yet challenging open-ended environment fordeveloping human-level multi-task agents. We first identify two main challengesof learning such policies: 1) the indistinguishability of tasks from the statedistribution, due to the vast scene diversity, and 2) the non-stationary natureof environment dynamics caused by partial observability. To tackle the firstchallenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encouragethe emergence of goal-relevant visual state representations. To tackle thesecond challenge, the policy is further fueled by an adaptive horizonprediction module that helps alleviate the learning uncertainty brought by thenon-stationary dynamics. Experiments on 20 Minecraft tasks show that our methodsignificantly outperforms the best baseline so far; in many of them, we doublethe performance. Our ablation and exploratory studies then explain how ourapproach beat the counterparts and also unveil the surprising bonus ofzero-shot generalization to new scenes (biomes). We hope our agent could helpshed some light on learning goal-conditioned, multi-task agents in challenging,open-ended environments like Minecraft.", "output": "Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently decades have witnessed the empirical success of framing KnowledgeGraph (KG) embeddings via language models. However, language model-based KGembeddings are usually deployed as static artifacts, which are challenging tomodify without re-training after deployment. To address this issue, we proposea new task of editing language model-based KG embeddings in this paper. Theproposed task aims to enable data-efficient and fast updates to KG embeddingswithout damaging the performance of the rest. We build four new datasets:E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledgeediting baselines demonstrating the limited ability of previous models tohandle the proposed challenging task. We further propose a simple yet strongbaseline dubbed KGEditor, which utilizes additional parametric layers of thehyper network to edit/add facts. Comprehensive experimental results demonstratethat KGEditor can perform better when updating specific facts while notaffecting the rest with low training resources. Code and datasets will beavailable in https://github.com/zjunlp/PromptKG/tree/main/deltaKG.", "output": "Editing Language Model-based Knowledge Graph Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial attacks insert small, imperceptible perturbations to inputsamples that cause large, undesired changes to the output of deep learningmodels. Despite extensive research on generating adversarial attacks andbuilding defense systems, there has been limited research on understandingadversarial attacks from an input-data perspective. This work introduces thenotion of sample attackability, where we aim to identify samples that are mostsusceptible to adversarial attacks (attackable samples) and conversely alsoidentify the least susceptible samples (robust samples). We propose adeep-learning-based method to detect the adversarially attackable and robustsamples in an unseen dataset for an unseen target model. Experiments onstandard image classification datasets enables us to assess the portability ofthe deep attackability detector across a range of architectures. We find thatthe deep attackability detector performs better than simple modeluncertainty-based measures for identifying the attackable/robust samples. Thissuggests that uncertainty is an inadequate proxy for measuring sample distanceto a decision boundary. In addition to better understanding adversarial attacktheory, it is found that the ability to identify the adversarially attackableand robust samples has implications for improving the efficiency ofsample-selection tasks, e.g. active learning in augmentation for adversarialtraining.", "output": "Identifying Adversarially Attackable and Robust Samples."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to accurately locate and navigate to a specific object is acrucial capability for embodied agents that operate in the real world andinteract with objects to complete tasks. Such object navigation tasks usuallyrequire large-scale training in visual environments with labeled objects, whichgeneralizes poorly to novel objects in unknown environments. In this work, wepresent a novel zero-shot object navigation method, Exploration with SoftCommonsense constraints (ESC), that transfers commonsense knowledge inpre-trained models to open-world object navigation without any navigationexperience nor any other training on the visual environments. First, ESCleverages a pre-trained vision and language model for open-world prompt-basedgrounding and a pre-trained commonsense language model for room and objectreasoning. Then ESC converts commonsense knowledge into navigation actions bymodeling it as soft logic predicates for efficient exploration. Extensiveexperiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC methodimproves significantly over baselines, and achieves new state-of-the-artresults for zero-shot object navigation (e.g., 158% relative Success Rateimprovement than CoW on MP3D).", "output": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human motion prediction is a classical problem in computer vision andcomputer graphics, which has a wide range of practical applications. Previouseffects achieve great empirical performance based on an encoding-decodingstyle. The methods of this style work by first encoding previous motions tolatent representations and then decoding the latent representations intopredicted motions. However, in practice, they are still unsatisfactory due toseveral issues, including complicated loss constraints, cumbersome trainingprocesses, and scarce switch of different categories of motions in prediction.In this paper, to address the above issues, we jump out of the foregoing styleand propose a novel framework from a new perspective. Specifically, ourframework works in a masked completion fashion. In the training stage, we learna motion diffusion model that generates motions from random noise. In theinference stage, with a denoising procedure, we make motion predictionconditioning on observed motions to output more continuous and controllablepredictions. The proposed framework enjoys promising algorithmic properties,which only needs one loss in optimization and is trained in an end-to-endmanner. Additionally, it accomplishes the switch of different categories ofmotions effectively, which is significant in realistic tasks, e.g., theanimation task. Comprehensive experiments on benchmarks confirm the superiorityof the proposed framework. The project page is available athttps://lhchen.top/Human-MAC.", "output": "HumanMAC: Masked Motion Completion for Human Motion Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans can easily imagine the complete 3D geometry of occluded objects andscenes. This appealing ability is vital for recognition and understanding. Toenable such capability in AI systems, we propose VoxFormer, a Transformer-basedsemantic scene completion framework that can output complete 3D volumetricsemantics from only 2D images. Our framework adopts a two-stage design where westart from a sparse set of visible and occupied voxel queries from depthestimation, followed by a densification stage that generates dense 3D voxelsfrom the sparse ones. A key idea of this design is that the visual features on2D images correspond only to the visible scene structures rather than theoccluded or empty spaces. Therefore, starting with the featurization andprediction of the visible structures is more reliable. Once we obtain the setof sparse queries, we apply a masked autoencoder design to propagate theinformation to all the voxels by self-attention. Experiments on SemanticKITTIshow that VoxFormer outperforms the state of the art with a relativeimprovement of 20.0% in geometry and 18.1% in semantics and reduces GPU memoryduring training to less than 16GB. Our code is available onhttps://github.com/NVlabs/VoxFormer.", "output": "VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Implicit Two Hands (Im2Hands), the first neural implicitrepresentation of two interacting hands. Unlike existing methods on two-handreconstruction that rely on a parametric hand model and/or low-resolutionmeshes, Im2Hands can produce fine-grained geometry of two hands with highhand-to-hand and hand-to-image coherency. To handle the shape complexity andinteraction context between two hands, Im2Hands models the occupancy volume oftwo hands - conditioned on an RGB image and coarse 3D keypoints - by two novelattention-based modules responsible for (1) initial occupancy estimation and(2) context-aware occupancy refinement, respectively. Im2Hands first learnsper-hand neural articulated occupancy in the canonical space designed for eachhand using query-image attention. It then refines the initial two-handoccupancy in the posed space to enhance the coherency between the two handshapes using query-anchor attention. In addition, we introduce an optionalkeypoint refinement module to enable robust two-hand shape estimation frompredicted hand keypoints in a single-image reconstruction scenario. Weexperimentally demonstrate the effectiveness of Im2Hands on two-handreconstruction in comparison to related methods, where ours achievesstate-of-the-art results. Our code is publicly available athttps://github.com/jyunlee/Im2Hands.", "output": "Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate compositional structures in data embeddings from pre-trainedvision-language models (VLMs). Traditionally, compositionality has beenassociated with algebraic operations on embeddings of words from a pre-existingvocabulary. In contrast, we seek to approximate representations from an encoderas combinations of a smaller set of vectors in the embedding space. Thesevectors can be seen as \"ideal words\" for generating concepts directly withinthe embedding space of the model. We first present a framework forunderstanding compositional structures from a geometric perspective. We thenexplain what these compositional structures entail probabilistically in thecase of VLM embeddings, providing intuitions for why they arise in practice.Finally, we empirically explore these structures in CLIP's embeddings and weevaluate their usefulness for solving different vision-language tasks such asclassification, debiasing, and retrieval. Our results show that simple linearalgebraic operations on embedding vectors can be used as compositional andinterpretable methods for regulating the behavior of VLMs.", "output": "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Link Traversal-based Query Processing (ltqp), in which a sparql query isevaluated over a web of documents rather than a single dataset, is often seenas a theoretically interesting yet impractical technique. However, in a timewhere the hypercentralization of data has increasingly come under scrutiny, adecentralized Web of Data with a simple document-based interface is appealing,as it enables data publishers to control their data and access rights. Whileltqp allows evaluating complex queries over such webs, it suffers fromperformance issues (due to the high number of documents containing data) aswell as information quality concerns (due to the many sources providing suchdocuments). In existing ltqp approaches, the burden of finding sources to queryis entirely in the hands of the data consumer. In this paper, we argue that tosolve these issues, data publishers should also be able to suggest sources ofinterest and guide the data consumer towards relevant and trustworthy data. Weintroduce a theoretical framework that enables such guided link traversal andstudy its properties. We illustrate with a theoretic example that this canimprove query results and reduce the number of network requests. We evaluateour proposal experimentally on a virtual linked web with specifications andindeed observe that not just the data quality but also the efficiency ofquerying improves.Under consideration in Theory and Practice of Logic Programming (TPLP).", "output": "Distributed Subweb Specifications for Traversing the Web."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT is a groundbreaking ``chatbot\"--an AI interface built on a largelanguage model that was trained on an enormous corpus of human text to emulatehuman conversation. Beyond its ability to converse in a plausible way, it hasattracted attention for its ability to competently answer questions from thebar exam and from MBA coursework, and to provide useful assistance in writingcomputer code. These apparent abilities have prompted discussion of ChatGPT asboth a threat to the integrity of higher education and conversely as a powerfulteaching tool. In this work we present a preliminary analysis of how twoversions of ChatGPT (ChatGPT3.5 and ChatGPT4) fare in the field offirst-semester university physics, using a modified version of the ForceConcept Inventory (FCI) to assess whether it can give correct responses toconceptual physics questions about kinematics and Newtonian dynamics. Wedemonstrate that, by some measures, ChatGPT3.5 can match or exceed the medianperformance of a university student who has completed one semester of collegephysics, though its performance is notably uneven and the results are nuanced.By these same measures, we find that ChatGPT4's performance is approaching thepoint of being indistinguishable from that of an expert physicist when it comesto introductory mechanics topics. After the completion of our work we becameaware of Ref [1], which preceded us to publication and which completes anextensive analysis of the abilities of ChatGPT3.5 in a physics class, includinga different modified version of the FCI. We view this work as confirming thatportion of their results, and extending the analysis to ChatGPT4, which showsrapid and notable improvement in most, but not all respects.", "output": "AI and the FCI: Can ChatGPT Project an Understanding of Introductory Physics?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Natural language understanding(NLU) is challenging for finance due to thelack of annotated data and the specialized language in that domain. As aresult, researchers have proposed to use pre-trained language model andmulti-task learning to learn robust representations. However, aggressivefine-tuning often causes over-fitting and multi-task learning may favor taskswith significantly larger amounts data, etc. To address these problems, in thispaper, we investigate model-agnostic meta-learning algorithm(MAML) inlow-resource financial NLU tasks. Our contribution includes: 1. we explore theperformance of MAML method with multiple types of tasks: GLUE datasets, SNLI,Sci-Tail and Financial PhraseBank; 2. we study the performance of MAML methodwith multiple single-type tasks: a real scenario stock price prediction problemwith twitter text data. Our models achieve the state-of-the-art performanceaccording to the experimental results, which demonstrate that our method canadapt fast and well to low-resource situations.", "output": "Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in Finance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weighted knowledge bases for description logics with typicality under a\"concept-wise\" multi-preferential semantics provide a logical interpretation ofMultiLayer Perceptrons. In this context, Answer Set Programming (ASP) has beenshown to be suitable for addressing defeasible reasoning in the finitelymany-valued case, providing a $\\Pi^p_2$ upper bound on the complexity of theproblem, nonetheless leaving unknown the exact complexity and only providing aproof-of-concept implementation. This paper fulfils the lack by providing a$P^{NP[log]}$-completeness result and new ASP encodings that deal with weightedknowledge bases with large search spaces.", "output": "Complexity and scalability of defeasible reasoning in many-valued weighted knowledge bases with typicality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A surge of advances in language models (LMs) has led to significant interestin using LMs to build co-writing systems, in which humans and LMs interactivelycontribute to a shared writing artifact. However, there is a lack of studiesassessing co-writing systems in interactive settings. We propose ahuman-centered evaluation framework, Parachute, for interactive co-writingsystems. Parachute showcases an integrative view of interaction evaluation,where each evaluation aspect consists of categorized practical metrics.Furthermore, we present Parachute with a use case to demonstrate how toevaluate and compare co-writing systems using Parachute.", "output": "Parachute: Evaluating Interactive Human-LM Co-writing Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While interacting in the world is a multi-sensory experience, many robotscontinue to predominantly rely on visual perception to map and navigate intheir environments. In this work, we propose Audio-Visual-Language Maps(AVLMaps), a unified 3D spatial map representation for storing cross-modalinformation from audio, visual, and language cues. AVLMaps integrate theopen-vocabulary capabilities of multimodal foundation models pre-trained onInternet-scale data by fusing their features into a centralized 3D voxel grid.In the context of navigation, we show that AVLMaps enable robot systems toindex goals in the map based on multimodal queries, e.g., textual descriptions,images, or audio snippets of landmarks. In particular, the addition of audioinformation enables robots to more reliably disambiguate goal locations.Extensive experiments in simulation show that AVLMaps enable zero-shotmultimodal goal navigation from multimodal prompts and provide 50% betterrecall in ambiguous scenarios. These capabilities extend to mobile robots inthe real world - navigating to landmarks referring to visual, audio, andspatial concepts. Videos and code are available at: https://avlmaps.github.io.", "output": "Audio Visual Language Maps for Robot Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents medBERTde, a pre-trained German BERT model specificallydesigned for the German medical domain. The model has been trained on a largecorpus of 4.7 Million German medical documents and has been shown to achievenew state-of-the-art performance on eight different medical benchmarks coveringa wide range of disciplines and medical document types. In addition toevaluating the overall performance of the model, this paper also conducts amore in-depth analysis of its capabilities. We investigate the impact of datadeduplication on the model's performance, as well as the potential benefits ofusing more efficient tokenization methods. Our results indicate thatdomain-specific models such as medBERTde are particularly useful for longertexts, and that deduplication of training data does not necessarily lead toimproved performance. Furthermore, we found that efficient tokenization playsonly a minor role in improving model performance, and attribute most of theimproved performance to the large amount of training data. To encourage furtherresearch, the pre-trained model weights and new benchmarks based onradiological data are made publicly available for use by the scientificcommunity.", "output": "MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Natural Language for Optimization (NL4Opt) Competition was created toinvestigate methods of extracting the meaning and formulation of anoptimization problem based on its text description. Specifically, the goal ofthe competition is to increase the accessibility and usability of optimizationsolvers by allowing non-experts to interface with them using natural language.We separate this challenging goal into two sub-tasks: (1) recognize and labelthe semantic entities that correspond to the components of the optimizationproblem; (2) generate a meaning representation (i.e., a logical form) of theproblem from its detected problem entities. The first task aims to reduceambiguity by detecting and tagging the entities of the optimization problems.The second task creates an intermediate representation of the linearprogramming (LP) problem that is converted into a format that can be used bycommercial solvers. In this report, we present the LP word problem dataset andshared tasks for the NeurIPS 2022 competition. Furthermore, we investigate andcompare the performance of the ChatGPT large language model against the winningsolutions. Through this competition, we hope to bring interest towards thedevelopment of novel machine learning applications and datasets foroptimization modeling.", "output": "NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We report the development of GPT-4, a large-scale, multimodal model which canaccept image and text inputs and produce text outputs. While less capable thanhumans in many real-world scenarios, GPT-4 exhibits human-level performance onvarious professional and academic benchmarks, including passing a simulated barexam with a score around the top 10% of test takers. GPT-4 is aTransformer-based model pre-trained to predict the next token in a document.The post-training alignment process results in improved performance on measuresof factuality and adherence to desired behavior. A core component of thisproject was developing infrastructure and optimization methods that behavepredictably across a wide range of scales. This allowed us to accuratelypredict some aspects of GPT-4's performance based on models trained with nomore than 1/1,000th the compute of GPT-4.", "output": "GPT-4 Technical Report."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multiple robots could perceive a scene (e.g., detect objects) collaborativelybetter than individuals, although easily suffer from adversarial attacks whenusing deep learning. This could be addressed by the adversarial defense, butits training requires the often-unknown attacking mechanism. Differently, wepropose ROBOSAC, a novel sampling-based defense strategy generalizable tounseen attackers. Our key idea is that collaborative perception should lead toconsensus rather than dissensus in results compared to individual perception.This leads to our hypothesize-and-verify framework: perception results with andwithout collaboration from a random subset of teammates are compared untilreaching a consensus. In such a framework, more teammates in the sampled subsetoften entail better perception performance but require longer sampling time toreject potential attackers. Thus, we derive how many sampling trials are neededto ensure the desired size of an attacker-free subset, or equivalently, themaximum size of such a subset that we can successfully sample within a givennumber of trials. We validate our method on the task of collaborative 3D objectdetection in autonomous driving scenarios.", "output": "Among Us: Adversarially Robust Collaborative Perception by Consensus."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tactile information plays a critical role in human dexterity. It revealsuseful contact information that may not be inferred directly from vision. Infact, humans can even perform in-hand dexterous manipulation without usingvision. Can we enable the same ability for the multi-finger robot hand? In thispaper, we present Touch Dexterity, a new system that can perform in-hand objectrotation using only touching without seeing the object. Instead of relying onprecise tactile sensing in a small region, we introduce a new system designusing dense binary force sensors (touch or no touch) overlaying one side of thewhole robot hand (palm, finger links, fingertips). Such a design is low-cost,giving a larger coverage of the object, and minimizing the Sim2Real gap at thesame time. We train an in-hand rotation policy using Reinforcement Learning ondiverse objects in simulation. Relying on touch-only sensing, we can directlydeploy the policy in a real robot hand and rotate novel objects that are notpresented in training. Extensive ablations are performed on how tactileinformation help in-hand manipulation.Our project is available athttps://touchdexterity.github.io.", "output": "Rotating without Seeing: Towards In-hand Dexterity through Touch."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A central challenge in human pose estimation, as well as in many othermachine learning and prediction tasks, is the generalization problem. Thelearned network does not have the capability to characterize the predictionerror, generate feedback information from the test sample, and correct theprediction error on the fly for each individual test sample, which results indegraded performance in generalization. In this work, we introduce aself-correctable and adaptable inference (SCAI) method to address thegeneralization challenge of network prediction and use human pose estimation asan example to demonstrate its effectiveness and performance. We learn acorrection network to correct the prediction result conditioned by a fitnessfeedback error. This feedback error is generated by a learned fitness feedbacknetwork which maps the prediction result to the original input domain andcompares it against the original input. Interestingly, we find that thisself-referential feedback error is highly correlated with the actual predictionerror. This strong correlation suggests that we can use this error as feedbackto guide the correction process. It can be also used as a loss function toquickly adapt and optimize the correction network during the inference process.Our extensive experimental results on human pose estimation demonstrate thatthe proposed SCAI method is able to significantly improve the generalizationcapability and performance of human pose estimation.", "output": "Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is important for deep reinforcement learning (DRL) algorithms to transfertheir learned policies to new environments that have different visual inputs.In this paper, we introduce Prompt based Proximal Policy Optimization($P^{3}O$), a three-stage DRL algorithm that transfers visual representationsfrom a target to a source environment by applying prompting. The process of$P^{3}O$ consists of three stages: pre-training, prompting, and predicting. Inparticular, we specify a prompt-transformer for representation conversion andpropose a two-step training process to train the prompt-transformer for thetarget environment, while the rest of the DRL pipeline remains unchanged. Weimplement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. Theexperimental results show that $P^{3}O$ outperforms the state-of-the-art visualtransferring schemes. In particular, $P^{3}O$ allows the learned policies toperform well in environments with different visual inputs, which is much moreeffective than retraining the policies in these environments.", "output": "$P^{3}O$: Transferring Visual Representations for Reinforcement Learning via Prompting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art machine learning models often learn spurious correlationsembedded in the training data. This poses risks when deploying these models forhigh-stake decision-making, such as in medical applications like skin cancerdetection. To tackle this problem, we propose Reveal to Revise (R2R), aframework entailing the entire eXplainable Artificial Intelligence (XAI) lifecycle, enabling practitioners to iteratively identify, mitigate, and(re-)evaluate spurious model behavior with a minimal amount of humaninteraction. In the first step (1), R2R reveals model weaknesses by findingoutliers in attributions or through inspection of latent concepts learned bythe model. Secondly (2), the responsible artifacts are detected and spatiallylocalized in the input data, which is then leveraged to (3) revise the modelbehavior. Concretely, we apply the methods of RRR, CDEP and ClArC for modelcorrection, and (4) (re-)evaluate the model's performance and remainingsensitivity towards the artifact. Using two medical benchmark datasets forMelanoma detection and bone age estimation, we apply our R2R framework to VGG,ResNet and EfficientNet architectures and thereby reveal and correct realdataset-intrinsic artifacts, as well as synthetic variants in a controlledsetting. Completing the XAI life cycle, we demonstrate multiple R2R iterationsto mitigate different biases. Code is available onhttps://github.com/maxdreyer/Reveal2Revise.", "output": "Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Numerous parties are calling for the democratisation of AI, but the phrase isused to refer to a variety of goals, the pursuit of which sometimes conflict.This paper identifies four kinds of AI democratisation that are commonlydiscussed: (1) the democratisation of AI use, (2) the democratisation of AIdevelopment, (3) the democratisation of AI profits, and (4) the democratisationof AI governance. Numerous goals and methods of achieving each form ofdemocratisation are discussed. The main takeaway from this paper is that AIdemocratisation is a multifarious and sometimes conflicting concept that shouldnot be conflated with improving AI accessibility. If we want to move beyondambiguous commitments to democratising AI, to productive discussions ofconcrete policies and trade-offs, then we need to recognise the principal roleof the democratisation of AI governance in navigating tradeoffs and risksacross decisions around use, development, and profits.", "output": "Democratising AI: Multiple Meanings, Goals, and Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The successes of foundation models such as ChatGPT and AlphaFold have spurredsignificant interest in building similar models for electronic medical records(EMRs) to improve patient care and hospital operations. However, recent hypehas obscured critical gaps in our understanding of these models' capabilities.We review over 80 foundation models trained on non-imaging EMR data (i.e.clinical text and/or structured data) and create a taxonomy delineating theirarchitectures, training data, and potential use cases. We find that most modelsare trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) orbroad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks thatdo not provide meaningful insights on their usefulness to health systems. Inlight of these findings, we propose an improved evaluation framework formeasuring the benefits of clinical foundation models that is more closelygrounded to metrics that matter in healthcare.", "output": "The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electronic health records (EHRs) store an extensive array of patientinformation, encompassing medical histories, diagnoses, treatments, and testoutcomes. These records are crucial for enabling healthcare providers to makewell-informed decisions regarding patient care. Summarizing clinical notesfurther assists healthcare professionals in pinpointing potential health risksand making better-informed decisions. This process contributes to reducingerrors and enhancing patient outcomes by ensuring providers have access to themost pertinent and current patient data. Recent research has shown thatincorporating prompts with large language models (LLMs) substantially booststhe efficacy of summarization tasks. However, we show that this approach alsoleads to increased output variance, resulting in notably divergent outputs evenwhen prompts share similar meanings. To tackle this challenge, we introduce amodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs softprompts to diminish variance while preserving the advantages of prompt-basedsummarization. Experimental findings on multiple clinical note tasks and LLMsindicate that our method not only bolsters performance but also effectivelycurbs variance for various LLMs, providing a more uniform and dependablesolution for summarizing vital medical information.", "output": "SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The advent of open-source AI communities has produced a cornucopia ofpowerful text-guided diffusion models that are trained on various datasets.While few explorations have been conducted on ensembling such models to combinetheir strengths. In this work, we propose a simple yet effective method calledSaliency-aware Noise Blending (SNB) that can empower the fused text-guideddiffusion models to achieve more controllable generation. Specifically, weexperimentally find that the responses of classifier-free guidance are highlyrelated to the saliency of generated images. Thus we propose to trust differentmodels in their areas of expertise by blending the predicted noises of twodiffusion models in a saliency-aware manner. SNB is training-free and can becompleted within a DDIM sampling process. Additionally, it can automaticallyalign the semantics of two noise spaces without requiring additionalannotations such as masks. Extensive experiments show the impressiveeffectiveness of SNB in various applications. Project page is available athttps://magicfusion.github.io/.", "output": "MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models have demonstrated surprising ability to performin-context learning, i.e., these models can be directly applied to solvenumerous downstream tasks by conditioning on a prompt constructed by a fewinput-output examples. However, prior research has shown that in-contextlearning can suffer from high instability due to variations in trainingexamples, example order, and prompt formats. Therefore, the construction of anappropriate prompt is essential for improving the performance of in-contextlearning. In this paper, we revisit this problem from the view of predictivebias. Specifically, we introduce a metric to evaluate the predictive bias of afixed prompt against labels or a given attributes. Then we empirically showthat prompts with higher bias always lead to unsatisfactory predictive quality.Based on this observation, we propose a novel search strategy based on thegreedy search to identify the near-optimal prompt for improving the performanceof in-context learning. We perform comprehensive experiments withstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.Our results indicate that our method can enhance the model's in-contextlearning performance in an effective and interpretable manner.", "output": "Fairness-guided Few-shot Prompting for Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present DreamBooth3D, an approach to personalize text-to-3D generativemodels from as few as 3-6 casually captured images of a subject. Our approachcombines recent advances in personalizing text-to-image models (DreamBooth)with text-to-3D generation (DreamFusion). We find that naively combining thesemethods fails to yield satisfactory subject-specific 3D assets due topersonalized text-to-image models overfitting to the input viewpoints of thesubject. We overcome this through a 3-stage optimization strategy where wejointly leverage the 3D consistency of neural radiance fields together with thepersonalization capability of text-to-image models. Our method can producehigh-quality, subject-specific 3D assets with text-driven modifications such asnovel poses, colors and attributes that are not seen in any of the input imagesof the subject.", "output": "DreamBooth3D: Subject-Driven Text-to-3D Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The usage and impact of deep learning for cleaner production andsustainability purposes remain little explored. This work shows how deeplearning can be harnessed to increase sustainability in production and productusage. Specifically, we utilize deep learning-based computer vision todetermine the wear states of products. The resulting insights serve as a basisfor novel product-service systems with improved integration and resultorientation. Moreover, these insights are expected to facilitate product usageimprovements and R&amp;D innovations. We demonstrate our approach on two products:machining tools and rotating X-ray anodes. From a technical standpoint, we showthat it is possible to recognize the wear state of these products usingdeep-learning-based computer vision. In particular, we detect wear throughmicroscopic images of the two products. We utilize a U-Net for semanticsegmentation to detect wear based on pixel granularity. The resulting mean dicecoefficients of 0.631 and 0.603 demonstrate the feasibility of the proposedapproach. Consequently, experts can now make better decisions, for example, toimprove the machining process parameters. To assess the impact of the proposedapproach on environmental sustainability, we perform life cycle assessmentsthat show gains for both products. The results indicate that the emissions ofCO2 equivalents are reduced by 12% for machining tools and by 44% for rotatinganodes. This work can serve as a guideline and inspire researchers andpractitioners to utilize computer vision in similar scenarios to developsustainable smart product-service systems and enable cleaner production.", "output": "Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latencymultilayer perceptrons~(MLPs) on graph tasks has become a hot research topic.However, MLPs rely exclusively on the node features and fail to capture thegraph structural information. Previous methods address this issue by processinggraph edges into extra inputs for MLPs, but such graph structures may beunavailable for various scenarios. To this end, we propose a Prototype-GuidedKnowledge Distillation~(PGKD) method, which does not require graphedges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze thegraph structural information in GNN teachers, and distill such information fromGNNs to MLPs via prototypes in an edge-free setting. Experimental results onpopular graph benchmarks demonstrate the effectiveness and robustness of theproposed PGKD.", "output": "Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies reveal that Autonomous Vehicles (AVs) can be manipulated byhidden backdoors, causing them to perform harmful actions when activated byphysical triggers. However, it is still unclear how these triggers can beactivated while adhering to traffic principles. Understanding thisvulnerability in a dynamic traffic environment is crucial. This work addressesthis gap by presenting physical trigger activation as a reachability problem ofcontrolled dynamic system. Our technique identifies security-critical areas intraffic systems where trigger conditions for accidents can be reached, andprovides intended trajectories for how those conditions can be reached. Testingon typical traffic scenarios showed the system can be successfully driven totrigger conditions with near 100% activation rate. Our method benefits fromidentifying AV vulnerability and enabling effective safety strategies.", "output": "Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks can be easily fooled into making incorrect predictionsthrough corruption of the input by adversarial perturbations:human-imperceptible artificial noise. So far adversarial training has been themost successful defense against such adversarial attacks. This work focuses onimproving adversarial training to boost adversarial robustness. We firstanalyze, from an instance-wise perspective, how adversarial vulnerabilityevolves during adversarial training. We find that during training an overallreduction of adversarial loss is achieved by sacrificing a considerableproportion of training samples to be more vulnerable to adversarial attack,which results in an uneven distribution of adversarial vulnerability amongdata. Such \"uneven vulnerability\", is prevalent across several popular robusttraining methods and, more importantly, relates to overfitting in adversarialtraining. Motivated by this observation, we propose a new adversarial trainingmethod: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). Itjointly smooths both input and weight loss landscapes in an adaptive,instance-specific, way to enhance robustness more for those samples with higheradversarial vulnerability. Extensive experiments demonstrate the superiority ofour method over existing defense methods. Noticeably, our method, when combinedwith the latest data augmentation and semi-supervised learning techniques,achieves state-of-the-art robustness against $\\ell_{\\infty}$-norm constrainedattacks on CIFAR10 of 59.32% for Wide ResNet34-10 without extra data, and61.55% for Wide ResNet28-10 with extra data. Code is available athttps://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.", "output": "Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fine-grained location prediction on smart phones can be used to improveapp/system performance. Application scenarios include video quality adaptationas a function of the 5G network quality at predicted user locations, andaugmented reality apps that speed up content rendering based on predicted userlocations. Such use cases require prediction error in the same range as the GPSerror, and no existing works on location prediction can achieve this level ofaccuracy. We present a system for fine-grained location prediction (FGLP) ofmobile users, based on GPS traces collected on the phones. FGLP has twocomponents: a federated learning framework and a prediction model. Theframework runs on the phones of the users and also on a server that coordinateslearning from all users in the system. FGLP represents the user location dataas relative points in an abstract 2D space, which enables learning acrossdifferent physical spaces. The model merges Bidirectional Long Short-TermMemory (BiLSTM) and Convolutional Neural Networks (CNN), where BiLSTM learnsthe speed and direction of the mobile users, and CNN learns information such asuser movement preferences. FGLP uses federated learning to protect user privacyand reduce bandwidth consumption. Our experimental results, using a datasetwith over 600,000 users, demonstrate that FGLP outperforms baseline models interms of prediction accuracy. We also demonstrate that FGLP works well inconjunction with transfer learning, which enables model reusability. Finally,benchmark results on several types of Android phones demonstrate FGLP'sfeasibility in real life.", "output": "FGLP: A Federated Fine-Grained Location Prediction System for Mobile Users."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article presents the design, implementation, and evaluation of FLSys, amobile-cloud federated learning (FL) system, which can be a key component foran open ecosystem of FL models and apps. FLSys is designed to work on smartphones with mobile sensing data. It balances model performance with resourceconsumption, tolerates communication failures, and achieves scalability. InFLSys, different DL models with different FL aggregation methods can be trainedand accessed concurrently by different apps. Furthermore, FLSys providesadvanced privacy preserving mechanisms and a common API for third-party appdevelopers to access FL models. FLSys adopts a modular design and isimplemented in Android and AWS cloud. We co-designed FLSys with a humanactivity recognition (HAR) model. HAR sensing data was collected in the wildfrom 100+ college students during a 4-month period. We implemented HAR-Wild, aCNN model tailored to mobile devices, with a data augmentation mechanism tomitigate the problem of non-Independent and Identically Distributed data. Asentiment analysis model is also used to demonstrate that FLSys effectivelysupports concurrent models. This article reports our experience and lessonslearned from conducting extensive experiments using simulations, Android/Linuxemulations, and Android phones that demonstrate FLSys achieves good modelutility and practical system performance.", "output": "FLSys: Toward an Open Ecosystem for Federated Learning Mobile Apps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For many interdisciplinary fields, ML interpretations need to be consistentwith what-if scenarios related to the current case, i.e., if one factorchanges, how does the model react? Although the attribution methods aresupported by the elegant axiomatic systems, they mainly focus on individualinputs, and are generally inconsistent. To support what-if scenarios, weintroduce a new notion called truthful interpretation, and apply Fourieranalysis of Boolean functions to get rigorous guarantees. Experimental resultsshow that for neighborhoods with various radii, our method achieves 2x - 50xlower interpretation error compared with the other methods.", "output": "Consistent and Truthful Interpretation with Fourier Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) is a privacy-preserving distributed deep learningparadigm that involves substantial communication and computation effort, whichis a problem for resource-constrained mobile and IoT devices. Modelpruning/sparsification develops sparse models that could solve this problem,but existing sparsification solutions cannot satisfy at the same time therequirements for low bidirectional communication overhead between the serverand the clients, low computation overhead at the clients, and good modelaccuracy, under the FL assumption that the server does not have access to rawdata to fine-tune the pruned models. We propose Complement Sparsification (CS),a pruning mechanism that satisfies all these requirements through acomplementary and collaborative pruning done at the server and the clients. Ateach round, CS creates a global sparse model that contains the weights thatcapture the general data distribution of all clients, while the clients createlocal sparse models with the weights pruned from the global model to capturethe local trends. For improved model performance, these two types ofcomplementary sparse models are aggregated into a dense model in each round,which is subsequently pruned in an iterative process. CS requires littlecomputation overhead on the top of vanilla FL for both the server and theclients. We demonstrate that CS is an approximation of vanilla FL and, thus,its models perform well. We evaluate CS experimentally with two popular FLbenchmark datasets. CS achieves substantial reduction in bidirectionalcommunication, while achieving performance comparable with vanilla FL. Inaddition, CS outperforms baseline pruning mechanisms for FL.", "output": "Complement Sparsification: Low-Overhead Model Pruning for Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mobile apps, such as mHealth and wellness applications, can benefit from deeplearning (DL) models trained with mobile sensing data collected by smart phonesor wearable devices. However, currently there is no mobile sensing DL systemthat simultaneously achieves good model accuracy while adapting to usermobility behavior, scales well as the number of users increases, and protectsuser data privacy. We propose Zone-based Federated Learning (ZoneFL) to addressthese requirements. ZoneFL divides the physical space into geographical zonesmapped to a mobile-edge-cloud system architecture for good model accuracy andscalability. Each zone has a federated training model, called a zone model,which adapts well to data and behaviors of users in that zone. Benefiting fromthe FL design, the user data privacy is protected during the ZoneFL training.We propose two novel zone-based federated training algorithms to optimize zonemodels to user mobility behavior: Zone Merge and Split (ZMS) and Zone GradientDiffusion (ZGD). ZMS optimizes zone models by adapting the zone geographicalpartitions through merging of neighboring zones or splitting of large zonesinto smaller ones. Different from ZMS, ZGD maintains fixed zones and optimizesa zone model by incorporating the gradients derived from neighboring zones'data. ZGD uses a self-attention mechanism to dynamically control the impact ofone zone on its neighbors. Extensive analysis and experimental resultsdemonstrate that ZoneFL significantly outperforms traditional FL in two modelsfor heart rate prediction and human activity recognition. In addition, wedeveloped a ZoneFL system using Android phones and AWS cloud. The system wasused in a heart rate prediction field study with 63 users for 4 months, and wedemonstrated the feasibility of ZoneFL in real-life.", "output": "Zone-based Federated Learning for Mobile Sensing Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The potential of large language models (LLMs) to reason like humans has beena highly contested topic in Machine Learning communities. However, thereasoning abilities of humans are multifaceted and can be seen in variousforms, including analogical, spatial and moral reasoning, among others. Thisfact raises the question whether LLMs can perform equally well across all thesedifferent domains. This research work aims to investigate the performance ofLLMs on different reasoning tasks by conducting experiments that directly useor draw inspirations from existing datasets on analogical and spatialreasoning. Additionally, to evaluate the ability of LLMs to reason like human,their performance is evaluted on more open-ended, natural language questions.My findings indicate that LLMs excel at analogical and moral reasoning, yetstruggle to perform as proficiently on spatial reasoning tasks. I believe theseexperiments are crucial for informing the future development of LLMs,particularly in contexts that require diverse reasoning proficiencies. Byshedding light on the reasoning abilities of LLMs, this study aims to pushforward our understanding of how they can better emulate the cognitiveabilities of humans.", "output": "Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present DiffuScene for indoor 3D scene synthesis based on a novel scenegraph denoising diffusion probabilistic model, which generates 3D instanceproperties stored in a fully-connected scene graph and then retrieves the mostsimilar object geometry for each graph node i.e. object instance which ischaracterized as a concatenation of different attributes, including location,size, orientation, semantic, and geometry features. Based on this scene graph,we designed a diffusion model to determine the placements and types of 3Dinstances. Our method can facilitate many downstream applications, includingscene completion, scene arrangement, and text-conditioned scene synthesis.Experiments on the 3D-FRONT dataset show that our method can synthesize morephysically plausible and diverse indoor scenes than state-of-the-art methods.Extensive ablation studies verify the effectiveness of our design choice inscene diffusion models.", "output": "DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Considering the ill-posed nature, contrastive regularization has beendeveloped for single image dehazing, introducing the information from negativeimages as a lower bound. However, the contrastive samples are nonconsensual, asthe negatives are usually represented distantly from the clear (i.e., positive)image, leaving the solution space still under-constricted. Moreover, theinterpretability of deep dehazing models is underexplored towards the physicsof the hazing process. In this paper, we propose a novel curricular contrastiveregularization targeted at a consensual contrastive space as opposed to anon-consensual one. Our negatives, which provide better lower-boundconstraints, can be assembled from 1) the hazy image, and 2) correspondingrestorations by other existing methods. Further, due to the differentsimilarities between the embeddings of the clear image and negatives, thelearning difficulty of the multiple components is intrinsically imbalanced. Totackle this issue, we customize a curriculum learning strategy to reweight theimportance of different negatives. In addition, to improve the interpretabilityin the feature space, we build a physics-aware dual-branch unit according tothe atmospheric scattering model. With the unit, as well as curricularcontrastive regularization, we establish our dehazing network, named C2PNet.Extensive experiments demonstrate that our C2PNet significantly outperformsstate-of-the-art methods, with extreme PSNR boosts of 3.94dB and 1.50dB,respectively, on SOTS-indoor and SOTS-outdoor datasets.", "output": "Curricular Contrastive Regularization for Physics-aware Single Image Dehazing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One-shot object detection (OSOD) aims to detect all object instances towardsthe given category specified by a query image. Most existing studies in OSODendeavor to explore effective cross-image correlation and alleviate thesemantic feature misalignment, however, ignoring the phenomenon of the modelbias towards the base classes and the generalization degradation on the novelclasses. Observing this, we propose a novel framework, namely Base-classSuppression and Prior Guidance (BSPG) network to overcome the problem.Specifically, the objects of base categories can be explicitly detected by abase-class predictor and adaptively eliminated by our base-class suppressionmodule. Moreover, a prior guidance module is designed to calculate thecorrelation of high-level features in a non-parametric manner, producing aclass-agnostic prior map to provide the target features with rich semantic cuesand guide the subsequent detection process. Equipped with the proposed twomodules, we endow the model with a strong discriminative ability to distinguishthe target objects from distractors belonging to the base classes. Extensiveexperiments show that our method outperforms the previous techniques by a largemargin and achieves new state-of-the-art performance under various evaluationsettings.", "output": "Adaptive Base-class Suppression and Prior Guidance Network for One-Shot Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integrated Gradients (IG) as well as its variants are well-known techniquesfor interpreting the decisions of deep neural networks. While IG-basedapproaches attain state-of-the-art performance, they often integrate noise intotheir explanation saliency maps, which reduce their interpretability. Tominimize the noise, we examine the source of the noise analytically and proposea new approach to reduce the explanation noise based on our analyticalfindings. We propose the Important Direction Gradient Integration (IDGI)framework, which can be easily incorporated into any IG-based method that usesthe Reimann Integration for integrated gradient computation. Extensiveexperiments with three IG-based methods show that IDGI improves themdrastically on numerous interpretability metrics.", "output": "IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Light Field Networks, the re-formulations of radiance fields to orientedrays, are magnitudes faster than their coordinate network counterparts, andprovide higher fidelity with respect to representing 3D structures from 2Dobservations. They would be well suited for generic scene representation andmanipulation, but suffer from one problem: they are limited to holistic andstatic scenes. In this paper, we propose the Dynamic Light Field Network(DyLiN) method that can handle non-rigid deformations, including topologicalchanges. We learn a deformation field from input rays to canonical rays, andlift them into a higher dimensional space to handle discontinuities. We furtherintroduce CoDyLiN, which augments DyLiN with controllable attribute inputs. Wetrain both models via knowledge distillation from pretrained dynamic radiancefields. We evaluated DyLiN using both synthetic and real world datasets thatinclude various non-rigid deformations. DyLiN qualitatively outperformed andquantitatively matched state-of-the-art methods in terms of visual fidelity,while being 25 - 71x computationally faster. We also tested CoDyLiN onattribute annotated data and it surpassed its teacher model. Project page:https://dylin2023.github.io .", "output": "DyLiN: Making Light Field Networks Dynamic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual place recognition (VPR) is an essential component of robot navigationand localization systems that allows them to identify a place using only imagedata. VPR is challenging due to the significant changes in a place's appearanceunder different illumination throughout the day, with seasonal weather and whenobserved from different viewpoints. Currently, no single VPR technique excelsin every environmental condition, each exhibiting unique benefits andshortcomings. As a result, VPR systems combining multiple techniques achievemore reliable VPR performance in changing environments, at the cost of highercomputational loads. Addressing this shortcoming, we propose an adaptive VPRsystem dubbed Adaptive Multi-Self Identification and Correction (A-MuSIC). Westart by developing a method to collect information of the runtime performanceof a VPR technique by analysing the frame-to-frame continuity of matchedqueries. We then demonstrate how to operate the method on a static ensemble oftechniques, generating data on which techniques are contributing the most forthe current environment. A-MuSIC uses the collected information to both selecta minimal subset of techniques and to decide when a re-selection is requiredduring navigation. A-MuSIC matches or beats state-of-the-art VPR performanceacross all tested benchmark datasets while maintaining its computational loadon par with individual techniques.", "output": "A-MuSIC: An Adaptive Ensemble System For Visual Place Recognition In Changing Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present PACE, a novel method for modifying motion-captured virtual agentsto interact with and move throughout dense, cluttered 3D scenes. Our approachchanges a given motion sequence of a virtual agent as needed to adjust to theobstacles and objects in the environment. We first take the individual framesof the motion sequence most important for modeling interactions with the sceneand pair them with the relevant scene geometry, obstacles, and semantics suchthat interactions in the agents motion match the affordances of the scene(e.g., standing on a floor or sitting in a chair). We then optimize the motionof the human by directly altering the high-DOF pose at each frame in the motionto better account for the unique geometric constraints of the scene. Ourformulation uses novel loss functions that maintain a realistic flow andnatural-looking motion. We compare our method with prior motion generatingtechniques and highlight the benefits of our method with a perceptual study andphysical plausibility metrics. Human raters preferred our method over the priorapproaches. Specifically, they preferred our method 57.1% of the time versusthe state-of-the-art method using existing motions, and 81.0% of the timeversus a state-of-the-art motion synthesis method. Additionally, our methodperforms significantly higher on established physical plausibility andinteraction metrics. Specifically, we outperform competing methods by over 1.2%in terms of the non-collision metric and by over 18% in terms of the contactmetric. We have integrated our interactive system with Microsoft HoloLens anddemonstrate its benefits in real-world indoor scenes. Our project website isavailable at https://gamma.umd.edu/pace/.", "output": "PACE: Data-Driven Virtual Agent Interaction in Dense and Cluttered Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While substantial progresses have been made in automated 2D portraitstylization, admirable 3D portrait stylization from a single user photo remainsto be an unresolved challenge. One primary obstacle here is the lack of highquality stylized 3D training data. In this paper, we propose a novel framework\\emph{AgileGAN3D} that can produce 3D artistically appealing and personalizedportraits with detailed geometry. New stylization can be obtained with just afew (around 20) unpaired 2D exemplars. We achieve this by first leveragingexisting 2D stylization capabilities, \\emph{style prior creation}, to produce alarge amount of augmented 2D style exemplars. These augmented exemplars aregenerated with accurate camera pose labels, as well as paired real face images,which prove to be critical for the downstream 3D stylization task. Capitalizingon the recent advancement of 3D-aware GAN models, we perform \\emph{guidedtransfer learning} on a pretrained 3D GAN generator to producemulti-view-consistent stylized renderings. In order to achieve 3D GAN inversionthat can preserve subject's identity well, we incorporate \\emph{multi-viewconsistency loss} in the training of our encoder. Our pipeline demonstratesstrong capability in turning user photos into a diverse range of 3D artisticportraits. Both qualitative results and quantitative evaluations have beenconducted to show the superior performance of our method. Code and pretrainedmodels will be released for reproduction purpose.", "output": "AgileGAN3D: Few-Shot 3D Portrait Stylization by Augmented Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Assessing the aesthetics of an image is challenging, as it is influenced bymultiple factors including composition, color, style, and high-level semantics.Existing image aesthetic assessment (IAA) methods primarily rely onhuman-labeled rating scores, which oversimplify the visual aestheticinformation that humans perceive. Conversely, user comments offer morecomprehensive information and are a more natural way to express human opinionsand preferences regarding image aesthetics. In light of this, we proposelearning image aesthetics from user comments, and exploring vision-languagepretraining methods to learn multimodal aesthetic representations.Specifically, we pretrain an image-text encoder-decoder model withimage-comment pairs, using contrastive and generative objectives to learn richand generic aesthetic semantics without human labels. To efficiently adapt thepretrained model for downstream IAA tasks, we further propose a lightweightrank-based adapter that employs text as an anchor to learn the aestheticranking concept. Our results show that our pretrained aesthetic vision-languagemodel outperforms prior works on image aesthetic captioning over theAVA-Captions dataset, and it has powerful zero-shot capability for aesthetictasks such as zero-shot style classification and zero-shot IAA, surpassing manysupervised baselines. With only minimal finetuning parameters using theproposed adapter module, our model achieves state-of-the-art IAA performanceover the AVA dataset.", "output": "VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an approach for adversarial attacks on dense prediction models(such as object detectors and segmentation). It is well known that the attacksgenerated by a single surrogate model do not transfer to arbitrary (blackbox)victim models. Furthermore, targeted attacks are often more challenging thanthe untargeted attacks. In this paper, we show that a carefully designedensemble can create effective attacks for a number of victim models. Inparticular, we show that normalization of the weights for individual modelsplays a critical role in the success of the attacks. We then demonstrate thatby adjusting the weights of the ensemble according to the victim model canfurther improve the performance of the attacks. We performed a number ofexperiments for object detectors and segmentation to highlight the significanceof the our proposed methods. Our proposed ensemble-based method outperformsexisting blackbox attack methods for object detection and segmentation. Finallywe show that our proposed method can also generate a single perturbation thatcan fool multiple blackbox detection and segmentation models simultaneously.Code is available at https://github.com/CSIPlab/EBAD.", "output": "Ensemble-based Blackbox Attacks on Dense Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Audio-visual speech recognition has received a lot of attention due to itsrobustness against acoustic noise. Recently, the performance of automatic,visual, and audio-visual speech recognition (ASR, VSR, and AV-ASR,respectively) has been substantially improved, mainly due to the use of largermodels and training sets. However, accurate labelling of datasets istime-consuming and expensive. Hence, in this work, we investigate the use ofautomatically-generated transcriptions of unlabelled datasets to increase thetraining set size. For this purpose, we use publicly-available pre-trained ASRmodels to automatically transcribe unlabelled datasets such as AVSpeech andVoxCeleb2. Then, we train ASR, VSR and AV-ASR models on the augmented trainingset, which consists of the LRS2 and LRS3 datasets as well as the additionalautomatically-transcribed data. We demonstrate that increasing the size of thetraining set, a recent trend in the literature, leads to reduced WER despiteusing noisy transcriptions. The proposed model achieves new state-of-the-artperformance on AV-ASR on LRS2 and LRS3. In particular, it achieves a WER of0.9% on LRS3, a relative improvement of 30% over the current state-of-the-artapproach, and outperforms methods that have been trained on non-publiclyavailable datasets with 26 times more training data.", "output": "Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mis- and disinformation can spread rapidly on video-sharing platforms (VSPs).Despite the growing use of VSPs, there has not been a proportional increase inour ability to understand this medium and the messages conveyed through it. Inthis work, we draw on our prior experiences to outline three core challengesfaced in studying VSPs in high-stakes and fast-paced settings: (1) navigatingthe unique affordances of VSPs, (2) understanding VSP content and determiningits authenticity, and (3) novel user behaviors on VSPs for spreadingmisinformation. By highlighting these challenges, we hope that researchers canreflect on how to adapt existing research methods and tools to these newcontexts, or develop entirely new ones.", "output": "The Challenges of Studying Misinformation on Video-Sharing Platforms During Crises and Mass-Convergence Events."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-time efficient perception is critical for autonomous navigation and cityscale sensing. Orthogonal to architectural improvements, streaming perceptionapproaches have exploited adaptive sampling improving real-time detectionperformance. In this work, we propose a learnable geometry-guided prior thatincorporates rough geometry of the 3D scene (a ground plane and a plane above)to resample images for efficient object detection. This significantly improvessmall and far-away object detection performance while also being more efficientboth in terms of latency and memory. For autonomous navigation, using the samedetector and scale, our approach improves detection rate by +4.1 $AP_{S}$ or+39% and in real-time performance by +5.3 $sAP_{S}$ or +63% for small objectsover state-of-the-art (SOTA). For fixed traffic cameras, our approach detectssmall objects at image scales other methods cannot. At the same scale, ourapproach improves detection of small objects by 195% (+12.5 $AP_{S}$) overnaive-downsampling and 63% (+4.2 $AP_{S}$) over SOTA.", "output": "Learned Two-Plane Perspective Prior based Image Resampling for Efficient Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In state estimation algorithms that use feature tracks as input, it iscustomary to assume that the errors in feature track positions are zero-meanGaussian. Using a combination of calibrated camera intrinsics, ground-truthcamera pose, and depth images, it is possible to compute ground-truth positionsfor feature tracks extracted using an image processing algorithm. We find thatfeature track errors are not zero-mean Gaussian and that the distribution oferrors is conditional on the type of motion, the speed of motion, and the imageprocessing algorithm used to extract the tracks.", "output": "Feature Tracks are not Zero-Mean Gaussian."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the recent advances in deep neural networks, standard convolutionalkernels limit the applications of these networks to the Euclidean domain only.Considering the geodesic nature of the measurement of the earth's surface,remote sensing is one such area that can benefit from non-Euclidean andspherical domains. For this purpose, we propose a novel Graph Neural Networkarchitecture for spatial and spatio-temporal classification using satelliteimagery. We propose a hybrid attention method to learn the relative importanceof irregular neighbors in remote sensing data. Instead of classifying eachpixel, we propose a method based on Simple Linear Iterative Clustering (SLIC)image segmentation and Graph Attention GAT. The superpixels obtained from SLICbecome the nodes of our Graph Convolution Network (GCN). We then construct aregion adjacency graph (RAG) where each superpixel is connected to every otheradjacent superpixel in the image, enabling information to propagate globally.Finally, we propose a Spatially driven Attention Graph Neural Network (SAG-NN)to classify each RAG. We also propose an extension to our SAG-NN forspatio-temporal data. Unlike regular grids of pixels in images, superpixels areirregular in nature and cannot be used to create spatio-temporal graphs. Weintroduce temporal bias by combining unconnected RAGs from each image into onesupergraph. This is achieved by introducing block adjacency matrices resultingin novel Spatio-Temporal driven Attention Graph Neural Network with BlockAdjacency matrix (STAG-NN-BA). We evaluate our proposed methods on two remotesensing datasets namely Asia14 and C2D2. In comparison with both non-graph andgraph-based approaches our SAG-NN and STAG-NN-BA achieved superior accuracy onall the datasets while incurring less computation cost. The code and datasetwill be made public via our GitHub repository.", "output": "Spatio-Temporal driven Attention Graph Neural Network with Block Adjacency matrix (STAG-NN-BA)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, the use of large convolutional kernels has become popular indesigning convolutional neural networks due to their ability to capturelong-range dependencies and provide large receptive fields. However, theincrease in kernel size also leads to a quadratic growth in the number ofparameters, resulting in heavy computation and memory requirements. To addressthis challenge, we propose a neighborhood attention (NA) module that upgradesthe standard convolution with a self-attention mechanism. The NA moduleefficiently extracts long-range dependencies in a sliding window pattern,thereby achieving similar performance to large convolutional kernels but withfewer parameters.Building upon the NA module, we propose a lightweight single imagesuper-resolution (SISR) network named TCSR. Additionally, we introduce anenhanced feed-forward network (EFFN) in TCSR to improve the SISR performance.EFFN employs a parameter-free spatial-shift operation for efficient featureaggregation. Our extensive experiments and ablation studies demonstrate thatTCSR outperforms existing lightweight SISR methods and achievesstate-of-the-art performance. Our codes are available at\\url{https://github.com/Aitical/TCSR}.", "output": "Incorporating Transformer Designs into Convolutions for Lightweight Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Edge computing has been getting a momentum with ever-increasing data at theedge of the network. In particular, huge amounts of video data and theirreal-time processing requirements have been increasingly hindering thetraditional cloud computing approach due to high bandwidth consumption and highlatency. Edge computing in essence aims to overcome this hindrance byprocessing most video data making use of edge servers, such as small-scaleon-premises server clusters, server-grade computing resources at mobile basestations and even mobile devices like smartphones and tablets; hence, the termedge-based video analytics. However, the actual realization of such analyticsrequires more than the simple, collective use of edge servers. In this paper,we survey state-of-the-art works on edge-based video analytics with respect toapplications, architectures, techniques, resource management, security andprivacy. We provide a comprehensive and detailed review on what works, whatdoesn't work and why. These findings give insights and suggestions for nextgeneration edge-based video analytics. We also identify open issues andresearch directions.", "output": "Edge-Based Video Analytics: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Train/Test-Time Adaptation with Retrieval (${\\rm T^3AR}$), amethod to adapt models both at train and test time by means of a retrievalmodule and a searchable pool of external samples. Before inference, ${\\rmT^3AR}$ adapts a given model to the downstream task using refined pseudo-labelsand a self-supervised contrastive objective function whose noise distributionleverages retrieved real samples to improve feature adaptation on the targetdata manifold. The retrieval of real images is key to ${\\rm T^3AR}$ since itdoes not rely solely on synthetic data augmentations to compensate for the lackof adaptation data, as typically done by other adaptation algorithms.Furthermore, thanks to the retrieval module, our method gives the user orservice provider the possibility to improve model adaptation on the downstreamtask by incorporating further relevant data or to fully remove samples that mayno longer be available due to changes in user preference after deployment.First, we show that ${\\rm T^3AR}$ can be used at training time to improvedownstream fine-grained classification over standard fine-tuning baselines, andthe fewer the adaptation data the higher the relative improvement (up to 13%).Second, we apply ${\\rm T^3AR}$ for test-time adaptation and show thatexploiting a pool of external images at test-time leads to more robustrepresentations over existing methods on DomainNet-126 and VISDA-C, especiallywhen few adaptation data are available (up to 8%).", "output": "Train/Test-Time Adaptation with Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision transformer emerges as a potential architecture for vision tasks.However, the intense computation and non-negligible delay hinder itsapplication in the real world. As a widespread model compression technique,existing post-training quantization methods still cause severe performancedrops. We find the main reasons lie in (1) the existing calibration metric isinaccurate in measuring the quantization influence for extremely low-bitrepresentation, and (2) the existing quantization paradigm is unfriendly to thepower-law distribution of Softmax. Based on these observations, we propose anovel Accurate Post-training Quantization framework for Vision Transformer,namely APQ-ViT. We first present a unified Bottom-elimination BlockwiseCalibration scheme to optimize the calibration metric to perceive the overallquantization disturbance in a blockwise manner and prioritize the crucialquantization errors that influence more on the final output. Then, we design aMatthew-effect Preserving Quantization for Softmax to maintain the power-lawcharacter and keep the function of the attention mechanism. Comprehensiveexperiments on large-scale classification and detection datasets demonstratethat our APQ-ViT surpasses the existing post-training quantization methods byconvincing margins, especially in lower bit-width settings (e.g., averagely upto 5.17% improvement for classification and 24.43% for detection on W4A4). Wealso highlight that APQ-ViT enjoys versatility and works well on diversetransformer variants.", "output": "Towards Accurate Post-Training Quantization for Vision Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection and multiple object tracking (MOT) are essential componentsof self-driving systems. Accurate detection and uncertainty quantification areboth critical for onboard modules, such as perception, prediction, andplanning, to improve the safety and robustness of autonomous vehicles.Collaborative object detection (COD) has been proposed to improve detectionaccuracy and reduce uncertainty by leveraging the viewpoints of multipleagents. However, little attention has been paid on how to leverage theuncertainty quantification from COD to enhance MOT performance. In this paper,as the first attempt, we design the uncertainty propagation framework toaddress this challenge, called MOT-CUP. Our framework first quantifies theuncertainty of COD through direct modeling and conformal prediction, andpropogates this uncertainty information during the motion prediction andassociation steps. MOT-CUP is designed to work with different collaborativeobject detectors and baseline MOT algorithms. We evaluate MOT-CUP on V2X-Sim, acomprehensive collaborative perception dataset, and demonstrate a 2%improvement in accuracy and a 2.67X reduction in uncertainty compared to thebaselines, e.g., SORT and ByteTrack. MOT-CUP demonstrates the importance ofuncertainty quantification in both COD and MOT, and provides the first attemptto improve the accuracy and reduce the uncertainty in MOT based on COD throughuncertainty propogation.", "output": "Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous navigation is the key to achieving the full automation ofagricultural research and production management (e.g., disease management andyield prediction) using agricultural robots. In this paper, we introduced avision-based autonomous navigation framework for agriculture robots intrellised cropping systems such as vineyards. To achieve this, we proposed anovel learning-based method to estimate the path traversibility heatmapdirectly from an RGB-D image and subsequently convert the heatmap to apreferred traversal path. An automatic annotation pipeline was developed toform a training dataset by projecting RTK GPS paths collected during the firstsetup in a vineyard in corresponding RGB-D images as ground-truth pathannotations, allowing a fast model training and fine-tuning without costlyhuman annotation. The trained path detection model was used to develop a fullnavigation framework consisting of row tracking and row switching modules,enabling a robot to traverse within a crop row and transit between crop rows tocover an entire vineyard autonomously. Extensive field trials were conducted inthree different vineyards to demonstrate that the developed path detectionmodel and navigation framework provided a cost-effective, accurate, and robustautonomous navigation solution in the vineyard and could be generalized tounseen vineyards with stable performance.", "output": "Vision-based Vineyard Navigation Solution with Automatic Annotation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies the problem of zero-short sketch-based image retrieval(ZS-SBIR), however with two significant differentiators to prior art (i) wetackle all variants (inter-category, intra-category, and cross datasets) ofZS-SBIR with just one network (``everything''), and (ii) we would really liketo understand how this sketch-photo matching operates (``explainable''). Ourkey innovation lies with the realization that such a cross-modal matchingproblem could be reduced to comparisons of groups of key local patches -- akinto the seasoned ``bag-of-words'' paradigm. Just with this change, we are ableto achieve both of the aforementioned goals, with the added benefit of nolonger requiring external semantic knowledge. Technically, ours is atransformer-based cross-modal network, with three novel components (i) aself-attention module with a learnable tokenizer to produce visual tokens thatcorrespond to the most informative local regions, (ii) a cross-attention moduleto compute local correspondences between the visual tokens across twomodalities, and finally (iii) a kernel-based relation network to assemble localputative matches and produce an overall similarity metric for a sketch-photopair. Experiments show ours indeed delivers superior performances across allZS-SBIR settings. The all important explainable goal is elegantly achieved byvisualizing cross-modal token correspondences, and for the first time, viasketch to photo synthesis by universal replacement of all matched photopatches. Code and model are available at\\url{https://github.com/buptLinfy/ZSE-SBIR}.", "output": "Zero-Shot Everything Sketch-Based Image Retrieval, and in Explainable Style."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have established new state of the art in a multitude ofcomputer vision tasks, including image restoration. Diffusion-based inverseproblem solvers generate reconstructions of exceptional visual quality fromheavily corrupted measurements. However, in what is widely known as theperception-distortion trade-off, the price of perceptually appealingreconstructions is often paid in declined distortion metrics, such as PSNR.Distortion metrics measure faithfulness to the observation, a crucialrequirement in inverse problems. In this work, we propose a novel framework forinverse problem solving, namely we assume that the observation comes from astochastic degradation process that gradually degrades and noises the originalclean image. We learn to reverse the degradation process in order to recoverthe clean image. Our technique maintains consistency with the originalmeasurement throughout the reverse process, and allows for great flexibility intrading off perceptual quality for improved distortion metrics and samplingspeedup via early-stopping. We demonstrate the efficiency of our method ondifferent high-resolution datasets and inverse problems, achieving greatimprovements over other state-of-the-art diffusion-based methods with respectto both perceptual and distortion metrics. Source code and pre-trained modelswill be released soon.", "output": "DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning has gained popularity among medical institutions since itenables collaborative training between clients (e.g., hospitals) withoutaggregating data. However, due to the high cost associated with creatingannotations, especially for large 3D image datasets, clinical institutions donot have enough supervised data for training locally. Thus, the performance ofthe collaborative model is subpar under limited supervision. On the other hand,large institutions have the resources to compile data repositories withhigh-resolution images and labels. Therefore, individual clients can utilizethe knowledge acquired in the public data repositories to mitigate the shortageof private annotated images. In this paper, we propose a federated few-shotlearning method with dual knowledge distillation. This method allows jointtraining with limited annotations across clients without jeopardizing privacy.The supervised learning of the proposed method extracts features from limitedlabeled data in each client, while the unsupervised data is used to distillboth feature and response-based knowledge from a national data repository tofurther improve the accuracy of the collaborative model and reduce thecommunication cost. Extensive evaluations are conducted on 3D magneticresonance knee images from a private clinical dataset. Our proposed methodshows superior performance and less training time than other semi-supervisedfederated learning methods. Codes and additional visualization results areavailable at https://github.com/hexiaoxiao-cs/fedml-knee.", "output": "Dealing With Heterogeneous 3D MR Knee Images: A Federated Few-Shot Learning Method With Dual Knowledge Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Transformer-based methods have been utilized to improve theperformance of human action recognition. However, most of these studies assumethat multi-view data is complete, which may not always be the case inreal-world scenarios. Therefore, this paper presents a novel Multi-viewKnowledge Distillation Transformer (MKDT) framework that consists of a teachernetwork and a student network. This framework aims to handle incomplete humanaction problems in real-world applications. Specifically, the multi-viewknowledge distillation transformer uses a hierarchical vision transformer withshifted windows to capture more spatial-temporal information. Experimentalresults demonstrate that our framework outperforms the CNN-based method onthree public datasets.", "output": "Multi-view knowledge distillation transformer for human action recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability of scene understanding has sparked active research for panoramicimage semantic segmentation. However, the performance is hampered by distortionof the equirectangular projection (ERP) and a lack of pixel-wise annotations.For this reason, some works treat the ERP and pinhole images equally andtransfer knowledge from the pinhole to ERP images via unsupervised domainadaptation (UDA). However, they fail to handle the domain gaps caused by: 1)the inherent differences between camera sensors and captured scenes; 2) thedistinct image formats (e.g., ERP and pinhole images). In this paper, wepropose a novel yet flexible dual-path UDA framework, DPPASS, taking ERP andtangent projection (TP) images as inputs. To reduce the domain gaps, we proposecross-projection and intra-projection training. The cross-projection trainingincludes tangent-wise feature contrastive training and prediction consistencytraining. That is, the former formulates the features with the same projectionlocations as positive examples and vice versa, for the models' awareness ofdistortion, while the latter ensures the consistency of cross-model predictionsbetween the ERP and TP. Moreover, adversarial intra-projection training isproposed to reduce the inherent gap, between the features of the pinhole imagesand those of the ERP and TP images, respectively. Importantly, the TP path canbe freely removed after training, leading to no additional inference cost.Extensive experiments on two benchmarks show that our DPPASS achieves +1.06$\\%$mIoU increment than the state-of-the-art approaches.", "output": "Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised Domain Adaptation (UDA) of semantic segmentation transferslabeled source knowledge to an unlabeled target domain by relying on accessingboth the source and target data. However, the access to source data is oftenrestricted or infeasible in real-world scenarios. Under the source datarestrictive circumstances, UDA is less practical. To address this, recent workshave explored solutions under the Source-Free Domain Adaptation (SFDA) setup,which aims to adapt a source-trained model to the target domain withoutaccessing source data. Still, existing SFDA approaches use only image-levelinformation for adaptation, making them sub-optimal in video applications. Thispaper studies SFDA for Video Semantic Segmentation (VSS), where temporalinformation is leveraged to address video adaptation. Specifically, we proposeSpatio-Temporal Pixel-Level (STPL) contrastive learning, a novel method thattakes full advantage of spatio-temporal information to tackle the absence ofsource data better. STPL explicitly learns semantic correlations among pixelsin the spatio-temporal space, providing strong self-supervision for adaptationto the unlabeled target domain. Extensive experiments show that STPL achievesstate-of-the-art performance on VSS benchmarks compared to current UDA and SFDAapproaches. Code is available at: https://github.com/shaoyuanlo/STPL", "output": "Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FlexNeRF, a method for photorealistic freeviewpoint rendering ofhumans in motion from monocular videos. Our approach works well with sparseviews, which is a challenging scenario when the subject is exhibitingfast/complex motions. We propose a novel approach which jointly optimizes acanonical time and pose configuration, with a pose-dependent motion field andpose-independent temporal deformations complementing each other. Thanks to ournovel temporal and cyclic consistency constraints along with additional losseson intermediate representation such as segmentation, our approach provides highquality outputs as the observed views become sparser. We empiricallydemonstrate that our method significantly outperforms the state-of-the-art onpublic benchmark datasets as well as a self-captured fashion dataset. Theproject page is available at: https://flex-nerf.github.io/", "output": "FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive learning-based video-language representation learning approaches,e.g., CLIP, have achieved outstanding performance, which pursue semanticinteraction upon pre-defined video-text pairs. To clarify this coarse-grainedglobal interaction and move a step further, we have to encounter challengingshell-breaking interactions for fine-grained cross-modal learning. In thispaper, we creatively model video-text as game players with multivariatecooperative game theory to wisely handle the uncertainty during fine-grainedsemantic interaction with diverse granularity, flexible combination, and vagueintensity. Concretely, we propose Hierarchical Banzhaf Interaction (HBI) tovalue possible correspondence between video frames and text words for sensitiveand explainable cross-modal contrast. To efficiently realize the cooperativegame of multiple video frames and multiple text words, the proposed methodclusters the original video frames (text words) and computes the BanzhafInteraction between the merged tokens. By stacking token merge modules, weachieve cooperative games at different semantic levels. Extensive experimentson commonly used text-video retrieval and video-question answering benchmarkswith superior performances justify the efficacy of our HBI. More encouragingly,it can also serve as a visualization tool to promote the understanding ofcross-modal interaction, which have a far-reaching impact on the community.Project page is available at https://jpthu17.github.io/HBI/.", "output": "Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "White matter (WM) tract segmentation based on diffusion magnetic resonanceimaging (dMRI) plays an important role in the analysis of human health andbrain diseases. However, the annotation of WM tracts is time-consuming andneeds experienced neuroanatomists. In this study, to explore tract segmentationin the challenging setting of minimal annotations, we propose a novel frameworkutilizing only one annotated subject (subject-level one-shot) for tractsegmentation. Our method is constructed by proposed registration-based peakaugmentation (RPA) and uncertainty-based refining (URe) modules. RPA modulesynthesizes pseudo subjects and their corresponding labels to improve the tractsegmentation performance. The proposed URe module alleviates the negativeinfluence of the low-confidence voxels on pseudo subjects. Experimental resultsshow that our method outperforms other state-of-the-art methods by a largemargin, and our proposed modules are effective. Overall, our method achievesaccurate whole-brain tract segmentation with only one annotated subject. Ourcode is available athttps://github.com/HaoXu0507/ISBI2023-One-Shot-WM-Tract-Segmentation.", "output": "A Registration- and Uncertainty-based Framework for White Matter Tract Segmentation With Only One Annotated Subject."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cell instance segmentation in cytology images has significant importance forbiology analysis and cancer screening, while remains challenging due to 1) theextensive overlapping translucent cell clusters that cause the ambiguousboundaries, and 2) the confusion of mimics and debris as nuclei. In this work,we proposed a De-overlapping Network (DoNet) in a decompose-and-recombinedstrategy. A Dual-path Region Segmentation Module (DRM) explicitly decomposesthe cell clusters into intersection and complement regions, followed by aSemantic Consistency-guided Recombination Module (CRM) for integration. Tofurther introduce the containment relationship of the nucleus in the cytoplasm,we design a Mask-guided Region Proposal Strategy (MRP) that integrates the cellattention maps for inner-cell instance prediction. We validate the proposedapproach on ISBI2014 and CPS datasets. Experiments show that our proposed DoNetsignificantly outperforms other state-of-the-art (SOTA) cell instancesegmentation methods. The code is available athttps://github.com/DeepDoNet/DoNet.", "output": "DoNet: Deep De-overlapping Network for Cytology Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, a growing number of work design unsupervised paradigms for pointcloud processing to alleviate the limitation of expensive manual annotation andpoor transferability of supervised methods. Among them, CrossPoint follows thecontrastive learning framework and exploits image and point cloud data forunsupervised point cloud understanding. Although the promising performance ispresented, the unbalanced architecture makes it unnecessarily complex andinefficient. For example, the image branch in CrossPoint is $\\sim$8.3x heavierthan the point cloud branch leading to higher complexity and latency. Toaddress this problem, in this paper, we propose a lightweightVision-and-Pointcloud Transformer (ViPFormer) to unify image and point cloudprocessing in a single architecture. ViPFormer learns in an unsupervised mannerby optimizing intra-modal and cross-modal contrastive objectives. Then thepretrained model is transferred to various downstream tasks, including 3D shapeclassification and semantic segmentation. Experiments on different datasetsshow ViPFormer surpasses previous state-of-the-art unsupervised methods withhigher accuracy, lower model complexity and runtime latency. Finally, theeffectiveness of each component in ViPFormer is validated by extensive ablationstudies. The implementation of the proposed method is available athttps://github.com/auniquesun/ViPFormer.", "output": "ViPFormer: Efficient Vision-and-Pointcloud Transformer for Unsupervised Pointcloud Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Layout is essential for graphic design and poster generation. Recently,applying deep learning models to generate layouts has attracted increasingattention. This paper focuses on using the GAN-based model conditioned on imagecontents to generate advertising poster graphic layouts, which requires anadvertising poster layout dataset with paired product images and graphiclayouts. However, the paired images and layouts in the existing dataset arecollected by inpainting and annotating posters, respectively. There exists adomain gap between inpainted posters (source domain data) and clean productimages (target domain data). Therefore, this paper combines unsupervised domainadaption techniques to design a GAN with a novel pixel-level discriminator(PD), called PDA-GAN, to generate graphic layouts according to image contents.The PD is connected to the shallow level feature map and computes the GAN lossfor each input-image pixel. Both quantitative and qualitative evaluationsdemonstrate that PDA-GAN can achieve state-of-the-art performances and generatehigh-quality image-aware graphic layouts for advertising posters.", "output": "Unsupervised Domain Adaption with Pixel-level Discriminator for Image-aware Layout Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the increasing popularity of LiDAR sensors, perception algorithmsusing 3D LiDAR data struggle with the 'sensor-bias problem'. Specifically, theperformance of perception algorithms significantly drops when an unseenspecification of LiDAR sensor is applied at test time due to the domaindiscrepancy. This paper presents a fast and flexible LiDAR augmentation methodfor the semantic segmentation task, called 'LiDomAug'. It aggregates raw LiDARscans and creates a LiDAR scan of any configurations with the consideration ofdynamic distortion and occlusion, resulting in instant domain augmentation. Ouron-demand augmentation module runs at 330 FPS, so it can be seamlesslyintegrated into the data loader in the learning framework. In our experiments,learning-based approaches aided with the proposed LiDomAug are less affected bythe sensor-bias issue and achieve new state-of-the-art domain adaptationperformances on SemanticKITTI and nuScenes dataset without the use of thetarget domain data. We also present a sensor-agnostic model that faithfullyworks on the various LiDAR configurations.", "output": "Instant Domain Augmentation for LiDAR Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study explores the potential of a fully convolutional mesh autoencodermodel for regenerating 3D nature faces with the presence of imperfect areas. Weutilize deep learning approaches in graph processing and analysis toinvestigate the capabilities model in recreating a filling part for facialscars. Our approach in dataset creation is able to generate a facial scarrationally in a virtual space that corresponds to the unique circumstances.Especially, we propose a new method which is named 3D Facial ImperfectionRegeneration(3D-FaIR) for reproducing a complete face reconstruction based onthe remaining features of the patient face. To further enhance the applicablecapacity of the present research, we develop an improved outlier technique toseparate the wounds of patients and provide appropriate wound cover models.Also, a Cir3D-FaIR dataset of imperfect faces and open codes was released athttps://github.com/SIMOGroup/3DFaIR. Our findings demonstrate the potential ofthe proposed approach to help patients recover more quickly and safely throughconvenient techniques. We hope that this research can contribute to thedevelopment of new products and innovative solutions for facial scarregeneration.", "output": "3D Facial Imperfection Regeneration: Deep learning approach and 3D printing prototypes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given the large-scale data and the high annotation cost,pretraining-finetuning becomes a popular paradigm in multiple computer visiontasks. Previous research has covered both the unsupervised pretraining andsupervised finetuning in this paradigm, while little attention is paid toexploiting the annotation budget for finetuning. To fill in this gap, weformally define this new active finetuning task focusing on the selection ofsamples for annotation in the pretraining-finetuning paradigm. We propose anovel method called ActiveFT for active finetuning task to select a subset ofdata distributing similarly with the entire unlabeled pool and maintainingenough diversity by optimizing a parametric model in the continuous space. Weprove that the Earth Mover's distance between the distributions of the selectedsubset and the entire data pool is also reduced in this process. Extensiveexperiments show the leading performance and high efficiency of ActiveFTsuperior to baselines on both image classification and semantic segmentation.Our code is released at https://github.com/yichen928/ActiveFT.", "output": "Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper aims to solve the video object segmentation (VOS) task in ascribble-supervised manner, in which VOS models are not only trained by thesparse scribble annotations but also initialized with the sparse targetscribbles for inference. Thus, the annotation burdens for both training andinitialization can be substantially lightened. The difficulties ofscribble-supervised VOS lie in two aspects. On the one hand, it requires thepowerful ability to learn from the sparse scribble annotations during training.On the other hand, it demands strong reasoning capability during inferencegiven only a sparse initial target scribble. In this work, we propose aReliability-Hierarchical Memory Network (RHMNet) to predict the target mask ina step-wise expanding strategy w.r.t. the memory reliability level. To bespecific, RHMNet first only uses the memory in the high-reliability level tolocate the region with high reliability belonging to the target, which ishighly similar to the initial target scribble. Then it expands the locatedhigh-reliability region to the entire target conditioned on the region itselfand the memories in all reliability levels. Besides, we propose ascribble-supervised learning mechanism to facilitate the learning of our modelto predict dense results. It mines the pixel-level relation within the singleframe and the frame-level relation within the sequence to take full advantageof the scribble annotations in sequence training samples. The favorableperformance on two popular benchmarks demonstrates that our method ispromising.", "output": "Reliability-Hierarchical Memory Network for Scribble-Supervised Video Object Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prompt-OVD is an efficient and effective framework for open-vocabulary objectdetection that utilizes class embeddings from CLIP as prompts, guiding theTransformer decoder to detect objects in both base and novel classes.Additionally, our novel RoI-based masked attention and RoI pruning techniqueshelp leverage the zero-shot classification ability of the VisionTransformer-based CLIP, resulting in improved detection performance at minimalcomputational cost. Our experiments on the OV-COCO and OVLVIS datasetsdemonstrate that Prompt-OVD achieves an impressive 21.2 times faster inferencespeed than the first end-to-end open-vocabulary detection method (OV-DETR),while also achieving higher APs than four two-stage-based methods operatingwithin similar inference time ranges. Code will be made available soon.", "output": "Prompt-Guided Transformers for End-to-End Open-Vocabulary Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite its success in image synthesis, we observe that diffusionprobabilistic models (DPMs) often lack contextual reasoning ability to learnthe relations among object parts in an image, leading to a slow learningprocess. To solve this issue, we propose a Masked Diffusion Transformer (MDT)that introduces a mask latent modeling scheme to explicitly enhance the DPMs'ability of contextual relation learning among object semantic parts in animage. During training, MDT operates on the latent space to mask certaintokens. Then, an asymmetric masking diffusion transformer is designed topredict masked tokens from unmasked ones while maintaining the diffusiongeneration process. Our MDT can reconstruct the full information of an imagefrom its incomplete contextual input, thus enabling it to learn the associatedrelations among image tokens. Experimental results show that MDT achievessuperior image synthesis performance, e.g. a new SoTA FID score on the ImageNetdataset, and has about 3x faster learning speed than the previous SoTA DiT. Thesource code is released at https://github.com/sail-sg/MDT.", "output": "Masked Diffusion Transformer is a Strong Image Synthesizer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural decoding of visual object classification via functional magneticresonance imaging (fMRI) data is challenging and is vital to understandunderlying brain mechanisms. This paper proposed a multi-pooling 3Dconvolutional neural network (MP3DCNN) to improve fMRI classification accuracy.MP3DCNN is mainly composed of a three-layer 3DCNN, where the first and secondlayers of 3D convolutions each have a branch of pooling connection. The resultsshowed that this model can improve the classification accuracy for categorical(face vs. object), face sub-categorical (male face vs. female face), and objectsub-categorical (natural object vs. artificial object) classifications from1.684% to 14.918% over the previous study in decoding brain mechanisms.", "output": "Multi-pooling 3D Convolutional Neural Network for fMRI Classification of Visual Brain States."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While impressive progress has been achieved, video instance segmentation(VIS) methods with per-clip input often fail on challenging videos withoccluded objects and crowded scenes. This is mainly because instance queries inthese methods cannot encode well the discriminative embeddings of instances,making the query-based segmenter difficult to distinguish those `hard'instances. To address these issues, we propose to mine discriminative queryembeddings (MDQE) to segment occluded instances on challenging videos. First,we initialize the positional embeddings and content features of object queriesby considering their spatial contextual information and the inter-frame objectmotion. Second, we propose an inter-instance mask repulsion loss to distanceeach instance from its nearby non-target instances. The proposed MDQE is thefirst VIS method with per-clip input that achieves state-of-the-art results onchallenging videos and competitive performance on simple videos. In specific,MDQE with ResNet50 achieves 33.0\\% and 44.5\\% mask AP on OVIS and YouTube-VIS2021, respectively. Code of MDQE can be found at\\url{https://github.com/MinghanLi/MDQE_CVPR2023}.", "output": "MDQE: Mining Discriminative Query Embeddings to Segment Occluded Instances on Challenging Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-language (VL) pre-training has recently gained much attention for itstransferability and flexibility in novel concepts (e.g., cross-modalitytransfer) across various visual tasks. However, VL-driven segmentation has beenunder-explored, and the existing approaches still have the burden of acquiringadditional training images or even segmentation annotations to adapt a VL modelto downstream segmentation tasks. In this paper, we introduce a novelimage-free segmentation task where the goal is to perform semantic segmentationgiven only a set of the target semantic categories, but without anytask-specific images and annotations. To tackle this challenging task, ourproposed method, coined IFSeg, generates VL-driven artificialimage-segmentation pairs and updates a pre-trained VL model to a segmentationtask. We construct this artificial training data by creating a 2D map of randomsemantic categories and another map of their corresponding word tokens. Giventhat a pre-trained VL model projects visual and text tokens into a common spacewhere tokens that share the semantics are located closely, this artificiallygenerated word map can replace the real image inputs for such a VL model.Through an extensive set of experiments, our model not only establishes aneffective baseline for this novel task but also demonstrates strongperformances compared to existing methods that rely on stronger supervision,such as task-specific images and segmentation masks. Code is available athttps://github.com/alinlab/ifseg.", "output": "IFSeg: Image-free Semantic Segmentation via Vision-Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) have enabled astounding progress in severalvision-based problems. Despite showing high predictive accuracy, recently,several works have revealed that they tend to provide overconfident predictionsand thus are poorly calibrated. The majority of the works addressing themiscalibration of DNNs fall under the scope of classification and consider onlyin-domain predictions. However, there is little to no progress in studying thecalibration of DNN-based object detection models, which are central to manyvision-based safety-critical applications. In this paper, inspired by thetrain-time calibration methods, we propose a novel auxiliary loss formulationthat explicitly aims to align the class confidence of bounding boxes with theaccurateness of predictions (i.e. precision). Since the original formulation ofour loss depends on the counts of true positives and false positives in aminibatch, we develop a differentiable proxy of our loss that can be usedduring training with other application-specific loss functions. We performextensive experiments on challenging in-domain and out-domain scenarios withsix benchmark datasets including MS-COCO, Cityscapes, Sim10k, and BDD100k. Ourresults reveal that our train-time loss surpasses strong calibration baselinesin reducing calibration error for both in and out-domain scenarios. Our sourcecode and pre-trained models are available athttps://github.com/akhtarvision/bpc_calibration", "output": "Bridging Precision and Confidence: A Train-Time Loss for Calibrating Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The creation of 2D realistic facial images and 3D face shapes usinggenerative networks has been a hot topic in recent years. Existing facegenerators exhibit exceptional performance on faces in small to medium poses(with respect to frontal faces) but struggle to produce realistic results forlarge poses. The distorted rendering results on large poses in 3D-awaregenerators further show that the generated 3D face shapes are far from thedistribution of 3D faces in reality. We find that the above issues are causedby the training dataset's pose imbalance.In this paper, we present LPFF, a large-pose Flickr face dataset comprised of19,590 high-quality real large-pose portrait images. We utilize our dataset totrain a 2D face generator that can process large-pose face images, as well as a3D-aware generator that can generate realistic human face geometry. To bettervalidate our pose-conditional 3D-aware generators, we develop a new FID measureto evaluate the 3D-level performance. Through this novel FID measure and otherexperiments, we show that LPFF can help 2D face generators extend their latentspace and better manipulate the large-pose data, and help 3D-aware facegenerators achieve better view consistency and more realistic 3D reconstructionresults.", "output": "LPFF: A Portrait Dataset for Face Generators Across Large Poses."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of 3D semantic scene graph (3DSSG) prediction in the point cloud ischallenging since (1) the 3D point cloud only captures geometric structureswith limited semantics compared to 2D images, and (2) long-tailed relationdistribution inherently hinders the learning of unbiased prediction. Since 2Dimages provide rich semantics and scene graphs are in nature coped withlanguages, in this study, we propose Visual-Linguistic Semantics AssistedTraining (VL-SAT) scheme that can significantly empower 3DSSG prediction modelswith discrimination about long-tailed and ambiguous semantic relations. The keyidea is to train a powerful multi-modal oracle model to assist the 3D model.This oracle learns reliable structural representations based on semantics fromvision, language, and 3D geometry, and its benefits can be heterogeneouslypassed to the 3D model during the training stage. By effectively utilizingvisual-linguistic semantics in training, our VL-SAT can significantly boostcommon 3DSSG prediction models, such as SGFN and SGGpoint, only with 3D inputsin the inference stage, especially when dealing with tail relation triplets.Comprehensive evaluations and ablation studies on the 3DSSG dataset havevalidated the effectiveness of the proposed scheme. Code is available athttps://github.com/wz7in/CVPR2023-VLSAT.", "output": "VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent vision architectures and self-supervised training methods enablevision models that are extremely accurate and general, but come with massiveparameter and computational costs. In practical settings, such as camera traps,users have limited resources, and may fine-tune a pretrained model on (oftenlimited) data from a small set of specific categories of interest. These usersmay wish to make use of modern, highly-accurate models, but are oftencomputationally constrained. To address this, we ask: can we quickly compresslarge generalist models into accurate and efficient specialists? For this, wepropose a simple and versatile technique called Few-Shot Task-Aware Compression(TACO). Given a large vision model that is pretrained to be accurate on a broadtask, such as classification over ImageNet-22K, TACO produces a smaller modelthat is accurate on specialized tasks, such as classification across vehicletypes or animal species. Crucially, TACO works in few-shot fashion, i.e. only afew task-specific samples are used, and the procedure has low computationaloverheads. We validate TACO on highly-accurate ResNet, ViT/DeiT, and ConvNeXtmodels, originally trained on ImageNet, LAION, or iNaturalist, which wespecialize and compress to a diverse set of \"downstream\" subtasks. TACO canreduce the number of non-zero parameters in existing models by up to 20xrelative to the original models, leading to inference speedups of up to3$\\times$, while remaining accuracy-competitive with the uncompressed models onthe specialized tasks.", "output": "Vision Models Can Be Efficiently Specialized via Few-Shot Task-Aware Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning-based recognition systems are deployed at scale for severalreal-world applications that inevitably involve our social life. Although beingof great support when making complex decisions, they might capture spuriousdata correlations and leverage sensitive attributes (e.g. age, gender,ethnicity). How to factor out this information while keeping a high predictionperformance is a task with still several open questions, many of which areshared with those of the domain adaptation and generalization literature whichfocuses on avoiding visual domain biases. In this work, we propose an in-depthstudy of the relationship between cross-domain learning (CD) and model fairnessby introducing a benchmark on face and medical images spanning severaldemographic groups as well as classification and localization tasks. Afterhaving highlighted the limits of the current evaluation metrics, we introduce anew Harmonic Fairness (HF) score to assess jointly how fair and accurate everymodel is with respect to a reference baseline. Our study covers 14 CDapproaches alongside three state-of-the-art fairness algorithms and shows howthe former can outperform the latter. Overall, our work paves the way for amore systematic analysis of fairness problems in computer vision. Codeavailable at: https://github.com/iurada/fairness_crossdomain", "output": "Fairness meets Cross-Domain Learning: a new perspective on Models and Metrics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Typical layout-to-image synthesis (LIS) models generate images for a closedset of semantic classes, e.g., 182 common objects in COCO-Stuff. In this work,we explore the freestyle capability of the model, i.e., how far can it generateunseen semantics (e.g., classes, attributes, and styles) onto a given layout,and call the task Freestyle LIS (FLIS). Thanks to the development oflarge-scale pre-trained language-image models, a number of discriminativemodels (e.g., image classification and object detection) trained on limitedbase classes are empowered with the ability of unseen class prediction.Inspired by this, we opt to leverage large-scale pre-trained text-to-imagediffusion models to achieve the generation of unseen semantics. The keychallenge of FLIS is how to enable the diffusion model to synthesize imagesfrom a specific layout which very likely violates its pre-learned knowledge,e.g., the model never sees \"a unicorn sitting on a bench\" during itspre-training. To this end, we introduce a new module called RectifiedCross-Attention (RCA) that can be conveniently plugged in the diffusion modelto integrate semantic masks. This \"plug-in\" is applied in each cross-attentionlayer of the model to rectify the attention maps between image and text tokens.The key idea of RCA is to enforce each text token to act on the pixels in aspecified region, allowing us to freely put a wide variety of semantics frompre-trained knowledge (which is general) onto the given layout (which isspecific). Extensive experiments show that the proposed diffusion networkproduces realistic and freestyle layout-to-image generation results withdiverse text inputs, which has a high potential to spawn a bunch of interestingapplications. Code is available at https://github.com/essunny310/FreestyleNet.", "output": "Freestyle Layout-to-Image Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have witnessed a rapid growth of deep generative models, withtext-to-image models gaining significant attention from the public. However,existing models often generate images that do not align well with humanaesthetic preferences, such as awkward combinations of limbs and facialexpressions. To address this issue, we collect a dataset of human choices ongenerated images from the Stable Foundation Discord channel. Our experimentsdemonstrate that current evaluation metrics for generative models do notcorrelate well with human choices. Thus, we train a human preference classifierwith the collected dataset and derive a Human Preference Score (HPS) based onthe classifier. Using the HPS, we propose a simple yet effective method toadapt Stable Diffusion to better align with human aesthetic preferences. Ourexperiments show that the HPS outperforms CLIP in predicting human choices andhas good generalization capability towards images generated from other models.By tuning Stable Diffusion with the guidance of the HPS, the adapted model isable to generate images that are more preferred by human users.", "output": "Better Aligning Text-to-Image Models with Human Preference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Noise is an important issue for radiographic and tomographic imagingtechniques. It becomes particularly critical in applications where additionalconstraints force a strong reduction of the Signal-to-Noise Ratio (SNR) perimage. These constraints may result from limitations on the maximum availableflux or permissible dose and the associated restriction on exposure time.Often, a high SNR per image is traded for the ability to distribute a giventotal exposure capacity per pixel over multiple channels, thus obtainingadditional information about the object by the same total exposure time. Thesecan be energy channels in the case of spectroscopic imaging or time channels inthe case of time-resolved imaging. In this paper, we report on a method forimproving the quality of noisy multi-channel (time or energy-resolved) imagingdatasets. The method relies on the recent Noise2Noise (N2N) self-superviseddenoising approach that learns to predict a noise-free signal without access tonoise-free data. N2N in turn requires drawing pairs of samples from a datadistribution sharing identical signals while being exposed to different samplesof random noise. The method is applicable if adjacent channels share enoughinformation to provide images with similar enough information but independentnoise. We demonstrate several representative case studies, namely spectroscopic(k-edge) X-ray tomography, in vivo X-ray cine-radiography, andenergy-dispersive (Bragg edge) neutron tomography. In all cases, the N2N methodshows dramatic improvement and outperforms conventional denoising methods. Forsuch imaging techniques, the method can therefore significantly improve imagequality, or maintain image quality with further reduced exposure time perimage.", "output": "Shot Noise Reduction in Radiographic and Tomographic Multi-Channel Imaging with Self-Supervised Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Active learning aims to identify the most informative data from an unlabeleddata pool that enables a model to reach the desired accuracy rapidly. Thisbenefits especially deep neural networks which generally require a huge numberof labeled samples to achieve high performance. Most existing active learningmethods have been evaluated in an ideal setting where only samples relevant tothe target task, i.e., in-distribution samples, exist in an unlabeled datapool. A data pool gathered from the wild, however, is likely to include samplesthat are irrelevant to the target task at all and/or too ambiguous to assign asingle class label even for the oracle. We argue that assuming an unlabeleddata pool consisting of samples from various distributions is more realistic.In this work, we introduce new active learning benchmarks that includeambiguous, task-irrelevant out-of-distribution as well as in-distributionsamples. We also propose an active learning method designed to acquireinformative in-distribution samples in priority. The proposed method leveragesboth labeled and unlabeled data pools and selects samples from clusters on thefeature space constructed via contrastive learning. Experimental resultsdemonstrate that the proposed method requires a lower annotation budget thanexisting active learning methods to reach the same level of accuracy.", "output": "Deep Active Learning with Contrastive Learning Under Realistic Data Pool Assumptions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic Neural Radiance Field (NeRF) is a powerful algorithm capable ofrendering photo-realistic novel view images from a monocular RGB video of adynamic scene. Although it warps moving points across frames from theobservation spaces to a common canonical space for rendering, dynamic NeRF doesnot model the change of the reflected color during the warping. As a result,this approach often fails drastically on challenging specular objects inmotion. We address this limitation by reformulating the neural radiance fieldfunction to be conditioned on surface position and orientation in theobservation space. This allows the specular surface at different poses to keepthe different reflected colors when mapped to the common canonical space.Additionally, we add the mask of moving objects to guide the deformation field.As the specular surface changes color during motion, the mask mitigates theproblem of failure to find temporal correspondences with only RGB supervision.We evaluate our model based on the novel view synthesis quality with aself-collected dataset of different moving specular objects in realisticenvironments. The experimental results demonstrate that our methodsignificantly improves the reconstruction quality of moving specular objectsfrom monocular RGB videos compared to the existing NeRF models. Our code anddata are available at the project website https://github.com/JokerYan/NeRF-DS.", "output": "NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The medical imaging community generates a wealth of datasets, many of whichare openly accessible and annotated for specific diseases and tasks such asmulti-organ or lesion segmentation. Current practices continue to limit modeltraining and supervised pre-training to one or a few similar datasets,neglecting the synergistic potential of other available annotated data. Wepropose MultiTalent, a method that leverages multiple CT datasets with diverseand conflicting class definitions to train a single model for a comprehensivestructure segmentation. Our results demonstrate improved segmentationperformance compared to previous related approaches, systematically, alsocompared to single dataset training using state-of-the-art methods, especiallyfor lesion segmentation and other challenging structures. We show thatMultiTalent also represents a powerful foundation model that offers a superiorpre-training for various segmentation tasks compared to commonly usedsupervised or unsupervised pre-training baselines. Our findings offer a newdirection for the medical imaging community to effectively utilize the wealthof available data for improved segmentation performance. The code and modelweights will be published here: [tba]", "output": "MultiTalent: A Multi-Dataset Approach to Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In-betweening is a technique for generating transitions given initial andtarget character states. The majority of existing works require multiple (often$&gt;$10) frames as input, which are not always accessible. Our work deals with afocused yet challenging problem: to generate the transition when given exactlytwo frames (only the first and last). To cope with this challenging scenario,we implement our bi-directional scheme which generates forward and backwardtransitions from the start and end frames with two adversarial autoregressivenetworks, and stitches them in the middle of the transition where there is nostrict ground truth. The autoregressive networks based on conditionalvariational autoencoders (CVAE) are optimized by searching for a pair ofoptimal latent codes that minimize a novel stitching loss between theiroutputs. Results show that our method achieves higher motion quality and morediverse results than existing methods on both the LaFAN1 and Human3.6mdatasets.", "output": "Diverse Motion In-betweening with Dual Posture Stitching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training has been widely acknowledged as the most effectivemethod to improve the adversarial robustness against adversarial examples forDeep Neural Networks (DNNs). So far, most existing works focus on enhancing theoverall model robustness, treating each class equally in both the training andtesting phases. Although revealing the disparity in robustness among classes,few works try to make adversarial training fair at the class level withoutsacrificing overall robustness. In this paper, we are the first totheoretically and empirically investigate the preference of different classesfor adversarial configurations, including perturbation margin, regularization,and weight averaging. Motivated by this, we further propose a\\textbf{C}lass-wise calibrated \\textbf{F}air \\textbf{A}dversarial trainingframework, named CFA, which customizes specific training configurations foreach class automatically. Experiments on benchmark datasets demonstrate thatour proposed CFA can improve both overall robustness and fairness notably overother state-of-the-art methods. Code is available at\\url{https://github.com/PKU-ML/CFA}.", "output": "CFA: Class-wise Calibrated Fair Adversarial Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study explores the concept of equivariance in vision-language foundationmodels (VLMs), focusing specifically on the multimodal similarity function thatis not only the major training objective but also the core delivery to supportdownstream tasks. Unlike the existing image-text similarity objective whichonly categorizes matched pairs as similar and unmatched pairs as dissimilar,equivariance also requires similarity to vary faithfully according to thesemantic changes. This allows VLMs to generalize better to nuanced and unseenmultimodal compositions. However, modeling equivariance is challenging as theground truth of semantic change is difficult to collect. For example, given animage-text pair about a dog, it is unclear to what extent the similaritychanges when the pixel is changed from dog to cat? To this end, we proposeEqSim, a regularization loss that can be efficiently calculated from any twomatched training pairs and easily pluggable into existing image-text retrievalfine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, wepresent a new challenging benchmark EqBen. Compared to the existing evaluationsets, EqBen is the first to focus on \"visual-minimal change\". Extensiveexperiments show the lack of equivariance in current VLMs and validate theeffectiveness of EqSim. Code is available at\\url{https://github.com/Wangt-CN/EqBen}.", "output": "Equivariant Similarity for Vision-Language Foundation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Binary Neural Network (BNN) represents convolution weights with 1-bit values,which enhances the efficiency of storage and computation. This paper ismotivated by a previously revealed phenomenon that the binary kernels insuccessful BNNs are nearly power-law distributed: their values are mostlyclustered into a small number of codewords. This phenomenon encourages us tocompact typical BNNs and obtain further close performance through learningnon-repetitive kernels within a binary kernel subspace. Specifically, we regardthe binarization process as kernel grouping in terms of a binary codebook, andour task lies in learning to select a smaller subset of codewords from the fullcodebook. We then leverage the Gumbel-Sinkhorn technique to approximate thecodeword selection process, and develop the Permutation Straight-ThroughEstimator (PSTE) that is able to not only optimize the selection processend-to-end but also maintain the non-repetitive occupancy of selectedcodewords. Experiments verify that our method reduces both the model size andbit-wise computational costs, and achieves accuracy improvements compared withstate-of-the-art BNNs under comparable budgets.", "output": "Compacting Binary Neural Networks by Sparse Kernel Selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-view volumetric rendering techniques have recently shown greatpotential in modeling and synthesizing high-quality head avatars. A commonapproach to capture full head dynamic performances is to track the underlyinggeometry using a mesh-based template or 3D cube-based graphics primitives.While these model-based approaches achieve promising results, they often failto learn complex geometric details such as the mouth interior, hair, andtopological changes over time. This paper presents a novel approach to buildinghighly photorealistic digital head avatars. Our method learns a canonical spacevia an implicit function parameterized by a neural network. It leveragesmultiresolution hash encoding in the learned feature space, allowing forhigh-quality, faster training and high-resolution rendering. At test time, ourmethod is driven by a monocular RGB video. Here, an image encoder extractsface-specific features that also condition the learnable canonical space. Thisencourages deformation-dependent texture variations during training. We alsopropose a novel optical flow based loss that ensures correspondences in thelearned canonical space, thus encouraging artifact-free and temporallyconsistent renderings. We show results on challenging facial expressions andshow free-viewpoint renderings at interactive real-time rates for medium imageresolutions. Our method outperforms all existing approaches, both visually andnumerically. We will release our multiple-identity dataset to encourage furtherresearch. Our Project page is available at:https://vcai.mpi-inf.mpg.de/projects/HQ3DAvatar/", "output": "HQ3DAvatar: High Quality Controllable 3D Head Avatar."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many skeletal action recognition models use GCNs to represent the human bodyby 3D body joints connected body parts. GCNs aggregate one- or few-hop graphneighbourhoods, and ignore the dependency between not linked body joints. Wepropose to form hypergraph to model hyper-edges between graph nodes (e.g.,third- and fourth-order hyper-edges capture three and four nodes) which helpcapture higher-order motion patterns of groups of body joints. We split actionsequences into temporal blocks, Higher-order Transformer (HoT) producesembeddings of each temporal block based on (i) the body joints, (ii) pairwiselinks of body joints and (iii) higher-order hyper-edges of skeleton bodyjoints. We combine such HoT embeddings of hyper-edges of orders 1, ..., r by anovel Multi-order Multi-mode Transformer (3Mformer) with two modules whoseorder can be exchanged to achieve coupled-mode attention on coupled-mode tokensbased on 'channel-temporal block', 'order-channel-body joint','channel-hyper-edge (any order)' and 'channel-only' pairs. The first module,called Multi-order Pooling (MP), additionally learns weighted aggregation alongthe hyper-edge mode, whereas the second module, Temporal block Pooling (TP),aggregates along the temporal block mode. Our end-to-end trainable networkyields state-of-the-art results compared to GCN-, transformer- andhypergraph-based counterparts.", "output": "3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works such as BARF and GARF can bundle adjust camera poses with neuralradiance fields (NeRF) which is based on coordinate-MLPs. Despite theimpressive results, these methods cannot be applied to Generalizable NeRFs(GeNeRFs) which require image feature extractions that are often based on morecomplicated 3D CNN or transformer architectures. In this work, we first analyzethe difficulties of jointly optimizing camera poses with GeNeRFs, and thenfurther propose our DBARF to tackle these issues. Our DBARF which bundleadjusts camera poses by taking a cost feature map as an implicit cost functioncan be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARFand its follow-up works, which can only be applied to per-scene optimized NeRFsand need accurate initial camera poses with the exception of forward-facingscenes, our method can generalize across scenes and does not require any goodinitialization. Experiments show the effectiveness and generalization abilityof our DBARF when evaluated on real-world datasets. Our code is available at\\url{https://aibluefisher.github.io/dbarf}.", "output": "DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image quality assessment is an important aspect of image acquisition,as poor-quality images may lead to misdiagnosis. Manual labelling of imagequality is a tedious task for population studies and can lead to misleadingresults. While much research has been done on automated analysis of imagequality to address this issue, relatively little work has been done to explainthe methodologies. In this work, we propose an explainable image qualityassessment system and validate our idea on two different objectives which areforeign object detection on Chest X-Rays (Object-CXR) and Left VentricularOutflow Tract (LVOT) detection on Cardiac Magnetic Resonance (CMR) volumes. Weapply a variety of techniques to measure the faithfulness of the saliencydetectors, and our explainable pipeline relies on NormGrad, an algorithm whichcan efficiently localise image quality issues with saliency maps of theclassifier. We compare NormGrad with a range of saliency detection methods andillustrate its superior performance as a result of applying these methodologiesfor measuring the faithfulness of the saliency detectors. We see that NormGradhas significant gains over other saliency detectors by reaching a repeatedPointing Game score of 0.853 for Object-CXR and 0.611 for LVOT datasets.", "output": "Explainable Image Quality Assessment for Medical Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For the visible-infrared person re-identification (VIReID) task, one of themajor challenges is the modality gaps between visible (VIS) and infrared (IR)images. However, the training samples are usually limited, while the modalitygaps are too large, which leads that the existing methods cannot effectivelymine diverse cross-modality clues. To handle this limitation, we propose anovel augmentation network in the embedding space, called diverse embeddingexpansion network (DEEN). The proposed DEEN can effectively generate diverseembeddings to learn the informative feature representations and reduce themodality discrepancy between the VIS and IR images. Moreover, the VIReID modelmay be seriously affected by drastic illumination changes, while all theexisting VIReID datasets are captured under sufficient illumination withoutsignificant light changes. Thus, we provide a low-light cross-modality (LLCM)dataset, which contains 46,767 bounding boxes of 1,064 identities captured by 9RGB/IR cameras. Extensive experiments on the SYSU-MM01, RegDB and LLCM datasetsshow the superiority of the proposed DEEN over several other state-of-the-artmethods. The code and dataset are released at: https://github.com/ZYK100/LLCM", "output": "Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection on drone images with low-latency is an important butchallenging task on the resource-constrained unmanned aerial vehicle (UAV)platform. This paper investigates optimizing the detection head based on thesparse convolution, which proves effective in balancing the accuracy andefficiency. Nevertheless, it suffers from inadequate integration of contextualinformation of tiny objects as well as clumsy control of the mask ratio in thepresence of foreground with varying scales. To address the issues above, wepropose a novel global context-enhanced adaptive sparse convolutional network(CEASC). It first develops a context-enhanced group normalization (CE-GN)layer, by replacing the statistics based on sparsely sampled features with theglobal contextual ones, and then designs an adaptive multi-layer maskingstrategy to generate optimal mask ratios at distinct scales for compactforeground coverage, promoting both the accuracy and efficiency. Extensiveexperimental results on two major benchmarks, i.e. VisDrone and UAVDT,demonstrate that CEASC remarkably reduces the GFLOPs and accelerates theinference procedure when plugging into the typical state-of-the-art detectionframeworks (e.g. RetinaNet and GFL V1) with competitive performance. Code isavailable at https://github.com/Cuogeihong/CEASC.", "output": "Adaptive Sparse Convolutional Networks with Global Context Enhancement for Faster Object Detection on Drone Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tactile sensing is one of the modalities humans rely on heavily to perceivethe world. Working with vision, this modality refines local geometry structure,measures deformation at the contact area, and indicates the hand-object contactstate.With the availability of open-source tactile sensors such as DIGIT, researchon visual-tactile learning is becoming more accessible and reproducible.Leveraging this tactile sensor, we propose a novel visual-tactile in-handobject reconstruction framework \\textbf{VTacO}, and extend it to\\textbf{VTacOH} for hand-object reconstruction. Since our method can supportboth rigid and deformable object reconstruction, no existing benchmarks areproper for the goal. We propose a simulation environment, VT-Sim, whichsupports generating hand-object interaction for both rigid and deformableobjects. With VT-Sim, we generate a large-scale training dataset and evaluateour method on it. Extensive experiments demonstrate that our proposed methodcan outperform the previous baseline methods qualitatively and quantitatively.Finally, we directly apply our model trained in simulation to variousreal-world test cases, which display qualitative results.Codes, models, simulation environment, and datasets are available at\\url{https://sites.google.com/view/vtaco/}.", "output": "Visual-Tactile Sensing for In-Hand Object Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Imaging is usually a mixture problem of incomplete color sampling, noisedegradation, and limited resolution. This mixture problem is typically solvedby a sequential solution that applies demosaicing (DM), denoising (DN), andsuper-resolution (SR) sequentially in a fixed and predefined pipeline(execution order of tasks), DM$\\to$DN$\\to$SR. The most recent work on imageprocessing focuses on developing more sophisticated architectures to achievehigher image quality. Little attention has been paid to the design of thepipeline, and it is still not clear how significant the pipeline is to imagequality. In this work, we comprehensively study the effects of pipelines on themixture problem of learning-based DN, DM, and SR, in both sequential and jointsolutions. On the one hand, in sequential solutions, we find that the pipelinehas a non-trivial effect on the resulted image quality. Our suggested pipelineDN$\\to$SR$\\to$DM yields consistently better performance than other sequentialpipelines in various experimental settings and benchmarks. On the other hand,in joint solutions, we propose an end-to-end Trinity Pixel Enhancement NETwork(TENet) that achieves state-of-the-art performance for the mixture problem. Wefurther present a novel and simple method that can integrate a certain pipelineinto a given end-to-end network by providing intermediate supervision using adetachable head. Extensive experiments show that an end-to-end network with theproposed pipeline can attain only a consistent but insignificant improvement.Our work indicates that the investigation of pipelines is applicable insequential solutions, but is not very necessary in end-to-end networks.\\RR{Code, models, and our contributed PixelShift200 dataset are available at\\url{https://github.com/guochengqian/TENet}", "output": "Rethinking Learning-based Demosaicing, Denoising, and Super-Resolution Pipeline."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks are vulnerable to a wide range of erroneous inputs such asadversarial, corrupted, out-of-distribution, and misclassified examples. Inthis work, we train a linear SVM classifier to detect these four types oferroneous data using hidden and softmax feature vectors of pre-trained neuralnetworks. Our results indicate that these faulty data types generally exhibitlinearly separable activation properties from correct examples, giving us theability to reject bad inputs with no extra training or overhead. Weexperimentally validate our findings across a diverse range of datasets,domains, pre-trained models, and adversarial attacks.", "output": "Utilizing Network Properties to Detect Erroneous Inputs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Purpose: Manual annotation of gastric X-ray images by doctors for gastritisdetection is time-consuming and expensive. To solve this, a self-supervisedlearning method is developed in this study. The effectiveness of the proposedself-supervised learning method in gastritis detection is verified using a fewannotated gastric X-ray images. Methods: In this study, we develop a novelmethod that can perform explicit self-supervised learning and learndiscriminative representations from gastric X-ray images. Models trained basedon the proposed method were fine-tuned on datasets comprising a few annotatedgastric X-ray images. Five self-supervised learning methods, i.e., SimSiam,BYOL, PIRL-jigsaw, PIRL-rotation, and SimCLR, were compared with the proposedmethod. Furthermore, three previous methods, one pretrained on ImageNet, onetrained from scratch, and one semi-supervised learning method, were comparedwith the proposed method. Results: The proposed method's harmonic mean score ofsensitivity and specificity after fine-tuning with the annotated data of 10,20, 30, and 40 patients were 0.875, 0.911, 0.915, and 0.931, respectively. Theproposed method outperformed all comparative methods, including the fiveself-supervised learning and three previous methods. Experimental resultsshowed the effectiveness of the proposed method in gastritis detection using afew annotated gastric X-ray images. Conclusions: This paper proposes a novelself-supervised learning method based on a teacher-student architecture forgastritis detection using gastric X-ray images. The proposed method can performexplicit self-supervised learning and learn discriminative representations fromgastric X-ray images. The proposed method exhibits potential clinical use ingastritis detection using a few annotated gastric X-ray images.", "output": "Self-Supervised Learning for Gastritis Detection with Gastric X-ray Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image registration is a crucial task in signal processing, but it oftenencounters issues with stability and efficiency. Non-learning registrationapproaches rely on optimizing similarity metrics between fixed and movingimages, which can be expensive in terms of time and space complexity. Thisproblem can be exacerbated when the images are large or there are significantdeformations between them. Recently, deep learning, specifically convolutionalneural network (CNN)-based methods, have been explored as an effective solutionto the weaknesses of non-learning approaches. To further advance learningapproaches in image registration, we introduce an attention mechanism in thedeformable image registration problem. Our proposed approach is based on aTransformer framework called AiR, which can be efficiently trained on GPGPUdevices. We treat the image registration problem as a language translation taskand use the Transformer to learn the deformation field. The method learns anunsupervised generated deformation map and is tested on two benchmark datasets.In summary, our approach shows promising effectiveness in addressing stabilityand efficiency issues in image registration tasks. The source code of AiR isavailable on Github.", "output": "Attention for Image Registration (AiR): an unsupervised Transformer approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video super-resolution (VSR), with the aim to restore a high-resolution videofrom its corresponding low-resolution version, is a spatial-temporal sequenceprediction problem. Recently, Transformer has been gaining popularity due toits parallel computing ability for sequence-to-sequence modeling. Thus, itseems to be straightforward to apply the vision Transformer to solve VSR.However, the typical block design of Transformer with a fully connectedself-attention layer and a token-wise feed-forward layer does not fit well forVSR due to the following two reasons. First, the fully connected self-attentionlayer neglects to exploit the data locality because this layer relies on linearlayers to compute attention maps. Second, the token-wise feed-forward layerlacks the feature alignment which is important for VSR since this layerindependently processes each of the input token embeddings without anyinteraction among them. In this paper, we make the first attempt to adaptTransformer for VSR. Specifically, to tackle the first issue, we present aspatial-temporal convolutional self-attention layer with a theoreticalunderstanding to exploit the locality information. For the second issue, wedesign a bidirectional optical flow-based feed-forward layer to discover thecorrelations across different video frames and also align features. Extensiveexperiments on several benchmark datasets demonstrate the effectiveness of ourproposed method. The code will be available athttps://github.com/caojiezhang/VSR-Transformer.", "output": "Video Super-Resolution Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a method to estimate the uncertainty of the outcome of an imageclassifier on a given input datum. Deep neural networks commonly used for imageclassification are deterministic maps from an input image to an output class.As such, their outcome on a given datum involves no uncertainty, so we mustspecify what variability we are referring to when defining, measuring andinterpreting uncertainty, and attributing \"confidence\" to the outcome. To thisend, we introduce the Wellington Posterior, which is the distribution ofoutcomes that would have been obtained in response to data that could have beengenerated by the same scene that produced the given image. Since there areinfinitely many scenes that could have generated any given image, theWellington Posterior involves inductive transfer from scenes other than the oneportrayed. We explore the use of data augmentation, dropout, ensembling,single-view reconstruction, and model linearization to compute a WellingtonPosterior. Additional methods include the use of conditional generative modelssuch as generative adversarial networks, neural radiance fields, andconditional prior networks. We test these methods against the empiricalposterior obtained by performing inference on multiple images of the sameunderlying scene. These developments are only a small step towards assessingthe reliability of deep network classifiers in a manner that is compatible withsafety-critical applications and human interpretation.", "output": "Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data hiding is the process of embedding information into a noise-tolerantsignal such as a piece of audio, video, or an image, including DigitalWatermarking for robust identity verification and Steganography to embed datafor the purpose of secure and secret communication. This survey provides asummary of recent advancements in deep learning techniques for data hiding inwatermarking and steganography, and categorizes them based on modelarchitectures and noise injection methods. The objective functions, evaluationmetrics, and datasets used for training these data hiding models arecomprehensively summarised. Finally, we propose and discuss possible futuredirections for the unification of digital watermarking and steganography insoftware engineering to promote Responsible AI.", "output": "Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most polyp segmentation methods use CNNs as their backbone, leading to twokey issues when exchanging information between the encoder and decoder: 1)taking into account the differences in contribution between different-levelfeatures and 2) designing an effective mechanism for fusing these features.Unlike existing CNN-based methods, we adopt a transformer encoder, which learnsmore powerful and robust representations. In addition, considering the imageacquisition influence and elusive properties of polyps, we introduce threestandard modules, including a cascaded fusion module (CFM), a camouflageidentification module (CIM), and a similarity aggregation module (SAM). Amongthese, the CFM is used to collect the semantic and location information ofpolyps from high-level features; the CIM is applied to capture polypinformation disguised in low-level features, and the SAM extends the pixelfeatures of the polyp area with high-level semantic position information to theentire polyp area, thereby effectively fusing cross-level features. Theproposed model, named Polyp-PVT, effectively suppresses noises in the featuresand significantly improves their expressive capabilities. Extensive experimentson five widely adopted datasets show that the proposed model is more robust tovarious challenging situations (\\emph{e.g.}, appearance changes, small objects,rotation) than existing representative methods. The proposed model is availableat https://github.com/DengPingFan/Polyp-PVT.", "output": "Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization for semantic segmentation is highly demanded in realapplications, where a trained model is expected to work well in previouslyunseen domains. One challenge lies in the lack of data which could cover thediverse distributions of the possible unseen domains for training. In thispaper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme,which is the first to exploit the diversity of web-crawled images forgeneralizable semantic segmentation. To explore and exploit the real-world datadistributions, we collect a web-crawled dataset which presents large diversityin terms of weather conditions, sites, lighting, camera styles, etc. We alsopresent a method which injects the style representation of the web-crawled datainto the source domain on-the-fly during training, which enables the network toexperience images of diverse styles with reliable labels for effectivetraining. Moreover, we use the web-crawled dataset with predicted pseudo labelsfor training to further enhance the capability of the network. Extensiveexperiments demonstrate that our method clearly outperforms existing domaingeneralization techniques.", "output": "WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Radiography imaging protocols focus on particular body regions, thereforeproducing images of great similarity and yielding recurrent anatomicalstructures across patients. To exploit this structured information, we proposethe use of Space-aware Memory Queues for In-painting and Detecting anomaliesfrom radiography images (abbreviated as SQUID). We show that SQUID cantaxonomize the ingrained anatomical structures into recurrent patterns; and inthe inference, it can identify anomalies (unseen/modified patterns) in theimage. SQUID surpasses 13 state-of-the-art methods in unsupervised anomalydetection by at least 5 points on two chest X-ray benchmark datasets measuredby the Area Under the Curve (AUC). Additionally, we have created a new dataset(DigitAnatomy), which synthesizes the spatial correlation and consistent shapein chest anatomy. We hope DigitAnatomy can prompt the development, evaluation,and interpretability of anomaly detection methods.", "output": "SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently 3D point cloud learning has been a hot topic in computer vision andautonomous driving. Due to the fact that it is difficult to manually annotate aqualitative large-scale 3D point cloud dataset, unsupervised domain adaptation(UDA) is popular in 3D point cloud learning which aims to transfer the learnedknowledge from the labeled source domain to the unlabeled target domain.However, the generalization and reconstruction errors caused by domain shiftwith simply-learned model are inevitable which substantially hinder the model'scapability from learning good representations. To address these issues, wepropose an end-to-end self-ensembling network (SEN) for 3D point cloud domainadaption tasks. Generally, our SEN resorts to the advantages of Mean Teacherand semi-supervised learning, and introduces a soft classification loss and aconsistency loss, aiming to achieve consistent generalization and accuratereconstruction. In SEN, a student network is kept in a collaborative mannerwith supervised learning and self-supervised learning, and a teacher networkconducts temporal consistency to learn useful representations and ensure thequality of point clouds reconstruction. Extensive experiments on several 3Dpoint cloud UDA benchmarks show that our SEN outperforms the state-of-the-artmethods on both classification and segmentation tasks. Moreover, furtheranalysis demonstrates that our SEN also achieves better reconstruction results.", "output": "Self-Ensemling for 3D Point Cloud Domain Adaption."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point cloud data have been widely explored due to its superior accuracy androbustness under various adverse situations. Meanwhile, deep neural networks(DNNs) have achieved very impressive success in various applications such assurveillance and autonomous driving. The convergence of point cloud and DNNshas led to many deep point cloud models, largely trained under the supervisionof large-scale and densely-labelled point cloud data. Unsupervised point cloudrepresentation learning, which aims to learn general and useful point cloudrepresentations from unlabelled point cloud data, has recently attractedincreasing attention due to the constraint in large-scale point cloudlabelling. This paper provides a comprehensive review of unsupervised pointcloud representation learning using DNNs. It first describes the motivation,general pipelines as well as terminologies of the recent studies. Relevantbackground including widely adopted point cloud datasets and DNN architecturesis then briefly presented. This is followed by an extensive discussion ofexisting unsupervised point cloud representation learning methods according totheir technical approaches. We also quantitatively benchmark and discuss thereviewed methods over multiple widely adopted point cloud datasets. Finally, weshare our humble opinion about several challenges and problems that could bepursued in future research in unsupervised point cloud representation learning.A project associated with this survey has been built athttps://github.com/xiaoaoran/3d_url_survey.", "output": "Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In some scenarios, classifier requires detecting out-of-distribution samplesfar from its training data. With desirable characteristics, reconstructionautoencoder-based methods deal with this problem by using input reconstructionerror as a metric of novelty vs. normality. We formulate the essence of suchapproach as a quadruplet domain translation with an intrinsic bias to onlyquery for a proxy of conditional data uncertainty. Accordingly, an improvementdirection is formalized as maximumly compressing the autoencoder's latent spacewhile ensuring its reconstructive power for acting as a described domaintranslator. From it, strategies are introduced including semanticreconstruction, data certainty decomposition and normalized L2 distance tosubstantially improve original methods, which together establishstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR ofCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our methodworks without any additional data, hard-to-implement structure, time-consumingpipeline, and even harming the classification accuracy of known classes.", "output": "Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical Ultrasound (US) is one of the most widely used imaging modalities inclinical practice. However, its use presents unique challenges such as variableimaging quality. The deep learning (DL) model can be used as an advancedmedical US image analysis tool, while the scarcity of big datasets greatlylimits its performance. To solve the common data shortage, we develop aGenerative Adversarial Network (GAN)-based semi-supervised data augmentationmethod GSDA. The GSDA is composed of the GAN and Convolutional Neural Network(CNN), in which GAN synthesizes and pseudo-labeled the US images with highresolution and high quality, and both real and synthesized images are employedto train CNN. To overcome the training difficulty for GAN and CNN under thesmall data regime, we employ the transfer learning technique for both of them.We also propose a novel evaluation standard to balance the classificationaccuracy and the time consumption. We evaluate our method on the BUSI datasetand GSDA outperforms existing state-of-the-art methods. With high-resolutionand high-quality images synthesized, GSDA obtain a 97.9% accuracy using merely780 images. With the promising results, we believe GSDA can be regarded as apotential auxiliary tool for medical US analysis.", "output": "GSDA: A Generative Adversarial Network-based Semi-Supervised Data Augmentation Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D object recognition has seen significant advances in recent years, showingimpressive performance on real-world 3D scan benchmarks, but lacking in objectpart reasoning, which is fundamental to higher-level scene understanding suchas inter-object similarities or object functionality. Thus, we propose toleverage large-scale synthetic datasets of 3D shapes annotated with partinformation to learn Neural Part Priors (NPPs), optimizable spacescharacterizing geometric part priors. Crucially, we can optimize over thelearned part priors in order to fit to real-world scanned 3D scenes at testtime, enabling robust part decomposition of the real objects in these scenesthat also estimates the complete geometry of the object while fittingaccurately to the observed real geometry. Moreover, this enables globaloptimization over geometrically similar detected objects in a scene, whichoften share strong geometric commonalities, enabling scene-consistent partdecompositions. Experiments on the ScanNet dataset demonstrate that NPPssignificantly outperforms state of the art in part decomposition and objectcompletion in real-world scenes.", "output": "Neural Part Priors: Learning to Optimize Part-Based Object Completion in RGB-D Scans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the last few years, there have been several revolutions in the field ofdeep learning, mainly headlined by the large impact of Generative AdversarialNetworks (GANs). GANs not only provide an unique architecture when definingtheir models, but also generate incredible results which have had a directimpact on society. Due to the significant improvements and new areas ofresearch that GANs have brought, the community is constantly coming up with newresearches that make it almost impossible to keep up with the times. Our surveyaims to provide a general overview of GANs, showing the latest architectures,optimizations of the loss functions, validation metrics and application areasof the most widely recognized variants. The efficiency of the differentvariants of the model architecture will be evaluated, as well as showing thebest application area; as a vital part of the process, the different metricsfor evaluating the performance of GANs and the frequently used loss functionswill be analyzed. The final objective of this survey is to provide a summary ofthe evolution and performance of the GANs which are having better results toguide future researchers in the field.", "output": "A survey on GANs for computer vision: Recent research, analysis and taxonomy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Improving the generalization ability of Deep Neural Networks (DNNs) iscritical for their practical uses, which has been a longstanding challenge.Some theoretical studies have uncovered that DNNs have preferences for somefrequency components in the learning process and indicated that this may affectthe robustness of learned features. In this paper, we propose Deep FrequencyFiltering (DFF) for learning domain-generalizable features, which is the firstendeavour to explicitly modulate the frequency components of different transferdifficulties across domains in the latent space during training. To achievethis, we perform Fast Fourier Transform (FFT) for the feature maps at differentlayers, then adopt a light-weight module to learn attention masks from thefrequency representations after FFT to enhance transferable components whilesuppressing the components not conducive to generalization. Further, weempirically compare the effectiveness of adopting different types of attentiondesigns for implementing DFF. Extensive experiments demonstrate theeffectiveness of our proposed DFF and show that applying our DFF on a plainbaseline outperforms the state-of-the-art methods on different domaingeneralization tasks, including close-set classification and open-setretrieval.", "output": "Deep Frequency Filtering for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present LASER, an image-based Monte Carlo Localization (MCL) framework for2D floor maps. LASER introduces the concept of latent space rendering, where 2Dpose hypotheses on the floor map are directly rendered into ageometrically-structured latent space by aggregating viewing ray features.Through a tightly coupled rendering codebook scheme, the viewing ray featuresare dynamically determined at rendering-time based on their geometries (i.e.length, incident-angle), endowing our representation with view-dependentfine-grain variability. Our codebook scheme effectively disentangles featureencoding from rendering, allowing the latent space rendering to run at speedsabove 10KHz. Moreover, through metric learning, our geometrically-structuredlatent space is common to both pose hypotheses and query images with arbitraryfield of views. As a result, LASER achieves state-of-the-art performance onlarge-scale indoor localization datasets (i.e. ZInD and Structured3D) for bothpanorama and perspective image queries, while significantly outperformingexisting learning-based methods in speed.", "output": "LASER: LAtent SpacE Rendering for 2D Visual Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A light stage uses a series of calibrated cameras and lights to capture asubject's facial appearance under varying illumination and viewpoint. Thiscaptured information is crucial for facial reconstruction and relighting.Unfortunately, light stages are often inaccessible: they are expensive andrequire significant technical expertise for construction and operation. In thispaper, we present SunStage: a lightweight alternative to a light stage thatcaptures comparable data using only a smartphone camera and the sun. Our methodonly requires the user to capture a selfie video outdoors, rotating in place,and uses the varying angles between the sun and the face as guidance in jointreconstruction of facial geometry, reflectance, camera pose, and lightingparameters. Despite the in-the-wild un-calibrated setting, our approach is ableto reconstruct detailed facial appearance and geometry, enabling compellingeffects such as relighting, novel view synthesis, and reflectance editing.Results and interactive demos are available athttps://sunstage.cs.washington.edu/.", "output": "SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study multi-modal few-shot object detection (FSOD) in this paper, usingboth few-shot visual examples and class semantic information for detection,which are complementary to each other by definition. Most of the previous workson multi-modal FSOD are fine-tuning-based which are inefficient for onlineapplications. Moreover, these methods usually require expertise like classnames to extract class semantic embedding, which are hard to get for rareclasses. Our approach is motivated by the high-level conceptual similarity of(metric-based) meta-learning and prompt-based learning to learn generalizablefew-shot and zero-shot object detection models respectively withoutfine-tuning. Specifically, we combine the few-shot visual classifier and textclassifier learned via meta-learning and prompt-based learning respectively tobuild the multi-modal classifier and detection models. In addition, to fullyexploit the pre-trained language models, we propose meta-learning-basedcross-modal prompting to generate soft prompts for novel classes present infew-shot visual examples, which are then used to learn the text classifier.Knowledge distillation is introduced to learn the soft prompt generator withoutusing human prior knowledge of class names, which may not be available for rareclasses. Our insight is that the few-shot support images naturally includerelated context information and semantics of the class. We comprehensivelyevaluate the proposed multi-modal FSOD models on multiple few-shot objectdetection benchmarks, achieving promising results.", "output": "Multi-Modal Few-Shot Object Detection with Meta-Learning-Based Cross-Modal Prompting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most existing deblurring methods focus on removing global blur caused bycamera shake, while they cannot well handle local blur caused by objectmovements. To fill the vacancy of local deblurring in real scenes, we establishthe first real local motion blur dataset (ReLoBlur), which is captured by asynchronized beam-splitting photographing system and corrected by apost-progressing pipeline. Based on ReLoBlur, we propose a Local Blur-AwareGated network (LBAG) and several local blur-aware techniques to bridge the gapbetween global and local deblurring: 1) a blur detection approach based onbackground subtraction to localize blurred regions; 2) a gate mechanism toguide our network to focus on blurred regions; and 3) a blur-aware patchcropping strategy to address data imbalance problem. Extensive experimentsprove the reliability of ReLoBlur dataset, and demonstrate that LBAG achievesbetter performance than state-of-the-art global deblurring methods without ourproposed local blur-aware techniques.", "output": "Real-World Deep Local Motion Deblurring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The regularity of images generated by convolutional neural networks, such asthe U-net, generative networks, or the deep image prior, is analyzed. In aresolution-independent, infinite dimensional setting, it is shown that suchimages, represented as functions, are always continuous and, in somecircumstances, even continuously differentiable, contradicting the widelyaccepted modeling of sharp edges in images via jump discontinuities. While suchstatements require an infinite dimensional setting, the connection to(discretized) neural networks used in practice is made by considering the limitas the resolution approaches infinity. As practical consequence, the results ofthis paper in particular provide analytical evidence that basic L2regularization of network weights might lead to over-smoothed outputs.", "output": "A Note on the Regularity of Images Generated by Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep hashing has been extensively utilized in massive image retrieval becauseof its efficiency and effectiveness. However, deep hashing models arevulnerable to adversarial examples, making it essential to develop adversarialdefense methods for image retrieval. Existing solutions achieved limiteddefense performance because of using weak adversarial samples for training andlacking discriminative optimization objectives to learn robust features. Inthis paper, we present a min-max based Center-guided Adversarial Training,namely CgAT, to improve the robustness of deep hashing networks through worstadversarial examples. Specifically, we first formulate the center code as asemantically-discriminative representative of the input image content, whichpreserves the semantic similarity with positive samples and dissimilarity withnegative examples. We prove that a mathematical formula can calculate thecenter code immediately. After obtaining the center codes in each optimizationiteration of the deep hashing network, they are adopted to guide theadversarial training process. On the one hand, CgAT generates the worstadversarial examples as augmented data by maximizing the Hamming distancebetween the hash codes of the adversarial examples and the center codes. On theother hand, CgAT learns to mitigate the effects of adversarial samples byminimizing the Hamming distance to the center codes. Extensive experiments onthe benchmark datasets demonstrate the effectiveness of our adversarialtraining algorithm in defending against adversarial attacks for deephashing-based retrieval. Compared with the current state-of-the-art defensemethod, we significantly improve the defense performance by an average of18.61\\%, 12.35\\%, and 11.56\\% on FLICKR-25K, NUS-WIDE, and MS-COCO,respectively. The code is available at https://github.com/xunguangwang/CgAT.", "output": "CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose CLIP-Dissect, a new technique to automaticallydescribe the function of individual hidden neurons inside vision networks.CLIP-Dissect leverages recent advances in multimodal vision/language models tolabel internal neurons with open-ended concepts without the need for anylabeled data or human examples. We show that CLIP-Dissect provides moreaccurate descriptions than existing methods for last layer neurons where theground-truth is available as well as qualitatively good descriptions for hiddenlayer neurons. In addition, our method is very flexible: it is model agnostic,can easily handle new concepts and can be extended to take advantage of bettermultimodal models in the future. Finally CLIP-Dissect is computationallyefficient and can label all neurons from five layers of ResNet-50 in just 4minutes, which is more than 10 times faster than existing methods. Our code isavailable at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.", "output": "CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we extend scene understanding to include that of human sketch.The result is a complete trilogy of scene representation from three diverse andcomplementary modalities -- sketch, photo, and text. Instead of learning arigid three-way embedding and be done with it, we focus on learning a flexiblejoint embedding that fully supports the ``optionality\" that thiscomplementarity brings. Our embedding supports optionality on two axes: (i)optionality across modalities -- use any combination of modalities as query fordownstream tasks like retrieval, (ii) optionality across tasks --simultaneously utilising the embedding for either discriminative (e.g.,retrieval) or generative tasks (e.g., captioning). This provides flexibility toend-users by exploiting the best of each modality, therefore serving the verypurpose behind our proposal of a trilogy in the first place. First, acombination of information-bottleneck and conditional invertible neuralnetworks disentangle the modality-specific component from modality-agnostic insketch, photo, and text. Second, the modality-agnostic instances from sketch,photo, and text are synergised using a modified cross-attention. Once learned,we show our embedding can accommodate a multi-facet of scene-related tasks,including those enabled for the first time by the inclusion of sketch, allwithout any task-specific modifications. Project Page:\\url{<a href=\"http://www.pinakinathc.me/scenetrilogy\">this http URL</a>}", "output": "SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo and Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We are concerned with retrieving a query person from multiple videos capturedby a non-overlapping camera network. Existing methods often rely on purelyvisual matching or consider temporal constraints but ignore the spatialinformation of the camera network. To address this issue, we propose apedestrian retrieval framework based on cross-camera trajectory generation,which integrates both temporal and spatial information. To obtain pedestriantrajectories, we propose a novel cross-camera spatio-temporal model thatintegrates pedestrians' walking habits and the path layout between cameras toform a joint probability distribution. Such a spatio-temporal model among acamera network can be specified using sparsely sampled pedestrian data. Basedon the spatio-temporal model, cross-camera trajectories can be extracted by theconditional random field model and further optimized by restricted non-negativematrix factorization. Finally, a trajectory re-ranking technique is proposed toimprove the pedestrian retrieval results. To verify the effectiveness of ourmethod, we construct the first cross-camera pedestrian trajectory dataset, thePerson Trajectory Dataset, in real surveillance scenarios. Extensiveexperiments verify the effectiveness and robustness of the proposed method.", "output": "Cross-Camera Trajectories Help Person Retrieval in a Camera Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point-cloud based 3D object detectors recently have achieved remarkableprogress. However, most studies are limited to the development of networkarchitectures for improving only their accuracy without consideration of thecomputational efficiency. In this paper, we first propose an autoencoder-styleframework comprising channel-wise compression and decompression via interchangetransfer-based knowledge distillation. To learn the map-view feature of ateacher network, the features from teacher and student networks areindependently passed through the shared autoencoder; here, we use a compressedrepresentation loss that binds the channel-wised compression knowledge fromboth student and teacher networks as a kind of regularization. The decompressedfeatures are transferred in opposite directions to reduce the gap in theinterchange reconstructions. Lastly, we present an head attention loss to matchthe 3D object detection information drawn by the multi-head self-attentionmechanism. Through extensive experiments, we verify that our method can trainthe lightweight model that is well-aligned with the 3D point cloud detectiontask and we demonstrate its superiority using the well-known public datasets;e.g., Waymo and nuScenes.", "output": "itKD: Interchange Transfer-based Knowledge Distillation for 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Embodied agents in vision navigation coupled with deep neural networks haveattracted increasing attention. However, deep neural networks have been shownvulnerable to malicious adversarial noises, which may potentially causecatastrophic failures in Embodied Vision Navigation. Among differentadversarial noises, universal adversarial perturbations (UAP), i.e., a constantimage-agnostic perturbation applied on every input frame of the agent, play acritical role in Embodied Vision Navigation since they arecomputation-efficient and application-practical during the attack. However,existing UAP methods ignore the system dynamics of Embodied Vision Navigationand might be sub-optimal. In order to extend UAP to the sequential decisionsetting, we formulate the disturbed environment under the universal noise$\\delta$, as a $\\delta$-disturbed Markov Decision Process ($\\delta$-MDP). Basedon the formulation, we analyze the properties of $\\delta$-MDP and propose twonovel Consistent Attack methods, named Reward UAP and Trajectory UAP, forattacking Embodied agents, which consider the dynamic of the MDP and calculateuniversal noises by estimating the disturbed distribution and the disturbed Qfunction. For various victim models, our Consistent Attack can cause asignificant drop in their performance in the PointGoal task in Habitat withdifferent datasets and different scenes. Extensive experimental resultsindicate that there exist serious potential risks for applying Embodied VisionNavigation methods to the real world.", "output": "Consistent Attack: Universal Adversarial Perturbation on Embodied Vision Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The high computational costs of video super-resolution (VSR) models hindertheir deployment on resource-limited devices, (e.g., smartphones and drones).Existing VSR models contain considerable redundant filters, which drag down theinference efficiency. To prune these unimportant filters, we develop astructured pruning scheme called Structured Sparsity Learning (SSL) accordingto the properties of VSR. In SSL, we design pruning schemes for several keycomponents in VSR models, including residual blocks, recurrent networks, andupsampling networks. Specifically, we develop a Residual Sparsity Connection(RSC) scheme for residual blocks of recurrent networks to liberate pruningrestrictions and preserve the restoration information. For upsampling networks,we design a pixel-shuffle pruning scheme to guarantee the accuracy of featurechannel-space conversion. In addition, we observe that pruning error would beamplified as the hidden states propagate along with recurrent networks. Toalleviate the issue, we design Temporal Finetuning (TF). Extensive experimentsshow that SSL can significantly outperform recent methods quantitatively andqualitatively.", "output": "Structured Sparsity Learning for Efficient Video Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transferring knowledge from task-agnostic pre-trained deep models fordownstream tasks is an important topic in computer vision research. Along withthe growth of computational capacity, we now have open-source vision-languagepre-trained models in large scales of the model architecture and amount ofdata. In this study, we focus on transferring knowledge for videoclassification tasks. Conventional methods randomly initialize the linearclassifier head for vision classification, but they leave the usage of the textencoder for downstream visual recognition tasks undiscovered. In this paper, werevise the role of the linear classifier and replace the classifier with thedifferent knowledge from pre-trained model. We utilize the well-pretrainedlanguage model to generate good semantic target for efficient transferringlearning. The empirical study shows that our method improves both theperformance and the training speed of video classification, with a negligiblechange in the model. Our simple yet effective tuning paradigm achievesstate-of-the-art performance and efficient training on various videorecognition scenarios, i.e., zero-shot, few-shot, general recognition. Inparticular, our paradigm achieves the state-of-the-art accuracy of 87.8% onKinetics-400, and also surpasses previous methods by 20~50% absolute top-1accuracy under zero-shot, few-shot settings on five popular video datasets.Code and models can be found at https://github.com/whwu95/Text4Vis .", "output": "Revisiting Classifier: Transferring Vision-Language Models for Video Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present PVO, a novel panoptic visual odometry framework to achieve morecomprehensive modeling of the scene motion, geometry, and panoptic segmentationinformation. Our PVO models visual odometry (VO) and video panopticsegmentation (VPS) in a unified view, which makes the two tasks mutuallybeneficial. Specifically, we introduce a panoptic update module into the VOModule with the guidance of image panoptic segmentation. This Panoptic-EnhancedVO Module can alleviate the impact of dynamic objects in the camera poseestimation with a panoptic-aware dynamic mask. On the other hand, theVO-Enhanced VPS Module also improves the segmentation accuracy by fusing thepanoptic segmentation result of the current frame on the fly to the adjacentframes, using geometric information such as camera pose, depth, and opticalflow obtained from the VO Module. These two modules contribute to each otherthrough recurrent iterative optimization. Extensive experiments demonstratethat PVO outperforms state-of-the-art methods in both visual odometry and videopanoptic segmentation tasks.", "output": "PVO: Panoptic Visual Odometry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is essential yet challenging for future home-assistant robots tounderstand and manipulate diverse 3D objects in daily human environments.Towards building scalable systems that can perform diverse manipulation tasksover various 3D shapes, recent works have advocated and demonstrated promisingresults learning visual actionable affordance, which labels every point overthe input 3D geometry with an action likelihood of accomplishing the downstreamtask (e.g., pushing or picking-up). However, these works only studiedsingle-gripper manipulation tasks, yet many real-world tasks require two handsto achieve collaboratively. In this work, we propose a novel learningframework, DualAfford, to learn collaborative affordance for dual-grippermanipulation tasks. The core design of the approach is to reduce the quadraticproblem for two grippers into two disentangled yet interconnected subtasks forefficient learning. Using the large-scale PartNet-Mobility and ShapeNetdatasets, we set up four benchmark tasks for dual-gripper manipulation.Experiments prove the effectiveness and superiority of our method over threebaselines.", "output": "DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Manipulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper describes our champion solution for the CVPR2022 Generic EventBoundary Captioning (GEBC) competition. GEBC requires the captioning model tohave a comprehension of instantaneous status changes around the given videoboundary, which makes it much more challenging than conventional videocaptioning task. In this paper, a Dual-Stream Transformer with improvements onboth video content encoding and captions generation is proposed: (1) We utilizethree pre-trained models to extract the video features from differentgranularities. Moreover, we exploit the types of boundary as hints to help themodel generate captions. (2) We particularly design an model, termed asDual-Stream Transformer, to learn discriminative representations for boundarycaptioning. (3) Towards generating content-relevant and human-like captions, weimprove the description quality by designing a word-level ensemble strategy.The promising results on the GEBC test split demonstrate the efficacy of ourproposed model.", "output": "Dual-Stream Transformer for Generic Event Boundary Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diabetic retinopathy (DR) and diabetic macular edema (DME) are leading causesof permanent blindness worldwide. Designing an automatic grading system withgood generalization ability for DR and DME is vital in clinical practice.However, prior works either grade DR or DME independently, without consideringinternal correlations between them, or grade them jointly by shared featurerepresentation, yet ignoring potential generalization issues caused bydifficult samples and data bias. Aiming to address these problems, we propose aframework for joint grading with the dynamic difficulty-aware weighted loss(DAW) and the dual-stream disentangled learning architecture (DETACH). Inspiredby curriculum learning, DAW learns from simple samples to difficult samplesdynamically via measuring difficulty adaptively. DETACH separates features ofgrading tasks to avoid potential emphasis on the bias. With the addition of DAWand DETACH, the model learns robust disentangled feature representations toexplore internal correlations between DR and DME and achieve better gradingperformance. Experiments on three benchmarks show the effectiveness androbustness of our framework under both the intra-dataset and cross-datasettests.", "output": "Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One key challenge of exemplar-guided image generation lies in establishingfine-grained correspondences between input and guided images. Prior approaches,despite the promising results, have relied on either estimating dense attentionto compute per-point matching, which is limited to only coarse scales due tothe quadratic memory cost, or fixing the number of correspondences to achievelinear complexity, which lacks flexibility. In this paper, we propose a dynamicsparse attention based Transformer model, termed Dynamic Sparse Transformer(DynaST), to achieve fine-level matching with favorable efficiency. The heartof our approach is a novel dynamic-attention unit, dedicated to covering thevariation on the optimal number of tokens one position should focus on.Specifically, DynaST leverages the multi-layer nature of Transformer structure,and performs the dynamic attention scheme in a cascaded manner to refinematching results and synthesize visually-pleasing outputs. In addition, weintroduce a unified training objective for DynaST, making it a versatilereference-based image translation framework for both supervised andunsupervised scenarios. Extensive experiments on three applications,pose-guided person image generation, edge-based face synthesis, and undistortedimage style transfer, demonstrate that DynaST achieves superior performance inlocal details, outperforming the state of the art while reducing thecomputational cost significantly. Our code is available athttps://github.com/Huage001/DynaST", "output": "DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the success of fully-supervised human skeleton sequence modeling,utilizing self-supervised pre-training for skeleton sequence representationlearning has been an active field because acquiring task-specific skeletonannotations at large scales is difficult. Recent studies focus on learningvideo-level temporal and discriminative information using contrastive learning,but overlook the hierarchical spatial-temporal nature of human skeletons.Different from such superficial supervision at the video level, we propose aself-supervised hierarchical pre-training scheme incorporated into ahierarchical Transformer-based skeleton sequence encoder (Hi-TRS), toexplicitly capture spatial, short-term, and long-term temporal dependencies atframe, clip, and video levels, respectively. To evaluate the proposedself-supervised pre-training scheme with Hi-TRS, we conduct extensiveexperiments covering three skeleton-based downstream tasks including actionrecognition, action detection, and motion prediction. Under both supervised andsemi-supervised evaluation protocols, our method achieves the state-of-the-artperformance. Additionally, we demonstrate that the prior knowledge learned byour model in the pre-training stage has strong transfer capability fordifferent downstream tasks.", "output": "Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The teacher-free online Knowledge Distillation (KD) aims to train an ensembleof multiple student models collaboratively and distill knowledge from eachother. Although existing online KD methods achieve desirable performance, theyoften focus on class probabilities as the core knowledge type, ignoring thevaluable feature representational information. We present a Mutual ContrastiveLearning (MCL) framework for online KD. The core idea of MCL is to performmutual interaction and transfer of contrastive distributions among a cohort ofnetworks in an online manner. Our MCL can aggregate cross-network embeddinginformation and maximize the lower bound to the mutual information between twonetworks. This enables each network to learn extra contrastive knowledge fromothers, leading to better feature representations, thus improving theperformance of visual recognition tasks. Beyond the final layer, we extend MCLto intermediate layers and perform an adaptive layer-matching mechanism trainedby meta-optimization. Experiments on image classification and transfer learningto visual recognition tasks show that layer-wise MCL can lead to consistentperformance gains against state-of-the-art online KD approaches. Thesuperiority demonstrates that layer-wise MCL can guide the network to generatebetter feature representations. Our code is publicly avaliable athttps://github.com/winycg/L-MCL.", "output": "Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One-to-one set matching is a key design for DETR to establish its end-to-endcapability, so that object detection does not require a hand-crafted NMS(non-maximum suppression) to remove duplicate detections. This end-to-endsignature is important for the versatility of DETR, and it has been generalizedto broader vision tasks. However, we note that there are few queries assignedas positive samples and the one-to-one set matching significantly reduces thetraining efficacy of positive samples. We propose a simple yet effective methodbased on a hybrid matching scheme that combines the original one-to-onematching branch with an auxiliary one-to-many matching branch during training.Our hybrid strategy has been shown to significantly improve accuracy. Ininference, only the original one-to-one match branch is used, thus maintainingthe end-to-end merit and the same inference efficiency of DETR. The method isnamed H-DETR, and it shows that a wide range of representative DETR methods canbe consistently improved across a wide range of visual tasks, includingDeformableDETR, PETRv2, PETR, and TransTrack, among others. The code isavailable at: https://github.com/HDETR", "output": "DETRs with Hybrid Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we revisit the weak-to-strong consistency framework,popularized by FixMatch from semi-supervised classification, where theprediction of a weakly perturbed image serves as supervision for its stronglyperturbed version. Intriguingly, we observe that such a simple pipeline alreadyachieves competitive results against recent advanced works, when transferred toour segmentation scenario. Its success heavily relies on the manual design ofstrong data augmentations, however, which may be limited and inadequate toexplore a broader perturbation space. Motivated by this, we propose anauxiliary feature perturbation stream as a supplement, leading to an expandedperturbation space. On the other, to sufficiently probe original image-levelaugmentations, we present a dual-stream perturbation technique, enabling twostrong views to be simultaneously guided by a common weak view. Consequently,our overall Unified Dual-Stream Perturbations approach (UniMatch) surpasses allexisting methods significantly across all evaluation protocols on the Pascal,Cityscapes, and COCO benchmarks. Its superiority is also demonstrated in remotesensing interpretation and medical image analysis. We hope our reproducedFixMatch and our results can inspire more future works. Code and logs areavailable at https://github.com/LiheYoung/UniMatch.", "output": "Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video denoising aims at removing noise from videos to recover clean ones.Some existing works show that optical flow can help the denoising by exploitingthe additional spatial-temporal clues from nearby frames. However, the flowestimation itself is also sensitive to noise, and can be unusable under largenoise levels. To this end, we propose a new multi-scale refined opticalflow-guided video denoising method, which is more robust to different noiselevels. Our method mainly consists of a denoising-oriented flow refinement(DFR) module and a flow-guided mutual denoising propagation (FMDP) module.Unlike previous works that directly use off-the-shelf flow solutions, DFR firstlearns robust multi-scale optical flows, and FMDP makes use of the flowguidance by progressively introducing and refining more flow information fromlow resolution to high resolution. Together with real noise degradationsynthesis, the proposed multi-scale flow-guided denoising network achievesstate-of-the-art performance on both synthetic Gaussian denoising and realvideo denoising. The codes will be made publicly available.", "output": "Learning Task-Oriented Flows to Mutually Guide Feature Alignment in Synthesized and Real Video Denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent research has shown a strong theoretical connection between variationalautoencoders (VAEs) and the rate-distortion theory. Motivated by this, weconsider the problem of lossy image compression from the perspective ofgenerative modeling. Starting with ResNet VAEs, which are originally designedfor data (image) distribution modeling, we redesign their latent variable modelusing a quantization-aware posterior and prior, enabling easy quantization andentropy coding at test time. Along with improved neural network architecture,we present a powerful and efficient model that outperforms previous methods onnatural image lossy compression. Our model compresses images in acoarse-to-fine fashion and supports parallel encoding and decoding, leading tofast execution on GPUs. Code is available athttps://github.com/duanzhiihao/lossy-vae.", "output": "Lossy Image Compression with Quantized Hierarchical VAEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces Augraphy, a Python library for constructing dataaugmentation pipelines which produce distortions commonly seen in real-worlddocument image datasets. Augraphy stands apart from other data augmentationtools by providing many different strategies to produce augmented versions ofclean document images that appear as if they have been altered by standardoffice operations, such as printing, scanning, and faxing through old or dirtymachines, degradation of ink over time, and handwritten markings. This paperdiscusses the Augraphy tool, and shows how it can be used both as a dataaugmentation tool for producing diverse training data for tasks such asdocument denoising, and also for generating challenging test data to evaluatemodel robustness on document image modeling tasks.", "output": "Augraphy: A Data Augmentation Library for Document Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we present an automated workflow to bring human figures, one ofthe most frequently appearing entities on pictorial maps, to the thirddimension. Our workflow is based on training data and neural networks forsingle-view 3D reconstruction of real humans from photos. We first let anetwork consisting of fully connected layers estimate the depth coordinate of2D pose points. The gained 3D pose points are inputted together with 2D masksof body parts into a deep implicit surface network to infer 3D signed distancefields (SDFs). By assembling all body parts, we derive 2D depth images and bodypart masks of the whole figure for different views, which are fed into a fullyconvolutional network to predict UV images. These UV images and the texture forthe given perspective are inserted into a generative network to inpaint thetextures for the other views. The textures are enhanced by a cartoonizationnetwork and facial details are resynthesized by an autoencoder. Finally, thegenerated textures are assigned to the inferred body parts in a ray marcher. Wetest our workflow with 12 pictorial human figures after having validatedseveral network configurations. The created 3D models look generally promising,especially when considering the challenges of silhouette-based 3D recovery andreal-time rendering of the implicit SDFs. Further improvement is needed toreduce gaps between the body parts and to add pictorial details to thetextures. Overall, the constructed figures may be used for animation andstorytelling in digital 3D maps.", "output": "Inferring Implicit 3D Representations from Human Figures on Pictorial Maps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked autoencoders have become popular training paradigms forself-supervised visual representation learning. These models randomly mask aportion of the input and reconstruct the masked portion according to the targetrepresentations. In this paper, we first show that a careful choice of thetarget representation is unnecessary for learning good representations, sincedifferent targets tend to derive similarly behaved models. Driven by thisobservation, we propose a multi-stage masked distillation pipeline and use arandomly initialized model as the teacher, enabling us to effectively trainhigh-capacity models without any efforts to carefully design targetrepresentations. Interestingly, we further explore using teachers of largercapacity, obtaining distilled students with remarkable transferring ability. Ondifferent tasks of classification, transfer learning, object detection, andsemantic segmentation, the proposed method to perform masked knowledgedistillation with bootstrapped teachers (dBOT) outperforms previousself-supervised methods by nontrivial margins. We hope our findings, as well asthe proposed method, could motivate people to rethink the roles of targetrepresentations in pre-training masked autoencoders.The code and pre-trainedmodels are publicly available at https://github.com/liuxingbin/dbot.", "output": "Exploring Target Representations for Masked Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the most neglected sources of energy loss is streetlights whichgenerate too much light in areas where it is not required. Energy waste hasenormous economic and environmental effects. In addition, due to theconventional manual nature of the operation, streetlights are frequently seenbeing turned ON during the day and OFF in the evening, which is regrettableeven in the twenty-first century. These issues require automated streetlightcontrol in order to be resolved. This study aims to develop a novel streetlightcontrolling method by combining a smart transport monitoring system powered bycomputer vision technology with a closed circuit television (CCTV) camera thatallows the light-emitting diode (LED) streetlight to automatically light upwith the appropriate brightness by detecting the presence of pedestrians orvehicles and dimming the streetlight in their absence using semantic imagesegmentation from the CCTV video streaming. Consequently, our modeldistinguishes daylight and nighttime, which made it feasible to automate theprocess of turning the streetlight 'ON' and 'OFF' to save energy consumptioncosts. According to the aforementioned approach, geolocation sensor data couldbe utilized to make more informed streetlight management decisions. To completethe tasks, we consider training the U-net model with ResNet-34 as its backbone.The validity of the models is guaranteed with the use of assessment matrices.The suggested concept is straightforward, economical, energy-efficient,long-lasting, and more resilient than conventional alternatives.", "output": "CNN based Intelligent Streetlight Management Using Smart CCTV Camera and Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision transformers (ViT) have shown promise in various vision tasks whilethe U-Net based on a convolutional neural network (CNN) remains dominant indiffusion models. We design a simple and general ViT-based architecture (namedU-ViT) for image generation with diffusion models. U-ViT is characterized bytreating all inputs including the time, condition and noisy image patches astokens and employing long skip connections between shallow and deep layers. Weevaluate U-ViT in unconditional and class-conditional image generation, as wellas text-to-image generation tasks, where U-ViT is comparable if not superior toa CNN-based U-Net of a similar size. In particular, latent diffusion modelswith U-ViT achieve record-breaking FID scores of 2.29 in class-conditionalimage generation on ImageNet 256x256, and 5.48 in text-to-image generation onMS-COCO, among methods without accessing large external datasets during thetraining of generative models. Our results suggest that, for diffusion-basedimage modeling, the long skip connection is crucial while the down-sampling andup-sampling operators in CNN-based U-Net are not always necessary. We believethat U-ViT can provide insights for future research on backbones in diffusionmodels and benefit generative modeling on large scale cross-modality datasets.", "output": "All are Worth Words: A ViT Backbone for Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prior work has shown that Visual Recognition datasets frequentlyunderrepresent bias groups $B$ (\\eg Female) within class labels $Y$ (\\egProgrammers). This dataset bias can lead to models that learn spuriouscorrelations between class labels and bias groups such as age, gender, or race.Most recent methods that address this problem require significant architecturalchanges or additional loss functions requiring more hyper-parameter tuning.Alternatively, data sampling baselines from the class imbalance literature (\\egUndersampling, Upweighting), which can often be implemented in a single line ofcode and often have no hyperparameters, offer a cheaper and more efficientsolution. However, these methods suffer from significant shortcomings. Forexample, Undersampling drops a significant part of the input distribution perepoch while Oversampling repeats samples, causing overfitting. To address theseshortcomings, we introduce a new class-conditioned sampling method: BiasMimicking. The method is based on the observation that if a class $c$ biasdistribution, \\ie $P_D(B|Y=c)$ is mimicked across every $c^{\\prime}\\neq c$,then $Y$ and $B$ are statistically independent. Using this notion, BM, througha novel training procedure, ensures that the model is exposed to the entiredistribution per epoch without repeating samples. Consequently, Bias Mimickingimproves underrepresented groups' accuracy of sampling methods by 3\\% over fourbenchmarks while maintaining and sometimes improving performance overnonsampling methods. Code: \\url{https://github.com/mqraitem/Bias-Mimicking}", "output": "Bias Mimicking: A Simple Sampling Approach for Bias Mitigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we present a novel multi-attribute face manipulation methodbased on textual descriptions. Previous text-based image editing methods eitherrequire test-time optimization for each individual image or are restricted tosingle attribute editing. Extending these methods to multi-attribute face imageediting scenarios will introduce undesired excessive attribute change, e.g.,text-relevant attributes are overly manipulated and text-irrelevant attributesare also changed. In order to address these challenges and achieve naturalediting over multiple face attributes, we propose a new decoupling trainingscheme where we use group sampling to get text segments from same attributecategories, instead of whole complex sentences. Further, to preserve otherexisting face attributes, we encourage the model to edit the latent code ofeach attribute separately via an entropy constraint. During the inferencephase, our model is able to edit new face images without any test-timeoptimization, even from complex textual prompts. We show extensive experimentsand analysis to demonstrate the efficacy of our method, which generates naturalmanipulated faces with minimal text-irrelevant attribute editing. Code andpre-trained model are available at https://github.com/hwang1996/ManiCLIP.", "output": "ManiCLIP: Multi-Attribute Face Manipulation from Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to forecast human-environment collisions from egocentricobservations is vital to enable collision avoidance in applications such as VR,AR, and wearable assistive robotics. In this work, we introduce the challengingproblem of predicting collisions in diverse environments from multi-viewegocentric videos captured from body-mounted cameras. Solving this problemrequires a generalizable perception system that can classify which human bodyjoints will collide and estimate a collision region heatmap to localizecollisions in the environment. To achieve this, we propose a transformer-basedmodel called COPILOT to perform collision prediction and localizationsimultaneously, which accumulates information across multi-view inputs througha novel 4D space-time-viewpoint attention mechanism. To train our model andenable future research on this task, we develop a synthetic data generationframework that produces egocentric videos of virtual humans moving andcolliding within diverse 3D environments. This framework is then used toestablish a large-scale dataset consisting of 8.6M egocentric RGBD frames.Extensive experiments show that COPILOT generalizes to unseen synthetic as wellas real-world scenes. We further demonstrate COPILOT outputs are useful fordownstream collision avoidance through simple closed-loop control. Please visitour project webpage at https://sites.google.com/stanford.edu/copilot.", "output": "COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pre-trained vision-language (V-L) models such as CLIP have shown excellentgeneralization ability to downstream tasks. However, they are sensitive to thechoice of input text prompts and require careful selection of prompt templatesto perform well. Inspired by the Natural Language Processing (NLP) literature,recent CLIP adaptation approaches learn prompts as the textual inputs tofine-tune CLIP for downstream tasks. We note that using prompting to adaptrepresentations in a single branch of CLIP (language or vision) is sub-optimalsince it does not allow the flexibility to dynamically adjust bothrepresentation spaces on a downstream task. In this work, we proposeMulti-modal Prompt Learning (MaPLe) for both vision and language branches toimprove alignment between the vision and language representations. Our designpromotes strong coupling between the vision-language prompts to ensure mutualsynergy and discourages learning independent uni-modal solutions. Further, welearn separate prompts across different early stages to progressively model thestage-wise feature relationships to allow rich context learning. We evaluatethe effectiveness of our approach on three representative tasks ofgeneralization to novel classes, new target datasets and unseen domain shifts.Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorableperformance and achieves an absolute gain of 3.45% on novel classes and 2.72%on overall harmonic-mean, averaged over 11 diverse image recognition datasets.Our code and pre-trained models are available athttps://github.com/muzairkhattak/multimodal-prompt-learning.", "output": "MaPLe: Multi-modal Prompt Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal semantic understanding often has to deal with uncertainty, whichmeans the obtained messages tend to refer to multiple targets. Such uncertaintyis problematic for our interpretation, including inter- and intra-modaluncertainty. Little effort has studied the modeling of this uncertainty,particularly in pre-training on unlabeled datasets and fine-tuning intask-specific downstream datasets. In this paper, we project therepresentations of all modalities as probabilistic distributions via aProbability Distribution Encoder (PDE) by utilizing sequence-levelinteractions. Compared to the existing deterministic methods, such uncertaintymodeling can convey richer multimodal semantic information and more complexrelationships. Furthermore, we integrate uncertainty modeling with popularpre-training frameworks and propose suitable pre-training tasks:Distribution-based Vision-Language Contrastive learning (D-VLC),Distribution-based Masked Language Modeling (D-MLM), and Distribution-basedImage-Text Matching (D-ITM). The fine-tuned models are applied to challengingdownstream tasks, including image-text retrieval, visual question answering,visual reasoning, and visual entailment, and achieve state-of-the-art results.", "output": "MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop a Distributionally Robust Optimization (DRO) formulation forMulticlass Logistic Regression (MLR), which could tolerate data contaminated byoutliers. The DRO framework uses a probabilistic ambiguity set defined as aball of distributions that are close to the empirical distribution of thetraining set in the sense of the Wasserstein metric. We relax the DROformulation into a regularized learning problem whose regularizer is a norm ofthe coefficient matrix. We establish out-of-sample performance guarantees forthe solutions to our model, offering insights on the role of the regularizer incontrolling the prediction error. We apply the proposed method in renderingdeep Vision Transformer (ViT)-based image classifiers robust to random andadversarial attacks. Specifically, using the MNIST and CIFAR-10 datasets, wedemonstrate reductions in test error rate by up to 83.5% and loss by up to91.3% compared with baseline methods, by adopting a novel random trainingmethod.", "output": "Distributionally Robust Multiclass Classification and Applications in Deep Image Classifiers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked Autoencoders (MAE) based on a reconstruction task have risen to be apromising paradigm for self-supervised learning (SSL) and achievestate-of-the-art performance across different benchmark datasets. However,despite its impressive empirical success, there is still limited theoreticalunderstanding of it. In this paper, we propose a theoretical understanding ofhow masking matters for MAE to learn meaningful features. We establish a closeconnection between MAE and contrastive learning, which shows that MAE implicitaligns the mask-induced positive pairs. Built upon this connection, we developthe first downstream guarantees for MAE methods, and analyze the effect of maskratio. Besides, as a result of the implicit alignment, we also point out thedimensional collapse issue of MAE, and propose a Uniformity-enhanced MAE(U-MAE) loss that can effectively address this issue and bring significantimprovements on real-world datasets, including CIFAR-10, ImageNet-100, andImageNet-1K. Code is available at (https://github.com/zhangq327/U-MAE).", "output": "How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is expensive to collect training data for every possible domain that avision model may encounter when deployed. We instead consider how simplyverbalizing the training domain (e.g. \"photos of birds\") as well as domains wewant to extend to but do not have data for (e.g. \"paintings of birds\") canimprove robustness. Using a multimodal model with a joint image and languageembedding space, our method LADS learns a transformation of the imageembeddings from the training domain to each unseen test domain, whilepreserving task relevant information. Without using any images from the unseentest domain, we show that over the extended domain containing both training andunseen test domains, LADS outperforms standard fine-tuning and ensembleapproaches over a suite of four benchmarks targeting domain adaptation anddataset bias.", "output": "Using Language to Extend to Unseen Domains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans are excellent at understanding language and vision to accomplish awide range of tasks. In contrast, creating general instruction-followingembodied agents remains a difficult challenge. Prior work that uses purelanguage-only models lack visual grounding, making it difficult to connectlanguage instructions with visual observations. On the other hand, methods thatuse pre-trained multimodal models typically come with divided language andvisual representations, requiring designing specialized network architecture tofuse them together. We propose a simple yet effective model for robots to solveinstruction-following tasks in vision-based environments. Our \\ours methodconsists of a multimodal transformer that encodes visual observations andlanguage instructions, and a transformer-based policy that predicts actionsbased on encoded representations. The multimodal transformer is pre-trained onmillions of image-text pairs and natural language text, thereby producinggeneric cross-modal representations of observations and instructions. Thetransformer-based policy keeps track of the full history of observations andactions, and predicts actions autoregressively. Despite its simplicity, we showthat this unified transformer model outperforms all state-of-the-artpre-trained or trained-from-scratch methods in both single-task and multi-tasksettings. Our model also shows better model scalability and generalizationability than prior work.", "output": "Instruction-Following Agents with Multimodal Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing Visual Object Tracking (VOT) only takes the target area in the firstframe as a template. This causes tracking to inevitably fail in fast-changingand crowded scenes, as it cannot account for changes in object appearancebetween frames. To this end, we revamped the tracking framework withProgressive Context Encoding Transformer Tracker (ProContEXT), which coherentlyexploits spatial and temporal contexts to predict object motion trajectories.Specifically, ProContEXT leverages a context-aware self-attention module toencode the spatial and temporal context, refining and updating the multi-scalestatic and dynamic templates to progressively perform accurate tracking. Itexplores the complementary between spatial and temporal context, raising a newpathway to multi-context modeling for transformer-based trackers. In addition,ProContEXT revised the token pruning technique to reduce computationalcomplexity. Extensive experiments on popular benchmark datasets such as GOT-10kand TrackingNet demonstrate that the proposed ProContEXT achievesstate-of-the-art performance.", "output": "ProContEXT: Exploring Progressive Context Transformer for Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Streaming perception is a fundamental task in autonomous driving thatrequires a careful balance between the latency and accuracy of the autopilotsystem. However, current methods for streaming perception are limited as theyrely only on the current and adjacent two frames to learn movement patterns,which restricts their ability to model complex scenes, often leading to poordetection results. To address this limitation, we propose LongShortNet, a noveldual-path network that captures long-term temporal motion and integrates itwith short-term spatial semantics for real-time perception. Our proposedLongShortNet is notable as it is the first work to extend long-term temporalmodeling to streaming perception, enabling spatiotemporal feature fusion. Weevaluate LongShortNet on the challenging Argoverse-HD dataset and demonstratethat it outperforms existing state-of-the-art methods with almost no additionalcomputational cost.", "output": "LongShortNet: Exploring Temporal and Semantic Features Fusion in Streaming Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D reconstruction of deformable (or non-rigid) scenes from a set of monocular2D image observations is a long-standing and actively researched area ofcomputer vision and graphics. It is an ill-posed inverse problem, since --without additional prior assumptions -- it permits infinitely many solutionsleading to accurate projection to the input 2D images. Non-rigid reconstructionis a foundational building block for downstream applications like robotics,AR/VR, or visual content creation. The key advantage of using monocular camerasis their omnipresence and availability to the end users as well as their easeof use compared to more sophisticated camera set-ups such as stereo ormulti-view systems. This survey focuses on state-of-the-art methods for densenon-rigid 3D reconstruction of various deformable objects and composite scenesfrom monocular videos or sets of monocular views. It reviews the fundamentalsof 3D reconstruction and deformation modeling from 2D image observations. Wethen start from general methods -- that handle arbitrary scenes and make only afew prior assumptions -- and proceed towards techniques making strongerassumptions about the observed objects and types of deformations (e.g. humanfaces, bodies, hands, and animals). A significant part of this STAR is alsodevoted to classification and a high-level comparison of the methods, as wellas an overview of the datasets for training and evaluation of the discussedtechniques. We conclude by discussing open challenges in the field and thesocial aspects associated with the usage of the reviewed methods.", "output": "State of the Art in Dense Monocular Non-Rigid 3D Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image recognition models that work in challenging environments (e.g.,extremely dark, blurry, or high dynamic range conditions) must be useful.However, creating training datasets for such environments is expensive and harddue to the difficulties of data collection and annotation. It is desirable ifwe could get a robust model without the need for hard-to-obtain datasets. Onesimple approach is to apply data augmentation such as color jitter and blur tostandard RGB (sRGB) images in simple scenes. Unfortunately, this approachstruggles to yield realistic images in terms of pixel intensity and noisedistribution due to not considering the non-linearity of Image SignalProcessors (ISPs) and noise characteristics of image sensors. Instead, wepropose a noise-accounted RAW image augmentation method. In essence, colorjitter and blur augmentation are applied to a RAW image before applyingnon-linear ISP, resulting in realistic intensity. Furthermore, we introduce anoise amount alignment method that calibrates the domain gap in the noiseproperty caused by the augmentation. We show that our proposed noise-accountedRAW augmentation method doubles the image recognition accuracy in challengingenvironments only with simple training data.", "output": "Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image Signal Processors (ISPs) play important roles in image recognitiontasks as well as in the perceptual quality of captured images. In most cases,experts make a lot of effort to manually tune many parameters of ISPs, but theparameters are sub-optimal. In the literature, two types of techniques havebeen actively studied: a machine learning-based parameter tuning technique anda DNN-based ISP technique. The former is lightweight but lacks expressivepower. The latter has expressive power, but the computational cost is too heavyon edge devices. To solve these problems, we propose \"DynamicISP,\" whichconsists of multiple classical ISP functions and dynamically controls theparameters of each frame according to the recognition result of the previousframe. We show our method successfully controls the parameters of multiple ISPfunctions and achieves state-of-the-art accuracy with low computational cost insingle and multi-category object detection tasks.", "output": "DynamicISP: Dynamically Controlled Image Signal Processor for Image Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most approaches for semantic segmentation use only information from colorcameras to parse the scenes, yet recent advancements show that using depth dataallows to further improve performances. In this work, we focus ontransformer-based deep learning architectures, that have achievedstate-of-the-art performances on the segmentation task, and we propose toemploy depth information by embedding it in the positional encoding.Effectively, we extend the network to multimodal data without adding anyparameters and in a natural way that makes use of the strength of transformers'self-attention modules. We also investigate the idea of performingcross-modality operations inside the attention module, swapping the key inputsbetween the depth and color branches. Our approach consistently improvesperformances on the Cityscapes benchmark.", "output": "DepthFormer: Multimodal Positional Encodings and Cross-Input Attention for Transformer-Based Segmentation Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For years, researchers have been devoted to generalizable object perceptionand manipulation, where cross-category generalizability is highly desired yetunderexplored. In this work, we propose to learn such cross-category skills viaGeneralizable and Actionable Parts (GAParts). By identifying and defining 9GAPart classes (lids, handles, etc.) in 27 object categories, we construct alarge-scale part-centric interactive dataset, GAPartNet, where we provide rich,part-level annotations (semantics, poses) for 8,489 part instances on 1,166objects. Based on GAPartNet, we investigate three cross-category tasks: partsegmentation, part pose estimation, and part-based object manipulation. Giventhe significant domain gaps between seen and unseen object categories, wepropose a robust 3D segmentation method from the perspective of domaingeneralization by integrating adversarial learning techniques. Our methodoutperforms all existing methods by a large margin, no matter on seen or unseencategories. Furthermore, with part segmentation and pose estimation results, weleverage the GAPart pose definition to design part-based manipulationheuristics that can generalize well to unseen object categories in both thesimulator and the real world. Our dataset, code, and demos are available on ourproject page.", "output": "GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Shadows in videos are difficult to detect because of the large shadowdeformation between frames. In this work, we argue that accounting for shadowdeformation is essential when designing a video shadow detection method. Tothis end, we introduce the shadow deformation attention trajectory (SODA), anew type of video self-attention module, specially designed to handle the largeshadow deformations in videos. Moreover, we present a new shadow contrastivelearning mechanism (SCOTCH) which aims at guiding the network to learn aunified shadow representation from massive positive shadow pairs acrossdifferent videos. We demonstrate empirically the effectiveness of our twocontributions in an ablation study. Furthermore, we show that SCOTCH and SODAsignificantly outperforms existing techniques for video shadow detection. Codeis available at the project page:https://lihaoliu-cambridge.github.io/scotch_and_soda/", "output": "SCOTCH and SODA: A Transformer Video Shadow Detection Framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To acquire a snapshot spectral image, coded aperture snapshot spectralimaging (CASSI) is proposed. A core problem of the CASSI system is to recoverthe reliable and fine underlying 3D spectral cube from the 2D measurement. Byalternately solving a data subproblem and a prior subproblem, deep unfoldingmethods achieve good performance. However, in the data subproblem, the usedsensing matrix is ill-suited for the real degradation process due to the deviceerrors caused by phase aberration, distortion; in the prior subproblem, it isimportant to design a suitable model to jointly exploit both spatial andspectral priors. In this paper, we propose a Residual Degradation LearningUnfolding Framework (RDLUF), which bridges the gap between the sensing matrixand the degradation process. Moreover, a Mix$S^2$ Transformer is designed viamixing priors across spectral and spatial to strengthen the spectral-spatialrepresentation capability. Finally, plugging the Mix$S^2$ Transformer into theRDLUF leads to an end-to-end trainable neural network RDLUF-Mix$S^2$.Experimental results establish the superior performance of the proposed methodover existing ones.", "output": "Residual Degradation Learning Unfolding Framework with Mixing Priors across Spectral and Spatial for Compressive Spectral Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The assumption of a uniform light color distribution, which holds true insingle light color scenes, is no longer applicable in scenes that have multiplelight colors. The spatial variability in multiple light colors causes the colorconstancy problem to be more challenging and requires the extraction of localsurface/light information. Motivated by this, we introduce a multi-tasklearning method to estimate multiple light colors from a single input image. Tohave better cues of the local surface/light colors under multiple light colorconditions, we design a multi-task learning framework with achromatic-pixeldetection and surface-color similarity prediction as our auxiliary tasks. Thesetasks facilitate the acquisition of local light color information and surfacecolor correlations. Moreover, to ensure that our model maintains the constancyof surface colors regardless of the variations of light colors, we alsopreserve local surface color features in our model. We demonstrate that ourmodel achieves 47.1% improvement compared to a state-of-the-artmulti-illuminant color constancy method on a multi-illuminant dataset (LSMI).While single light colors are not our main focus, our model also maintains arobust performance on the single illuminant dataset (NUS-8) and provides 18.5%improvement on the state-of-the-art single color constancy method.", "output": "MIMT: Multi-Illuminant Color Constancy via Multi-Task Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot learning (FSL) is a challenging learning problem in which only a fewsamples are available for each class. Decision interpretation is more importantin few-shot classification since there is a greater chance of error than intraditional classification. However, most of the previous FSL methods areblack-box models. In this paper, we propose an inherently interpretable modelfor FSL based on human-friendly attributes. Moreover, we propose an onlineattribute selection mechanism that can effectively filter out irrelevantattributes in each episode. The attribute selection mechanism improves theaccuracy and helps with interpretability by reducing the number of participatedattributes in each episode. We propose a mechanism that automatically detectsthe episodes where the pool of human-friendly attributes are not adequate, andcompensates by engaging learned unknown attributes. We demonstrate that theproposed method achieves results on par with black-box few-shot-learning modelson four widely used datasets.", "output": "Interpretable Few-shot Learning with Online Attribute Selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "DreamFusion has recently demonstrated the utility of a pre-trainedtext-to-image diffusion model to optimize Neural Radiance Fields (NeRF),achieving remarkable text-to-3D synthesis results. However, the method has twoinherent limitations: (a) extremely slow optimization of NeRF and (b)low-resolution image space supervision on NeRF, leading to low-quality 3Dmodels with a long processing time. In this paper, we address these limitationsby utilizing a two-stage optimization framework. First, we obtain a coarsemodel using a low-resolution diffusion prior and accelerate with a sparse 3Dhash grid structure. Using the coarse representation as the initialization, wefurther optimize a textured 3D mesh model with an efficient differentiablerenderer interacting with a high-resolution latent diffusion model. Our method,dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is2x faster than DreamFusion (reportedly taking 1.5 hours on average), while alsoachieving higher resolution. User studies show 61.7% raters to prefer ourapproach over DreamFusion. Together with the image-conditioned generationcapabilities, we provide users with new ways to control 3D synthesis, openingup new avenues to various creative applications.", "output": "Magic3D: High-Resolution Text-to-3D Content Creation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model-based deep learning has achieved astounding successes due in part tothe availability of large-scale real-world data. However, processing suchmassive amounts of data comes at a considerable cost in terms of computations,storage, training and the search for good neural architectures. Datasetdistillation has thus recently come to the fore. This paradigm involvesdistilling information from large real-world datasets into tiny and compactsynthetic datasets such that processing the latter ideally yields similarperformances as the former. State-of-the-art methods primarily rely on learningthe synthetic dataset by matching the gradients obtained during trainingbetween the real and synthetic data. However, these gradient-matching methodssuffer from the so-called accumulated trajectory error caused by thediscrepancy between the distillation and subsequent evaluation. To mitigate theadverse impact of this accumulated trajectory error, we propose a novelapproach that encourages the optimization algorithm to seek a flat trajectory.We show that the weights trained on synthetic data are robust against theaccumulated errors perturbations with the regularization towards the flattrajectory. Our method, called Flat Trajectory Distillation (FTD), is shown toboost the performance of gradient-matching methods by up to 4.7% on a subset ofimages of the ImageNet dataset with higher resolution images. We also validatethe effectiveness and generalizability of our method with datasets of differentresolutions and demonstrate its applicability to neural architecture search.Code is available at https://github.com/AngusDujw/FTD-distillation.", "output": "Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents an end-to-end neural mapping method for cameralocalization, dubbed NeuMap, encoding a whole scene into a grid of latentcodes, with which a Transformer-based auto-decoder regresses 3D coordinates ofquery pixels. State-of-the-art feature matching methods require each scene tobe stored as a 3D point cloud with per-point features, consuming severalgigabytes of storage per scene. While compression is possible, performancedrops significantly at high compression rates. Conversely, coordinateregression methods achieve high compression by storing scene information in aneural network but suffer from reduced robustness. NeuMap combines theadvantages of both approaches by utilizing 1) learnable latent codes forefficient scene representation and 2) a scene-agnostic Transformer-basedauto-decoder to infer coordinates for query pixels. This scene-agnostic networkdesign learns robust matching priors from large-scale data and enables rapidoptimization of codes for new scenes while keeping the network weights fixed.Extensive evaluations on five benchmarks show that NeuMap significantlyoutperforms other coordinate regression methods and achieves comparableperformance to feature matching methods while requiring a much smaller scenerepresentation size. For example, NeuMap achieves 39.1% accuracy in the Aachennight benchmark with only 6MB of data, whereas alternative methods require100MB or several gigabytes and fail completely under high compression settings.The codes are available at https://github.com/Tangshitao/NeuMap", "output": "NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "GAN inversion and editing via StyleGAN maps an input image into the embeddingspaces ($\\mathcal{W}$, $\\mathcal{W^+}$, and $\\mathcal{F}$) to simultaneouslymaintain image fidelity and meaningful manipulation. From latent space$\\mathcal{W}$ to extended latent space $\\mathcal{W^+}$ to feature space$\\mathcal{F}$ in StyleGAN, the editability of GAN inversion decreases while itsreconstruction quality increases. Recent GAN inversion methods typicallyexplore $\\mathcal{W^+}$ and $\\mathcal{F}$ rather than $\\mathcal{W}$ to improvereconstruction fidelity while maintaining editability. As $\\mathcal{W^+}$ and$\\mathcal{F}$ are derived from $\\mathcal{W}$ that is essentially the foundationlatent space of StyleGAN, these GAN inversion methods focusing on$\\mathcal{W^+}$ and $\\mathcal{F}$ spaces could be improved by stepping back to$\\mathcal{W}$. In this work, we propose to first obtain the precise latent codein foundation latent space $\\mathcal{W}$. We introduce contrastive learning toalign $\\mathcal{W}$ and the image space for precise latent code discovery. %Theobtaining process is by using contrastive learning to align $\\mathcal{W}$ andthe image space. Then, we leverage a cross-attention encoder to transform theobtained latent code in $\\mathcal{W}$ into $\\mathcal{W^+}$ and $\\mathcal{F}$,accordingly. Our experiments show that our exploration of the foundation latentspace $\\mathcal{W}$ improves the representation ability of latent codes in$\\mathcal{W^+}$ and features in $\\mathcal{F}$, which yields state-of-the-artreconstruction fidelity and editability results on the standard benchmarks.Project page: https://kumapowerliu.github.io/CLCAE.", "output": "Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We revisit and advance visual prompting (VP), an input prompting techniquefor vision tasks. VP can reprogram a fixed, pre-trained source model toaccomplish downstream tasks in the target domain by simply incorporatinguniversal prompts (in terms of input perturbation patterns) into downstreamdata points. Yet, it remains elusive why VP stays effective even given aruleless label mapping (LM) between the source classes and the target classes.Inspired by the above, we ask: How is LM interrelated with VP? And how toexploit such a relationship to improve its accuracy on target tasks? We peerinto the influence of LM on VP and provide an affirmative answer that a better'quality' of LM (assessed by mapping precision and explanation) canconsistently improve the effectiveness of VP. This is in contrast to the priorart where the factor of LM was missing. To optimize LM, we propose a new VPframework, termed ILM-VP (iterative label mapping-based visual prompting),which automatically re-maps the source labels to the target labels andprogressively improves the target task accuracy of VP. Further, when using acontrastive language-image pretrained (CLIP) model, we propose to integrate anLM process to assist the text prompt selection of CLIP and to improve thetarget task accuracy. Extensive experiments demonstrate that our proposalsignificantly outperforms state-of-the-art VP methods. As highlighted below, weshow that when reprogramming an ImageNet-pretrained ResNet-18 to 13 targettasks, our method outperforms baselines by a substantial margin, e.g., 7.9% and6.7% accuracy improvements in transfer learning to the target Flowers102 andCIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and7.1% accuracy improvements on Flowers102 and DTD respectively. Our code isavailable at https://github.com/OPTML-Group/ILM-VP.", "output": "Understanding and Improving Visual Prompting: A Label-Mapping Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents the first significant object detection framework,NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploitinga novel voxel representation that incorporates multi-scale 3D neural volumetricfeatures, we demonstrate it is possible to regress the 3D bounding boxes ofobjects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPNis a general framework and can be applied to detect objects without classlabels. We experimented NeRF-RPN with various backbone architectures, RPN headdesigns and loss functions. All of them can be trained in an end-to-end mannerto estimate high quality 3D bounding boxes. To facilitate future research inobject detection for NeRF, we built a new benchmark dataset which consists ofboth synthetic and real-world data with careful labeling and clean up. Code anddataset are available at https://github.com/lyclyc52/NeRF_RPN.", "output": "NeRF-RPN: A general framework for object detection in NeRFs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a neural incremental Structure-from-Motion (SfM)approach, Level-S$^2$fM, which estimates the camera poses and scene geometryfrom a set of uncalibrated images by learning coordinate MLPs for the implicitsurfaces and the radiance fields from the established keypoint correspondences.Our novel formulation poses some new challenges due to inevitable two-view andfew-view configurations in the incremental SfM pipeline, which complicates theoptimization of coordinate MLPs for volumetric neural rendering with unknowncamera poses. Nevertheless, we demonstrate that the strong inductive basisconveying in the 2D correspondences is promising to tackle those challenges byexploiting the relationship between the ray sampling schemes. Based on this, werevisit the pipeline of incremental SfM and renew the key components, includingtwo-view geometry initialization, the camera poses registration, the 3D pointstriangulation, and Bundle Adjustment, with a fresh perspective based on neuralimplicit surfaces. By unifying the scene geometry in small MLP networks throughcoordinate MLPs, our Level-S$^2$fM treats the zero-level set of the implicitsurface as an informative top-down regularization to manage the reconstructed3D points, reject the outliers in correspondences via querying SDF, and refinethe estimated geometries by NBA (Neural BA). Not only does our Level-S$^2$fMlead to promising results on camera pose estimation and scene geometryreconstruction, but it also shows a promising way for neural implicit renderingwithout knowing camera extrinsic beforehand.", "output": "Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale commonsense knowledge bases empower a broad range of AIapplications, where the automatic extraction of commonsense knowledge (CKE) isa fundamental and challenging problem. CKE from text is known for sufferingfrom the inherent sparsity and reporting bias of commonsense in text. Visualperception, on the other hand, contains rich commonsense knowledge aboutreal-world entities, e.g., (person, can_hold, bottle), which can serve aspromising sources for acquiring grounded commonsense knowledge. In this work,we present CLEVER, which formulates CKE as a distantly supervisedmulti-instance learning problem, where models learn to summarize commonsenserelations from a bag of images about an entity pair without any humanannotation on image instances. To address the problem, CLEVER leveragesvision-language pre-training models for deep understanding of each image in thebag, and selects informative instances from the bag to summarize commonsenseentity relations via a novel contrastive attention mechanism. Comprehensiveexperimental results in held-out and human evaluation show that CLEVER canextract commonsense knowledge in promising quality, outperforming pre-trainedlanguage model-based methods by 3.9 AUC and 6.4 mAUC points. The predictedcommonsense scores show strong correlation with human judgment with a 0.78Spearman coefficient. Moreover, the extracted commonsense can also be groundedinto images with reasonable interpretability. The data and codes can beobtained at https://github.com/thunlp/CLEVER.", "output": "Visually Grounded Commonsense Knowledge Acquisition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Fields (NeRF) have attracted significant attention due totheir ability to synthesize novel scene views with great accuracy. However,inherent to their underlying formulation, the sampling of points along a raywith zero width may result in ambiguous representations that lead to furtherrendering artifacts such as aliasing in the final scene. To address this issue,the recent variant mip-NeRF proposes an Integrated Positional Encoding (IPE)based on a conical view frustum. Although this is expressed with an integralformulation, mip-NeRF instead approximates this integral as the expected valueof a multivariate Gaussian distribution. This approximation is reliable forshort frustums but degrades with highly elongated regions, which arises whendealing with distant scene objects under a larger depth of field. In thispaper, we explore the use of an exact approach for calculating the IPE by usinga pyramid-based integral formulation instead of an approximated conical-basedone. We denote this formulation as Exact-NeRF and contribute the first approachto offer a precise analytical solution to the IPE within the NeRF domain. Ourexploratory work illustrates that such an exact formulation Exact-NeRF matchesthe accuracy of mip-NeRF and furthermore provides a natural extension to morechallenging scenarios without further modification, such as in the case ofunbounded scenes. Our contribution aims to both address the hitherto unexploredissues of frustum approximation in earlier NeRF work and additionally provideinsight into the potential future consideration of analytical solutions infuture NeRF extensions.", "output": "Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-supervised semantic segmentation focuses on the exploration of a smallamount of labeled data and a large amount of unlabeled data, which is more inline with the demands of real-world image understanding applications. However,it is still hindered by the inability to fully and effectively leverageunlabeled images. In this paper, we reveal that cross-window consistency (CWC)is helpful in comprehensively extracting auxiliary supervision from unlabeleddata. Additionally, we propose a novel CWC-driven progressive learningframework to optimize the deep network by mining weak-to-strong constraintsfrom massive unlabeled data. More specifically, this paper presents a biasedcross-window consistency (BCC) loss with an importance factor, which helps thedeep network explicitly constrain confidence maps from overlapping regions indifferent windows to maintain semantic consistency with larger contexts. Inaddition, we propose a dynamic pseudo-label memory bank (DPM) to providehigh-consistency and high-reliability pseudo-labels to further optimize thenetwork. Extensive experiments on three representative datasets of urban views,medical scenarios, and satellite scenes demonstrate our framework consistentlyoutperforms the state-of-the-art methods with a large margin. Code will beavailable publicly.", "output": "Progressive Learning with Cross-Window Consistency for Semi-Supervised Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Basis Restricted Elastic Shape Analysis (BaRe-ESA), a novelRiemannian framework for human body scan representation, interpolation andextrapolation. BaRe-ESA operates directly on unregistered meshes, i.e., withoutthe need to establish prior point to point correspondences or to assume aconsistent mesh structure. Our method relies on a latent space representation,which is equipped with a Riemannian (non-Euclidean) metric associated to aninvariant higher-order metric on the space of surfaces. Experimental results onthe FAUST and DFAUST datasets show that BaRe-ESA brings significantimprovements with respect to previous solutions in terms of shape registration,interpolation and extrapolation. The efficiency and strength of our model isfurther demonstrated in applications such as motion transfer and randomgeneration of body shape and pose.", "output": "BaRe-ESA: A Riemannian Framework for Unregistered Human Body Shapes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The well-documented presence of texture bias in modern convolutional neuralnetworks has led to a plethora of algorithms that promote an emphasis on shapecues, often to support generalization to new domains. Yet, common datasets,benchmarks and general model selection strategies are missing, and there is noagreed, rigorous evaluation protocol. In this paper, we investigatedifficulties and limitations when training networks with reduced texture bias.In particular, we also show that proper evaluation and meaningful comparisonsbetween methods are not trivial. We introduce BiasBed, a testbed for texture-and style-biased training, including multiple datasets and a range of existingalgorithms. It comes with an extensive evaluation protocol that includesrigorous hypothesis testing to gauge the significance of the results, despitethe considerable training instability of some style bias methods. Our extensiveexperiments, shed new light on the need for careful, statistically foundedevaluation protocols for style bias (and beyond). E.g., we find that somealgorithms proposed in the literature do not significantly mitigate the impactof style bias at all. With the release of BiasBed, we hope to foster a commonunderstanding of consistent and meaningful comparisons, and consequently fasterprogress towards learning methods free of texture bias. Code is available athttps://github.com/D1noFuzi/BiasBed", "output": "BiasBed -- Rigorous Texture Bias Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-modal alignment is essential for vision-language pre-training (VLP)models to learn the correct corresponding information across differentmodalities. For this purpose, inspired by the success of masked languagemodeling (MLM) tasks in the NLP pre-training area, numerous masked modelingtasks have been proposed for VLP to further promote cross-modal interactions.The core idea of previous masked modeling tasks is to focus on reconstructingthe masked tokens based on visible context for learning local-to-localalignment. However, most of them pay little attention to the global semanticfeatures generated for the masked data, resulting in a limited cross-modalalignment ability of global representations. Therefore, in this paper, wepropose a novel Semantic Completion Learning (SCL) task, complementary toexisting masked modeling tasks, to facilitate global-to-local alignment.Specifically, the SCL task complements the missing semantics of masked data bycapturing the corresponding information from the other modality, promotinglearning more representative global features which have a great impact on theperformance of downstream tasks. Moreover, we present a flexible visionencoder, which enables our model to perform image-text and video-textmultimodal tasks simultaneously. Experimental results show that our proposedmethod obtains state-of-the-art performance on various vision-languagebenchmarks, such as visual question answering, image-text retrieval, andvideo-text retrieval.", "output": "Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object trackers deployed on low-power devices need to be light-weight,however, most of the current state-of-the-art (SOTA) methods rely on usingcompute-heavy backbones built using CNNs or transformers. Large sizes of suchmodels do not allow their deployment in low-power conditions and designingcompressed variants of large tracking models is of great importance. This paperdemonstrates how highly compressed light-weight object trackers can be designedusing neural architectural pruning of large CNN and transformer based trackers.Further, a comparative study on architectural choices best suited to designlight-weight trackers is provided. A comparison between SOTA trackers usingCNNs, transformers as well as the combination of the two is presented to studytheir stability at various compression ratios. Finally results for extremepruning scenarios going as low as 1% in some cases are shown to study thelimits of network pruning in object tracking. This work provides deeperinsights into designing highly efficient trackers from existing SOTA methods.", "output": "On Designing Light-Weight Object Trackers through Network Pruning: Use CNNs or Transformers?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot (FS) and zero-shot (ZS) learning are two different approaches forscaling temporal action detection (TAD) to new classes. The former adapts apretrained vision model to a new task represented by as few as a single videoper class, whilst the latter requires no training examples by exploiting asemantic description of the new class. In this work, we introduce a newmulti-modality few-shot (MMFS) TAD problem, which can be considered as amarriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and newclass names jointly. To tackle this problem, we further introduce a novelMUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled byefficiently bridging pretrained vision and language models whilst maximallyreusing already learned capacity. Concretely, we construct multi-modal promptsby mapping support videos into the textual token space of a vision-languagemodel using a meta-learned adapter-equipped visual semantics tokenizer. Totackle large intra-class variation, we further design a query featureregulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14demonstrate that our MUPPET outperforms state-of-the-art alternative methods,often by a large margin. We also show that our MUPPET can be easily extended totackle the few-shot object detection problem and again achieves thestate-of-the-art performance on MS-COCO dataset. The code will be available inhttps://github.com/sauradip/MUPPET", "output": "Multi-Modal Few-Shot Temporal Action Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a lightweight network to improve descriptors of keypoints withinthe same image. The network takes the original descriptors and the geometricproperties of keypoints as the input, and uses an MLP-based self-boosting stageand a Transformer-based cross-boosting stage to enhance the descriptors. Theboosted descriptors can be either real-valued or binary ones. We use theproposed network to boost both hand-crafted (ORB, SIFT) and thestate-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluatethem on image matching, visual localization, and structure-from-motion tasks.The results show that our method significantly improves the performance of eachtask, particularly in challenging cases such as large illumination changes orrepetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms onembedded GPU to process 2000 features, which is fast enough to be applied to apractical system. The code and trained weights are publicly available atgithub.com/SJTU-ViSYS/FeatureBooster.", "output": "FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "According to the Global Electricity Review 2022, worldwide renewable energygeneration has increased by 20\\% primarily due to the installation of largerenewable energy power plants. However, monitoring renewable energy assets inthese large plants remains challenging due to environmental factors that canresult in reduced power generation, malfunctioning, and degradation of assetlife. Therefore, the detection of surface defects on renewable energy assets iscrucial for maintaining the performance and efficiency of these plants. Thispaper proposes an innovative detection framework to achieve an economicalsurface monitoring system for renewable energy assets. High-resolution imagesof the assets are captured regularly and inspected to identify surface orstructural damages on solar panels and wind turbine blades. We use the VisionTransformer (ViT), one of the latest attention-based deep learning (DL) modelsin computer vision, to classify surface defects. The ViT model outperformedother DL models, including MobileNet, VGG16, Xception, EfficientNetB7, andResNet50, achieving high accuracy scores above 97% for both wind and solarplant assets. From the results, our proposed model demonstrates its potentialfor monitoring and detecting damages in renewable energy assets for efficientand reliable operation of renewable power plants.", "output": "Identification of Surface Defects on Solar PV Panels and Wind Turbine Blades using Attention based Deep Learning Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Corgi, a novel method for text-to-image generation. Corgi is basedon our proposed shifted diffusion model, which achieves better image embeddinggeneration from input text. Unlike the baseline diffusion model used in DALL-E2, our method seamlessly encodes prior knowledge of the pre-trained CLIP modelin its diffusion process by designing a new initialization distribution and anew transition step of the diffusion. Compared to the strong DALL-E 2 baseline,our method performs better in generating image embedding from the text in termsof both efficiency and effectiveness, resulting in better text-to-imagegeneration. Extensive large-scale experiments are conducted and evaluated interms of both quantitative measures and human evaluation, indicating a strongergeneration ability of our method compared to existing ones. Furthermore, ourmodel enables semi-supervised and language-free training for text-to-imagegeneration, where only part or none of the images in the training dataset havean associated caption. Trained with only 1.7% of the images being captioned,our semi-supervised model obtains FID results comparable to DALL-E 2 onzero-shot text-to-image generation evaluated on MS-COCO. Corgi also achievesnew state-of-the-art results across different datasets on downstreamlanguage-free text-to-image generation tasks, outperforming the previousmethod, Lafite, by a large margin.", "output": "Shifted Diffusion for Text-to-image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable AI (XAI) is slowly becoming a key component for many AIapplications. Rule-based and modified backpropagation XAI approaches howeveroften face challenges when being applied to modern model architecturesincluding innovative layer building blocks, which is caused by two reasons.Firstly, the high flexibility of rule-based XAI methods leads to numerouspotential parameterizations. Secondly, many XAI methods break theimplementation-invariance axiom because they struggle with certain modelcomponents, e.g., BatchNorm layers. The latter can be addressed with modelcanonization, which is the process of re-structuring the model to disregardproblematic components without changing the underlying function. While modelcanonization is straightforward for simple architectures (e.g., VGG, ResNet),it can be challenging for more complex and highly interconnected models (e.g.,DenseNet). Moreover, there is only little quantifiable evidence that modelcanonization is beneficial for XAI. In this work, we propose canonizations forcurrently relevant model blocks applicable to popular deep neural networkarchitectures,including VGG, ResNet, EfficientNet, DenseNets, as well asRelation Networks. We further suggest a XAI evaluation framework with which wequantify and compare the effect sof model canonization for various XAI methodsin image classification tasks on the Pascal-VOC and ILSVRC2017 datasets, aswell as for Visual Question Answering using CLEVR-XAI. Moreover, addressing theformer issue outlined above, we demonstrate how our evaluation framework can beapplied to perform hyperparameter search for XAI methods to optimize thequality of explanations.", "output": "Optimizing Explanations by Network Canonization and Hyperparameter Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We tackle open-world semantic segmentation, which aims at learning to segmentarbitrary visual concepts in images, by using only image-text pairs withoutdense annotations. Existing open-world segmentation methods have shownimpressive advances by employing contrastive learning (CL) to learn diversevisual concepts and transferring the learned image-level understanding to thesegmentation task. However, these CL-based methods suffer from a train-testdiscrepancy, since it only considers image-text alignment during training,whereas segmentation requires region-text alignment during testing. In thispaper, we proposed a novel Text-grounded Contrastive Learning (TCL) frameworkthat enables a model to directly learn region-text alignment. Our methodgenerates a segmentation mask for a given text, extracts text-grounded imageembedding from the masked region, and aligns it with text embedding via TCL. Bylearning region-text alignment directly, our framework encourages a model todirectly improve the quality of generated segmentation masks. In addition, fora rigorous and fair comparison, we present a unified evaluation protocol withwidely used 8 semantic segmentation datasets. TCL achieves state-of-the-artzero-shot segmentation performances with large margins in all datasets. Code isavailable at https://github.com/kakaobrain/tcl.", "output": "Learning to Generate Text-grounded Mask for Open-world Semantic Segmentation from Only Image-Text Pairs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce DiffRF, a novel approach for 3D radiance field synthesis basedon denoising diffusion probabilistic models. While existing diffusion-basedmethods operate on images, latent codes, or point cloud data, we are the firstto directly generate volumetric radiance fields. To this end, we propose a 3Ddenoising model which directly operates on an explicit voxel gridrepresentation. However, as radiance fields generated from a set of posedimages can be ambiguous and contain artifacts, obtaining ground truth radiancefield samples is non-trivial. We address this challenge by pairing thedenoising formulation with a rendering loss, enabling our model to learn adeviated prior that favours good image quality instead of trying to replicatefitting errors like floating artifacts. In contrast to 2D-diffusion models, ourmodel learns multi-view consistent priors, enabling free-view synthesis andaccurate shape generation. Compared to 3D GANs, our diffusion-based approachnaturally enables conditional generation such as masked completion orsingle-view 3D synthesis at inference time.", "output": "DiffRF: Rendering-Guided 3D Radiance Field Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present ObjectMatch, a semantic and object-centric camera pose estimatorfor RGB-D SLAM pipelines. Modern camera pose estimators rely on directcorrespondences of overlapping regions between frames; however, they cannotalign camera frames with little or no overlap. In this work, we propose toleverage indirect correspondences obtained via semantic object identification.For instance, when an object is seen from the front in one frame and from theback in another frame, we can provide additional pose constraints throughcanonical object correspondences. We first propose a neural network to predictsuch correspondences on a per-pixel level, which we then combine in our energyformulation with state-of-the-art keypoint matching solved with a jointGauss-Newton optimization. In a pairwise setting, our method improvesregistration recall of state-of-the-art feature matching, including from 24% to45% in pairs with 10% or less inter-frame overlap. In registering RGB-Dsequences, our method outperforms cutting-edge SLAM baselines in challenging,low-frame-rate scenarios, achieving more than 35% reduction in trajectory errorin multiple scenes.", "output": "ObjectMatch: Robust Registration using Canonical Object Correspondences."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current 3D scene stylization methods transfer textures and colors as stylesusing arbitrary style references, lacking meaningful semantic correspondences.We introduce Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR) toaddress this limitation. This controllable method stylizes a 3D scene usingradiance fields with a single stylized 2D view as a reference. We propose a rayregistration process based on the stylized reference view to obtain pseudo-raysupervision in novel views. Then we exploit semantic correspondences in contentimages to fill occluded regions with perceptually similar styles, resulting innon-photorealistic and continuous novel view sequences. Our experimentalresults demonstrate that Ref-NPR outperforms existing scene and videostylization methods regarding visual quality and semantic correspondence. Thecode and data are publicly available on the project page athttps://ref-npr.github.io.", "output": "Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inspired by the impressive performance of recent face image editing methods,several studies have been naturally proposed to extend these methods to theface video editing task. One of the main challenges here is temporalconsistency among edited frames, which is still unresolved. To this end, wepropose a novel face video editing framework based on diffusion autoencodersthat can successfully extract the decomposed features - for the first time as aface video editing model - of identity and motion from a given video. Thismodeling allows us to edit the video by simply manipulating the temporallyinvariant feature to the desired direction for the consistency. Another uniquestrength of our model is that, since our model is based on diffusion models, itcan satisfy both reconstruction and edit capabilities at the same time, and isrobust to corner cases in wild face videos (e.g. occluded faces) unlike theexisting GAN-based methods.", "output": "Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale multi-modal training with image-text pairs imparts stronggeneralization to CLIP model. Since training on a similar scale for videos isinfeasible, recent approaches focus on the effective transfer of image-basedCLIP to the video domain. In this pursuit, new parametric modules are added tolearn temporal information and inter-frame relationships which requiremeticulous design efforts. Furthermore, when the resulting models are learnedon videos, they tend to overfit on the given task distribution and lack ingeneralization aspect. This begs the following question: How to effectivelytransfer image-level CLIP representations to videos? In this work, we show thata simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient tobridge the domain gap from images to videos. Our qualitative analysisillustrates that the frame-level processing from CLIP image-encoder followed byfeature pooling and similarity matching with corresponding text embeddingshelps in implicitly modeling the temporal cues within ViFi-CLIP. Suchfine-tuning helps the model to focus on scene dynamics, moving objects andinter-object relationships. For low-data regimes where full fine-tuning is notviable, we propose a `bridge and prompt' approach that first uses fine-tuningto bridge the domain gap and then learns prompts on language and vision side toadapt CLIP representations. We extensively evaluate this simple yet strongbaseline on zero-shot, base-to-novel generalization, few-shot and fullysupervised settings across five video benchmarks. Our code is available athttps://github.com/muzairkhattak/ViFi-CLIP.", "output": "Fine-tuned CLIP Models are Efficient Video Learners."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although existing semi-supervised learning models achieve remarkable successin learning with unannotated in-distribution data, they mostly fail to learn onunlabeled data sampled from novel semantic classes due to their closed-setassumption. In this work, we target a pragmatic but under-explored GeneralizedNovel Category Discovery (GNCD) setting. The GNCD setting aims to categorizeunlabeled training data coming from known and novel classes by leveraging theinformation of partially labeled known classes. We propose a two-stageContrastive Affinity Learning method with auxiliary visual Prompts, dubbedPromptCAL, to address this challenging problem. Our approach discovers reliablepairwise sample affinities to learn better semantic clustering of both knownand novel classes for the class token and visual prompts. First, we propose adiscriminative prompt regularization loss to reinforce semanticdiscriminativeness of prompt-adapted pre-trained vision transformer for refinedaffinity relationships.Besides, we propose contrastive affinity learning tocalibrate semantic representations based on our iterative semi-supervisedaffinity graph generation method for semantically-enhanced supervision.Extensive experimental evaluation demonstrates that our PromptCAL method ismore effective in discovering novel classes even with limited annotations andsurpasses the current state-of-the-art on generic and fine-grained benchmarks(e.g., with nearly 11% gain on CUB-200, and 9% on ImageNet-100) on overallaccuracy. Our code is available at https://github.com/sheng-eatamath/PromptCAL.", "output": "PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion-based generative models have achieved remarkable success in imagegeneration. Their guidance formulation allows an external model toplug-and-play control the generation process for various tasks withoutfinetuning the diffusion model. However, the direct use of publicly availableoff-the-shelf models for guidance fails due to their poor performance on noisyinputs. For that, the existing practice is to fine-tune the guidance modelswith labeled data corrupted with noises. In this paper, we argue that thispractice has limitations in two aspects: (1) performing on inputs withextremely various noises is too hard for a single guidance model; (2)collecting labeled datasets hinders scaling up for various tasks. To tackle thelimitations, we propose a novel strategy that leverages multiple experts whereeach expert is specialized in a particular noise range and guides the reverseprocess of the diffusion at its corresponding timesteps. However, as it isinfeasible to manage multiple networks and utilize labeled data, we present apractical guidance framework termed Practical Plug-And-Play (PPAP), whichleverages parameter-efficient fine-tuning and data-free knowledge transfer. Weexhaustively conduct ImageNet class conditional generation experiments to showthat our method can successfully guide diffusion with small trainableparameters and no labeled data. Finally, we show that image classifiers, depthestimators, and semantic segmentation models can guide publicly available GLIDEthrough our framework in a plug-and-play manner. Our code is available athttps://github.com/riiid/PPAP.", "output": "Towards Practical Plug-and-Play Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a neural technique for learning to select a local sub-regionaround a point which can be used for mesh parameterization. The motivation forour framework is driven by interactive workflows used for decaling, texturing,or painting on surfaces. Our key idea is to incorporate segmentationprobabilities as weights of a classical parameterization method, implemented asa novel differentiable parameterization layer within a neural networkframework. We train a segmentation network to select 3D regions that areparameterized into 2D and penalized by the resulting distortion, giving rise tosegmentations which are distortion-aware. Following training, a user can useour system to interactively select a point on the mesh and obtain a large,meaningful region around the selection which induces a low-distortionparameterization. Our code and project page are currently available.", "output": "DA Wand: Distortion-Aware Selection using Neural Mesh Parameterization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Post-training quantization (PTQ) is a neural network compression techniquethat converts a full-precision model into a quantized model usinglower-precision data types. Although it can help reduce the size andcomputational cost of deep neural networks, it can also introduce quantizationnoise and reduce prediction accuracy, especially in extremely low-bit settings.How to determine the appropriate quantization parameters (e.g., scaling factorsand rounding of weights) is the main problem facing now. Existing methodsattempt to determine these parameters by minimize the distance between featuresbefore and after quantization, but such an approach only considers localinformation and may not result in the most optimal quantization parameters. Weanalyze this issue and ropose PD-Quant, a method that addresses this limitationby considering global information. It determines the quantization parameters byusing the information of differences between network prediction before andafter quantization. In addition, PD-Quant can alleviate the overfitting problemin PTQ caused by the small number of calibration sets by adjusting thedistribution of activations. Experiments show that PD-Quant leads to betterquantization parameters and improves the prediction accuracy of quantizedmodels, especially in low-bit settings. For example, PD-Quant pushes theaccuracy of ResNet-18 up to 53.14% and RegNetX-600MF up to 40.67% in weight2-bit activation 2-bit. The code is released athttps://github.com/hustvl/PD-Quant.", "output": "PD-Quant: Post-Training Quantization based on Prediction Difference Metric."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Purpose: The purpose of this paper is to present a method for real-time 2D-3Dnon-rigid registration using a single fluoroscopic image. Such a method canfind applications in surgery, interventional radiology and radiotherapy. Byestimating a three-dimensional displacement field from a 2D X-ray image,anatomical structures segmented in the preoperative scan can be projected ontothe 2D image, thus providing a mixed reality view. Methods: A dataset composedof displacement fields and 2D projections of the anatomy is generated from thepreoperative scan. From this dataset, a neural network is trained to recoverthe unknown 3D displacement field from a single projection image. Results: Ourmethod is validated on lung 4D CT data at different stages of the lungdeformation. The training is performed on a 3D CT using random (nondomain-specific) diffeomorphic deformations, to which perturbations mimickingthe pose uncertainty are added. The model achieves a mean TRE over a series oflandmarks ranging from 2.3 to 5.5 mm depending on the amplitude of deformation.Conclusion: In this paper, a CNN-based method for real-time 2D-3D non-rigidregistration is presented. This method is able to cope with pose estimationuncertainties, making it applicable to actual clinical scenarios, such as lungsurgery, where the C-arm pose is planned before the intervention.", "output": "CNN-based real-time 2D-3D deformable registration from a single X-ray projection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose an ID-preserving talking head generation framework,which advances previous methods in two aspects. First, as opposed tointerpolating from sparse flow, we claim that dense landmarks are crucial toachieving accurate geometry-aware flow fields. Second, inspired byface-swapping methods, we adaptively fuse the source identity during synthesis,so that the network better preserves the key characteristics of the imageportrait. Although the proposed model surpasses prior generation fidelity onestablished benchmarks, to further make the talking head generation qualifiedfor real usage, personalized fine-tuning is usually needed. However, thisprocess is rather computationally demanding that is unaffordable to standardusers. To solve this, we propose a fast adaptation model using a meta-learningapproach. The learned model can be adapted to a high-quality personalized modelas fast as 30 seconds. Last but not the least, a spatial-temporal enhancementmodule is proposed to improve the fine details while ensuring temporalcoherency. Extensive experiments prove the significant superiority of ourapproach over the state of the arts in both one-shot and personalized settings.", "output": "MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In biological research, fluorescence staining is a key technique to revealthe locations and morphology of subcellular structures. However, it is slow,expensive, and harmful to cells. In this paper, we model it as a deep learningtask termed subcellular structure prediction (SSP), aiming to predict the 3Dfluorescent images of multiple subcellular structures from a 3Dtransmitted-light image. Unfortunately, due to the limitations of currentbiotechnology, each image is partially labeled in SSP. Besides, naturally,subcellular structures vary considerably in size, which causes the multi-scaleissue of SSP. To overcome these challenges, we propose Re-parameterizingMixture-of-Diverse-Experts (RepMode), a network that dynamically organizes itsparameters with task-aware priors to handle specified single-label predictiontasks. In RepMode, the Mixture-of-Diverse-Experts (MoDE) block is designed tolearn the generalized parameters for all tasks, and gating re-parameterization(GatRep) is performed to generate the specialized parameters for each task, bywhich RepMode can maintain a compact practical topology exactly like a plainnetwork, and meanwhile achieves a powerful theoretical topology. Comprehensiveexperiments show that RepMode can achieve state-of-the-art overall performancein SSP.", "output": "RepMode: Learning to Re-parameterize Diverse Experts for Subcellular Structure Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce anchored radial observations (ARO), a novel shape encoding forlearning implicit field representation of 3D shapes that is category-agnosticand generalizable amid significant shape variations. The main idea behind ourwork is to reason about shapes through partial observations from a set ofviewpoints, called anchors. We develop a general and unified shaperepresentation by employing a fixed set of anchors, via Fibonacci sampling, anddesigning a coordinate-based deep neural network to predict the occupancy valueof a query point in space. Differently from prior neural implicit models thatuse global shape feature, our shape encoder operates on contextual,query-specific features. To predict point occupancy, locally observed shapeinformation from the perspective of the anchors surrounding the input querypoint are encoded and aggregated through an attention module, before implicitdecoding is performed. We demonstrate the quality and generality of ournetwork, coined ARO-Net, on surface reconstruction from sparse point clouds,with tests on novel and unseen object categories, \"one-shape\" training, andcomparisons to state-of-the-art neural and classical methods for reconstructionand tessellation.", "output": "ARO-Net: Learning Implicit Fields from Anchored Radial Observations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatically estimating 3D skeleton, shape, camera viewpoints, and partarticulation from sparse in-the-wild image ensembles is a severelyunder-constrained and challenging problem. Most prior methods rely onlarge-scale image datasets, dense temporal correspondence, or human annotationslike camera pose, 2D keypoints, and shape templates. We propose Hi-LASSIE,which performs 3D articulated reconstruction from only 20-30 online images inthe wild without any user-defined shape or skeleton templates. We follow therecent work of LASSIE that tackles a similar problem setting and make twosignificant advances. First, instead of relying on a manually annotated 3Dskeleton, we automatically estimate a class-specific skeleton from the selectedreference image. Second, we improve the shape reconstructions with novelinstance-specific optimization strategies that allow reconstructions tofaithful fit on each instance while preserving the class-specific priorslearned across all images. Experiments on in-the-wild image ensembles show thatHi-LASSIE obtains higher fidelity state-of-the-art 3D reconstructions despiterequiring minimum user input.", "output": "Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery from Sparse Image Ensemble."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of semantic segmentation calibration. Lots of solutionshave been proposed to approach model miscalibration of confidence in imageclassification. However, to date, confidence calibration research on semanticsegmentation is still limited. We provide a systematic study on the calibrationof semantic segmentation models and propose a simple yet effective approach.First, we find that model capacity, crop size, multi-scale testing, andprediction correctness have impact on calibration. Among them, predictioncorrectness, especially misprediction, is more important to miscalibration dueto over-confidence. Next, we propose a simple, unifying, and effectiveapproach, namely selective scaling, by separating correct/incorrect predictionfor scaling and more focusing on misprediction logit smoothing. Then, we studypopular existing calibration methods and compare them with selective scaling onsemantic segmentation calibration. We conduct extensive experiments with avariety of benchmarks on both in-domain and domain-shift calibration and showthat selective scaling consistently outperforms other methods.", "output": "On Calibrating Semantic Segmentation Models: Analyses and An Algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open vocabulary object detection has been greatly advanced by the recentdevelopment of vision-language pretrained model, which helps recognize novelobjects with only semantic categories. The prior works mainly focus onknowledge transferring to the object proposal classification and employclass-agnostic box and mask prediction. In this work, we propose CondHead, aprincipled dynamic network design to better generalize the box regression andmask segmentation for open vocabulary setting. The core idea is toconditionally parameterize the network heads on semantic embedding and thus themodel is guided with class-specific knowledge to better detect novelcategories. Specifically, CondHead is composed of two streams of network heads,the dynamically aggregated head and the dynamically generated head. The formeris instantiated with a set of static heads that are conditionally aggregated,these heads are optimized as experts and are expected to learn sophisticatedprediction. The latter is instantiated with dynamically generated parametersand encodes general class-specific information. With such a conditional design,the detection model is bridged by the semantic embedding to offer stronglygeneralizable class-wise box and mask prediction. Our method brings significantimprovement to the state-of-the-art open vocabulary object detection methodswith very minor overhead, e.g., it surpasses a RegionClip model by 3.0detection AP on novel categories, with only 1.1% more computation.", "output": "Learning to Detect and Segment for Open Vocabulary Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Radiance Fields (RF) are popular to represent casually-captured scenes fornew view synthesis and several applications beyond it. Mixed reality onpersonal spaces needs understanding and manipulating scenes represented as RFs,with semantic segmentation of objects as an important step. Prior segmentationefforts show promise but don't scale to complex objects with diverseappearance. We present the ISRF method to interactively segment objects withfine structure and appearance. Nearest neighbor feature matching usingdistilled semantic features identifies high-confidence seed regions. Bilateralsearch in a joint spatio-semantic space grows the region to recover accuratesegmentation. We show state-of-the-art results of segmenting objects from RFsand compositing them to another scene, changing appearance, etc., and aninteractive segmentation tool that others can use.Project Page: https://rahul-goel.github.io/isrf/", "output": "Interactive Segmentation of Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-language models (VLMs) pre-trained on large-scale image-text pairshave demonstrated impressive transferability on various visual tasks.Transferring knowledge from such powerful VLMs is a promising direction forbuilding effective video recognition models. However, current exploration inthis field is still limited. We believe that the greatest value of pre-trainedVLMs lies in building a bridge between visual and textual domains. In thispaper, we propose a novel framework called BIKE, which utilizes the cross-modalbridge to explore bidirectional knowledge: i) We introduce the Video AttributeAssociation mechanism, which leverages the Video-to-Text knowledge to generatetextual auxiliary attributes for complementing video recognition. ii) We alsopresent a Temporal Concept Spotting mechanism that uses the Text-to-Videoexpertise to capture temporal saliency in a parameter-free manner, leading toenhanced video representation. Extensive studies on six popular video datasets,including Kinetics-400 &amp; 600, UCF-101, HMDB-51, ActivityNet and Charades, showthat our method achieves state-of-the-art performance in various recognitionscenarios, such as general, zero-shot, and few-shot video recognition. Our bestmodel achieves a state-of-the-art accuracy of 88.6% on the challengingKinetics-400 using the released CLIP model. The code is available athttps://github.com/whwu95/BIKE .", "output": "Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most existing text-video retrieval methods focus on cross-modal matchingbetween the visual content of videos and textual query sentences. However, inreal-world scenarios, online videos are often accompanied by relevant textinformation such as titles, tags, and even subtitles, which can be utilized tomatch textual queries. This insight has motivated us to propose a novelapproach to text-video retrieval, where we directly generate associatedcaptions from videos using zero-shot video captioning with knowledge fromweb-scale pre-trained models (e.g., CLIP and GPT-2). Given the generatedcaptions, a natural question arises: what benefits do they bring to text-videoretrieval? To answer this, we introduce Cap4Video, a new framework thatleverages captions in three ways: i) Input data: video-caption pairs canaugment the training data. ii) Intermediate feature interaction: we performcross-modal feature interaction between the video and caption to produceenhanced video representations. iii) Output score: the Query-Caption matchingbranch can complement the original Query-Video matching branch for text-videoretrieval. We conduct comprehensive ablation studies to demonstrate theeffectiveness of our approach. Without any post-processing, Cap4Video achievesstate-of-the-art performance on four standard text-video retrieval benchmarks:MSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code isavailable at https://github.com/whwu95/Cap4Video .", "output": "Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Searching long egocentric videos with natural language queries (NLQ) hascompelling applications in augmented reality and robotics, where a fluid indexinto everything that a person (agent) has seen before could augment humanmemory and surface relevant information on demand. However, the structurednature of the learning problem (free-form text query inputs, localized videotemporal window outputs) and its needle-in-a-haystack nature makes it bothtechnically challenging and expensive to supervise. We introduceNarrations-as-Queries (NaQ), a data augmentation strategy that transformsstandard video-text narrations into training data for a video querylocalization model. Validating our idea on the Ego4D benchmark, we find it hastremendous impact in practice. NaQ improves multiple top models by substantialmargins (even doubling their accuracy), and yields the very best results todate on the Ego4D NLQ challenge, soundly outperforming all challenge winners inthe CVPR and ECCV 2022 competitions and topping the current public leaderboard.Beyond achieving the state-of-the-art for NLQ, we also demonstrate uniqueproperties of our approach such as the ability to perform zero-shot andfew-shot NLQ, and improved performance on queries about long-tail objectcategories. Code and models:{\\small\\url{<a href=\"http://vision.cs.utexas.edu/projects/naq\">this http URL</a>}}.", "output": "NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial robustness assessment for video recognition models has raisedconcerns owing to their wide applications on safety-critical tasks. Comparedwith images, videos have much high dimension, which brings huge computationalcosts when generating adversarial videos. This is especially serious for thequery-based black-box attacks where gradient estimation for the threat modelsis usually utilized, and high dimensions will lead to a large number ofqueries. To mitigate this issue, we propose to simultaneously eliminate thetemporal and spatial redundancy within the video to achieve an effective andefficient gradient estimation on the reduced searching space, and thus querynumber could decrease. To implement this idea, we design the novel Adversarialspatial-temporal Focus (AstFocus) attack on videos, which performs attacks onthe simultaneously focused key frames and key regions from the inter-frames andintra-frames in the video. AstFocus attack is based on the cooperativeMulti-Agent Reinforcement Learning (MARL) framework. One agent is responsiblefor selecting key frames, and another agent is responsible for selecting keyregions. These two agents are jointly trained by the common rewards receivedfrom the black-box threat models to perform a cooperative prediction. Bycontinuously querying, the reduced searching space composed of key frames andkey regions is becoming precise, and the whole query number becomes less thanthat on the original video. Extensive experiments on four mainstream videorecognition models and three widely used action recognition datasetsdemonstrate that the proposed AstFocus attack outperforms the SOTA methods,which is prevenient in fooling rate, query number, time, and perturbationmagnitude at the same.", "output": "Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although modern trackers exhibit competitive performance for underwater imagedegradation assessment, two problems remain when these are applied tounderwater object tracking (UOT). A single-object tracker is trained onopen-air datasets, which results in a serious sample imbalance betweenunderwater objects and open-air objects when it is applied to UOT. Moreover,underwater targets such as fish and dolphins usually have a similar appearance,and it is challenging for models to discriminate weak discriminative features.Existing detection-based post-processing approaches struggle to distinguish atracked target from similar objects. In this study, the UOSTrack is proposed,which involves the use of underwater images and open-air sequence hybridtraining (UOHT), and motion-based post-processing (MBPP). The UOHT trainingparadigm is designed to train the sample-imbalanced underwater tracker. Inparticular, underwater object detection (UOD) images are converted into imagepairs through customised data augmentation, such that the tracker is exposed tomore underwater domain training samples and learns the feature expressions ofunderwater objects. The MBPP paradigm is proposed to exclude similar objectsnear the target. In particular, it employs the estimation box predicted using aKalman filter and the candidate boxes in each frame to reconfirm the trackedtarget that is hidden in the candidate area when it has been lost. UOSTrackprovides an average performance improvement of 3.5 % compared to OSTrack onsimilar object challenge attribute in UOT100 and UTB180. The averageperformance improvements provided by UOSTrack are 1 % and 3 %, respectively.The results from two UOT benchmarks demonstrate that UOSTrack sets a newstate-of-the-art benchmark, and the effectiveness of UOHT and MBPP, and thegeneralisation and applicability of the MBPP for use in UOT.", "output": "Sample Imbalance Adjustment and Similar Object Exclusion in Underwater Object Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-world object detection (OWOD), as a more general and challenging goal,requires the model trained from data on known objects to detect both known andunknown objects and incrementally learn to identify these unknown objects. Theexisting works which employ standard detection framework and fixedpseudo-labelling mechanism (PLM) have the following problems: (i) The inclusionof detecting unknown objects substantially reduces the model's ability todetect known ones. (ii) The PLM does not adequately utilize the prioriknowledge of inputs. (iii) The fixed selection manner of PLM cannot guaranteethat the model is trained in the right direction. We observe that humanssubconsciously prefer to focus on all foreground objects and then identify eachone in detail, rather than localize and identify a single objectsimultaneously, for alleviating the confusion. This motivates us to propose anovel solution called CAT: LoCalization and IdentificAtion Cascade DetectionTransformer which decouples the detection process via the shared decoder in thecascade decoding way. In the meanwhile, we propose the self-adaptivepseudo-labelling mechanism which combines the model-driven with input-drivenPLM and self-adaptively generates robust pseudo-labels for unknown objects,significantly improving the ability of CAT to retrieve unknown objects.Comprehensive experiments on two benchmark datasets, i.e., MS-COCO and PASCALVOC, show that our model outperforms the state-of-the-art in terms of allmetrics in the task of OWOD, incremental object detection (IOD) and open-setdetection.", "output": "CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modelling and understanding time remains a challenge in contemporary videounderstanding models. With language emerging as a key driver towards powerfulgeneralization, it is imperative for foundational video-language models to havea sense of time. In this paper, we consider a specific aspect of temporalunderstanding: consistency of time order as elicited by before/after relations.We establish that seven existing video-language models struggle to understandeven such simple temporal relations. We then question whether it is feasible toequip these foundational models with temporal awareness without re-trainingthem from scratch. Towards this, we propose a temporal adaptation recipe on topof one such model, VideoCLIP, based on post-pretraining on a small amount ofvideo-text data. We conduct a zero-shot evaluation of the adapted models on sixdatasets for three downstream tasks which require varying degrees of timeawareness. We observe encouraging performance gains especially when the taskneeds higher time awareness. Our work serves as a first step towards probingand instilling a sense of time in existing video-language models without theneed for data and compute-intense training from scratch.", "output": "Test of Time: Instilling Video-Language Models with a Sense of Time."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern 3D-GANs synthesize geometry and texture by training on large-scaledatasets with a consistent structure. Training such models on stylized,artistic data, with often unknown, highly variable geometry, and camerainformation has not yet been shown possible. Can we train a 3D GAN on suchartistic data, while maintaining multi-view consistency and texture quality? Tothis end, we propose an adaptation framework, where the source domain is apre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artisticdatasets. We then distill the knowledge from a 2D generator to the source 3Dgenerator. To do that, we first propose an optimization-based method to alignthe distributions of camera parameters across domains. Second, we proposeregularizations necessary to learn high-quality texture, while avoidingdegenerate geometric solutions, such as flat shapes. Third, we show adeformation-based technique for modeling exaggerated geometry of artisticdomains, enabling -- as a byproduct -- personalized geometric editing. Finally,we propose a novel inversion method for 3D-GANs linking the latent spaces ofthe source and the target domains. Our contributions -- for the first time --allow for the generation, editing, and animation of personalized artistic 3Davatars on artistic datasets.", "output": "3DAvatarGAN: Bridging Domains for Personalized Editable Avatars."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we consider the problem of estimating the 3D position ofmultiple humans in a scene as well as their body shape and articulation from asingle RGB video recorded with a static camera. In contrast to expensivemarker-based or multi-view systems, our lightweight setup is ideal for privateusers as it enables an affordable 3D motion capture that is easy to install anddoes not require expert knowledge. To deal with this challenging setting, weleverage recent advances in computer vision using large-scale pre-trainedmodels for a variety of modalities, including 2D body joints, joint angles,normalized disparity maps, and human segmentation masks. Thus, we introduce thefirst non-linear optimization-based approach that jointly solves for theabsolute 3D position of each human, their articulated pose, their individualshapes as well as the scale of the scene. In particular, we estimate the scenedepth and person unique scale from normalized disparity predictions using the2D body joints and joint angles. Given the per-frame scene depth, wereconstruct a point-cloud of the static scene in 3D space. Finally, given theper-frame 3D estimates of the humans and scene point-cloud, we perform aspace-time coherent optimization over the video to ensure temporal, spatial andphysical plausibility. We evaluate our method on established multi-person 3Dhuman pose benchmarks where we consistently outperform previous methods and wequalitatively demonstrate that our method is robust to in-the-wild conditionsincluding challenging scenes with people of different sizes.", "output": "Scene-Aware 3D Multi-Human Motion Capture from a Single Camera."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For infrastructure inspections, damage representation does not constantlymatch the predefined classes of damage grade, resulting in detailed clusters ofunseen damages or more complex clusters from overlapped space between twogrades. The damage representation has fundamentally complex features;consequently, not all the damage classes can be perfectly predefined. Theproposed MN-pair contrastive learning method helps to explore an embeddingdamage representation beyond the predefined classes by including more detailedclusters. It maximizes both the similarity of M-1 positive images close to ananchor and dissimilarity of N-1 negative images using both weighting lossfunctions. It learns faster than the N-pair algorithm using one positive image.We proposed a pipeline to obtain the damage representation and used adensity-based clustering on a 2-D reduction space to automate finer clusterdiscrimination. We also visualized the explanation of the damage feature usingGrad-CAM for MN-pair damage metric learning. We demonstrated our method inthree experimental studies: steel product defect, concrete crack, and theeffectiveness of our method and discuss future works.", "output": "MN-Pair Contrastive Damage Representation and Clustering for Prognostic Explanation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Purpose: The purpose of this article is to present a system that enhances thesecurity, efficiency, and reconfigurability of an Internet-of-Things (IoT)system used for surveillance and monitoring. Methods: A Multi-ProcessorSystem-On-Chip (MPSoC) composed of Central Processor Unit (CPU) andField-Programmable Gate Array (FPGA) is proposed for increasing the securityand the frame rate of a smart IoT edge device. The private encryption key issafely embedded in the FPGA unit to avoid being exposed in the Random AccessMemory (RAM). This allows the edge device to securely store and authenticatethe key, protecting the data transmitted from the same Integrated Circuit (IC).Additionally, the edge device can simultaneously publish and route a camerastream using a lightweight communication protocol, achieving a frame rate of 14frames per Second (fps). The performance of the MPSoC is compared to a NVIDIAJetson Nano (NJN) and a Raspberry Pi 4 (RPI4) and it is found that the RPI4 isthe most cost-effective solution but with lower frame rate, the NJN is thefastest because it can achieve higher frame-rate but it is not secure, and theMPSoC is the optimal solution because it offers a balanced frame rate and it issecure because it never exposes the secure key into the memory. Results: Theproposed system successfully addresses the challenges of security, scalability,and efficiency in an IoT system used for surveillance and monitoring. Theencryption key is securely stored and authenticated, and the edge device isable to simultaneously publish and route a camera stream feed high-definitionimages at 14 fps.", "output": "Secure Video Streaming Using Dedicated Hardware."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing benchmarks for evaluating long video understanding falls short ontwo critical aspects, either lacking in scale or quality of annotations. Theselimitations arise from the difficulty in collecting dense annotations for longvideos, which often require manually labeling each frame. In this work, weintroduce an automated Annotation and Video Stream Alignment Pipeline(abbreviated ASAP). We demonstrate the generality of ASAP by aligning unlabeledvideos of four different sports with corresponding freely available dense webannotations (i.e. commentary). We then leverage ASAP scalability to createLCric, a large-scale long video understanding benchmark, with over 1000 hoursof densely annotated long Cricket videos (with an average sample length of ~50mins) collected at virtually zero annotation cost. We benchmark and analyzestate-of-the-art video understanding models on LCric through a large set ofcompositional multi-choice and regression queries. We establish a humanbaseline that indicates significant room for new research to explore. Our humanstudies indicate that ASAP can align videos and annotations with high fidelity,precision, and speed. The dataset along with the code for ASAP and baselinescan be accessed here: https://asap-benchmark.github.io/.", "output": "Building Scalable Video Understanding Benchmarks through Sports."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computed tomography (CT) is one of the modalities for effective lung cancerscreening, diagnosis, treatment, and prognosis. The features extracted from CTimages are now used to quantify spatial and temporal variations in tumors.However, CT images obtained from various scanners with customized acquisitionprotocols may introduce considerable variations in texture features, even forthe same patient. This presents a fundamental challenge to downstream studiesthat require consistent and reliable feature analysis. Existing CT imageharmonization models rely on GAN-based supervised or semi-supervised learning,with limited performance. This work addresses the issue of CT imageharmonization using a new diffusion-based model, named DiffusionCT, tostandardize CT images acquired from different vendors and protocols.DiffusionCT operates in the latent space by mapping a latent non-standarddistribution into a standard one. DiffusionCT incorporates an Unet-basedencoder-decoder, augmented by a diffusion model integrated into the bottleneckpart. The model is designed in two training phases. The encoder-decoder isfirst trained, without embedding the diffusion model, to learn the latentrepresentation of the input data. The latent diffusion model is then trained inthe next training phase while fixing the encoder-decoder. Finally, the decodersynthesizes a standardized image with the transformed latent representation.The experimental results demonstrate a significant improvement in theperformance of the standardization task using DiffusionCT.", "output": "DiffusionCT: Latent Diffusion Model for CT Image Standardization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans universally dislike the task of cleaning up a messy room. If machineswere to help us with this task, they must understand human criteria for regulararrangements, such as several types of symmetry, co-linearity orco-circularity, spacing uniformity in linear or circular patterns, and furtherinter-object relationships that relate to style and functionality. Previousapproaches for this task relied on human input to explicitly specify goalstate, or synthesized scenes from scratch -- but such methods do not addressthe rearrangement of existing messy scenes without providing a goal state. Inthis paper, we present LEGO-Net, a data-driven transformer-based iterativemethod for LEarning reGular rearrangement of Objects in messy rooms. LEGO-Netis partly inspired by diffusion models -- it starts with an initial messy stateand iteratively ''de-noises'' the position and orientation of objects to aregular state while reducing distance traveled. Given randomly perturbed objectpositions and orientations in an existing dataset of professionally-arrangedscenes, our method is trained to recover a regular re-arrangement. Resultsdemonstrate that our method is able to reliably rearrange room scenes andoutperform other methods. We additionally propose a metric for evaluatingregularity in room arrangements using number-theoretic machinery.", "output": "LEGO-Net: Learning Regular Rearrangements of Objects in Rooms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modeling and re-rendering dynamic 3D scenes is a challenging task in 3Dvision. Prior approaches build on NeRF and rely on implicit representations.This is slow since it requires many MLP evaluations, constraining real-worldapplications. We show that dynamic 3D scenes can be explicitly represented bysix planes of learned features, leading to an elegant solution we callHexPlane. A HexPlane computes features for points in spacetime by fusingvectors extracted from each plane, which is highly efficient. Pairing aHexPlane with a tiny MLP to regress output colors and training via volumerendering gives impressive results for novel view synthesis on dynamic scenes,matching the image quality of prior work but reducing training time by morethan $100\\times$. Extensive ablations confirm our HexPlane design and show thatit is robust to different feature fusion mechanisms, coordinate systems, anddecoding mechanisms. HexPlane is a simple and effective solution forrepresenting 4D volumes, and we hope they can broadly contribute to modelingspacetime for dynamic 3D scenes.", "output": "HexPlane: A Fast Representation for Dynamic Scenes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce k-planes, a white-box model for radiance fields in arbitrarydimensions. Our model uses d choose 2 planes to represent a d-dimensionalscene, providing a seamless way to go from static (d=3) to dynamic (d=4)scenes. This planar factorization makes adding dimension-specific priors easy,e.g. temporal smoothness and multi-resolution spatial structure, and induces anatural decomposition of static and dynamic components of a scene. We use alinear feature decoder with a learned color basis that yields similarperformance as a nonlinear black-box MLP decoder. Across a range of syntheticand real, static and dynamic, fixed and varying appearance scenes, k-planesyields competitive and often state-of-the-art reconstruction fidelity with lowmemory usage, achieving 1000x compression over a full 4D grid, and fastoptimization with a pure PyTorch implementation. For video results and code,please see https://sarafridov.github.io/K-Planes.", "output": "K-Planes: Explicit Radiance Fields in Space, Time, and Appearance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to accurately locate and navigate to a specific object is acrucial capability for embodied agents that operate in the real world andinteract with objects to complete tasks. Such object navigation tasks usuallyrequire large-scale training in visual environments with labeled objects, whichgeneralizes poorly to novel objects in unknown environments. In this work, wepresent a novel zero-shot object navigation method, Exploration with SoftCommonsense constraints (ESC), that transfers commonsense knowledge inpre-trained models to open-world object navigation without any navigationexperience nor any other training on the visual environments. First, ESCleverages a pre-trained vision and language model for open-world prompt-basedgrounding and a pre-trained commonsense language model for room and objectreasoning. Then ESC converts commonsense knowledge into navigation actions bymodeling it as soft logic predicates for efficient exploration. Extensiveexperiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC methodimproves significantly over baselines, and achieves new state-of-the-artresults for zero-shot object navigation (e.g., 158% relative Success Rateimprovement than CoW on MP3D).", "output": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-Supervised Domain Adaptation (SSDA) involves learning to classify unseentarget data with a few labeled and lots of unlabeled target data, along withmany labeled source data from a related domain. Current SSDA approaches usuallyaim at aligning the target data to the labeled source data with feature spacemapping and pseudo-label assignments. Nevertheless, such a source-orientedmodel can sometimes align the target data to source data of the wrong classes,degrading the classification performance. This paper presents a novelsource-adaptive paradigm that adapts the source data to match the target data.Our key idea is to view the source data as a noisily-labeled version of theideal target data. Then, we propose an SSDA model that cleans up the labelnoise dynamically with the help of a robust cleaner component designed from thetarget perspective. Since the paradigm is very different from the core ideasbehind existing SSDA approaches, our proposed model can be easily coupled withthem to improve their performance. Empirical results on two state-of-the-artSSDA approaches demonstrate that the proposed model effectively cleans up thenoise within the source labels and exhibits superior performance over thoseapproaches across benchmark datasets. Our code is available athttps://github.com/chu0802/SLA .", "output": "Semi-Supervised Domain Adaptation with Source Label Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have recently exhibited remarkable abilities to synthesizestriking image samples since the introduction of denoising diffusionprobabilistic models (DDPMs). Their key idea is to disrupt images into noisethrough a fixed forward process and learn its reverse process to generatesamples from noise in a denoising way. For conditional DDPMs, most existingpractices relate conditions only to the reverse process and fit it to thereversal of unconditional forward process. We find this will limit thecondition modeling and generation in a small time window. In this paper, wepropose a novel and flexible conditional diffusion model by introducingconditions into the forward process. We utilize extra latent space to allocatean exclusive diffusion trajectory for each condition based on some shiftingrules, which will disperse condition modeling to all timesteps and improve thelearning capacity of model. We formulate our method, which we call\\textbf{ShiftDDPMs}, and provide a unified point of view on existing relatedmethods. Extensive qualitative and quantitative experiments on image synthesisdemonstrate the feasibility and effectiveness of ShiftDDPMs.", "output": "ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human motion prediction is a classical problem in computer vision andcomputer graphics, which has a wide range of practical applications. Previouseffects achieve great empirical performance based on an encoding-decodingstyle. The methods of this style work by first encoding previous motions tolatent representations and then decoding the latent representations intopredicted motions. However, in practice, they are still unsatisfactory due toseveral issues, including complicated loss constraints, cumbersome trainingprocesses, and scarce switch of different categories of motions in prediction.In this paper, to address the above issues, we jump out of the foregoing styleand propose a novel framework from a new perspective. Specifically, ourframework works in a masked completion fashion. In the training stage, we learna motion diffusion model that generates motions from random noise. In theinference stage, with a denoising procedure, we make motion predictionconditioning on observed motions to output more continuous and controllablepredictions. The proposed framework enjoys promising algorithmic properties,which only needs one loss in optimization and is trained in an end-to-endmanner. Additionally, it accomplishes the switch of different categories ofmotions effectively, which is significant in realistic tasks, e.g., theanimation task. Comprehensive experiments on benchmarks confirm the superiorityof the proposed framework. The project page is available athttps://lhchen.top/Human-MAC.", "output": "HumanMAC: Masked Motion Completion for Human Motion Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to perceive and understand 3D scenes is crucial for manyapplications in computer vision and robotics. Inverse graphics is an appealingapproach to 3D scene understanding that aims to infer the 3D scene structurefrom 2D images. In this paper, we introduce probabilistic modeling to theinverse graphics framework to quantify uncertainty and achieve robustness in 6Dpose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood(3DNEL) as a unified probabilistic model over RGB-D images, and developefficient inference procedures on 3D scene descriptions. 3DNEL effectivelycombines learned neural embeddings from RGB with depth information to improverobustness in sim-to-real 6D object pose estimation from RGB-D images.Performance on the YCB-Video dataset is on par with state-of-the-art yet ismuch more robust in challenging regimes. In contrast to discriminativeapproaches, 3DNEL's probabilistic generative formulation jointly modelsmulti-object scenes, quantifies uncertainty in a principled way, and handlesobject pose tracking under heavy occlusion. Finally, 3DNEL provides aprincipled framework for incorporating prior knowledge about the scene andobjects, which allows natural extension to additional tasks like camera posetracking from video.", "output": "3D Neural Embedding Likelihood for Robust Probabilistic Inverse Graphics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "HybrIK relies on a combination of analytical inverse kinematics and deeplearning to produce more accurate 3D pose estimation from 2D monocular images.HybrIK has three major components: (1) pretrained convolution backbone, (2)deconvolution to lift 3D pose from 2D convolution features, (3) analyticalinverse kinematics pass correcting deep learning prediction using learneddistribution of plausible twist and swing angles. In this paper we propose anenhancement of the 2D to 3D lifting module, replacing deconvolution withTransformer, resulting in accuracy and computational efficiency improvementrelative to the original HybrIK method. We demonstrate our results on commonlyused H36M, PW3D, COCO and HP3D datasets. Our code is publicly availablehttps://github.com/boreshkinai/hybrik-transformer.", "output": "3D Human Pose and Shape Estimation via HybrIK-Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a method to fit arbitrarily accurate blendshape rig models bysolving the inverse rig problem in realistic human face animation. The methodconsiders blendshape models with different levels of added corrections andsolves the regularized least-squares problem using coordinate descent, i.e.,iteratively estimating blendshape weights. Besides making the optimizationeasier to solve, this approach ensures that mutually exclusive controllers willnot be activated simultaneously and improves the goodness of fit after eachiteration. We show experimentally that the proposed method yields solutionswith mesh error comparable to or lower than the state-of-the-art approacheswhile significantly reducing the cardinality of the weight vector (over 20percent), hence giving a high-fidelity reconstruction of the referenceexpression that is easier to manipulate in the post-production manually. Pythonscripts for the algorithm will be publicly available upon acceptance of thepaper.", "output": "High-fidelity Interpretable Inverse Rig: An Accurate and Sparse Solution Optimizing the Quartic Blendshape Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world deployment of reliable object detectors is crucial forapplications such as autonomous driving. However, general-purpose objectdetectors like Faster R-CNN are prone to providing overconfident predictionsfor outlier objects. Recent outlier-aware object detection approaches estimatethe density of instance-wide features with class-conditional Gaussians andtrain on synthesized outlier features from their low-likelihood regions.However, this strategy does not guarantee that the synthesized outlier featureswill have a low likelihood according to the other class-conditional Gaussians.We propose a novel outlier-aware object detection framework that distinguishesoutliers from inlier objects by learning the joint data distribution of allinlier classes with an invertible normalizing flow. The appropriate sampling ofthe flow model ensures that the synthesized outliers have a lower likelihoodthan inliers of all object classes, thereby modeling a better decision boundarybetween inlier and outlier objects. Our approach significantly outperforms thestate-of-the-art for outlier-aware object detection on both image and videodatasets.", "output": "Normalizing Flow based Feature Synthesis for Outlier-Aware Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents a new depth- and semantics-aware conditional generativemodel, named TITAN-Next, for cross-domain image-to-image translation in amulti-modal setup between LiDAR and camera sensors. The proposed modelleverages scene semantics as a mid-level representation and is able totranslate raw LiDAR point clouds to RGB-D camera images by solely relying onsemantic scene segments. We claim that this is the first framework of its kindand it has practical applications in autonomous vehicles such as providing afail-safe mechanism and augmenting available data in the target image domain.The proposed model is evaluated on the large-scale and challengingSemantic-KITTI dataset, and experimental findings show that it considerablyoutperforms the original TITAN-Net and other strong baselines by 23.7$\\%$margin in terms of IoU.", "output": "Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D Panoramic Color Images from LiDAR Point Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a simple method to approximate Rao's distance between multivariatenormal distributions based on discretizing curves joining normal distributionsand approximating Rao's distances between successive nearby normaldistributions on the curves by the square root of Jeffreys divergence, thesymmetrized Kullback-Leibler divergence. We consider experimentally the linearinterpolation curves in the ordinary, natural and expectation parameterizationsof the normal distributions, and compare these curves with a curve derived fromthe Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normalmanifold into the cone of $(d+1)\\times (d+1)$ symmetric positive-definitematrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report onour experiments and assess the quality of our approximation technique bycomparing the numerical approximations with both lower and upper bounds.Finally, we present several information-geometric properties of the Calvo andOller's isometric embedding.", "output": "A numerical approximation method for the Fisher-Rao distance between multivariate normal distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans can easily imagine the complete 3D geometry of occluded objects andscenes. This appealing ability is vital for recognition and understanding. Toenable such capability in AI systems, we propose VoxFormer, a Transformer-basedsemantic scene completion framework that can output complete 3D volumetricsemantics from only 2D images. Our framework adopts a two-stage design where westart from a sparse set of visible and occupied voxel queries from depthestimation, followed by a densification stage that generates dense 3D voxelsfrom the sparse ones. A key idea of this design is that the visual features on2D images correspond only to the visible scene structures rather than theoccluded or empty spaces. Therefore, starting with the featurization andprediction of the visible structures is more reliable. Once we obtain the setof sparse queries, we apply a masked autoencoder design to propagate theinformation to all the voxels by self-attention. Experiments on SemanticKITTIshow that VoxFormer outperforms the state of the art with a relativeimprovement of 20.0% in geometry and 18.1% in semantics and reduces GPU memoryduring training to less than 16GB. Our code is available onhttps://github.com/NVlabs/VoxFormer.", "output": "VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Backdoor attacks pose a new and emerging threat to AI security, where DeepNeural Networks (DNNs) are trained on datasets added to hidden triggerpatterns. Although the poisoned model behaves normally on benign samples, itproduces anomalous results on samples containing the trigger pattern.Nevertheless, most existing backdoor attacks face two significant drawbacks:their trigger patterns are visible and easy to detect by human inspection, andtheir injection process leads to the loss of natural sample features andtrigger patterns, thereby reducing the attack success rate and the modelaccuracy. In this paper, we propose a novel backdoor attack named SATBA thatovercomes these limitations by using spatial attention mechanism and U-typemodel. Our attack leverages spatial attention mechanism to extract datafeatures and generate invisible trigger patterns that are correlated with cleandata. Then it uses U-type model to plant these trigger patterns into theoriginal data without causing noticeable feature loss. We evaluate our attackon three prominent image classification DNNs across three standard datasets anddemonstrate that it achieves high attack success rate and robustness againstbackdoor defenses. Additionally, we also conduct extensive experiments on imagesimilarity to highlight the stealthiness of our attack.", "output": "SATBA: An Invisible Backdoor Attack Based On Spatial Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning the 3D structure of a non-rigid dynamic scene from a single movingcamera is an under-constrained problem. Inspired by the remarkable progress ofneural radiance fields (NeRFs) in photo-realistic novel view synthesis ofstatic scenes, extensions have been proposed for dynamic settings. Thesemethods heavily rely on neural priors in order to regularize the problem. Inthis work, we take a step back and reinvestigate how current implementationsmay entail deleterious effects, including limited expressiveness, entanglementof light and density fields, and sub-optimal motion localization. As a remedy,we advocate for a bridge between classic non-rigid-structure-from-motion(\\nrsfm) and NeRF, enabling the well-studied priors of the former to constrainthe latter. To this end, we propose a framework that factorizes time and spaceby formulating a scene as a composition of bandlimited, high-dimensionalsignals. We demonstrate compelling results across complex dynamic scenes thatinvolve changes in lighting, texture and long-range dynamics.", "output": "BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent large-scale Contrastive Language-Image Pretraining (CLIP) modelhas shown great potential in various downstream tasks via leveraging thepretrained vision and language knowledge. Scene text, which contains richtextual and visual information, has an inherent connection with a model likeCLIP. Recently, pretraining approaches based on vision language models havemade effective progresses in the field of text detection. In contrast to theseworks, this paper proposes a new method, termed TCM, focusing on Turning theCLIP Model directly for text detection without pretraining process. Wedemonstrate the advantages of the proposed TCM as follows: (1) The underlyingprinciple of our framework can be applied to improve existing scene textdetector. (2) It facilitates the few-shot training capability of existingmethods, e.g., by using 10% of labeled data, we significantly improve theperformance of the baseline method with an average of 22% in terms of theF-measure on 4 benchmarks. (3) By turning the CLIP model into existing scenetext detection methods, we further achieve promising domain adaptation ability.The code will be publicly released at https://github.com/wenwenyu/TCM.", "output": "Turning a CLIP Model into a Scene Text Detector."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Implicit Two Hands (Im2Hands), the first neural implicitrepresentation of two interacting hands. Unlike existing methods on two-handreconstruction that rely on a parametric hand model and/or low-resolutionmeshes, Im2Hands can produce fine-grained geometry of two hands with highhand-to-hand and hand-to-image coherency. To handle the shape complexity andinteraction context between two hands, Im2Hands models the occupancy volume oftwo hands - conditioned on an RGB image and coarse 3D keypoints - by two novelattention-based modules responsible for (1) initial occupancy estimation and(2) context-aware occupancy refinement, respectively. Im2Hands first learnsper-hand neural articulated occupancy in the canonical space designed for eachhand using query-image attention. It then refines the initial two-handoccupancy in the posed space to enhance the coherency between the two handshapes using query-anchor attention. In addition, we introduce an optionalkeypoint refinement module to enable robust two-hand shape estimation frompredicted hand keypoints in a single-image reconstruction scenario. Weexperimentally demonstrate the effectiveness of Im2Hands on two-handreconstruction in comparison to related methods, where ours achievesstate-of-the-art results. Our code is publicly available athttps://github.com/jyunlee/Im2Hands.", "output": "Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate compositional structures in data embeddings from pre-trainedvision-language models (VLMs). Traditionally, compositionality has beenassociated with algebraic operations on embeddings of words from a pre-existingvocabulary. In contrast, we seek to approximate representations from an encoderas combinations of a smaller set of vectors in the embedding space. Thesevectors can be seen as \"ideal words\" for generating concepts directly withinthe embedding space of the model. We first present a framework forunderstanding compositional structures from a geometric perspective. We thenexplain what these compositional structures entail probabilistically in thecase of VLM embeddings, providing intuitions for why they arise in practice.Finally, we empirically explore these structures in CLIP's embeddings and weevaluate their usefulness for solving different vision-language tasks such asclassification, debiasing, and retrieval. Our results show that simple linearalgebraic operations on embedding vectors can be used as compositional andinterpretable methods for regulating the behavior of VLMs.", "output": "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point cloud sampling is a less explored research topic for this datarepresentation. The most commonly used sampling methods are still classicalrandom sampling and farthest point sampling. With the development of neuralnetworks, various methods have been proposed to sample point clouds in atask-based learning manner. However, these methods are mostly generative-based,rather than selecting points directly using mathematical statistics. Inspiredby the Canny edge detection algorithm for images and with the help of theattention mechanism, this paper proposes a non-generative Attention-based Pointcloud Edge Sampling method (APES), which captures salient points in the pointcloud outline. Both qualitative and quantitative experimental results show thesuperior performance of our sampling method on common benchmark tasks.", "output": "Attention-based Point Cloud Edge Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing 3D instance segmentation methods are predominated by the bottom-updesign -- manually fine-tuned algorithm to group points into clusters followedby a refinement network. However, by relying on the quality of the clusters,these methods generate susceptible results when (1) nearby objects with thesame semantic class are packed together, or (2) large objects with looselyconnected regions. To address these limitations, we introduce ISBNet, a novelcluster-free method that represents instances as kernels and decodes instancemasks via dynamic convolution. To efficiently generate high-recall anddiscriminative kernels, we propose a simple strategy named Instance-awareFarthest Point Sampling to sample candidates and leverage the local aggregationlayer inspired by PointNet++ to encode candidate features. Moreover, we showthat predicting and leveraging the 3D axis-aligned bounding boxes in thedynamic convolution further boosts performance. Our method set newstate-of-the-art results on ScanNetV2 (55.9), S3DIS (60.8), and STPLS3D (49.2)in terms of AP and retains fast inference time (237ms per scene on ScanNetV2).The source code and trained models are available athttps://github.com/VinAIResearch/ISBNet.", "output": "ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, forgery faces pose pressing security concerns over fake news,fraud, impersonation, etc. Despite the demonstrated success in intra-domainface forgery detection, existing detection methods lack generalizationcapability and tend to suffer from dramatic performance drops when deployed tounforeseen domains. To mitigate this issue, this paper designs a more generalfake face detection model based on the vision transformer(ViT) architecture. Inthe training phase, the pretrained ViT weights are freezed, and only theLow-Rank Adaptation(LoRA) modules are updated. Additionally, the Single CenterLoss(SCL) is applied to supervise the training process, further improving thegeneralization capability of the model. The proposed method achievesstate-of-the-arts detection performances in both cross-manipulation andcross-dataset evaluations.", "output": "Enhancing General Face Forgery Detection via Vision Transformer with Low-Rank Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we tackle the problem of learning universal robotic dexterousgrasping from a point cloud observation under a table-top setting. The goal isto grasp and lift up objects in high-quality and diverse ways and generalizeacross hundreds of categories and even the unseen. Inspired by successfulpipelines used in parallel gripper grasping, we split the task into two stages:1) grasp proposal (pose) generation and 2) goal-conditioned grasp execution.For the first stage, we propose a novel probabilistic model of grasp poseconditioned on the point cloud observation that factorizes rotation fromtranslation and articulation. Trained on our synthesized large-scale dexterousgrasp dataset, this model enables us to sample diverse and high-qualitydexterous grasp poses for the object point cloud.For the second stage, wepropose to replace the motion planning used in parallel gripper grasping with agoal-conditioned grasp policy, due to the complexity involved in dexterousgrasping execution. Note that it is very challenging to learn this highlygeneralizable grasp policy that only takes realistic inputs without oraclestates. We thus propose several important innovations, including statecanonicalization, object curriculum, and teacher-student distillation.Integrating the two stages, our final pipeline becomes the first to achieveuniversal generalization for dexterous grasping, demonstrating an averagesuccess rate of more than 60\\% on thousands of object instances, whichsignificantly outperforms all baselines, meanwhile showing only a minimalgeneralization gap.", "output": "UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-supervised semantic segmentation (SSS) has recently gained increasingresearch interest as it can reduce the requirement for large-scalefully-annotated training data. The current methods often suffer from theconfirmation bias from the pseudo-labelling process, which can be alleviated bythe co-training framework. The current co-training-based SSS methods rely onhand-crafted perturbations to prevent the different sub-nets from collapsinginto each other, but these artificial perturbations cannot lead to the optimalsolution. In this work, we propose a new conflict-based cross-view consistency(CCVC) method based on a two-branch co-training framework which aims atenforcing the two sub-nets to learn informative features from irrelevant views.In particular, we first propose a new cross-view consistency (CVC) strategythat encourages the two sub-nets to learn distinct features from the same inputby introducing a feature discrepancy loss, while these distinct features areexpected to generate consistent prediction scores of the input. The CVCstrategy helps to prevent the two sub-nets from stepping into the collapse. Inaddition, we further propose a conflict-based pseudo-labelling (CPL) method toguarantee the model will learn more useful information from conflictingpredictions, which will lead to a stable training process. We validate our newCCVC approach on the SSS benchmark datasets where our method achieves newstate-of-the-art performance. Our code is available athttps://github.com/xiaoyao3302/CCVC.", "output": "Conflict-Based Cross-View Consistency for Semi-Supervised Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies implicit surface reconstruction leveraging differentiableray casting. Previous works such as IDR and NeuS overlook the spatial contextin 3D space when predicting and rendering the surface, thereby may fail tocapture sharp local topologies such as small holes and structures. To mitigatethe limitation, we propose a flexible neural implicit representation leveraginghierarchical voxel grids, namely Neural Deformable Anchor (NeuDA), forhigh-fidelity surface reconstruction. NeuDA maintains the hierarchical anchorgrids where each vertex stores a 3D position (or anchor) instead of the directembedding (or feature). We optimize the anchor grids such that different localgeometry structures can be adaptively encoded. Besides, we dig into thefrequency encoding strategies and introduce a simple hierarchical positionalencoding method for the hierarchical anchor structure to flexibly exploit theproperties of high-frequency and low-frequency geometry and appearance.Experiments on both the DTU and BlendedMVS datasets demonstrate that NeuDA canproduce promising mesh surfaces.", "output": "NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Flow-based methods have demonstrated promising results in addressing theill-posed nature of super-resolution (SR) by learning the distribution ofhigh-resolution (HR) images with the normalizing flow. However, these methodscan only perform a predefined fixed-scale SR, limiting their potential inreal-world applications. Meanwhile, arbitrary-scale SR has gained moreattention and achieved great progress. Nonetheless, previous arbitrary-scale SRmethods ignore the ill-posed problem and train the model with per-pixel L1loss, leading to blurry SR outputs. In this work, we propose \"Local ImplicitNormalizing Flow\" (LINF) as a unified solution to the above problems. LINFmodels the distribution of texture details under different scaling factors withnormalizing flow. Thus, LINF can generate photo-realistic HR images with richtexture details in arbitrary scale factors. We evaluate LINF with extensiveexperiments and show that LINF achieves the state-of-the-art perceptual qualitycompared with prior arbitrary-scale SR methods.", "output": "Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of rig inversion is central in facial animation as it allows fora realistic and appealing performance of avatars. With the increasingcomplexity of modern blendshape models, execution times increase beyondpractically feasible solutions. A possible approach towards a faster solutionis clustering, which exploits the spacial nature of the face, leading to adistributed method. In this paper, we go a step further, involving clustercoupling to get more confident estimates of the overlapping components. Ouralgorithm applies the Alternating Direction Method of Multipliers, sharing theoverlapping weights between the subproblems. The results obtained with thistechnique show a clear advantage over the naive clustered approach, as measuredin different metrics of success and visual inspection. The method applies to anarbitrary clustering of the face. We also introduce a novel method for choosingthe number of clusters in a data-free manner. The method tends to find aclustering such that the resulting clustering graph is sparse but withoutlosing essential information. Finally, we give a new variant of a data-freeclustering algorithm that produces good scores with respect to the mentionedstrategy for choosing the optimal clustering.", "output": "Distributed Solution of the Inverse Rig Problem in Blendshape Facial Animation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose PARASOL, a multi-modal synthesis model that enables disentangled,parametric control of the visual style of the image by jointly conditioningsynthesis on both content and a fine-grained visual style embedding. We train alatent diffusion model (LDM) using specific losses for each modality and adaptthe classifier-free guidance for encouraging disentangled control overindependent content and style modalities at inference time. We leverageauxiliary semantic and style-based search to create training triplets forsupervision of the LDM, ensuring complementarity of content and style cues.PARASOL shows promise for enabling nuanced control over visual style indiffusion models for image creation and stylization, as well as generativesearch where text-based search results may be adapted to more closely matchuser intent by interpolating both content and style descriptors.", "output": "PARASOL: Parametric Style Control for Diffusion Image Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While interacting in the world is a multi-sensory experience, many robotscontinue to predominantly rely on visual perception to map and navigate intheir environments. In this work, we propose Audio-Visual-Language Maps(AVLMaps), a unified 3D spatial map representation for storing cross-modalinformation from audio, visual, and language cues. AVLMaps integrate theopen-vocabulary capabilities of multimodal foundation models pre-trained onInternet-scale data by fusing their features into a centralized 3D voxel grid.In the context of navigation, we show that AVLMaps enable robot systems toindex goals in the map based on multimodal queries, e.g., textual descriptions,images, or audio snippets of landmarks. In particular, the addition of audioinformation enables robots to more reliably disambiguate goal locations.Extensive experiments in simulation show that AVLMaps enable zero-shotmultimodal goal navigation from multimodal prompts and provide 50% betterrecall in ambiguous scenarios. These capabilities extend to mobile robots inthe real world - navigating to landmarks referring to visual, audio, andspatial concepts. Videos and code are available at: https://avlmaps.github.io.", "output": "Audio Visual Language Maps for Robot Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The intrinsic rotation invariance lies at the core of matching point cloudswith handcrafted descriptors. However, it is widely despised by recent deepmatchers that obtain the rotation invariance extrinsically via dataaugmentation. As the finite number of augmented rotations can never span thecontinuous SO(3) space, these methods usually show instability when facingrotations that are rarely seen. To this end, we introduce RoITr, aRotation-Invariant Transformer to cope with the pose variations in the pointcloud matching task. We contribute both on the local and global levels.Starting from the local level, we introduce an attention mechanism embeddedwith Point Pair Feature (PPF)-based coordinates to describe the pose-invariantgeometry, upon which a novel attention-based encoder-decoder architecture isconstructed. We further propose a global transformer with rotation-invariantcross-frame spatial awareness learned by the self-attention mechanism, whichsignificantly improves the feature distinctiveness and makes the model robustwith respect to the low overlap. Experiments are conducted on both the rigidand non-rigid public benchmarks, where RoITr outperforms all thestate-of-the-art models by a considerable margin in the low-overlappingscenarios. Especially when the rotations are enlarged on the challenging3DLoMatch benchmark, RoITr surpasses the existing methods by at least 13 and 5percentage points in terms of Inlier Ratio and Registration Recall,respectively.", "output": "Rotation-Invariant Transformer for Point Cloud Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lithography is fundamental to integrated circuit fabrication, necessitatinglarge computation overhead. The advancement of machine learning (ML)-basedlithography models alleviates the trade-offs between manufacturing processexpense and capability. However, all previous methods regard the lithographysystem as an image-to-image black box mapping, utilizing network parameters tolearn by rote mappings from massive mask-to-aerial or mask-to-resist imagepairs, resulting in poor generalization capability. In this paper, we propose anew ML-based paradigm disassembling the rigorous lithographic model intonon-parametric mask operations and learned optical kernels containingdeterminant source, pupil, and lithography information. By optimizingcomplex-valued neural fields to perform optical kernel regression fromcoordinates, our method can accurately restore lithography system using asmall-scale training dataset with fewer parameters, demonstrating superiorgeneralization capability as well. Experiments show that our framework can use31% of parameters while achieving 69$\\times$ smaller mean squared error with1.3$\\times$ higher throughput than the state-of-the-art.", "output": "Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Person re-ID matches persons across multiple non-overlapping cameras. Despitethe increasing deployment of airborne platforms in surveillance, currentexisting person re-ID benchmarks' focus is on ground-ground matching and verylimited efforts on aerial-aerial matching. We propose a new benchmark dataset -AG-ReID, which performs person re-ID matching in a new setting: across aerialand ground cameras. Our dataset contains 21,983 images of 388 identities and 15soft attributes for each identity. The data was collected by a UAV flying ataltitudes between 15 to 45 meters and a ground-based CCTV camera on auniversity campus. Our dataset presents a novel elevated-viewpoint challengefor person re-ID due to the significant difference in person appearance acrossthese cameras. We propose an explainable algorithm to guide the person re-IDmodel's training with soft attributes to address this challenge. Experimentsdemonstrate the efficacy of our method on the aerial-ground person re-ID task.The dataset will be published and the baseline codes will be open-sourced athttps://github.com/huynguyen792/AG-ReID to facilitate research in this area.", "output": "Aerial-Ground Person Re-ID."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multiple robots could perceive a scene (e.g., detect objects) collaborativelybetter than individuals, although easily suffer from adversarial attacks whenusing deep learning. This could be addressed by the adversarial defense, butits training requires the often-unknown attacking mechanism. Differently, wepropose ROBOSAC, a novel sampling-based defense strategy generalizable tounseen attackers. Our key idea is that collaborative perception should lead toconsensus rather than dissensus in results compared to individual perception.This leads to our hypothesize-and-verify framework: perception results with andwithout collaboration from a random subset of teammates are compared untilreaching a consensus. In such a framework, more teammates in the sampled subsetoften entail better perception performance but require longer sampling time toreject potential attackers. Thus, we derive how many sampling trials are neededto ensure the desired size of an attacker-free subset, or equivalently, themaximum size of such a subset that we can successfully sample within a givennumber of trials. We validate our method on the task of collaborative 3D objectdetection in autonomous driving scenarios.", "output": "Among Us: Adversarially Robust Collaborative Perception by Consensus."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce an Extended Textual Conditioning space in text-to-image models,referred to as $P+$. This space consists of multiple textual conditions,derived from per-layer prompts, each corresponding to a layer of the denoisingU-net of the diffusion model.We show that the extended space provides greater disentangling and controlover image synthesis. We further introduce Extended Textual Inversion (XTI),where the images are inverted into $P+$, and represented by per-layer tokens.We show that XTI is more expressive and precise, and converges faster thanthe original Textual Inversion (TI) space. The extended inversion method doesnot involve any noticeable trade-off between reconstruction and editability andinduces more regular inversions.We conduct a series of extensive experiments to analyze and understand theproperties of the new space, and to showcase the effectiveness of our methodfor personalizing text-to-image models. Furthermore, we utilize the uniqueproperties of this space to achieve previously unattainable results inobject-style mixing using text-to-image models. Project page:https://prompt-plus.github.io", "output": "$P+$: Extended Textual Conditioning in Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While generative modeling has been ubiquitous in natural language processingand computer vision, its application to image retrieval remains unexplored. Inthis paper, we recast image retrieval as a form of generative modeling byemploying a sequence-to-sequence model, contributing to the current unifiedtheme. Our framework, IRGen, is a unified model that enables end-to-enddifferentiable search, thus achieving superior performance thanks to directoptimization. While developing IRGen we tackle the key technical challenge ofconverting an image into quite a short sequence of semantic units in order toenable efficient and effective retrieval. Empirical experiments demonstratethat our model yields significant improvement over three commonly usedbenchmarks, for example, 22.9\\% higher than the best baseline method inprecision@10 on In-shop dataset with comparable recall@10 score.", "output": "IRGen: Generative Modeling for Image Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a new methodology to alleviate the fundamental trade-offbetween accuracy and latency in spiking neural networks (SNNs). The approachinvolves decoding confidence information over time from the SNN outputs andusing it to develop a decision-making agent that can dynamically determine whento terminate each inference.The proposed method, Dynamic Confidence, provides several significantbenefits to SNNs. 1. It can effectively optimize latency dynamically atruntime, setting it apart from many existing low-latency SNN algorithms. Ourexperiments on CIFAR-10 and ImageNet datasets have demonstrated an average 40%speedup across eight different settings after applying Dynamic Confidence. 2.The decision-making agent in Dynamic Confidence is straightforward to constructand highly robust in parameter space, making it extremely easy to implement. 3.The proposed method enables visualizing the potential of any given SNN, whichsets a target for current SNNs to approach. For instance, if an SNN canterminate at the most appropriate time point for each input sample, a ResNet-50SNN can achieve an accuracy as high as 82.47% on ImageNet within just 4.71 timesteps on average. Unlocking the potential of SNNs needs a highly-reliabledecision-making agent to be constructed and fed with a high-quality estimationof ground truth. In this regard, Dynamic Confidence represents a meaningfulstep toward realizing the potential of SNNs.", "output": "Unleashing the Potential of Spiking Neural Networks by Dynamic Confidence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents our Facial Action Units (AUs) recognition submission tothe fifth Affective Behavior Analysis in-the-wild Competition (ABAW). Ourapproach consists of three main modules: (i) a pre-trained facialrepresentation encoder which produce a strong facial representation from eachinput face image in the input sequence; (ii) an AU-specific feature generatorthat specifically learns a set of AU features from each facial representation;and (iii) a spatio-temporal graph learning module that constructs aspatio-temporal graph representation. This graph representation describes AUscontained in all frames and predicts the occurrence of each AU based on boththe modeled spatial information within the corresponding face and the learnedtemporal dynamics among frames. The experimental results show that our approachoutperformed the baseline and the spatio-temporal graph representation learningallows our model to generate the best results among all ablated systems. Ourmodel ranks at the 4th place in the AU recognition track at the 5th ABAWCompetition.", "output": "Spatio-Temporal AU Relational Graph Representation Learning For Facial Action Units Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Depth-from-defocus (DFD), modeling the relationship between depth and defocuspattern in images, has demonstrated promising performance in depth estimation.Recently, several self-supervised works try to overcome the difficulties inacquiring accurate depth ground-truth. However, they depend on the all-in-focus(AIF) images, which cannot be captured in real-world scenarios. Such limitationdiscourages the applications of DFD methods. To tackle this issue, we propose acompletely self-supervised framework that estimates depth purely from a sparsefocal stack. We show that our framework circumvents the needs for the depth andAIF image ground-truth, and receives superior predictions, thus closing the gapbetween the theoretical success of DFD works and their applications in the realworld. In particular, we propose (i) a more realistic setting for DFD tasks,where no depth or AIF image ground-truth is available; (ii) a novelself-supervision framework that provides reliable predictions of depth and AIFimage under the challenging setting. The proposed framework uses a neural modelto predict the depth and AIF image, and utilizes an optical model to validateand refine the prediction. We verify our framework on three benchmark datasetswith rendered focal stacks and real focal stacks. Qualitative and quantitativeevaluations show that our method provides a strong baseline for self-supervisedDFD tasks.", "output": "Fully Self-Supervised Depth Estimation from Defocus Clue."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visible-modal object tracking gives rise to a series of downstreammulti-modal tracking tributaries. To inherit the powerful representations ofthe foundation model, a natural modus operandi for multi-modal tracking is fullfine-tuning on the RGB-based parameters. Albeit effective, this manner is notoptimal due to the scarcity of downstream data and poor transferability, etc.In this paper, inspired by the recent success of the prompt learning inlanguage models, we develop Visual Prompt multi-modal Tracking (ViPT), whichlearns the modal-relevant prompts to adapt the frozen pre-trained foundationmodel to various downstream multimodal tracking tasks. ViPT finds a better wayto stimulate the knowledge of the RGB-based model that is pre-trained at scale,meanwhile only introducing a few trainable parameters (less than 1% of modelparameters). ViPT outperforms the full fine-tuning paradigm on multipledownstream tracking tasks including RGB+Depth, RGB+Thermal, and RGB+Eventtracking. Extensive experiments show the potential of visual prompt learningfor multi-modal tracking, and ViPT can achieve state-of-the-art performancewhile satisfying parameter efficiency. Code and models are available athttps://github.com/jiawen-zhu/ViPT.", "output": "Visual Prompt Multi-Modal Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning to predict agent motions with relationship reasoning is importantfor many applications. In motion prediction tasks, maintaining motionequivariance under Euclidean geometric transformations and invariance of agentinteraction is a critical and fundamental principle. However, such equivarianceand invariance properties are overlooked by most existing methods. To fill thisgap, we propose EqMotion, an efficient equivariant motion prediction model withinvariant interaction reasoning. To achieve motion equivariance, we propose anequivariant geometric feature learning module to learn a Euclideantransformable feature through dedicated designs of equivariant operations. Toreason agent's interactions, we propose an invariant interaction reasoningmodule to achieve a more stable interaction modeling. To further promote morecomprehensive motion features, we propose an invariant pattern feature learningmodule to learn an invariant pattern feature, which cooperates with theequivariant geometric feature to enhance network expressiveness. We conductexperiments for the proposed model on four distinct scenarios: particledynamics, molecule dynamics, human skeleton motion prediction and pedestriantrajectory prediction. Experimental results show that our method is not onlygenerally applicable, but also achieves state-of-the-art predictionperformances on all the four tasks, improving by 24.0/30.1/8.6/9.2%. Code isavailable at https://github.com/MediaBrain-SJTU/EqMotion.", "output": "EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic Scene Completion (SSC) aims to simultaneously predict the volumetricoccupancy and semantic category of a 3D scene. In this paper, we propose areal-time semantic scene completion method with a feature aggregation strategyand conditioned prediction module. Feature aggregation fuses feature withdifferent receptive fields and gathers context to improve scene completionperformance. And the conditioned prediction module adopts a two-step predictionscheme that takes volumetric occupancy as a condition to enhance semanticcompletion prediction. We conduct experiments on three recognized benchmarksNYU, NYUCAD, and SUNCG. Our method achieves competitive performance at a speedof 110 FPS on one GTX 1080 Ti GPU.", "output": "Real-time 3D Semantic Scene Completion Via Feature Aggregation and Conditioned Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding and manipulating deformable objects (e.g., ropes and fabrics)is an essential yet challenging task with broad applications. Difficulties comefrom complex states and dynamics, diverse configurations and high-dimensionalaction space of deformable objects. Besides, the manipulation tasks usuallyrequire multiple steps to accomplish, and greedy policies may easily lead tolocal optimal states. Existing studies usually tackle this problem usingreinforcement learning or imitating expert demonstrations, with limitations inmodeling complex states or requiring hand-crafted expert policies. In thispaper, we study deformable object manipulation using dense visual affordance,with generalization towards diverse states, and propose a novel kind offoresightful dense affordance, which avoids local optima by estimating states'values for long-term manipulation. We propose a framework for learning thisrepresentation, with novel designs such as multi-stage stable learning andefficient self-supervised data collection without experts. Experimentsdemonstrate the superiority of our proposed foresightful dense affordance.Project page: https://hyperplane-lab.github.io/DeformableAffordance", "output": "Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A central challenge in human pose estimation, as well as in many othermachine learning and prediction tasks, is the generalization problem. Thelearned network does not have the capability to characterize the predictionerror, generate feedback information from the test sample, and correct theprediction error on the fly for each individual test sample, which results indegraded performance in generalization. In this work, we introduce aself-correctable and adaptable inference (SCAI) method to address thegeneralization challenge of network prediction and use human pose estimation asan example to demonstrate its effectiveness and performance. We learn acorrection network to correct the prediction result conditioned by a fitnessfeedback error. This feedback error is generated by a learned fitness feedbacknetwork which maps the prediction result to the original input domain andcompares it against the original input. Interestingly, we find that thisself-referential feedback error is highly correlated with the actual predictionerror. This strong correlation suggests that we can use this error as feedbackto guide the correction process. It can be also used as a loss function toquickly adapt and optimize the correction network during the inference process.Our extensive experimental results on human pose estimation demonstrate thatthe proposed SCAI method is able to significantly improve the generalizationcapability and performance of human pose estimation.", "output": "Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autoregressive transformers have shown remarkable success in videogeneration. However, the transformers are prohibited from directly learning thelong-term dependency in videos due to the quadratic complexity ofself-attention, and inherently suffering from slow inference time and errorpropagation due to the autoregressive process. In this paper, we proposeMemory-efficient Bidirectional Transformer (MeBT) for end-to-end learning oflong-term dependency in videos and fast inference. Based on recent advances inbidirectional transformers, our method learns to decode the entirespatio-temporal volume of a video in parallel from partially observed patches.The proposed transformer achieves a linear time complexity in both encoding anddecoding, by projecting observable context tokens into a fixed number of latenttokens and conditioning them to decode the masked tokens through thecross-attention. Empowered by linear complexity and bidirectional modeling, ourmethod demonstrates significant improvement over the autoregressiveTransformers for generating moderately long videos in both quality and speed.Videos and code are available at https://sites.google.com/view/mebt-cvpr2023 .", "output": "Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised audio-visual source localization aims to locate sound-sourceobjects in video frames without extra annotations. Recent methods oftenapproach this goal with the help of contrastive learning, which assumes onlythe audio and visual contents from the same video are positive samples for eachother. However, this assumption would suffer from false negative samples inreal-world training. For example, for an audio sample, treating the frames fromthe same audio class as negative samples may mislead the model and thereforeharm the learned representations e.g., the audio of a siren wailing mayreasonably correspond to the ambulances in multiple images). Based on thisobservation, we propose a new learning strategy named False Negative AwareContrastive (FNAC) to mitigate the problem of misleading the training with suchfalse negative samples. Specifically, we utilize the intra-modal similaritiesto identify potentially similar samples and construct corresponding adjacencymatrices to guide contrastive learning. Further, we propose to strengthen therole of true negative samples by explicitly leveraging the visual features ofsound sources to facilitate the differentiation of authentic sounding sourceregions. FNAC achieves state-of-the-art performances on Flickr-SoundNet,VGG-Sound, and AVSBench, which demonstrates the effectiveness of our method inmitigating the false negative issue. The code is available at\\url{https://github.com/OpenNLPLab/FNAC_AVL}.", "output": "Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) has been gaining popularity as a collaborativelearning framework to train deep learning-based object detection models over adistributed population of clients. Despite its advantages, FL is vulnerable tomodel hijacking. The attacker can control how the object detection systemshould misbehave by implanting Trojaned gradients using only a small number ofcompromised clients in the collaborative learning process. This paperintroduces STDLens, a principled approach to safeguarding FL against suchattacks. We first investigate existing mitigation mechanisms and analyze theirfailures caused by the inherent errors in spatial clustering analysis ongradients. Based on the insights, we introduce a three-tier forensic frameworkto identify and expel Trojaned gradients and reclaim the performance over thecourse of FL. We consider three types of adaptive attacks and demonstrate therobustness of STDLens against advanced adversaries. Extensive experiments showthat STDLens can protect FL against different model hijacking attacks andoutperform existing methods in identifying and removing Trojaned gradients withsignificantly higher precision and much lower false-positive rates.", "output": "STDLens: Model Hijacking-Resilient Federated Learning for Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have explored the use of weight sparsity to improve the trainingefficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).These works aim to reduce training FLOPs but training with sparse weights oftenleads to accuracy loss or requires longer training schedules, making theresulting training efficiency less clear. In contrast, we focus on usingsparsity to increase accuracy while using the same FLOPs as the dense model andshow training efficiency gains through higher accuracy. In this work, weintroduce Sparse-IFT, a family of Sparse Iso-FLOP Transformations which areused as drop-in replacements for dense layers to improve their representationalcapacity and FLOP efficiency. Each transformation is parameterized by a singlehyperparameter (sparsity level) and provides a larger search space to findoptimal sparse masks. Without changing any training hyperparameters, replacingdense layers with Sparse-IFT leads to significant improvements across computervision (CV) and natural language processing (NLP) tasks, including ResNet-18 onImageNet (+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matchinglarger dense model variants that use 2x or more FLOPs. To our knowledge, thisis the first work to demonstrate the use of sparsity for improving the accuracyof dense models via a simple-to-use set of sparse transformations. Code isavailable at: https://github.com/CerebrasResearch/Sparse-IFT.", "output": "Sparse Iso-FLOP Transformations for Maximizing Training Efficiency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several works have developed end-to-end pipelines for generating lip-syncedtalking faces with various real-world applications, such as teaching andlanguage translation in videos. However, these prior works fail to createrealistic-looking videos since they focus little on people's expressions andemotions. Moreover, these methods' effectiveness largely depends on the facesin the training dataset, which means they may not perform well on unseen faces.To mitigate this, we build a talking face generation framework conditioned on acategorical emotion to generate videos with appropriate expressions, makingthem more realistic and convincing. With a broad range of six emotions, i.e.,\\emph{happiness}, \\emph{sadness}, \\emph{fear}, \\emph{anger}, \\emph{disgust},and \\emph{neutral}, we show that our model can adapt to arbitrary identities,emotions, and languages. Our proposed framework is equipped with auser-friendly web interface with a real-time experience for talking facegeneration with emotions. We also conduct a user study for subjectiveevaluation of our interface's usability, design, and functionality. Projectpage: https://midas.iiitd.edu.in/emo/", "output": "Emotionally Enhanced Talking Face Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the rapid development of depth learning, object detection in aviaticremote sensing images has become increasingly popular in recent years. Most ofthe current Anchor Free detectors based on key point detection samplingdirectly regression and classification features, with the design of object lossfunction based on the horizontal bounding box. It is more challenging forcomplex and diverse aviatic remote sensing object. In this paper, we propose anAnchor Free aviatic remote sensing object detector (BWP-Det) to detect rotatingand multi-scale object. Specifically, we design a interactivedouble-branch(IDB) up-sampling network, in which one branch graduallyup-sampling is used for the prediction of Heatmap, and the other branch is usedfor the regression of boundary box parameters. We improve a weightedmulti-scale convolution (WmConv) in order to highlight the difference betweenforeground and background. We extracted Pixel level attention features from themiddle layer to guide the two branches to pay attention to effective objectinformation in the sampling process. Finally, referring to the calculation ideaof horizontal IoU, we design a rotating IoU based on the split polar coordinateplane, namely JIoU, which is expressed as the intersection ratio followingdiscretization of the inner ellipse of the rotating bounding box, to solve thecorrelation between angle and side length in the regression process of therotating bounding box. Ultimately, BWP-Det, our experiments on DOTA, UCAS-AODand NWPU VHR-10 datasets show, achieves advanced performance with simplermodels and fewer regression parameters.", "output": "Anchor Free remote sensing detector based on solving discrete polar coordinate equation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inspired by the success of volumetric 3D pose estimation, some recent humanmesh estimators propose to estimate 3D skeletons as intermediaterepresentations, from which, the dense 3D meshes are regressed by exploitingthe mesh topology. However, body shape information is lost in extractingskeletons, leading to mediocre performance. The advanced motion capture systemssolve the problem by placing dense physical markers on the body surface, whichallows to extract realistic meshes from their non-rigid motions. However, theycannot be applied to wild images without markers. In this work, we present anintermediate representation, named virtual markers, which learns 64 landmarkkeypoints on the body surface based on the large-scale mocap data in agenerative style, mimicking the effects of physical markers. The virtualmarkers can be accurately detected from wild images and can reconstruct theintact meshes with realistic shapes by simple interpolation. Our approachoutperforms the state-of-the-art methods on three datasets. In particular, itsurpasses the existing methods by a notable margin on the SURREAL dataset,which has diverse body shapes. Code is available athttps://github.com/ShirleyMaxx/VirtualMarker.", "output": "3D Human Mesh Estimation from Virtual Markers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual place recognition (VPR) is a fundamental task of computer vision forvisual localization. Existing methods are trained using image pairs that eitherdepict the same place or not. Such a binary indication does not considercontinuous relations of similarity between images of the same place taken fromdifferent positions, determined by the continuous nature of camera pose. Thebinary similarity induces a noisy supervision signal into the training of VPRmethods, which stall in local minima and require expensive hard miningalgorithms to guarantee convergence. Motivated by the fact that two images ofthe same place only partially share visual cues due to camera pose differences,we deploy an automatic re-annotation strategy to re-label VPR datasets. Wecompute graded similarity labels for image pairs based on availablelocalization metadata. Furthermore, we propose a new Generalized ContrastiveLoss (GCL) that uses graded similarity labels for training contrastivenetworks. We demonstrate that the use of the new labels and GCL allow todispense from hard-pair mining, and to train image descriptors that performbetter in VPR by nearest neighbor search, obtaining superior or comparableresults than methods that require expensive hard-pair mining and re-rankingtechniques. Code and models available at:https://github.com/marialeyvallina/generalized_contrastive_loss", "output": "Data-efficient Large Scale Place Recognition with Graded Similarity Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we formally address universal object detection, which aims todetect every scene and predict every category. The dependence on humanannotations, the limited visual information, and the novel categories in theopen world severely restrict the universality of traditional detectors. Wepropose UniDetector, a universal object detector that has the ability torecognize enormous categories in the open world. The critical points for theuniversality of UniDetector are: 1) it leverages images of multiple sources andheterogeneous label spaces for training through the alignment of image and textspaces, which guarantees sufficient information for universal representations.2) it generalizes to the open world easily while keeping the balance betweenseen and unseen classes, thanks to abundant information from both vision andlanguage modalities. 3) it further promotes the generalization ability to novelcategories through our proposed decoupling training manner and probabilitycalibration. These contributions allow UniDetector to detect over 7kcategories, the largest measurable category size so far, with only about 500classes participating in training. Our UniDetector behaves the strong zero-shotgeneralization ability on large-vocabulary datasets like LVIS, ImageNetBoxes,and VisualGenome - it surpasses the traditional supervised baselines by morethan 4\\% on average without seeing any corresponding images. On 13 publicdetection datasets with various scenes, UniDetector also achievesstate-of-the-art performance with only a 3\\% amount of training data.", "output": "Detecting Everything in the Open World: Towards Universal Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-line-of-sight (NLOS) tracking has drawn increasing attention in recentyears, due to its ability to detect object motion out of sight. Most previousworks on NLOS tracking rely on active illumination, e.g., laser, and sufferfrom high cost and elaborate experimental conditions. Besides, these techniquesare still far from practical application due to oversimplified settings. Incontrast, we propose a purely passive method to track a person walking in aninvisible room by only observing a relay wall, which is more in line with realapplication scenarios, e.g., security. To excavate imperceptible changes invideos of the relay wall, we introduce difference frames as an essentialcarrier of temporal-local motion messages. In addition, we propose PAC-Net,which consists of alternating propagation and calibration, making it capable ofleveraging both dynamic and static messages on a frame-level granularity. Toevaluate the proposed method, we build and publish the first dynamic passiveNLOS tracking dataset, NLOS-Track, which fills the vacuum of realistic NLOSdatasets. NLOS-Track contains thousands of NLOS video clips and correspondingtrajectories. Both real-shot and synthetic data are included. Our codes anddataset are available at https://againstentropy.github.io/NLOS-Track/.", "output": "Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Context-Aware Emotion Recognition (CAER) is a crucial and challenging taskthat aims to perceive the emotional states of the target person with contextualinformation. Recent approaches invariably focus on designing sophisticatedarchitectures or mechanisms to extract seemingly meaningful representationsfrom subjects and contexts. However, a long-overlooked issue is that a contextbias in existing datasets leads to a significantly unbalanced distribution ofemotional states among different context scenarios. Concretely, the harmfulbias is a confounder that misleads existing models to learn spuriouscorrelations based on conventional likelihood estimation, significantlylimiting the models' performance. To tackle the issue, this paper provides acausality-based perspective to disentangle the models from the impact of suchbias, and formulate the causalities among variables in the CAER task via atailored causal graph. Then, we propose a Contextual Causal Intervention Module(CCIM) based on the backdoor adjustment to de-confound the confounder andexploit the true causal effect for model training. CCIM is plug-in andmodel-agnostic, which improves diverse state-of-the-art approaches byconsiderable margins. Extensive experiments on three benchmark datasetsdemonstrate the effectiveness of our CCIM and the significance of causalinsight.", "output": "Context De-confounded Emotion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The performance of convolutional neural networks has continued to improveover the last decade. At the same time, as model complexity grows, it becomesincreasingly more difficult to explain model decisions. Such explanations maybe of critical importance for reliable operation of human-machine pairingsetups, or for model selection when the \"best\" model among manyequally-accurate models must be established. Saliency maps represent onepopular way of explaining model decisions by highlighting image regions modelsdeem important when making a prediction. However, examining salience maps atscale is not practical. In this paper, we propose five novel methods ofleveraging model salience to explain a model behavior at scale. These methodsask: (a) what is the average entropy for a model's salience maps, (b) how doesmodel salience change when fed out-of-set samples, (c) how closely does modelsalience follow geometrical transformations, (d) what is the stability of modelsalience across independent training runs, and (e) how does model saliencereact to salience-guided image degradations. To assess the proposed measures ona concrete and topical problem, we conducted a series of experiments for thetask of synthetic face detection with two types of models: those trainedtraditionally with cross-entropy loss, and those guided by human salience whentraining to increase model generalizability. These two types of models arecharacterized by different, interpretable properties of their salience maps,which allows for the evaluation of the correctness of the proposed measures. Weoffer source codes for each measure along with this paper.", "output": "Explain To Me: Salience-Based Explainability for Synthetic Face Detection Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When a small number of poisoned samples are injected into the trainingdataset of a deep neural network, the network can be induced to exhibitmalicious behavior during inferences, which poses potential threats toreal-world applications. While they have been intensively studied inclassification, backdoor attacks on semantic segmentation have been largelyoverlooked. Unlike classification, semantic segmentation aims to classify everypixel within a given image. In this work, we explore backdoor attacks onsegmentation models to misclassify all pixels of a victim class by injecting aspecific trigger on non-victim pixels during inferences, which is dubbedInfluencer Backdoor Attack (IBA). IBA is expected to maintain theclassification accuracy of non-victim pixels and misleads classifications ofall victim pixels in every single inference. Specifically, we consider twotypes of IBA scenarios, i.e., 1) Free-position IBA: the trigger can bepositioned freely except for pixels of the victim class, and 2) Long-distanceIBA: the trigger can only be positioned somewhere far from victim pixels, giventhe possible practical constraint. Based on the context aggregation ability ofsegmentation models, we propose techniques to improve IBA for the scenarios.Concretely, for free-position IBA, we propose a simple, yet effective NearestNeighbor trigger injection strategy for poisoned sample creation. Forlong-distance IBA, we propose a novel Pixel Random Labeling strategy. Ourextensive experiments reveal that current segmentation models do suffer frombackdoor attacks, and verify that our proposed techniques can further increaseattack performance.", "output": "Influencer Backdoor Attack on Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Diffusion Probabilistic Model (DPM) has emerged as a highly effectivegenerative model in the field of computer vision. Its intermediate latentvectors offer rich semantic information, making it an attractive option forvarious downstream tasks such as segmentation and detection. In order toexplore its potential further, we have taken a step forward and considered amore complex scenario in the medical image domain, specifically, under anunsupervised adaptation condition. To this end, we propose a Diffusion-basedand Prototype-guided network (DP-Net) for unsupervised domain adaptivesegmentation. Concretely, our DP-Net consists of two stages: 1) DistributionAligned Diffusion (DADiff), which involves training a domain discriminator tominimize the difference between the intermediate features generated by the DPM,thereby aligning the inter-domain distribution; and 2) Prototype-guidedConsistency Learning (PCL), which utilizes feature centroids as prototypes andapplies a prototype-guided loss to ensure that the segmentor learns consistentcontent from both source and target domains. Our approach is evaluated onfundus datasets through a series of experiments, which demonstrate that theperformance of the proposed method is reliable and outperforms state-of-the-artmethods. Our work presents a promising direction for using DPM in complexmedical image scenarios, opening up new possibilities for further research inmedical imaging.", "output": "Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Music-driven choreography is a challenging problem with a wide variety ofindustrial applications. Recently, many methods have been proposed tosynthesize dance motions from music for a single dancer. However, generatingdance motion for a group remains an open problem. In this paper, we present$\\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dancegeneration. Unlike existing datasets that only support single dance, our newdataset contains group dance videos, hence supporting the study of groupchoreography. We propose a semi-autonomous labeling method with humans in theloop to obtain the 3D ground truth for our dataset. The proposed datasetconsists of 16.7 hours of paired music and 3D motion from in-the-wild videos,covering 7 dance styles and 16 music genres. We show that naively applyingsingle dance generation technique to creating group dance motion may lead tounsatisfactory results, such as inconsistent movements and collisions betweendancers. Based on our new dataset, we propose a new method that takes an inputmusic sequence and a set of 3D positions of dancers to efficiently producemultiple group-coherent choreographies. We propose new evaluation metrics formeasuring group dance quality and perform intensive experiments to demonstratethe effectiveness of our method. Our project facilitates future research ongroup dance generation and is available at:https://aioz-ai.github.io/AIOZ-GDANCE/", "output": "Music-Driven Group Choreography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a scene-level inverse rendering framework that uses multi-viewimages to decompose the scene into geometry, a SVBRDF, and 3D spatially-varyinglighting. Because multi-view images provide a variety of information about thescene, multi-view images in object-level inverse rendering have been taken forgranted. However, owing to the absence of multi-view HDR synthetic dataset,scene-level inverse rendering has mainly been studied using single-view image.We were able to successfully perform scene-level inverse rendering usingmulti-view images by expanding OpenRooms dataset and designing efficientpipelines to handle multi-view images, and splitting spatially-varyinglighting. Our experiments show that the proposed method not only achievesbetter performance than single-view-based methods, but also achieves robustperformance on unseen real-world scene. Also, our sophisticated 3Dspatially-varying lighting volume allows for photorealistic object insertion inany 3D location.", "output": "MAIR: Multi-view Attention Inverse Rendering with 3D Spatially-Varying Lighting Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is important for deep reinforcement learning (DRL) algorithms to transfertheir learned policies to new environments that have different visual inputs.In this paper, we introduce Prompt based Proximal Policy Optimization($P^{3}O$), a three-stage DRL algorithm that transfers visual representationsfrom a target to a source environment by applying prompting. The process of$P^{3}O$ consists of three stages: pre-training, prompting, and predicting. Inparticular, we specify a prompt-transformer for representation conversion andpropose a two-step training process to train the prompt-transformer for thetarget environment, while the rest of the DRL pipeline remains unchanged. Weimplement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. Theexperimental results show that $P^{3}O$ outperforms the state-of-the-art visualtransferring schemes. In particular, $P^{3}O$ allows the learned policies toperform well in environments with different visual inputs, which is much moreeffective than retraining the policies in these environments.", "output": "$P^{3}O$: Transferring Visual Representations for Reinforcement Learning via Prompting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformers have achieved widespread success in computer vision. At theirheart, there is a Self-Attention (SA) mechanism, an inductive bias thatassociates each token in the input with every other token through a weightedbasis. The standard SA mechanism has quadratic complexity with the sequencelength, which impedes its utility to long sequences appearing in highresolution vision. Recently, inspired by operator learning for PDEs, AdaptiveFourier Neural Operators (AFNO) were introduced for high resolution attentionbased on global convolution that is efficiently implemented via FFT. However,the AFNO global filtering cannot well represent small and moderate scalestructures that commonly appear in natural images. To leverage thecoarse-to-fine scale structures we introduce a Multiscale Wavelet Attention(MWA) by leveraging wavelet neural operators which incurs linear complexity inthe sequence size. We replace the attention in ViT with MWA and our experimentswith CIFAR and ImageNet classification demonstrate significant improvement overalternative Fourier-based attentions such as AFNO and Global Filter Network(GFN).", "output": "Multiscale Attention via Wavelet Neural Operators for Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive Language-Image Pre-training, benefiting from large-scaleunlabeled text-image pairs, has demonstrated great performance in open-worldvision understanding tasks. However, due to the limited Text-3D data pairs,adapting the success of 2D Vision-Language Models (VLM) to the 3D space remainsan open problem. Existing works that leverage VLM for 3D understandinggenerally resort to constructing intermediate 2D representations for the 3Ddata, but at the cost of losing 3D geometry information. To take a step towardopen-world 3D vision understanding, we propose Contrastive Language-Image-PointCloud Pretraining (CLIP$^2$) to directly learn the transferable 3D point cloudrepresentation in realistic scenarios with a novel proxy alignment mechanism.Specifically, we exploit naturally-existed correspondences in 2D and 3Dscenarios, and build well-aligned and instance-based text-image-point proxiesfrom those complex scenarios. On top of that, we propose a cross-modalcontrastive objective to learn semantic and instance-level aligned point cloudrepresentation. Experimental results on both indoor and outdoor scenarios showthat our learned 3D representation has great transfer ability in downstreamtasks, including zero-shot and few-shot 3D recognition, which boosts thestate-of-the-art methods by large margins. Furthermore, we provide analyses ofthe capability of different representations in real scenarios and present theoptional ensemble scheme.", "output": "CLIP$^2$: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video captioning aims to describe the content of videos using naturallanguage. Although significant progress has been made, there is still much roomto improve the performance for real-world applications, mainly due to thelong-tail words challenge. In this paper, we propose a text with knowledgegraph augmented transformer (TextKG) for video captioning. Notably, TextKG is atwo-stream transformer, formed by the external stream and internal stream. Theexternal stream is designed to absorb additional knowledge, which models theinteractions between the additional knowledge, e.g., pre-built knowledge graph,and the built-in information of videos, e.g., the salient object regions,speech transcripts, and video captions, to mitigate the long-tail wordschallenge. Meanwhile, the internal stream is designed to exploit themulti-modality information in videos (e.g., the appearance of video frames,speech transcripts, and video captions) to ensure the quality of captionresults. In addition, the cross attention mechanism is also used in between thetwo streams for sharing information. In this way, the two streams can help eachother for more accurate results. Extensive experiments conducted on fourchallenging video captioning datasets, i.e., YouCookII, ActivityNet Captions,MSRVTT, and MSVD, demonstrate that the proposed method performs favorablyagainst the state-of-the-art methods. Specifically, the proposed TextKG methodoutperforms the best published results by improving 18.7% absolute CIDEr scoreson the YouCookII dataset.", "output": "Text with Knowledge Graph Augmented Transformer for Video Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art machine learning models often learn spurious correlationsembedded in the training data. This poses risks when deploying these models forhigh-stake decision-making, such as in medical applications like skin cancerdetection. To tackle this problem, we propose Reveal to Revise (R2R), aframework entailing the entire eXplainable Artificial Intelligence (XAI) lifecycle, enabling practitioners to iteratively identify, mitigate, and(re-)evaluate spurious model behavior with a minimal amount of humaninteraction. In the first step (1), R2R reveals model weaknesses by findingoutliers in attributions or through inspection of latent concepts learned bythe model. Secondly (2), the responsible artifacts are detected and spatiallylocalized in the input data, which is then leveraged to (3) revise the modelbehavior. Concretely, we apply the methods of RRR, CDEP and ClArC for modelcorrection, and (4) (re-)evaluate the model's performance and remainingsensitivity towards the artifact. Using two medical benchmark datasets forMelanoma detection and bone age estimation, we apply our R2R framework to VGG,ResNet and EfficientNet architectures and thereby reveal and correct realdataset-intrinsic artifacts, as well as synthetic variants in a controlledsetting. Completing the XAI life cycle, we demonstrate multiple R2R iterationsto mitigate different biases. Code is available onhttps://github.com/maxdreyer/Reveal2Revise.", "output": "Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked Image Modeling (MIM) has achieved impressive representativeperformance with the aim of reconstructing randomly masked images. Despite theempirical success, most previous works have neglected the important fact thatit is unreasonable to force the model to reconstruct something beyond recovery,such as those masked objects. In this work, we show that uniformly randommasking widely used in previous works unavoidably loses some key objects andchanges original semantic information, resulting in a misalignment problem andhurting the representative learning eventually. To address this issue, weaugment MIM with a new masking strategy namely the DPPMask by substituting therandom process with Determinantal Point Process (DPPs) to reduce the semanticchange of the image after masking. Our method is simple yet effective andrequires no extra learnable parameters when implemented within variousframeworks. In particular, we evaluate our method on two representative MIMframeworks, MAE and iBOT. We show that DPPMask surpassed random sampling underboth lower and higher masking ratios, indicating that DPPMask makes thereconstruction task more reasonable. We further test our method on thebackground challenge and multi-class classification tasks, showing that ourmethod is more robust at various tasks.", "output": "DPPMask: Masked Image Modeling with Determinantal Point Processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing defense methods against adversarial attacks can be categorized intotraining time and test time defenses. Training time defense, i.e., adversarialtraining, requires a significant amount of extra time for training and is oftennot able to be generalized to unseen attacks. On the other hand, test timedefense by test time weight adaptation requires access to perform gradientdescent on (part of) the model weights, which could be infeasible for modelswith frozen weights. To address these challenges, we propose DRAM, a noveldefense method to Detect and Reconstruct multiple types of Adversarial attacksvia Masked autoencoder (MAE). We demonstrate how to use MAE losses to build aKS-test to detect adversarial attacks. Moreover, the MAE losses can be used torepair adversarial samples from unseen attack types. In this sense, DRAMneither requires model weight updates in test time nor augments the trainingset with more adversarial samples. Evaluating DRAM on the large-scale ImageNetdata, we achieve the best detection rate of 82% on average on eight types ofadversarial attacks compared with other detection baselines. Forreconstruction, DRAM improves the robust accuracy by 6% ~ 41% for StandardResNet50 and 3% ~ 8% for Robust ResNet50 compared with other self-supervisiontasks, such as rotation prediction and contrastive learning.", "output": "Test-time Defense against Adversarial Attacks: Detection and Reconstruction of Adversarial Examples via Masked Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The advent of open-source AI communities has produced a cornucopia ofpowerful text-guided diffusion models that are trained on various datasets.While few explorations have been conducted on ensembling such models to combinetheir strengths. In this work, we propose a simple yet effective method calledSaliency-aware Noise Blending (SNB) that can empower the fused text-guideddiffusion models to achieve more controllable generation. Specifically, weexperimentally find that the responses of classifier-free guidance are highlyrelated to the saliency of generated images. Thus we propose to trust differentmodels in their areas of expertise by blending the predicted noises of twodiffusion models in a saliency-aware manner. SNB is training-free and can becompleted within a DDIM sampling process. Additionally, it can automaticallyalign the semantics of two noise spaces without requiring additionalannotations such as masks. Extensive experiments show the impressiveeffectiveness of SNB in various applications. Project page is available athttps://magicfusion.github.io/.", "output": "MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose VADER, a spatio-temporal matching, alignment, and changesummarization method to help fight misinformation spread via manipulatedvideos. VADER matches and coarsely aligns partial video fragments to candidatevideos using a robust visual descriptor and scalable search over adaptivelychunked video content. A transformer-based alignment module then refines thetemporal localization of the query fragment within the matched video. Aspace-time comparator module identifies regions of manipulation between alignedcontent, invariant to any changes due to any residual temporal misalignments orartifacts arising from non-editorial changes of the content. Robustly matchingvideo to a trusted source enables conclusions to be drawn on video provenance,enabling informed trust decisions on content encountered.", "output": "VADER: Video Alignment Differencing and Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the great success in 2D editing using user-friendly tools, such asPhotoshop, semantic strokes, or even text prompts, similar capabilities in 3Dareas are still limited, either relying on 3D modeling skills or allowingediting within only a few categories. In this paper, we present a novelsemantic-driven NeRF editing approach, which enables users to edit a neuralradiance field with a single image, and faithfully delivers edited novel viewswith high fidelity and multi-view consistency. To achieve this goal, we proposea prior-guided editing field to encode fine-grained geometric and textureediting in 3D space, and develop a series of techniques to aid the editingprocess, including cyclic constraints with a proxy mesh to facilitate geometricsupervision, a color compositing mechanism to stabilize semantic-driven textureediting, and a feature-cluster-based regularization to preserve the irrelevantcontent unchanged. Extensive experiments and editing examples on bothreal-world and synthetic data demonstrate that our method achievesphoto-realistic 3D editing using only a single edited image, pushing the boundof semantic-driven editing in 3D real-world scenes. Our project webpage:https://zju3dv.github.io/sine/.", "output": "SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated diagnosis prediction from medical images is a valuable resource tosupport clinical decision-making. However, such systems usually need to betrained on large amounts of annotated data, which often is scarce in themedical domain. Zero-shot methods address this challenge by allowing a flexibleadaption to new settings with different clinical findings without relying onlabeled data. Further, to integrate automated diagnosis in the clinicalworkflow, methods should be transparent and explainable, increasing medicalprofessionals' trust and facilitating correctness verification. In this work,we introduce Xplainer, a novel framework for explainable zero-shot diagnosis inthe clinical setting. Xplainer adapts the classification-by-descriptionapproach of contrastive vision-language models to the multi-label medicaldiagnosis task. Specifically, instead of directly predicting a diagnosis, weprompt the model to classify the existence of descriptive observations, which aradiologist would look for on an X-Ray scan, and use the descriptorprobabilities to estimate the likelihood of a diagnosis. Our model isexplainable by design, as the final diagnosis prediction is directly based onthe prediction of the underlying descriptors. We evaluate Xplainer on two chestX-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness inimproving the performance and explainability of zero-shot diagnosis. Ourresults suggest that Xplainer provides a more detailed understanding of thedecision-making process and can be a valuable tool for clinical diagnosis.", "output": "Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present DreamBooth3D, an approach to personalize text-to-3D generativemodels from as few as 3-6 casually captured images of a subject. Our approachcombines recent advances in personalizing text-to-image models (DreamBooth)with text-to-3D generation (DreamFusion). We find that naively combining thesemethods fails to yield satisfactory subject-specific 3D assets due topersonalized text-to-image models overfitting to the input viewpoints of thesubject. We overcome this through a 3-stage optimization strategy where wejointly leverage the 3D consistency of neural radiance fields together with thepersonalization capability of text-to-image models. Our method can producehigh-quality, subject-specific 3D assets with text-driven modifications such asnovel poses, colors and attributes that are not seen in any of the input imagesof the subject.", "output": "DreamBooth3D: Subject-Driven Text-to-3D Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The usage and impact of deep learning for cleaner production andsustainability purposes remain little explored. This work shows how deeplearning can be harnessed to increase sustainability in production and productusage. Specifically, we utilize deep learning-based computer vision todetermine the wear states of products. The resulting insights serve as a basisfor novel product-service systems with improved integration and resultorientation. Moreover, these insights are expected to facilitate product usageimprovements and R&amp;D innovations. We demonstrate our approach on two products:machining tools and rotating X-ray anodes. From a technical standpoint, we showthat it is possible to recognize the wear state of these products usingdeep-learning-based computer vision. In particular, we detect wear throughmicroscopic images of the two products. We utilize a U-Net for semanticsegmentation to detect wear based on pixel granularity. The resulting mean dicecoefficients of 0.631 and 0.603 demonstrate the feasibility of the proposedapproach. Consequently, experts can now make better decisions, for example, toimprove the machining process parameters. To assess the impact of the proposedapproach on environmental sustainability, we perform life cycle assessmentsthat show gains for both products. The results indicate that the emissions ofCO2 equivalents are reduced by 12% for machining tools and by 44% for rotatinganodes. This work can serve as a guideline and inspire researchers andpractitioners to utilize computer vision in similar scenarios to developsustainable smart product-service systems and enable cleaner production.", "output": "Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal alignment facilitates the retrieval of instances from one modalitywhen queried using another. In this paper, we consider a novel setting wheresuch an alignment is between (i) instruction steps that are depicted asassembly diagrams (commonly seen in Ikea assembly manuals) and (ii) videosegments from in-the-wild videos; these videos comprising an enactment of theassembly actions in the real world. To learn this alignment, we introduce anovel supervised contrastive learning method that learns to align videos withthe subtle details in the assembly diagrams, guided by a set of novel losses.To study this problem and demonstrate the effectiveness of our method, weintroduce a novel dataset: IAW for Ikea assembly in the wild consisting of 183hours of videos from diverse furniture assembly collections and nearly 8,300illustrations from their associated instruction manuals and annotated for theirground truth alignments. We define two tasks on this dataset: First, nearestneighbor retrieval between video segments and illustrations, and, second,alignment of instruction steps and the segments for each video. Extensiveexperiments on IAW demonstrate superior performances of our approach againstalternatives.", "output": "Aligning Step-by-Step Instructional Diagrams to Video Demonstrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The extraction of structured clinical information from free-text radiologyreports in the form of radiology graphs has been demonstrated to be a valuableapproach for evaluating the clinical correctness of report-generation methods.However, the direct generation of radiology graphs from chest X-ray (CXR)images has not been attempted. To address this gap, we propose a novel approachcalled Prior-RadGraphFormer that utilizes a transformer model with priorknowledge in the form of a probabilistic knowledge graph (PKG) to generateradiology graphs directly from CXR images. The PKG models the statisticalrelationship between radiology entities, including anatomical structures andmedical observations. This additional contextual information enhances theaccuracy of entity and relation extraction. The generated radiology graphs canbe applied to various downstream tasks, such as free-text or structured reportsgeneration and multi-label classification of pathologies. Our approachrepresents a promising method for generating radiology graphs directly from CXRimages, and has significant potential for improving medical image analysis andclinical decision-making.", "output": "Prior-RadGraphFormer: A Prior-Knowledge-Enhanced Transformer for Generating Radiology Graphs from X-Rays."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks can be easily fooled into making incorrect predictionsthrough corruption of the input by adversarial perturbations:human-imperceptible artificial noise. So far adversarial training has been themost successful defense against such adversarial attacks. This work focuses onimproving adversarial training to boost adversarial robustness. We firstanalyze, from an instance-wise perspective, how adversarial vulnerabilityevolves during adversarial training. We find that during training an overallreduction of adversarial loss is achieved by sacrificing a considerableproportion of training samples to be more vulnerable to adversarial attack,which results in an uneven distribution of adversarial vulnerability amongdata. Such \"uneven vulnerability\", is prevalent across several popular robusttraining methods and, more importantly, relates to overfitting in adversarialtraining. Motivated by this observation, we propose a new adversarial trainingmethod: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). Itjointly smooths both input and weight loss landscapes in an adaptive,instance-specific, way to enhance robustness more for those samples with higheradversarial vulnerability. Extensive experiments demonstrate the superiority ofour method over existing defense methods. Noticeably, our method, when combinedwith the latest data augmentation and semi-supervised learning techniques,achieves state-of-the-art robustness against $\\ell_{\\infty}$-norm constrainedattacks on CIFAR10 of 59.32% for Wide ResNet34-10 without extra data, and61.55% for Wide ResNet28-10 with extra data. Code is available athttps://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.", "output": "Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical datasets and especially biobanks, often contain extensive tabulardata with rich clinical information in addition to images. In practice,clinicians typically have less data, both in terms of diversity and scale, butstill wish to deploy deep learning solutions. Combined with increasing medicaldataset sizes and expensive annotation costs, the necessity for unsupervisedmethods that can pretrain multimodally and predict unimodally has risen.To address these needs, we propose the first self-supervised contrastivelearning framework that takes advantage of images and tabular data to trainunimodal encoders. Our solution combines SimCLR and SCARF, two leadingcontrastive learning strategies, and is simple and effective. In ourexperiments, we demonstrate the strength of our framework by predicting risksof myocardial infarction and coronary artery disease (CAD) using cardiac MRimages and 120 clinical features from 40,000 UK Biobank subjects. Furthermore,we show the generalizability of our approach to natural images using the DVMcar advertisement dataset.We take advantage of the high interpretability of tabular data and throughattribution and ablation experiments find that morphometric tabular features,describing size and shape, have outsized importance during the contrastivelearning process and improve the quality of the learned embeddings. Finally, weintroduce a novel form of supervised contrastive learning, label as a feature(LaaF), by appending the ground truth label as a tabular feature duringmultimodal pretraining, outperforming all supervised contrastive baselines.", "output": "Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Realistic face rendering from multi-view images is beneficial to variouscomputer vision and graphics applications. Due to the complex spatially-varyingreflectance properties and geometry characteristics of faces, however, itremains challenging to recover 3D facial representations both faithfully andefficiently in the current studies. This paper presents a novel 3D facerendering model, namely NeuFace, to learn accurate and physically-meaningfulunderlying 3D representations by neural rendering techniques. It naturallyincorporates the neural BRDFs into physically based rendering, capturingsophisticated facial geometry and appearance clues in a collaborative manner.Specifically, we introduce an approximated BRDF integration and a simple yetnew low-rank prior, which effectively lower the ambiguities and boost theperformance of the facial BRDFs. Extensive experiments demonstrate thesuperiority of NeuFace in human face rendering, along with a decentgeneralization ability to common objects.", "output": "NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neuromorphic (event-based) image sensors draw inspiration from thehuman-retina to create an electronic device that can process visual stimuli ina way that closely resembles its biological counterpart. These sensors processinformation significantly different than the traditional RGB sensors.Specifically, the sensory information generated by event-based image sensorsare orders of magnitude sparser compared to that of RGB sensors. The firstgeneration of neuromorphic image sensors, Dynamic Vision Sensor (DVS), areinspired by the computations confined to the photoreceptors and the firstretinal synapse. In this work, we highlight the capability of the secondgeneration of neuromorphic image sensors, Integrated Retinal Functionality inCMOS Image Sensors (IRIS), which aims to mimic full retinal computations fromphotoreceptors to output of the retina (retinal ganglion cells) for targetedfeature-extraction. The feature of choice in this work is Object MotionSensitivity (OMS) that is processed locally in the IRIS sensor. We study thecapability of OMS in solving the ego-motion problem of the event-based cameras.Our results show that OMS can accomplish standard computer vision tasks withsimilar efficiency to conventional RGB and DVS solutions but offers drasticbandwidth reduction. This cuts the wireless and computing power budgets andopens up vast opportunities in high-speed, robust, energy-efficient, andlow-bandwidth real-time decision making.", "output": "Object Motion Sensitivity: A Bio-inspired Solution to the Ego-motion Problem for Event-based Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For many interdisciplinary fields, ML interpretations need to be consistentwith what-if scenarios related to the current case, i.e., if one factorchanges, how does the model react? Although the attribution methods aresupported by the elegant axiomatic systems, they mainly focus on individualinputs, and are generally inconsistent. To support what-if scenarios, weintroduce a new notion called truthful interpretation, and apply Fourieranalysis of Boolean functions to get rigorous guarantees. Experimental resultsshow that for neighborhoods with various radii, our method achieves 2x - 50xlower interpretation error compared with the other methods.", "output": "Consistent and Truthful Interpretation with Fourier Analysis."}]