[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this technical report, we describe Neonatal Resuscitation TrainingSimulator (NRTS), an Android mobile app designed to support medical experts toinput, transmit and record data during a High-Fidelity Simulation course forneonatal resuscitation. This mobile app allows one to automatically send allthe recorded data from \"Neonatal Intensive Care Unit\" (NICU) of CasaleMonferrato Children's Hospital, (Italy) to a server located at the Departmentof Science and Technological Innovation (DiSIT), University of PiemonteOrientale (Italy). Finally, the medical instructor can view statistics on asimulation exercise that may be used during the de-briefing phase for theevaluation of multidisciplinary teams involved in the simulation scenarios.", "output": "NRTS: A Client-Server architecture for supporting data recording, transmission and evaluation of multidisciplinary teams during the neonatal resuscitation simulation scenario."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, Artificial Intelligence has become more and more relevant inour society. Creating AI systems is almost always the prerogative of IT and AIexperts. However, users may need to create intelligent solutions tailored totheir specific needs. In this way, AI systems can be enhanced if new approachesare devised to allow non-technical users to be directly involved in thedefinition and personalization of AI technologies. End-User Development (EUD)can provide a solution to these problems, allowing people to create, customize,or adapt AI-based systems to their own needs. This paper presents a systematicliterature review that aims to shed the light on the current landscape of EUDfor AI systems, i.e., how users, even without skills in AI and/or programming,can customize the AI behavior to their needs. This study also discusses thecurrent challenges of EUD for AI, the potential benefits, and the futureimplications of integrating EUD into the overall AI development process.", "output": "End-User Development for Artificial Intelligence: A Systematic Literature Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work explores the impact of moderation on users' enjoyment ofconversational AI systems. While recent advancements in Large Language Models(LLMs) have led to highly capable conversational AIs that are increasinglydeployed in real-world settings, there is a growing concern over AI safety andthe need to moderate systems to encourage safe language and prevent harm.However, some users argue that current approaches to moderation limit thetechnology, compromise free expression, and limit the value delivered by thetechnology. This study takes an unbiased stance and shows that moderation doesnot necessarily detract from user enjoyment. Heavy handed moderation does seemto have a nefarious effect, but models that are moderated to be safer can leadto a better user experience. By deploying various conversational AIs in theChai platform, the study finds that user retention can increase with a level ofmoderation and safe system design. These results demonstrate the importance ofappropriately defining safety in models in a way that is both responsible andfocused on serving users.", "output": "Safer Conversational AI as a Source of User Delight."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Loneliness and social isolation are serious and widespread problems amongolder people, affecting their physical and mental health, quality of life, andlongevity. In this paper, we propose a ChatGPT-based conversational companionsystem for elderly people. The system is designed to provide companionship andhelp reduce feelings of loneliness and social isolation. The system wasevaluated with a preliminary study. The results showed that the system was ableto generate responses that were relevant to the created elderly personas.However, it is essential to acknowledge the limitations of ChatGPT, such aspotential biases and misinformation, and to consider the ethical implicationsof using AI-based companionship for the elderly, including privacy concerns.", "output": "Towards Designing a ChatGPT Conversational Companion for Elderly People."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Construct theory in social psychology, developed by George Kelly are mentalconstructs to predict and anticipate events. Constructs are how humansinterpret, curate, predict and validate data; information. AI today is biasedbecause it is trained with a narrow construct as defined by the training datalabels. Machine Learning algorithms for facial recognition discriminate againstdarker skin colors and in the ground breaking research papers (Buolamwini, Joyand Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities inCommercial Gender Classification. FAT (2018), the inclusion of phenotypiclabeling is proposed as a viable solution. In Construct theory, phenotype isjust one of the many subelements that make up the construct of a face. In thispaper, we present 15 main elements of the construct of face, with 50subelements and tested Google Cloud Vision API and Microsoft Cognitive ServicesAPI using FairFace dataset that currently has data for 7 races, genders andages, and we retested against FairFace Plus dataset curated by us. Our resultsshow exactly where they have gaps for inclusivity. Based on our experimentresults, we propose that validated, inclusive constructs become industrystandards for AI ML models going forward.", "output": "Introducing Construct Theory as a Standard Methodology for Inclusive AI Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Support vector clustering is an important clustering method. However, itsuffers from a scalability issue due to its computational expensive clusterassignment step. In this paper we accelertate the support vector clustering viaspectrum-preserving data compression. Specifically, we first compress theoriginal data set into a small amount of spectrally representative aggregateddata points. Then, we perform standard support vector clustering on thecompressed data set. Finally, we map the clustering results of the compresseddata set back to discover the clusters in the original data set. Our extensiveexperimental results on real-world data set demonstrate dramatically speedupsover standard support vector clustering without sacrificing clustering quality.", "output": "Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Evolutionary algorithms have been used to evolve a population of actors togenerate diverse experiences for training reinforcement learning agents, whichhelps to tackle the temporal credit assignment problem and improves theexploration efficiency. However, when adapting this approach to addressconstrained problems, balancing the trade-off between the reward and constraintviolation is hard. In this paper, we propose a novel evolutionary constrainedreinforcement learning (ECRL) algorithm, which adaptively balances the rewardand constraint violation with stochastic ranking, and at the same time,restricts the policy's behaviour by maintaining a set of Lagrange relaxationcoefficients with a constraint buffer. Extensive experiments on robotic controlbenchmarks show that our ECRL achieves outstanding performance compared tostate-of-the-art algorithms. Ablation analysis shows the benefits ofintroducing stochastic ranking and constraint buffer.", "output": "Evolving Constrained Reinforcement Learning Policy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The necessity for cooperation among intelligent machines has popularisedcooperative multi-agent reinforcement learning (MARL) in AI research. However,many research endeavours heavily rely on parameter sharing among agents, whichconfines them to only homogeneous-agent setting and leads to traininginstability and lack of convergence guarantees. To achieve effectivecooperation in the general heterogeneous-agent setting, we proposeHeterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve theaforementioned issues. Central to our findings are the multi-agent advantagedecomposition lemma and the sequential update scheme. Based on these, wedevelop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL)that is free of parameter-sharing constraint, and derive HATRPO and HAPPO bytractable approximations. Furthermore, we discover a novel framework namedHeterogeneous-Agent Mirror Learning (HAML), which strengthens theoreticalguarantees for HATRPO and HAPPO and provides a general template for cooperativeMARL algorithmic designs. We prove that all algorithms derived from HAMLinherently enjoy monotonic improvement of joint reward and convergence to NashEquilibrium. As its natural outcome, HAML validates more novel algorithms inaddition to HATRPO and HAPPO, including HAA2C, HADDPG, and HATD3, whichconsistently outperform their existing MA-counterparts. We comprehensively testHARL algorithms on six challenging benchmarks and demonstrate their superioreffectiveness and stability for coordinating heterogeneous agents compared tostrong baselines such as MAPPO and QMIX.", "output": "Heterogeneous-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a theory for the previously unexplained divergent behavior noticedin the training of large language models. We argue that the phenomenon is anartifact of the dominant optimization algorithm used for training, called Adam.We observe that Adam can enter a state in which the parameter update vector hasa relatively large norm and is essentially uncorrelated with the direction ofdescent on the training loss landscape, leading to divergence. This artifact ismore likely to be observed in the training of a deep model with a large batchsize, which is the typical setting of large-scale language model training. Toargue the theory, we present observations from the training runs of thelanguage models of different scales: 7 billion, 30 billion, 65 billion, and 546billion parameters.", "output": "A Theory on Adam Instability in Large-Scale Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes using ChatGPT, an innovative technology with variousapplications, as an assistant for psychotherapy. ChatGPT can serve as a patientinformation collector, a companion for patients in between therapy sessions,and an organizer of gathered information for therapists to facilitate treatmentprocesses. The research identifies five research questions and discovers usefulprompts for fine-tuning the assistant, which shows that ChatGPT can participatein positive conversations, listen attentively, offer validation and potentialcoping strategies without providing explicit medical advice, and helptherapists discover new insights from multiple conversations with the samepatient. Using ChatGPT as an assistant for psychotherapy poses severalchallenges that need to be addressed, including technical as well ashuman-centric challenges which are discussed.", "output": "ChatGPT as a Therapist Assistant: A Suitability Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current studies on adversarial robustness mainly focus on aggregating localrobustness results from a set of data samples to evaluate and rank differentmodels. However, the local statistics may not well represent the true globalrobustness of the underlying unknown data distribution. To address thischallenge, this paper makes the first attempt to present a new framework,called GREAT Score , for global robustness evaluation of adversarialperturbation using generative models. Formally, GREAT Score carries thephysical meaning of a global statistic capturing a mean certified attack-proofperturbation level over all samples drawn from a generative model. Forfinite-sample evaluation, we also derive a probabilistic guarantee on thesample complexity and the difference between the sample mean and the true mean.GREAT Score has several advantages: (1) Robustness evaluations using GREATScore are efficient and scalable to large models, by sparing the need ofrunning adversarial attacks. In particular, we show high correlation andsignificantly reduced computation cost of GREAT Score when compared to theattack-based model ranking on RobustBench (Croce,et. al. 2021). (2) The use ofgenerative models facilitates the approximation of the unknown datadistribution. In our ablation study with different generative adversarialnetworks (GANs), we observe consistency between global robustness evaluationand the quality of GANs. (3) GREAT Score can be used for remote auditing ofprivacy-sensitive black-box models, as demonstrated by our robustnessevaluation on several online facial recognition services.", "output": "GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weakly-supervised semantic segmentation aims to reduce labeling costs bytraining semantic segmentation models using weak supervision, such asimage-level class labels. However, most approaches struggle to produce accuratelocalization maps and suffer from false predictions in class-relatedbackgrounds (i.e., biased objects), such as detecting a railroad with the trainclass. Recent methods that remove biased objects require additional supervisionfor manually identifying biased objects for each problematic class andcollecting their datasets by reviewing predictions, limiting theirapplicability to the real-world dataset with multiple labels and complexrelationships for biasing. Following the first observation that biased featurescan be separated and eliminated by matching biased objects with backgrounds inthe same dataset, we propose a fully-automatic/model-agnostic biased removalframework called MARS (Model-Agnostic biased object Removal without additionalSupervision), which utilizes semantically consistent features of anunsupervised technique to eliminate biased objects in pseudo labels.Surprisingly, we show that MARS achieves new state-of-the-art results on twopopular benchmarks, PASCAL VOC 2012 (val: 77.7%, test: 77.2%) and MS COCO 2014(val: 49.4%), by consistently improving the performance of various WSSS modelsby at least 30% without additional supervision.", "output": "MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficiently and accurately translating a corpus into a low-resource languageremains a challenge, regardless of the strategies employed, whether manual,automated, or a combination of the two. Many Christian organizations arededicated to the task of translating the Holy Bible into languages that lack amodern translation. Bible translation (BT) work is currently underway for over3000 extremely low resource languages. We introduce the eBible corpus: adataset containing 1009 translations of portions of the Bible with data in 833different languages across 75 language families. In addition to a BTbenchmarking dataset, we introduce model performance benchmarks built on the NoLanguage Left Behind (NLLB) neural machine translation (NMT) models. Finally,we describe several problems specific to the domain of BT and consider how theestablished data and model benchmarks might be used for future translationefforts. For a BT task trained with NLLB, Austronesian and Trans-New Guinealanguage families achieve 35.1 and 31.6 BLEU scores respectively, which spursfuture innovations for NMT for low-resource languages in Papua New Guinea.", "output": "The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A classic solution technique for Markov decision processes (MDP) andstochastic games (SG) is value iteration (VI). Due to its good practicalperformance, this approximative approach is typically preferred over exacttechniques, even though no practical bounds on the imprecision of the resultcould be given until recently. As a consequence, even the most used modelcheckers could return arbitrarily wrong results. Over the past decade,different works derived stopping criteria, indicating when the precisionreaches the desired level, for various settings, in particular MDP withreachability, total reward, and mean payoff, and SG with reachability.In this paper, we provide the first stopping criteria for VI on SG with totalreward and mean payoff, yielding the first anytime algorithms in thesesettings. To this end, we provide the solution in two flavours: First through areduction to the MDP case and second directly on SG. The former is simpler andautomatically utilizes any advances on MDP. The latter allows for more localcomputations, heading towards better practical efficiency.Our solution unifies the previously mentioned approaches for MDP and SG andtheir underlying ideas. To achieve this, we isolate objective-specificsubroutines as well as identify objective-independent concepts. Thesestructural concepts, while surprisingly simple, form the very essence of theunified solution.", "output": "Stopping Criteria for Value Iteration on Stochastic Games with Quantitative Objectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the interest in autonomous systems continues to grow, one of the majorchallenges is collecting sufficient and representative real-world data. Despitethe strong practical and commercial interest in autonomous landing systems inthe aerospace field, there is a lack of open-source datasets of aerial images.To address this issue, we present a dataset-lard-of high-quality aerial imagesfor the task of runway detection during approach and landing phases. Most ofthe dataset is composed of synthetic images but we also provide manuallylabelled images from real landing footages, to extend the detection task to amore realistic setting. In addition, we offer the generator which can producesuch synthetic front-view images and enables automatic annotation of the runwaycorners through geometric transformations. This dataset paves the way forfurther research such as the analysis of dataset quality or the development ofmodels to cope with the detection tasks. Find data, code and more up-to-dateinformation at ", "output": "LARD -- Landing Approach Runway Detection -- Dataset for Vision Based Landing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advancements in computer science and AI lead to the development of larger,more complex knowledge bases. These are susceptible to contradictions,particularly when multiple experts are involved. To ensure integrity duringchanges, procedures are needed. This work addresses the problem from a logicalprogramming perspective. Integrity violations can be interpreted as specialoperations on proofs of integrity constraints, with SLDNF proofs being thefocus. We define a proof tree as a special data structure and demonstrate theimplication of the existence of an SLDNF proof through such a tree. Proof treesare more convenient than SLDNF trees and allow set-oriented considerations ofproofs. They also present the proof structure more clearly, enabling furtherapplications. Using this structure, we determine a minimal set of conditionsthat specify when a change in the knowledge base affects the validity of anintegrity constraint. Additionally, this approach allows for the reuse of largeparts of the old proof when searching for a new one, which reduces the effortcompared to previous approaches.", "output": "\\\"Uberpr\\\"ufung von Integrit\\\"atsbedingungen in Deduktiven Datenbanken."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The proliferation of fake reviews of doctors has potentially detrimentalconsequences for patient well-being and has prompted concern among consumerprotection groups and regulatory bodies. Yet despite significant advancementsin the fields of machine learning and natural language processing, thereremains limited comprehension of the characteristics differentiating fraudulentfrom authentic reviews. This study utilizes a novel pre-labeled dataset of38048 physician reviews to establish the effectiveness of large language modelsin classifying reviews. Specifically, we compare the performance of traditionalML models, such as logistic regression and support vector machines, togenerative pre-trained transformer models. Furthermore, we use GPT4, the newestmodel in the GPT family, to uncover the key dimensions along which fake andgenuine physician reviews differ. Our findings reveal significantly superiorperformance of GPT-3 over traditional ML models in this context. Additionally,our analysis suggests that GPT3 requires a smaller training sample thantraditional models, suggesting its appropriateness for tasks with scarcetraining data. Moreover, the superiority of GPT3 performance increases in thecold start context i.e., when there are no prior reviews of a doctor. Finally,we employ GPT4 to reveal the crucial dimensions that distinguish fake physicianreviews. In sharp contrast to previous findings in the literature that wereobtained using simulated data, our findings from a real-world dataset show thatfake reviews are generally more clinically detailed, more reserved insentiment, and have better structure and grammar than authentic ones.", "output": "Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Languages are not created randomly but rather to communicate information.There is a strong association between languages and their underlying meanings,resulting in a sparse joint distribution that is heavily peaked according totheir correlations. Moreover, these peak values happen to match with themarginal distribution of languages due to the sparsity. With the advent of LLMstrained on big data and large models, we can now precisely assess the marginaldistribution of languages, providing a convenient means of exploring the sparsestructures in the joint distribution for effective inferences. In this paper,we categorize languages as either unambiguous or {epsilon}-ambiguous andpresent quantitative results to demonstrate that the emergent abilities ofLLMs, such as language understanding, in-context learning, chain-of-thoughtprompting, and effective instruction fine-tuning, can all be attributed toBayesian inference on the sparse joint distribution of languages.", "output": "A Latent Space Theory for Emergent Abilities in Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Resource allocation is the assignment of resources to activities that must beexecuted in a business process at a particular moment at run-time. Whileresource allocation is well-studied in other fields, such as manufacturing,there exist only a few methods in business process management. Existing methodsare not suited for application in large business processes or focus onoptimizing resource allocation for a single case rather than for all casescombined. To fill this gap, this paper proposes two learning-based methods forresource allocation in business processes: a deep reinforcement learning-basedapproach and a score-based value function approximation approach. The twomethods are compared against existing heuristics in a set of scenarios thatrepresent typical business process structures and on a complete network thatrepresents a realistic business process. The results show that ourlearning-based methods outperform or are competitive with common heuristics inmost scenarios and outperform heuristics in the complete network.", "output": "Learning policies for resource allocation in business processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advances in GPT-based large language models (LLMs) are revolutionizingnatural language processing, exponentially increasing its use across variousdomains. Incorporating uni-directional attention, these autoregressive LLMs cangenerate long and coherent paragraphs. However, for visual question answering(VQA) tasks that require both vision and language processing, models withbi-directional attention or models employing fusion techniques are oftenemployed to capture the context of multiple modalities all at once. As GPT doesnot natively process vision tokens, to exploit the advancements in GPT modelsfor VQA in robotic surgery, we design an end-to-end trainable Language-VisionGPT (LV-GPT) model that expands the GPT2 model to include vision input (image).The proposed LV-GPT incorporates a feature extractor (vision tokenizer) andvision token embedding (token type and pose). Given the limitations ofunidirectional attention in GPT models and their ability to generate coherentlong paragraphs, we carefully sequence the word tokens before vision tokens,mimicking the human thought process of understanding the question to infer ananswer from an image. Quantitatively, we prove that the LV-GPT modeloutperforms other state-of-the-art VQA models on two publically availablesurgical-VQA datasets (based on endoscopic vision challenge robotic scenesegmentation 2018 and CholecTriplet2021) and on our newly annotated dataset(based on the holistic surgical scene dataset). We further annotate all threedatasets to include question-type annotations to allow sub-type analysis.Furthermore, we extensively study and present the effects of token sequencing,token type and pose embedding for vision tokens in the LV-GPT model.", "output": "SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a new learning-based approach for approximately solvingthe Kidney-Exchange Problem (KEP), an NP-hard problem on graphs. The problemconsists of, given a pool of kidney donors and patients waiting for kidneydonations, optimally selecting a set of donations to optimize the quantity andquality of transplants performed while respecting a set of constraints aboutthe arrangement of these donations. The proposed technique consists of two mainsteps: the first is a Graph Neural Network (GNN) trained without supervision;the second is a deterministic non-learned search heuristic that uses the outputof the GNN to find paths and cycles. To allow for comparisons, we alsoimplemented and tested an exact solution method using integer programming, twogreedy search heuristics without the machine learning module, and the GNN alonewithout a heuristic. We analyze and compare the methods and conclude that thelearning-based two-stage approach is the best solution quality, outputtingapproximate solutions on average 1.1 times more valuable than the ones from thedeterministic heuristic alone.", "output": "Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to learn and predict simple functions is a key aspect of humanintelligence. Recent works have started to explore this ability usingtransformer architectures, however it remains unclear whether this issufficient to recapitulate the extrapolation abilities of people in thisdomain. Here, we propose to address this gap by augmenting the transformerarchitecture with two simple inductive learning biases, that are directlyadapted from recent models of abstract reasoning in cognitive science. Theresults we report demonstrate that these biases are helpful in the context oflarge neural network models, as well as shed light on the types of inductivelearning biases that may contribute to human abilities in extrapolation.", "output": "Beyond Transformers for Function Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models are becoming increasingly pervasive and ubiquitous insociety via deployment in sociotechnical systems. Yet these language models, beit for classification or generation, have been shown to be biased and behaveirresponsibly, causing harm to people at scale. It is crucial to audit theselanguage models rigorously. Existing auditing tools leverage either or bothhumans and AI to find failures. In this work, we draw upon literature inhuman-AI collaboration and sensemaking, and conduct interviews with researchexperts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiroand Lundberg, 2022), which is powered by a generative large language model(LLM). Through the design process we highlight the importance of sensemakingand human-AI communication to leverage complementary strengths of humans andgenerative models in collaborative auditing. To evaluate the effectiveness ofthe augmented tool, AdaTest++, we conduct user studies with participantsauditing two commercial language models: OpenAI's GPT-3 and Azure's sentimentanalysis model. Qualitative analysis shows that AdaTest++ effectively leverageshuman strengths such as schematization, hypothesis formation and testing.Further, with our tool, participants identified a variety of failures modes,covering 26 different topics over 2 tasks, that have been shown before informal audits and also those previously under-reported.", "output": "Supporting Human-AI Collaboration in Auditing LLMs with LLMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models have become increasingly popular for flood predictiondue to their superior accuracy and efficiency compared to traditional methods.However, current machine learning methods often rely on separate spatial ortemporal feature analysis and have limitations on the types, number, anddimensions of input data. This study presented a CNN-RNN hybrid feature fusionmodelling approach for urban flood prediction, which integrated the strengthsof CNNs in processing spatial features and RNNs in analyzing differentdimensions of time sequences. This approach allowed for both static and dynamicflood predictions. Bayesian optimization was applied to identify the seven mostinfluential flood-driven factors and determine the best combination strategy.By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM,BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. Thismodel achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE were0.007, 0.025, 0.973 and 0.755, respectively) under various rainfall inputconditions. Additionally, the processing speed was significantly improved, withan inference time of 1.158s (approximately 1/125 of the traditional computationtime) compared to the physically-based models.", "output": "Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We find that improvements in speedrunning world records follow a power lawpattern. Using this observation, we answer an outstanding question fromprevious work: How do we improve on the baseline of predicting no improvementwhen forecasting speedrunning world records out to some time horizon, such asone month? Using a random effects model, we improve on this baseline forrelative mean square error made on predicting out-of-sample world recordimprovements as the comparison metric at a $p &lt; 10^{-5}$ significance level.The same set-up improves textit{even} on the ex-post best exponential movingaverage forecasts at a $p = 0.15$ significance level while having access tosubstantially fewer data points. We demonstrate the effectiveness of thisapproach by applying it to Machine Learning benchmarks and achieving forecaststhat exceed a baseline. Finally, we interpret the resulting model to suggestthat 1) ML benchmarks are far from saturation and 2) sudden large improvementsin Machine Learning are unlikely but cannot be ruled out.", "output": "Power Law Trends in Speedrunning and Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the relationship between assumptions of state separability and bothpreparation and measurement contextuality, and the relationship of both ofthese to the frame problem, the problem of predicting what does not change inconsequence of an action. We state a quantum analog of the latter and prove itsundecidability. We show how contextuality is generically induced in statepreparation and measurement by basis choice, thermodynamic exchange, and theimposition of a priori causal models, and how fine-tuning assumptions appearubiquitously in settings characterized as non-contextual.", "output": "Separability, Contextuality, and the Quantum Frame Problem."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural network (DNN) partition is a research problem that involvessplitting a DNN into multiple parts and offloading them to specific locations.Because of the recent advancement in multi-access edge computing and edgeintelligence, DNN partition has been considered as a powerful tool forimproving DNN inference performance when the computing resources of edge andend devices are limited and the remote transmission of data from these devicesto clouds is costly. This paper provides a comprehensive survey on the recentadvances and challenges in DNN partition approaches over the cloud, edge, andend devices based on a detailed literature collection. We review how DNNpartition works in various application scenarios, and provide a unifiedmathematical model of the DNN partition problem. We developed afive-dimensional classification framework for DNN partition approaches,consisting of deployment locations, partition granularity, partitionconstraints, optimization objectives, and optimization algorithms. Eachexisting DNN partition approache can be perfectly defined in this framework byinstantiating each dimension into specific values. In addition, we suggest aset of metrics for comparing and evaluating the DNN partition approaches. Basedon this, we identify and discuss research challenges that have not yet beeninvestigated or fully addressed. We hope that this work helps DNN partitionresearchers by highlighting significant future research directions in thisdomain.", "output": "A Survey on Deep Neural Network Partition over Cloud, Edge and End Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As AI agents are increasingly used in the real open world with unknowns ornovelties, they need the ability to (1) recognize objects that (i) they havelearned and (ii) detect items that they have not seen or learned before, and(2) learn the new items incrementally to become more and more knowledgeable andpowerful. (1) is called novelty detection or out-of-distribution (OOD)detection and (2) is called class incremental learning (CIL), which is asetting of continual learning (CL). In existing research, OOD detection and CILare regarded as two completely different problems. This paper theoreticallyproves that OOD detection actually is necessary for CIL. We first show that CILcan be decomposed into two sub-problems: within-task prediction (WP) andtask-id prediction (TP). We then prove that TP is correlated with OODdetection. The key theoretical result is that regardless of whether WP and OODdetection (or TP) are defined explicitly or implicitly by a CIL algorithm, goodWP and good OOD detection are necessary and sufficient conditions for good CIL,which unifies novelty or OOD detection and continual learning (CIL, inparticular). A good CIL algorithm based on our theory can naturally be used inopen world learning, which is able to perform both novelty/OOD detection andcontinual learning. Based on the theoretical result, new CIL methods are alsodesigned, which outperform strong baselines in terms of CIL accuracy and itscontinual OOD detection by a large margin.", "output": "Open-World Continual Learning: Unifying Novelty Detection and Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work investigates the formal policy synthesis of continuous-statestochastic dynamic systems given high-level specifications in linear temporallogic. To learn an optimal policy that maximizes the satisfaction probability,we take a product between a dynamic system and the translated automaton toconstruct a product system on which we solve an optimal planning problem. Sincethis product system has a hybrid product state space that results in rewardsparsity, we introduce a generalized optimal backup order, in reverse to thetopological order, to guide the value backups and accelerate the learningprocess. We provide the optimality proof for using the generalized optimalbackup order in this optimal planning problem. Further, this paper presents anactor-critic reinforcement learning algorithm when topological order applies.This algorithm leverages advanced mathematical techniques and enjoys theproperty of hyperparameter self-tuning. We provide proof of the optimality andconvergence of our proposed reinforcement learning algorithm. We use neuralnetworks to approximate the value function and policy function for hybridproduct state space. Furthermore, we observe that assigning integer numbers toautomaton states can rank the value or policy function approximated by neuralnetworks. To break the ordinal relationship, we use an individual neuralnetwork for each automaton state's value (policy) function, termed modularlearning. We conduct two experiments. First, to show the efficacy of ourreinforcement learning algorithm, we compare it with baselines on a classiccontrol task, CartPole. Second, we demonstrate the empirical performance of ourformal policy synthesis framework on motion planning of a Dubins car with atemporal specification.", "output": "Topological Guided Actor-Critic Modular Learning of Continuous Systems with Temporal Objectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently developed graph contrastive learning (GCL) approaches compare twodifferent \"views\" of the same graph in order to learn node/graphrepresentations. The core assumption of these approaches is that by graphaugmentation, it is possible to generate several structurally different butsemantically similar graph structures, and therefore, the identity labels ofthe original and augmented graph/nodes should be identical. However, in thispaper, we observe that this assumption does not always hold, for example, anyperturbation to nodes or edges in a molecular graph will change the graphlabels to some degree. Therefore, we believe that augmenting the graphstructure should be accompanied by an adaptation of the labels used for thecontrastive loss. Based on this idea, we propose ID-MixGCL, which allows forsimultaneous modulation of both the input graph and the corresponding identitylabels, with a controllable degree of change, leading to the capture offine-grained representations from unlabeled graphs. Experimental resultsdemonstrate that ID-MixGCL improves performance on graph classification andnode classification tasks, as demonstrated by significant improvements on theCora, IMDB-B, and IMDB-M datasets compared to state-of-the-art techniques, by3-29% absolute points.", "output": "ID-MixGCL: Identity Mixup for Graph Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While deep reinforcement learning has shown important empirical success, ittends to learn relatively slow due to slow propagation of rewards informationand slow update of parametric neural networks. Non-parametric episodic memory,on the other hand, provides a faster learning alternative that does not requirerepresentation learning and uses maximum episodic return as state-action valuesfor action selection. Episodic memory and reinforcement learning both havetheir own strengths and weaknesses. Notably, humans can leverage multiplememory systems concurrently during learning and benefit from all of them. Inthis work, we propose a method called Two-Memory reinforcement learning agent(2M) that combines episodic memory and reinforcement learning to distill bothof their strengths. The 2M agent exploits the speed of the episodic memory partand the optimality and the generalization capacity of the reinforcementlearning part to complement each other. Our experiments demonstrate that the 2Magent is more data efficient and outperforms both pure episodic memory and purereinforcement learning, as well as a state-of-the-art memory-augmented RLagent. Moreover, the proposed approach provides a general framework that can beused to combine any episodic memory agent with other off-policy reinforcementlearning algorithms.", "output": "Two-Memory Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum walks (QWs) have the property that classical random walks (RWs) donot possess -- coexistence of linear spreading and localization -- and thisproperty is utilized to implement various kinds of applications. This paperproposes a quantum-walk-based algorithm for multi-armed-bandit (MAB) problemsby associating the two operations that make MAB problems difficult --exploration and exploitation -- with these two behaviors of QWs. We show thatthis new policy based on the QWs realizes high performance compared with thecorresponding RW-based one.", "output": "Bandit Algorithm Driven by a Classical Random Walk and a Quantum Walk."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Asymmetrical multiplayer (AMP) game is a popular game genre which involvesmultiple types of agents competing or collaborating with each other in thegame. It is difficult to train powerful agents that can defeat top humanplayers in AMP games by typical self-play training method because ofunbalancing characteristics in their asymmetrical environments. We proposeasymmetric-evolution training (AET), a novel multi-agent reinforcement learningframework that can train multiple kinds of agents simultaneously in AMP game.We designed adaptive data adjustment (ADA) and environment randomization (ER)to optimize the AET process. We tested our method in a complex AMP game namedTom &amp; Jerry, and our AIs trained without using any human data can achieve awin rate of 98.5% against top human players over 65 matches. The ablationexperiments indicated that the proposed modules are beneficial to theframework.", "output": "Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNN) suffer from severe inefficiency. It is mainlycaused by the exponential growth of node dependency with the increase oflayers. It extremely limits the application of stochastic optimizationalgorithms so that the training of GNN is usually time-consuming. To addressthis problem, we propose to decouple a multi-layer GNN as multiple simplemodules for more efficient training, which is comprised of classical forwardtraining (FT)and designed backward training (BT). Under the proposed framework,each module can be trained efficiently in FT by stochastic algorithms withoutdistortion of graph information owing to its simplicity. To avoid the onlyunidirectional information delivery of FT and sufficiently train shallowmodules with the deeper ones, we develop a backward training mechanism thatmakes the former modules perceive the latter modules. The backward trainingintroduces the reversed information delivery into the decoupled modules as wellas the forward information delivery. To investigate how the decoupling andgreedy training affect the representational capacity, we theoretically provethat the error produced by linear modules will not accumulate on unsupervisedtasks in most cases. The theoretical and experimental results show that theproposed framework is highly efficient with reasonable performance.", "output": "Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The release of ChatGPT has uncovered a range of possibilities whereby largelanguage models (LLMs) can substitute human intelligence. In this paper, weseek to understand whether ChatGPT has the potential to reproducehuman-generated label annotations in social computing tasks. Such anachievement could significantly reduce the cost and complexity of socialcomputing research. As such, we use ChatGPT to re-label five seminal datasetscovering stance detection (2x), sentiment analysis, hate speech, and botdetection. Our results highlight that ChatGPT does have the potential to handlethese data annotation tasks, although a number of challenges remain. ChatGPTobtains an average precision 0.609. Performance is highest for the sentimentanalysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, weshow that performance varies substantially across individual labels. We believethis work can open up new lines of analysis and act as a basis for futureresearch into the exploitation of ChatGPT for human annotation tasks.", "output": "Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum computation has a strong implication for advancing the currentlimitation of machine learning algorithms to deal with higher data dimensionsor reducing the overall training parameters for a deep neural network model.Based on a gate-based quantum computer, a parameterized quantum circuit wasdesigned to solve a model-free reinforcement learning problem with the deep-Qlearning method. This research has investigated and evaluated its potential.Therefore, a novel PQC based on the latest Qiskit and PyTorch framework wasdesigned and trained to compare with a full-classical deep neural network withand without integrated PQC. At the end of the research, the research draws itsconclusion and prospects on developing deep quantum learning in solving a mazeproblem or other reinforcement learning problems.", "output": "Deep Reinforcement Learning Using Hybrid Quantum Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several algorithms for learning the structure of dynamic Bayesian networks(DBNs) require an a priori ordering of variables, which influences thedetermined graph topology. However, it is often unclear how to determine thisorder if feature importance is unknown, especially as an exhaustive search isusually impractical. In this paper, we introduce Ranking Approaches for UnknownStructures (RAUS), an automated framework to systematically inform variableordering and learn networks end-to-end. RAUS leverages existing statisticalmethods (Cramers V, chi-squared test, and information gain) to compare variableordering, resultant generated network topologies, and DBN performance. RAUSenables end-users with limited DBN expertise to implement models via commandline interface. We evaluate RAUS on the task of predicting impending acutekidney injury (AKI) from inpatient clinical laboratory data. Longitudinalobservations from 67,460 patients were collected from our electronic healthrecord (EHR) and Kidney Disease Improving Global Outcomes (KDIGO) criteria werethen applied to define AKI events. RAUS learns multiple DBNs simultaneously topredict a future AKI event at different time points (i.e., 24-, 48-, 72-hoursin advance of AKI). We also compared the results of the learned AKI predictionmodels and variable orderings to baseline techniques (logistic regression,random forests, and extreme gradient boosting). The DBNs generated by RAUSachieved 73-83% area under the receiver operating characteristic curve (AUCROC)within 24-hours before AKI; and 71-79% AUCROC within 48-hours before AKI of anystage in a 7-day observation window. Insights from this automated framework canhelp efficiently implement and interpret DBNs for clinical decision support.The source code for RAUS is available in GitHub at .", "output": "Automated Dynamic Bayesian Networks for Predicting Acute Kidney Injury Before Onset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Questions remain on the robustness of data-driven learning methods whencrossing the gap from simulation to reality. We utilize weight anchoring, amethod known from continual learning, to cultivate and fixate desired behaviorin Neural Networks. Weight anchoring may be used to find a solution to alearning problem that is nearby the solution of another learning problem.Thereby, learning can be carried out in optimal environments without neglectingor unlearning desired behavior. We demonstrate this approach on the example oflearning mixed QoS-efficient discrete resource scheduling with infrequentpriority messages. Results show that this method provides performancecomparable to the state of the art of augmenting a simulation environment,alongside significantly increased robustness and steerability.", "output": "Robust Deep Reinforcement Learning Scheduling via Weight Anchoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent progress of text-to-image generation has been recognized inarchitectural design. Our study is the first to investigate the potential oftext-to-image generators in supporting creativity during the early stages ofthe architectural design process. We conducted a laboratory study with 17architecture students, who developed a concept for a culture center using threepopular text-to-image generators: Midjourney, Stable Diffusion, and DALL-E.Through standardized questionnaires and group interviews, we found that imagegeneration could be a meaningful part of the design process when designconstraints are carefully considered. Generative tools support serendipitousdiscovery of ideas and an imaginative mindset, enriching the design process. Weidentified several challenges of image generators and provided considerationsfor software development and educators to support creativity and emphasizedesigners' imaginative mindset. By understanding the limitations and potentialof text-to-image generators, architects and designers can leverage thistechnology in their design process and education, facilitating innovation andeffective communication of concepts.", "output": "Using Text-to-Image Generation for Architectural Design Ideation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spiking neural networks (SNNs) have gained attention as models of sparse andevent-driven communication of biological neurons, and as such have shownincreasing promise for energy-efficient applications in neuromorphic hardware.As with classical artificial neural networks (ANNs), predictive uncertaintiesare important for decision making in high-stakes applications, such asautonomous vehicles, medical diagnosis, and high frequency trading. Yet,discussion of uncertainty estimation in SNNs is limited, and approaches foruncertainty estimation in artificial neural networks (ANNs) are not directlyapplicable to SNNs. Here, we propose an efficient Monte Carlo(MC)-dropout basedapproach for uncertainty estimation in SNNs. Our approach exploits thetime-step mechanism of SNNs to enable MC-dropout in a computationally efficientmanner, without introducing significant overheads during training and inferencewhile demonstrating high accuracy and uncertainty quality.", "output": "Efficient Uncertainty Estimation in Spiking Neural Networks via MC-dropout."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Facial Expression Recognition (FER) is an active research domain that hasshown great progress recently, notably thanks to the use of large deep learningmodels. However, such approaches are particularly energy intensive, which makestheir deployment difficult for edge devices. To address this issue, SpikingNeural Networks (SNNs) coupled with event cameras are a promising alternative,capable of processing sparse and asynchronous events with lower energyconsumption. In this paper, we establish the first use of event cameras forFER, named \"Event-based FER\", and propose the first related benchmarks byconverting popular video FER datasets to event streams. To deal with this newtask, we propose \"Spiking-FER\", a deep convolutional SNN model, and compare itagainst a similar Artificial Neural Network (ANN). Experiments show that theproposed approach achieves comparable performance to the ANN architecture,while consuming less energy by orders of magnitude (up to 65.39x). In addition,an experimental study of various event-based data augmentation techniques isperformed to provide insights into the efficient transformations specific toevent-based FER.", "output": "Spiking-Fer: Spiking Neural Network for Facial Expression Recognition With Event Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point cloud based 3D deep model has wide applications in many applicationssuch as autonomous driving, house robot, and so on. Inspired by the recentprompt learning in natural language processing, this work proposes a novelMulti-view Vision-Prompt Fusion Network (MvNet) for few-shot 3D point cloudclassification. MvNet investigates the possibility of leveraging theoff-the-shelf 2D pre-trained models to achieve the few-shot classification,which can alleviate the over-dependence issue of the existing baseline modelstowards the large-scale annotated 3D point cloud data. Specifically, MvNetfirst encodes a 3D point cloud into multi-view image features for a number ofdifferent views. Then, a novel multi-view prompt fusion module is developed toeffectively fuse information from different views to bridge the gap between 3Dpoint cloud data and 2D pre-trained models. A set of 2D image prompts can thenbe derived to better describe the suitable prior knowledge for a large-scalepre-trained image model for few-shot 3D point cloud classification. Extensiveexperiments on ModelNet, ScanObjectNN, and ShapeNet datasets demonstrate thatMvNet achieves new state-of-the-art performance for 3D few-shot point cloudimage classification. The source code of this work will be available soon.", "output": "Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain-adaptive trajectory imitation is a skill that some predators learn forsurvival, by mapping dynamic information from one domain (their speed andsteering direction) to a different domain (current position of the movingprey). An intelligent agent with this skill could be exploited for a diversityof tasks, including the recognition of abnormal motion in traffic once it haslearned to imitate representative trajectories. Towards this direction, wepropose DATI, a deep reinforcement learning agent designed for domain-adaptivetrajectory imitation using a cycle-consistent generative adversarial method.Our experiments on a variety of synthetic families of reference trajectoriesshow that DATI outperforms baseline methods for imitation learning and optimalcontrol in this setting, keeping the same per-task hyperparameters. Itsgeneralization to a real-world scenario is shown through the discovery ofabnormal motion patterns in maritime traffic, opening the door for the use ofdeep reinforcement learning methods for spatially-unconstrained trajectory datamining.", "output": "Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The wireless metaverse will create diverse user experiences at theintersection of the physical, digital, and virtual worlds. These experienceswill enable novel interactions between the constituents (e.g., extended reality(XR) users and avatars) of the three worlds. However, remarkably, to date,there is no holistic vision that identifies the full set of metaverse worlds,constituents, and experiences, and the implications of their associatedinteractions on next-generation communication and computing systems. In thispaper, we present a holistic vision of a limitless, wireless metaverse thatdistills the metaverse into an intersection of seven worlds and experiencesthat include the: i) physical, digital, and virtual worlds, along with the ii)cyber, extended, live, and parallel experiences. We then articulate how theseexperiences bring forth interactions between diverse metaverse constituents,namely, a) humans and avatars and b) connected intelligence systems and theirdigital twins (DTs). Then, we explore the wireless, computing, and artificialintelligence (AI) challenges that must be addressed to establishmetaverse-ready networks that support these experiences and interactions. Weparticularly highlight the need for end-to-end synchronization of DTs, and therole of human-level AI and reasoning abilities for cognitive avatars. Moreover,we articulate a sequel of open questions that should ignite the quest for thefuture metaverse. We conclude with a set of recommendations to deploy thelimitless metaverse over future wireless systems.", "output": "The Seven Worlds and Experiences of the Wireless Metaverse: Challenges and Opportunities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trust is a crucial factor affecting the adoption of machine learning (ML)models. Qualitative studies have revealed that end-users, particularly in themedical domain, need models that can express their uncertainty indecision-making allowing users to know when to ignore the model'srecommendations. However, existing approaches for quantifying decision-makinguncertainty are not model-agnostic, or they rely on complex statisticalderivations that are not easily understood by laypersons or end-users, makingthem less useful for explaining the model's decision-making process. This workproposes a set of class-independent meta-heuristics that can characterize thecomplexity of an instance in terms of factors are mutually relevant to bothhuman and ML decision-making. The measures are integrated into a meta-learningframework that estimates the risk of misclassification. The proposed frameworkoutperformed predicted probabilities in identifying instances at risk of beingmisclassified. The proposed measures and framework hold promise for improvingmodel development for more complex instances, as well as providing a new meansof model abstention and explanation.", "output": "A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot relation reasoning on knowledge graphs (FS-KGR) aims to inferlong-tail data-poor relations, which has drawn increasing attention these yearsdue to its practicalities. The pre-training of previous methods needs tomanually construct the meta-relation set, leading to numerous labor costs.Self-supervised learning (SSL) is treated as a solution to tackle the issue,but still at an early stage for FS-KGR task. Moreover, most of the existingmethods ignore leveraging the beneficial information from aliasing relations(AR), i.e., data-rich relations with similar contextual semantics to the targetdata-poor relation. Therefore, we proposed a novel Self-Supervised Learningmodel by leveraging Aliasing Relations to assist FS-KGR, termed SARF.Concretely, four main components are designed in our model, i.e., SSL reasoningmodule, AR-assisted mechanism, fusion module, and scoring function. We firstgenerate the representation of the co-occurrence patterns in a generativemanner. Meanwhile, the representations of aliasing relations are learned toenhance reasoning in the AR-assist mechanism. Besides, multiple strategies,i.e., simple summation and learnable fusion, are offered for representationfusion. Finally, the generated representation is used for scoring. Extensiveexperiments on three few-shot benchmarks demonstrate that SARF achievesstate-of-the-art performance compared with other methods in most cases.", "output": "SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative DNNs are a powerful tool for image synthesis, but they are limitedby their computational load. On the other hand, given a trained model and atask, e.g. faces generation within a range of characteristics, the output imagequality will be unevenly distributed among images with differentcharacteristics. It follows, that we might restrain the models complexity onsome instances, maintaining a high quality. We propose a method for diminishingcomputations by adding so-called early exit branches to the originalarchitecture, and dynamically switching the computational path depending on howdifficult it will be to render the output. We apply our method on two differentSOTA models performing generative tasks: generation from a semantic map, andcross-reenactment of face expressions; showing it is able to output images withcustom lower-quality thresholds. For a threshold of LPIPS &lt;=0.1, we diminishtheir computations by up to a half. This is especially relevant for real-timeapplications such as synthesis of faces, when quality loss needs to becontained, but most of the inputs need fewer computations than the complexinstances.", "output": "FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated augmentation is an emerging and effective technique to search fordata augmentation policies to improve generalizability of deep neural networktraining. Most existing work focuses on constructing a unified policyapplicable to all data samples in a given dataset, without considering sampleor class variations. In this paper, we propose a novel two-stage dataaugmentation algorithm, named Label-Aware AutoAugment (LA3), which takesadvantage of the label information, and learns augmentation policies separatelyfor samples of different labels. LA3 consists of two learning stages, where inthe first stage, individual augmentation methods are evaluated and ranked foreach label via Bayesian Optimization aided by a neural predictor, which allowsus to identify effective augmentation techniques for each label under a lowsearch cost. And in the second stage, a composite augmentation policy isconstructed out of a selection of effective as well as complementaryaugmentations, which produces significant performance boost and can be easilydeployed in typical model training. Extensive experiments demonstrate that LA3achieves excellent performance matching or surpassing existing methods onCIFAR-10 and CIFAR-100, and achieves a new state-of-the-art ImageNet accuracyof 79.97% on ResNet-50 among auto-augmentation methods, while maintaining a lowcomputational cost.", "output": "LA3: Efficient Label-Aware AutoAugment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scientific understanding is a fundamental goal of science, allowing us toexplain the world. There is currently no good way to measure the scientificunderstanding of agents, whether these be humans or Artificial Intelligencesystems. Without a clear benchmark, it is challenging to evaluate and comparedifferent levels of and approaches to scientific understanding. In thisRoadmap, we propose a framework to create a benchmark for scientificunderstanding, utilizing tools from philosophy of science. We adopt abehavioral notion according to which genuine understanding should be recognizedas an ability to perform certain tasks. We extend this notion by considering aset of questions that can gauge different levels of scientific understanding,covering information retrieval, the capability to arrange information toproduce an explanation, and the ability to infer how things would be differentunder different circumstances. The Scientific Understanding Benchmark (SUB),which is formed by a set of these tests, allows for the evaluation andcomparison of different approaches. Benchmarking plays a crucial role inestablishing trust, ensuring quality control, and providing a basis forperformance evaluation. By aligning machine and human scientific understandingwe can improve their utility, ultimately advancing scientific understanding andhelping to discover new insights within machines.", "output": "Towards a Benchmark for Scientific Understanding in Humans and Machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning has recently attracted attention from the researchcommunity, as it aims to solve long-standing limitations of classicsupervisedly-trained models. However, most research on this subject has tackledcontinual learning in simple image classification scenarios. In this paper, wepresent a benchmark of state-of-the-art continual learning methods on videoaction recognition. Besides the increased complexity due to the temporaldimension, the video setting imposes stronger requirements on computingresources for top-performing rehearsal methods. To counteract the increasedmemory requirements, we present two method-agnostic variants for rehearsalmethods, exploiting measures of either model confidence or data information toselect memorable samples. Our experiments show that, as expected from theliterature, rehearsal methods outperform other approaches; moreover, theproposed memory-efficient variants are shown to be effective at retaining acertain level of performance with a smaller buffer size.", "output": "A baseline on continual learning methods for video action recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In symbolic regression, the goal is to find an analytical expression thataccurately fits experimental data with the minimal use of mathematical symbolssuch as operators, variables, and constants. However, the combinatorial spaceof possible expressions can make it challenging for traditional evolutionaryalgorithms to find the correct expression in a reasonable amount of time. Toaddress this issue, Neural Symbolic Regression (NSR) algorithms have beendeveloped that can quickly identify patterns in the data and generateanalytical expressions. However, these methods, in their current form, lack thecapability to incorporate user-defined prior knowledge, which is often requiredin natural sciences and engineering fields. To overcome this limitation, wepropose a novel neural symbolic regression method, named Neural SymbolicRegression with Hypothesis (NSRwH) that enables the explicit incorporation ofassumptions about the expected structure of the ground-truth expression intothe prediction process. Our experiments demonstrate that the proposedconditioned deep learning model outperforms its unconditioned counterparts interms of accuracy while also providing control over the predicted expressionstructure.", "output": "Controllable Neural Symbolic Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In multi-agent reinforcement learning (MARL), self-interested agents attemptto establish equilibrium and achieve coordination depending on game structure.However, existing MARL approaches are mostly bound by the simultaneous actionsof all agents in the Markov game (MG) framework, and few works consider theformation of equilibrium strategies via asynchronous action coordination. Inview of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium,we construct a spatio-temporal sequential decision-making structure derivedfrom the MG and propose an N-level policy model based on a conditionalhypernetwork shared by all agents. This approach allows for asymmetric trainingwith symmetric execution, with each agent responding optimally conditioned onthe decisions made by superior agents. Agents can learn heterogeneous SEpolicies while still maintaining parameter sharing, which leads to reduced costfor learning and storage and enhanced scalability as the number of agentsincreases. Experiments demonstrate that our method effectively converges to theSE policies in repeated matrix game scenarios, and performs admirably inimmensely complex settings including cooperative tasks and mixed tasks.", "output": "Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a model-free reinforcement learning architecture, calleddistributed attentional actor architecture after conditional attention (DA6-X),to provide better interpretability of conditional coordinated behaviors. Theunderlying principle involves reusing the saliency vector, which represents theconditional states of the environment, such as the global position of agents.Hence, agents with DA6-X flexibility built into their policy exhibit superiorperformance by considering the additional information in the conditional statesduring the decision-making process. The effectiveness of the proposed methodwas experimentally evaluated by comparing it with conventional methods in anobjects collection game. By visualizing the attention weights from DA6-X, weconfirmed that agents successfully learn situation-dependent coordinatedbehaviors by correctly identifying various conditional states, leading toimproved interpretability of agents along with superior performance.", "output": "Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce CPDL+, a family of expressive logics rooted in PropositionalDynamic Logic (PDL). In terms of expressive power, CPDL+ strictly contains PDLextended with intersection and converse (a.k.a. ICPDL) as well as ConjunctiveQueries (CQ), Conjunctive Regular Path Queries (CRPQ), or some known extensionsthereof (Regular Queries and CQPDL). We investigate the expressive power,characterization of bisimulation, satisfiability, and model checking for CPDL+.We argue that natural subclasses of CPDL+ can be defined in terms of thetree-width of the underlying graphs of the formulas. We show that the class ofCPDL+ formulas of tree-width 2 is equivalent to ICPDL, and that it alsocoincides with CPDL+ formulas of tree-width 1. However, beyond tree-width 2,incrementing the tree-width strictly increases the expressive power. Wecharacterize the expressive power for every class of fixed tree-width formulasin terms of a bisimulation game with pebbles. Based on this characterization,we show that CPDL+ has a tree-like model property. We prove that thesatisfiability problem is decidable in 2ExpTime on fixed tree-width formulas,coinciding with the complexity of ICPDL. We also exhibit classes for whichsatisfiability is reduced to ExpTime. Finally, we establish that the modelchecking problem for fixed tree-width formulas is in ptime, contrary to thefull class CPDL+.", "output": "PDL on Steroids: on Expressive Extensions of PDL with Intersection and Converse."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Objective: This paper proposes a deep learning model for breast cancerdetection from reconstructed images of microwave imaging scan data and aims toimprove the accuracy and efficiency of breast tumor detection, which could havea significant impact on breast cancer diagnosis and treatment. Methods: Ourframework consists of different convolutional neural network (CNN)architectures for feature extraction and a region-based CNN for tumordetection. We use 7 different architectures: DenseNet201, ResNet50,InceptionV3, InceptionResNetV3, MobileNetV2, NASNetMobile and NASNetLarge andcompare its performance to find the best architecture out of the seven. Anexperimental dataset of MRI-derived breast phantoms was used. Results:NASNetLarge is the best architecture which can be used for the CNN model withaccuracy of 88.41% and loss of 27.82%. Given that the model's AUC is 0.786, itcan be concluded that it is suitable for use in its present form, while itcould be improved upon and trained on other datasets that are comparable.Impact: One of the main causes of death in women is breast cancer, and earlyidentification is essential for enhancing the results for patients. Due to itsnon-invasiveness and capacity to produce high-resolution images, microwaveimaging is a potential tool for breast cancer screening. The complexity oftumors makes it difficult to adequately detect them in microwave images. Theresults of this research show that deep learning has a lot of potential forbreast cancer detection in microwave images", "output": "Breast cancer detection using deep learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Driven by deep learning techniques, perception technology in autonomousdriving has developed rapidly in recent years. To achieve accurate and robustperception capabilities, autonomous vehicles are often equipped with multiplesensors, making sensor fusion a crucial part of the perception system. Amongthese fused sensors, radars and cameras enable a complementary andcost-effective perception of the surrounding environment regardless of lightingand weather conditions. This review aims to provide a comprehensive guidelinefor radar-camera fusion, particularly concentrating on perception tasks relatedto object detection and semantic segmentation. Based on the principles of theradar and camera sensors, we delve into the data processing process andrepresentations, followed by an in-depth analysis and summary of radar-camerafusion datasets. In the review of methodologies in radar-camera fusion, weaddress interrogative questions, including \"why to fuse\", \"what to fuse\",\"where to fuse\", \"when to fuse\", and \"how to fuse\", subsequently discussingvarious challenges and potential research directions within this domain. Toease the retrieval and comparison of datasets and fusion methods, we alsoprovide an interactive website:", "output": "Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent work, Lissovoi, Oliveto, and Warwicker (Artificial Intelligence(2023)) proved that the Move Acceptance Hyper-Heuristic (MAHH) leaves the localoptimum of the multimodal cliff benchmark with remarkable efficiency. With its$O(n^3)$ runtime, for almost all cliff widths $d,$ the MAHH massivelyoutperforms the $Theta(n^d)$ runtime of simple elitist evolutionary algorithms(EAs). For the most prominent multimodal benchmark, the jump functions, thegiven runtime estimates of $O(n^{2m} m^{-Theta(m)})$ and$Omega(2^{Omega(m)})$, for gap size $m ge 2$, are far apart and the realperformance of MAHH is still an open question.In this work, we resolve this question. We prove that for any choice of theMAHH selection parameter~$p$, the expected runtime of the MAHH on a jumpfunction with gap size $m = o(n^{1/2})$ is at least $Omega(n^{2m-1} /(2m-1)!)$. This renders the MAHH much slower than simple elitist evolutionaryalgorithms with their typical $O(n^m)$ runtime.We also show that the MAHH with the global bit-wise mutation operator insteadof the local one-bit operator optimizes jump functions in time $O(min{mn^m,frac{n^{2m-1}}{m!Omega(m)^{m-2}}})$, essentially the minimum of theoptimization times of the $(1+1)$ EA and the MAHH. This suggests that combiningseveral ways to cope with local optima can be a fruitful approach.", "output": "How the Move Acceptance Hyper-Heuristic Copes With Local Optima: Drastic Differences Between Jumps and Cliffs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current approaches to program synthesis with Large Language Models (LLMs)exhibit a \"near miss syndrome\": they tend to generate programs thatsemantically resemble the correct answer (as measured by text similaritymetrics or human evaluation), but achieve a low or even zero accuracy asmeasured by unit tests due to small imperfections, such as the wrong input oroutput format. This calls for an approach known as Synthesize, Execute, Debug(SED), whereby a draft of the solution is generated first, followed by aprogram repair phase addressing the failed tests. To effectively apply thisapproach to instruction-driven LLMs, one needs to determine which promptsperform best as instructions for LLMs, as well as strike a balance betweenrepairing unsuccessful programs and replacing them with newly generated ones.We explore these trade-offs empirically, comparing replace-focused,repair-focused, and hybrid debug strategies, as well as differenttemplate-based and model-based prompt-generation techniques. We use OpenAICodex as the LLM and Program Synthesis Benchmark 2 as a database of problemdescriptions and tests for evaluation. The resulting framework outperforms bothconventional usage of Codex without the repair phase and traditional geneticprogramming approaches.", "output": "Fully Autonomous Programming with Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present work in progress on the role of artificialintelligence (AI) chatbots, such as ChatGPT, in facilitating data access tofederated knowledge graphs. In particular, we provide examples from the fieldof bioinformatics, to illustrate the potential use of Conversational AI todescribe datasets, as well as generate and explain (federated) queries acrossdatasets for the benefit of domain experts.", "output": "On the Potential of Artificial Intelligence Chatbots for Data Exploration of Federated Bioinformatics Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents our efforts to democratize ChatGPT across language. Werelease a large language model \"Phoenix\", achieving competitive performanceamong open-source English and Chinese models while excelling in languages withlimited resources (covering both Latin and non-Latin languages). We believethis work will be beneficial to make ChatGPT more accessible, especially incountries where people cannot use ChatGPT due to restrictions from OpenAI orlocal goverments. Our data, code, and models are available at", "output": "Phoenix: Democratizing ChatGPT across Languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive language-image pretraining (CLIP) has demonstrated remarkablesuccess in various image tasks. However, how to extend CLIP with effectivetemporal modeling is still an open and crucial problem. Existing factorized orjoint spatial-temporal modeling trades off between the efficiency andperformance. While modeling temporal information within straight through tubeis widely adopted in literature, we find that simple frame alignment alreadyprovides enough essence without temporal attention. To this end, in this paper,we proposed a novel Implicit Learnable Alignment (ILA) method, which minimizesthe temporal modeling effort while achieving incredibly high performance.Specifically, for a frame pair, an interactive point is predicted in eachframe, serving as a mutual information rich region. By enhancing the featuresaround the interactive point, two frames are implicitly aligned. The alignedfeatures are then pooled into a single token, which is leveraged in thesubsequent spatial self-attention. Our method allows eliminating the costly orinsufficient temporal self-attention in video. Extensive experiments onbenchmarks demonstrate the superiority and generality of our module.Particularly, the proposed ILA achieves a top-1 accuracy of 88.7% onKinetics-400 with much fewer FLOPs compared with Swin-L and ViViT-H. Code isreleased at  .", "output": "Implicit Temporal Modeling with Learnable Alignment for Video Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep reinforcement learning algorithms that learn policies by trial-and-errormust learn from limited amounts of data collected by actively interacting withthe environment. While many prior works have shown that proper regularizationtechniques are crucial for enabling data-efficient RL, a general understandingof the bottlenecks in data-efficient RL has remained unclear. Consequently, ithas been difficult to devise a universal technique that works well across alldomains. In this paper, we attempt to understand the primary bottleneck insample-efficient deep RL by examining several potential hypotheses such asnon-stationarity, excessive action distribution shift, and overfitting. Weperform thorough empirical analysis on state-based DeepMind control suite (DMC)tasks in a controlled and systematic way to show that high temporal-difference(TD) error on the validation set of transitions is the main culprit thatseverely affects the performance of deep RL algorithms, and prior methods thatlead to good performance do in fact, control the validation TD error to be low.This observation gives us a robust principle for making deep RL efficient: wecan hill-climb on the validation TD error by utilizing any form ofregularization techniques from supervised learning. We show that a simpleonline model selection method that targets the validation TD error is effectiveacross state-based DMC and Gym tasks.", "output": "Efficient Deep Reinforcement Learning Requires Regulating Overfitting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conformation Generation is a fundamental problem in drug discovery andcheminformatics. And organic molecule conformation generation, particularly invacuum and protein pocket environments, is most relevant to drug design.Recently, with the development of geometric neural networks, the data-drivenschemes have been successfully applied in this field, both for molecularconformation generation (in vacuum) and binding pose generation (in proteinpocket). The former beats the traditional ETKDG method, while the latterachieves similar accuracy compared with the widely used molecular dockingsoftware. Although these methods have shown promising results, some researchershave recently questioned whether deep learning (DL) methods perform better inmolecular conformation generation via a parameter-free method. To our surprise,what they have designed is some kind analogous to the famous infinite monkeytheorem, the monkeys that are even equipped with physics education. To discussthe feasibility of their proving, we constructed a real infinite stochasticmonkey for molecular conformation generation, showing that even with a morestochastic sampler for geometry generation, the coverage of the benchmarkQM-computed conformations are higher than those of most DL-based methods. Byextending their physical monkey algorithm for binding pose prediction, we alsodiscover that the successful docking rate also achieves near-best performanceamong existing DL-based docking models. Thus, though their conclusions areright, their proof process needs more concern.", "output": "Infinite Physical Monkey: Do Deep Learning Methods Really Perform Better in Conformation Generation?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite a growing body of work at the intersection of deep learning andformal languages, there has been relatively little systematic exploration oftransformer models for reasoning about typed lambda calculi. This is aninteresting area of inquiry for two reasons. First, typed lambda calculi arethe lingua franc of programming languages. A set of heuristics that relatevarious typed lambda calculi to effective neural architectures would provide asystematic method for mapping language features (e.g., polymorphism, subtyping,inheritance, etc.) to architecture choices. Second, transformer models arewidely used in deep learning architectures applied to code, but the design andhyperparameter space for them is large and relatively unexplored in programminglanguage applications. Therefore, we suggest a benchmark that allows us toexplore exactly this through perhaps the simplest and most fundamental propertyof a programming language: the relationship between terms and types.Consequently, we begin this inquiry of transformer architectures for typedlambda calculi by exploring the effect of transformer warm-up and optimizerselection in the task of type inference: i.e., predicting the types of lambdacalculus terms using only transformers. We find that the optimization landscapeis difficult even in this simple setting. One particular experimental findingis that optimization by Adafactor converges much faster compared to theoptimization by Adam and RAdam. We conjecture that such different performanceof optimizers might be related to the difficulties of generalization overformally generated dataset.", "output": "Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The big data software stack based on Apache Spark and Hadoop has becomemission critical in many enterprises. Performance of Spark and Hadoop jobsdepends on a large number of configuration settings. Manual tuning is expensiveand brittle. There have been prior efforts to develop on-line and off-lineautomatic tuning approaches to make the big data stack less dependent on manualtuning. These, however, demonstrated only modest performance improvements withvery simple, single-user workloads on small data sets. This paper presentsKERMIT - the autonomic architecture for big data capable of automaticallytuning Apache Spark and Hadoop on-line, and achieving performance results 30%faster than rule-of-thumb tuning by a human administrator and up to 92% as fastas the fastest possible tuning established by performing an exhaustive searchof the tuning parameter space. KERMIT can detect important workload changeswith up to 99% accuracy, and predict future workload types with up to 96%accuracy. It is capable of identifying and classifying complex multi-userworkloads without being explicitly trained on examples of these workloads. Itdoes not rely on the past workload history to predict the future workloadclasses and their associated performance. KERMIT can identify and learn newworkload classes, and adapt to workload drift, without human intervention.", "output": "Autonomic Architecture for Big Data Performance Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Video Pre-trained Transformer. VPT uses four SOTA encoder modelsfrom prior work to convert a video into a sequence of compact embeddings. Ourbackbone, based on a reference Flan-T5-11B architecture, learns a universalrepresentation of the video that is a non-linear sum of the encoder models. Itlearns using an autoregressive causal language modeling loss by predicting thewords spoken in YouTube videos. Finally, we evaluate on standard downstreambenchmarks by training fully connected prediction heads for each task. To thebest of our knowledge, this is the first use of multiple frozen SOTA models asencoders in an \"embedding -&gt; backbone -&gt; prediction head\" design pattern - allothers have trained their own joint encoder models. Additionally, we includemore modalities than the current SOTA, Merlot Reserve, by adding explicit SceneGraph information. For these two reasons, we believe it could combine theworld's best open-source models to achieve SOTA performance. Initialexperiments demonstrate the model is learning appropriately, but moreexperimentation and compute is necessary, and already in progress, to realizeour loftier goals. Alongside this work, we build on the YT-20M dataset,reproducing it and adding 25,000 personally selected YouTube videos to itscorpus. All code and model checkpoints are open sourced under a standard MITlicense.", "output": "Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The latent space of GANs contains rich semantics reflecting the trainingdata. Different methods propose to learn edits in latent space corresponding tosemantic attributes, thus allowing to modify generated images. Most supervisedmethods rely on the guidance of classifiers to produce such edits. However,classifiers can lead to out-of-distribution regions and be fooled byadversarial samples. We propose an alternative formulation based on theWasserstein loss that avoids such problems, while maintaining performanceon-par with classifier-based approaches. We demonstrate the effectiveness ofour method on two datasets (digits and faces) using StyleGAN2.", "output": "Wasserstein Loss for Semantic Editing in the Latent Space of GANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An ever-growing number of vulnerabilities are reported every day. Yet thesevulnerabilities are not all the same; Some are more targeted than others.Correctly estimating the likelihood of a vulnerability being exploited is acritical task for system administrators. This aids the system administrators inprioritizing and patching the right vulnerabilities. Our work makes use ofoutlier detection techniques to predict vulnerabilities that are likely to beexploited in highly imbalanced and high-dimensional datasets such as theNational Vulnerability Database. We propose a dimensionality reductiontechnique, OutCenTR, that enhances the baseline outlier detection models. Wefurther demonstrate the effectiveness and efficiency of OutCenTR empiricallywith 4 benchmark and 12 synthetic datasets. The results of our experiments showon average a 5-fold improvement of F1 score in comparison with state-of-the-artdimensionality reduction techniques such as PCA and GRP.", "output": "OutCenTR: A novel semi-supervised framework for predicting exploits of vulnerabilities in high-dimensional datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in Large Language Models, such as ChatGPT, havedemonstrated significant potential to impact various aspects of human life.However, ChatGPT still faces challenges in aspects like faithfulness. Takingquestion answering as a representative application, we seek to understand whyChatGPT falls short in answering questions faithfully. To address thisquestion, we attempt to analyze the failures of ChatGPT in complex open-domainquestion answering and identifies the abilities under the failures.Specifically, we categorize ChatGPT's failures into four types: comprehension,factualness, specificity, and inference. We further pinpoint three criticalabilities associated with QA failures: knowledge memorization, knowledgeassociation, and knowledge reasoning. Additionally, we conduct experimentscentered on these abilities and propose potential approaches to enhancefaithfulness. The results indicate that furnishing the model with fine-grainedexternal knowledge, hints for knowledge association, and guidance for reasoningcan empower the model to answer questions more faithfully.", "output": "Why Does ChatGPT Fall Short in Answering Questions Faithfully?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evolution of convolutional neural networks (CNNs) can be largelyattributed to the design of its architecture, i.e., the network wiring pattern.Neural architecture search (NAS) advances this by automating the search for theoptimal network architecture, but the resulting network instance may notgeneralize well in different tasks. To overcome this, exploring network designprinciples that are generalizable across tasks is a more practical solution. Inthis study, We explore a novel brain-inspired design principle based on thecore-periphery property of the human brain network to guide the design of CNNs.Our work draws inspiration from recent studies suggesting that artificial andbiological neural networks may have common principles in optimizing networkarchitecture. We implement the core-periphery principle in the design ofnetwork wiring patterns and the sparsification of the convolution operation.The resulting core-periphery principle guided CNNs (CP-CNNs) are evaluated onthree different datasets. The experiments demonstrate the effectiveness andsuperiority compared to CNNs and ViT-based methods. Overall, our workcontributes to the growing field of brain-inspired AI by incorporating insightsfrom the human brain into the design of neural networks.", "output": "CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In situ visualization and steering of computational modeling can beeffectively achieved using reactive programming, which leverages temporalabstraction and data caching mechanisms to create dynamic workflows. However,implementing a temporal cache for large-scale simulations can be challenging.Implicit neural networks have proven effective in compressing large volumedata. However, their application to distributed data has yet to be fullyexplored. In this work, we develop an implicit neural representation fordistributed volume data and incorporate it into the DIVA reactive programmingsystem. This implementation enables us to build an in situ temporal cachingsystem with a capacity 100 times larger than previously achieved. We integrateour implementation into the Ascent infrastructure and evaluate its performanceusing real-world simulations.", "output": "Distributed Neural Representation for Reactive in situ Visualization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training segmentation models for medical images continues to be challengingdue to the limited availability and acquisition expense of data annotations.Segment Anything Model (SAM) is a foundation model trained on over 1 billionannotations, predominantly for natural images, that is intended to be able tosegment the user-defined object of interest in an interactive manner. Despiteits impressive performance on natural images, it is unclear how the model isaffected when shifting to medical image domains. Here, we perform an extensiveevaluation of SAM's ability to segment medical images on a collection of 11medical imaging datasets from various modalities and anatomies. In ourexperiments, we generated point prompts using a standard method that simulatesinteractive segmentation. Experimental results show that SAM's performancebased on single prompts highly varies depending on the task and the dataset,i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset,evaluated by IoU. Performance appears to be high for tasks includingwell-circumscribed objects with unambiguous prompts and poorer in many otherscenarios such as segmentation of tumors. When multiple prompts are provided,performance improves only slightly overall, but more so for datasets where theobject is not contiguous. An additional comparison to RITM showed a much betterperformance of SAM for one prompt but a similar performance of the two methodsfor a larger number of prompts. We conclude that SAM shows impressiveperformance for some datasets given the zero-shot learning setup but poor tomoderate performance for multiple other datasets. While SAM as a model and as alearning paradigm might be impactful in the medical imaging domain, extensiveresearch is needed to identify the proper ways of adapting it in this domain.", "output": "Segment Anything Model for Medical Image Analysis: an Experimental Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE),efficiently learn a rich representation of the input. However, for adapting todownstream tasks, they require a sufficient amount of labeled data since theirrich features capture not only objects but also less relevant image background.In contrast, Instance Discrimination (ID) methods focus on objects. In thiswork, we study how to combine the efficiency and scalability of MIM with theability of ID to perform downstream classification in the absence of largeamounts of labeled data. To this end, we introduce Masked AutoencoderContrastive Tuning (MAE-CT), a sequential approach that applies NearestNeighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes therich features such that they form semantic clusters of objects without usingany labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CTmatches or excels previous self-supervised methods trained on ImageNet inlinear probing, k-NN and low-shot classification accuracy as well as inunsupervised clustering accuracy. Notably, similar results can be achievedwithout additional image augmentations. While ID methods generally rely onhand-crafted augmentations to avoid shortcut learning, we find that nearestneighbor lookup is sufficient and that this data-driven augmentation effectimproves with model size. MAE-CT is compute efficient. For instance, startingfrom a MAE pre-trained ViT-L/16, MAE-CT increases the ImageNet 1% low-shotaccuracy from 67.7% to 72.6%, linear probing accuracy from 76.0% to 80.2% andk-NN accuracy from 60.6% to 79.1% in just five hours using eight A100 GPUs.", "output": "Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts suchas floaters or flawed geometry when rendered outside the camera trajectory.Existing evaluation protocols often do not capture these effects, since theyusually only assess image quality at every 8th frame of the training capture.To push forward progress in novel-view synthesis, we propose a new dataset andevaluation procedure, where two camera trajectories are recorded of the scene:one used for training, and the other for evaluation. In this more challengingin-the-wild setting, we find that existing hand-crafted regularizers do notremove floaters nor improve scene geometry. Thus, we propose a 3Ddiffusion-based method that leverages local 3D priors and a novel density-basedscore distillation sampling loss to discourage artifacts during NeRFoptimization. We show that this data-driven prior removes floaters and improvesscene geometry for casual captures.", "output": "Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper discusses the representation of ontologies in the first-orderlogical environment {ttfamily FOLE}. An ontology defines the primitives withwhich to model the knowledge resources for a community of discourse. Theseprimitives consist of classes, relationships and properties. An ontology usesformal axioms to constrain the interpretation of these primitives. In short, anontology specifies a logical theory. This paper continues the discussion of therepresentation and interpretation of ontologies in the first-order logicalenvironment {ttfamily FOLE}. The formalism and semantics of (many-sorted)first-order logic can be developed in both a emph{classification form} and anemph{interpretation form}. Two papers, the current paper, defining the conceptof a structure, and ``The {ttfamily ERA} of {ttfamily FOLE}:Superstructure'', defining the concept of a sound logic, represent theemph{classification form}, corresponding to ideas discussed in the``Information Flow Framework''. Two papers, ``The {ttfamily FOLE} Table'',defining the concept of a relational table, and ``The {ttfamily FOLE}Database'', defining the concept of a relational database, represent theemph{interpretation form}, expanding on material found in the paper ``DatabaseSemantics''. Although the classification form follows theentity-relationship-attribute data model of Chen, the interpretation formincorporates the relational data model of Codd. A fifth paper ``{ttfamilyFOLE} Equivalence'' proves that the classification form is equivalent to theinterpretation form. In general, the {ttfamily FOLE} representation uses aconceptual structures approach, that is completely compatible with the theoryof institutions, formal concept analysis and information flow.", "output": "The ERA of FOLE: Foundation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The abundance of data has given machine learning considerable momentum innatural sciences and engineering, though modeling of physical processes isoften difficult. A particularly tough problem is the efficient representationof geometric boundaries. Triangularized geometric boundaries are wellunderstood and ubiquitous in engineering applications. However, it isnotoriously difficult to integrate them into machine learning approaches due totheir heterogeneity with respect to size and orientation. In this work, weintroduce an effective theory to model particle-boundary interactions, whichleads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modifygraph structures to obey boundary conditions. The new BGNNs are tested oncomplex 3D granular flow processes of hoppers, rotating drums and mixers, whichare all standard components of modern industrial machinery but still havecomplicated geometry. BGNNs are evaluated in terms of computational efficiencyas well as prediction accuracy of particle flows and mixing entropies. BGNNsare able to accurately reproduce 3D granular flows within simulationuncertainties over hundreds of thousands of simulation timesteps. Most notably,in our experiments, particles stay within the geometric objects without usinghandcrafted conditions or restrictions.", "output": "Boundary Graph Neural Networks for 3D Simulations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection in autonomous driving applications implies that thedetection and tracking of semantic objects are commonly native to urban drivingenvironments, as pedestrians and vehicles. One of the major challenges instate-of-the-art deep-learning based object detection are false positives whichoccur with overconfident scores. This is highly undesirable in autonomousdriving and other critical robotic-perception domains because of safetyconcerns. This paper proposes an approach to alleviate the problem ofoverconfident predictions by introducing a novel probabilistic layer to deepobject detection networks in testing. The suggested approach avoids thetraditional Sigmoid or Softmax prediction layer which often producesoverconfident predictions. It is demonstrated that the proposed techniquereduces overconfidence in the false positives without degrading the performanceon the true positives. The approach is validated on the 2D-KITTI objectiondetection through the YOLOV4 and SECOND (Lidar-based detector). The proposedapproach enables interpretable probabilistic predictions without therequirement of re-training the network and therefore is very practical.", "output": "Probabilistic Approach for Road-Users Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As more and more AI agents are used in practice, it is time to think abouthow to make these agents fully autonomous so that they can (1) learn bythemselves continually in a self-motivated and self-initiated manner ratherthan being retrained offline periodically on the initiation of human engineersand (2) accommodate or adapt to unexpected or novel circumstances. As thereal-world is an open environment that is full of unknowns or novelties, thecapabilities of detecting novelties, characterizing them,accommodating/adapting to them, gathering ground-truth training data andincrementally learning the unknowns/novelties become critical in making the AIagent more and more knowledgeable, powerful and self-sustainable over time. Thekey challenge here is how to automate the process so that it is carried outcontinually on the agent's own initiative and through its own interactions withhumans, other agents and the environment just like human on-the-job learning.This paper proposes a framework (called SOLA) for this learning paradigm topromote the research of building autonomous and continual learning enabled AIagents. To show feasibility, an implemented agent is also described.", "output": "AI Autonomy : Self-Initiated Open-World Continual Learning and Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The analysis of logs is a vital activity undertaken for fault or cyberincident detection, investigation and technical forensics analysis for systemand cyber resilience. The potential application of AI algorithms for Loganalysis could augment such complex and laborious tasks. However, such solutionhas its constraints the heterogeneity of log sources and limited to no labelsfor training a classifier. When such labels become available, the need for theclassifier to be updated. This practice-based research seeks to address thesechallenges with the use of Transformer construct to train a new model with onlynormal log entries. Log augmentation through multiple forms of perturbation isapplied as a form of self-supervised training for feature learning. The modelis further finetuned using a form of reinforcement learning with a limited setof label samples to mimic real-world situation with the availability of labels.The experimental results of our model construct show promise with comparativeevaluation measurements paving the way for future practical applications.", "output": "AI based Log Analyser: A Practical Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning is a machine learning training paradigm that enablesclients to jointly train models without sharing their own localized data.However, the implementation of federated learning in practice still facesnumerous challenges, such as the large communication overhead due to therepetitive server-client synchronization and the lack of adaptivity bySGD-based model updates. Despite that various methods have been proposed forreducing the communication cost by gradient compression or quantization, andthe federated versions of adaptive optimizers such as FedAdam are proposed toadd more adaptivity, the current federated learning framework still cannotsolve the aforementioned challenges all at once. In this paper, we propose anovel communication-efficient adaptive federated learning method (FedCAMS) withtheoretical convergence guarantees. We show that in the nonconvex stochasticoptimization setting, our proposed FedCAMS achieves the same convergence rateof $O(frac{1}{sqrt{TKm}})$ as its non-compressed counterparts. Extensiveexperiments on various benchmarks verify our theoretical analysis.", "output": "Communication-Efficient Adaptive Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current best performing models for knowledge graph reasoning (KGR) introducegeometry objects or probabilistic distributions to embed entities andfirst-order logical (FOL) queries into low-dimensional vector spaces. They canbe summarized as a center-size framework (point/box/cone, Beta/Gaussiandistribution, etc.). However, they have limited logical reasoning ability. Andit is difficult to generalize to various features, because the center and sizeare one-to-one constrained, unable to have multiple centers or sizes. Toaddress these challenges, we instead propose a novel KGR framework namedFeature-Logic Embedding framework, FLEX, which is the first KGR framework thatcan not only TRULY handle all FOL operations including conjunction,disjunction, negation and so on, but also support various feature spaces.Specifically, the logic part of feature-logic framework is based on vectorlogic, which naturally models all FOL operations. Experiments demonstrate thatFLEX significantly outperforms existing state-of-the-art methods on benchmarkdatasets.", "output": "FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a multi-agent reinforcement learning dynamics, and analyze itsconvergence in infinite-horizon discounted Markov potential games. We focus onthe independent and decentralized setting, where players do not have knowledgeof the game model and cannot coordinate. In each stage, players update theirestimate of a perturbed Q-function that evaluates their total contingent payoffbased on the realized one-stage reward in an asynchronous manner. Then, playersindependently update their policies by incorporating a smoothed optimalone-stage deviation strategy based on the estimated Q-function. A key featureof the learning dynamics is that the Q-function estimates are updated at afaster timescale than the policies. We prove that the policies induced by ourlearning dynamics converge to a stationary Nash equilibrium in Markov potentialgames with probability 1. Our results highlight the efficacy of simple learningdynamics in reaching a stationary Nash equilibrium even in environments withminimal information available.", "output": "Independent and Decentralized Learning in Markov Potential Games."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Reinforcement Learning (DRL) algorithms have been increasingly employedduring the last decade to solve various decision-making problems such asautonomous driving and robotics. However, these algorithms have faced greatchallenges when deployed in safety-critical environments since they oftenexhibit erroneous behaviors that can lead to potentially critical errors. Oneway to assess the safety of DRL agents is to test them to detect possiblefaults leading to critical failures during their execution. This raises thequestion of how we can efficiently test DRL policies to ensure theircorrectness and adherence to safety requirements. Most existing works ontesting DRL agents use adversarial attacks that perturb states or actions ofthe agent. However, such attacks often lead to unrealistic states of theenvironment. Their main goal is to test the robustness of DRL agents ratherthan testing the compliance of agents' policies with respect to requirements.Due to the huge state space of DRL environments, the high cost of testexecution, and the black-box nature of DRL algorithms, the exhaustive testingof DRL agents is impossible. In this paper, we propose a Search-based TestingApproach of Reinforcement Learning Agents (STARLA) to test the policy of a DRLagent by effectively searching for failing executions of the agent within alimited testing budget. We use machine learning models and a dedicated geneticalgorithm to narrow the search towards faulty episodes. We apply STARLA onDeep-Q-Learning agents which are widely used as benchmarks and show that itsignificantly outperforms Random Testing by detecting more faults related tothe agent's policy. We also investigate how to extract rules that characterizefaulty episodes of the DRL agent using our search results. Such rules can beused to understand the conditions under which the agent fails and thus assessits deployment risks.", "output": "A Search-Based Testing Approach for Deep Reinforcement Learning Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep cooperative multi-agent reinforcement learning has demonstrated itsremarkable success over a wide spectrum of complex control tasks. However,recent advances in multi-agent learning mainly focus on value decompositionwhile leaving entity interactions still intertwined, which easily leads toover-fitting on noisy interactions between entities. In this work, we introducea novel interactiOn Pattern disenTangling (OPT) method, to disentangle not onlythe joint value function into agent-wise value functions for decentralizedexecution, but also the entity interactions into interaction prototypes, eachof which represents an underlying interaction pattern within a subgroup of theentities. OPT facilitates filtering the noisy interactions between irrelevantentities and thus significantly improves generalizability as well asinterpretability. Specifically, OPT introduces a sparse disagreement mechanismto encourage sparsity and diversity among discovered interaction prototypes.Then the model selectively restructures these prototypes into a compactinteraction pattern by an aggregator with learnable weights. To alleviate thetraining instability issue caused by partial observability, we propose tomaximize the mutual information between the aggregation weights and the historybehaviors of each agent. Experiments on both single-task and multi-taskbenchmarks demonstrate that the proposed method yields results superior to thestate-of-the-art counterparts. Our code is available at", "output": "Interaction Pattern Disentangling for Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training machine learning (ML) algorithms is a computationally intensiveprocess, which is frequently memory-bound due to repeatedly accessing largetraining datasets. As a result, processor-centric systems (e.g., CPU, GPU)suffer from costly data movement between memory units and processing units,which consumes large amounts of energy and execution cycles. Memory-centriccomputing systems, i.e., with processing-in-memory (PIM) capabilities, canalleviate this data movement bottleneck.Our goal is to understand the potential of modern general-purpose PIMarchitectures to accelerate ML training. To do so, we (1) implement severalrepresentative classic ML algorithms (namely, linear regression, logisticregression, decision tree, K-Means clustering) on a real-world general-purposePIM architecture, (2) rigorously evaluate and characterize them in terms ofaccuracy, performance and scaling, and (3) compare to their counterpartimplementations on CPU and GPU. Our evaluation on a real memory-centriccomputing system with more than 2500 PIM cores shows that general-purpose PIMarchitectures can greatly accelerate memory-bound ML workloads, when thenecessary operations and datatypes are natively supported by PIM hardware. Forexample, our PIM implementation of decision tree is $27times$ faster than astate-of-the-art CPU version on an 8-core Intel Xeon, and $1.34times$ fasterthan a state-of-the-art GPU version on an NVIDIA A100. Our K-Means clusteringon PIM is $2.8times$ and $3.2times$ than state-of-the-art CPU and GPUversions, respectively.To our knowledge, our work is the first one to evaluate ML training on areal-world PIM architecture. We conclude with key observations, takeaways, andrecommendations that can inspire users of ML workloads, programmers of PIMarchitectures, and hardware designers &amp; architects of future memory-centriccomputing systems.", "output": "An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "If autonomous AI systems are to be reliably safe in novel situations, theywill need to incorporate general principles guiding them to recognize and avoidharmful behaviours. Such principles may need to be supported by a bindingsystem of regulation, which would need the underlying principles to be widelyaccepted. They should also be specific enough for technical implementation.Drawing inspiration from law, this article explains how negative human rightscould fulfil the role of such principles and serve as a foundation both for aninternational regulatory system and for building technical safety constraintsfor future AI systems.", "output": "Negative Human Rights as a Basis for Long-term AI Safety and Regulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a network of smart sensors for edge computing application thatsample a signal of interest and send updates to a base station for remoteglobal monitoring. Sensors are equipped with sensing and compute, and caneither send raw data or process them on-board before transmission. Limitedhardware resources at the edge generate a fundamental latency-accuracytrade-off: raw measurements are inaccurate but timely, whereas accurateprocessed updates are available after computational delay. Also, if sensoron-board processing entails data compression, latency caused by wirelesscommunication might be higher for raw measurements. Hence, one needs to decidewhen sensors should transmit raw measurements or rely on local processing tomaximize overall network performance. To tackle this sensing design problem, wemodel an estimation-theoretic optimization framework that embeds computationand communication delays, and propose a Reinforcement Learning-based approachto dynamically allocate computational resources at each sensor. Effectivenessof our proposed approach is validated through numerical simulations with casestudies motivated by the Internet of Drones and self-driving vehicles.", "output": "To Compute or not to Compute? Adaptive Smart Sensing in Resource-Constrained Edge Computing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In contrast to centralized model training that involves data collection,federated learning (FL) enables remote clients to collaboratively train a modelwithout exposing their private data. However, model performance usuallydegrades in FL due to the heterogeneous data generated by clients of diversecharacteristics. One promising strategy to maintain good performance is bylimiting the local training from drifting far away from the global model.Previous studies accomplish this by regularizing the distance between therepresentations learned by the local and global models. However, they onlyconsider representations from the early layers of a model or the layerpreceding the output layer. In this study, we introduce FedIntR, which providesa more fine-grained regularization by integrating the representations ofintermediate layers into the local training process. Specifically, FedIntRcomputes a regularization term that encourages the closeness between theintermediate layer representations of the local and global models.Additionally, FedIntR automatically determines the contribution of each layer'srepresentation to the regularization term based on the similarity between localand global representations. We conduct extensive experiments on variousdatasets to show that FedIntR can achieve equivalent or higher performancecompared to the state-of-the-art approaches. Our code is available at", "output": "Federated Learning with Intermediate Representation Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extracting information from radiofrequency (RF) signals using artificialneural networks at low energy cost is a critical need for a wide range ofapplications from radars to health. These RF inputs are composed of multiplesfrequencies. Here we show that magnetic tunnel junctions can process analogueRF inputs with multiple frequencies in parallel and perform synapticoperations. Using a backpropagation-free method called extreme learning, weclassify noisy images encoded by RF signals, using experimental data frommagnetic tunnel junctions functioning as both synapses and neurons. We achievethe same accuracy as an equivalent software neural network. These results are akey step for embedded radiofrequency artificial intelligence.", "output": "Classification of multi-frequency RF signals by extreme learning, using magnetic tunnel junctions as neurons and synapses."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, machine learning (ML) has become a popular approach to supportself-adaptation. ML has been used to deal with several problems inself-adaptation, such as maintaining an up-to-date runtime model underuncertainty and scalable decision-making. Yet, exploiting ML comes withinherent challenges. In this paper, we focus on a particularly importantchallenge for learning-based self-adaptive systems: drift in adaptation spaces.With adaptation space we refer to the set of adaptation options a self-adaptivesystem can select from at a given time to adapt based on the estimated qualityproperties of the adaptation options. Drift of adaptation spaces originatesfrom uncertainties, affecting the quality properties of the adaptation options.Such drift may imply that eventually no adaptation option can satisfy theinitial set of the adaptation goals, deteriorating the quality of the system,or adaptation options may emerge that allow enhancing the adaptation goals. InML, such shift corresponds to novel class appearance, a type of concept driftin target data that common ML techniques have problems dealing with. To tacklethis problem, we present a novel approach to self-adaptation that enhanceslearning-based self-adaptive systems with a lifelong ML layer. We refer to thisapproach as lifelong self-adaptation. The lifelong ML layer tracks the systemand its environment, associates this knowledge with the current tasks,identifies new tasks based on differences, and updates the learning models ofthe self-adaptive system accordingly. A human stakeholder may be involved tosupport the learning process and adjust the learning and goal models. Wepresent a general architecture for lifelong self-adaptation and apply it to thecase of drift of adaptation spaces that affects the decision-making inself-adaptation. We validate the approach for a series of scenarios using theDeltaIoT exemplar.", "output": "Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A model-agnostic variable importance method can be used with arbitraryprediction functions. Here we present some model-free methods that do notrequire access to the prediction function. This is useful when that function isproprietary and not available, or just extremely expensive. It is also usefulwhen studying residuals from a model. The cohort Shapley (CS) method ismodel-free but has exponential cost in the dimension of the input space. Asupervised on-manifold Shapley method from Frye et al. (2020) is also modelfree but requires as input a second black box model that has to be trained forthe Shapley value problem. We introduce an integrated gradient (IG) version ofcohort Shapley, called IGCS, with cost $mathcal{O}(nd)$. We show that over thevast majority of the relevant unit cube that the IGCS value function is closeto a multilinear function for which IGCS matches CS. Another benefit of IGCS isthat is allows IG methods to be used with binary predictors. We use some areabetween curves (ABC) measures to quantify the performance of IGCS. On a problemfrom high energy physics we verify that IGCS has nearly the same ABCs as CSdoes. We also use it on a problem from computational chemistry in 1024variables. We see there that IGCS attains much higher ABCs than we get fromMonte Carlo sampling. The code is publicly available at", "output": "Model free variable importance for high dimensional data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-parametric episodic memory can be used to quickly latch ontohigh-rewarded experience in reinforcement learning tasks. In contrast toparametric deep reinforcement learning approaches in which reward signals needto be back-propagated slowly, these methods only need to discover the solutiononce, and may then repeatedly solve the task. However, episodic controlsolutions are stored in discrete tables, and this approach has so far only beenapplied to discrete action space problems. Therefore, this paper introducesContinuous Episodic Control (CEC), a novel non-parametric episodic memoryalgorithm for sequential decision making in problems with a continuous actionspace. Results on several sparse-reward continuous control environments showthat our proposed method learns faster than state-of-the-art model-free RL andmemory-augmented RL algorithms, while maintaining good long-run performance aswell. In short, CEC can be a fast approach for learning in continuous controltasks.", "output": "Continuous Episodic Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Power grids, across the world, play an important societal and economical roleby providing uninterrupted, reliable and transient-free power to severalindustries, businesses and household consumers. With the advent of renewablepower resources and EVs resulting into uncertain generation and highly dynamicload demands, it has become ever so important to ensure robust operation ofpower networks through suitable management of transient stability issues andlocalize the events of blackouts. In the light of ever increasing stress on themodern grid infrastructure and the grid operators, this paper presents areinforcement learning (RL) framework, PowRL, to mitigate the effects ofunexpected network events, as well as reliably maintain electricity everywhereon the network at all times. The PowRL leverages a novel heuristic for overloadmanagement, along with the RL-guided decision making on optimal topologyselection to ensure that the grid is operated safely and reliably (with nooverloads). PowRL is benchmarked on a variety of competition datasets hosted bythe L2RPN (Learning to Run a Power Network). Even with its reduced actionspace, PowRL tops the leaderboard in the L2RPN NeurIPS 2020 challenge(Robustness track) at an aggregate level, while also being the top performingagent in the L2RPN WCCI 2020 challenge. Moreover, detailed analysis depictsstate-of-the-art performances by the PowRL agent in some of the test scenarios.", "output": "PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A reliable critic is central to on-policy actor-critic learning. But itbecomes challenging to learn a reliable critic in a multi-agent sparse rewardscenario due to two factors: 1) The joint action space grows exponentially withthe number of agents 2) This, combined with the reward sparseness andenvironment noise, leads to large sample requirements for accurate learning. Weshow that regularising the critic with spectral normalization (SN) enables itto learn more robustly, even in multi-agent on-policy sparse reward scenarios.Our experiments show that the regularised critic is quickly able to learn fromthe sparse rewarding experience in the complex SMAC and RWARE domains. Thesefindings highlight the importance of regularisation in the critic for stablelearning.", "output": "Effects of Spectral Normalization in Multi-agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatio-temporal modeling as a canonical task of multivariate time seriesforecasting has been a significant research topic in AI community. To addressthe underlying heterogeneity and non-stationarity implied in the graph streams,in this study, we propose Spatio-Temporal Meta-Graph Learning as a novel GraphStructure Learning mechanism on spatio-temporal data. Specifically, weimplement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN)by plugging the Meta-Graph Learner powered by a Meta-Node Bank into GCRNencoder-decoder. We conduct a comprehensive evaluation on two benchmarkdatasets (METR-LA and PEMS-BAY) and a large-scale spatio-temporal dataset thatcontains a variaty of non-stationary phenomena. Our model outperformed thestate-of-the-arts to a large degree on all three datasets (over 27% MAE and 34%RMSE). Besides, through a series of qualitative evaluations, we demonstratethat our model can explicitly disentangle locations and time slots withdifferent patterns and be robustly adaptive to different anomalous situations.Codes and datasets are available at ", "output": "MegaCRN: Meta-Graph Convolutional Recurrent Network for Spatio-Temporal Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As an important variant of entity alignment (EA), multi-modal entityalignment (MMEA) aims to discover identical entities across different knowledgegraphs (KGs) with relevant images attached. We noticed that current MMEAalgorithms all globally adopt the KG-level modality fusion strategies formulti-modal entity representation but ignore the variation in modalitypreferences for individual entities, hurting the robustness to potential noiseinvolved in modalities (e.g., blurry images and relations). In this paper, wepresent MEAformer, a multi-modal entity alignment transformer approach for metamodality hybrid, which dynamically predicts the mutual correlation coefficientsamong modalities for entity-level feature aggregation. A modal-aware hardentity replay strategy is further proposed for addressing vague entity details.Experimental results show that our model not only achieves SOTA performance onmultiple training scenarios including supervised, unsupervised, iterative, andlow resource, but also has a comparable number of parameters, optimistic speed,and good interpretability. Our code and data are available at", "output": "MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph alignment, which aims at identifying corresponding entities acrossmultiple networks, has been widely applied in various domains. As the graphs tobe aligned are usually constructed from different sources, the inconsistencyissues of structures and features between two graphs are ubiquitous inreal-world applications. Most existing methods follow the``embed-then-cross-compare'' paradigm, which computes node embeddings in eachgraph and then processes node correspondences based on cross-graph embeddingcomparison. However, we find these methods are unstable and sub-optimal whenstructure or feature inconsistency appears. To this end, we propose SLOTAlign,an unsupervised graph alignment framework that jointly performs StructureLearning and Optimal Transport Alignment. We convert graph alignment to anoptimal transport problem between two intra-graph matrices without therequirement of cross-graph comparison. We further incorporate multi-viewstructure learning to enhance graph representation power and reduce the effectof structure and feature inconsistency inherited across graphs. Moreover, analternating scheme based algorithm has been developed to address the jointoptimization problem in SLOTAlign, and the provable convergence result is alsoestablished. Finally, we conduct extensive experiments on six unsupervisedgraph alignment datasets and the DBP15K knowledge graph (KG) alignmentbenchmark dataset. The proposed SLOTAlign shows superior performance andstrongest robustness over seven unsupervised graph alignment methods and fivespecialized KG alignment methods.", "output": "Robust Attributed Graph Alignment via Joint Structure Learning and Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Minecraft is a great testbed for human creativity that has inspired thedesign of various structures and even functioning machines, including flyingmachines. EvoCraft is an API for programmatically generating structures inMinecraft, but the initial work in this domain was not capable of evolvingflying machines. This paper applies fitness-based evolution and qualitydiversity search in order to evolve flying machines. Although fitness alone canoccasionally produce flying machines, thanks in part to a more sophisticatedfitness function than was used previously, the quality diversity algorithmMAP-Elites is capable of discovering flying machines much more reliably, atleast when an appropriate behavior characterization is used to guide the searchfor diverse solutions.", "output": "Evolving Flying Machines in Minecraft Using Quality Diversity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We re-replicate 14 psychology studies from the Many Labs 2 replicationproject (Klein et al., 2018) with OpenAI's text-davinci-003 model, colloquiallyknown as GPT3.5. Among the eight studies we could analyse, our GPT samplereplicated 37.5% of the original results and 37.5% of the Many Labs 2 results.We could not analyse the remaining six studies, due to an unexpected phenomenonwe call the \"correct answer\" effect. Different runs of GPT3.5 answered nuancedquestions probing political orientation, economic preference, judgement, andmoral philosophy with zero or near-zero variation in responses: with thesupposedly \"correct answer.\" Most but not all of these \"correct answers\" wererobust to changing the order of answer choices. One exception occurred in theMoral Foundations Theory survey (Graham et al., 2009), for which GPT3.5 almostalways identified as a conservative in the original condition (N=1,030, 99.6%)and as a liberal in the reverse-order condition (N=1,030, 99.3%). GPT3.5'sresponses to subsequent questions revealed post-hoc rationalisation; there wasa relative bias in the direction of its previously reported politicalorientation. But both self-reported GPT conservatives and self-reported GPTliberals revealed right-leaning Moral Foundations, although the right-leaningbias of self-reported GPT liberals was weaker. We hypothesise that this patternwas learned from a conservative bias in the model's largely Internet-basedtraining data. Since AI models of the future may be trained on much of the sameInternet data as GPT3.5, our results raise concerns that a hypothetical AI-ledfuture may be subject to a diminished diversity of thought.", "output": "\"Correct answers\" from the psychology of artificial intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For deep ordinal classification, learning a well-structured feature spacespecific to ordinal classification is helpful to properly capture the ordinalnature among classes. Intuitively, when Euclidean distance metric is used, anideal ordinal layout in feature space would be that the sample clusters arearranged in class order along a straight line in space. However, enforcingsamples to conform to a specific layout in the feature space is a challengingproblem. To address this problem, in this paper, we propose a novel ConstrainedProxies Learning (CPL) method, which can learn a proxy for each ordinal classand then adjusts the global layout of classes by constraining these proxies.Specifically, we propose two kinds of strategies: hard layout constraint andsoft layout constraint. The hard layout constraint is realized by directlycontrolling the generation of proxies to force them to be placed in a strictlinear layout or semicircular layout (i.e., two instantiations of strictordinal layout). The soft layout constraint is realized by constraining thatthe proxy layout should always produce unimodal proxy-to-proxies similaritydistribution for each proxy (i.e., to be a relaxed ordinal layout). Experimentsshow that the proposed CPL method outperforms previous deep ordinalclassification methods under the same setting of feature extractor.", "output": "Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The current approach to exploring and monitoring complex underwaterecosystems, such as coral reefs, is to conduct surveys using diver-held orstatic cameras, or deploying sensor buoys. These approaches often fail tocapture the full variation and complexity of interactions between differentreef organisms and their habitat. The CUREE platform presented in this paperprovides a unique set of capabilities in the form of robot behaviors andperception algorithms to enable scientists to explore different aspects of anecosystem. Examples of these capabilities include low-altitude visual surveys,soundscape surveys, habitat characterization, and animal following. Wedemonstrate these capabilities by describing two field deployments on coralreefs in the US Virgin Islands. In the first deployment, we show that CUREE canidentify the preferred habitat type of snapping shrimp in a reef through acombination of a visual survey, habitat characterization, and a soundscapesurvey. In the second deployment, we demonstrate CUREE's ability to followarbitrary animals by separately following a barracuda and stingray for severalminutes each in midwater and benthic environments, respectively.", "output": "CUREE: A Curious Underwater Robot for Ecosystem Exploration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rewrite systems [6, 10, 12] have been widely employing equality saturation[9], which is an optimisation methodology that uses a saturated e-graph torepresent all possible sequences of rewrite simultaneously, and then extractsthe optimal one. As such, optimal results can be achieved by avoiding thephase-ordering problem. However, we observe that when the e-graph is notsaturated, it cannot represent all possible rewrite opportunities and thereforethe phase-ordering problem is re-introduced during the construction phase ofthe e-graph. To address this problem, we propose MCTS-GEB, a domain-generalrewrite system that applies reinforcement learning (RL) to e-graphconstruction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3]to efficiently plan for the optimal e-graph construction, and therefore it caneffectively eliminate the phase-ordering problem at the construction phase andachieve better performance within a reasonable time. Evaluation in twodifferent domains shows MCTS-GEB can outperform the state-of-the-art rewritesystems by up to 49x, while the optimisation can generally take less than anhour, indicating MCTS-GEB is a promising building block for the futuregeneration of rewrite systems.", "output": "MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recently proposed open-world object and open-set detection have achieveda breakthrough in finding never-seen-before objects and distinguishing themfrom known ones. However, their studies on knowledge transfer from knownclasses to unknown ones are not deep enough, resulting in the scanty capabilityfor detecting unknowns hidden in the background. In this paper, we propose theunknown sniffer (UnSniffer) to find both unknown and known objects. Firstly,the generalized object confidence (GOC) score is introduced, which only usesknown samples for supervision and avoids improper suppression of unknowns inthe background. Significantly, such confidence score learned from known objectscan be generalized to unknown ones. Additionally, we propose a negative energysuppression loss to further suppress the non-object samples in the background.Next, the best box of each unknown is hard to obtain during inference due tolacking their semantic information in training. To solve this issue, weintroduce a graph-based determination scheme to replace hand-designednon-maximum suppression (NMS) post-processing. Finally, we present the UnknownObject Detection Benchmark, the first publicly benchmark that encompassesprecision evaluation for unknown detection to our knowledge. Experiments showthat our method is far better than the existing state-of-the-art methods.", "output": "Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article provides a curated review of selected papers published inprominent economics journals that use machine learning (ML) tools for researchand policy analysis. The review focuses on three key questions: (1) when ML isused in economics, (2) what ML models are commonly preferred, and (3) how theyare used for economic applications. The review highlights that ML isparticularly used to process nontraditional and unstructured data, capturestrong nonlinearity, and improve prediction accuracy. Deep learning models aresuitable for nontraditional data, whereas ensemble learning models arepreferred for traditional datasets. While traditional econometric models maysuffice for analyzing low-complexity data, the increasing complexity ofeconomic data due to rapid digitalization and the growing literature suggeststhat ML is becoming an essential addition to the econometrician's toolbox.", "output": "Machine Learning for Economics Research: When What and How?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Harnessing logical reasoning ability is a comprehensive natural languageunderstanding endeavor. With the release of Generative Pretrained Transformer 4(GPT-4), highlighted as \"advanced\" at reasoning tasks, we are eager to learnthe GPT-4 performance on various logical reasoning tasks. This report analysesmultiple logical reasoning datasets, with popular benchmarks like LogiQA andReClor, and newly-released datasets like AR-LSAT. We test the multi-choicereading comprehension and natural language inference tasks with benchmarksrequiring logical reasoning. We further construct a logical reasoningout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.We also make a performance comparison between ChatGPT and GPT-4. Experimentresults show that ChatGPT performs significantly better than the RoBERTafine-tuning method on most logical reasoning benchmarks. With early access tothe GPT-4 API we are able to conduct intense experiments on the GPT-4 model.The results show GPT-4 yields even higher performance on most logical reasoningdatasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-knowndatasets like LogiQA and ReClor. However, the performance drops significantlywhen handling newly released and out-of-distribution datasets. Logicalreasoning remains challenging for ChatGPT and GPT-4, especially onout-of-distribution and natural language inference datasets. We release theprompt-style logical reasoning datasets as a benchmark suite and name itLogiEval.", "output": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graphs (KGs) store rich facts about the real world. In this paper,we study KG alignment, which aims to find alignment between not only entitiesbut also relations and classes in different KGs. Alignment at the entity levelcan cross-fertilize alignment at the schema level. We propose a new KGalignment approach, called DAAKG, based on deep learning and active learning.With deep learning, it learns the embeddings of entities, relations andclasses, and jointly aligns them in a semi-supervised manner. With activelearning, it estimates how likely an entity, relation or class pair can beinferred, and selects the best batch for human labeling. We design twoapproximation algorithms for efficient solution to batch selection. Ourexperiments on benchmark datasets show the superior accuracy and generalizationof DAAKG and validate the effectiveness of all its modules.", "output": "Deep Active Alignment of Knowledge Graph Entities and Schemata."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Being able to create meaningful symbols and proficiently use them for highercognitive functions such as communication, reasoning, planning, etc., isessential and unique for human intelligence. Current deep neural networks arestill far behind human's ability to create symbols for such higher cognitivefunctions. Here we propose a solution, named SEA-net, to endow neural networkswith ability of symbol creation, semantic understanding and communication.SEA-net generates symbols that dynamically configure the network to performspecific tasks. These symbols capture compositional semantic information thatenables the system to acquire new functions purely by symbolic manipulation orcommunication. In addition, we found that these self-generated symbols exhibitan intrinsic structure resembling that of natural language, suggesting a commonframework underlying the generation and understanding of symbols in both humanbrains and artificial neural networks. We hope that it will be instrumental inproducing more capable systems in the future that can synergize the strengthsof connectionist and symbolic approaches for AI.", "output": "Emergence of Symbols in Neural Networks for Semantic Understanding and Communication."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper discusses the results for the second edition of the MonocularDepth Estimation Challenge (MDEC). This edition was open to methods using anyform of supervision, including fully-supervised, self-supervised, multi-task orproxy depth. The challenge was based around the SYNS-Patches dataset, whichfeatures a wide diversity of environments with high-quality dense ground-truth.This includes complex natural environments, e.g. forests or fields, which aregreatly underrepresented in current benchmarks.The challenge received eight unique submissions that outperformed theprovided SotA baseline on any of the pointcloud- or image-based metrics. Thetop supervised submission improved relative F-Score by 27.62%, while the topself-supervised improved it by 16.61%. Supervised submissions generallyleveraged large collections of datasets to improve data diversity.Self-supervised submissions instead updated the network architecture andpretrained backbones. These results represent a significant progress in thefield, while highlighting avenues for future research, such as reducinginterpolation artifacts at depth boundaries, improving self-supervised indoorperformance and overall natural image accuracy.", "output": "The Second Monocular Depth Estimation Challenge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Structured reconstruction is a non-trivial dense prediction problem, whichextracts structural information (eg, building corners and edges) from a rasterimage, then reconstructs it to a 2D planar graph accordingly. Compared withcommon segmentation or detection problems, it significantly relays on thecapability that leveraging holistic geometric information for structuralreasoning. Current transformer-based approaches tackle this challenging problemin a two-stage manner, which detect corners in the first model and classify theproposed edges (corner-pairs) in the second model. However, they separatetwo-stage into different models and only share the backbone encoder. Unlike theexisting modeling strategies, we present an enhanced corner representationmethod: 1) It fuses knowledge between the corner detection and edge predictionby sharing feature in different granularity; 2) Corner candidates are proposedin four heatmap channels w.r.t its direction. Both qualitative and quantitativeevaluations demonstrate that our proposed method can better reconstructfine-grained structures, such as adjacent corners and tiny edges. Consequently,it outperforms the state-of-the-art model by +1.9%@F-1 on Corner and+3.0%@F-1 on Edge.", "output": "CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Itinerary recommendation is a complex sequence prediction problem withnumerous real-world applications. This task becomes even more challenging whenconsidering the optimization of multiple user queuing times and crowd levels,as well as numerous involved parameters, such as attraction popularity, queuingtime, walking time, and operating hours. Existing solutions typically focus onsingle-person perspectives and fail to address real-world issues resulting fromnatural crowd behavior, like the Selfish Routing problem. In this paper, weintroduce the Strategic and Crowd-Aware Itinerary Recommendation (SCAIR)algorithm, which optimizes group utility in real-world settings. We model theroute recommendation strategy as a Markov Decision Process and propose a StateEncoding mechanism that enables real-time planning and allocation in lineartime. We evaluate our algorithm against various competitive and realisticbaselines using a theme park dataset, demonstrating that SCAIR outperformsthese baselines in addressing the Selfish Routing problem across four themeparks.", "output": "Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a phenomenal large language model, ChatGPT has achieved unparalleledsuccess in various real-world tasks and increasingly plays an important role inour daily lives and work. However, extensive concerns are also raised about thepotential ethical issues, especially about whether ChatGPT-like artificialgeneral intelligence (AGI) will replace human jobs. To this end, in this paper,we introduce a preliminary data-driven study on the future of ChatGPT-enabledlabor market from the view of Human-AI Symbiosis instead of Human-AIConfrontation. To be specific, we first conduct an in-depth analysis oflarge-scale job posting data in BOSS Zhipin, the largest online recruitmentplatform in China. The results indicate that about 28% of occupations in thecurrent labor market require ChatGPT-related skills. Furthermore, based on alarge-scale occupation-centered knowledge graph, we develop a semanticinformation enhanced collaborative filtering algorithm to predict the futureoccupation-skill relations in the labor market. As a result, we find thatadditional 45% occupations in the future will require ChatGPT-related skills.In particular, industries related to technology, products, and operations areexpected to have higher proficiency requirements for ChatGPT-related skills,while the manufacturing, services, education, and health science relatedindustries will have lower requirements for ChatGPT-related skills.", "output": "The Future of ChatGPT-enabled Labor Market: A Preliminary Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel approach to efficiently compute tight non-convexenclosures of the image through neural networks with ReLU, sigmoid, orhyperbolic tangent activation functions. In particular, we abstract theinput-output relation of each neuron by a polynomial approximation, which isevaluated in a set-based manner using polynomial zonotopes. While our approachcan also can be beneficial for open-loop neural network verification, our mainapplication is reachability analysis of neural network controlled systems,where polynomial zonotopes are able to capture the non-convexity caused by theneural network as well as the system dynamics. This results in a superiorperformance compared to other methods, as we demonstrate on various benchmarks.", "output": "Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Construct theory in social psychology, developed by George Kelly are mentalconstructs to predict and anticipate events. Constructs are how humansinterpret, curate, predict and validate data; information. AI today is biasedbecause it is trained with a narrow construct as defined by the training datalabels. Machine Learning algorithms for facial recognition discriminate againstdarker skin colors and in the ground breaking research papers (Buolamwini, Joyand Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities inCommercial Gender Classification. FAT (2018), the inclusion of phenotypiclabeling is proposed as a viable solution. In Construct theory, phenotype isjust one of the many subelements that make up the construct of a face. In thispaper, we present 15 main elements of the construct of face, with 50subelements and tested Google Cloud Vision API and Microsoft Cognitive ServicesAPI using FairFace dataset that currently has data for 7 races, genders andages, and we retested against FairFace Plus dataset curated by us. Our resultsshow exactly where they have gaps for inclusivity. Based on our experimentresults, we propose that validated, inclusive constructs become industrystandards for AI ML models going forward.", "output": "Introducing Construct Theory as a Standard Methodology for Inclusive AI Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents a novel domain adaption paradigm for studying contrastiveself-supervised representation learning and knowledge transfer using remotesensing satellite data. Major state-of-the-art remote sensing visual domainefforts primarily focus on fully supervised learning approaches that relyentirely on human annotations. On the other hand, human annotations in remotesensing satellite imagery are always subject to limited quantity due to highcosts and domain expertise, making transfer learning a viable alternative. Theproposed approach investigates the knowledge transfer of selfsupervisedrepresentations across the distinct source and target data distributions indepth in the remote sensing data domain. In this arrangement, self-supervisedcontrastive learning-based pretraining is performed on the source dataset, anddownstream tasks are performed on the target datasets in a round-robin fashion.Experiments are conducted on three publicly available datasets, UC MercedLanduse (UCMD), SIRI-WHU, and MLRSNet, for different downstream classificationtasks versus label efficiency. In self-supervised knowledge transfer, theproposed approach achieves state-of-the-art performance with label efficiencylabels and outperforms a fully supervised setting. A more in-depth qualitativeexamination reveals consistent evidence for explainable representationlearning. The source code and trained models are published on GitHub.", "output": "Domain Adaptable Self-supervised Representation Learning on Remote Sensing Satellite Imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weakly-supervised semantic segmentation aims to reduce labeling costs bytraining semantic segmentation models using weak supervision, such asimage-level class labels. However, most approaches struggle to produce accuratelocalization maps and suffer from false predictions in class-relatedbackgrounds (i.e., biased objects), such as detecting a railroad with the trainclass. Recent methods that remove biased objects require additional supervisionfor manually identifying biased objects for each problematic class andcollecting their datasets by reviewing predictions, limiting theirapplicability to the real-world dataset with multiple labels and complexrelationships for biasing. Following the first observation that biased featurescan be separated and eliminated by matching biased objects with backgrounds inthe same dataset, we propose a fully-automatic/model-agnostic biased removalframework called MARS (Model-Agnostic biased object Removal without additionalSupervision), which utilizes semantically consistent features of anunsupervised technique to eliminate biased objects in pseudo labels.Surprisingly, we show that MARS achieves new state-of-the-art results on twopopular benchmarks, PASCAL VOC 2012 (val: 77.7%, test: 77.2%) and MS COCO 2014(val: 49.4%), by consistently improving the performance of various WSSS modelsby at least 30% without additional supervision.", "output": "MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online media has revolutionized the way political information is disseminatedand consumed on a global scale, and this shift has compelled political figuresto adopt new strategies of capturing and retaining voter attention. Thesestrategies often rely on emotional persuasion and appeal, and as visual contentbecomes increasingly prevalent in virtual space, much of politicalcommunication too has come to be marked by evocative video content and imagery.The present paper offers a novel approach to analyzing material of this kind.We apply a deep-learning-based computer-vision algorithm to a sample of 220YouTube videos depicting political leaders from 15 different countries, whichis based on an existing trained convolutional neural network architectureprovided by the Python library fer. The algorithm returns emotion scoresrepresenting the relative presence of 6 emotional states (anger, disgust, fear,happiness, sadness, and surprise) and a neutral expression for each frame ofthe processed YouTube video. We observe statistically significant differencesin the average score of expressed negative emotions between groups of leaderswith varying degrees of populist rhetoric as defined by the Global Party Survey(GPS), indicating that populist leaders tend to express negative emotions to agreater extent during their public performance than their non-populistcounterparts. Overall, our contribution provides insight into thecharacteristics of visual self-representation among political leaders, as wellas an open-source workflow for further computational studies of theirnon-verbal communication.", "output": "The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperspectral image (HSI) classification is challenging due to spatialvariability caused by complex imaging conditions. Prior methods suffer fromlimited representation ability, as they train specially designed networks fromscratch on limited annotated data. We propose a tri-spectral image generationpipeline that transforms HSI into high-quality tri-spectral images, enablingthe use of off-the-shelf ImageNet pretrained backbone networks for featureextraction. Motivated by the observation that there are many homogeneous areaswith distinguished semantic and geometric properties in HSIs, which can be usedto extract useful contexts, we propose an end-to-end segmentation network namedDCN-T. It adopts transformers to effectively encode regional adaptation andglobal aggregation spatial contexts within and between the homogeneous areasdiscovered by similarity-based clustering. To fully exploit the rich spectrumsof the HSI, we adopt an ensemble approach where all segmentation results of thetri-spectral images are integrated into the final prediction through a votingscheme. Extensive experiments on three public benchmarks show that our proposedmethod outperforms state-of-the-art methods for HSI classification.", "output": "DCN-T: Dual Context Network with Transformer for Hyperspectral Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the interest in autonomous systems continues to grow, one of the majorchallenges is collecting sufficient and representative real-world data. Despitethe strong practical and commercial interest in autonomous landing systems inthe aerospace field, there is a lack of open-source datasets of aerial images.To address this issue, we present a dataset-lard-of high-quality aerial imagesfor the task of runway detection during approach and landing phases. Most ofthe dataset is composed of synthetic images but we also provide manuallylabelled images from real landing footages, to extend the detection task to amore realistic setting. In addition, we offer the generator which can producesuch synthetic front-view images and enables automatic annotation of the runwaycorners through geometric transformations. This dataset paves the way forfurther research such as the analysis of dataset quality or the development ofmodels to cope with the detection tasks. Find data, code and more up-to-dateinformation at ", "output": "LARD -- Landing Approach Runway Detection -- Dataset for Vision Based Landing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present KeyMorph, a deep learning-based image registration framework thatrelies on automatically detecting corresponding keypoints. State-of-the-artdeep learning methods for registration often are not robust to largemisalignments, are not interpretable, and do not incorporate the symmetries ofthe problem. In addition, most models produce only a single prediction attest-time. Our core insight which addresses these shortcomings is thatcorresponding keypoints between images can be used to obtain the optimaltransformation via a differentiable closed-form expression. We use thisobservation to drive the end-to-end learning of keypoints tailored for theregistration task, and without knowledge of ground-truth keypoints. Thisframework not only leads to substantially more robust registration but alsoyields better interpretability, since the keypoints reveal which parts of theimage are driving the final alignment. Moreover, KeyMorph can be designed to beequivariant under image translations and/or symmetric with respect to the inputimage ordering. Finally, we show how multiple deformation fields can becomputed efficiently and in closed-form at test time corresponding to differenttransformation variants. We demonstrate the proposed framework in solving 3Daffine and spline-based registration of multi-modal brain MRI scans. Inparticular, we show registration accuracy that surpasses currentstate-of-the-art methods, especially in the context of large displacements. Ourcode is available at ", "output": "A robust and interpretable deep learning framework for multi-modal registration via keypoints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Universal moving object segmentation aims to provide a general model forvideos from all types of natural scenes, as previous approaches are usuallyeffective for specific or similar scenes. In this paper, we propose a methodcalled Learning Temporal Distribution and Spatial Correlation (LTS) that hasthe potential to be a general solution for universal moving objectsegmentation. In the proposed approach, the distribution from temporal pixelsis first learned by our Defect Iterative Distribution Learning (DIDL) networkfor a scene-independent segmentation. Then, the Stochastic Bayesian Refinement(SBR) Network, which learns the spatial correlation, is proposed to improve thebinary mask generated by the DIDL network. Benefiting from the sceneindependence of the temporal distribution and the accuracy improvementresulting from the spatial correlation, the proposed approach performs well foralmost all videos from diverse and complex natural scenes with fixedparameters. Comprehensive experiments on standard datasets including LASIESTA,CDNet2014, BMC, SBMI2015 and 128 real world videos demonstrate the superiorityof proposed approach compared to state-of-the-art methods with or without theuse of deep learning networks. To the best of our knowledge, this work has highpotential to be a general solution for moving object segmentation in real worldenvironments.", "output": "Learning Temporal Distribution and Spatial Correlation for Universal Moving Object Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advances in GPT-based large language models (LLMs) are revolutionizingnatural language processing, exponentially increasing its use across variousdomains. Incorporating uni-directional attention, these autoregressive LLMs cangenerate long and coherent paragraphs. However, for visual question answering(VQA) tasks that require both vision and language processing, models withbi-directional attention or models employing fusion techniques are oftenemployed to capture the context of multiple modalities all at once. As GPT doesnot natively process vision tokens, to exploit the advancements in GPT modelsfor VQA in robotic surgery, we design an end-to-end trainable Language-VisionGPT (LV-GPT) model that expands the GPT2 model to include vision input (image).The proposed LV-GPT incorporates a feature extractor (vision tokenizer) andvision token embedding (token type and pose). Given the limitations ofunidirectional attention in GPT models and their ability to generate coherentlong paragraphs, we carefully sequence the word tokens before vision tokens,mimicking the human thought process of understanding the question to infer ananswer from an image. Quantitatively, we prove that the LV-GPT modeloutperforms other state-of-the-art VQA models on two publically availablesurgical-VQA datasets (based on endoscopic vision challenge robotic scenesegmentation 2018 and CholecTriplet2021) and on our newly annotated dataset(based on the holistic surgical scene dataset). We further annotate all threedatasets to include question-type annotations to allow sub-type analysis.Furthermore, we extensively study and present the effects of token sequencing,token type and pose embedding for vision tokens in the LV-GPT model.", "output": "SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Homography estimation is a basic image-alignment method in many applications.Recently, with the development of convolutional neural networks (CNNs), somelearning based approaches have shown great success in this task. However, theperformance across different domains has never been researched. Unlike othercommon tasks (eg, classification, detection, segmentation), CNN basedhomography estimation models show a domain shift immunity, which means a modelcan be trained on one dataset and tested on another without any transferlearning. To explain this unusual performance, we need to determine how CNNsestimate homography. In this study, we first show the domain shift immunity ofdifferent deep homography estimation models. We then use a shallow network witha specially designed dataset to analyze the features used for estimation. Theresults show that networks use low-level texture information to estimatehomography. We also design some experiments to compare the performance betweendifferent texture densities and image features distorted on some commondatasets to demonstrate our findings. Based on these findings, we provide anexplanation of the domain shift immunity of deep homography estimation.", "output": "Analyzing the Domain Shift Immunity of Deep Homography Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Fields (NeRFs) are a very recent and very popular approachfor the problems of novel view synthesis and 3D reconstruction. A popular scenerepresentation used by NeRFs is to combine a uniform, voxel-based subdivisionof the scene with an MLP. Based on the observation that a (sparse) point cloudof the scene is often available, this paper proposes to use an adaptiverepresentation based on tetrahedra and a Delaunay representation instead of theuniform subdivision or point-based representations. We show that such arepresentation enables efficient training and leads to state-of-the-artresults. Our approach elegantly combines concepts from 3D geometry processing,triangle-based rendering, and modern neural radiance fields. Compared tovoxel-based representations, ours provides more detail around parts of thescene likely to be close to the surface. Compared to point-basedrepresentations, our approach achieves better performance.", "output": "Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Detection of baby cries is an important part of baby monitoring and healthcare. Almost all existing methods use supervised SVM, CNN, or their varieties.In this work, we propose to use weakly supervised anomaly detection to detect ababy cry. In this weak supervision, we only need weak annotation if there is acry in an audio file. We design a data mining technique using the pre-trainedVGGish feature extractor and an anomaly detection network on long untrimmedaudio files. The obtained datasets are used to train a simple CNN featurenetwork for cry/non-cry classification. This CNN is then used as a featureextractor in an anomaly detection framework to achieve better cry detectionperformance.", "output": "Weakly Supervised Detection of Baby Cry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world adversarial physical patches were shown to be successful incompromising state-of-the-art models in a variety of computer visionapplications. Existing defenses that are based on either input gradient orfeatures analysis have been compromised by recent GAN-based attacks thatgenerate naturalistic patches. In this paper, we propose Jedi, a new defenseagainst adversarial patches that is resilient to realistic patch attacks. Jeditackles the patch localization problem from an information theory perspective;leverages two new ideas: (1) it improves the identification of potential patchregions using entropy analysis: we show that the entropy of adversarial patchesis high, even in naturalistic patches; and (2) it improves the localization ofadversarial patches, using an autoencoder that is able to complete patchregions from high entropy kernels. Jedi achieves high-precision adversarialpatch localization, which we show is critical to successfully repair theimages. Since Jedi relies on an input entropy analysis, it is model-agnostic,and can be applied on pre-trained off-the-shelf models without changes to thetraining or inference of the protected models. Jedi detects on average 90% ofadversarial patches across different benchmarks and recovers up to 94% ofsuccessful patch attacks (Compared to 75% and 65% for LGS and Jujutsu,respectively).", "output": "Jedi: Entropy-based Localization and Removal of Adversarial Patches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Selecting appropriate datasets is critical in modern computer vision.However, no general-purpose tools exist to evaluate the extent to which twodatasets differ. For this, we propose representing images - and by extensiondatasets - using Distributions of Neuron Activations (DNAs). DNAs fitdistributions, such as histograms or Gaussians, to activations of neurons in apre-trained feature extractor through which we pass the image(s) to represent.This extractor is frozen for all datasets, and we rely on its generallyexpressive power in feature space. By comparing two DNAs, we can evaluate theextent to which two datasets differ with granular control over the comparisonattributes of interest, providing the ability to customise the way distancesare measured to suit the requirements of the task at hand. Furthermore, DNAsare compact, representing datasets of any size with less than 15 megabytes. Wedemonstrate the value of DNAs by evaluating their applicability on severaltasks, including conditional dataset comparison, synthetic image evaluation,and transfer learning, and across diverse datasets, ranging from synthetic catimages to celebrity faces and urban driving scenes.", "output": "Visual DNA: Representing and Comparing Images using Distributions of Neuron Activations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As AI agents are increasingly used in the real open world with unknowns ornovelties, they need the ability to (1) recognize objects that (i) they havelearned and (ii) detect items that they have not seen or learned before, and(2) learn the new items incrementally to become more and more knowledgeable andpowerful. (1) is called novelty detection or out-of-distribution (OOD)detection and (2) is called class incremental learning (CIL), which is asetting of continual learning (CL). In existing research, OOD detection and CILare regarded as two completely different problems. This paper theoreticallyproves that OOD detection actually is necessary for CIL. We first show that CILcan be decomposed into two sub-problems: within-task prediction (WP) andtask-id prediction (TP). We then prove that TP is correlated with OODdetection. The key theoretical result is that regardless of whether WP and OODdetection (or TP) are defined explicitly or implicitly by a CIL algorithm, goodWP and good OOD detection are necessary and sufficient conditions for good CIL,which unifies novelty or OOD detection and continual learning (CIL, inparticular). A good CIL algorithm based on our theory can naturally be used inopen world learning, which is able to perform both novelty/OOD detection andcontinual learning. Based on the theoretical result, new CIL methods are alsodesigned, which outperform strong baselines in terms of CIL accuracy and itscontinual OOD detection by a large margin.", "output": "Open-World Continual Learning: Unifying Novelty Detection and Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study proposes a deep learning model for the classification andsegmentation of brain tumors from magnetic resonance imaging (MRI) scans. Theclassification model is based on the EfficientNetB1 architecture and is trainedto classify images into four classes: meningioma, glioma, pituitary adenoma,and no tumor. The segmentation model is based on the U-Net architecture and istrained to accurately segment the tumor from the MRI images. The models areevaluated on a publicly available dataset and achieve high accuracy andsegmentation metrics, indicating their potential for clinical use in thediagnosis and treatment of brain tumors.", "output": "Brain tumor multi classification and segmentation in MRO images using deep learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The various aspects like modeling and interpreting 3D environments andsurroundings have enticed humans to progress their research in 3D ComputerVision, Computer Graphics, and Machine Learning. An attempt made by Mildenhallet al in their paper about NeRFs (Neural Radiance Fields) led to a boom inComputer Graphics, Robotics, Computer Vision, and the possible scope ofHigh-Resolution Low Storage Augmented Reality and Virtual Reality-based 3Dmodels have gained traction from res with more than 500 preprints related toNeRFs published. This paper serves as a bridge for people starting to studythese fields by building on the basics of Mathematics, Geometry, ComputerVision, and Computer Graphics to the difficulties encountered in ImplicitRepresentations at the intersection of all these disciplines. This surveyprovides the history of rendering, Implicit Learning, and NeRFs, theprogression of research on NeRFs, and the potential applications andimplications of NeRFs in today's world. In doing so, this survey categorizesall the NeRF-related research in terms of the datasets used, objectivefunctions, applications solved, and evaluation criteria for these applications.", "output": "Neural Radiance Fields: Past, Present, and Future."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the development of the medical image field, researchers seek to developa class of datasets to block the need for medical knowledge, such astext{MedMNIST} (v2). MedMNIST (v2) includes a large number of small-sized (28$times$ 28 or 28 $times$ 28 $times$ 28) medical samples and thecorresponding expert annotations (class label). The existing baseline model(Google AutoML Vision, ResNet-50+3D) can reach an average accuracy of over 70%on MedMNIST (v2) datasets, which is comparable to the performance of expertdecision-making. Nevertheless, we note that there are two insurmountableobstacles to modeling on MedMNIST (v2): 1) the raw images are cropped to lowscales may cause effective recognition information to be dropped and theclassifier to have difficulty in tracing accurate decision boundaries; 2) thelabelers' subjective insight may cause many uncertainties in the label space.To address these issues, we develop a Complex Mixer (C-Mixer) with apre-training framework to alleviate the problem of insufficient information anduncertainty in the label space by introducing an incentive imaginary matrix anda self-supervised scheme with random masking. Our method (incentive learningand self-supervised learning with masking) shows surprising potential on boththe standard MedMNIST (v2) dataset, the customized weakly supervised datasets,and other image enhancement tasks.", "output": "Complex Mixer for MedMNIST Classification Decathlon."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Very low-resolution face recognition (VLRFR) poses unique challenges, such astiny regions of interest and poor resolution due to extreme standoff distanceor wide viewing angle of the acquisition devices. In this paper, we studyprincipled approaches to elevate the recognizability of a face in the embeddingspace instead of the visual quality. We first formulate a robust learning-basedface recognizability measure, namely recognizability index (RI), based on twocriteria: (i) proximity of each face embedding against the unrecognizable facescluster center and (ii) closeness of each face embedding against its positiveand negative class prototypes. We then devise an index diversion loss to pushthe hard-to-recognize face embedding with low RI away from unrecognizable facescluster to boost the RI, which reflects better recognizability. Additionally, aperceptibility attention mechanism is introduced to attend to the mostrecognizable face regions, which offers better explanatory and discriminativetraits for embedding learning. Our proposed model is trained end-to-end andsimultaneously serves recognizability-aware embedding learning and face qualityestimation. To address VLRFR, our extensive evaluations on three challenginglow-resolution datasets and face quality assessment demonstrate the superiorityof the proposed model over the state-of-the-art methods.", "output": "Recognizability Embedding Enhancement for Very Low-Resolution Face Recognition and Quality Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How can we teach a computer to recognize 10,000 different actions? Deeplearning has evolved from supervised and unsupervised to self-supervisedapproaches. In this paper, we present a new contrastive learning-basedframework for decision tree-based classification of actions, includinghuman-human interactions (HHI) and human-object interactions (HOI). The keyidea is to translate the original multi-class action recognition into a seriesof binary classification tasks on a pre-constructed decision tree. Under thenew framework of contrastive learning, we present the design of an interactionadjacent matrix (IAM) with skeleton graphs as the backbone for modeling variousaction-related attributes such as periodicity and symmetry. Through theconstruction of various pretext tasks, we obtain a series of binaryclassification nodes on the decision tree that can be combined to supporthigher-level recognition tasks. Experimental justification for the potential ofour approach in real-world applications ranges from interaction recognition tosymmetry detection. In particular, we have demonstrated the promisingperformance of video-based autism spectrum disorder (ASD) diagnosis on theCalTech interview video database.", "output": "Video-based Contrastive Learning on Decision Trees: from Action Recognition to Autism Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rendering scheme in neural radiance field (NeRF) is effective inrendering a pixel by casting a ray into the scene. However, NeRF yields blurredrendering results when the training images are captured at non-uniform scales,and produces aliasing artifacts if the test images are taken in distant views.To address this issue, Mip-NeRF proposes a multiscale representation as aconical frustum to encode scale information. Nevertheless, this approach isonly suitable for offline rendering since it relies on integrated positionalencoding (IPE) to query a multilayer perceptron (MLP). To overcome thislimitation, we propose mip voxel grids (Mip-VoG), an explicit multiscalerepresentation with a deferred architecture for real-time anti-aliasingrendering. Our approach includes a density Mip-VoG for scene geometry and afeature Mip-VoG with a small MLP for view-dependent color. Mip-VoG encodesscene scale using the level of detail (LOD) derived from ray differentials anduses quadrilinear interpolation to map a queried 3D location to its featuresand density from two neighboring downsampled voxel grids. To our knowledge, ourapproach is the first to offer multiscale training and real-time anti-aliasingrendering simultaneously. We conducted experiments on multiscale datasets, andthe results show that our approach outperforms state-of-the-art real-timerendering baselines.", "output": "Multiscale Representation for Real-Time Anti-Aliasing Neural Rendering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-view shape reconstruction has achieved impressive progresses thanks tothe latest advances in neural implicit surface rendering. However, existingmethods based on signed distance function (SDF) are limited to closed surfaces,failing to reconstruct a wide range of real-world objects that containopen-surface structures. In this work, we introduce a new neural renderingframework, coded NeUDF, that can reconstruct surfaces with arbitrary topologiessolely from multi-view supervision. To gain the flexibility of representingarbitrary surfaces, NeUDF leverages the unsigned distance function (UDF) assurface representation. While a naive extension of an SDF-based neural renderercannot scale to UDF, we propose two new formulations of weight functionspecially tailored for UDF-based volume rendering. Furthermore, to cope withopen surface rendering, where the in/out test is no longer valid, we present adedicated normal regularization strategy to resolve the surface orientationambiguity. We extensively evaluate our method over a number of challengingdatasets, including DTU}, MGN, and Deep Fashion 3D. Experimental resultsdemonstrate that nEudf can significantly outperform the state-of-the-art methodin the task of multi-view surface reconstruction, especially for complex shapeswith open boundaries.", "output": "NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Developing and integrating advanced image sensors with novel algorithms incamera systems are prevalent with the increasing demand for computationalphotography and imaging on mobile platforms. However, the lack of high-qualitydata for research and the rare opportunity for an in-depth exchange of viewsfrom industry and academia constrain the development of mobile intelligentphotography and imaging (MIPI). With the success of the 1st MIPI Workshop@ECCV2022, we introduce the second MIPI challenge, including four tracks focusing onnovel image sensors and imaging algorithms. This paper summarizes and reviewsthe RGBW Joint Remosaic and Denoise track on MIPI 2023. In total, 81participants were successfully registered, and 4 teams submitted results in thefinal testing phase. The final results are evaluated using objective metrics,including PSNR, SSIM, LPIPS, and KLD. A detailed description of the top threemodels developed in this challenge is provided in this paper. More details ofthis challenge and the link to the dataset can be found at", "output": "MIPI 2023 Challenge on RGBW Fusion: Methods and Results."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing pedestrian attribute recognition (PAR) algorithms are mainlydeveloped based on a static image. However, the performance is not reliable forimages with challenging factors, such as heavy occlusion, motion blur, etc. Inthis work, we propose to understand human attributes using video frames thatcan make full use of temporal information. Specifically, we formulate thevideo-based PAR as a vision-language fusion problem and adopt pre-trained bigmodels CLIP to extract the feature embeddings of given video frames. To betterutilize the semantic information, we take the attribute list as another inputand transform the attribute words/phrase into the corresponding sentence viasplit, expand, and prompt. Then, the text encoder of CLIP is utilized forlanguage embedding. The averaged visual tokens and text tokens are concatenatedand fed into a fusion Transformer for multi-modal interactive learning. Theenhanced tokens will be fed into a classification head for pedestrian attributeprediction. Extensive experiments on a large-scale video-based PAR datasetfully validated the effectiveness of our proposed framework.", "output": "Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weak feature representation problem has influenced the performance offew-shot classification task for a long time. To alleviate this problem, recentresearchers build connections between support and query instances throughembedding patch features to generate discriminative representations. However,we observe that there exists semantic mismatches (foreground/ background) amongthese local patches, because the location and size of the target object are notfixed. What is worse, these mismatches result in unreliable similarityconfidences, and complex dense connection exacerbates the problem. According tothis, we propose a novel Clustered-patch Element Connection (CEC) layer tocorrect the mismatch problem. The CEC layer leverages Patch Cluster and ElementConnection operations to collect and establish reliable connections with highsimilarity patch features, respectively. Moreover, we propose a CECNet,including CEC layer based attention module and distance metric. The former isutilized to generate a more discriminative representation benefiting from theglobal clustered-patch features, and the latter is introduced to reliablymeasure the similarity between pair-features. Extensive experiments demonstratethat our CECNet outperforms the state-of-the-art methods on classificationbenchmark. Furthermore, our CEC approach can be extended into few-shotsegmentation and detection tasks, which achieves competitive performances.", "output": "Clustered-patch Element Connection for Few-shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose a task called \"Scene Style Text Editing (SSTE)\",changing the text content as well as the text style of the source image whilekeeping the original text scene. Existing methods neglect to fine-grainedadjust the style of the foreground text, such as its rotation angle, color, andfont type. To tackle this task, we propose a quadruple framework named\"QuadNet\" to embed and adjust foreground text styles in the latent featurespace. Specifically, QuadNet consists of four parts, namely backgroundinpainting, style encoder, content encoder, and fusion generator. Thebackground inpainting erases the source text content and recovers theappropriate background with a highly authentic texture. The style encoderextracts the style embedding of the foreground text. The content encoderprovides target text representations in the latent feature space to implementthe content edits. The fusion generator combines the information yielded fromthe mentioned parts and generates the rendered text images. Practically, ourmethod is capable of performing promisingly on real-world datasets with merelystring-level annotation. To the best of our knowledge, our work is the first tofinely manipulate the foreground text content and style by deeply semanticediting in the latent feature space. Extensive experiments demonstrate thatQuadNet has the ability to generate photo-realistic foreground text and avoidsource text shadows in real-world scenes when editing text content.", "output": "Scene Style Text Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Class-Incremental Learning (CIL) aims to solve the neural networks'catastrophic forgetting problem, which refers to the fact that once the networkupdates on a new task, its performance on previously-learned tasks dropsdramatically. Most successful CIL methods incrementally train a featureextractor with the aid of stored exemplars, or estimate the featuredistribution with the stored prototypes. However, the stored exemplars wouldviolate the data privacy concerns, while the stored prototypes might notreasonably be consistent with a proper feature distribution, hindering theexploration of real-world CIL applications. In this paper, we propose a methodof textit{e}mbedding distillation and textit{Ta}sk-orientedtextit{g}eneration (textit{eTag}) for CIL, which requires neither theexemplar nor the prototype. Instead, eTag achieves a data-free manner to trainthe neural networks incrementally. To prevent the feature extractor fromforgetting, eTag distills the embeddings of the network's intermediate blocks.Additionally, eTag enables a generative network to produce suitable features,fitting the needs of the top incremental classifier. Experimental resultsconfirmed that our proposed eTag considerably outperforms the state-of-the-artmethods on CIFAR-100 and ImageNet-subfootnote{Our code is available in theSupplementary Materials.", "output": "eTag: Class-Incremental Learning with Embedding Distillation and Task-Oriented Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Picking cluttered general objects is a challenging task due to the complexgeometries and various stacking configurations. Many prior works utilize poseestimation for picking, but pose estimation is difficult on cluttered objects.In this paper, we propose Cluttered Objects Descriptors (CODs), a densecluttered objects descriptor that can represent rich object structures, and usethe pre-trained CODs network along with its intermediate outputs to train apicking policy. Additionally, we train the policy with reinforcement learning,which enable the policy to learn picking without supervision. We conductexperiments to demonstrate that our CODs is able to consistently represent seenand unseen cluttered objects, which allowed for the picking policy to robustlypick cluttered general objects. The resulting policy can pick 96.69% of unseenobjects in our experimental environment which is twice as cluttered as thetraining scenarios.", "output": "Reinforcement Learning for Picking Cluttered General Objects with Dense Object Descriptors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adapting a trained model to perform satisfactorily on continually changingtesting domains/environments is an important and challenging task. In thiswork, we propose a novel framework, SATA, which aims to satisfy the followingcharacteristics required for online adaptation: 1) can work seamlessly withdifferent (preferably small) batch sizes to reduce latency; 2) should continueto work well for the source domain; 3) should have minimal tunablehyper-parameters and storage requirements. Given a pre-trained network trainedon source domain data, the proposed SATA framework modifies the batch-normaffine parameters using source anchoring based self-distillation. This ensuresthat the model incorporates the knowledge of the newly encountered domains,without catastrophically forgetting about the previously seen ones. We alsopropose a source-prototype driven contrastive alignment to ensure naturalgrouping of the target samples, while maintaining the already learnt semanticinformation. Extensive evaluation on three benchmark datasets under challengingsettings justify the effectiveness of SATA for real-world applications.", "output": "SATA: Source Anchoring and Target Alignment Network for Continual Test Time Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpreting and explaining the behavior of deep neural networks is criticalfor many tasks. Explainable AI provides a way to address this challenge, mostlyby providing per-pixel relevance to the decision. Yet, interpreting suchexplanations may require expert knowledge. Some recent attempts towardinterpretability adopt a concept-based framework, giving a higher-levelrelationship between some concepts and model decisions. This paper proposesBottleneck Concept Learner (BotCL), which represents an image solely by thepresence/absence of concepts learned through training over the target taskwithout explicit supervision over the concepts. It uses self-supervision andtailored regularizers so that learned concepts can be human-understandable.Using some image classification tasks as our testbed, we demonstrate BotCL'spotential to rebuild neural networks for better interpretability. Code isavailable at  and a simple demo is available at", "output": "Learning Bottleneck Concepts in Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given the great threat of adversarial attacks against Deep Neural Networks(DNNs), numerous works have been proposed to boost transferability to attackreal-world applications. However, existing attacks often utilize advancedgradient calculation or input transformation but ignore the white-box model.Inspired by the fact that DNNs are over-parameterized for superior performance,we propose diversifying the high-level features (DHF) for more transferableadversarial examples. In particular, DHF perturbs the high-level features byrandomly transforming the high-level features and mixing them with the featureof benign samples when calculating the gradient at each iteration. Due to theredundancy of parameters, such transformation does not affect theclassification performance but helps identify the invariant features acrossdifferent models, leading to much better transferability. Empirical evaluationson ImageNet dataset show that DHF could effectively improve the transferabilityof existing momentum-based attacks. Incorporated into the inputtransformation-based attacks, DHF generates more transferable adversarialexamples and outperforms the baselines with a clear margin when attackingseveral defense models, showing its generalization to various attacks and higheffectiveness for boosting transferability.", "output": "Diversifying the High-level Features for better Adversarial Transferability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cone Beam Computed Tomography (CBCT) is widely used in dentistry fordiagnostics and treatment planning. CBCT Imaging has a long acquisition timeand consequently, the patient is likely to move. This motion causes significantartifacts in the reconstructed data which may lead to misdiagnosis. Existingmotion correction algorithms only address this issue partially, struggling withinconsistencies due to truncation, accuracy, and execution speed. On the otherhand, a short-scan reconstruction using a subset of motion-free projectionswith appropriate weighting methods can have a sufficient clinical image qualityfor most diagnostic purposes. Therefore, a framework is used in this study toextract the motion-free part of the scanned projections with which a cleanshort-scan volume can be reconstructed without using correction algorithms.Motion artifacts are detected using deep learning with a slice-based predictionscheme followed by volume averaging to get the final result. A realistic motionsimulation strategy and data augmentation has been implemented to address datascarcity. The framework has been validated by testing it with realmotion-affected data while the model was trained only with simulated motiondata. This shows the feasibility to apply the proposed framework to a broadvariety of motion cases for further research.", "output": "Motion Artifacts Detection in Short-scan Dental CBCT Reconstructions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Talking head generation is to generate video based on a given source identityand target motion. However, current methods face several challenges that limitthe quality and controllability of the generated videos. First, the generatedface often has unexpected deformation and severe distortions. Second, thedriving image does not explicitly disentangle movement-relevant information,such as poses and expressions, which restricts the manipulation of differentattributes during generation. Third, the generated videos tend to haveflickering artifacts due to the inconsistency of the extracted landmarksbetween adjacent frames. In this paper, we propose a novel model that produceshigh-fidelity talking head videos with free control over head pose andexpression. Our method leverages both self-supervised learned landmarks and 3Dface model-based landmarks to model the motion. We also introduce a novelmotion-aware multi-scale feature alignment module to effectively transfer themotion without face distortion. Furthermore, we enhance the smoothness of thesynthesized talking head videos with a feature context adaptation andpropagation module. We evaluate our model on challenging datasets anddemonstrate its state-of-the-art performance. More information is available at", "output": "High-Fidelity and Freely Controllable Talking Head Video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning aims to learn on non-stationary data streams withoutcatastrophically forgetting previous knowledge. Prevalent replay-based methodsaddress this challenge by rehearsing on a small buffer holding the seen data,for which a delicate sample selection strategy is required. However, existingselection schemes typically seek only to maximize the utility of the ongoingselection, overlooking the interference between successive rounds of selection.Motivated by this, we dissect the interaction of sequential selection stepswithin a framework built on influence functions. We manage to identify a newclass of second-order influences that will gradually amplify incidental bias inthe replay buffer and compromise the selection process. To regularize thesecond-order effects, a novel selection objective is proposed, which also hasclear connections to two widely adopted criteria. Furthermore, we present anefficient implementation for optimizing the proposed criterion. Experiments onmultiple continual learning benchmarks demonstrate the advantage of ourapproach over state-of-the-art methods. Code is available at", "output": "Regularizing Second-Order Influences for Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D shape completion from point clouds is a challenging task, especially fromscans of real-world objects. Considering the paucity of 3D shape ground truthsfor real scans, existing works mainly focus on benchmarking this task onsynthetic data, e.g. 3D computer-aided design models. However, the domain gapbetween synthetic and real data limits the generalizability of these methods.Thus, we propose a new task, SCoDA, for the domain adaptation of real scanshape completion from synthetic data. A new dataset, ScanSalon, is contributedwith a bunch of elaborate 3D models created by skillful artists according toscans. To address this new task, we propose a novel cross-domain feature fusionmethod for knowledge transfer and a novel volume-consistent self-trainingframework for robust learning from real data. Extensive experiments prove ourmethod is effective to bring an improvement of 6%~7% mIoU.", "output": "SCoDA: Domain Adaptive Shape Completion for Real Scans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent progress of text-to-image generation has been recognized inarchitectural design. Our study is the first to investigate the potential oftext-to-image generators in supporting creativity during the early stages ofthe architectural design process. We conducted a laboratory study with 17architecture students, who developed a concept for a culture center using threepopular text-to-image generators: Midjourney, Stable Diffusion, and DALL-E.Through standardized questionnaires and group interviews, we found that imagegeneration could be a meaningful part of the design process when designconstraints are carefully considered. Generative tools support serendipitousdiscovery of ideas and an imaginative mindset, enriching the design process. Weidentified several challenges of image generators and provided considerationsfor software development and educators to support creativity and emphasizedesigners' imaginative mindset. By understanding the limitations and potentialof text-to-image generators, architects and designers can leverage thistechnology in their design process and education, facilitating innovation andeffective communication of concepts.", "output": "Using Text-to-Image Generation for Architectural Design Ideation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spiking neural networks (SNNs) have gained attention as models of sparse andevent-driven communication of biological neurons, and as such have shownincreasing promise for energy-efficient applications in neuromorphic hardware.As with classical artificial neural networks (ANNs), predictive uncertaintiesare important for decision making in high-stakes applications, such asautonomous vehicles, medical diagnosis, and high frequency trading. Yet,discussion of uncertainty estimation in SNNs is limited, and approaches foruncertainty estimation in artificial neural networks (ANNs) are not directlyapplicable to SNNs. Here, we propose an efficient Monte Carlo(MC)-dropout basedapproach for uncertainty estimation in SNNs. Our approach exploits thetime-step mechanism of SNNs to enable MC-dropout in a computationally efficientmanner, without introducing significant overheads during training and inferencewhile demonstrating high accuracy and uncertainty quality.", "output": "Efficient Uncertainty Estimation in Spiking Neural Networks via MC-dropout."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Facial Expression Recognition (FER) is an active research domain that hasshown great progress recently, notably thanks to the use of large deep learningmodels. However, such approaches are particularly energy intensive, which makestheir deployment difficult for edge devices. To address this issue, SpikingNeural Networks (SNNs) coupled with event cameras are a promising alternative,capable of processing sparse and asynchronous events with lower energyconsumption. In this paper, we establish the first use of event cameras forFER, named \"Event-based FER\", and propose the first related benchmarks byconverting popular video FER datasets to event streams. To deal with this newtask, we propose \"Spiking-FER\", a deep convolutional SNN model, and compare itagainst a similar Artificial Neural Network (ANN). Experiments show that theproposed approach achieves comparable performance to the ANN architecture,while consuming less energy by orders of magnitude (up to 65.39x). In addition,an experimental study of various event-based data augmentation techniques isperformed to provide insights into the efficient transformations specific toevent-based FER.", "output": "Spiking-Fer: Spiking Neural Network for Facial Expression Recognition With Event Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Point cloud based 3D deep model has wide applications in many applicationssuch as autonomous driving, house robot, and so on. Inspired by the recentprompt learning in natural language processing, this work proposes a novelMulti-view Vision-Prompt Fusion Network (MvNet) for few-shot 3D point cloudclassification. MvNet investigates the possibility of leveraging theoff-the-shelf 2D pre-trained models to achieve the few-shot classification,which can alleviate the over-dependence issue of the existing baseline modelstowards the large-scale annotated 3D point cloud data. Specifically, MvNetfirst encodes a 3D point cloud into multi-view image features for a number ofdifferent views. Then, a novel multi-view prompt fusion module is developed toeffectively fuse information from different views to bridge the gap between 3Dpoint cloud data and 2D pre-trained models. A set of 2D image prompts can thenbe derived to better describe the suitable prior knowledge for a large-scalepre-trained image model for few-shot 3D point cloud classification. Extensiveexperiments on ModelNet, ScanObjectNN, and ShapeNet datasets demonstrate thatMvNet achieves new state-of-the-art performance for 3D few-shot point cloudimage classification. The source code of this work will be available soon.", "output": "Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mammographic image analysis is a fundamental problem in the computer-aideddiagnosis scheme, which has recently made remarkable progress with the advanceof deep learning. However, the construction of a deep learning model requirestraining data that are large and sufficiently diverse in terms of image styleand quality. In particular, the diversity of image style may be majorlyattributed to the vendor factor. However, mammogram collection from vendors asmany as possible is very expensive and sometimes impractical forlaboratory-scale studies. Accordingly, to further augment the generalizationcapability of deep learning models to various vendors with limited resources, anew contrastive learning scheme is developed. Specifically, the backbonenetwork is firstly trained with a multi-style and multi-view unsupervisedself-learning scheme for the embedding of invariant features to various vendorstyles. Afterward, the backbone network is then recalibrated to the downstreamtasks of mass detection, multi-view mass matching, BI-RADS classification andbreast density classification with specific supervised learning. The proposedmethod is evaluated with mammograms from four vendors and two unseen publicdatasets. The experimental results suggest that our approach can effectivelyimprove analysis performance on both seen and unseen domains, and outperformsmany state-of-the-art (SOTA) generalization methods.", "output": "Domain Generalization for Mammographic Image Analysis via Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monocular depth estimation is critical for endoscopists to perform spatialperception and 3D navigation of surgical sites. However, most of the existingmethods ignore the important geometric structural consistency, which inevitablyleads to performance degradation and distortion of 3D reconstruction. Toaddress this issue, we introduce a gradient loss to penalize edge fluctuationsambiguous around stepped edge structures and a normal loss to explicitlyexpress the sensitivity to frequently small structures, and propose a geometricconsistency loss to spreads the spatial information across the sample grids toconstrain the global geometric anatomy structures. In addition, we develop asynthetic RGB-Depth dataset that captures the anatomical structures underreflections and illumination variations. The proposed method is extensivelyvalidated across different datasets and clinical images and achieves mean RMSEvalues of 0.066 (stomach), 0.029 (small intestine), and 0.139 (colon) on theEndoSLAM dataset. The generalizability of the proposed method achieves meanRMSE values of 12.604 (T1-L1), 9.930 (T2-L2), and 13.893 (T3-L3) on theColonDepth dataset. The experimental results show that our method exceedsprevious state-of-the-art competitors and generates more consistent depth mapsand reasonable anatomical structures. The quality of intraoperative 3Dstructure perception from endoscopic videos of the proposed method meets theaccuracy requirements of video-CT registration algorithms for endoscopicnavigation. The dataset and the source code will be available at", "output": "A geometry-aware deep network for depth estimation in monocular endoscopy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While lightweight ViT framework has made tremendous progress in imagesuper-resolution, its uni-dimensional self-attention modeling, as well ashomogeneous aggregation scheme, limit its effective receptive field (ERF) toinclude more comprehensive interactions from both spatial and channeldimensions. To tackle these drawbacks, this work proposes two enhancedcomponents under a new Omni-SR architecture. First, an Omni Self-Attention(OSA) block is proposed based on dense interaction principle, which cansimultaneously model pixel-interaction from both spatial and channeldimensions, mining the potential correlations across omni-axis (i.e., spatialand channel). Coupling with mainstream window partitioning strategies, OSA canachieve superior performance with compelling computational budgets. Second, amulti-scale interaction scheme is proposed to mitigate sub-optimal ERF (i.e.,premature saturation) in shallow models, which facilitates local propagationand meso-/global-scale interactions, rendering an omni-scale aggregationbuilding block. Extensive experiments demonstrate that Omni-SR achievesrecord-high performance on lightweight super-resolution benchmarks (e.g., 26.95dB@Urban100 $times 4$ with only 792K parameters). Our code is available aturl{", "output": "Omni Aggregation Networks for Lightweight Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Implicit Neural Representation (INR) has been emerging in computer vision inrecent years. It has been shown to be effective in parameterising continuoussignals such as dense 3D models from discrete image data, e.g. the neuralradius field (NeRF). However, INR is under-explored in 2D image processingtasks. Considering the basic definition and the structure of INR, we areinterested in its effectiveness in low-level vision problems such as imagerestoration. In this work, we revisit INR and investigate its application inlow-level image restoration tasks including image denoising, super-resolution,inpainting, and deblurring. Extensive experimental evaluations suggest thesuperior performance of INR in several low-level vision tasks with limitedresources, outperforming its counterparts by over 2dB. Code and models areavailable at ", "output": "Revisiting Implicit Neural Representations in Low-Level Vision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Precipitation forecasts are less accurate compared to other meteorologicalfields because several key processes affecting precipitation distribution andintensity occur below the resolved scale of global weather prediction models.This requires to use higher resolution simulations. To generate an uncertaintyprediction associated with the forecast, ensembles of simulations are runsimultaneously. However, the computational cost is a limiting factor here.Thus, instead of generating an ensemble system from simulations there is atrend of using neural networks. Unfortunately the data for high resolutionensemble runs is not available. We propose a new approach to generatingensemble weather predictions for high-resolution precipitation withoutrequiring high-resolution training data. The method uses generative adversarialnetworks to learn the complex patterns of precipitation and produce diverse andrealistic precipitation fields, allowing to generate realistic precipitationensemble members using only the available control forecast. We demonstrate thefeasibility of generating realistic precipitation ensemble members on unseenhigher resolutions. We use evaluation metrics such as RMSE, CRPS, rankhistogram and ROC curves to demonstrate that our generated ensemble is almostidentical to the ECMWF IFS ensemble.", "output": "Towards replacing precipitation ensemble predictions systems using machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models excel at generating photorealistic images from text-queries.Naturally, many approaches have been proposed to use these generative abilitiesto augment training datasets for downstream tasks, such as classification.However, diffusion models are themselves trained on large noisily supervised,but nonetheless, annotated datasets. It is an open question whether thegeneralization capabilities of diffusion models beyond using the additionaldata of the pre-training process for augmentation lead to improved downstreamperformance. We perform a systematic evaluation of existing methods to generateimages from diffusion models and study new extensions to assess their benefitfor data augmentation. While we find that personalizing diffusion modelstowards the target data outperforms simpler prompting strategies, we also showthat using the training data of the diffusion model alone, via a simple nearestneighbor retrieval procedure, leads to even stronger downstream performance.Overall, our study probes the limitations of diffusion models for dataaugmentation but also highlights its potential in generating new training datato improve performance on simple downstream vision tasks.", "output": "A data augmentation perspective on diffusion models and retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image-text retrieval is one of the major tasks of cross-modal retrieval.Several approaches for this task map images and texts into a common space tocreate correspondences between the two modalities. However, due to the content(semantics) richness of an image, redundant secondary information in an imagemay cause false matches. To address this issue, this paper presents a semanticoptimization approach, implemented as a Visual Semantic Loss (VSL), to assistthe model in focusing on an image's main content. This approach is inspired byhow people typically annotate the content of an image by describing its maincontent. Thus, we leverage the annotated texts corresponding to an image toassist the model in capturing the main content of the image, reducing thenegative impact of secondary content. Extensive experiments on two benchmarkdatasets (MSCOCO and Flickr30K) demonstrate the superior performance of ourmethod. The code is available at: ", "output": "Image-text Retrieval via preserving main Semantics of Vision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deaf individuals confront significant communication obstacles on a dailybasis. Their inability to hear makes it difficult for them to communicate withthose who do not understand sign language. Moreover, it presents difficultiesin educational, occupational, and social contexts. By providing alternativecommunication channels, technology can play a crucial role in overcoming theseobstacles. One such technology that can facilitate communication between deafand hearing individuals is sign language recognition. We will create a robustsystem for sign language recognition in order to convert Indian Sign Languageto text or speech. We will evaluate the proposed system and compare CNN andLSTM models. Since there are both static and gesture sign languages, a robustmodel is required to distinguish between them. In this study, we discoveredthat a CNN model captures letters and characters for recognition of static signlanguage better than an LSTM model, but it outperforms CNN by monitoring hands,faces, and pose in gesture sign language phrases and sentences. The creation ofa text-to-sign language paradigm is essential since it will enhance the signlanguage-dependent deaf and hard-of-hearing population's communication skills.Even though the sign-to-text translation is just one side of communication, notall deaf or hard-of-hearing people are proficient in reading or writing text.Some may have difficulty comprehending written language due to educational orliteracy issues. Therefore, a text-to-sign language paradigm would allow themto comprehend text-based information and participate in a variety of social,educational, and professional settings.Keywords: deaf and hard-of-hearing, DHH, Indian sign language, CNN, LSTM,static and gesture sign languages, text-to-sign language model, MediaPipeHolistic, sign language recognition, SLR, SLT", "output": "Indian Sign Language Recognition Using Mediapipe Holistic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An outbreak of the coronavirus disease which occurred three years later andit has hit the world again with many evolutions. The effects on the human racehave already been profound. We can only safeguard ourselves against thispandemic by mandating a \"Face Mask\" also maintaining the \"Social Distancing.\"The necessity of protective face masks in all gatherings is required by manycivil institutions in India. As a result of the substantial human resourceutilization, personally examining the whole country with a huge population likeIndia, to determine whether the execution of mask wearing and social distancemaintained is unfeasible. The COVID-19 Social Distancing Detector System is asingle-stage detector that employs deep learning to integrate high-end semanticdata to a CNN module in order to maintain social distances and simultaneouslymonitor violations within a specified region. By deploying current Securityfootages, CCTV cameras, and computer vision (CV), it will also be able toidentify those who are experiencing the calamity of social separation.Providing tools for safety and security, this technology disposes the need fora labor-force based surveillance system, yet a manual governing body is stillrequired to monitor, track, and inform on the violations that are committed.Any sort of infrastructure, including universities, hospitals, offices of thegovernment, schools, and building sites, can employ the technology. Therefore,the risk management system created to report and analyze video streams alongwith the social distance detector system might help to ensure our protectionand security as well as the security of our loved ones. Furthermore, we willdiscuss about deployment and improvement of the project overall.", "output": "Social Distance Detection Using Deep Learning And Risk Management System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D reconstruction from a single-RGB image in unconstrained real-worldscenarios presents numerous challenges due to the inherent diversity andcomplexity of objects and environments. In this paper, we introduceAnything-3D, a methodical framework that ingeniously combines a series ofvisual-language models and the Segment-Anything object segmentation model toelevate objects to 3D, yielding a reliable and versatile system for single-viewconditioned 3D reconstruction task. Our approach employs a BLIP model togenerate textural descriptions, utilizes the Segment-Anything model for theeffective extraction of objects of interest, and leverages a text-to-imagediffusion model to lift object into a neural radiance field. Demonstrating itsability to produce accurate and detailed 3D reconstructions for a wide array ofobjects, emph{Anything-3Dfootnotemark[2]} shows promise in addressing thelimitations of existing methodologies. Through comprehensive experiments andevaluations on various datasets, we showcase the merits of our approach,underscoring its potential to contribute meaningfully to the field of 3Dreconstruction. Demos and code will be available athref{", "output": "Anything-3D: Towards Single-view Anything Reconstruction in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the 3D-aware image attribute editing problem in this paper, whichhas wide applications in practice. Recent methods solved the problem bytraining a shared encoder to map images into a 3D generator's latent space orby per-image latent code optimization and then edited images in the latentspace. Despite their promising results near the input view, they still sufferfrom the 3D inconsistency of produced images at large camera poses andimprecise image attribute editing, like affecting unspecified attributes duringediting. For more efficient image inversion, we train a shared encoder for allimages. To alleviate 3D inconsistency at large camera poses, we propose twonovel methods, an alternating training scheme and a multi-view identity loss,to maintain 3D consistency and subject identity. As for imprecise imageediting, we attribute the problem to the gap between the latent space of realimages and that of generated images. We compare the latent space and inversionmanifold of GAN models and demonstrate that editing in the inversion manifoldcan achieve better results in both quantitative and qualitative evaluations.Extensive experiments show that our method produces more 3D consistent imagesand achieves more precise image editing than previous work. Source code andpretrained models can be found on our project page:", "output": "PREIM3D: 3D Consistent Precise Image Attribute Editing from a Single Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Enhancing the robustness of vision algorithms in real-world scenarios ischallenging. One reason is that existing robustness benchmarks are limited, asthey either rely on synthetic data or ignore the effects of individual nuisancefactors. We introduce OOD-CV-v2, a benchmark dataset that includesout-of-distribution examples of 10 object categories in terms of pose, shape,texture, context and the weather conditions, and enables benchmarking of modelsfor image classification, object detection, and 3D pose estimation. In additionto this novel dataset, we contribute extensive experiments using popularbaseline methods, which reveal that: 1) Some nuisance factors have a muchstronger negative effect on the performance compared to others, also dependingon the vision task. 2) Current approaches to enhance robustness have onlymarginal effects, and can even reduce robustness. 3) We do not observesignificant differences between convolutional and transformer architectures. Webelieve our dataset provides a rich test bed to study robustness and will helppush forward research in this area.Our dataset can be accessed from <a href=\" http URL</a>", "output": "OOD-CV-v2: An extended Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The duality of content and style is inherent to the nature of art. Forhumans, these two elements are clearly different: content refers to the objectsand concepts in the piece of art, and style to the way it is expressed. Thisduality poses an important challenge for computer vision. The visual appearanceof objects and concepts is modulated by the style that may reflect the author'semotions, social trends, artistic movement, etc., and their deep comprehensionundoubtfully requires to handle both. A promising step towards a generalparadigm for art analysis is to disentangle content and style, whereas relyingon human annotations to cull a single aspect of artworks has limitations inlearning semantic concepts and the visual appearance of paintings. We thuspresent GOYA, a method that distills the artistic knowledge captured in arecent generative model to disentangle content and style. Experiments show thatsynthetically generated images sufficiently serve as a proxy of the realdistribution of artworks, allowing GOYA to separately represent the twoelements of art while keeping more information than existing methods.", "output": "Not Only Generative Art: Stable Diffusion for Content-Style Disentanglement in Art Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video Copy Detection (VCD) has been developed to identify instances ofunauthorized or duplicated video content. This paper presents our first andsecond solutions to the Meta AI Video Similarity Challenge (VSC22), CVPR 2023.In order to compete in this challenge, we propose Feature-CompatibleProgressive Learning (FCPL) for VCD. FCPL trains various models that producemutually-compatible features, meaning that the features derived from multipledistinct models can be directly compared with one another. We find this mutualcompatibility enables feature ensemble. By implementing progressive learningand utilizing labeled ground truth pairs, we effectively gradually enhanceperformance. Experimental results demonstrate the superiority of the proposedFCPL over other competitors. Our code is available at and", "output": "Feature-compatible Progressive Learning for Video Copy Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative DNNs are a powerful tool for image synthesis, but they are limitedby their computational load. On the other hand, given a trained model and atask, e.g. faces generation within a range of characteristics, the output imagequality will be unevenly distributed among images with differentcharacteristics. It follows, that we might restrain the models complexity onsome instances, maintaining a high quality. We propose a method for diminishingcomputations by adding so-called early exit branches to the originalarchitecture, and dynamically switching the computational path depending on howdifficult it will be to render the output. We apply our method on two differentSOTA models performing generative tasks: generation from a semantic map, andcross-reenactment of face expressions; showing it is able to output images withcustom lower-quality thresholds. For a threshold of LPIPS &lt;=0.1, we diminishtheir computations by up to a half. This is especially relevant for real-timeapplications such as synthesis of faces, when quality loss needs to becontained, but most of the inputs need fewer computations than the complexinstances.", "output": "FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The end-to-end speech translation (E2E-ST) model has gradually become amainstream paradigm due to its low latency and less error propagation. However,it is non-trivial to train such a model well due to the task complexity anddata scarcity. The speech-and-text modality differences result in the E2E-STmodel performance usually inferior to the corresponding machine translation(MT) model. Based on the above observation, existing methods often usesharingmechanisms to carry out implicit knowledge transfer by imposing variousconstraints. However, the final model often performs worse on the MT task thanthe MT model trained alone, which means that the knowledge transfer ability ofthis method is also limited. To deal with these problems, we propose the FCCL(Fine- and Coarse- Granularity Contrastive Learning) approach for E2E-ST, whichmakes explicit knowledge transfer through cross-modal multi-grained contrastivelearning. A key ingredient of our approach is applying contrastive learning atboth sentence- and frame-level to give the comprehensive guide for extractingspeech representations containing rich semantic information.In addition, weadopt a simple whitening method to alleviate the representation degeneration inthe MT model, which adversely affects contrast learning. Experiments on theMuST-C benchmark show that our proposed approach significantly outperforms thestate-of-the-art E2E-ST baselines on all eight language pairs. Further analysisindicates that FCCL can free up its capacity from learning grammaticalstructure information and force more layers to learn semantic information.", "output": "Improving Speech Translation by Cross-Modal Multi-Grained Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated augmentation is an emerging and effective technique to search fordata augmentation policies to improve generalizability of deep neural networktraining. Most existing work focuses on constructing a unified policyapplicable to all data samples in a given dataset, without considering sampleor class variations. In this paper, we propose a novel two-stage dataaugmentation algorithm, named Label-Aware AutoAugment (LA3), which takesadvantage of the label information, and learns augmentation policies separatelyfor samples of different labels. LA3 consists of two learning stages, where inthe first stage, individual augmentation methods are evaluated and ranked foreach label via Bayesian Optimization aided by a neural predictor, which allowsus to identify effective augmentation techniques for each label under a lowsearch cost. And in the second stage, a composite augmentation policy isconstructed out of a selection of effective as well as complementaryaugmentations, which produces significant performance boost and can be easilydeployed in typical model training. Extensive experiments demonstrate that LA3achieves excellent performance matching or surpassing existing methods onCIFAR-10 and CIFAR-100, and achieves a new state-of-the-art ImageNet accuracyof 79.97% on ResNet-50 among auto-augmentation methods, while maintaining a lowcomputational cost.", "output": "LA3: Efficient Label-Aware AutoAugment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the success of deep learning in video understanding tasks, processingevery frame in a video is computationally expensive and often unnecessary inreal-time applications. Frame selection aims to extract the most informativeand representative frames to help a model better understand video content.Existing frame selection methods either individually sample frames based onper-frame importance prediction, without considering interaction among frames,or adopt reinforcement learning agents to find representative frames insuccession, which are costly to train and may lead to potential stabilityissues. To overcome the limitations of existing methods, we propose aSearch-Map-Search learning paradigm which combines the advantages of heuristicsearch and supervised learning to select the best combination of frames from avideo as one entity. By combining search with learning, the proposed method canbetter capture frame interactions while incurring a low inference overhead.Specifically, we first propose a hierarchical search method conducted on eachtraining video to search for the optimal combination of frames with the lowesterror on the downstream task. A feature mapping function is then learned to mapthe frames of a video to the representation of its target optimal framecombination. During inference, another search is performed on an unseen videoto select a combination of frames whose feature representation is close to theprojected feature representation. Extensive experiments based on several actionrecognition benchmarks demonstrate that our frame selection method effectivelyimproves performance of action recognition models, and significantlyoutperforms a number of competitive baselines.", "output": "Search-Map-Search: A Frame Selection Paradigm for Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a second order gradient based method with ADAM and RMSprop for thetraining of generative adversarial networks. The proposed method is fastest toobtain similar accuracy when compared to prominent second order methods. Unlikestate-of-the-art recent methods, it does not require solving a linear system,or it does not require additional mixed second derivative terms. We derive thefixed point iteration corresponding to proposed method, and show that theproposed method is convergent. The proposed method produces better orcomparable inception scores, and comparable quality of images compared to otherrecently proposed state-of-the-art second order methods. Compared to firstorder methods such as ADAM, it produces significantly better inception scores.The proposed method is compared and validated on popular datasets such as FFHQ,LSUN, CIFAR10, MNIST, and Fashion MNIST for image generationtasksfootnote{Accepted in IJCNN 2023}. Codes:url{", "output": "Adaptive Consensus Optimization Method for GANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduceDropDim, a structured dropout method designed for regularizingthe self-attention mechanism, which is a key component of the transformer. Incontrast to the general dropout method, which randomly drops neurons, DropDimdrops part of the embedding dimensions. In this way, the semantic informationcan be completely discarded. Thus, the excessive coadapting between differentembedding dimensions can be broken, and the self-attention is forced to encodemeaningful featureswith a certain number of embedding dimensions erased.Experiments on a wide range of tasks executed on the MUST-C English-Germanydataset show that DropDim can effectively improve model performance, reduceover-fitting, and show complementary effects with other regularization methods.When combined with label smoothing, the WER can be reduced from 19.1% to 15.1%on the ASR task, and the BLEU value can be increased from26.90 to 28.38 on theMT task. On the ST task, the model can reach a BLEU score of 22.99, an increaseby 1.86 BLEU points compared to the strong baseline.", "output": "DropDim: A Regularization Method for Transformer Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We demonstrate our solution for the 2019 COCO panoptic segmentation task. Ourmethod first performs instance segmentation and semantic segmentationseparately, then combines the two to generate panoptic segmentation results. Toenhance the performance, we add several expert models of Mask R-CNN in instancesegmentation to tackle the data imbalance problem in the training data; alsoHTC model is adopted yielding our best instance segmentation results. Insemantic segmentation, we trained several models with various backbones and usean ensemble strategy which further boosts the segmentation results. In the end,we analyze various combinations of instance and semantic segmentation, andreport on their performance for the final panoptic segmentation results. Ourbest model achieves $PQ$ 47.1 on 2019 COCO panoptic test-dev data.", "output": "Ensembling Instance and Semantic Segmentation for Panoptic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To transfer the knowledge learned from a labeled source domain to anunlabeled target domain, many studies have worked on universal domainadaptation (UniDA), where there is no constraint on the label sets of thesource domain and target domain. However, the existing UniDA methods rely onsource samples with correct annotations. Due to the limited resources in thereal world, it is difficult to obtain a large amount of perfectly clean labeleddata in a source domain in some applications. As a result, we propose a novelrealistic scenario named Noisy UniDA, in which classifiers are trained usingnoisy labeled data from the source domain as well as unlabeled domain data fromthe target domain that has an uncertain class distribution. A multi-headconvolutional neural network framework is proposed in this paper to address allof the challenges faced in the Noisy UniDA at once. Our network comprises asingle common feature generator and multiple classifiers with various decisionbounds. We can detect noisy samples in the source domain, identify unknownclasses in the target domain, and align the distribution of the source andtarget domains by optimizing the divergence between the outputs of the variousclassifiers. The proposed method outperformed the existing methods in most ofthe settings after a thorough analysis of the various domain adaptionscenarios. The source code is available aturl{", "output": "Noisy Universal Domain Adaptation via Divergence Optimization for Visual Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning has recently attracted attention from the researchcommunity, as it aims to solve long-standing limitations of classicsupervisedly-trained models. However, most research on this subject has tackledcontinual learning in simple image classification scenarios. In this paper, wepresent a benchmark of state-of-the-art continual learning methods on videoaction recognition. Besides the increased complexity due to the temporaldimension, the video setting imposes stronger requirements on computingresources for top-performing rehearsal methods. To counteract the increasedmemory requirements, we present two method-agnostic variants for rehearsalmethods, exploiting measures of either model confidence or data information toselect memorable samples. Our experiments show that, as expected from theliterature, rehearsal methods outperform other approaches; moreover, theproposed memory-efficient variants are shown to be effective at retaining acertain level of performance with a smaller buffer size.", "output": "A baseline on continual learning methods for video action recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tremendous efforts have been made on document image rectification, but how tolearn effective representation of such distorted images is stillunder-explored. In this paper, we present DocMAE, a novel self-supervisedframework for document image rectification. Our motivation is to encode thestructural cues in document images by leveraging masked autoencoder to benefitthe rectification, i.e., the document boundaries, and text lines. Specifically,we first mask random patches of the background-excluded document images andthen reconstruct the missing pixels. With such a self-supervised learningapproach, the network is encouraged to learn the intrinsic structure ofdeformed documents by restoring document boundaries and missing text lines.Transfer performance in the downstream rectification task validates theeffectiveness of our method. Extensive experiments are conducted to demonstratethe effectiveness of our method.", "output": "DocMAE: Document Image Rectification via Self-supervised Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Objective: This paper proposes a deep learning model for breast cancerdetection from reconstructed images of microwave imaging scan data and aims toimprove the accuracy and efficiency of breast tumor detection, which could havea significant impact on breast cancer diagnosis and treatment. Methods: Ourframework consists of different convolutional neural network (CNN)architectures for feature extraction and a region-based CNN for tumordetection. We use 7 different architectures: DenseNet201, ResNet50,InceptionV3, InceptionResNetV3, MobileNetV2, NASNetMobile and NASNetLarge andcompare its performance to find the best architecture out of the seven. Anexperimental dataset of MRI-derived breast phantoms was used. Results:NASNetLarge is the best architecture which can be used for the CNN model withaccuracy of 88.41% and loss of 27.82%. Given that the model's AUC is 0.786, itcan be concluded that it is suitable for use in its present form, while itcould be improved upon and trained on other datasets that are comparable.Impact: One of the main causes of death in women is breast cancer, and earlyidentification is essential for enhancing the results for patients. Due to itsnon-invasiveness and capacity to produce high-resolution images, microwaveimaging is a potential tool for breast cancer screening. The complexity oftumors makes it difficult to adequately detect them in microwave images. Theresults of this research show that deep learning has a lot of potential forbreast cancer detection in microwave images", "output": "Breast cancer detection using deep learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a new task, novel view synthesis for LiDAR sensors. Whiletraditional model-based LiDAR simulators with style-transfer neural networkscan be applied to render novel views, they fall short in producing accurate andrealistic LiDAR patterns, because the renderers they rely on exploit gameengines, which are not differentiable. We address this by formulating, to thebest of our knowledge, the first differentiable LiDAR renderer, and propose anend-to-end framework, LiDAR-NeRF, leveraging a neural radiance field (NeRF) toenable jointly learning the geometry and the attributes of 3D points. Toevaluate the effectiveness of our approach, we establish an object-centricmulti-view LiDAR dataset, dubbed NeRF-MVL. It contains observations of objectsfrom 9 categories seen from 360-degree viewpoints captured with multiple LiDARsensors. Our extensive experiments on the scene-level KITTI-360 dataset, and onour object-level NeRF-MVL show that our LiDAR- NeRF surpasses the model-basedalgorithms significantly.", "output": "LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Driven by deep learning techniques, perception technology in autonomousdriving has developed rapidly in recent years. To achieve accurate and robustperception capabilities, autonomous vehicles are often equipped with multiplesensors, making sensor fusion a crucial part of the perception system. Amongthese fused sensors, radars and cameras enable a complementary andcost-effective perception of the surrounding environment regardless of lightingand weather conditions. This review aims to provide a comprehensive guidelinefor radar-camera fusion, particularly concentrating on perception tasks relatedto object detection and semantic segmentation. Based on the principles of theradar and camera sensors, we delve into the data processing process andrepresentations, followed by an in-depth analysis and summary of radar-camerafusion datasets. In the review of methodologies in radar-camera fusion, weaddress interrogative questions, including \"why to fuse\", \"what to fuse\",\"where to fuse\", \"when to fuse\", and \"how to fuse\", subsequently discussingvarious challenges and potential research directions within this domain. Toease the retrieval and comparison of datasets and fusion methods, we alsoprovide an interactive website:", "output": "Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this report, we summarize the first NTIRE challenge on light field (LF)image super-resolution (SR), which aims at super-resolving LF images under thestandard bicubic degradation with a magnification factor of 4. This challengedevelops a new LF dataset called NTIRE-2023 for validation and test, andprovides a toolbox called BasicLFSR to facilitate model development. Comparedwith single image SR, the major challenge of LF image SR lies in how to exploitcomplementary angular information from plenty of views with varyingdisparities. In total, 148 participants have registered the challenge, and 11teams have successfully submitted results with PSNR scores higher than thebaseline method LF-InterNet cite{LF-InterNet}. These newly developed methodshave set new state-of-the-art in LF image SR, e.g., the winning method achievesaround 1 dB PSNR improvement over the existing state-of-the-art method DistgSSRcite{DistgLF}. We report the solutions proposed by the participants, andsummarize their common trends and useful tricks. We hope this challenge canstimulate future research and inspire new ideas in LF image SR.", "output": "NTIRE 2023 Challenge on Light Field Image Super-Resolution: Dataset, Methods and Results."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Our goal is to synthesize 3D human motions given textual inputs describingsimultaneous actions, for example 'waving hand' while 'walking' at the sametime. We refer to generating such simultaneous movements as performing 'spatialcompositions'. In contrast to temporal compositions that seek to transitionfrom one action to another, spatial compositing requires understanding whichbody parts are involved in which action, to be able to move themsimultaneously. Motivated by the observation that the correspondence betweenactions and body parts is encoded in powerful language models, we extract thisknowledge by prompting GPT-3 with text such as \"what are the body partsinvolved in the action &lt;action name&gt;?\", while also providing the parts list andfew-shot examples. Given this action-part mapping, we combine body parts fromtwo motions together and establish the first automated method to spatiallycompose two actions. However, training data with compositional actions isalways limited by the combinatorics. Hence, we further create synthetic datawith this approach, and use it to train a new state-of-the-art text-to-motiongeneration model, called SINC (\"SImultaneous actioN Compositions for 3D humanmotions\"). In our experiments, we find training on additional syntheticGPT-guided compositional motions improves text-to-motion generation.", "output": "SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Concerns about reproducibility in artificial intelligence (AI) have emerged,as researchers have reported unsuccessful attempts to directly reproducepublished findings in the field. Replicability, the ability to affirm a findingusing the same procedures on new data, has not been well studied. In thispaper, we examine both reproducibility and replicability of a corpus of 16papers on table structure recognition (TSR), an AI task aimed at identifyingcell locations of tables in digital documents. We attempt to reproducepublished results using codes and datasets provided by the original authors. Wethen examine replicability using a dataset similar to the original as well as anew dataset, GenTSR, consisting of 386 annotated tables extracted fromscientific papers. Out of 16 papers studied, we reproduce results consistentwith the original in only four. Two of the four papers are identified asreplicable using the similar dataset under certain IoU values. No paper isidentified as replicable using the new dataset. We offer observations on thecauses of irreproducibility and irreplicability. All code and data areavailable on Codeocean at ", "output": "A Study on Reproducibility and Replicability of Table Structure Recognition Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding the complex traffic environment is crucial for self-drivingvehicles. Existing benchmarks in autonomous driving mainly cast sceneunderstanding as perception problems, e.g., perceiving lanelines with vanilladetection or segmentation methods. As such, we argue that the perceptionpipeline provides limited information for autonomous vehicles to drive in theright way, especially without the aid of high-definition (HD) map. Forinstance, following the wrong traffic signal at a complicated crossroad wouldlead to a catastrophic incident. By introducing Road Genome (OpenLane-V2), weintend to shift the community's attention and take a step further beyondperception - to the task of topology reasoning for scene structure. The goal ofRoad Genome is to understand the scene structure by investigating therelationship of perceived entities among traffic elements and lanes. Built ontop of prevailing datasets, the newly minted benchmark comprises 2,000sequences of multi-view images captured from diverse real-world scenarios. Weannotate data with high-quality manual checks in the loop. Three subtaskscompromise the gist of Road Genome, including the 3D lane detection inheritedfrom OpenLane. We have/will host Challenges in the upcoming future attop-tiered venues.", "output": "Road Genome: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning inference brings together the data and the Convolutional NeuralNetwork (CNN). This is problematic in case the user wants to preserve theprivacy of the data and the service provider does not want to reveal theweights of his CNN. Secure Inference allows the two parties to engage in aprotocol that preserves their respective privacy concerns, while revealing onlythe inference result to the user. This is known as Multi-Party Computation(MPC). A major bottleneck of MPC algorithms is communication, as the partiesmust send data back and forth. The linear component of a CNN (i.e.convolutions) can be done efficiently with minimal communication, but thenon-linear part (i.e., ReLU) requires the bulk of communication bandwidth. Wepropose two ways to accelerate Secure Inference. The first is based on theobservation that the ReLU outcome of many convolutions is highly correlated.Therefore, we replace the per pixel ReLU operation by a ReLU operation perpatch. Each layer in the network will benefit from a patch of a different sizeand we devise an algorithm to choose the optimal set of patch sizes through anovel reduction of the problem to a knapsack problem. The second way toaccelerate Secure Inference is based on cutting the number of bit comparisonsrequired for a secure ReLU operation. We demonstrate the cumulative effect ofthese tools in the semi-honest secure 3-party setting for four problems:Classifying ImageNet using ResNet50 backbone, classifying CIFAR100 usingResNet18 backbone, semantic segmentation of ADE20K using MobileNetV2 backboneand semantic segmentation of Pascal VOC 2012 using ResNet50 backbone. Oursource code is publicly available:$href{", "output": "Securing Neural Networks with Knapsack Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Randomized smoothing (RS) is a well known certified defense againstadversarial attacks, which creates a smoothed classifier by predicting the mostlikely class under random noise perturbations of inputs during inference. Whileinitial work focused on robustness to $ell_2$ norm perturbations using noisesampled from a Gaussian distribution, subsequent works have shown thatdifferent noise distributions can result in robustness to other $ell_p$ normbounds as well. In general, a specific noise distribution is optimal fordefending against a given $ell_p$ norm based attack. In this work, we aim toimprove the certified adversarial robustness against multiple perturbationbounds simultaneously. Towards this, we firstly present a noveltextit{certification scheme}, that effectively combines the certificatesobtained using different noise distributions to obtain optimal results againstmultiple perturbation bounds. We further propose a novel textit{training noisedistribution} along with a textit{regularized training scheme} to improve thecertification within both $ell_1$ and $ell_2$ perturbation normssimultaneously. Contrary to prior works, we compare the certified robustness ofdifferent training algorithms across the same natural (clean) accuracy, ratherthan across fixed noise levels used for training and certification. We alsoempirically invalidate the argument that training and certifying the classifierwith the same amount of noise gives the best results. The proposed approachachieves improvements on the ACR (Average Certified Radius) metric across both$ell_1$ and $ell_2$ perturbation bounds.", "output": "Certified Adversarial Robustness Within Multiple Perturbation Bounds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we focus on the problem of rendering novel views from a NeuralRadiance Field (NeRF) under unobserved light conditions. To this end, weintroduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real worldobjects under one-light-at-time (OLAT) conditions, annotated with accurateground-truth camera and light poses. Our acquisition pipeline leverages tworobotic arms holding, respectively, a camera and an omni-directional point-wiselight source. We release a total of 20 scenes depicting a variety of objectswith complex geometry and challenging materials. Each scene includes 2000images, acquired from 50 different points of views under 40 different OLATconditions. By leveraging the dataset, we perform an ablation study on therelighting capability of variants of the vanilla NeRF architecture and identifya lightweight architecture that can render novel views of an object under novellight conditions, which we use to establish a non-trivial baseline for thedataset. Dataset and benchmark are available at", "output": "ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In our work, we propose a novel yet simple approach to obtain an adaptivelearning rate for gradient-based descent methods on classification tasks.Instead of the traditional approach of selecting adaptive learning rates viathe decayed expectation of gradient-based terms, we use the angle between thecurrent gradient and the new gradient: this new gradient is computed from thedirection orthogonal to the current gradient, which further helps us indetermining a better adaptive learning rate based on angle history, thereby,leading to relatively better accuracy compared to the existing state-of-the-artoptimizers. On a wide variety of benchmark datasets with prominent imageclassification architectures such as ResNet, DenseNet, EfficientNet, and VGG,we find that our method leads to the highest accuracy in most of the datasets.Moreover, we prove that our method is convergent.", "output": "Angle based dynamic learning rate for gradient descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive language-image pretraining (CLIP) has demonstrated remarkablesuccess in various image tasks. However, how to extend CLIP with effectivetemporal modeling is still an open and crucial problem. Existing factorized orjoint spatial-temporal modeling trades off between the efficiency andperformance. While modeling temporal information within straight through tubeis widely adopted in literature, we find that simple frame alignment alreadyprovides enough essence without temporal attention. To this end, in this paper,we proposed a novel Implicit Learnable Alignment (ILA) method, which minimizesthe temporal modeling effort while achieving incredibly high performance.Specifically, for a frame pair, an interactive point is predicted in eachframe, serving as a mutual information rich region. By enhancing the featuresaround the interactive point, two frames are implicitly aligned. The alignedfeatures are then pooled into a single token, which is leveraged in thesubsequent spatial self-attention. Our method allows eliminating the costly orinsufficient temporal self-attention in video. Extensive experiments onbenchmarks demonstrate the superiority and generality of our module.Particularly, the proposed ILA achieves a top-1 accuracy of 88.7% onKinetics-400 with much fewer FLOPs compared with Swin-L and ViViT-H. Code isreleased at  .", "output": "Implicit Temporal Modeling with Learnable Alignment for Video Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sign language (SL) is the primary method of communication for the 70 millionDeaf people around the world. Video dictionaries of isolated signs are a coreSL learning tool. Replacing these with 3D avatars can aid learning and enableAR/VR applications, improving access to technology and online media. However,little work has attempted to estimate expressive 3D avatars from SL video;occlusion, noise, and motion blur make this task difficult. We address this byintroducing novel linguistic priors that are universally applicable to SL andprovide constraints on 3D hand pose that help resolve ambiguities withinisolated signs. Our method, SGNify, captures fine-grained hand pose, facialexpression, and body movement fully automatically from in-the-wild monocular SLvideos. We evaluate SGNify quantitatively by using a commercial motion-capturesystem to compute 3D avatars synchronized with monocular video. SGNifyoutperforms state-of-the-art 3D body-pose- and shape-estimation methods on SLvideos. A perceptual study shows that SGNify's 3D reconstructions aresignificantly more comprehensible and natural than those of previous methodsand are on par with the source videos. Code and data are available at$href{<a href=\" http URL</a>}{text{sgnify.is.tue.mpg.de}}$.", "output": "Reconstructing Signing Avatars From Video Using Linguistic Priors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Video Pre-trained Transformer. VPT uses four SOTA encoder modelsfrom prior work to convert a video into a sequence of compact embeddings. Ourbackbone, based on a reference Flan-T5-11B architecture, learns a universalrepresentation of the video that is a non-linear sum of the encoder models. Itlearns using an autoregressive causal language modeling loss by predicting thewords spoken in YouTube videos. Finally, we evaluate on standard downstreambenchmarks by training fully connected prediction heads for each task. To thebest of our knowledge, this is the first use of multiple frozen SOTA models asencoders in an \"embedding -&gt; backbone -&gt; prediction head\" design pattern - allothers have trained their own joint encoder models. Additionally, we includemore modalities than the current SOTA, Merlot Reserve, by adding explicit SceneGraph information. For these two reasons, we believe it could combine theworld's best open-source models to achieve SOTA performance. Initialexperiments demonstrate the model is learning appropriately, but moreexperimentation and compute is necessary, and already in progress, to realizeour loftier goals. Alongside this work, we build on the YT-20M dataset,reproducing it and adding 25,000 personally selected YouTube videos to itscorpus. All code and model checkpoints are open sourced under a standard MITlicense.", "output": "Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The latent space of GANs contains rich semantics reflecting the trainingdata. Different methods propose to learn edits in latent space corresponding tosemantic attributes, thus allowing to modify generated images. Most supervisedmethods rely on the guidance of classifiers to produce such edits. However,classifiers can lead to out-of-distribution regions and be fooled byadversarial samples. We propose an alternative formulation based on theWasserstein loss that avoids such problems, while maintaining performanceon-par with classifier-based approaches. We demonstrate the effectiveness ofour method on two datasets (digits and faces) using StyleGAN2.", "output": "Wasserstein Loss for Semantic Editing in the Latent Space of GANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training segmentation models for medical images continues to be challengingdue to the limited availability and acquisition expense of data annotations.Segment Anything Model (SAM) is a foundation model trained on over 1 billionannotations, predominantly for natural images, that is intended to be able tosegment the user-defined object of interest in an interactive manner. Despiteits impressive performance on natural images, it is unclear how the model isaffected when shifting to medical image domains. Here, we perform an extensiveevaluation of SAM's ability to segment medical images on a collection of 11medical imaging datasets from various modalities and anatomies. In ourexperiments, we generated point prompts using a standard method that simulatesinteractive segmentation. Experimental results show that SAM's performancebased on single prompts highly varies depending on the task and the dataset,i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset,evaluated by IoU. Performance appears to be high for tasks includingwell-circumscribed objects with unambiguous prompts and poorer in many otherscenarios such as segmentation of tumors. When multiple prompts are provided,performance improves only slightly overall, but more so for datasets where theobject is not contiguous. An additional comparison to RITM showed a much betterperformance of SAM for one prompt but a similar performance of the two methodsfor a larger number of prompts. We conclude that SAM shows impressiveperformance for some datasets given the zero-shot learning setup but poor tomoderate performance for multiple other datasets. While SAM as a model and as alearning paradigm might be impactful in the medical imaging domain, extensiveresearch is needed to identify the proper ways of adapting it in this domain.", "output": "Segment Anything Model for Medical Image Analysis: an Experimental Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE),efficiently learn a rich representation of the input. However, for adapting todownstream tasks, they require a sufficient amount of labeled data since theirrich features capture not only objects but also less relevant image background.In contrast, Instance Discrimination (ID) methods focus on objects. In thiswork, we study how to combine the efficiency and scalability of MIM with theability of ID to perform downstream classification in the absence of largeamounts of labeled data. To this end, we introduce Masked AutoencoderContrastive Tuning (MAE-CT), a sequential approach that applies NearestNeighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes therich features such that they form semantic clusters of objects without usingany labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CTmatches or excels previous self-supervised methods trained on ImageNet inlinear probing, k-NN and low-shot classification accuracy as well as inunsupervised clustering accuracy. Notably, similar results can be achievedwithout additional image augmentations. While ID methods generally rely onhand-crafted augmentations to avoid shortcut learning, we find that nearestneighbor lookup is sufficient and that this data-driven augmentation effectimproves with model size. MAE-CT is compute efficient. For instance, startingfrom a MAE pre-trained ViT-L/16, MAE-CT increases the ImageNet 1% low-shotaccuracy from 67.7% to 72.6%, linear probing accuracy from 76.0% to 80.2% andk-NN accuracy from 60.6% to 79.1% in just five hours using eight A100 GPUs.", "output": "Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces GenCorres, a novel unsupervised joint shape matching(JSM) approach. The basic idea of GenCorres is to learn a parametric meshgenerator to fit an unorganized deformable shape collection while constrainingdeformations between adjacent synthetic shapes to preserve geometric structuressuch as local rigidity and local conformality. GenCorres presents threeappealing advantages over existing JSM techniques. First, GenCorres performsJSM among a synthetic shape collection whose size is much bigger than the inputshapes and fully leverages the data-driven power of JSM. Second, GenCorresunifies consistent shape matching and pairwise matching (i.e., by enforcingdeformation priors between adjacent synthetic shapes). Third, the generatorprovides a concise encoding of consistent shape correspondences. However,learning a mesh generator from an unorganized shape collection is challenging.It requires a good initial fitting to each shape and can easily get trapped bylocal minimums. GenCorres addresses this issue by learning an implicitgenerator from the input shapes, which provides intermediate shapes between twoarbitrary shapes. We introduce a novel approach for computing correspondencesbetween adjacent implicit surfaces and force the correspondences to preservegeometric structures and be cycle-consistent. Synthetic shapes of the implicitgenerator then guide initial fittings (i.e., via template-based deformation)for learning the mesh generator. Experimental results show that GenCorresconsiderably outperforms state-of-the-art JSM techniques on benchmark datasets.The synthetic shapes of GenCorres preserve local geometric features and yieldcompetitive performance gains against state-of-the-art deformable shapegenerators.", "output": "GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the problem of fitting a parametric human body model (SMPL) topoint cloud data. Optimization-based methods require careful initialization andare prone to becoming trapped in local optima. Learning-based methods addressthis but do not generalize well when the input pose is far from those seenduring training. For rigid point clouds, remarkable generalization has beenachieved by leveraging SE(3)-equivariant networks, but these methods do notwork on articulated objects. In this work we extend this idea to human bodiesand propose ArtEq, a novel part-based SE(3)-equivariant neural architecture forSMPL model estimation from point clouds. Specifically, we learn a partdetection network by leveraging local SO(3) invariance, and regress shape andpose using articulated SE(3) shape-invariant and pose-equivariant networks, alltrained end-to-end. Our novel equivariant pose regression module leverages thepermutation-equivariant property of self-attention layers to preserverotational equivariance. Experimental results show that ArtEq can generalize toposes not seen during training, outperforming state-of-the-art methods by74.5%, without requiring an optimization refinement step. Further, comparedwith competing works, our method is more than three orders of magnitude fasterduring inference and has 97.3% fewer parameters. The code and model will beavailable for research purposes at ", "output": "Generalizing Neural Human Fitting to Unseen Poses With Articulated SE(3) Equivariance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models arise as a powerful generative tool recently. Despite thegreat progress, existing diffusion models mainly focus on uni-modal control,i.e., the diffusion process is driven by only one modality of condition. Tofurther unleash the users' creativity, it is desirable for the model to becontrollable by multiple modalities simultaneously, e.g., generating andediting faces by describing the age (text-driven) while drawing the face shape(mask-driven). In this work, we present Collaborative Diffusion, wherepre-trained uni-modal diffusion models collaborate to achieve multi-modal facegeneration and editing without re-training. Our key insight is that diffusionmodels driven by different modalities are inherently complementary regardingthe latent denoising steps, where bilateral connections can be establishedupon. Specifically, we propose dynamic diffuser, a meta-network that adaptivelyhallucinates multi-modal denoising steps by predicting the spatial-temporalinfluence functions for each pre-trained uni-modal model. CollaborativeDiffusion not only collaborates generation capabilities from uni-modaldiffusion models, but also integrates multiple uni-modal manipulations toperform multi-modal editing. Extensive qualitative and quantitative experimentsdemonstrate the superiority of our framework in both image quality andcondition consistency.", "output": "Collaborative Diffusion for Multi-Modal Face Generation and Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts suchas floaters or flawed geometry when rendered outside the camera trajectory.Existing evaluation protocols often do not capture these effects, since theyusually only assess image quality at every 8th frame of the training capture.To push forward progress in novel-view synthesis, we propose a new dataset andevaluation procedure, where two camera trajectories are recorded of the scene:one used for training, and the other for evaluation. In this more challengingin-the-wild setting, we find that existing hand-crafted regularizers do notremove floaters nor improve scene geometry. Thus, we propose a 3Ddiffusion-based method that leverages local 3D priors and a novel density-basedscore distillation sampling loss to discourage artifacts during NeRFoptimization. We show that this data-driven prior removes floaters and improvesscene geometry for casual captures.", "output": "Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Farm3D, a method to learn category-specific 3D reconstructors forarticulated objects entirely from \"free\" virtual supervision from a pre-trained2D diffusion-based image generator. Recent approaches can learn, given acollection of single-view images of an object category, a monocular network topredict the 3D shape, albedo, illumination and viewpoint of any objectoccurrence. We propose a framework using an image generator like StableDiffusion to generate virtual training data for learning such a reconstructionnetwork from scratch. Furthermore, we include the diffusion model as a score tofurther improve learning. The idea is to randomise some aspects of thereconstruction, such as viewpoint and illumination, generating synthetic viewsof the reconstructed 3D object, and have the 2D network assess the quality ofthe resulting image, providing feedback to the reconstructor. Different fromwork based on distillation which produces a single 3D asset for each textualprompt in hours, our approach produces a monocular reconstruction network thatcan output a controllable 3D asset from a given image, real or generated, inonly seconds. Our network can be used for analysis, including monocularreconstruction, or for synthesis, generating articulated assets for real-timeapplications such as video games.", "output": "Farm3D: Learning Articulated 3D Animals by Distilling 2D Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a novel optimization algorithm for image recovery under learnedsparse and low-rank constraints, which we parameterize as weighted extensionsof the $ell_p^p$-vector and $mathcal S_p^p$ Schatten-matrix quasi-norms for$0!&lt;p!le1$, respectively. Our proposed algorithm generalizes the IterativelyReweighted Least Squares (IRLS) method, used for signal recovery under $ell_1$and nuclear-norm constrained minimization. Further, we interpret our overallminimization approach as a recurrent network that we then employ to deal withinverse low-level computer vision problems. Thanks to the convergenceguarantees that our IRLS strategy offers, we are able to train the derivedreconstruction networks using a memory-efficient implicit back-propagationscheme, which does not pose any restrictions on their effective depth. Toassess our networks' performance, we compare them against other existingreconstruction methods on several inverse problems, namely image deblurring,super-resolution, demosaicking and sparse recovery. Our reconstruction resultsare shown to be very competitive and in many cases outperform those of existingunrolled networks, whose number of parameters is orders of magnitude higherthan that of our learned models.", "output": "Learning Sparse and Low-Rank Priors for Image Recovery via Iterative Reweighted Least Squares Minimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural radiance fields (NeRFs) enable novel view synthesis with unprecedentedvisual quality. However, to render photorealistic images, NeRFs requirehundreds of deep multilayer perceptron (MLP) evaluations - for each pixel. Thisis prohibitively expensive and makes real-time rendering infeasible, even onpowerful modern GPUs. In this paper, we propose a novel approach to distill andbake NeRFs into highly efficient mesh-based neural representations that arefully compatible with the massively parallel graphics rendering pipeline. Werepresent scenes as neural radiance features encoded on a two-layer duplexmesh, which effectively overcomes the inherent inaccuracies in 3D surfacereconstruction by learning the aggregated radiance information from a reliableinterval of ray-surface intersections. To exploit local geometric relationshipsof nearby pixels, we leverage screen-space convolutions instead of the MLPsused in NeRFs to achieve high-quality appearance. Finally, the performance ofthe whole framework is further boosted by a novel multi-view distillationoptimization strategy. We demonstrate the effectiveness and superiority of ourapproach via extensive experiments on a range of standard datasets.", "output": "Learning Neural Duplex Radiance Fields for Real-Time View Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the rapid progress of generative adversarial networks (GANs) in imagesynthesis in recent years, the existing image synthesis approaches work ineither geometry domain or appearance domain alone which often introducesvarious synthesis artifacts. This paper presents an innovative HierarchicalComposition GAN (HIC-GAN) that incorporates image synthesis in geometry andappearance domains into an end-to-end trainable network and achieves superiorsynthesis realism in both domains simultaneously. We design an innovativehierarchical composition mechanism that is capable of learning realisticcomposition geometry and handling occlusions while multiple foreground objectsare involved in image composition. In addition, we introduce a novel attentionmask mechanism that guides to adapt the appearance of foreground objects whichalso helps to provide better training reference for learning in geometrydomain. Extensive experiments on scene text image synthesis, portrait editingand indoor rendering tasks show that the proposed HIC-GAN achieves superiorsynthesis performance qualitatively and quantitatively.", "output": "Hierarchy Composition GAN for High-fidelity Image Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The high complexity of deep learning models is associated with the difficultyof explaining what evidence they recognize as correlating with specific diseaselabels. This information is critical for building trust in models and findingtheir biases. Until now, automated deep learning visualization solutions haveidentified regions of images used by classifiers, but these solutions are toocoarse, too noisy, or have a limited representation of the way images canchange. We propose a novel method for formulating and presenting spatialexplanations of disease evidence, called deformation field interpretation withgenerative adversarial networks (DeFI-GAN). An adversarially trained generatorproduces deformation fields that modify images of diseased patients to resembleimages of healthy patients. We validate the method studying chronic obstructivepulmonary disease (COPD) evidence in chest x-rays (CXRs) and Alzheimer'sdisease (AD) evidence in brain MRIs. When extracting disease evidence inlongitudinal data, we show compelling results against a baseline producingdifference maps. DeFI-GAN also highlights disease biomarkers not found byprevious methods and potential biases that may help in investigations of thedataset and of the adopted learning methods.", "output": "Interpretation of Disease Evidence for Medical Images Using Adversarial Deformation Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training, especially projected gradient descent (PGD), has provento be a successful approach for improving robustness against adversarialattacks. After adversarial training, gradients of models with respect to theirinputs have a preferential direction. However, the direction of alignment isnot mathematically well established, making it difficult to evaluatequantitatively. We propose a novel definition of this direction as thedirection of the vector pointing toward the closest point of the support of theclosest inaccurate class in decision space. To evaluate the alignment with thisdirection after adversarial training, we apply a metric that uses generativeadversarial networks to produce the smallest residual needed to change theclass present in the image. We show that PGD-trained models have a higheralignment than the baseline according to our definition, that our metricpresents higher alignment values than a competing metric formulation, and thatenforcing this alignment increases the robustness of models.", "output": "Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual place recognition is a challenging task in computer vision and a keycomponent of camera-based localization and navigation systems. Recently,Convolutional Neural Networks (CNNs) achieved high results and goodgeneralization capabilities. They are usually trained using pairs or tripletsof images labeled as either similar or dissimilar, in a binary fashion. Inpractice, the similarity between two images is not binary, but continuous.Furthermore, training these CNNs is computationally complex and involves costlypair and triplet mining strategies. We propose a Generalized Contrastive loss(GCL) function that relies on image similarity as a continuous measure, and useit to train a siamese CNN. Furthermore, we present three techniques forautomatic annotation of image pairs with labels indicating their degree ofsimilarity, and deploy them to re-annotate the MSLS, TB-Places, and 7Scenesdatasets. We demonstrate that siamese CNNs trained using the GCL function andthe improved annotations consistently outperform their binary counterparts. Ourmodels trained on MSLS outperform the state-of-the-art methods, includingNetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on thePittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasonsdatasets. Furthermore, training a siamese network using the GCL function doesnot require complex pair mining. We release the source code at", "output": "Generalized Contrastive Optimization of Siamese Networks for Place Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection in autonomous driving applications implies that thedetection and tracking of semantic objects are commonly native to urban drivingenvironments, as pedestrians and vehicles. One of the major challenges instate-of-the-art deep-learning based object detection are false positives whichoccur with overconfident scores. This is highly undesirable in autonomousdriving and other critical robotic-perception domains because of safetyconcerns. This paper proposes an approach to alleviate the problem ofoverconfident predictions by introducing a novel probabilistic layer to deepobject detection networks in testing. The suggested approach avoids thetraditional Sigmoid or Softmax prediction layer which often producesoverconfident predictions. It is demonstrated that the proposed techniquereduces overconfidence in the false positives without degrading the performanceon the true positives. The approach is validated on the 2D-KITTI objectiondetection through the YOLOV4 and SECOND (Lidar-based detector). The proposedapproach enables interpretable probabilistic predictions without therequirement of re-training the network and therefore is very practical.", "output": "Probabilistic Approach for Road-Users Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The interpretability of medical image analysis models is considered a keyresearch field. We use a dataset of eye-tracking data from five radiologists tocompare the outputs of interpretability methods and the heatmaps representingwhere radiologists looked. We conduct a class-independent analysis of thesaliency maps generated by two methods selected from the literature: Grad-CAMand attention maps from an attention-gated model. For the comparison, we useshuffled metrics, which avoid biases from fixation locations. We achieve scorescomparable to an interobserver baseline in one shuffled metric, highlightingthe potential of saliency maps from Grad-CAM to mimic a radiologist's attentionover an image. We also divide the dataset into subsets to evaluate in whichcases similarities are higher.", "output": "Comparing radiologists' gaze and saliency maps generated by interpretability methods for chest x-rays."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning-based image reconstruction models, such as those based on the U-Net,require a large set of labeled images if good generalization is to beguaranteed. In some imaging domains, however, labeled data with pixel- orvoxel-level label accuracy are scarce due to the cost of acquiring them. Thisproblem is exacerbated further in domains like medical imaging, where there isno single ground truth label, resulting in large amounts of repeat variabilityin the labels. Therefore, training reconstruction networks to generalize betterby learning from both labeled and unlabeled examples (called semi-supervisedlearning) is problem of practical and theoretical interest. However,traditional semi-supervised learning methods for image reconstruction oftennecessitate handcrafting a differentiable regularizer specific to some givenimaging problem, which can be extremely time-consuming. In this work, wepropose \"supervision by denoising\" (SUD), a framework that enables us tosupervise reconstruction models using their own denoised output as soft labels.SUD unifies stochastic averaging and spatial denoising techniques under aspatio-temporal denoising framework and alternates denoising and model weightupdate steps in an optimization framework for semi-supervision. As exampleapplications, we apply SUD to two problems arising from biomedical imaging --anatomical brain reconstruction (3D) and cortical parcellation (2D) -- todemonstrate a significant improvement in the image reconstructions oversupervised-only and stochastic averaging baselines.", "output": "Supervision by Denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Geolocation is a fundamental component of route planning and navigation forunmanned vehicles, but GNSS-based geolocation fails under denial-of-serviceconditions. Cross-view geo-localization (CVGL), which aims to estimate thegeographical location of the ground-level camera by matching against enormousgeo-tagged aerial (emph{e.g.}, satellite) images, has received lots ofattention but remains extremely challenging due to the drastic appearancedifferences across aerial-ground views. In existing methods, globalrepresentations of different views are extracted primarily using Siamese-likearchitectures, but their interactive benefits are seldom taken into account. Inthis paper, we present a novel approach using cross-view knowledge generativetechniques in combination with transformers, namely mutual generativetransformer learning (MGTL), for CVGL. Specifically, by taking the initialrepresentations produced by the backbone network, MGTL develops two separategenerative sub-modules -- one for aerial-aware knowledge generation fromground-view semantics and vice versa -- and fully exploits the entirely mutualbenefits through the attention mechanism. Moreover, to better capture theco-visual relationships between aerial and ground views, we introduce acascaded attention masking algorithm to further boost accuracy. Extensiveexperiments on challenging public benchmarks, emph{i.e.}, {CVACT} and {CVUSA},demonstrate the effectiveness of the proposed method which sets new recordscompared with the existing state-of-the-art models.", "output": "Co-visual pattern augmented generative transformer learning for automobile geo-localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a new method that generates photo-realistic humans under novelviews and poses given a monocular video as input. Despite the significantprogress recently on this topic, with several methods exploring sharedcanonical neural radiance fields in dynamic scene scenarios, learning auser-controlled model for unseen poses remains a challenging task. To tacklethis problem, we introduce an effective method to a) integrate observationsacross several frames and b) encode the appearance at each individual frame. Weaccomplish this by utilizing both the human pose that models the body shape aswell as point clouds that partially cover the human as input. Our approachsimultaneously learns a shared set of latent codes anchored to the human poseamong several frames, and an appearance-dependent code anchored to incompletepoint clouds generated by each frame and its predicted depth. The former humanpose-based code models the shape of the performer whereas the latter pointcloud-based code predicts fine-level details and reasons about missingstructures at the unseen poses. To further recover non-visible regions in queryframes, we employ a temporal transformer to integrate features of points inquery frames and tracked body points from automatically-selected key frames.Experiments on various sequences of dynamic humans from different datasetsincluding ZJU-MoCap show that our method significantly outperforms existingapproaches under unseen poses and novel views given monocular videos as input.", "output": "Neural Rendering of Humans in Novel View and Pose from Monocular Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Satellite Image Time Series (SITS) of the Earth's surface provide detailedland cover maps, with their quality in the spatial and temporal dimensionsconsistently improving. These image time series are integral for developingsystems that aim to produce accurate, up-to-date land cover maps of the Earth'ssurface. Applications are wide-ranging, with notable examples includingecosystem mapping, vegetation process monitoring and anthropogenic land-usechange tracking. Recently proposed methods for SITS classification havedemonstrated respectable merit, but these methods tend to lack nativemechanisms that exploit the temporal dimension of the data; commonly resultingin extensive data pre-processing contributing to prohibitively long trainingtimes. To overcome these shortcomings, Temporal CNNs have recently beenemployed for SITS classification tasks with encouraging results. This paperseeks to survey this method against a plethora of other contemporary methodsfor SITS classification to validate the existing findings in recent literature.Comprehensive experiments are carried out on two benchmark SITS datasets withthe results demonstrating that Temporal CNNs display a superior performance tothe comparative benchmark algorithms across both studied datasets, achievingaccuracies of 95.0% and 87.3% respectively. Investigations into the TemporalCNN architecture also highlighted the non-trivial task of optimising the modelfor a new dataset.", "output": "Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification: A survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The target of space-time video super-resolution (STVSR) is to increase boththe frame rate (also referred to as the temporal resolution) and the spatialresolution of a given video. Recent approaches solve STVSR using end-to-enddeep neural networks. A popular solution is to first increase the frame rate ofthe video; then perform feature refinement among different frame features; andlast increase the spatial resolutions of these features. The temporalcorrelation among features of different frames is carefully exploited in thisprocess. The spatial correlation among features of different (spatial)resolutions, despite being also very important, is however not emphasized. Inthis paper, we propose a spatial-temporal feature interaction network toenhance STVSR by exploiting both spatial and temporal correlations amongfeatures of different frames and spatial resolutions. Specifically, thespatial-temporal frame interpolation module is introduced to interpolate low-and high-resolution intermediate frame features simultaneously andinteractively. The spatial-temporal local and global refinement modules arerespectively deployed afterwards to exploit the spatial-temporal correlationamong different features for their refinement. Finally, a novel motionconsistency loss is employed to enhance the motion continuity amongreconstructed frames. We conduct experiments on three standard benchmarks,Vid4, Vimeo-90K and Adobe240, and the results demonstrate that our methodimproves the state of the art methods by a considerable margin. Our codes willbe available at", "output": "Enhancing Space-time Video Super-resolution via Spatial-temporal Feature Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing spatial localization techniques for autonomous vehicles mostly use apre-built 3D-HD map, often constructed using a survey-grade 3D mapping vehicle,which is not only expensive but also laborious. This paper shows that by usingan off-the-shelf high-definition satellite image as a ready-to-use map, we areable to achieve cross-view vehicle localization up to a satisfactory accuracy,providing a cheaper and more practical way for localization. While theutilization of satellite imagery for cross-view localization is an establishedconcept, the conventional methodology focuses primarily on image retrieval.This paper introduces a novel approach to cross-view localization that departsfrom the conventional image retrieval method. Specifically, our method develops(1) a Geometric-align Feature Extractor (GaFE) that leverages measured 3Dpoints to bridge the geometric gap between ground and overhead views, (2) aPose Aware Branch (PAB) adopting a triplet loss to encourage pose-aware featureextraction, and (3) a Recursive Pose Refine Branch (RPRB) using theLevenberg-Marquardt (LM) algorithm to align the initial pose towards the truevehicle pose iteratively. Our method is validated on KITTI and Ford Multi-AVSeasonal datasets as ground view and Google Maps as the satellite view. Theresults demonstrate the superiority of our method in cross-view localizationwith median spatial and angular errors within $1$ meter and $1^circ$,respectively.", "output": "Satellite Image Based Cross-view Localization for Autonomous Vehicle."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent masked image modeling (MIM) has received much attention inself-supervised learning (SSL), which requires the target model to recover themasked part of the input image. Although MIM-based pre-training methods achievenew state-of-the-art performance when transferred to many downstream tasks, thevisualizations show that the learned representations are less separable,especially compared to those based on contrastive learning pre-training. Thisinspires us to think whether the linear separability of MIM pre-trainedrepresentation can be further improved, thereby improving the pre-trainingperformance. Since MIM and contrastive learning tend to utilize different dataaugmentations and training strategies, combining these two pretext tasks is nottrivial. In this work, we propose a novel and flexible pre-training framework,named MimCo, which combines MIM and contrastive learning through two-stagepre-training. Specifically, MimCo takes a pre-trained contrastive learningmodel as the teacher model and is pre-trained with two types of learningtargets: patch-level and image-level reconstruction losses.Extensive transfer experiments on downstream tasks demonstrate the superiorperformance of our MimCo pre-training framework. Taking ViT-S as an example,when using the pre-trained MoCov3-ViT-S as the teacher model, MimCo only needs100 epochs of pre-training to achieve 82.53% top-1 finetuning accuracy onImagenet-1K, which outperforms the state-of-the-art self-supervised learningcounterparts.", "output": "MimCo: Masked Image Modeling Pre-training with Contrastive Teacher."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current weakly supervised video anomaly detection algorithms mostly usemultiple instance learning (MIL) or their varieties. Almost all recentapproaches focus on how to select the correct snippets for training to improvethe performance. They overlook or do not realize the power of videoclassification in boosting the performance of anomaly detection. In this paper,we study explicitly the power of video classification supervision using a BERTor LSTM. With this BERT or LSTM, CNN features of all snippets of a video can beaggregated into a single feature which can be used for video classification.This simple yet powerful video classification supervision, combined into theMIL framework, brings extraordinary performance improvement on all three majorvideo anomaly detection datasets. Particularly it improves the mean averageprecision (mAP) on the XD-Violence from SOTA 78.84% to new 82.10%. The sourcecode is available at", "output": "Overlooked Video Classification in Weakly Supervised Video Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Background and objective: Self-supervised learning is rapidly advancingcomputer-aided diagnosis in the medical field. Masked image modeling (MIM) isone of the self-supervised learning methods that masks a subset of input pixelsand attempts to predict the masked pixels. Traditional MIM methods often employa random masking strategy. In comparison to ordinary images, medical imagesoften have a small region of interest for disease detection. Consequently, wefocus on fixing the problem in this work, which is evaluated by automaticCOVID-19 identification. Methods: In this study, we propose a novelregion-guided masked image modeling method (RGMIM) for COVID-19 detection inthis paper. In our method, we devise a new masking strategy that employed lungmask information to identify valid regions to learn more useful information forCOVID-19 detection. The proposed method was contrasted with fiveself-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam). Wepresent a quantitative evaluation of open COVID-19 CXR datasets as well asmasking ratio hyperparameter studies. Results: When using the entire trainingset, RGMIM outperformed other comparable methods, achieving 0.962 detectionaccuracy. Specifically, RGMIM significantly improved COVID-19 detection insmall data volumes, such as 5% and 10% of the training set (846 and 1,693images) compared to other methods, and achieved 0.957 detection accuracy evenwhen only 50% of the training set was used. Conclusions: RGMIM can mask morevalid lung-related regions, facilitating the learning of discriminativerepresentations and the subsequent high-accuracy COVID-19 detection. RGMIMoutperforms other state-of-the-art self-supervised learning methods inexperiments, particularly when limited training data is used.", "output": "RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating photos satisfying multiple constraints find broad utility in thecontent creation industry. A key hurdle to accomplishing this task is the needfor paired data consisting of all modalities (i.e., constraints) and theircorresponding output. Moreover, existing methods need retraining using paireddata across all modalities to introduce a new condition. This paper proposes asolution to this problem based on denoising diffusion probabilistic models(DDPMs). Our motivation for choosing diffusion models over other generativemodels comes from the flexible internal structure of diffusion models. Sinceeach sampling step in the DDPM follows a Gaussian distribution, we show thatthere exists a closed-form solution for generating an image given variousconstraints. Our method can unite multiple diffusion models trained on multiplesub-tasks and conquer the combined task through our proposed sampling strategy.We also introduce a novel reliability parameter that allows using differentoff-the-shelf diffusion models trained across various datasets during samplingtime alone to guide it to the desired outcome satisfying multiple constraints.We perform experiments on various standard multimodal tasks to demonstrate theeffectiveness of our approach. More details can be found in", "output": "Unite and Conquer: Plug & Play Multi-Modal Synthesis using Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Talking head synthesis is a promising approach for the video productionindustry. Recently, a lot of effort has been devoted in this research area toimprove the generation quality or enhance the model generalization. However,there are few works able to address both issues simultaneously, which isessential for practical applications. To this end, in this paper, we turnattention to the emerging powerful Latent Diffusion Models, and model theTalking head generation as an audio-driven temporally coherent denoisingprocess (DiffTalk). More specifically, instead of employing audio signals asthe single driving factor, we investigate the control mechanism of the talkingface, and incorporate reference face images and landmarks as conditions forpersonality-aware generalized synthesis. In this way, the proposed DiffTalk iscapable of producing high-quality talking head videos in synchronization withthe source audio, and more importantly, it can be naturally generalized acrossdifferent identities without any further fine-tuning. Additionally, ourDiffTalk can be gracefully tailored for higher-resolution synthesis withnegligible extra computational cost. Extensive experiments show that theproposed DiffTalk efficiently synthesizes high-fidelity audio-driven talkinghead videos for generalized novel identities. For more video results, pleaserefer to url{", "output": "DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Purpose: Acquiring fully-sampled MRI $k$-space data is time-consuming, andcollecting accelerated data can reduce the acquisition time. Employing 2DCartesian-rectilinear subsampling schemes is a conventional approach foraccelerated acquisitions; however, this often results in imprecisereconstructions, even with the use of Deep Learning (DL), especially at highacceleration factors. Non-rectilinear or non-Cartesian trajectories can beimplemented in MRI scanners as alternative subsampling options. This workinvestigates the impact of the $k$-space subsampling scheme on the quality ofreconstructed accelerated MRI measurements produced by trained DL models.Methods: The Recurrent Variational Network (RecurrentVarNet) was used as theDL-based MRI-reconstruction architecture. Cartesian, fully-sampled multi-coil$k$-space measurements from three datasets were retrospectively subsampled withdifferent accelerations using eight distinct subsampling schemes: fourCartesian-rectilinear, two Cartesian non-rectilinear, and two non-Cartesian.Experiments were conducted in two frameworks: scheme-specific, where a distinctmodel was trained and evaluated for each dataset-subsampling scheme pair, andmulti-scheme, where for each dataset a single model was trained on datarandomly subsampled by any of the eight schemes and evaluated on datasubsampled by all schemes.Results: In both frameworks, RecurrentVarNets trained and evaluated onnon-rectilinearly subsampled data demonstrated superior performance,particularly for high accelerations. In the multi-scheme setting,reconstruction performance on rectilinearly subsampled data improved whencompared to the scheme-specific experiments.Conclusion: Our findings demonstrate the potential for using DL-basedmethods, trained on non-rectilinearly subsampled measurements, to optimize scantime and image quality.", "output": "On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In convolutional neural networks, the convolutions are conventionallyperformed using a square kernel with a fixed N $times$ N receptive field (RF).However, what matters most to the network is the effective receptive field(ERF) that indicates the extent with which input pixels contribute to an outputpixel. Inspired by the property that ERFs typically exhibit a Gaussiandistribution, we propose a Gaussian Mask convolutional kernel (GMConv) in thiswork. Specifically, GMConv utilizes the Gaussian function to generate aconcentric symmetry mask that is placed over the kernel to refine the RF. OurGMConv can directly replace the standard convolutions in existing CNNs and canbe easily trained end-to-end by standard back-propagation. We evaluate ourapproach through extensive experiments on image classification and objectdetection tasks. Over several tasks and standard base models, our approachcompares favorably against the standard convolution. For instance, using GMConvfor AlexNet and ResNet-50, the top-1 accuracy on ImageNet classification isboosted by 0.98% and 0.85%, respectively.", "output": "GMConv: Modulating Effective Receptive Fields for Convolutional Kernels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern incremental learning for semantic segmentation methods usually learnnew categories based on dense annotations. Although achieve promising results,pixel-by-pixel labeling is costly and time-consuming. Weakly incrementallearning for semantic segmentation (WILSS) is a novel and attractive task,which aims at learning to segment new classes from cheap and widely availableimage-level labels. Despite the comparable results, the image-level labels cannot provide details to locate each segment, which limits the performance ofWILSS. This inspires us to think how to improve and effectively utilize thesupervision of new classes given image-level labels while avoiding forgettingold ones. In this work, we propose a novel and data-efficient framework forWILSS, named FMWISS. Specifically, we propose pre-training basedco-segmentation to distill the knowledge of complementary foundation models forgenerating dense pseudo labels. We further optimize the noisy pseudo masks witha teacher-student architecture, where a plug-in teacher is optimized with aproposed dense contrastive loss. Moreover, we introduce memory-based copy-pasteaugmentation to improve the catastrophic forgetting problem of old classes.Extensive experiments on Pascal VOC and COCO datasets demonstrate the superiorperformance of our framework, e.g., FMWISS achieves 70.7% and 73.3% in the 15-5VOC setting, outperforming the state-of-the-art method by 3.4% and 6.1%,respectively.", "output": "Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel type of map for visual navigation, a renderable neuralradiance map (RNR-Map), which is designed to contain the overall visualinformation of a 3D environment. The RNR-Map has a grid form and consists oflatent codes at each pixel. These latent codes are embedded from imageobservations, and can be converted to the neural radiance field which enablesimage rendering given a camera pose. The recorded latent codes implicitlycontain visual information about the environment, which makes the RNR-Mapvisually descriptive. This visual information in RNR-Map can be a usefulguideline for visual localization and navigation. We develop localization andnavigation frameworks that can effectively utilize the RNR-Map. We evaluate theproposed frameworks on camera tracking, visual localization, and image-goalnavigation. Experimental results show that the RNR-Map-based localizationframework can find the target location based on a single query image with fastspeed and competitive accuracy compared to other baselines. Also, thislocalization framework is robust to environmental changes, and even finds themost visually similar places when a query image from a different environment isgiven. The proposed navigation framework outperforms the existing image-goalnavigation methods in difficult scenarios, under odometry and actuation noises.The navigation framework shows 65.7% success rate in curved scenarios of theNRNS dataset, which is an improvement of 18.6% over the currentstate-of-the-art. Project page: ", "output": "Renderable Neural Radiance Map for Visual Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For deep ordinal classification, learning a well-structured feature spacespecific to ordinal classification is helpful to properly capture the ordinalnature among classes. Intuitively, when Euclidean distance metric is used, anideal ordinal layout in feature space would be that the sample clusters arearranged in class order along a straight line in space. However, enforcingsamples to conform to a specific layout in the feature space is a challengingproblem. To address this problem, in this paper, we propose a novel ConstrainedProxies Learning (CPL) method, which can learn a proxy for each ordinal classand then adjusts the global layout of classes by constraining these proxies.Specifically, we propose two kinds of strategies: hard layout constraint andsoft layout constraint. The hard layout constraint is realized by directlycontrolling the generation of proxies to force them to be placed in a strictlinear layout or semicircular layout (i.e., two instantiations of strictordinal layout). The soft layout constraint is realized by constraining thatthe proxy layout should always produce unimodal proxy-to-proxies similaritydistribution for each proxy (i.e., to be a relaxed ordinal layout). Experimentsshow that the proposed CPL method outperforms previous deep ordinalclassification methods under the same setting of feature extractor.", "output": "Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The current approach to exploring and monitoring complex underwaterecosystems, such as coral reefs, is to conduct surveys using diver-held orstatic cameras, or deploying sensor buoys. These approaches often fail tocapture the full variation and complexity of interactions between differentreef organisms and their habitat. The CUREE platform presented in this paperprovides a unique set of capabilities in the form of robot behaviors andperception algorithms to enable scientists to explore different aspects of anecosystem. Examples of these capabilities include low-altitude visual surveys,soundscape surveys, habitat characterization, and animal following. Wedemonstrate these capabilities by describing two field deployments on coralreefs in the US Virgin Islands. In the first deployment, we show that CUREE canidentify the preferred habitat type of snapping shrimp in a reef through acombination of a visual survey, habitat characterization, and a soundscapesurvey. In the second deployment, we demonstrate CUREE's ability to followarbitrary animals by separately following a barracuda and stingray for severalminutes each in midwater and benthic environments, respectively.", "output": "CUREE: A Curious Underwater Robot for Ecosystem Exploration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Active visual exploration addresses the issue of limited sensor capabilitiesin real-world scenarios, where successive observations are actively chosenbased on the environment. To tackle this problem, we introduce a new techniquecalled Attention-Map Entropy (AME). It leverages the internal uncertainty ofthe transformer-based model to determine the most informative observations. Incontrast to existing solutions, it does not require additional loss components,which simplifies the training. Through experiments, which also mimicretina-like sensors, we show that such simplified training significantlyimproves the performance of reconstruction, segmentation and classification onpublicly available datasets.", "output": "Active Visual Exploration Based on Attention-Map Entropy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recently proposed open-world object and open-set detection have achieveda breakthrough in finding never-seen-before objects and distinguishing themfrom known ones. However, their studies on knowledge transfer from knownclasses to unknown ones are not deep enough, resulting in the scanty capabilityfor detecting unknowns hidden in the background. In this paper, we propose theunknown sniffer (UnSniffer) to find both unknown and known objects. Firstly,the generalized object confidence (GOC) score is introduced, which only usesknown samples for supervision and avoids improper suppression of unknowns inthe background. Significantly, such confidence score learned from known objectscan be generalized to unknown ones. Additionally, we propose a negative energysuppression loss to further suppress the non-object samples in the background.Next, the best box of each unknown is hard to obtain during inference due tolacking their semantic information in training. To solve this issue, weintroduce a graph-based determination scheme to replace hand-designednon-maximum suppression (NMS) post-processing. Finally, we present the UnknownObject Detection Benchmark, the first publicly benchmark that encompassesprecision evaluation for unknown detection to our knowledge. Experiments showthat our method is far better than the existing state-of-the-art methods.", "output": "Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper raises the new task of Fisheye Semantic Completion (FSC), wheredense texture, structure, and semantics of a fisheye image are inferred evenbeyond the sensor field-of-view (FoV). Fisheye cameras have larger FoV thanordinary pinhole cameras, yet its unique special imaging model naturally leadsto a blind area at the edge of the image plane. This is suboptimal forsafety-critical applications since important perception tasks, such as semanticsegmentation, become very challenging within the blind zone. Previous worksconsidered the out-FoV outpainting and in-FoV segmentation separately. However,we observe that these two tasks are actually closely coupled. To jointlyestimate the tightly intertwined complete fisheye image and scene semantics, weintroduce the new FishDreamer which relies on successful ViTs enhanced with anovel Polar-aware Cross Attention module (PCA) to leverage dense context andguide semantically-consistent content generation while considering differentpolar distributions. In addition to the contribution of the novel task andarchitecture, we also derive Cityscapes-BF and KITTI360-BF datasets tofacilitate training and evaluation of this new track. Our experimentsdemonstrate that the proposed FishDreamer outperforms methods solving each taskin isolation and surpasses alternative approaches on the Fisheye SemanticCompletion. Code and datasets are publicly available at", "output": "FishDreamer: Towards Fisheye Semantic Completion via Unified Image Outpainting and Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrails (condensation trails) are line-shaped ice clouds caused by aircraftand are likely the largest contributor of aviation-induced climate change.Contrail avoidance is potentially an inexpensive way to significantly reducethe climate impact of aviation. An automated contrail detection system is anessential tool to develop and evaluate contrail avoidance systems. In thispaper, we present a human-labeled dataset named OpenContrails to train andevaluate contrail detection models based on GOES-16 Advanced Baseline Imager(ABI) data. We propose and evaluate a contrail detection model thatincorporates temporal context for improved detection accuracy. The humanlabeled dataset and the contrail detection outputs are publicly available onGoogle Cloud Storage at gs://goes_contrails_dataset.", "output": "OpenContrails: Benchmarking Contrail Detection on GOES-16 ABI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data augmentation is a promising technique for unsupervised anomaly detectionin industrial applications, where the availability of positive samples is oftenlimited due to factors such as commercial competition and sample collectiondifficulties. In this paper, how to effectively select and apply dataaugmentation methods for unsupervised anomaly detection is studied. The impactof various data augmentation methods on different anomaly detection algorithmsis systematically investigated through experiments. The experimental resultsshow that the performance of different industrial image anomaly detection(termed as IAD) algorithms is not significantly affected by the specific dataaugmentation method employed and that combining multiple data augmentationmethods does not necessarily yield further improvements in the accuracy ofanomaly detection, although it can achieve excellent results on specificmethods. These findings provide useful guidance on selecting appropriate dataaugmentation methods for different requirements in IAD.", "output": "What makes a good data augmentation for few-shot unsupervised image anomaly detection?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since the number of incident energies is limited, it is difficult to directlyacquire hyperspectral images (HSI) with high spatial resolution. Consideringthe high dimensionality and correlation of HSI, super-resolution (SR) of HSIremains a challenge in the absence of auxiliary high-resolution images.Furthermore, it is very important to extract the spatial features effectivelyand make full use of the spectral information. This paper proposes a novel HSIsuper-resolution algorithm, termed dual-domain network based on hybridconvolution (SRDNet). Specifically, a dual-domain network is designed to fullyexploit the spatial-spectral and frequency information among the hyper-spectraldata. To capture inter-spectral self-similarity, a self-attention learningmechanism (HSL) is devised in the spatial domain. Meanwhile the pyramidstructure is applied to increase the acceptance field of attention, whichfurther reinforces the feature representation ability of the network. Moreover,to further improve the perceptual quality of HSI, a frequency loss(HFL) isintroduced to optimize the model in the frequency domain. The dynamic weightingmechanism drives the network to gradually refine the generated frequency andexcessive smoothing caused by spatial loss. Finally, In order to better fullyobtain the mapping relationship between high-resolution space andlow-resolution space, a hybrid module of 2D and 3D units with progressiveupsampling strategy is utilized in our method. Experiments on a widely usedbenchmark dataset illustrate that the proposed SRDNet method enhances thetexture information of HSI and is superior to state-of-the-art methods.", "output": "Hyperspectral Image Super-Resolution via Dual-domain Network Based on Hybrid Convolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Brain extraction is a critical preprocessing step in various neuroimagingstudies, particularly enabling accurate separation of brain from non-braintissue and segmentation of relevant within-brain tissue compartments andstructures using Magnetic Resonance Imaging (MRI) data. FSL's Brain ExtractionTool (BET), although considered the current gold standard for automatic brainextraction, presents limitations and can lead to errors such as over-extractionin brains with lesions affecting the outer parts of the brain, inaccuratedifferentiation between brain tissue and surrounding meninges, andsusceptibility to image quality issues. Recent advances in computer visionresearch have led to the development of the Segment Anything Model (SAM) byMeta AI, which has demonstrated remarkable potential in zero-shot segmentationof objects in real-world scenarios. In the current paper, we present acomparative analysis of brain extraction techniques comparing SAM with a widelyused and current gold standard technique called BET on a variety of brain scanswith varying image qualities, MR sequences, and brain lesions affectingdifferent brain regions. We find that SAM outperforms BET based on average Dicecoefficient, IoU and accuracy metrics, particularly in cases where imagequality is compromised by signal inhomogeneities, non-isotropic voxelresolutions, or the presence of brain lesions that are located near (orinvolve) the outer regions of the brain and the meninges. In addition, SAM hasalso unsurpassed segmentation properties allowing a fine grain separation ofdifferent issue compartments and different brain structures. These resultssuggest that SAM has the potential to emerge as a more accurate, robust andversatile tool for a broad range of brain extraction and segmentationapplications.", "output": "SAM vs BET: A Comparative Study for Brain Extraction and Segmentation of Magnetic Resonance Images using Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have proven to be highly effective in generatinghigh-quality images. However, adapting large pre-trained diffusion models tonew domains remains an open challenge, which is critical for real-worldapplications. This paper proposes DiffFit, a parameter-efficient strategy tofine-tune large pre-trained diffusion models that enable fast adaptation to newdomains. DiffFit is embarrassingly simple that only fine-tunes the bias termand newly-added scaling factors in specific layers, yet resulting insignificant training speed-up and reduced model storage costs. Compared withfull fine-tuning, DiffFit achieves 2$times$ training speed-up and only needsto store approximately 0.12% of the total model parameters. Intuitivetheoretical analysis has been provided to justify the efficacy of scalingfactors on fast adaptation. On 8 downstream datasets, DiffFit achieves superioror competitive performances compared to the full fine-tuning while being moreefficient. Remarkably, we show that DiffFit can adapt a pre-trainedlow-resolution generative model to a high-resolution one by adding minimalcost. Among diffusion-based methods, DiffFit sets a new state-of-the-art FID of3.02 on ImageNet 512$times$512 benchmark by fine-tuning only 25 epochs from apublic pre-trained ImageNet 256$times$256 checkpoint while being 30$times$more training efficient than the closest competitor.", "output": "DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper discusses the results for the second edition of the MonocularDepth Estimation Challenge (MDEC). This edition was open to methods using anyform of supervision, including fully-supervised, self-supervised, multi-task orproxy depth. The challenge was based around the SYNS-Patches dataset, whichfeatures a wide diversity of environments with high-quality dense ground-truth.This includes complex natural environments, e.g. forests or fields, which aregreatly underrepresented in current benchmarks.The challenge received eight unique submissions that outperformed theprovided SotA baseline on any of the pointcloud- or image-based metrics. Thetop supervised submission improved relative F-Score by 27.62%, while the topself-supervised improved it by 16.61%. Supervised submissions generallyleveraged large collections of datasets to improve data diversity.Self-supervised submissions instead updated the network architecture andpretrained backbones. These results represent a significant progress in thefield, while highlighting avenues for future research, such as reducinginterpolation artifacts at depth boundaries, improving self-supervised indoorperformance and overall natural image accuracy.", "output": "The Second Monocular Depth Estimation Challenge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Structured reconstruction is a non-trivial dense prediction problem, whichextracts structural information (eg, building corners and edges) from a rasterimage, then reconstructs it to a 2D planar graph accordingly. Compared withcommon segmentation or detection problems, it significantly relays on thecapability that leveraging holistic geometric information for structuralreasoning. Current transformer-based approaches tackle this challenging problemin a two-stage manner, which detect corners in the first model and classify theproposed edges (corner-pairs) in the second model. However, they separatetwo-stage into different models and only share the backbone encoder. Unlike theexisting modeling strategies, we present an enhanced corner representationmethod: 1) It fuses knowledge between the corner detection and edge predictionby sharing feature in different granularity; 2) Corner candidates are proposedin four heatmap channels w.r.t its direction. Both qualitative and quantitativeevaluations demonstrate that our proposed method can better reconstructfine-grained structures, such as adjacent corners and tiny edges. Consequently,it outperforms the state-of-the-art model by +1.9%@F-1 on Corner and+3.0%@F-1 on Edge.", "output": "CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The University of California San Francisco Brain Metastases StereotacticRadiosurgery (UCSF-BMSR) dataset is a public, clinical, multimodal brain MRIdataset consisting of 560 brain MRIs from 412 patients with expert annotationsof 5136 brain metastases. Data consists of registered and skull stripped T1post-contrast, T1 pre-contrast, FLAIR and subtraction (T1 pre-contrast - T1post-contrast) images and voxelwise segmentations of enhancing brain metastasesin NifTI format. The dataset also includes patient demographics, surgicalstatus and primary cancer types. The UCSF-BSMR has been made publicly availablein the hopes that researchers will use these data to push the boundaries of AIapplications for brain metastases.", "output": "The University of California San Francisco, Brain Metastases Stereotactic Radiosurgery (UCSF-BMSR) MRI Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional neural networks have demonstrated impressive results in manycomputer vision tasks. However, the increasing size of these networks raisesconcerns about the information overload resulting from the large number ofnetwork parameters. In this paper, we propose Frequency Regularization torestrict the non-zero elements of the network parameters in frequency domain.The proposed approach operates at the tensor level, and can be applied toalmost all network architectures. Specifically, the tensors of parameters aremaintained in the frequency domain, where high frequency components can beeliminated by zigzag setting tensor elements to zero. Then, the inversediscrete cosine transform (IDCT) is used to reconstruct the spatial tensors formatrix operations during network training. Since high frequency components ofimages are known to be less critical, a large proportion of these parameterscan be set to zero when networks are trained with the proposed frequencyregularization. Comprehensive evaluations on various state-of-the-art networkarchitectures, including LeNet, Alexnet, VGG, Resnet, ViT, UNet, GAN, and VAE,demonstrate the effectiveness of the proposed frequency regularization. Underthe condition of a very small accuracy decrease (less than 2%), a LeNet5 with0.4M parameters can be represented by only 776 float16 numbers(over1100$times$), and a UNet with 34M parameters can be represented by only 759float16 numbers (over 80000$times$).", "output": "Frequency Regularization: Restricting Information Redundancy of Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optical coherence tomography angiography (OCTA) is a non-invasive imagingmodality that extends the functionality of OCT by extracting moving red bloodcell signals from surrounding static biological tissues. OCTA has emerged as avaluable tool for analyzing skin microvasculature, enabling more accuratediagnosis and treatment monitoring. Most existing OCTA extraction algorithms,such as speckle variance (SV)- and eigen-decomposition (ED)-OCTA, implement alarger number of repeated (NR) OCT scans at the same position to producehigh-quality angiography images. However, a higher NR requires a longer dataacquisition time, leading to more unpredictable motion artifacts. In thisstudy, we propose a vasculature extraction pipeline that uses only one-repeatedOCT scan to generate OCTA images. The pipeline is based on the proposedVasculature Extraction Transformer (VET), which leverages convolutionalprojection to better learn the spatial relationships between image patches. Incomparison to OCTA images obtained via the SV-OCTA (PSNR: 17.809) and ED-OCTA(PSNR: 18.049) using four-repeated OCT scans, OCTA images extracted by VETexhibit moderate quality (PSNR: 17.515) and higher image contrast whilereducing the required data acquisition time from ~8 s to ~2 s. Based on visualobservations, the proposed VET outperforms SV and ED algorithms when using neckand face OCTA data in areas that are challenging to scan. This study representsthat the VET has the capacity to extract vascularture images from a fastone-repeated OCT scan, facilitating accurate diagnosis for patients.", "output": "Deep-Learning-based Vasculature Extraction for Single-Scan Optical Coherence Tomography Angiography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers have shown great performance in single tasks such asclassification and segmentation. However, real-world problems are not isolated,which calls for vision transformers that can perform multiple tasksconcurrently. Existing multi-task vision transformers are handcrafted andheavily rely on human expertise. In this work, we propose a novel one-shotneural architecture search framework, dubbed AutoTaskFormer (AutomatedMulti-Task Vision TransFormer), to automate this process. AutoTaskFormer notonly identifies the weights to share across multiple tasks automatically, butalso provides thousands of well-trained vision transformers with a wide rangeof parameters (e.g., number of heads and network depth) for deployment undervarious resource constraints. Experiments on both small-scale (2-taskCityscapes and 3-task NYUv2) and large-scale (16-task Taskonomy) datasets showthat AutoTaskFormer outperforms state-of-the-art handcrafted visiontransformers in multi-task learning. The entire code and models will beopen-sourced.", "output": "AutoTaskFormer: Searching Vision Transformers for Multi-task Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Obtaining sufficient labelled data for model training is impractical for mostreal-life applications. Therefore, we address the problem of domaingeneralization for semantic segmentation tasks to reduce the need to acquireand label additional data. Recent work on domain generalization increase datadiversity by varying domain-variant features such as colour, style and texturein images. However, excessive stylization or even uniform stylization mayreduce performance. Performance reduction is especially pronounced for pixelsfrom minority classes, which are already more challenging to classify comparedto pixels from majority classes. Therefore, we introduce a module, $ASH_{+}$,that modulates stylization strength for each pixel depending on the pixel'ssemantic content. In this work, we also introduce a parameter that balances theelement-wise and channel-wise proportion of stylized features with the originalsource domain features in the stylized source domain images. This learnedparameter replaces an empirically determined global hyperparameter, allowingfor more fine-grained control over the output stylized image. We conductmultiple experiments to validate the effectiveness of our proposed method.Finally, we evaluate our model on the publicly available benchmark semanticsegmentation datasets (Cityscapes and SYNTHIA). Quantitative and qualitativecomparisons indicate that our approach is competitive with state-of-the-art.Code is made available at url{", "output": "Adaptive Stylization Modulation for Domain Generalized Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Synthesizing novel views from a single view image is a highly ill-posedproblem. We discover an effective solution to reduce the learning ambiguity byexpanding the single-view view synthesis problem to a multi-view setting.Specifically, we leverage the reliable and explicit stereo prior to generate apseudo-stereo viewpoint, which serves as an auxiliary input to construct the 3Dspace. In this way, the challenging novel view synthesis process is decoupledinto two simpler problems of stereo synthesis and 3D reconstruction. In orderto synthesize a structurally correct and detail-preserved stereo image, wepropose a self-rectified stereo synthesis to amend erroneous regions in anidentify-rectify manner. Hard-to-train and incorrect warping samples are firstdiscovered by two strategies, 1) pruning the network to reveal low-confidentpredictions; and 2) bidirectionally matching between stereo images to allow thediscovery of improper mapping. These regions are then inpainted to form thefinal pseudo-stereo. With the aid of this extra input, a preferable 3Dreconstruction can be easily obtained, and our method can work with arbitrary3D representations. Extensive experiments show that our method outperformsstate-of-the-art single-view view synthesis methods and stereo synthesismethods.", "output": "Single-View View Synthesis with Self-Rectified Pseudo-Stereo."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a new technique called CHATTY: Coupled Holistic AdversarialTransport Terms with Yield for Unsupervised Domain Adaptation. Adversarialtraining is commonly used for learning domain-invariant representations byreversing the gradients from a domain discriminator head to train the featureextractor layers of a neural network. We propose significant modifications tothe adversarial head, its training objective, and the classifier head. With theaim of reducing class confusion, we introduce a sub-network which displaces theclassifier outputs of the source and target domain samples in a learnablemanner. We control this movement using a novel transport loss that spreadsclass clusters away from each other and makes it easier for the classifier tofind the decision boundaries for both the source and target domains. Theresults of adding this new loss to a careful selection of previously proposedlosses leads to improvement in UDA results compared to the previousstate-of-the-art methods on benchmark datasets. We show the importance of theproposed loss term using ablation studies and visualization of the movement oftarget domain sample in representation space.", "output": "CHATTY: Coupled Holistic Adversarial Transport Terms with Yield for Unsupervised Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The popularity of Neural Radiance Fields (NeRFs) for view synthesis has ledto a desire for NeRF editing tools. Here, we focus on inpainting regions in aview-consistent and controllable manner. In addition to the typical NeRF inputsand masks delineating the unwanted region in each view, we require only asingle inpainted view of the scene, i.e., a reference view. We use monoculardepth estimators to back-project the inpainted view to the correct 3Dpositions. Then, via a novel rendering technique, a bilateral solver canconstruct view-dependent effects in non-reference views, making the inpaintedregion appear consistent from any view. For non-reference disoccluded regions,which cannot be supervised by the single reference view, we devise a methodbased on image inpainters to guide both the geometry and appearance. Ourapproach shows superior performance to NeRF inpainting baselines, with theadditional advantage that a user can control the generated scene via a singleinpainted image. Project page: ", "output": "Reference-guided Controllable Inpainting of Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Style transfer aims to render the style of a given image for style referenceto another given image for content reference, and has been widely adopted inartistic generation and image editing. Existing approaches either apply theholistic style of the style image in a global manner, or migrate local colorsand textures of the style image to the content counterparts in a pre-definedway. In either case, only one result can be generated for a specific pair ofcontent and style images, which therefore lacks flexibility and is hard tosatisfy different users with different preferences. We propose here a novelstrategy termed Any-to-Any Style Transfer to address this drawback, whichenables users to interactively select styles of regions in the style image andapply them to the prescribed content regions. In this way, personalizable styletransfer is achieved through human-computer interaction. At the heart of ourapproach lies in (1) a region segmentation module based on Segment Anything,which supports region selection with only some clicks or drawing on images andthus takes user inputs conveniently and flexibly; (2) and an attention fusionmodule, which converts inputs from users to controlling signals for the styletransfer model. Experiments demonstrate the effectiveness for personalizablestyle transfer. Notably, our approach performs in a plug-and-play mannerportable to any style transfer method and enhance the controllablity. Our codeis available href{", "output": "Any-to-Any Style Transfer: Making Picasso and Da Vinci Collaborate."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image recognition often faces the problem of insufficient data inpractical applications. Image recognition and processing under few-shotconditions will produce overfitting, low recognition accuracy, low reliabilityand insufficient robustness. It is often the case that the difference ofcharacteristics is subtle, and the recognition is affected by perspectives,background, occlusion and other factors, which increases the difficulty ofrecognition. Furthermore, in fine-grained images, the few-shot problem leads toinsufficient useful feature information in the images. Considering thecharacteristics of few-shot and fine-grained image recognition, this study hasestablished a recognition model based on attention and Siamese neural network.Aiming at the problem of few-shot samples, a Siamese neural network suitablefor classification model is proposed. The Attention-Based neural network isused as the main network to improve the classification effect. Covid- 19 lungsamples have been selected for testing the model. The results show that theless the number of image samples are, the more obvious the advantage shows thanthe ordinary neural network.", "output": "Application of attention-based Siamese composite neural network in medical image recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-local attention module has been proven to be crucial for imagerestoration. Conventional non-local attention processes features of each layerseparately, so it risks missing correlation between features among differentlayers. To address this problem, we aim to design attention modules thataggregate information from different layers. Instead of finding correlated keypixels within the same layer, each query pixel is encouraged to attend to keypixels at multiple previous layers of the network. In order to efficientlyembed such attention design into neural network backbones, we propose a novelAdaptive Cross-Layer Attention (ACLA) module. Two adaptive designs are proposedfor ACLA: (1) adaptively selecting the keys for non-local attention at eachlayer; (2) automatically searching for the insertion locations for ACLAmodules. By these two adaptive designs, ACLA dynamically selects a flexiblenumber of keys to be aggregated for non-local attention at previous layer whilemaintaining a compact neural network with compelling performance. Extensiveexperiments on image restoration tasks, including single imagesuper-resolution, image denoising, image demosaicing, and image compressionartifacts reduction, validate the effectiveness and efficiency of ACLA. Thecode of ACLA is available at url{", "output": "Adaptive Cross-Layer Attention for Image Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel approach to efficiently compute tight non-convexenclosures of the image through neural networks with ReLU, sigmoid, orhyperbolic tangent activation functions. In particular, we abstract theinput-output relation of each neuron by a polynomial approximation, which isevaluated in a set-based manner using polynomial zonotopes. While our approachcan also can be beneficial for open-loop neural network verification, our mainapplication is reachability analysis of neural network controlled systems,where polynomial zonotopes are able to capture the non-convexity caused by theneural network as well as the system dynamics. This results in a superiorperformance compared to other methods, as we demonstrate on various benchmarks.", "output": "Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Support vector clustering is an important clustering method. However, itsuffers from a scalability issue due to its computational expensive clusterassignment step. In this paper we accelertate the support vector clustering viaspectrum-preserving data compression. Specifically, we first compress theoriginal data set into a small amount of spectrally representative aggregateddata points. Then, we perform standard support vector clustering on thecompressed data set. Finally, we map the clustering results of the compresseddata set back to discover the clusters in the original data set. Our extensiveexperimental results on real-world data set demonstrate dramatically speedupsover standard support vector clustering without sacrificing clustering quality.", "output": "Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Evolutionary algorithms have been used to evolve a population of actors togenerate diverse experiences for training reinforcement learning agents, whichhelps to tackle the temporal credit assignment problem and improves theexploration efficiency. However, when adapting this approach to addressconstrained problems, balancing the trade-off between the reward and constraintviolation is hard. In this paper, we propose a novel evolutionary constrainedreinforcement learning (ECRL) algorithm, which adaptively balances the rewardand constraint violation with stochastic ranking, and at the same time,restricts the policy's behaviour by maintaining a set of Lagrange relaxationcoefficients with a constraint buffer. Extensive experiments on robotic controlbenchmarks show that our ECRL achieves outstanding performance compared tostate-of-the-art algorithms. Ablation analysis shows the benefits ofintroducing stochastic ranking and constraint buffer.", "output": "Evolving Constrained Reinforcement Learning Policy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The necessity for cooperation among intelligent machines has popularisedcooperative multi-agent reinforcement learning (MARL) in AI research. However,many research endeavours heavily rely on parameter sharing among agents, whichconfines them to only homogeneous-agent setting and leads to traininginstability and lack of convergence guarantees. To achieve effectivecooperation in the general heterogeneous-agent setting, we proposeHeterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve theaforementioned issues. Central to our findings are the multi-agent advantagedecomposition lemma and the sequential update scheme. Based on these, wedevelop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL)that is free of parameter-sharing constraint, and derive HATRPO and HAPPO bytractable approximations. Furthermore, we discover a novel framework namedHeterogeneous-Agent Mirror Learning (HAML), which strengthens theoreticalguarantees for HATRPO and HAPPO and provides a general template for cooperativeMARL algorithmic designs. We prove that all algorithms derived from HAMLinherently enjoy monotonic improvement of joint reward and convergence to NashEquilibrium. As its natural outcome, HAML validates more novel algorithms inaddition to HATRPO and HAPPO, including HAA2C, HADDPG, and HATD3, whichconsistently outperform their existing MA-counterparts. We comprehensively testHARL algorithms on six challenging benchmarks and demonstrate their superioreffectiveness and stability for coordinating heterogeneous agents compared tostrong baselines such as MAPPO and QMIX.", "output": "Heterogeneous-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a theory for the previously unexplained divergent behavior noticedin the training of large language models. We argue that the phenomenon is anartifact of the dominant optimization algorithm used for training, called Adam.We observe that Adam can enter a state in which the parameter update vector hasa relatively large norm and is essentially uncorrelated with the direction ofdescent on the training loss landscape, leading to divergence. This artifact ismore likely to be observed in the training of a deep model with a large batchsize, which is the typical setting of large-scale language model training. Toargue the theory, we present observations from the training runs of thelanguage models of different scales: 7 billion, 30 billion, 65 billion, and 546billion parameters.", "output": "A Theory on Adam Instability in Large-Scale Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a framework for descriptively analyzing sets of partial ordersbased on the concept of depth functions. Despite intensive studies of depthfunctions in linear and metric spaces, there is very little discussion on depthfunctions for non-standard data types such as partial orders. We introduce anadaptation of the well-known simplicial depth to the set of all partial orders,the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for acomparison of machine learning algorithms based on multidimensional performancemeasures. Concretely, we analyze the distribution of different classifierperformances over a sample of standard benchmark data sets. Our resultspromisingly demonstrate that our approach differs substantially from existingbenchmarking approaches and, therefore, adds a new perspective to the vividdebate on the comparison of classifiers.", "output": "Depth Functions for Partial Orders with a Descriptive Analysis of Machine Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current studies on adversarial robustness mainly focus on aggregating localrobustness results from a set of data samples to evaluate and rank differentmodels. However, the local statistics may not well represent the true globalrobustness of the underlying unknown data distribution. To address thischallenge, this paper makes the first attempt to present a new framework,called GREAT Score , for global robustness evaluation of adversarialperturbation using generative models. Formally, GREAT Score carries thephysical meaning of a global statistic capturing a mean certified attack-proofperturbation level over all samples drawn from a generative model. Forfinite-sample evaluation, we also derive a probabilistic guarantee on thesample complexity and the difference between the sample mean and the true mean.GREAT Score has several advantages: (1) Robustness evaluations using GREATScore are efficient and scalable to large models, by sparing the need ofrunning adversarial attacks. In particular, we show high correlation andsignificantly reduced computation cost of GREAT Score when compared to theattack-based model ranking on RobustBench (Croce,et. al. 2021). (2) The use ofgenerative models facilitates the approximation of the unknown datadistribution. In our ablation study with different generative adversarialnetworks (GANs), we observe consistency between global robustness evaluationand the quality of GANs. (3) GREAT Score can be used for remote auditing ofprivacy-sensitive black-box models, as demonstrated by our robustnessevaluation on several online facial recognition services.", "output": "GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) presents a decentralized approach to model trainingin the agri-food sector and offers the potential for improved machine learningperformance, while ensuring the safety and privacy of individual farms or datasilos. However, the conventional FL approach has two major limitations. First,the heterogeneous data on individual silos can cause the global model toperform well for some clients but not all, as the update direction on someclients may hinder others after they are aggregated. Second, it is lacking withrespect to the efficiency perspective concerning communication costs during FLand large model sizes. This paper proposes a new technical solution thatutilizes network pruning on client models and aggregates the pruned models.This method enables local models to be tailored to their respective datadistribution and mitigate the data heterogeneity present in agri-food data.Moreover, it allows for more compact models that consume less data duringtransmission. We experiment with a soybean yield forecasting dataset and findthat this approach can improve inference performance by 15.5% to 20% comparedto FedAvg, while reducing local model sizes by up to 84% and the data volumecommunicated between the clients and the server by 57.1% to 64.7%.", "output": "Model Pruning Enables Localized and Efficient Federated Learning for Yield Forecasting and Data Sharing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum support vector machines have the potential to achieve a quantumspeedup for solving certain machine learning problems. The key challenge fordoing so is finding good quantum kernels for a given data set -- a task calledkernel alignment. In this paper we study this problem using the Pegasosalgorithm, which is an algorithm that uses stochastic gradient descent to solvethe support vector machine optimization problem. We extend Pegasos to thequantum case and and demonstrate its effectiveness for kernel alignment. Unlikeprevious work which performs kernel alignment by training a QSVM within anouter optimization loop, we show that using Pegasos it is possible tosimultaneously train the support vector machine and align the kernel. Ourexperiments show that this approach is capable of aligning quantum feature mapswith high accuracy, and outperforms existing quantum kernel alignmenttechniques. Specifically, we demonstrate that Pegasos is particularly effectivefor non-stationary data, which is an important challenge in real-worldapplications.", "output": "Quantum Kernel Alignment with Stochastic Gradient Descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online media has revolutionized the way political information is disseminatedand consumed on a global scale, and this shift has compelled political figuresto adopt new strategies of capturing and retaining voter attention. Thesestrategies often rely on emotional persuasion and appeal, and as visual contentbecomes increasingly prevalent in virtual space, much of politicalcommunication too has come to be marked by evocative video content and imagery.The present paper offers a novel approach to analyzing material of this kind.We apply a deep-learning-based computer-vision algorithm to a sample of 220YouTube videos depicting political leaders from 15 different countries, whichis based on an existing trained convolutional neural network architectureprovided by the Python library fer. The algorithm returns emotion scoresrepresenting the relative presence of 6 emotional states (anger, disgust, fear,happiness, sadness, and surprise) and a neutral expression for each frame ofthe processed YouTube video. We observe statistically significant differencesin the average score of expressed negative emotions between groups of leaderswith varying degrees of populist rhetoric as defined by the Global Party Survey(GPS), indicating that populist leaders tend to express negative emotions to agreater extent during their public performance than their non-populistcounterparts. Overall, our contribution provides insight into thecharacteristics of visual self-representation among political leaders, as wellas an open-source workflow for further computational studies of theirnon-verbal communication.", "output": "The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Individuals high in social anxiety symptoms often exhibit elevated stateanxiety in social situations. Research has shown it is possible to detect stateanxiety by leveraging digital biomarkers and machine learning techniques.However, most existing work trains models on an entire group of participants,failing to capture individual differences in their psychological and behavioralresponses to social contexts. To address this concern, in Study 1, we collectedlinguistic data from N=35 high socially anxious participants in a variety ofsocial contexts, finding that digital linguistic biomarkers significantlydiffer between evaluative vs. non-evaluative social contexts and betweenindividuals having different trait psychological symptoms, suggesting thelikely importance of personalized approaches to detect state anxiety. In Study2, we used the same data and results from Study 1 to model a multilayerpersonalized machine learning pipeline to detect state anxiety that considerscontextual and individual differences. This personalized model outperformed thebaseline F1-score by 28.0%. Results suggest that state anxiety can be moreaccurately detected with personalized machine learning approaches, and thatlinguistic biomarkers hold promise for identifying periods of state anxiety inan unobtrusive way.", "output": "Personalized State Anxiety Detection: An Empirical Study with Linguistic Biomarkers and A Machine Learning Pipeline."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The importance of predicting stock market prices cannot be overstated. It isa pivotal task for investors and financial institutions as it enables them tomake informed investment decisions, manage risks, and ensure the stability ofthe financial system. Accurate stock market predictions can help investorsmaximize their returns and minimize their losses, while financial institutionscan use this information to develop effective risk management policies.However, stock market prediction is a challenging task due to the complexnature of the stock market and the multitude of factors that can affect stockprices. As a result, advanced technologies such as deep learning are beingincreasingly utilized to analyze vast amounts of data and provide valuableinsights into the behavior of the stock market. While deep learning has shownpromise in accurately predicting stock prices, there is still much research tobe done in this area.", "output": "Identifying Trades Using Technical Analysis and ML/DL Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the impacts of business cycles on machine learning (ML) predictions.Using the S&amp;P 500 index, we find that ML models perform worse during mostrecessions, and the inclusion of recession history or the risk-free rate doesnot necessarily improve their performance. Investigating recessions wheremodels perform well, we find that they exhibit lower market volatility thanother recessions. This implies that the improved performance is not due to themerit of ML methods but rather factors such as effective monetary policies thatstabilized the market. We recommend that ML practitioners evaluate their modelsduring both recessions and expansions.", "output": "Stock Price Predictability and the Business Cycle via Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Asset-specific factors are commonly used to forecast financial returns andquantify asset-specific risk premia. Using various machine learning models, wedemonstrate that the information contained in these factors leads to evenlarger economic gains in terms of forecasts of sector returns and themeasurement of sector-specific risk premia. To capitalize on the strongpredictive results of individual models for the performance of differentsectors, we develop a novel online ensemble algorithm that learns to optimizepredictive performance. The algorithm continuously adapts over time todetermine the optimal combination of individual models by solely analyzingtheir most recent prediction performance. This makes it particularly suited fortime series problems, rolling window backtesting procedures, and systems ofpotentially black-box models. We derive the optimal gain function, express thecorresponding regret bounds in terms of the out-of-sample R-squared measure,and derive optimal learning rate for the algorithm. Empirically, the newensemble outperforms both individual machine learning models and their simpleaverages in providing better measurements of sector risk premia. Moreover, itallows for performance attribution of different factors across various sectors,without conditioning on a specific model. Finally, by utilizing monthlypredictions from our ensemble, we develop a sector rotation strategy thatsignificantly outperforms the market. The strategy remains robust againstvarious financial factors, periods of financial distress, and conservativetransaction costs. Notably, the strategy's efficacy persists over time,exhibiting consistent improvement throughout an extended backtesting period andyielding substantial profits during the economic turbulence of the COVID-19pandemic.", "output": "Online Ensemble of Models for Optimal Predictive Performance with Applications to Sector Rotation Strategy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Languages are not created randomly but rather to communicate information.There is a strong association between languages and their underlying meanings,resulting in a sparse joint distribution that is heavily peaked according totheir correlations. Moreover, these peak values happen to match with themarginal distribution of languages due to the sparsity. With the advent of LLMstrained on big data and large models, we can now precisely assess the marginaldistribution of languages, providing a convenient means of exploring the sparsestructures in the joint distribution for effective inferences. In this paper,we categorize languages as either unambiguous or {epsilon}-ambiguous andpresent quantitative results to demonstrate that the emergent abilities ofLLMs, such as language understanding, in-context learning, chain-of-thoughtprompting, and effective instruction fine-tuning, can all be attributed toBayesian inference on the sparse joint distribution of languages.", "output": "A Latent Space Theory for Emergent Abilities in Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) have been widely used in various video analytictasks. These tasks demand real-time responses. Due to the limited processingpower on mobile devices, a common way to support such real-time analytics is tooffload the processing to an edge server. This paper examines how to speed upthe edge server DNN processing for multiple clients. In particular, we observebatching multiple DNN requests significantly speeds up the processing time.Based on this observation, we first design a novel scheduling algorithm toexploit the batching benefits of all requests that run the same DNN. This iscompelling since there are only a handful of DNNs and many requests tend to usethe same DNN. Our algorithms are general and can support different objectives,such as minimizing the completion time or maximizing the on-time ratio. We thenextend our algorithm to handle requests that use different DNNs with or withoutshared layers. Finally, we develop a collaborative approach to further improveperformance by adaptively processing some of the requests or portions of therequests locally at the clients. This is especially useful when the networkand/or server is congested. Our implementation shows the effectiveness of ourapproach under different request distributions (e.g., Poisson, Pareto, andConstant inter-arrivals).", "output": "Scheduling DNNs on Edge Servers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a new learning-based approach for approximately solvingthe Kidney-Exchange Problem (KEP), an NP-hard problem on graphs. The problemconsists of, given a pool of kidney donors and patients waiting for kidneydonations, optimally selecting a set of donations to optimize the quantity andquality of transplants performed while respecting a set of constraints aboutthe arrangement of these donations. The proposed technique consists of two mainsteps: the first is a Graph Neural Network (GNN) trained without supervision;the second is a deterministic non-learned search heuristic that uses the outputof the GNN to find paths and cycles. To allow for comparisons, we alsoimplemented and tested an exact solution method using integer programming, twogreedy search heuristics without the machine learning module, and the GNN alonewithout a heuristic. We analyze and compare the methods and conclude that thelearning-based two-stage approach is the best solution quality, outputtingapproximate solutions on average 1.1 times more valuable than the ones from thedeterministic heuristic alone.", "output": "Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to learn and predict simple functions is a key aspect of humanintelligence. Recent works have started to explore this ability usingtransformer architectures, however it remains unclear whether this issufficient to recapitulate the extrapolation abilities of people in thisdomain. Here, we propose to address this gap by augmenting the transformerarchitecture with two simple inductive learning biases, that are directlyadapted from recent models of abstract reasoning in cognitive science. Theresults we report demonstrate that these biases are helpful in the context oflarge neural network models, as well as shed light on the types of inductivelearning biases that may contribute to human abilities in extrapolation.", "output": "Beyond Transformers for Function Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We used survival analysis to quantify the impact of postdischarge evaluationand management (E/M) services in preventing hospital readmission or death. Ourapproach avoids a specific pitfall of applying machine learning to thisproblem, which is an inflated estimate of the effect of interventions, due tosurvivors bias -- where the magnitude of inflation may be conditional onheterogeneous confounders in the population. This bias arises simply because inorder to receive an intervention after discharge, a person must not have beenreadmitted in the intervening period. After deriving an expression for thisphantom effect, we controlled for this and other biases within an inherentlyinterpretable Bayesian survival framework. We identified case managementservices as being the most impactful for reducing readmissions overall,particularly for patients discharged to long term care facilities, with highresource utilization in the quarter preceding admission.", "output": "Interpretable (not just posthoc-explainable) heterogeneous survivor bias-corrected treatment effects for assignment of postdischarge interventions to prevent readmissions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Fields (NeRFs) are a very recent and very popular approachfor the problems of novel view synthesis and 3D reconstruction. A popular scenerepresentation used by NeRFs is to combine a uniform, voxel-based subdivisionof the scene with an MLP. Based on the observation that a (sparse) point cloudof the scene is often available, this paper proposes to use an adaptiverepresentation based on tetrahedra and a Delaunay representation instead of theuniform subdivision or point-based representations. We show that such arepresentation enables efficient training and leads to state-of-the-artresults. Our approach elegantly combines concepts from 3D geometry processing,triangle-based rendering, and modern neural radiance fields. Compared tovoxel-based representations, ours provides more detail around parts of thescene likely to be close to the surface. Compared to point-basedrepresentations, our approach achieves better performance.", "output": "Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents two novel deterministic initialization procedures forK-means clustering based on a modified crowding distance. The procedures, namedCKmeans and FCKmeans, use more crowded points as initial centroids.Experimental studies on multiple datasets demonstrate that the proposedapproach outperforms Kmeans and Kmeans++ in terms of clustering accuracy. Theeffectiveness of CKmeans and FCKmeans is attributed to their ability to selectbetter initial centroids based on the modified crowding distance. Overall, theproposed approach provides a promising alternative for improving K-meansclustering.", "output": "CKmeans and FCKmeans : Two Deterministic Initialization Procedures For Kmeans Algorithm Using Crowding Distance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models have become increasingly popular for flood predictiondue to their superior accuracy and efficiency compared to traditional methods.However, current machine learning methods often rely on separate spatial ortemporal feature analysis and have limitations on the types, number, anddimensions of input data. This study presented a CNN-RNN hybrid feature fusionmodelling approach for urban flood prediction, which integrated the strengthsof CNNs in processing spatial features and RNNs in analyzing differentdimensions of time sequences. This approach allowed for both static and dynamicflood predictions. Bayesian optimization was applied to identify the seven mostinfluential flood-driven factors and determine the best combination strategy.By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM,BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. Thismodel achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE were0.007, 0.025, 0.973 and 0.755, respectively) under various rainfall inputconditions. Additionally, the processing speed was significantly improved, withan inference time of 1.158s (approximately 1/125 of the traditional computationtime) compared to the physically-based models.", "output": "Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel approach to the instance selection problem in machinelearning (or data mining). Our approach is based on recent results on(proportional) representation in approval-based multi-winner elections. In ourmodel, instances play a double role as voters and candidates. Each instance inthe training set (acting as a voter) approves of the instances (playing therole of candidates) belonging to its local set (except itself), a conceptalready existing in the literature. We then select the election winners using arepresentative voting rule, and such winners are the data instances kept in thereduced training set.", "output": "Data as voters: instance selection using approval-based multi-winner voting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A key challenge in sequential decision making is optimizing systems safelyunder partial information. While much of the literature has focused on thecases of either partially known states or partially known dynamics, it isfurther exacerbated in cases where both states and dynamics are partiallyknown. Computing heparin doses for patients fits this paradigm since theconcentration of heparin in the patient cannot be measured directly and therates at which patients metabolize heparin vary greatly between individuals.While many proposed solutions are model free, they require complex models andhave difficulty ensuring safety. However, if some of the structure of thedynamics is known, a model based approach can be leveraged to provide safepolicies. In this paper we propose such a framework to address the challenge ofoptimizing personalized heparin doses. We use a predictive model parameterizedindividually by patient to predict future therapeutic effects. We then leveragethis model using a scenario generation based approach that is capable ofensuring patient safety. We validate our models with numerical experiments bycomparing the predictive capabilities of our model against existing machinelearning techniques and demonstrating how our dosing algorithm can treatpatients in a simulated ICU environment.", "output": "Model Based Reinforcement Learning for Personalized Heparin Dosing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We find that improvements in speedrunning world records follow a power lawpattern. Using this observation, we answer an outstanding question fromprevious work: How do we improve on the baseline of predicting no improvementwhen forecasting speedrunning world records out to some time horizon, such asone month? Using a random effects model, we improve on this baseline forrelative mean square error made on predicting out-of-sample world recordimprovements as the comparison metric at a $p &lt; 10^{-5}$ significance level.The same set-up improves textit{even} on the ex-post best exponential movingaverage forecasts at a $p = 0.15$ significance level while having access tosubstantially fewer data points. We demonstrate the effectiveness of thisapproach by applying it to Machine Learning benchmarks and achieving forecaststhat exceed a baseline. Finally, we interpret the resulting model to suggestthat 1) ML benchmarks are far from saturation and 2) sudden large improvementsin Machine Learning are unlikely but cannot be ruled out.", "output": "Power Law Trends in Speedrunning and Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the advances of IoT developments, copious sensor data are communicatedthrough wireless networks and create the opportunity of building Digital Twinsto mirror and simulate the complex physical world. Digital Twin has long beenbelieved to rely heavily on domain knowledge, but we argue that this leads to ahigh barrier of entry and slow development due to the scarcity and cost ofhuman experts. In this paper, we propose Digital Twin Graph (DTG), a generaldata structure associated with a processing framework that constructs digitaltwins in a fully automated and domain-agnostic manner. This work represents thefirst effort that takes a completely data-driven and (unconventional) graphlearning approach to addresses key digital twin challenges.", "output": "Digital Twin Graph: Automated Domain-Agnostic Construction, Fusion, and Simulation of IoT-Enabled World."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Introduction: The value of integrating federal HIV services data with HIVsurveillance is currently unknown. Upstream and complete case capture isessential in preventing future HIV transmission. Methods: This study integratedRyan White, Social Security Disability Insurance, Medicare, Children HealthInsurance Programs and Medicaid demographic aggregates from 2005 to 2018 forpeople living with HIV and compared them with Centers for Disease Control andPrevention HIV surveillance by demographic aggregate. Surveillance Unknown,Service Known (SUSK) candidate aggregates were identified from aggregates whereservices aggregate volumes exceeded surveillance aggregate volumes. Adistribution approach and a deep learning model series were used to identifySUSK candidate aggregates where surveillance cases exceeded services cases inaggregate. Results: Medicare had the most candidate SUSK aggregates. Medicaidmay have candidate SUSK aggregates where cases approach parity withsurveillance. Deep learning was able to detect candidate SUSK aggregates evenwhere surveillance cases exceed service cases. Conclusions: Integration of CMScase level records with HIV surveillance records can increase case discoveryand life course model quality; especially for cases who die after seeking HIVservices but before they become surveillance cases. The ethical implicationsfor both the availability and reuse of clinical HIV Data without the knowledgeand consent of the persons described remains an opportunity for the developmentof big data ethics in public health research. Future work should develop bigdata ethics to support researchers and assure their subjects that informationwhich describes them is not misused.", "output": "Too sick for surveillance: Can federal HIV service data improve federal HIV surveillance efforts?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world adversarial physical patches were shown to be successful incompromising state-of-the-art models in a variety of computer visionapplications. Existing defenses that are based on either input gradient orfeatures analysis have been compromised by recent GAN-based attacks thatgenerate naturalistic patches. In this paper, we propose Jedi, a new defenseagainst adversarial patches that is resilient to realistic patch attacks. Jeditackles the patch localization problem from an information theory perspective;leverages two new ideas: (1) it improves the identification of potential patchregions using entropy analysis: we show that the entropy of adversarial patchesis high, even in naturalistic patches; and (2) it improves the localization ofadversarial patches, using an autoencoder that is able to complete patchregions from high entropy kernels. Jedi achieves high-precision adversarialpatch localization, which we show is critical to successfully repair theimages. Since Jedi relies on an input entropy analysis, it is model-agnostic,and can be applied on pre-trained off-the-shelf models without changes to thetraining or inference of the protected models. Jedi detects on average 90% ofadversarial patches across different benchmarks and recovers up to 94% ofsuccessful patch attacks (Compared to 75% and 65% for LGS and Jujutsu,respectively).", "output": "Jedi: Entropy-based Localization and Removal of Adversarial Patches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The natural world is full of complex systems characterized by intricaterelations between their components: from social interactions betweenindividuals in a social network to electrostatic interactions between atoms ina protein. Topological Deep Learning (TDL) provides a comprehensive frameworkto process and extract knowledge from data associated with these systems, suchas predicting the social community to which an individual belongs or predictingwhether a protein can be a reasonable target for drug development. TDL hasdemonstrated theoretical and practical advantages that hold the promise ofbreaking ground in the applied sciences and beyond. However, the rapid growthof the TDL literature has also led to a lack of unification in notation andlanguage across Topological Neural Network (TNN) architectures. This presents areal obstacle for building upon existing works and for deploying TNNs to newreal-world problems. To address this issue, we provide an accessibleintroduction to TDL, and compare the recently published TNNs using a unifiedmathematical and graphical notation. Through an intuitive and critical reviewof the emerging field of TDL, we extract valuable insights into currentchallenges and exciting opportunities for future development.", "output": "Architectures of Topological Deep Learning: A Survey on Topological Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As AI agents are increasingly used in the real open world with unknowns ornovelties, they need the ability to (1) recognize objects that (i) they havelearned and (ii) detect items that they have not seen or learned before, and(2) learn the new items incrementally to become more and more knowledgeable andpowerful. (1) is called novelty detection or out-of-distribution (OOD)detection and (2) is called class incremental learning (CIL), which is asetting of continual learning (CL). In existing research, OOD detection and CILare regarded as two completely different problems. This paper theoreticallyproves that OOD detection actually is necessary for CIL. We first show that CILcan be decomposed into two sub-problems: within-task prediction (WP) andtask-id prediction (TP). We then prove that TP is correlated with OODdetection. The key theoretical result is that regardless of whether WP and OODdetection (or TP) are defined explicitly or implicitly by a CIL algorithm, goodWP and good OOD detection are necessary and sufficient conditions for good CIL,which unifies novelty or OOD detection and continual learning (CIL, inparticular). A good CIL algorithm based on our theory can naturally be used inopen world learning, which is able to perform both novelty/OOD detection andcontinual learning. Based on the theoretical result, new CIL methods are alsodesigned, which outperform strong baselines in terms of CIL accuracy and itscontinual OOD detection by a large margin.", "output": "Open-World Continual Learning: Unifying Novelty Detection and Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study proposes a deep learning model for the classification andsegmentation of brain tumors from magnetic resonance imaging (MRI) scans. Theclassification model is based on the EfficientNetB1 architecture and is trainedto classify images into four classes: meningioma, glioma, pituitary adenoma,and no tumor. The segmentation model is based on the U-Net architecture and istrained to accurately segment the tumor from the MRI images. The models areevaluated on a publicly available dataset and achieve high accuracy andsegmentation metrics, indicating their potential for clinical use in thediagnosis and treatment of brain tumors.", "output": "Brain tumor multi classification and segmentation in MRO images using deep learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently developed graph contrastive learning (GCL) approaches compare twodifferent \"views\" of the same graph in order to learn node/graphrepresentations. The core assumption of these approaches is that by graphaugmentation, it is possible to generate several structurally different butsemantically similar graph structures, and therefore, the identity labels ofthe original and augmented graph/nodes should be identical. However, in thispaper, we observe that this assumption does not always hold, for example, anyperturbation to nodes or edges in a molecular graph will change the graphlabels to some degree. Therefore, we believe that augmenting the graphstructure should be accompanied by an adaptation of the labels used for thecontrastive loss. Based on this idea, we propose ID-MixGCL, which allows forsimultaneous modulation of both the input graph and the corresponding identitylabels, with a controllable degree of change, leading to the capture offine-grained representations from unlabeled graphs. Experimental resultsdemonstrate that ID-MixGCL improves performance on graph classification andnode classification tasks, as demonstrated by significant improvements on theCora, IMDB-B, and IMDB-M datasets compared to state-of-the-art techniques, by3-29% absolute points.", "output": "ID-MixGCL: Identity Mixup for Graph Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Kernel-based modal statistical methods include mode estimation, regression,and clustering. Estimation accuracy of these methods depends on the kernel usedas well as the bandwidth. We study effect of the selection of the kernelfunction to the estimation accuracy of these methods. In particular, wetheoretically show a (multivariate) optimal kernel that minimizes itsanalytically-obtained asymptotic error criterion when using an optimalbandwidth, among a certain kernel class defined via the number of its signchanges.", "output": "Optimal Kernel for Kernel-Based Modal Statistical Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyper-parameters optimization (HPO) is vital for machine learning models.Besides model accuracy, other tuning intentions such as model training time andenergy consumption are also worthy of attention from data analytic serviceproviders. Hence, it is essential to take both model hyperparameters and systemparameters into consideration to execute cross-layer multi-objectivehyperparameter auto-tuning. Towards this challenging target, we proposeHyperTuner in this paper. To address the formulated high-dimensional black-boxmulti-objective optimization problem, HyperTuner first conducts multi-objectiveparameter importance ranking with its MOPIR algorithm and then leverages theproposed ADUMBO algorithm to find the Pareto-optimal configuration set. Duringeach iteration, ADUMBO selects the most promising configuration from thegenerated Pareto candidate set via maximizing a new well-designed metric, whichcan adaptively leverage the uncertainty as well as the predicted mean acrossall the surrogate models along with the iteration times. We evaluate HyperTuneron our local distributed TensorFlow cluster and experimental results show thatit is always able to find a better Pareto configuration front superior in bothconvergence and diversity compared with the other four baseline algorithms.Besides, experiments with different training datasets, different optimizationobjectives and different machine learning platforms verify that HyperTuner canwell adapt to various data analytic service scenarios.", "output": "HyperTuner: A Cross-Layer Multi-Objective Hyperparameter Auto-Tuning Framework for Data Analytic Services."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study an online learning algorithm with a robust lossfunction $mathcal{L}_{sigma}$ for regression over a reproducing kernelHilbert space (RKHS). The loss function $mathcal{L}_{sigma}$ involving ascaling parameter $sigma&gt;0$ can cover a wide range of commonly used robustlosses. The proposed algorithm is then a robust alternative for online leastsquares regression aiming to estimate the conditional mean function. Forproperly chosen $sigma$ and step size, we show that the last iterate of thisonline algorithm can achieve optimal capacity independent convergence in themean square distance. Moreover, if additional information on the underlyingfunction space is known, we also establish optimal capacity dependent rates forstrong convergence in RKHS. To the best of our knowledge, both of the tworesults are new to the existing literature of online learning.", "output": "Optimality of Robust Online Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work brings the leading accuracy, sample efficiency, and robustness ofdeep equivariant neural networks to the extreme computational scale. This isachieved through a combination of innovative model architecture, massiveparallelization, and models and implementations optimized for efficient GPUutilization. The resulting Allegro architecture bridges the accuracy-speedtradeoff of atomistic simulations and enables description of dynamics instructures of unprecedented complexity at quantum fidelity. To illustrate thescalability of Allegro, we perform nanoseconds-long stable simulations ofprotein dynamics and scale up to a 44-million atom structure of a complete,all-atom, explicitly solvated HIV capsid on the Perlmutter supercomputer. Wedemonstrate excellent strong scaling up to 100 million atoms and 70% weakscaling to 5120 A100 GPUs.", "output": "Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we present a novel application of detecting fruit pickeractivities based on time series data generated from wearable sensors. Duringharvesting, fruit pickers pick fruit into wearable bags and empty these bagsinto harvesting bins located in the orchard. Once full, these bins are quicklytransported to a cooled pack house to improve the shelf life of picked fruits.For farmers and managers, the knowledge of when a picker bag is emptied isimportant for managing harvesting bins more effectively to minimise the timethe picked fruit is left out in the heat (resulting in reduced shelf life). Wepropose a means to detect these bag-emptying events using human activityrecognition with wearable sensors and machine learning methods. We develop asemi-supervised approach to labelling the data. A feature-based machinelearning ensemble model and a deep recurrent convolutional neural network aredeveloped and tested on a real-world dataset. When compared, the neural networkachieves 86% detection accuracy.", "output": "Fruit Picker Activity Recognition with Wearable Sensors and Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we provide a theory of using graph neural networks (GNNs) fortextit{multi-node representation learning}, where we are interested inlearning a representation for a set of more than one node such as a link.Existing GNNs are mainly designed to learn single-node representations. When wewant to learn a node-set representation involving multiple nodes, a commonpractice in previous works is to directly aggregate the single-noderepresentations obtained by a GNN. In this paper, we show a fundamentallimitation of such an approach, namely the inability to capture the dependenceamong multiple nodes in a node set, and argue that directly aggregatingindividual node representations fails to produce an effective jointrepresentation for multiple nodes. A straightforward solution is to distinguishtarget nodes from others. Formalizing this idea, we propose text{labelingtrick}, which first labels nodes in the graph according to their relationshipswith the target node set before applying a GNN and then aggregates noderepresentations obtained in the labeled graph for multi-node representations.The labeling trick also unifies a few previous successful works for multi-noderepresentation learning, including SEAL, Distance Encoding, ID-GNN, and NBFNet.Besides node sets in graphs, we also extend labeling tricks to posets, subsetsand hypergraphs. Experiments verify that the labeling trick technique can boostGNNs on various tasks, including undirected link prediction, directed linkprediction, hyperedge prediction, and subgraph prediction. Our work explainsthe superior performance of previous node-labeling-based methods andestablishes a theoretical foundation for using GNNs for multi-noderepresentation learning.", "output": "Improving Graph Neural Networks on Multi-node Tasks with Labeling Tricks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic graph representation learning is growing as a trending yetchallenging research task owing to the widespread demand for graph dataanalysis in real world applications. Despite the encouraging performance ofmany recent works that build upon recurrent neural networks (RNNs) and graphneural networks (GNNs), they fail to explicitly model the impact of edgetemporal states on node features over time slices. Additionally, they arechallenging to extract global structural features because of the inherentover-smoothing disadvantage of GNNs, which further restricts the performance.In this paper, we propose a recurrent difference graph transformer (RDGT)framework, which firstly assigns the edges in each snapshot with various typesand weights to illustrate their specific temporal states explicitly, then astructure-reinforced graph transformer is employed to capture the temporal noderepresentations by a recurrent learning paradigm. Experimental results on fourreal-world datasets demonstrate the superiority of RDGT for discrete dynamicgraph representation learning, as it consistently outperforms competing methodsin dynamic link prediction tasks.", "output": "Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While deep reinforcement learning has shown important empirical success, ittends to learn relatively slow due to slow propagation of rewards informationand slow update of parametric neural networks. Non-parametric episodic memory,on the other hand, provides a faster learning alternative that does not requirerepresentation learning and uses maximum episodic return as state-action valuesfor action selection. Episodic memory and reinforcement learning both havetheir own strengths and weaknesses. Notably, humans can leverage multiplememory systems concurrently during learning and benefit from all of them. Inthis work, we propose a method called Two-Memory reinforcement learning agent(2M) that combines episodic memory and reinforcement learning to distill bothof their strengths. The 2M agent exploits the speed of the episodic memory partand the optimality and the generalization capacity of the reinforcementlearning part to complement each other. Our experiments demonstrate that the 2Magent is more data efficient and outperforms both pure episodic memory and purereinforcement learning, as well as a state-of-the-art memory-augmented RLagent. Moreover, the proposed approach provides a general framework that can beused to combine any episodic memory agent with other off-policy reinforcementlearning algorithms.", "output": "Two-Memory Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning has attracted increasing attention due to the promise ofbalancing privacy and large-scale learning; numerous approaches have beenproposed. However, most existing approaches focus on problems with balanceddata, and prediction performance is far from satisfactory for many real-worldapplications where the number of samples in different classes is highlyimbalanced. To address this challenging problem, we developed a novel federatedlearning method for imbalanced data by directly optimizing the area under curve(AUC) score. In particular, we formulate the AUC maximization problem as afederated compositional minimax optimization problem, develop a localstochastic compositional gradient descent ascent with momentum algorithm, andprovide bounds on the computational and communication complexities of ouralgorithm. To the best of our knowledge, this is the first work to achieve suchfavorable theoretical results. Finally, extensive experimental results confirmthe efficacy of our method.", "output": "Federated Compositional Deep AUC Maximization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although procurement fraud is always a critical problem in almost every freemarket, audit departments still have a strong reliance on reporting frominformed sources when detecting them. With our generous cooperator, SF Express,sharing the access to the database related with procurements took place from2015 to 2017 in their company, our team studies how machine learning techniquescould help with the audition of one of the most profound crime among currentchinese market, namely procurement frauds. By representing each procurementevent as 9 specific features, we construct neural network models to identifysuspicious procurements and classify their fraud types. Through testing ourmodels over 50000 samples collected from the procurement database, we haveproven that such models -- despite having space for improvements -- are usefulin detecting procurement frauds.", "output": "Automatic Procurement Fraud Detection with Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Asymmetrical multiplayer (AMP) game is a popular game genre which involvesmultiple types of agents competing or collaborating with each other in thegame. It is difficult to train powerful agents that can defeat top humanplayers in AMP games by typical self-play training method because ofunbalancing characteristics in their asymmetrical environments. We proposeasymmetric-evolution training (AET), a novel multi-agent reinforcement learningframework that can train multiple kinds of agents simultaneously in AMP game.We designed adaptive data adjustment (ADA) and environment randomization (ER)to optimize the AET process. We tested our method in a complex AMP game namedTom &amp; Jerry, and our AIs trained without using any human data can achieve awin rate of 98.5% against top human players over 65 matches. The ablationexperiments indicated that the proposed modules are beneficial to theframework.", "output": "Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNN) suffer from severe inefficiency. It is mainlycaused by the exponential growth of node dependency with the increase oflayers. It extremely limits the application of stochastic optimizationalgorithms so that the training of GNN is usually time-consuming. To addressthis problem, we propose to decouple a multi-layer GNN as multiple simplemodules for more efficient training, which is comprised of classical forwardtraining (FT)and designed backward training (BT). Under the proposed framework,each module can be trained efficiently in FT by stochastic algorithms withoutdistortion of graph information owing to its simplicity. To avoid the onlyunidirectional information delivery of FT and sufficiently train shallowmodules with the deeper ones, we develop a backward training mechanism thatmakes the former modules perceive the latter modules. The backward trainingintroduces the reversed information delivery into the decoupled modules as wellas the forward information delivery. To investigate how the decoupling andgreedy training affect the representational capacity, we theoretically provethat the error produced by linear modules will not accumulate on unsupervisedtasks in most cases. The theoretical and experimental results show that theproposed framework is highly efficient with reasonable performance.", "output": "Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale pre-trained models have achieved remarkable success in a varietyof scenarios and applications, but how to leverage them to improve theprediction reliability of downstream models is undesirably under-explored.Moreover, modern neural networks have been found to be poorly calibrated andmake overconfident predictions regardless of inherent sample difficulty anddata uncertainty. To address this issue, we propose to utilize large-scalepre-trained models to guide downstream model training with sampledifficulty-aware entropy regularization. Pre-trained models that have beenexposed to large-scale datasets and do not overfit the downstream trainingclasses enable us to measure each training sample difficulty via feature-spaceGaussian modeling and relative Mahalanobis distance computation. Importantly,by adaptively penalizing overconfident prediction based on the sample'sdifficulty, we simultaneously improve accuracy and uncertainty calibration onvarious challenging benchmarks, consistently surpassing competitive baselinesfor reliable prediction.", "output": "Learning Sample Difficulty from Pre-trained Models for Reliable Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The K Nearest Neighbors (KNN) classifier is widely used in many fields suchas fingerprint-based localization or medicine. It determines the classmembership of unlabelled sample based on the class memberships of the Klabelled samples, the so-called nearest neighbors, that are closest to theunlabelled sample. The choice of K has been the topic of various studies andproposed KNN-variants. Yet no variant has been proven to outperform all othervariants. In this paper a new KNN-variant is proposed which ensures that the Knearest neighbors are indeed close to the unlabelled sample and finds K alongthe way. The proposed algorithm is tested and compared to the standard KNN intheoretical scenarios and for indoor localization based on ion-mobilityspectrometry fingerprints. It achieves a higher classification accuracy thanthe KNN in the tests, while requiring having the same computational demand.", "output": "Flexible K Nearest Neighbors Classifier: Derivation and Application for Ion-mobility Spectrometry-based Indoor Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum computation has a strong implication for advancing the currentlimitation of machine learning algorithms to deal with higher data dimensionsor reducing the overall training parameters for a deep neural network model.Based on a gate-based quantum computer, a parameterized quantum circuit wasdesigned to solve a model-free reinforcement learning problem with the deep-Qlearning method. This research has investigated and evaluated its potential.Therefore, a novel PQC based on the latest Qiskit and PyTorch framework wasdesigned and trained to compare with a full-classical deep neural network withand without integrated PQC. At the end of the research, the research draws itsconclusion and prospects on developing deep quantum learning in solving a mazeproblem or other reinforcement learning problems.", "output": "Deep Reinforcement Learning Using Hybrid Quantum Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several algorithms for learning the structure of dynamic Bayesian networks(DBNs) require an a priori ordering of variables, which influences thedetermined graph topology. However, it is often unclear how to determine thisorder if feature importance is unknown, especially as an exhaustive search isusually impractical. In this paper, we introduce Ranking Approaches for UnknownStructures (RAUS), an automated framework to systematically inform variableordering and learn networks end-to-end. RAUS leverages existing statisticalmethods (Cramers V, chi-squared test, and information gain) to compare variableordering, resultant generated network topologies, and DBN performance. RAUSenables end-users with limited DBN expertise to implement models via commandline interface. We evaluate RAUS on the task of predicting impending acutekidney injury (AKI) from inpatient clinical laboratory data. Longitudinalobservations from 67,460 patients were collected from our electronic healthrecord (EHR) and Kidney Disease Improving Global Outcomes (KDIGO) criteria werethen applied to define AKI events. RAUS learns multiple DBNs simultaneously topredict a future AKI event at different time points (i.e., 24-, 48-, 72-hoursin advance of AKI). We also compared the results of the learned AKI predictionmodels and variable orderings to baseline techniques (logistic regression,random forests, and extreme gradient boosting). The DBNs generated by RAUSachieved 73-83% area under the receiver operating characteristic curve (AUCROC)within 24-hours before AKI; and 71-79% AUCROC within 48-hours before AKI of anystage in a 7-day observation window. Insights from this automated framework canhelp efficiently implement and interpret DBNs for clinical decision support.The source code for RAUS is available in GitHub at .", "output": "Automated Dynamic Bayesian Networks for Predicting Acute Kidney Injury Before Onset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Questions remain on the robustness of data-driven learning methods whencrossing the gap from simulation to reality. We utilize weight anchoring, amethod known from continual learning, to cultivate and fixate desired behaviorin Neural Networks. Weight anchoring may be used to find a solution to alearning problem that is nearby the solution of another learning problem.Thereby, learning can be carried out in optimal environments without neglectingor unlearning desired behavior. We demonstrate this approach on the example oflearning mixed QoS-efficient discrete resource scheduling with infrequentpriority messages. Results show that this method provides performancecomparable to the state of the art of augmenting a simulation environment,alongside significantly increased robustness and steerability.", "output": "Robust Deep Reinforcement Learning Scheduling via Weight Anchoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning aims to learn on non-stationary data streams withoutcatastrophically forgetting previous knowledge. Prevalent replay-based methodsaddress this challenge by rehearsing on a small buffer holding the seen data,for which a delicate sample selection strategy is required. However, existingselection schemes typically seek only to maximize the utility of the ongoingselection, overlooking the interference between successive rounds of selection.Motivated by this, we dissect the interaction of sequential selection stepswithin a framework built on influence functions. We manage to identify a newclass of second-order influences that will gradually amplify incidental bias inthe replay buffer and compromise the selection process. To regularize thesecond-order effects, a novel selection objective is proposed, which also hasclear connections to two widely adopted criteria. Furthermore, we present anefficient implementation for optimizing the proposed criterion. Experiments onmultiple continual learning benchmarks demonstrate the advantage of ourapproach over state-of-the-art methods. Code is available at", "output": "Regularizing Second-Order Influences for Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Industry 4.0 is a blend of the hyper-connected digital industry within twoworld of Information Technology (IT) and Operational Technology (OT). With thisamalgamate opportunity, smart manufacturing involves production assets with themanufacturing equipment having its own intelligence, while the system-wideintelligence is provided by the cyber layer. However Smart manufacturing nowbecomes one of the prime targets of cyber threats due to vulnerabilities in theexisting process of operation. Since smart manufacturing covers a vast area ofproduction industries from cyber physical system to additive manufacturing, toautonomous vehicles, to cloud based IIoT (Industrial IoT), to roboticproduction, cyber threat stands out with this regard questioning about how toconnect manufacturing resources by network, how to integrate a whole processchain for a factory production etc. Cybersecurity confidentiality, integrityand availability expose their essential existence for the proper operationalthread model known as digital thread ensuring secure manufacturing. In thiswork, a literature survey is presented from the existing threat models, attackvectors and future challenges over the digital thread of smart manufacturing.", "output": "Cyber Security in Smart Manufacturing (Threats, Landscapes Challenges)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spiking neural networks (SNNs) have gained attention as models of sparse andevent-driven communication of biological neurons, and as such have shownincreasing promise for energy-efficient applications in neuromorphic hardware.As with classical artificial neural networks (ANNs), predictive uncertaintiesare important for decision making in high-stakes applications, such asautonomous vehicles, medical diagnosis, and high frequency trading. Yet,discussion of uncertainty estimation in SNNs is limited, and approaches foruncertainty estimation in artificial neural networks (ANNs) are not directlyapplicable to SNNs. Here, we propose an efficient Monte Carlo(MC)-dropout basedapproach for uncertainty estimation in SNNs. Our approach exploits thetime-step mechanism of SNNs to enable MC-dropout in a computationally efficientmanner, without introducing significant overheads during training and inferencewhile demonstrating high accuracy and uncertainty quality.", "output": "Efficient Uncertainty Estimation in Spiking Neural Networks via MC-dropout."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As operating frequencies and clock speeds in processors have increased overthe years, interconnects affect both the reliability and performance of entireelectronic systems. Fault detection and diagnosis of the interconnects arecrucial for prognostics and health management (PHM) of electronics. However,existing research works utilizing electrical signals as prognostic factors havelimitations, such as the inability to distinguish the root cause of defects,which eventually requires additional destructive evaluation, and vulnerabilityto noise that results in a false alarm. Herein, we realize the non-destructivedetection and diagnosis of defects in Cu interconnects, achieving earlydetection, high diagnostic accuracy, and noise robustness. To the best of ourknowledge, this study first simultaneously analyzes the root cause and severityusing electrical signal patterns. In this paper, we experimentally show thatS-parameter patterns have the ability for fault diagnosis and they areeffective input data for learning algorithms. Furthermore, we propose a novelseverity rating ensemble learning (SREL) approach to enhance diagnosticaccuracy and noise-robustness. Our method, with a maximum accuracy of 99.3%,outperforms conventional machine learning and multi-class convolutional neuralnetworks (CNN) as additional noise levels increase.", "output": "SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress in deep learning, a special form of machine learning, has ledto remarkable capabilities machines can now be endowed with: they can read andunderstand free flowing text, reason and bargain with human counterparts,translate texts between languages, learn how to take decisions to maximizecertain outcomes, etc. Today, machines have revolutionized the detection ofcancer, the prediction of protein structures, the design of drugs, the controlof nuclear fusion reactors etc. Although these capabilities are still in theirinfancy, it seems clear that their continued refinement and application willresult in a technological impact on nearly all social and economic areas ofhuman activity, the likes of which we have not seen before. In this article, Iwill share my view as to how AI will likely impact asset management in generaland I will provide a mental framework that will equip readers with a simplecriterion to assess whether and to what degree a given fund really exploitsdeep learning and whether a large disruption risk from deep learning exist.", "output": "The impact of the AI revolution on asset management."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mammographic image analysis is a fundamental problem in the computer-aideddiagnosis scheme, which has recently made remarkable progress with the advanceof deep learning. However, the construction of a deep learning model requirestraining data that are large and sufficiently diverse in terms of image styleand quality. In particular, the diversity of image style may be majorlyattributed to the vendor factor. However, mammogram collection from vendors asmany as possible is very expensive and sometimes impractical forlaboratory-scale studies. Accordingly, to further augment the generalizationcapability of deep learning models to various vendors with limited resources, anew contrastive learning scheme is developed. Specifically, the backbonenetwork is firstly trained with a multi-style and multi-view unsupervisedself-learning scheme for the embedding of invariant features to various vendorstyles. Afterward, the backbone network is then recalibrated to the downstreamtasks of mass detection, multi-view mass matching, BI-RADS classification andbreast density classification with specific supervised learning. The proposedmethod is evaluated with mammograms from four vendors and two unseen publicdatasets. The experimental results suggest that our approach can effectivelyimprove analysis performance on both seen and unseen domains, and outperformsmany state-of-the-art (SOTA) generalization methods.", "output": "Domain Generalization for Mammographic Image Analysis via Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the recent rise of neural operators, scientific machine learning offersnew solutions to quantify uncertainties associated with high-fidelity numericalsimulations. Traditional neural networks, such as Convolutional Neural Networks(CNN) or Physics-Informed Neural Networks (PINN), are restricted to theprediction of solutions in a predefined configuration. With neural operators,one can learn the general solution of Partial Differential Equations, such asthe elastic wave equation, with varying parameters. There have been very fewapplications of neural operators in seismology. All of them were limited totwo-dimensional settings, although the importance of three-dimensional (3D)effects is well known.In this work, we apply the Fourier Neural Operator (FNO) to predict groundmotion time series from a 3D geological description. We used a high-fidelitysimulation code, SEM3D, to build an extensive database of ground motionsgenerated by 30,000 different geologies. With this database, we show that theFNO can produce accurate ground motion even when the underlying geologyexhibits large heterogeneities. Intensity measures at moderate and largeperiods are especially well reproduced.We present the first seismological application of Fourier Neural Operators in3D. Thanks to the generalizability of our database, we believe that our modelcan be used to assess the influence of geological features such as sedimentarybasins on ground motion, which is paramount to evaluating site effects.", "output": "Fourier Neural Operator Surrogate Model to Predict 3D Seismic Waves Propagation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Partially-observable problems pose a trade-off between reducing costs andgathering information. They can be solved optimally by planning in beliefspace, but that is often prohibitively expensive. Model-predictive control(MPC) takes the alternative approach of using a state estimator to form abelief over the state, and then plan in state space. This ignores potentialfuture observations during planning and, as a result, cannot actively increaseor preserve the certainty of its own state estimate. We find a middle-groundbetween planning in belief space and completely ignoring its dynamics by onlyreasoning about its future accuracy. Our approach, filter-aware MPC, penalisesthe loss of information by what we call \"trackability\", the expected error ofthe state estimator. We show that model-based simulation allows condensingtrackability into a neural network, which allows fast planning. In experimentsinvolving visual navigation, realistic every-day environments and a two-linkrobot arm, we show that filter-aware MPC vastly improves regular MPC.", "output": "Filter-Aware Model-Predictive Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies the deflation algorithm when applied to estimate alow-rank symmetric spike contained in a large tensor corrupted by additiveGaussian noise. Specifically, we provide a precise characterization of thelarge-dimensional performance of deflation in terms of the alignments of thevectors obtained by successive rank-1 approximation and of their estimatedweights, assuming non-trivial (fixed) correlations among spike components. Ouranalysis allows an understanding of the deflation mechanism in the presence ofnoise and can be exploited for designing more efficient signal estimationmethods.", "output": "Hotelling Deflation on Large Symmetric Spiked Tensors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As growing usage of social media websites in the recent decades, the amountof news articles spreading online rapidly, resulting in an unprecedented scaleof potentially fraudulent information. Although a plenty of studies haveapplied the supervised machine learning approaches to detect such content, thelack of gold standard training data has hindered the development. Analysing thesingle data format, either fake text description or fake image, is themainstream direction for the current research. However, the misinformation inreal-world scenario is commonly formed as a text-image pair where the newsarticle/news title is described as text content, and usually followed by therelated image. Given the strong ability of learning features without labelleddata, contrastive learning, as a self-learning approach, has emerged andachieved success on the computer vision. In this paper, our goal is to explorethe constrastive learning in the domain of misinformation identification. Wedeveloped a self-learning model and carried out the comprehensive experimentson a public data set named COSMOS. Comparing to the baseline classifier, ourmodel shows the superior performance of non-matched image-text pair detection(approximately 10%) when the training data is insufficient. In addition, weobserved the stability for contrsative learning and suggested the use of itoffers large reductions in the number of training data, whilst maintainingcomparable classification results.", "output": "Harnessing the Power of Text-image Contrastive Models for Automatic Detection of Online Misinformation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Precipitation forecasts are less accurate compared to other meteorologicalfields because several key processes affecting precipitation distribution andintensity occur below the resolved scale of global weather prediction models.This requires to use higher resolution simulations. To generate an uncertaintyprediction associated with the forecast, ensembles of simulations are runsimultaneously. However, the computational cost is a limiting factor here.Thus, instead of generating an ensemble system from simulations there is atrend of using neural networks. Unfortunately the data for high resolutionensemble runs is not available. We propose a new approach to generatingensemble weather predictions for high-resolution precipitation withoutrequiring high-resolution training data. The method uses generative adversarialnetworks to learn the complex patterns of precipitation and produce diverse andrealistic precipitation fields, allowing to generate realistic precipitationensemble members using only the available control forecast. We demonstrate thefeasibility of generating realistic precipitation ensemble members on unseenhigher resolutions. We use evaluation metrics such as RMSE, CRPS, rankhistogram and ROC curves to demonstrate that our generated ensemble is almostidentical to the ECMWF IFS ensemble.", "output": "Towards replacing precipitation ensemble predictions systems using machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models excel at generating photorealistic images from text-queries.Naturally, many approaches have been proposed to use these generative abilitiesto augment training datasets for downstream tasks, such as classification.However, diffusion models are themselves trained on large noisily supervised,but nonetheless, annotated datasets. It is an open question whether thegeneralization capabilities of diffusion models beyond using the additionaldata of the pre-training process for augmentation lead to improved downstreamperformance. We perform a systematic evaluation of existing methods to generateimages from diffusion models and study new extensions to assess their benefitfor data augmentation. While we find that personalizing diffusion modelstowards the target data outperforms simpler prompting strategies, we also showthat using the training data of the diffusion model alone, via a simple nearestneighbor retrieval procedure, leads to even stronger downstream performance.Overall, our study probes the limitations of diffusion models for dataaugmentation but also highlights its potential in generating new training datato improve performance on simple downstream vision tasks.", "output": "A data augmentation perspective on diffusion models and retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent rise in popularity of Hyperparameter Optimization (HPO) for deeplearning has highlighted the role that good hyperparameter (HP) space designcan play in training strong models. In turn, designing a good HP space iscritically dependent on understanding the role of different HPs. This motivatesresearch on HP Importance (HPI), e.g., with the popular method of functionalANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable tothe subspaces most relevant to algorithm designers, such as those defined bytop performance. To overcome this problem, we derive a novel formulation off-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearsondivergence (PED) to enable a closed-form computation of HPI. We demonstratethat this new algorithm, dubbed PED-ANOVA, is able to successfully identifyimportant HPs in different subspaces while also being extremely computationallyefficient.", "output": "PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deaf individuals confront significant communication obstacles on a dailybasis. Their inability to hear makes it difficult for them to communicate withthose who do not understand sign language. Moreover, it presents difficultiesin educational, occupational, and social contexts. By providing alternativecommunication channels, technology can play a crucial role in overcoming theseobstacles. One such technology that can facilitate communication between deafand hearing individuals is sign language recognition. We will create a robustsystem for sign language recognition in order to convert Indian Sign Languageto text or speech. We will evaluate the proposed system and compare CNN andLSTM models. Since there are both static and gesture sign languages, a robustmodel is required to distinguish between them. In this study, we discoveredthat a CNN model captures letters and characters for recognition of static signlanguage better than an LSTM model, but it outperforms CNN by monitoring hands,faces, and pose in gesture sign language phrases and sentences. The creation ofa text-to-sign language paradigm is essential since it will enhance the signlanguage-dependent deaf and hard-of-hearing population's communication skills.Even though the sign-to-text translation is just one side of communication, notall deaf or hard-of-hearing people are proficient in reading or writing text.Some may have difficulty comprehending written language due to educational orliteracy issues. Therefore, a text-to-sign language paradigm would allow themto comprehend text-based information and participate in a variety of social,educational, and professional settings.Keywords: deaf and hard-of-hearing, DHH, Indian sign language, CNN, LSTM,static and gesture sign languages, text-to-sign language model, MediaPipeHolistic, sign language recognition, SLR, SLT", "output": "Indian Sign Language Recognition Using Mediapipe Holistic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain-adaptive trajectory imitation is a skill that some predators learn forsurvival, by mapping dynamic information from one domain (their speed andsteering direction) to a different domain (current position of the movingprey). An intelligent agent with this skill could be exploited for a diversityof tasks, including the recognition of abnormal motion in traffic once it haslearned to imitate representative trajectories. Towards this direction, wepropose DATI, a deep reinforcement learning agent designed for domain-adaptivetrajectory imitation using a cycle-consistent generative adversarial method.Our experiments on a variety of synthetic families of reference trajectoriesshow that DATI outperforms baseline methods for imitation learning and optimalcontrol in this setting, keeping the same per-task hyperparameters. Itsgeneralization to a real-world scenario is shown through the discovery ofabnormal motion patterns in maritime traffic, opening the door for the use ofdeep reinforcement learning methods for spatially-unconstrained trajectory datamining.", "output": "Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The paper proposes the use of structured neural networks for reinforcementlearning based nonlinear adaptive control. The focus is on partially observablesystems, with separate neural networks for the state and feedforward observerand the state feedback and feedforward controller. The observer dynamics aremodelled by recurrent neural networks while a standard network is used for thecontroller. As discussed in the paper, this leads to a separation of theobserver dynamics to the recurrent neural network part, and the state feedbackto the feedback and feedforward network. The structured approach reduces thecomputational complexity and gives the reinforcement learning based controlleran {em understandable} structure as compared to when one single neural networkis used. As shown by simulation the proposed structure has the additional andmain advantage that the training becomes significantly faster. Two ways toinclude feedforward structure are presented, one related to state feedbackcontrol and one related to classical feedforward control. The latter methodintroduces further structure with a separate recurrent neural network thatprocesses only the measured disturbance. When evaluated with simulation on anonlinear cascaded double tank process, the method with most structure performsthe best, with excellent feedforward disturbance rejection gains.", "output": "Observer-Feedback-Feedforward Controller Structures in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There has recently been an increased interest in reinforcement learning fornonlinear control problems. However standard reinforcement learning algorithmscan often struggle even on seemingly simple set-point control problems. Thispaper argues that three ideas can improve reinforcement learning methods evenfor highly nonlinear set-point control problems: 1) Make use of a priorfeedback controller to aid amplitude exploration. 2) Use integrated errors. 3)Train on model ensembles. Together these ideas lead to more efficient training,and a trained set-point controller that is more robust to modelling errors andthus can be directly deployed to real-world nonlinear systems. The claim issupported by experiments with a real-world nonlinear cascaded tank process anda simulated strongly nonlinear pH-control system.", "output": "Robust nonlinear set-point control with reinforcement learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trust is a crucial factor affecting the adoption of machine learning (ML)models. Qualitative studies have revealed that end-users, particularly in themedical domain, need models that can express their uncertainty indecision-making allowing users to know when to ignore the model'srecommendations. However, existing approaches for quantifying decision-makinguncertainty are not model-agnostic, or they rely on complex statisticalderivations that are not easily understood by laypersons or end-users, makingthem less useful for explaining the model's decision-making process. This workproposes a set of class-independent meta-heuristics that can characterize thecomplexity of an instance in terms of factors are mutually relevant to bothhuman and ML decision-making. The measures are integrated into a meta-learningframework that estimates the risk of misclassification. The proposed frameworkoutperformed predicted probabilities in identifying instances at risk of beingmisclassified. The proposed measures and framework hold promise for improvingmodel development for more complex instances, as well as providing a new meansof model abstention and explanation.", "output": "A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While reinforcement learning has made great improvements, state-of-the-artalgorithms can still struggle with seemingly simple set-point feedback controlproblems. One reason for this is that the learned controller may not be able toexcite the system dynamics well enough initially, and therefore it can take along time to get data that is informative enough to learn for good control. Thepaper contributes by augmentation of reinforcement learning with a simpleguiding feedback controller, for example, a proportional controller. The keyadvantage in set point control is a much improved excitation that improves theconvergence properties of the reinforcement learning controller significantly.This can be very important in real-world control where quick and accurateconvergence is needed. The proposed method is evaluated with simulation and ona real-world double tank process with promising results.", "output": "Aiding reinforcement learning for set point control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Foundation models are large machine learning models that can tackle variousdownstream tasks once trained on diverse and large-scale data, leading researchtrends in natural language processing, computer vision, and reinforcementlearning. However, no foundation model exists for optical multilayer thin filmstructure inverse design. Current inverse design algorithms either fail toexplore the global design space or suffer from low computational efficiency. Tobridge this gap, we propose the Opto Generative Pretrained Transformer(OptoGPT). OptoGPT is a decoder-only transformer that auto-regressivelygenerates designs based on specific spectrum targets. Trained on a largedataset of 10 million designs, our model demonstrates remarkable capabilities:1) autonomous global design exploration by determining the number of layers (upto 20) while selecting the material (up to 18 distinct types) and thickness ateach layer, 2) efficient designs for structural color, absorbers, filters,distributed brag reflectors, and Fabry-Perot resonators within 0.1 seconds(comparable to simulation speeds), 3) the ability to output diverse designs,and 4) seamless integration of user-defined constraints. By overcoming designbarriers regarding optical targets, material selections, and designconstraints, OptoGPT can serve as a foundation model for optical multilayerthin film structure inverse design.", "output": "OptoGPT: A Foundation Model for Inverse Design in Optical Multilayer Thin Film Structures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot relation reasoning on knowledge graphs (FS-KGR) aims to inferlong-tail data-poor relations, which has drawn increasing attention these yearsdue to its practicalities. The pre-training of previous methods needs tomanually construct the meta-relation set, leading to numerous labor costs.Self-supervised learning (SSL) is treated as a solution to tackle the issue,but still at an early stage for FS-KGR task. Moreover, most of the existingmethods ignore leveraging the beneficial information from aliasing relations(AR), i.e., data-rich relations with similar contextual semantics to the targetdata-poor relation. Therefore, we proposed a novel Self-Supervised Learningmodel by leveraging Aliasing Relations to assist FS-KGR, termed SARF.Concretely, four main components are designed in our model, i.e., SSL reasoningmodule, AR-assisted mechanism, fusion module, and scoring function. We firstgenerate the representation of the co-occurrence patterns in a generativemanner. Meanwhile, the representations of aliasing relations are learned toenhance reasoning in the AR-assist mechanism. Besides, multiple strategies,i.e., simple summation and learnable fusion, are offered for representationfusion. Finally, the generated representation is used for scoring. Extensiveexperiments on three few-shot benchmarks demonstrate that SARF achievesstate-of-the-art performance compared with other methods in most cases.", "output": "SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study new types of dynamic allocation problems the {sl Halting Bandit}models. As an application, we obtain new proofs for the classic Gittins indexdecomposition result and recent results of the authors in `Multi-armed banditsunder general depreciation and commitment.'", "output": "Optimal Activation of Halting Multi-Armed Bandit Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Investments in movie production are associated with a high level of risk asmovie revenues have long-tailed and bimodal distributions. Accurate predictionof box-office revenue may mitigate the uncertainty and encourage investment.However, learning effective representations for actors, directors, anduser-generated content-related keywords remains a challenging open problem. Inthis work, we investigate the effects of self-supervised pretraining andpropose visual grounding of content keywords in objects from movie posters as apertaining objective. Experiments on a large dataset of 35,794 moviesdemonstrate significant benefits of self-supervised training and visualgrounding. In particular, visual grounding pretraining substantially improveslearning on movies with content keywords and achieves 14.5% relativeperformance gains compared to a finetuned BERT model with identicalarchitecture.", "output": "Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a second order gradient based method with ADAM and RMSprop for thetraining of generative adversarial networks. The proposed method is fastest toobtain similar accuracy when compared to prominent second order methods. Unlikestate-of-the-art recent methods, it does not require solving a linear system,or it does not require additional mixed second derivative terms. We derive thefixed point iteration corresponding to proposed method, and show that theproposed method is convergent. The proposed method produces better orcomparable inception scores, and comparable quality of images compared to otherrecently proposed state-of-the-art second order methods. Compared to firstorder methods such as ADAM, it produces significantly better inception scores.The proposed method is compared and validated on popular datasets such as FFHQ,LSUN, CIFAR10, MNIST, and Fashion MNIST for image generationtasksfootnote{Accepted in IJCNN 2023}. Codes:url{", "output": "Adaptive Consensus Optimization Method for GANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cellular coverage quality estimation has been a critical task forself-organized networks. In real-world scenarios, deep-learning-poweredcoverage quality estimation methods cannot scale up to large areas due tolittle ground truth can be provided during network design &amp; optimization. Inaddition they fall short in produce expressive embeddings to adequately capturethe variations of the cells' configurations. To deal with this challenge, weformulate the task in a graph representation and so that we can applystate-of-the-art graph neural networks, that show exemplary performance. Wepropose a novel training framework that can both produce quality cellconfiguration embeddings for estimating multiple KPIs, while we show it iscapable of generalising to large (area-wide) scenarios given very few labeledcells. We show that our framework yields comparable accuracy with models thathave been trained using massively labeled samples.", "output": "Learning Cellular Coverage from Real Network Configurations using GNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In symbolic regression, the goal is to find an analytical expression thataccurately fits experimental data with the minimal use of mathematical symbolssuch as operators, variables, and constants. However, the combinatorial spaceof possible expressions can make it challenging for traditional evolutionaryalgorithms to find the correct expression in a reasonable amount of time. Toaddress this issue, Neural Symbolic Regression (NSR) algorithms have beendeveloped that can quickly identify patterns in the data and generateanalytical expressions. However, these methods, in their current form, lack thecapability to incorporate user-defined prior knowledge, which is often requiredin natural sciences and engineering fields. To overcome this limitation, wepropose a novel neural symbolic regression method, named Neural SymbolicRegression with Hypothesis (NSRwH) that enables the explicit incorporation ofassumptions about the expected structure of the ground-truth expression intothe prediction process. Our experiments demonstrate that the proposedconditioned deep learning model outperforms its unconditioned counterparts interms of accuracy while also providing control over the predicted expressionstructure.", "output": "Controllable Neural Symbolic Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A nuclear reactor based on MIT BEAVRS benchmark was used as a typical powergenerating Pressurized Water Reactor (PWR). The PARCS v3.2 nodal-diffusion coresimulator was used as a full-core reactor physics solver to emulate theoperation of a reactor and to generate training, and validation data for theANN. The ANN was implemented with dedicated Python 3.8 code with Google'sTensorFlow 2.0 library. The effort was based to a large extent on the processof appropriate automatic transformation of data generated by PARCS simulator,which was later used in the process of the ANN development. Various methodsthat allow obtaining better accuracy of the ANN predicted results were studied,such as trying different ANN architectures to find the optimal number ofneurons in the hidden layers of the network. Results were later compared withthe architectures proposed in the literature. For the selected bestarchitecture predictions were made for different core parameters and theirdependence on core loading patterns. In this study, a special focus was put onthe prediction of the fuel cycle length for a given core loading pattern, as itcan be considered one of the targets for plant economic operation. Forinstance, the length of a single fuel cycle depending on the initial coreloading pattern was predicted with very good accuracy (&gt;99%). This workcontributes to the exploration of the usefulness of neural networks in solvingnuclear reactor design problems. Thanks to the application of ANN, designerscan avoid using an excessive amount of core simulator runs and more rapidlyexplore the space of possible solutions before performing more detailed designconsiderations.", "output": "Prediction of the evolution of the nuclear reactor core parameters using artificial neural network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a model-free reinforcement learning architecture, calleddistributed attentional actor architecture after conditional attention (DA6-X),to provide better interpretability of conditional coordinated behaviors. Theunderlying principle involves reusing the saliency vector, which represents theconditional states of the environment, such as the global position of agents.Hence, agents with DA6-X flexibility built into their policy exhibit superiorperformance by considering the additional information in the conditional statesduring the decision-making process. The effectiveness of the proposed methodwas experimentally evaluated by comparing it with conventional methods in anobjects collection game. By visualizing the attention weights from DA6-X, weconfirmed that agents successfully learn situation-dependent coordinatedbehaviors by correctly identifying various conditional states, leading toimproved interpretability of agents along with superior performance.", "output": "Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A framework to learn a multi-modal distribution is proposed, denoted as theConditional Quantum Generative Adversarial Network (C-qGAN). The neural networkstructure is strictly within a quantum circuit and, as a consequence, is shownto represents a more efficient state preparation procedure than currentmethods. This methodology has the potential to speed-up algorithms, such asMonte Carlo analysis. In particular, after demonstrating the effectiveness ofthe network in the learning task, the technique is applied to price Asianoption derivatives, providing the foundation for further research on otherpath-dependent options.", "output": "Conditional Generative Models for Learning Stochastic Processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) have shown state-of-the-art improvements in nodeclassification tasks on graphs. While these improvements have been largelydemonstrated in a multi-class classification scenario, a more general andrealistic scenario in which each node could have multiple labels has so farreceived little attention. The first challenge in conducting focused studies onmulti-label node classification is the limited number of publicly availablemulti-label graph datasets. Therefore, as our first contribution, we collectand release three real-world biological datasets and develop a multi-labelgraph generator to generate datasets with tunable properties. While high labelsimilarity (high homophily) is usually attributed to the success of GNNs, weargue that a multi-label scenario does not follow the usual semantics ofhomophily and heterophily so far defined for a multi-class scenario. As oursecond contribution, besides defining homophily for the multi-label scenario,we develop a new approach that dynamically fuses the feature and labelcorrelation information to learn label-informed representations. Finally, weperform a large-scale comparative study with $10$ methods and $9$ datasetswhich also showcase the effectiveness of our approach. We release our benchmarkat url{", "output": "Multi-label Node Classification On Graph-Structured Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have made transformed changes for human society.One of the key computation in LLMs is the softmax unit. This operation isimportant in LLMs because it allows the model to generate a distribution overpossible next words or phrases, given a sequence of input words. Thisdistribution is then used to select the most likely next word or phrase, basedon the probabilities assigned by the model. The softmax unit plays a crucialrole in training LLMs, as it allows the model to learn from the data byadjusting the weights and biases of the neural network.In the area of convex optimization such as using central path method to solvelinear programming. The softmax function has been used a crucial tool forcontrolling the progress and stability of potential function [Cohen, Lee andSong STOC 2019, Brand SODA 2020].In this work, inspired the softmax unit, we define a softmax regressionproblem. Formally speaking, given a matrix $A in mathbb{R}^{n times d}$ anda vector $b in mathbb{R}^n$, the goal is to use greedy type algorithm tosolve begin{align*} min_{x} | langle exp(Ax), {bf 1}_n rangle^{-1}exp(Ax) - b |_2^2. end{align*} In certain sense, our provable convergenceresult provides theoretical support for why we can use greedy algorithm totrain softmax function in practice.", "output": "Attention Scheme Inspired Softmax Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We demonstrate that gauge-equivariant pooling and unpooling layers canperform as well as traditional restriction and prolongation layers in multigridpreconditioner models for lattice QCD. These layers introduce a gauge degree offreedom on the coarse grid, allowing for the use of explicitlygauge-equivariant layers on the coarse grid. We investigate the construction ofcoarse-grid gauge fields and study their efficiency in the preconditionermodel. We show that a combined multigrid neural network using a Galerkinconstruction for the coarse-grid gauge field eliminates critical slowing down.", "output": "Gauge-equivariant pooling layers for preconditioners in lattice QCD."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Randomized smoothing (RS) is a well known certified defense againstadversarial attacks, which creates a smoothed classifier by predicting the mostlikely class under random noise perturbations of inputs during inference. Whileinitial work focused on robustness to $ell_2$ norm perturbations using noisesampled from a Gaussian distribution, subsequent works have shown thatdifferent noise distributions can result in robustness to other $ell_p$ normbounds as well. In general, a specific noise distribution is optimal fordefending against a given $ell_p$ norm based attack. In this work, we aim toimprove the certified adversarial robustness against multiple perturbationbounds simultaneously. Towards this, we firstly present a noveltextit{certification scheme}, that effectively combines the certificatesobtained using different noise distributions to obtain optimal results againstmultiple perturbation bounds. We further propose a novel textit{training noisedistribution} along with a textit{regularized training scheme} to improve thecertification within both $ell_1$ and $ell_2$ perturbation normssimultaneously. Contrary to prior works, we compare the certified robustness ofdifferent training algorithms across the same natural (clean) accuracy, ratherthan across fixed noise levels used for training and certification. We alsoempirically invalidate the argument that training and certifying the classifierwith the same amount of noise gives the best results. The proposed approachachieves improvements on the ACR (Average Certified Radius) metric across both$ell_1$ and $ell_2$ perturbation bounds.", "output": "Certified Adversarial Robustness Within Multiple Perturbation Bounds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In our work, we propose a novel yet simple approach to obtain an adaptivelearning rate for gradient-based descent methods on classification tasks.Instead of the traditional approach of selecting adaptive learning rates viathe decayed expectation of gradient-based terms, we use the angle between thecurrent gradient and the new gradient: this new gradient is computed from thedirection orthogonal to the current gradient, which further helps us indetermining a better adaptive learning rate based on angle history, thereby,leading to relatively better accuracy compared to the existing state-of-the-artoptimizers. On a wide variety of benchmark datasets with prominent imageclassification architectures such as ResNet, DenseNet, EfficientNet, and VGG,we find that our method leads to the highest accuracy in most of the datasets.Moreover, we prove that our method is convergent.", "output": "Angle based dynamic learning rate for gradient descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep reinforcement learning algorithms that learn policies by trial-and-errormust learn from limited amounts of data collected by actively interacting withthe environment. While many prior works have shown that proper regularizationtechniques are crucial for enabling data-efficient RL, a general understandingof the bottlenecks in data-efficient RL has remained unclear. Consequently, ithas been difficult to devise a universal technique that works well across alldomains. In this paper, we attempt to understand the primary bottleneck insample-efficient deep RL by examining several potential hypotheses such asnon-stationarity, excessive action distribution shift, and overfitting. Weperform thorough empirical analysis on state-based DeepMind control suite (DMC)tasks in a controlled and systematic way to show that high temporal-difference(TD) error on the validation set of transitions is the main culprit thatseverely affects the performance of deep RL algorithms, and prior methods thatlead to good performance do in fact, control the validation TD error to be low.This observation gives us a robust principle for making deep RL efficient: wecan hill-climb on the validation TD error by utilizing any form ofregularization techniques from supervised learning. We show that a simpleonline model selection method that targets the validation TD error is effectiveacross state-based DMC and Gym tasks.", "output": "Efficient Deep Reinforcement Learning Requires Regulating Overfitting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interactive Theorem Provers (ITPs) are an indispensable tool in the arsenalof formal method experts as a platform for construction and (formal)verification of proofs. The complexity of the proofs in conjunction with thelevel of expertise typically required for the process to succeed can oftenhinder the adoption of ITPs. A recent strain of work has investigated methodsto incorporate machine learning models trained on ITP user activity traces as aviable path towards full automation. While a valuable line of investigation,many problems still require human supervision to be completed fully, thusapplying learning methods to assist the user with useful recommendations canprove more fruitful.Following the vein of user assistance, we introduce CoProver, a proofrecommender system based on transformers, capable of learning from past actionsduring proof construction, all while exploring knowledge stored in the ITPconcerning previous proofs. CoProver employs a neurally learnt sequence-basedencoding of sequents, capturing long distance relationships between terms andhidden cues therein. We couple CoProver with the Prototype Verification System(PVS) and evaluate its performance on two key areas, namely: (1) Next ProofAction Recommendation, and (2) Relevant Lemma Retrieval given a library oftheories. We evaluate CoProver on a series of well-established metricsoriginating from the recommender system and information retrieval communities,respectively. We show that CoProver successfully outperforms prior state of theart applied to recommendation in the domain. We conclude by discussing futuredirections viable for CoProver (and similar approaches) such as argumentprediction, proof summarization, and more.", "output": "CoProver: A Recommender System for Proof Construction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conformation Generation is a fundamental problem in drug discovery andcheminformatics. And organic molecule conformation generation, particularly invacuum and protein pocket environments, is most relevant to drug design.Recently, with the development of geometric neural networks, the data-drivenschemes have been successfully applied in this field, both for molecularconformation generation (in vacuum) and binding pose generation (in proteinpocket). The former beats the traditional ETKDG method, while the latterachieves similar accuracy compared with the widely used molecular dockingsoftware. Although these methods have shown promising results, some researchershave recently questioned whether deep learning (DL) methods perform better inmolecular conformation generation via a parameter-free method. To our surprise,what they have designed is some kind analogous to the famous infinite monkeytheorem, the monkeys that are even equipped with physics education. To discussthe feasibility of their proving, we constructed a real infinite stochasticmonkey for molecular conformation generation, showing that even with a morestochastic sampler for geometry generation, the coverage of the benchmarkQM-computed conformations are higher than those of most DL-based methods. Byextending their physical monkey algorithm for binding pose prediction, we alsodiscover that the successful docking rate also achieves near-best performanceamong existing DL-based docking models. Thus, though their conclusions areright, their proof process needs more concern.", "output": "Infinite Physical Monkey: Do Deep Learning Methods Really Perform Better in Conformation Generation?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite a growing body of work at the intersection of deep learning andformal languages, there has been relatively little systematic exploration oftransformer models for reasoning about typed lambda calculi. This is aninteresting area of inquiry for two reasons. First, typed lambda calculi arethe lingua franc of programming languages. A set of heuristics that relatevarious typed lambda calculi to effective neural architectures would provide asystematic method for mapping language features (e.g., polymorphism, subtyping,inheritance, etc.) to architecture choices. Second, transformer models arewidely used in deep learning architectures applied to code, but the design andhyperparameter space for them is large and relatively unexplored in programminglanguage applications. Therefore, we suggest a benchmark that allows us toexplore exactly this through perhaps the simplest and most fundamental propertyof a programming language: the relationship between terms and types.Consequently, we begin this inquiry of transformer architectures for typedlambda calculi by exploring the effect of transformer warm-up and optimizerselection in the task of type inference: i.e., predicting the types of lambdacalculus terms using only transformers. We find that the optimization landscapeis difficult even in this simple setting. One particular experimental findingis that optimization by Adafactor converges much faster compared to theoptimization by Adam and RAdam. We conjecture that such different performanceof optimizers might be related to the difficulties of generalization overformally generated dataset.", "output": "Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, a simple and effective robust adaptive beamforming technique isproposed for uniform linear arrays, which is based on the power spectralestimation and uncertainty region (PSEUR) of the interference plus noise (IPN)components. In particular, two algorithms are presented to find the angularsector of interference in every snapshot based on the adopted spatialuncertainty region of the interference direction. Moreover, a power spectrum isintroduced based on the estimation of the power of interference and noisecomponents, which allows the development of a robust approach to IPN covariancematrix reconstruction. The proposed method has two main advantages. First, anangular region that contains the interference direction is updated based on thestatistics of the array data. Secondly, the proposed IPN-PSEUR method avoidsestimating the power spectrum of the whole range of possible directions of theinterference sector. Simulation results show that the performance of theproposed IPN-PSEUR beamformer is almost always close to the optimal valueacross a wide range of signal-to-noise ratios.", "output": "Study of Robust Adaptive Beamforming with Covariance Matrix Reconstruction Based on Power Spectral Estimation and Uncertainty Region."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Video Pre-trained Transformer. VPT uses four SOTA encoder modelsfrom prior work to convert a video into a sequence of compact embeddings. Ourbackbone, based on a reference Flan-T5-11B architecture, learns a universalrepresentation of the video that is a non-linear sum of the encoder models. Itlearns using an autoregressive causal language modeling loss by predicting thewords spoken in YouTube videos. Finally, we evaluate on standard downstreambenchmarks by training fully connected prediction heads for each task. To thebest of our knowledge, this is the first use of multiple frozen SOTA models asencoders in an \"embedding -&gt; backbone -&gt; prediction head\" design pattern - allothers have trained their own joint encoder models. Additionally, we includemore modalities than the current SOTA, Merlot Reserve, by adding explicit SceneGraph information. For these two reasons, we believe it could combine theworld's best open-source models to achieve SOTA performance. Initialexperiments demonstrate the model is learning appropriately, but moreexperimentation and compute is necessary, and already in progress, to realizeour loftier goals. Alongside this work, we build on the YT-20M dataset,reproducing it and adding 25,000 personally selected YouTube videos to itscorpus. All code and model checkpoints are open sourced under a standard MITlicense.", "output": "Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The dual use of machine learning applications, where models can be used forboth beneficial and malicious purposes, presents a significant challenge. Thishas recently become a particular concern in chemistry, where chemical datasetscontaining sensitive labels (e.g. toxicological information) could be used todevelop predictive models that identify novel toxins or chemical warfareagents. To mitigate dual use risks, we propose a model-agnostic method ofselectively noising datasets while preserving the utility of the data fortraining deep neural networks in a beneficial region. We evaluate theeffectiveness of the proposed method across least squares, a multilayerperceptron, and a graph neural network. Our findings show selectively noiseddatasets can induce model variance and bias in predictions for sensitive labelswith control, suggesting the safe sharing of datasets containing sensitiveinformation is feasible. We also find omitting sensitive data often increasesmodel variance sufficiently to mitigate dual use. This work is proposed as afoundation for future research on enabling more secure and collaborative datasharing practices and safer machine learning applications in chemistry.", "output": "Censoring chemical data to mitigate dual use risk."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An ever-growing number of vulnerabilities are reported every day. Yet thesevulnerabilities are not all the same; Some are more targeted than others.Correctly estimating the likelihood of a vulnerability being exploited is acritical task for system administrators. This aids the system administrators inprioritizing and patching the right vulnerabilities. Our work makes use ofoutlier detection techniques to predict vulnerabilities that are likely to beexploited in highly imbalanced and high-dimensional datasets such as theNational Vulnerability Database. We propose a dimensionality reductiontechnique, OutCenTR, that enhances the baseline outlier detection models. Wefurther demonstrate the effectiveness and efficiency of OutCenTR empiricallywith 4 benchmark and 12 synthetic datasets. The results of our experiments showon average a 5-fold improvement of F1 score in comparison with state-of-the-artdimensionality reduction techniques such as PCA and GRP.", "output": "OutCenTR: A novel semi-supervised framework for predicting exploits of vulnerabilities in high-dimensional datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Opioid and substance misuse is rampant in the United States today, with thephenomenon known as the \"opioid crisis\". The relationship between substance useand mental health has been extensively studied, with one possible relationshipbeing: substance misuse causes poor mental health. However, the lack ofevidence on the relationship has resulted in opioids being largely inaccessiblethrough legal means. This study analyzes the substance use posts on socialmedia with opioids being sold through crypto market listings. We use the DrugAbuse Ontology, state-of-the-art deep learning, and knowledge-aware BERT-basedmodels to generate sentiment and emotion for the social media posts tounderstand users' perceptions on social media by investigating questions suchas: which synthetic opioids people are optimistic, neutral, or negative about?or what kind of drugs induced fear and sorrow? or what kind of drugs peoplelove or are thankful about? or which drugs people think negatively about? orwhich opioids cause little to no sentimental reaction. We discuss how wecrawled crypto market data and its use in extracting posts for fentanyl,fentanyl analogs, and other novel synthetic opioids. We also perform topicanalysis associated with the generated sentiments and emotions to understandwhich topics correlate with people's responses to various drugs. Additionally,we analyze time-aware neural models built on these features while consideringhistorical sentiment and emotional activity of posts related to a drug. Themost effective model performs well (statistically significant) with(macroF1=82.12, recall =83.58) to identify substance use disorder.", "output": "\"Can We Detect Substance Use Disorder?\": Knowledge and Time Aware Classification on Social Media from Darkweb."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The evolution of convolutional neural networks (CNNs) can be largelyattributed to the design of its architecture, i.e., the network wiring pattern.Neural architecture search (NAS) advances this by automating the search for theoptimal network architecture, but the resulting network instance may notgeneralize well in different tasks. To overcome this, exploring network designprinciples that are generalizable across tasks is a more practical solution. Inthis study, We explore a novel brain-inspired design principle based on thecore-periphery property of the human brain network to guide the design of CNNs.Our work draws inspiration from recent studies suggesting that artificial andbiological neural networks may have common principles in optimizing networkarchitecture. We implement the core-periphery principle in the design ofnetwork wiring patterns and the sparsification of the convolution operation.The resulting core-periphery principle guided CNNs (CP-CNNs) are evaluated onthree different datasets. The experiments demonstrate the effectiveness andsuperiority compared to CNNs and ViT-based methods. Overall, our workcontributes to the growing field of brain-inspired AI by incorporating insightsfrom the human brain into the design of neural networks.", "output": "CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training segmentation models for medical images continues to be challengingdue to the limited availability and acquisition expense of data annotations.Segment Anything Model (SAM) is a foundation model trained on over 1 billionannotations, predominantly for natural images, that is intended to be able tosegment the user-defined object of interest in an interactive manner. Despiteits impressive performance on natural images, it is unclear how the model isaffected when shifting to medical image domains. Here, we perform an extensiveevaluation of SAM's ability to segment medical images on a collection of 11medical imaging datasets from various modalities and anatomies. In ourexperiments, we generated point prompts using a standard method that simulatesinteractive segmentation. Experimental results show that SAM's performancebased on single prompts highly varies depending on the task and the dataset,i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset,evaluated by IoU. Performance appears to be high for tasks includingwell-circumscribed objects with unambiguous prompts and poorer in many otherscenarios such as segmentation of tumors. When multiple prompts are provided,performance improves only slightly overall, but more so for datasets where theobject is not contiguous. An additional comparison to RITM showed a much betterperformance of SAM for one prompt but a similar performance of the two methodsfor a larger number of prompts. We conclude that SAM shows impressiveperformance for some datasets given the zero-shot learning setup but poor tomoderate performance for multiple other datasets. While SAM as a model and as alearning paradigm might be impactful in the medical imaging domain, extensiveresearch is needed to identify the proper ways of adapting it in this domain.", "output": "Segment Anything Model for Medical Image Analysis: an Experimental Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE),efficiently learn a rich representation of the input. However, for adapting todownstream tasks, they require a sufficient amount of labeled data since theirrich features capture not only objects but also less relevant image background.In contrast, Instance Discrimination (ID) methods focus on objects. In thiswork, we study how to combine the efficiency and scalability of MIM with theability of ID to perform downstream classification in the absence of largeamounts of labeled data. To this end, we introduce Masked AutoencoderContrastive Tuning (MAE-CT), a sequential approach that applies NearestNeighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes therich features such that they form semantic clusters of objects without usingany labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CTmatches or excels previous self-supervised methods trained on ImageNet inlinear probing, k-NN and low-shot classification accuracy as well as inunsupervised clustering accuracy. Notably, similar results can be achievedwithout additional image augmentations. While ID methods generally rely onhand-crafted augmentations to avoid shortcut learning, we find that nearestneighbor lookup is sufficient and that this data-driven augmentation effectimproves with model size. MAE-CT is compute efficient. For instance, startingfrom a MAE pre-trained ViT-L/16, MAE-CT increases the ImageNet 1% low-shotaccuracy from 67.7% to 72.6%, linear probing accuracy from 76.0% to 80.2% andk-NN accuracy from 60.6% to 79.1% in just five hours using eight A100 GPUs.", "output": "Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the well-studied problem of learning a linear combination of $k$ReLU activations with respect to a Gaussian distribution on inputs in $d$dimensions. We give the first polynomial-time algorithm that succeeds whenever$k$ is a constant. All prior polynomial-time learners require additionalassumptions on the network, such as positive combining coefficients or thematrix of hidden weight vectors being well-conditioned.Our approach is based on analyzing random contractions of higher-order momenttensors. We use a multi-scale analysis to argue that sufficiently close neuronscan be collapsed together, sidestepping the conditioning issues present inprior work. This allows us to design an iterative procedure to discoverindividual neurons.", "output": "Learning Narrow One-Hidden-Layer ReLU Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "People form judgments and make decisions based on the information that theyobserve. A growing portion of that information is not only provided, butcarefully curated by social media platforms. Although lawmakers largely agreethat platforms should not operate without any oversight, there is littleconsensus on how to regulate social media. There is consensus, however, thatcreating a strict, global standard of \"acceptable\" content is untenable (e.g.,in the US, it is incompatible with Section 230 of the Communications DecencyAct and the First Amendment).In this work, we propose that algorithmic filtering should be regulated withrespect to a flexible, user-driven baseline. We provide a concrete frameworkfor regulating and auditing a social media platform according to such abaseline. In particular, we introduce the notion of a baseline feed: thecontent that a user would see without filtering (e.g., on Twitter, this couldbe the chronological timeline). We require that the feeds a platform filterscontain \"similar\" informational content as their respective baseline feeds, andwe design a principled way to measure similarity. This approach is motivated byrelated suggestions that regulations should increase user agency. We present anauditing procedure that checks whether a platform honors this requirement.Notably, the audit needs only black-box access to a platform's filteringalgorithm, and it does not access or infer private user information. We providetheoretical guarantees on the strength of the audit. We further show thatrequiring closeness between filtered and baseline feeds does not impose a largeperformance cost, nor does it create echo chambers.", "output": "A User-Driven Framework for Regulating and Auditing Social Media."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) have received tremendous attention and achievedgreat success in various applications, such as image and video analysis,natural language processing, recommendation systems, and drug discovery.However, inherent uncertainties derived from different root causes have beenrealized as serious hurdles for DNNs to find robust and trustworthy solutionsfor real-world problems. A lack of consideration of such uncertainties may leadto unnecessary risk. For example, a self-driving autonomous car can misdetect ahuman on the road. A deep learning-based medical assistant may misdiagnosecancer as a benign tumor.In this work, we study how to measure different uncertainty causes for DNNsand use them to solve diverse decision-making problems more effectively. In thefirst part of this thesis, we develop a general learning framework to quantifymultiple types of uncertainties caused by different root causes, such asvacuity (i.e., uncertainty due to a lack of evidence) and dissonance (i.e.,uncertainty due to conflicting evidence), for graph neural networks. We providea theoretical analysis of the relationships between different uncertaintytypes. We further demonstrate that dissonance is most effective formisclassification detection and vacuity is most effective forOut-of-Distribution (OOD) detection. In the second part of the thesis, we studythe significant impact of OOD objects on semi-supervised learning (SSL) forDNNs and develop a novel framework to improve the robustness of existing SSLalgorithms against OODs. In the last part of the thesis, we create a generallearning framework to quantity multiple uncertainty types for multi-labeltemporal neural networks. We further develop novel uncertainty fusion operatorsto quantify the fused uncertainty of a subsequence for early event detection.", "output": "Multidimensional Uncertainty Quantification for Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The class of recurrent mixture density networks is an important class ofprobabilistic models used extensively in sequence modeling andsequence-to-sequence mapping applications. In this class of models, the densityof a target sequence in each time-step is modeled by a Gaussian mixture modelwith the parameters given by a recurrent neural network. In this paper, wegeneralize recurrent mixture density networks by defining a Gaussian mixturemodel on a non-linearly transformed target sequence in each time-step. Thenon-linearly transformed space is created by normalizing flow. We observed thatthis model significantly improves the fit to image sequences measured by thelog-likelihood. We also applied the proposed model on some speech and imagedata, and observed that the model has significant modeling power outperformingother state-of-the-art methods in terms of the log-likelihood.", "output": "FRMDN: Flow-based Recurrent Mixture Density Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training, especially projected gradient descent (PGD), has provento be a successful approach for improving robustness against adversarialattacks. After adversarial training, gradients of models with respect to theirinputs have a preferential direction. However, the direction of alignment isnot mathematically well established, making it difficult to evaluatequantitatively. We propose a novel definition of this direction as thedirection of the vector pointing toward the closest point of the support of theclosest inaccurate class in decision space. To evaluate the alignment with thisdirection after adversarial training, we apply a metric that uses generativeadversarial networks to produce the smallest residual needed to change theclass present in the image. We show that PGD-trained models have a higheralignment than the baseline according to our definition, that our metricpresents higher alignment values than a competing metric formulation, and thatenforcing this alignment increases the robustness of models.", "output": "Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The central objective function of a variational autoencoder (VAE) is itsvariational lower bound (the ELBO). Here we show that for standard (i.e.,Gaussian) VAEs the ELBO converges to a value given by the sum of threeentropies: the (negative) entropy of the prior distribution, the expected(negative) entropy of the observable distribution, and the average entropy ofthe variational distributions (the latter is already part of the ELBO). Ourderived analytical results are exact and apply for small as well as forintricate deep networks for encoder and decoder. Furthermore, they apply forfinitely and infinitely many data points and at any stationary point (includinglocal maxima and saddle points). The result implies that the ELBO can forstandard VAEs often be computed in closed-form at stationary points while theoriginal ELBO requires numerical approximations of integrals. As a maincontribution, we provide the proof that the ELBO for VAEs is at stationarypoints equal to entropy sums. Numerical experiments then show that the obtainedanalytical results are sufficiently precise also in those vicinities ofstationary points that are reached in practice. Furthermore, we discuss how thenovel entropy form of the ELBO can be used to analyze and understand learningbehavior. More generally, we believe that our contributions can be useful forfuture theoretical and practical studies on VAE learning as they provide novelinformation on those points in parameters space that optimization of VAEsconverges to.", "output": "The ELBO of Variational Autoencoders Converges to a Sum of Three Entropies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies experimental designs for estimation and inference onwelfare-maximizing policies in the presence of spillover effects. Units areorganized into a finite number of large clusters and interact in unknown wayswithin each cluster. As a first contribution, I introduce a single-waveexperiment that, by carefully varying the randomization across cluster pairs,estimates the marginal effect of a change in treatment probabilities, takingspillover effects into account. Using the marginal effect, I propose a test forpolicy optimality. As a second contribution, I design a multiple-waveexperiment to estimate treatment rules and maximize welfare. I derive strongsmall-sample guarantees on the difference between the maximum attainablewelfare and the welfare evaluated at the estimated policy. I illustrate themethod's properties in simulations calibrated to existing experiments oninformation diffusion and cash-transfer programs, and in a large scale fieldexperiment implemented in rural Pakistan.", "output": "Policy design in experiments with unknown interference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncertainty estimation methods using deep learning approaches strive againstseparating how uncertain the state of the world manifests to us via measurement(objective end) from the way this gets scrambled with the model specificationand training procedure used to predict such state (subjective means) -- e.g.,number of neurons, depth, connections, priors (if the model is bayesian),weight initialization, etc. This poses the question of the extent to which onecan eliminate the degrees of freedom associated with these specifications andstill being able to capture the objective end. Here, a novel non-parametricquantile estimation method for continuous random variables is introduced, basedon the simplest neural network architecture with one degree of freedom: asingle neuron. Its advantage is first shown in synthetic experiments comparingwith the quantile estimation achieved from ranking the order statistics(specifically for small sample size) and with quantile regression. Inreal-world applications, the method can be used to quantify predictiveuncertainty under the split conformal prediction setting, whereby predictionintervals are estimated from the residuals of a pre-trained model on a held-outvalidation set and then used to quantify the uncertainty in future predictions-- the single neuron used here as a structureless ``thermometer'' that measureshow uncertain the pre-trained model is. Benchmarking regression andclassification experiments demonstrate that the method is competitive inquality and coverage with state-of-the-art solutions, with the added benefit ofbeing more computationally efficient.", "output": "Can a single neuron learn predictive uncertainty?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The abundance of data has given machine learning considerable momentum innatural sciences and engineering, though modeling of physical processes isoften difficult. A particularly tough problem is the efficient representationof geometric boundaries. Triangularized geometric boundaries are wellunderstood and ubiquitous in engineering applications. However, it isnotoriously difficult to integrate them into machine learning approaches due totheir heterogeneity with respect to size and orientation. In this work, weintroduce an effective theory to model particle-boundary interactions, whichleads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modifygraph structures to obey boundary conditions. The new BGNNs are tested oncomplex 3D granular flow processes of hoppers, rotating drums and mixers, whichare all standard components of modern industrial machinery but still havecomplicated geometry. BGNNs are evaluated in terms of computational efficiencyas well as prediction accuracy of particle flows and mixing entropies. BGNNsare able to accurately reproduce 3D granular flows within simulationuncertainties over hundreds of thousands of simulation timesteps. Most notably,in our experiments, particles stay within the geometric objects without usinghandcrafted conditions or restrictions.", "output": "Boundary Graph Neural Networks for 3D Simulations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We show that the topes of a complex of oriented matroids (abbreviated COM) ofVC-dimension $d$ admit a proper labeled sample compression scheme of size $d$.This considerably extends results of Moran and Warmuth on ample classes, ofBen-David and Litman on affine arrangements of hyperplanes, and of the authorson complexes of uniform oriented matroids, and is a step towards the samplecompression conjecture -- one of the oldest open problems in computationallearning theory. On the one hand, our approach exploits the rich combinatorialcell structure of COMs via oriented matroid theory. On the other hand, viewingtope graphs of COMs as partial cubes creates a fruitful link to metric graphtheory.", "output": "Labeled sample compression schemes for complexes of oriented matroids."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study mid-cap companies, i.e. publicly traded companieswith less than US $10 billion in market capitalisation. Using a large datasetof US mid-cap companies observed over 30 years, we look to predict the defaultprobability term structure over the medium term and understand which datasources (i.e. fundamental, market or pricing data) contribute most to thedefault risk. Whereas existing methods typically require that data fromdifferent time periods are first aggregated and turned into cross-sectionalfeatures, we frame the problem as a multi-label time-series classificationproblem. We adapt transformer models, a state-of-the-art deep learning modelemanating from the natural language processing domain, to the credit riskmodelling setting. We also interpret the predictions of these models usingattention heat maps. To optimise the model further, we present a custom lossfunction for multi-label classification and a novel multi-channel architecturewith differential training that gives the model the ability to use all inputdata efficiently. Our results show the proposed deep learning architecture'ssuperior performance, resulting in a 13% improvement in AUC (Area Under thereceiver operating characteristic Curve) over traditional models. We alsodemonstrate how to produce an importance ranking for the different data sourcesand the temporal relationships using a Shapley approach specific to thesemodels.", "output": "A transformer-based model for default prediction in mid-cap corporate markets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection in autonomous driving applications implies that thedetection and tracking of semantic objects are commonly native to urban drivingenvironments, as pedestrians and vehicles. One of the major challenges instate-of-the-art deep-learning based object detection are false positives whichoccur with overconfident scores. This is highly undesirable in autonomousdriving and other critical robotic-perception domains because of safetyconcerns. This paper proposes an approach to alleviate the problem ofoverconfident predictions by introducing a novel probabilistic layer to deepobject detection networks in testing. The suggested approach avoids thetraditional Sigmoid or Softmax prediction layer which often producesoverconfident predictions. It is demonstrated that the proposed techniquereduces overconfidence in the false positives without degrading the performanceon the true positives. The approach is validated on the 2D-KITTI objectiondetection through the YOLOV4 and SECOND (Lidar-based detector). The proposedapproach enables interpretable probabilistic predictions without therequirement of re-training the network and therefore is very practical.", "output": "Probabilistic Approach for Road-Users Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep-unfolding neural networks (NNs) have received great attention since theyachieve satisfactory performance with relatively low complexity. Typically,these deep-unfolding NNs are restricted to a fixed-depth for all inputs.However, the optimal number of layers required for convergence changes withdifferent inputs. In this paper, we first develop a framework of deepdeterministic policy gradient (DDPG)-driven deep-unfolding with adaptive depthfor different inputs, where the trainable parameters of deep-unfolding NN arelearned by DDPG, rather than updated by the stochastic gradient descentalgorithm directly. Specifically, the optimization variables, trainableparameters, and architecture of deep-unfolding NN are designed as the state,action, and state transition of DDPG, respectively. Then, this framework isemployed to deal with the channel estimation problem in massive multiple-inputmultiple-output systems. Specifically, first of all we formulate the channelestimation problem with an off-grid basis and develop a sparse Bayesianlearning (SBL)-based algorithm to solve it. Secondly, the SBL-based algorithmis unfolded into a layer-wise structure with a set of introduced trainableparameters. Thirdly, the proposed DDPG-driven deep-unfolding framework isemployed to solve this channel estimation problem based on the unfoldedstructure of the SBL-based algorithm. To realize adaptive depth, we design thehalting score to indicate when to stop, which is a function of the channelreconstruction error. Furthermore, the proposed framework is extended torealize the adaptive depth of the general deep neural networks (DNNs).Simulation results show that the proposed algorithm outperforms theconventional optimization algorithms and DNNs with fixed depth with muchreduced number of layers.", "output": "DDPG-Driven Deep-Unfolding with Adaptive Depth for Channel Estimation with Sparse Bayesian Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study the challenging task of Byzantine-robustdecentralized training on arbitrary communication graphs. Unlike federatedlearning where workers communicate through a server, workers in thedecentralized environment can only talk to their neighbors, making it harder toreach consensus and benefit from collaborative training. To address theseissues, we propose a ClippedGossip algorithm for Byzantine-robust consensus andoptimization, which is the first to provably converge to a$O(delta_{max}zeta^2/gamma^2)$ neighborhood of the stationary point fornon-convex objectives under standard assumptions. Finally, we demonstrate theencouraging empirical performance of ClippedGossip under a large number ofattacks.", "output": "Byzantine-Robust Decentralized Learning via ClippedGossip."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning-based image reconstruction models, such as those based on the U-Net,require a large set of labeled images if good generalization is to beguaranteed. In some imaging domains, however, labeled data with pixel- orvoxel-level label accuracy are scarce due to the cost of acquiring them. Thisproblem is exacerbated further in domains like medical imaging, where there isno single ground truth label, resulting in large amounts of repeat variabilityin the labels. Therefore, training reconstruction networks to generalize betterby learning from both labeled and unlabeled examples (called semi-supervisedlearning) is problem of practical and theoretical interest. However,traditional semi-supervised learning methods for image reconstruction oftennecessitate handcrafting a differentiable regularizer specific to some givenimaging problem, which can be extremely time-consuming. In this work, wepropose \"supervision by denoising\" (SUD), a framework that enables us tosupervise reconstruction models using their own denoised output as soft labels.SUD unifies stochastic averaging and spatial denoising techniques under aspatio-temporal denoising framework and alternates denoising and model weightupdate steps in an optimization framework for semi-supervision. As exampleapplications, we apply SUD to two problems arising from biomedical imaging --anatomical brain reconstruction (3D) and cortical parcellation (2D) -- todemonstrate a significant improvement in the image reconstructions oversupervised-only and stochastic averaging baselines.", "output": "Supervision by Denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) are a class of deep learning-based methods forprocessing graph domain information. GNNs have recently become a widely usedgraph analysis method due to their superior ability to learn representationsfor complex graph data. However, due to privacy concerns and regulationrestrictions, centralized GNNs can be difficult to apply to data-sensitivescenarios. Federated learning (FL) is an emerging technology developed forprivacy-preserving settings when several parties need to train a shared globalmodel collaboratively. Although several research works have applied FL to trainGNNs (Federated GNNs), there is no research on their robustness to backdoorattacks.This paper bridges this gap by conducting two types of backdoor attacks inFederated GNNs: centralized backdoor attacks (CBA) and distributed backdoorattacks (DBA). Our experiments show that the DBA attack success rate is higherthan CBA in almost all evaluated cases. For CBA, the attack success rate of alllocal triggers is similar to the global trigger even if the training set of theadversarial party is embedded with the global trigger. To further explore theproperties of two backdoor attacks in Federated GNNs, we evaluate the attackperformance for a different number of clients, trigger sizes, poisoningintensities, and trigger densities. Moreover, we explore the robustness of DBAand CBA against one defense. We find that both attacks are robust against theinvestigated defense, necessitating the need to consider backdoor attacks inFederated GNNs as a novel threat that requires custom defenses.", "output": "More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the training of over-parameterized model functions via gradient descent,sometimes the parameters do not change significantly and remain close to theirinitial values. This phenomenon is called lazy training, and motivatesconsideration of the linear approximation of the model function around theinitial parameters. In the lazy regime, this linear approximation imitates thebehavior of the parameterized function whose associated kernel, called thetangent kernel, specifies the training performance of the model. Lazy trainingis known to occur in the case of (classical) neural networks with large widths.In this paper, we show that the training of geometrically local parameterizedquantum circuits enters the lazy regime for large numbers of qubits. Moreprecisely, we prove bounds on the rate of changes of the parameters of such ageometrically local parameterized quantum circuit in the training process, andon the precision of the linear approximation of the associated quantum modelfunction; both of these bounds tend to zero as the number of qubits grows. Wesupport our analytic results with numerical simulations.", "output": "Quantum Lazy Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of machine learning interatomic potentials has immenselycontributed to the accuracy of simulations of molecules and crystals. However,creating interatomic potentials for magnetic systems that account for bothmagnetic moments and structural degrees of freedom remains a challenge. Thiswork introduces SpinGNN, a spin-dependent interatomic potential approach thatemploys the graph neural network (GNN) to describe magnetic systems. SpinGNNconsists of two types of edge GNNs: Heisenberg edge GNN (HEGNN) andspin-distance edge GNN (SEGNN). HEGNN is tailored to capture Heisenberg-typespin-lattice interactions, while SEGNN accurately models multi-body andhigh-order spin-lattice coupling. The effectiveness of SpinGNN is demonstratedby its exceptional precision in fitting a high-order spin Hamiltonian and twocomplex spin-lattice Hamiltonians with great precision. Furthermore, itsuccessfully models the subtle spin-lattice coupling in BiFeO3 and performslarge-scale spin-lattice dynamics simulations, predicting its antiferromagneticground state, magnetic phase transition, and domain wall energy landscape withhigh accuracy. Our study broadens the scope of graph neural network potentialsto magnetic systems, serving as a foundation for carrying out large-scalespin-lattice dynamic simulations of such systems.", "output": "Spin-Dependent Graph Neural Network Potential for Magnetic Materials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We describe a simple and effective method (Spectral Attribute removaL; SAL)to remove private or guarded information from neural representations. Ourmethod uses matrix decomposition to project the input representations intodirections with reduced covariance with the guarded information rather thanmaximal covariance as factorization methods normally use. We begin with linearinformation removal and proceed to generalize our algorithm to the case ofnonlinear information removal using kernels. Our experiments demonstrate thatour algorithm retains better main task performance after removing the guardedinformation compared to previous work. In addition, our experiments demonstratethat we need a relatively small amount of guarded attribute data to removeinformation about these attributes, which lowers the exposure to sensitive dataand is more suitable for low-resource scenarios. Code is available at", "output": "Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As more and more AI agents are used in practice, it is time to think abouthow to make these agents fully autonomous so that they can (1) learn bythemselves continually in a self-motivated and self-initiated manner ratherthan being retrained offline periodically on the initiation of human engineersand (2) accommodate or adapt to unexpected or novel circumstances. As thereal-world is an open environment that is full of unknowns or novelties, thecapabilities of detecting novelties, characterizing them,accommodating/adapting to them, gathering ground-truth training data andincrementally learning the unknowns/novelties become critical in making the AIagent more and more knowledgeable, powerful and self-sustainable over time. Thekey challenge here is how to automate the process so that it is carried outcontinually on the agent's own initiative and through its own interactions withhumans, other agents and the environment just like human on-the-job learning.This paper proposes a framework (called SOLA) for this learning paradigm topromote the research of building autonomous and continual learning enabled AIagents. To show feasibility, an implemented agent is also described.", "output": "AI Autonomy : Self-Initiated Open-World Continual Learning and Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The analysis of logs is a vital activity undertaken for fault or cyberincident detection, investigation and technical forensics analysis for systemand cyber resilience. The potential application of AI algorithms for Loganalysis could augment such complex and laborious tasks. However, such solutionhas its constraints the heterogeneity of log sources and limited to no labelsfor training a classifier. When such labels become available, the need for theclassifier to be updated. This practice-based research seeks to address thesechallenges with the use of Transformer construct to train a new model with onlynormal log entries. Log augmentation through multiple forms of perturbation isapplied as a form of self-supervised training for feature learning. The modelis further finetuned using a form of reinforcement learning with a limited setof label samples to mimic real-world situation with the availability of labels.The experimental results of our model construct show promise with comparativeevaluation measurements paving the way for future practical applications.", "output": "AI based Log Analyser: A Practical Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Satellite Image Time Series (SITS) of the Earth's surface provide detailedland cover maps, with their quality in the spatial and temporal dimensionsconsistently improving. These image time series are integral for developingsystems that aim to produce accurate, up-to-date land cover maps of the Earth'ssurface. Applications are wide-ranging, with notable examples includingecosystem mapping, vegetation process monitoring and anthropogenic land-usechange tracking. Recently proposed methods for SITS classification havedemonstrated respectable merit, but these methods tend to lack nativemechanisms that exploit the temporal dimension of the data; commonly resultingin extensive data pre-processing contributing to prohibitively long trainingtimes. To overcome these shortcomings, Temporal CNNs have recently beenemployed for SITS classification tasks with encouraging results. This paperseeks to survey this method against a plethora of other contemporary methodsfor SITS classification to validate the existing findings in recent literature.Comprehensive experiments are carried out on two benchmark SITS datasets withthe results demonstrating that Temporal CNNs display a superior performance tothe comparative benchmark algorithms across both studied datasets, achievingaccuracies of 95.0% and 87.3% respectively. Investigations into the TemporalCNN architecture also highlighted the non-trivial task of optimising the modelfor a new dataset.", "output": "Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification: A survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Two-sample tests are important areas aiming to determine whether twocollections of observations follow the same distribution or not. We proposetwo-sample tests based on integral probability metric (IPM) forhigh-dimensional samples supported on a low-dimensional manifold. Wecharacterize the properties of proposed tests with respect to the number ofsamples $n$ and the structure of the manifold with intrinsic dimension $d$.When an atlas is given, we propose two-step test to identify the differencebetween general distributions, which achieves the type-II risk in the order of$n^{-1/max{d,2}}$. When an atlas is not given, we propose H\"older IPM testthat applies for data distributions with $(s,beta)$-H\"older densities, whichachieves the type-II risk in the order of $n^{-(s+beta)/d}$. To mitigate theheavy computation burden of evaluating the H\"older IPM, we approximate theH\"older function class using neural networks. Based on the approximationtheory of neural networks, we show that the neural network IPM test has thetype-II risk in the order of $n^{-(s+beta)/d}$, which is in the same order ofthe type-II risk as the H\"older IPM test. Our proposed tests are adaptive tolow-dimensional geometric structure because their performance crucially dependson the intrinsic dimension instead of the data dimension.", "output": "A Manifold Two-Sample Test Study: Integral Probability Metric with Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning is a machine learning training paradigm that enablesclients to jointly train models without sharing their own localized data.However, the implementation of federated learning in practice still facesnumerous challenges, such as the large communication overhead due to therepetitive server-client synchronization and the lack of adaptivity bySGD-based model updates. Despite that various methods have been proposed forreducing the communication cost by gradient compression or quantization, andthe federated versions of adaptive optimizers such as FedAdam are proposed toadd more adaptivity, the current federated learning framework still cannotsolve the aforementioned challenges all at once. In this paper, we propose anovel communication-efficient adaptive federated learning method (FedCAMS) withtheoretical convergence guarantees. We show that in the nonconvex stochasticoptimization setting, our proposed FedCAMS achieves the same convergence rateof $O(frac{1}{sqrt{TKm}})$ as its non-compressed counterparts. Extensiveexperiments on various benchmarks verify our theoretical analysis.", "output": "Communication-Efficient Adaptive Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current best performing models for knowledge graph reasoning (KGR) introducegeometry objects or probabilistic distributions to embed entities andfirst-order logical (FOL) queries into low-dimensional vector spaces. They canbe summarized as a center-size framework (point/box/cone, Beta/Gaussiandistribution, etc.). However, they have limited logical reasoning ability. Andit is difficult to generalize to various features, because the center and sizeare one-to-one constrained, unable to have multiple centers or sizes. Toaddress these challenges, we instead propose a novel KGR framework namedFeature-Logic Embedding framework, FLEX, which is the first KGR framework thatcan not only TRULY handle all FOL operations including conjunction,disjunction, negation and so on, but also support various feature spaces.Specifically, the logic part of feature-logic framework is based on vectorlogic, which naturally models all FOL operations. Experiments demonstrate thatFLEX significantly outperforms existing state-of-the-art methods on benchmarkdatasets.", "output": "FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We give the first polynomial time algorithms for escaping fromhigh-dimensional saddle points under a moderate number of constraints. Givengradient access to a smooth function $f colon mathbb R^d to mathbb R$ weshow that (noisy) gradient descent methods can escape from saddle points undera logarithmic number of inequality constraints. This constitutes the firsttangible progress (without reliance on NP-oracles or altering the definitionsto only account for certain constraints) on the main open question of thebreakthrough work of Ge et al. who showed an analogous result for unconstrainedand equality-constrained problems. Our results hold for both regular andstochastic gradient descent.", "output": "HOUDINI: Escaping from Moderately Constrained Saddles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a multi-agent reinforcement learning dynamics, and analyze itsconvergence in infinite-horizon discounted Markov potential games. We focus onthe independent and decentralized setting, where players do not have knowledgeof the game model and cannot coordinate. In each stage, players update theirestimate of a perturbed Q-function that evaluates their total contingent payoffbased on the realized one-stage reward in an asynchronous manner. Then, playersindependently update their policies by incorporating a smoothed optimalone-stage deviation strategy based on the estimated Q-function. A key featureof the learning dynamics is that the Q-function estimates are updated at afaster timescale than the policies. We prove that the policies induced by ourlearning dynamics converge to a stationary Nash equilibrium in Markov potentialgames with probability 1. Our results highlight the efficacy of simple learningdynamics in reaching a stationary Nash equilibrium even in environments withminimal information available.", "output": "Independent and Decentralized Learning in Markov Potential Games."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we present and study Continuous Generative Neural Networks(CGNNs), namely, generative models in the continuous setting: the output of aCGNN belongs to an infinite-dimensional function space. The architecture isinspired by DCGAN, with one fully connected layer, several convolutional layersand nonlinear activation functions. In the continuous $L^2$ setting, thedimensions of the spaces of each layer are replaced by the scales of amultiresolution analysis of a compactly supported wavelet. We presentconditions on the convolutional filters and on the nonlinearity that guaranteethat a CGNN is injective. This theory finds applications to inverse problems,and allows for deriving Lipschitz stability estimates for (possibly nonlinear)infinite-dimensional inverse problems with unknowns belonging to the manifoldgenerated by a CGNN. Several numerical simulations, including signaldeblurring, illustrate and validate this approach.", "output": "Continuous Generative Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) is a privacy-promoting framework that enablespotentially large number of clients to collaboratively train machine learningmodels. In a FL system, a server coordinates the collaboration by collectingand aggregating clients' model updates while the clients' data remains localand private. A major challenge in federated learning arises when the local datais heterogeneous -- the setting in which performance of the learned globalmodel may deteriorate significantly compared to the scenario where the data isidentically distributed across the clients. In this paper we propose FedDPMS(Federated Differentially Private Means Sharing), an FL algorithm in whichclients deploy variational auto-encoders to augment local datasets with datasynthesized using differentially private means of latent data representationscommunicated by a trusted server. Such augmentation ameliorates effects of dataheterogeneity across the clients without compromising privacy. Our experimentson deep image classification tasks demonstrate that FedDPMS outperformscompeting state-of-the-art FL methods specifically designed for heterogeneousdata settings.", "output": "Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The existing analysis of asynchronous stochastic gradient descent (SGD)degrades dramatically when any delay is large, giving the impression thatperformance depends primarily on the delay. On the contrary, we prove muchbetter guarantees for the same asynchronous SGD algorithm regardless of thedelays in the gradients, depending instead just on the number of paralleldevices used to implement the algorithm. Our guarantees are strictly betterthan the existing analyses, and we also argue that asynchronous SGD outperformssynchronous minibatch SGD in the settings we consider. For our analysis, weintroduce a novel recursion based on \"virtual iterates\" and delay-adaptivestepsizes, which allow us to derive state-of-the-art guarantees for both convexand non-convex objectives.", "output": "Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Reinforcement Learning (DRL) algorithms have been increasingly employedduring the last decade to solve various decision-making problems such asautonomous driving and robotics. However, these algorithms have faced greatchallenges when deployed in safety-critical environments since they oftenexhibit erroneous behaviors that can lead to potentially critical errors. Oneway to assess the safety of DRL agents is to test them to detect possiblefaults leading to critical failures during their execution. This raises thequestion of how we can efficiently test DRL policies to ensure theircorrectness and adherence to safety requirements. Most existing works ontesting DRL agents use adversarial attacks that perturb states or actions ofthe agent. However, such attacks often lead to unrealistic states of theenvironment. Their main goal is to test the robustness of DRL agents ratherthan testing the compliance of agents' policies with respect to requirements.Due to the huge state space of DRL environments, the high cost of testexecution, and the black-box nature of DRL algorithms, the exhaustive testingof DRL agents is impossible. In this paper, we propose a Search-based TestingApproach of Reinforcement Learning Agents (STARLA) to test the policy of a DRLagent by effectively searching for failing executions of the agent within alimited testing budget. We use machine learning models and a dedicated geneticalgorithm to narrow the search towards faulty episodes. We apply STARLA onDeep-Q-Learning agents which are widely used as benchmarks and show that itsignificantly outperforms Random Testing by detecting more faults related tothe agent's policy. We also investigate how to extract rules that characterizefaulty episodes of the DRL agent using our search results. Such rules can beused to understand the conditions under which the agent fails and thus assessits deployment risks.", "output": "A Search-Based Testing Approach for Deep Reinforcement Learning Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep cooperative multi-agent reinforcement learning has demonstrated itsremarkable success over a wide spectrum of complex control tasks. However,recent advances in multi-agent learning mainly focus on value decompositionwhile leaving entity interactions still intertwined, which easily leads toover-fitting on noisy interactions between entities. In this work, we introducea novel interactiOn Pattern disenTangling (OPT) method, to disentangle not onlythe joint value function into agent-wise value functions for decentralizedexecution, but also the entity interactions into interaction prototypes, eachof which represents an underlying interaction pattern within a subgroup of theentities. OPT facilitates filtering the noisy interactions between irrelevantentities and thus significantly improves generalizability as well asinterpretability. Specifically, OPT introduces a sparse disagreement mechanismto encourage sparsity and diversity among discovered interaction prototypes.Then the model selectively restructures these prototypes into a compactinteraction pattern by an aggregator with learnable weights. To alleviate thetraining instability issue caused by partial observability, we propose tomaximize the mutual information between the aggregation weights and the historybehaviors of each agent. Experiments on both single-task and multi-taskbenchmarks demonstrate that the proposed method yields results superior to thestate-of-the-art counterparts. Our code is available at", "output": "Interaction Pattern Disentangling for Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Molecular representation learning (MRL) is a key step to build the connectionbetween machine learning and chemical science. In particular, it encodesmolecules as numerical vectors preserving the molecular structures andfeatures, on top of which the downstream tasks (e.g., property prediction) canbe performed. Recently, MRL has achieved considerable progress, especially inmethods based on deep molecular graph learning. In this survey, wesystematically review these graph-based molecular representation techniques,especially the methods incorporating chemical domain knowledge. Specifically,we first introduce the features of 2D and 3D molecular graphs. Then wesummarize and categorize MRL methods into three groups based on their input.Furthermore, we discuss some typical chemical applications supported by MRL. Tofacilitate studies in this fast-developing area, we also list the benchmarksand commonly used datasets in the paper. Finally, we share our thoughts onfuture research directions.", "output": "Graph-based Molecular Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we present a novel trajectory comparison algorithm to identifyabnormal vital sign trends, with the aim of improving recognition ofdeteriorating health.There is growing interest in continuous wearable vital sign sensors formonitoring patients remotely at home. These monitors are usually coupled to analerting system, which is triggered when vital sign measurements fall outside apredefined normal range. Trends in vital signs, such as increasing heart rate,are often indicative of deteriorating health, but are rarely incorporated intoalerting systems.We introduce a dynamic time warp distance-based measure to compare timeseries trajectories. We split each multi-variable sign time series into 180minute, non-overlapping epochs. We then calculate the distance between allpairs of epochs. Each epoch is characterized by its mean pairwise distance(average link distance) to all other epochs, with clusters forming with nearbyepochs.We demonstrate in synthetically generated data that this method can identifyabnormal epochs and cluster epochs with similar trajectories. We then applythis method to a real-world data set of vital signs from 8 patients who hadrecently been discharged from hospital after contracting COVID-19. We show howoutlier epochs correspond well with the abnormal vital signs and identifypatients who were subsequently readmitted to hospital.", "output": "Outlier detection of vital sign trajectories from COVID-19 patients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training machine learning (ML) algorithms is a computationally intensiveprocess, which is frequently memory-bound due to repeatedly accessing largetraining datasets. As a result, processor-centric systems (e.g., CPU, GPU)suffer from costly data movement between memory units and processing units,which consumes large amounts of energy and execution cycles. Memory-centriccomputing systems, i.e., with processing-in-memory (PIM) capabilities, canalleviate this data movement bottleneck.Our goal is to understand the potential of modern general-purpose PIMarchitectures to accelerate ML training. To do so, we (1) implement severalrepresentative classic ML algorithms (namely, linear regression, logisticregression, decision tree, K-Means clustering) on a real-world general-purposePIM architecture, (2) rigorously evaluate and characterize them in terms ofaccuracy, performance and scaling, and (3) compare to their counterpartimplementations on CPU and GPU. Our evaluation on a real memory-centriccomputing system with more than 2500 PIM cores shows that general-purpose PIMarchitectures can greatly accelerate memory-bound ML workloads, when thenecessary operations and datatypes are natively supported by PIM hardware. Forexample, our PIM implementation of decision tree is $27times$ faster than astate-of-the-art CPU version on an 8-core Intel Xeon, and $1.34times$ fasterthan a state-of-the-art GPU version on an NVIDIA A100. Our K-Means clusteringon PIM is $2.8times$ and $3.2times$ than state-of-the-art CPU and GPUversions, respectively.To our knowledge, our work is the first one to evaluate ML training on areal-world PIM architecture. We conclude with key observations, takeaways, andrecommendations that can inspire users of ML workloads, programmers of PIMarchitectures, and hardware designers &amp; architects of future memory-centriccomputing systems.", "output": "An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Testing an integrated circuit (IC) is a highly compute-intensive process. Fortoday's complex designs, tests for many hard-to-detect faults are typicallygenerated using deterministic test generation (DTG) algorithms. MachineLearning (ML) is being increasingly used to increase the test coverage anddecrease the overall testing time. Such proposals primarily reduce the wastedwork in the classic Path Oriented Decision Making (PODEM) algorithm withoutcompromising on the test quality. With variants of PODEM, many times there is aneed to backtrack because further progress cannot be made. There is thus a needto predict the best strategy at different points in the execution of thealgorithm. The novel contribution of this paper is a 2-level predictor: the toplevel is a meta predictor that chooses one of several predictors at the lowerlevel. We choose the best predictor given a circuit and a target net. Theaccuracy of the top-level meta predictor was found to be 99%. This leads to asignificantly reduced number of backtracking decisions compared tostate-of-the-art ML-based and conventional solutions. As compared to a popular,state-of-the-art commercial ATPG tool, our 2-level predictor (HybMT) shows anoverall reduction of 32.6% in the CPU time without compromising on the faultcoverage for the EPFL benchmark circuits. HybMT also shows a speedup of 24.4%and 95.5% over the existing state-of-the-art (the baseline) while obtainingequal or better fault coverage for the ISCAS'85 and EPFL benchmark circuits,respectively.", "output": "HybMT: Hybrid Meta-Predictor based ML Algorithm for Fast Test Vector Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The variational lower bound (a.k.a. ELBO or free energy) is the centralobjective for many established as well as many novel algorithms forunsupervised learning. Learning algorithms change model parameters such thatthe variational lower bound increases. Learning usually proceeds untilparameters have converged to values close to a stationary point of the learningdynamics. In this purely theoretical contribution, we show that (for a verylarge class of generative models) the variational lower bound is at allstationary points of learning equal to a sum of entropies. For standard machinelearning models with one set of latents and one set observed variables, the sumconsists of three entropies: (A) the (average) entropy of the variationaldistributions, (B) the negative entropy of the model's prior distribution, and(C) the (expected) negative entropy of the observable distributions. Theobtained result applies under realistic conditions including: finite numbers ofdata points, at any stationary points (including saddle points) and for anyfamily of (well behaved) variational distributions. The class of generativemodels for which we show the equality to entropy sums contains many well-knowngenerative models. As concrete examples we discuss Sigmoid Belief Networks,probabilistic PCA and (Gaussian and non-Gaussian) mixture models. The resultsalso apply for standard (Gaussian) variational autoencoders, which has beenshown in parallel (Damm et al., 2023). The prerequisites we use to showequality to entropy sums are relatively mild. Concretely, the distributions ofa given generative model have to be of the exponential family (with constantbase measure), and the model has to satisfy a parameterization criterion (whichis usually fulfilled). Proving the equality of the ELBO to entropy sums atstationary points (under the stated conditions) is the main contribution ofthis work.", "output": "On the Convergence of the ELBO to Entropy Sums."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a new approach to probabilistic unsupervised learning based onthe recognition-parametrised model (RPM): a normalised semi-parametrichypothesis class for joint distributions over observed and latent variables.Under the key assumption that observations are conditionally independent givenlatents, the RPM combines parametric prior and observation-conditioned latentdistributions with non-parametric observation marginals. This approach leads toa flexible learnt recognition model capturing latent dependence betweenobservations, without the need for an explicit, parametric generative model.The RPM admits exact maximum-likelihood learning for discrete latents, even forpowerful neural-network-based recognition. We develop effective approximationsapplicable in the continuous-latent case. Experiments demonstrate theeffectiveness of the RPM on high-dimensional data, learning imageclassification from weak indirect supervision; direct image-level latentDirichlet allocation; and recognition-parametrised Gaussian process factoranalysis (RP-GPFA) applied to multi-factorial spatiotemporal datasets. The RPMprovides a powerful framework to discover meaningful latent structureunderlying observational data, a function critical to both animal andartificial intelligence.", "output": "Unsupervised representation learning with recognition-parametrised probabilistic models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scientists often use observational time series data to study complex naturalprocesses, but regression analyses often assume simplistic dynamics. Recentadvances in deep learning have yielded startling improvements to theperformance of models of complex processes, but deep learning is generally notused for scientific analysis. Here we show that deep learning can be used toanalyze complex processes, providing flexible function approximation whilepreserving interpretability. Our approach relaxes standard simplifyingassumptions (e.g., linearity, stationarity, and homoscedasticity) that areimplausible for many natural systems and may critically affect theinterpretation of data. We evaluate our model on incremental human languageprocessing, a domain with complex continuous dynamics. We demonstratesubstantial improvements on behavioral and neuroimaging data, and we show thatour model enables discovery of novel patterns in exploratory analyses, controlsfor diverse confounds in confirmatory analyses, and opens up research questionsthat are otherwise hard to study.", "output": "A Deep Learning Approach to Analyzing Continuous-Time Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The binding complexes formed by proteins and small molecule ligands areubiquitous and critical to life. Despite recent advancements in proteinstructure prediction, existing algorithms are so far unable to systematicallypredict the binding ligand structures along with their regulatory effects onprotein folding. To address this discrepancy, we present NeuralPLexer, acomputational approach that can directly predict protein-ligand complexstructures solely using protein sequence and ligand molecular graph inputs.NeuralPLexer adopts a deep generative model to sample the 3D structures of thebinding complex and their conformational changes at an atomistic resolution.The model is based on a diffusion process that incorporates essentialbiophysical constraints and a multi-scale geometric deep learning system toiteratively sample residue-level contact maps and all heavy-atom coordinates ina hierarchical manner. NeuralPLexer achieves state-of-the-art performancecompared to all existing methods on benchmarks for both protein-ligand blinddocking and flexible binding site structure recovery. Moreover, owing to itsspecificity in sampling both ligand-free-state and ligand-bound-stateensembles, NeuralPLexer consistently outperforms AlphaFold2 in terms of globalprotein structure accuracy on both representative structure pairs with largeconformational changes (average TM-score=0.93) and recently determinedligand-binding proteins (average TM-score=0.89). Case studies reveal that thepredicted conformational variations are consistent with structure determinationexperiments for important targets, including human KRAS$^textrm{G12C}$,ketol-acid reductoisomerase, and purine GPCRs. Our study suggests that adata-driven approach can capture the structural cooperativity between proteinsand small molecules, showing promise in accelerating the design of enzymes,drug molecules, and beyond.", "output": "State-specific protein-ligand complex structure prediction with a multi-scale deep generative model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In contrast to centralized model training that involves data collection,federated learning (FL) enables remote clients to collaboratively train a modelwithout exposing their private data. However, model performance usuallydegrades in FL due to the heterogeneous data generated by clients of diversecharacteristics. One promising strategy to maintain good performance is bylimiting the local training from drifting far away from the global model.Previous studies accomplish this by regularizing the distance between therepresentations learned by the local and global models. However, they onlyconsider representations from the early layers of a model or the layerpreceding the output layer. In this study, we introduce FedIntR, which providesa more fine-grained regularization by integrating the representations ofintermediate layers into the local training process. Specifically, FedIntRcomputes a regularization term that encourages the closeness between theintermediate layer representations of the local and global models.Additionally, FedIntR automatically determines the contribution of each layer'srepresentation to the regularization term based on the similarity between localand global representations. We conduct extensive experiments on variousdatasets to show that FedIntR can achieve equivalent or higher performancecompared to the state-of-the-art approaches. Our code is available at", "output": "Federated Learning with Intermediate Representation Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Background and objective: Self-supervised learning is rapidly advancingcomputer-aided diagnosis in the medical field. Masked image modeling (MIM) isone of the self-supervised learning methods that masks a subset of input pixelsand attempts to predict the masked pixels. Traditional MIM methods often employa random masking strategy. In comparison to ordinary images, medical imagesoften have a small region of interest for disease detection. Consequently, wefocus on fixing the problem in this work, which is evaluated by automaticCOVID-19 identification. Methods: In this study, we propose a novelregion-guided masked image modeling method (RGMIM) for COVID-19 detection inthis paper. In our method, we devise a new masking strategy that employed lungmask information to identify valid regions to learn more useful information forCOVID-19 detection. The proposed method was contrasted with fiveself-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam). Wepresent a quantitative evaluation of open COVID-19 CXR datasets as well asmasking ratio hyperparameter studies. Results: When using the entire trainingset, RGMIM outperformed other comparable methods, achieving 0.962 detectionaccuracy. Specifically, RGMIM significantly improved COVID-19 detection insmall data volumes, such as 5% and 10% of the training set (846 and 1,693images) compared to other methods, and achieved 0.957 detection accuracy evenwhen only 50% of the training set was used. Conclusions: RGMIM can mask morevalid lung-related regions, facilitating the learning of discriminativerepresentations and the subsequent high-accuracy COVID-19 detection. RGMIMoutperforms other state-of-the-art self-supervised learning methods inexperiments, particularly when limited training data is used.", "output": "RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, machine learning (ML) has become a popular approach to supportself-adaptation. ML has been used to deal with several problems inself-adaptation, such as maintaining an up-to-date runtime model underuncertainty and scalable decision-making. Yet, exploiting ML comes withinherent challenges. In this paper, we focus on a particularly importantchallenge for learning-based self-adaptive systems: drift in adaptation spaces.With adaptation space we refer to the set of adaptation options a self-adaptivesystem can select from at a given time to adapt based on the estimated qualityproperties of the adaptation options. Drift of adaptation spaces originatesfrom uncertainties, affecting the quality properties of the adaptation options.Such drift may imply that eventually no adaptation option can satisfy theinitial set of the adaptation goals, deteriorating the quality of the system,or adaptation options may emerge that allow enhancing the adaptation goals. InML, such shift corresponds to novel class appearance, a type of concept driftin target data that common ML techniques have problems dealing with. To tacklethis problem, we present a novel approach to self-adaptation that enhanceslearning-based self-adaptive systems with a lifelong ML layer. We refer to thisapproach as lifelong self-adaptation. The lifelong ML layer tracks the systemand its environment, associates this knowledge with the current tasks,identifies new tasks based on differences, and updates the learning models ofthe self-adaptive system accordingly. A human stakeholder may be involved tosupport the learning process and adjust the learning and goal models. Wepresent a general architecture for lifelong self-adaptation and apply it to thecase of drift of adaptation spaces that affects the decision-making inself-adaptation. We validate the approach for a series of scenarios using theDeltaIoT exemplar.", "output": "Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A model-agnostic variable importance method can be used with arbitraryprediction functions. Here we present some model-free methods that do notrequire access to the prediction function. This is useful when that function isproprietary and not available, or just extremely expensive. It is also usefulwhen studying residuals from a model. The cohort Shapley (CS) method ismodel-free but has exponential cost in the dimension of the input space. Asupervised on-manifold Shapley method from Frye et al. (2020) is also modelfree but requires as input a second black box model that has to be trained forthe Shapley value problem. We introduce an integrated gradient (IG) version ofcohort Shapley, called IGCS, with cost $mathcal{O}(nd)$. We show that over thevast majority of the relevant unit cube that the IGCS value function is closeto a multilinear function for which IGCS matches CS. Another benefit of IGCS isthat is allows IG methods to be used with binary predictors. We use some areabetween curves (ABC) measures to quantify the performance of IGCS. On a problemfrom high energy physics we verify that IGCS has nearly the same ABCs as CSdoes. We also use it on a problem from computational chemistry in 1024variables. We see there that IGCS attains much higher ABCs than we get fromMonte Carlo sampling. The code is publicly available at", "output": "Model free variable importance for high dimensional data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "DAG(Directed Acyclic Graph) from causal inference does not differentiatecausal effects and correlated changes. And the general effect of a populationis usually approximated by averaging correlations over all individuals. SinceAI(Artificial Intelligence) enables large-scale structure modeling on big data,the complex hidden confoundings have made these approximation errors no longerignorable but snowballed to considerable modeling bias - Such CausalRepresentation Bias (CRB) leads to many problems: ungeneralizable causalmodels, unrevealed individual-level features, hardly utilized causal knowledgein DL(Deep Learning), etc. In short, DAG must be redefined to enable a newframework for causal AI.The observational time series in statistics can only represent correlatedchanges, while the DL-based autoencoder can represent them as individualizedfeature changes in latent space to estimate the causal effects directly. Inthis paper, we introduce the redefined do-DAG to visualize CRB, propose ageneric solution Causal Representation Learning (CRL) framework, along with anovel architecture for its realization, and experimentally verify thefeasibility.", "output": "Realization of Causal Representation Learning and Redefined DAG for Causal AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning allows multiple clients to collaboratively train a modelwithout exchanging their data, thus preserving data privacy. Unfortunately, itsuffers significant performance degradation under heterogeneous data atclients. Common solutions in local training involve designing a specificauxiliary loss to regularize weight divergence or feature inconsistency.However, we discover that these approaches fall short of the expectedperformance because they ignore the existence of a vicious cycle betweenclassifier divergence and feature mapping inconsistency across clients, suchthat client models are updated in inconsistent feature space with divergedclassifiers. We then propose a simple yet effective framework named Federatedlearning with Feature Anchors (FedFA) to align the feature mappings andcalibrate classifier across clients during local training, which allows clientmodels updating in a shared feature space with consistent classifiers. Wedemonstrate that this modification brings similar classifiers and a virtuouscycle between feature consistency and classifier similarity across clients.Extensive experiments show that FedFA significantly outperforms thestate-of-the-art federated learning algorithms on various image classificationdatasets under label and feature distribution skews.", "output": "FedFA: Federated Learning with Feature Anchors to Align Features and Classifiers for Heterogeneous Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-parametric episodic memory can be used to quickly latch ontohigh-rewarded experience in reinforcement learning tasks. In contrast toparametric deep reinforcement learning approaches in which reward signals needto be back-propagated slowly, these methods only need to discover the solutiononce, and may then repeatedly solve the task. However, episodic controlsolutions are stored in discrete tables, and this approach has so far only beenapplied to discrete action space problems. Therefore, this paper introducesContinuous Episodic Control (CEC), a novel non-parametric episodic memoryalgorithm for sequential decision making in problems with a continuous actionspace. Results on several sparse-reward continuous control environments showthat our proposed method learns faster than state-of-the-art model-free RL andmemory-augmented RL algorithms, while maintaining good long-run performance aswell. In short, CEC can be a fast approach for learning in continuous controltasks.", "output": "Continuous Episodic Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Power grids, across the world, play an important societal and economical roleby providing uninterrupted, reliable and transient-free power to severalindustries, businesses and household consumers. With the advent of renewablepower resources and EVs resulting into uncertain generation and highly dynamicload demands, it has become ever so important to ensure robust operation ofpower networks through suitable management of transient stability issues andlocalize the events of blackouts. In the light of ever increasing stress on themodern grid infrastructure and the grid operators, this paper presents areinforcement learning (RL) framework, PowRL, to mitigate the effects ofunexpected network events, as well as reliably maintain electricity everywhereon the network at all times. The PowRL leverages a novel heuristic for overloadmanagement, along with the RL-guided decision making on optimal topologyselection to ensure that the grid is operated safely and reliably (with nooverloads). PowRL is benchmarked on a variety of competition datasets hosted bythe L2RPN (Learning to Run a Power Network). Even with its reduced actionspace, PowRL tops the leaderboard in the L2RPN NeurIPS 2020 challenge(Robustness track) at an aggregate level, while also being the top performingagent in the L2RPN WCCI 2020 challenge. Moreover, detailed analysis depictsstate-of-the-art performances by the PowRL agent in some of the test scenarios.", "output": "PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A reliable critic is central to on-policy actor-critic learning. But itbecomes challenging to learn a reliable critic in a multi-agent sparse rewardscenario due to two factors: 1) The joint action space grows exponentially withthe number of agents 2) This, combined with the reward sparseness andenvironment noise, leads to large sample requirements for accurate learning. Weshow that regularising the critic with spectral normalization (SN) enables itto learn more robustly, even in multi-agent on-policy sparse reward scenarios.Our experiments show that the regularised critic is quickly able to learn fromthe sparse rewarding experience in the complex SMAC and RWARE domains. Thesefindings highlight the importance of regularisation in the critic for stablelearning.", "output": "Effects of Spectral Normalization in Multi-agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatio-temporal modeling as a canonical task of multivariate time seriesforecasting has been a significant research topic in AI community. To addressthe underlying heterogeneity and non-stationarity implied in the graph streams,in this study, we propose Spatio-Temporal Meta-Graph Learning as a novel GraphStructure Learning mechanism on spatio-temporal data. Specifically, weimplement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN)by plugging the Meta-Graph Learner powered by a Meta-Node Bank into GCRNencoder-decoder. We conduct a comprehensive evaluation on two benchmarkdatasets (METR-LA and PEMS-BAY) and a large-scale spatio-temporal dataset thatcontains a variaty of non-stationary phenomena. Our model outperformed thestate-of-the-arts to a large degree on all three datasets (over 27% MAE and 34%RMSE). Besides, through a series of qualitative evaluations, we demonstratethat our model can explicitly disentangle locations and time slots withdifferent patterns and be robustly adaptive to different anomalous situations.Codes and datasets are available at ", "output": "MegaCRN: Meta-Graph Convolutional Recurrent Network for Spatio-Temporal Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Kernel machines have sustained continuous progress in the field of quantumchemistry. In particular, they have proven to be successful in the low-dataregime of force field reconstruction. This is because many equivariances andinvariances due to physical symmetries can be incorporated into the kernelfunction to compensate for much larger datasets. So far, the scalability ofkernel machines has however been hindered by its quadratic memory and cubicalruntime complexity in the number of training points. While it is known, thatiterative Krylov subspace solvers can overcome these burdens, their convergencecrucially relies on effective preconditioners, which are elusive in practice.Effective preconditioners need to partially pre-solve the learning problem in acomputationally cheap and numerically robust manner. Here, we consider thebroad class of Nystr\"om-type methods to construct preconditioners based onsuccessively more sophisticated low-rank approximations of the original kernelmatrix, each of which provides a different set of computational trade-offs. Allconsidered methods aim to identify a representative subset of inducing (kernel)columns to approximate the dominant kernel spectrum.", "output": "Reconstructing Kernel-based Machine Learning Force Fields with Super-linear Convergence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven approaches coupled with physical knowledge are powerfultechniques to model systems. The goal of such models is to efficiently solvefor the underlying field by combining measurements with known physical laws. Asmany systems contain unknown elements, such as missing parameters, noisy data,or incomplete physical laws, this is widely approached as an uncertaintyquantification problem. The common techniques to handle all the variablestypically depend on the numerical scheme used to approximate the posterior, andit is desirable to have a method which is independent of any suchdiscretization. Information field theory (IFT) provides the tools necessary toperform statistics over fields that are not necessarily Gaussian. We extend IFTto physics-informed IFT (PIFT) by encoding the functional priors withinformation about the physical laws which describe the field. The posteriorsderived from this PIFT remain independent of any numerical scheme and cancapture multiple modes, allowing for the solution of problems which areill-posed. We demonstrate our approach through an analytical example involvingthe Klein-Gordon equation. We then develop a variant of stochastic gradientLangevin dynamics to draw samples from the joint posterior over the field andmodel parameters. We apply our method to numerical examples with variousdegrees of model-form error and to inverse problems involving nonlineardifferential equations. As an addendum, the method is equipped with a metricwhich allows the posterior to automatically quantify model-form uncertainty.Because of this, our numerical experiments show that the method remains robustto even an incorrect representation of the physics given sufficient data. Wenumerically demonstrate that the method correctly identifies when the physicscannot be trusted, in which case it automatically treats learning the field asa regression problem.", "output": "Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic neural networks are a recent technique that promises a remedy for theincreasing size of modern deep learning models by dynamically adapting theircomputational cost to the difficulty of the inputs. In this way, the model canadjust to a limited computational budget. However, the poor quality ofuncertainty estimates in deep learning models makes it difficult to distinguishbetween hard and easy samples. To address this challenge, we present acomputationally efficient approach for post-hoc uncertainty quantification indynamic neural networks. We show that adequately quantifying and accounting forboth aleatoric and epistemic uncertainty through a probabilistic treatment ofthe last layers improves the predictive performance and aids decision-makingwhen determining the computational budget. In the experiments, we showimprovements on CIFAR-100, ImageNet, and Caltech-256 in terms of accuracy,capturing uncertainty, and calibration error.", "output": "Fixing Overconfidence in Dynamic Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present InstructABSA, Aspect Based Sentiment Analysis(ABSA) using the instruction learning paradigm for the ABSA subtasks: AspectTerm Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and JointTask modeling. Our method introduces positive, negative, and neutral examplesto each training sample, and instruction tunes the model (Tk-Instruct) the ABSAsubtasks, yielding significant performance improvements. Experimental resultson the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSAoutperforms the previous state-of-the-art (SOTA) approaches on the three ABSAsubtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7xlarger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATEsubtask by 5.69% points, Rest15 ATSC subtask by 9.59% points, and on the Lapt14Joint Task by 3.37% points. Our results also suggest a strong generalizationability to new domains across all three subtasks", "output": "InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traffic analysis is crucial for urban operations and planning, while theavailability of dense urban traffic data beyond loop detectors is still scarce.We present a large-scale floating vehicle dataset of per-street segment trafficinformation, Metropolitan Segment Traffic Speeds from Massive Floating Car Datain 10 Cities (MeTS-10), available for 10 global cities with a 15-minuteresolution for collection periods ranging between 108 and 361 days in 2019-2021and covering more than 1500 square kilometers per metropolitan area. MeTS-10features traffic speed information at all street levels from main arterials tolocal streets for Antwerp, Bangkok, Barcelona, Berlin, Chicago, Istanbul,London, Madrid, Melbourne and Moscow. The dataset leverages theindustrial-scale floating vehicle Traffic4cast data with speeds and vehiclecounts provided in a privacy-preserving spatio-temporal aggregation. We detailthe efficient matching approach mapping the data to the OpenStreetMap roadgraph. We evaluate the dataset by comparing it with publicly availablestationary vehicle detector data (for Berlin, London, and Madrid) and the Ubertraffic speed dataset (for Barcelona, Berlin, and London). The comparisonhighlights the differences across datasets in spatio-temporal coverage andvariations in the reported traffic caused by the binning method. MeTS-10enables novel, city-wide analysis of mobility and traffic patterns for tenmajor world cities, overcoming current limitations of spatially sparse vehicledetector data. The large spatial and temporal coverage offers an opportunityfor joining the MeTS-10 with other datasets, such as traffic surveys in trafficplanning studies or vehicle detector data in traffic control settings.", "output": "Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Classy Ensemble, a novel ensemble-generation algorithm forclassification tasks, which aggregates models through a weighted combination ofper-class accuracy. Tested over 153 machine learning datasets we demonstratethat Classy Ensemble outperforms two other well-known aggregation algorithms --order-based pruning and clustering-based pruning -- as well as the recentlyintroduced lexigarden ensemble generator. We then present three enhancements:1) Classy Cluster Ensemble, which combines Classy Ensemble and cluster-basedpruning; 2) Deep Learning experiments, showing the merits of Classy Ensembleover four image datasets: Fashion MNIST, CIFAR10, CIFAR100, and ImageNet; and3) Classy Evolutionary Ensemble, wherein an evolutionary algorithm is used toselect the set of models which Classy Ensemble picks from.", "output": "Classy Ensemble: A Novel Ensemble Algorithm for Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in deep learning have relied heavily on the use of largeTransformers due to their ability to learn at scale. However, the core buildingblock of Transformers, the attention operator, exhibits quadratic cost insequence length, limiting the amount of context accessible. Existingsubquadratic methods based on low-rank and sparse approximations need to becombined with dense attention layers to match Transformers, indicating a gap incapability. In this work, we propose Hyena, a subquadratic drop-in replacementfor attention constructed by interleaving implicitly parametrized longconvolutions and data-controlled gating. In recall and reasoning tasks onsequences of thousands to hundreds of thousands of tokens, Hyena improvesaccuracy by more than 50 points over operators relying on state-spaces andother implicit and explicit methods, matching attention-based models. We set anew state-of-the-art for dense-attention-free architectures on languagemodeling in standard datasets (WikiText103 and The Pile), reaching Transformerquality with a 20% reduction in training compute required at sequence length2K. Hyena operators are twice as fast as highly optimized attention at sequencelength 8K, and 100x faster at sequence length 64K.", "output": "Hyena Hierarchy: Towards Larger Convolutional Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Community detection is a fundamental problem in computational sciences withextensive applications in various fields. The most commonly used methods arethe algorithms designed to maximize modularity over different partitions of thenetwork nodes. Using 80 real and random networks from a wide range of contexts,we investigate the extent to which current heuristic modularity maximizationalgorithms succeed in returning maximum-modularity (optimal) partitions. Weevaluate (1) the ratio of the algorithms' output modularity to the maximummodularity for each input graph, and (2) the maximum similarity between theiroutput partition and any optimal partition of that graph. We compare eightexisting heuristic algorithms against an exact integer programming method thatglobally maximizes modularity. The average modularity-based heuristic algorithmreturns optimal partitions for only 16.9% of the 80 graphs considered.Additionally, results on adjusted mutual information reveal substantialdissimilarity between the sub-optimal partitions and any optimal partition ofthe networks in our experiments. More importantly, our results show thatnear-optimal partitions are often disproportionately dissimilar to any optimalpartition. Taken together, our analysis points to a crucial limitation ofcommonly used modularity-based heuristics for discovering communities: theyrarely produce an optimal partition or a partition resembling an optimalpartition. If modularity is to be used for detecting communities, exact orapproximate optimization algorithms are recommendable for a moremethodologically sound usage of modularity within its applicability limits.", "output": "Heuristic Modularity Maximization Algorithms for Community Detection Rarely Return an Optimal Partition or Anything Similar."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For deep ordinal classification, learning a well-structured feature spacespecific to ordinal classification is helpful to properly capture the ordinalnature among classes. Intuitively, when Euclidean distance metric is used, anideal ordinal layout in feature space would be that the sample clusters arearranged in class order along a straight line in space. However, enforcingsamples to conform to a specific layout in the feature space is a challengingproblem. To address this problem, in this paper, we propose a novel ConstrainedProxies Learning (CPL) method, which can learn a proxy for each ordinal classand then adjusts the global layout of classes by constraining these proxies.Specifically, we propose two kinds of strategies: hard layout constraint andsoft layout constraint. The hard layout constraint is realized by directlycontrolling the generation of proxies to force them to be placed in a strictlinear layout or semicircular layout (i.e., two instantiations of strictordinal layout). The soft layout constraint is realized by constraining thatthe proxy layout should always produce unimodal proxy-to-proxies similaritydistribution for each proxy (i.e., to be a relaxed ordinal layout). Experimentsshow that the proposed CPL method outperforms previous deep ordinalclassification methods under the same setting of feature extractor.", "output": "Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph machine learning models have been successfully deployed in a variety ofapplication areas. One of the most prominent types of models - Graph NeuralNetworks (GNNs) - provides an elegant way of extracting expressive node-levelrepresentation vectors, which can be used to solve node-related problems, suchas classifying users in a social network. However, many tasks requirerepresentations at the level of the whole graph, e.g., molecular applications.In order to convert node-level representations into a graph-level vector, aso-called readout function must be applied. In this work, we study existingreadout methods, including simple non-trainable ones, as well as complex,parametrized models. We introduce a concept of ensemble-based readout functionsthat combine either representations or predictions. Our experiments show thatsuch ensembles allow for better performance than simple single readouts orsimilar performance as the complex, parametrized ones, but at a fraction of themodel complexity.", "output": "Graph-level representations using ensemble-based readout functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rewrite systems [6, 10, 12] have been widely employing equality saturation[9], which is an optimisation methodology that uses a saturated e-graph torepresent all possible sequences of rewrite simultaneously, and then extractsthe optimal one. As such, optimal results can be achieved by avoiding thephase-ordering problem. However, we observe that when the e-graph is notsaturated, it cannot represent all possible rewrite opportunities and thereforethe phase-ordering problem is re-introduced during the construction phase ofthe e-graph. To address this problem, we propose MCTS-GEB, a domain-generalrewrite system that applies reinforcement learning (RL) to e-graphconstruction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3]to efficiently plan for the optimal e-graph construction, and therefore it caneffectively eliminate the phase-ordering problem at the construction phase andachieve better performance within a reasonable time. Evaluation in twodifferent domains shows MCTS-GEB can outperform the state-of-the-art rewritesystems by up to 49x, while the optimisation can generally take less than anhour, indicating MCTS-GEB is a promising building block for the futuregeneration of rewrite systems.", "output": "MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recently proposed open-world object and open-set detection have achieveda breakthrough in finding never-seen-before objects and distinguishing themfrom known ones. However, their studies on knowledge transfer from knownclasses to unknown ones are not deep enough, resulting in the scanty capabilityfor detecting unknowns hidden in the background. In this paper, we propose theunknown sniffer (UnSniffer) to find both unknown and known objects. Firstly,the generalized object confidence (GOC) score is introduced, which only usesknown samples for supervision and avoids improper suppression of unknowns inthe background. Significantly, such confidence score learned from known objectscan be generalized to unknown ones. Additionally, we propose a negative energysuppression loss to further suppress the non-object samples in the background.Next, the best box of each unknown is hard to obtain during inference due tolacking their semantic information in training. To solve this issue, weintroduce a graph-based determination scheme to replace hand-designednon-maximum suppression (NMS) post-processing. Finally, we present the UnknownObject Detection Benchmark, the first publicly benchmark that encompassesprecision evaluation for unknown detection to our knowledge. Experiments showthat our method is far better than the existing state-of-the-art methods.", "output": "Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many research fields in artificial intelligence, it has been shown thatdeep neural networks are useful to estimate unknown functions on highdimensional input spaces. However, their generalization performance is not yetcompletely clarified from the theoretical point of view because they arenonidentifiable and singular learning machines. Moreover, a ReLU function isnot differentiable, to which algebraic or analytic methods in singular learningtheory cannot be applied. In this paper, we study a deep ReLU neural network inoverparametrized cases and prove that the Bayesian free energy, which is equalto the minus log marginal likelihoodor the Bayesian stochastic complexity, isbounded even if the number of layers are larger than necessary to estimate anunknown data-generating function. Since the Bayesian generalization error isequal to the increase of the free energy as a function of a sample size, ourresult also shows that the Bayesian generalization error does not increase evenif a deep ReLU neural network is designed to be sufficiently large or in anopeverparametrized state.", "output": "Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article provides a curated review of selected papers published inprominent economics journals that use machine learning (ML) tools for researchand policy analysis. The review focuses on three key questions: (1) when ML isused in economics, (2) what ML models are commonly preferred, and (3) how theyare used for economic applications. The review highlights that ML isparticularly used to process nontraditional and unstructured data, capturestrong nonlinearity, and improve prediction accuracy. Deep learning models aresuitable for nontraditional data, whereas ensemble learning models arepreferred for traditional datasets. While traditional econometric models maysuffice for analyzing low-complexity data, the increasing complexity ofeconomic data due to rapid digitalization and the growing literature suggeststhat ML is becoming an essential addition to the econometrician's toolbox.", "output": "Machine Learning for Economics Research: When What and How?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Processing-in-memory (PIM) promises to alleviate the data movement bottleneckin modern computing systems. However, current real-world PIM systems have theinherent disadvantage that their hardware is more constrained than inconventional processors (CPU, GPU), due to the difficulty and cost of buildingprocessing elements near or inside the memory. As a result, general-purpose PIMarchitectures support fairly limited instruction sets and struggle to executecomplex operations such as transcendental functions and other hard-to-calculateoperations (e.g., square root). These operations are particularly important forsome modern workloads, e.g., activation functions in machine learningapplications.In order to provide support for transcendental (and other hard-to-calculate)functions in general-purpose PIM systems, we present emph{TransPimLib}, alibrary that provides CORDIC-based and LUT-based methods for trigonometricfunctions, hyperbolic functions, exponentiation, logarithm, square root, etc.We develop an implementation of TransPimLib for the UPMEM PIM architecture andperform a thorough evaluation of TransPimLib's methods in terms of performanceand accuracy, using microbenchmarks and three full workloads (Blackscholes,Sigmoid, Softmax). We open-source all our code and datasetsat~url{", "output": "TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Brain extraction is a critical preprocessing step in various neuroimagingstudies, particularly enabling accurate separation of brain from non-braintissue and segmentation of relevant within-brain tissue compartments andstructures using Magnetic Resonance Imaging (MRI) data. FSL's Brain ExtractionTool (BET), although considered the current gold standard for automatic brainextraction, presents limitations and can lead to errors such as over-extractionin brains with lesions affecting the outer parts of the brain, inaccuratedifferentiation between brain tissue and surrounding meninges, andsusceptibility to image quality issues. Recent advances in computer visionresearch have led to the development of the Segment Anything Model (SAM) byMeta AI, which has demonstrated remarkable potential in zero-shot segmentationof objects in real-world scenarios. In the current paper, we present acomparative analysis of brain extraction techniques comparing SAM with a widelyused and current gold standard technique called BET on a variety of brain scanswith varying image qualities, MR sequences, and brain lesions affectingdifferent brain regions. We find that SAM outperforms BET based on average Dicecoefficient, IoU and accuracy metrics, particularly in cases where imagequality is compromised by signal inhomogeneities, non-isotropic voxelresolutions, or the presence of brain lesions that are located near (orinvolve) the outer regions of the brain and the meninges. In addition, SAM hasalso unsurpassed segmentation properties allowing a fine grain separation ofdifferent issue compartments and different brain structures. These resultssuggest that SAM has the potential to emerge as a more accurate, robust andversatile tool for a broad range of brain extraction and segmentationapplications.", "output": "SAM vs BET: A Comparative Study for Brain Extraction and Segmentation of Magnetic Resonance Images using Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study dynamic discrete choice models, where a commonly studied probleminvolves estimating parameters of agent reward functions (also known as\"structural\" parameters), using agent behavioral data. Maximum likelihoodestimation for such models requires dynamic programming, which is limited bythe curse of dimensionality. In this work, we present a novel algorithm thatprovides a data-driven method for selecting and aggregating states, whichlowers the computational and sample complexity of estimation. Our method worksin two stages. In the first stage, we use a flexible inverse reinforcementlearning approach to estimate agent Q-functions. We use these estimatedQ-functions, along with a clustering algorithm, to select a subset of statesthat are the most pivotal for driving changes in Q-functions. In the secondstage, with these selected \"aggregated\" states, we conduct maximum likelihoodestimation using a commonly used nested fixed-point algorithm. The proposedtwo-stage approach mitigates the curse of dimensionality by reducing theproblem dimension. Theoretically, we derive finite-sample bounds on theassociated estimation error, which also characterize the trade-off ofcomputational complexity, estimation error, and sample complexity. Wedemonstrate the empirical performance of the algorithm in two classic dynamicdiscrete choice estimation applications.", "output": "A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robust feature selection is vital for creating reliable and interpretableMachine Learning (ML) models. When designing statistical prediction models incases where domain knowledge is limited and underlying interactions areunknown, choosing the optimal set of features is often difficult. To mitigatethis issue, we introduce a Multidata (M) causal feature selection approach thatsimultaneously processes an ensemble of time series datasets and produces asingle set of causal drivers. This approach uses the causal discoveryalgorithms PC1 or PCMCI that are implemented in the Tigramite Python package.These algorithms utilize conditional independence tests to infer parts of thecausal graph. Our causal feature selection approach filters outcausally-spurious links before passing the remaining causal features as inputsto ML models (Multiple linear regression, Random Forest) that predict thetargets. We apply our framework to the statistical intensity prediction ofWestern Pacific Tropical Cyclones (TC), for which it is often difficult toaccurately choose drivers and their dimensionality reduction (time lags,vertical levels, and area-averaging). Using more stringent significancethresholds in the conditional independence tests helps eliminate spuriouscausal relationships, thus helping the ML model generalize better to unseen TCcases. M-PC1 with a reduced number of features outperforms M-PCMCI, non-causalML, and other feature selection methods (lagged correlation, random), evenslightly outperforming feature selection based on eXplainable ArtificialIntelligence. The optimal causal drivers obtained from our causal featureselection help improve our understanding of underlying relationships andsuggest new potential drivers of TC intensification.", "output": "Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Our recent intensive study has found that physics-informed neural networks(PINN) tend to be local approximators after training. This observation leads tothis novel physics-informed radial basis network (PIRBN), which can maintainthe local property throughout the entire training process. Compared to deepneural networks, a PIRBN comprises of only one hidden layer and a radial basis\"activation\" function. Under appropriate conditions, we demonstrated that thetraining of PIRBNs using gradient descendent methods can converge to Gaussianprocesses. Besides, we studied the training dynamics of PIRBN via the neuraltangent kernel (NTK) theory. In addition, comprehensive investigationsregarding the initialisation strategies of PIRBN were conducted. Based onnumerical examples, PIRBN has been demonstrated to be more effective andefficient than PINN in solving PDEs with high-frequency features and ill-posedcomputational domains. Moreover, the existing PINN numerical techniques, suchas adaptive learning, decomposition and different types of loss functions, areapplicable to PIRBN. The programs that can regenerate all numerical results canbe found at ", "output": "Physics-informed radial basis network (PIRBN): A local approximating neural network for solving nonlinear PDEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the first $varepsilon$-differentially private, computationallyefficient algorithm that estimates the means of product distributions over${0,1}^d$ accurately in total-variation distance, whilst attaining theoptimal sample complexity to within polylogarithmic factors. The prior work hadeither solved this problem efficiently and optimally under weaker notions ofprivacy, or had solved it optimally while having exponential running times.", "output": "A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study aims to relate the time-frequency patterns of acoustic emission(AE) and other multi-modal sensor data collected in a hybrid directed energydeposition (DED) process to the pore formations at high spatial (0.5 mm) andtime (&lt; 1ms) resolutions. Adapting an explainable AI method in LIME (LocalInterpretable Model-Agnostic Explanations), certain high-frequency waveformsignatures of AE are to be attributed to two major pathways for pore formationin a DED process, namely, spatter events and insufficient fusion betweenadjacent printing tracks from low heat input. This approach opens an excitingpossibility to predict, in real-time, the presence of a pore in every voxel(0.5 mm in size) as they are printed, a major leap forward compared to priorefforts. Synchronized multimodal sensor data including force, AE, vibration andtemperature were gathered while an SS316L material sample was printed andsubsequently machined. A deep convolution neural network classifier was used toidentify the presence of pores on a voxel surface based on time-frequencypatterns (spectrograms) of the sensor data collected during the process chain.The results suggest signals collected during DED were more sensitive comparedto those from machining for detecting porosity in voxels (classification testaccuracy of 87%). The underlying explanations drawn from LIME analysis suggeststhat energy captured in high frequency AE waveforms are 33% lower for porousvoxels indicating a relatively lower laser-material interaction in the meltpool, and hence insufficient fusion and poor overlap between adjacent printingtracks. The porous voxels for which spatter events were prevalent duringprinting had about 27% higher energy contents in the high frequency AE bandcompared to other porous voxels. These signatures from AE signal can furtherthe understanding of pore formation from spatter and insufficient fusion.", "output": "In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Water is the lifeblood of river networks, and its quality plays a crucialrole in sustaining both aquatic ecosystems and human societies. Real-timemonitoring of water quality is increasingly reliant on in-situ sensortechnology. Anomaly detection is crucial for identifying erroneous patterns insensor data, but can be a challenging task due to the complexity andvariability of the data, even under normal conditions. This paper presents asolution to the challenging task of anomaly detection for river network sensordata, which is essential for accurate and continuous monitoring. We use a graphneural network model, the recently proposed Graph Deviation Network (GDN),which employs graph attention-based forecasting to capture the complexspatio-temporal relationships between sensors. We propose an alternate anomalyscoring method, GDN+, based on the learned graph. To evaluate the model'sefficacy, we introduce new benchmarking simulation experiments withhighly-sophisticated dependency structures and subsequence anomalies of varioustypes. We further examine the strengths and weaknesses of this baselineapproach, GDN, in comparison to other benchmarking methods on complexreal-world river network data. Findings suggest that GDN+ outperforms thebaseline approach in high-dimensional data, while also providing improvedinterpretability. We also introduce software called gnnad.", "output": "Graph Neural Network-Based Anomaly Detection for River Network Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-frequency trading requires fast data processing without information lagsfor precise stock price forecasting. This high-paced stock price forecasting isusually based on vectors that need to be treated as sequential andtime-independent signals due to the time irregularities that are inherent inhigh-frequency trading. A well-documented and tested method that considersthese time-irregularities is a type of recurrent neural network, named longshort-term memory neural network. This type of neural network is formed basedon cells that perform sequential and stale calculations via gates and stateswithout knowing whether their order, within the cell, is optimal. In thispaper, we propose a revised and real-time adjusted long short-term memory cellthat selects the best gate or state as its final output. Our cell is runningunder a shallow topology, has a minimal look-back period, and is trainedonline. This revised cell achieves lower forecasting error compared to otherrecurrent neural networks for online high-frequency trading forecasting taskssuch as the limit order book mid-price prediction as it has been tested on twohigh-liquid US and two less-liquid Nordic stocks.", "output": "Optimum Output Long Short-Term Memory Cell for High-Frequency Trading Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-local attention module has been proven to be crucial for imagerestoration. Conventional non-local attention processes features of each layerseparately, so it risks missing correlation between features among differentlayers. To address this problem, we aim to design attention modules thataggregate information from different layers. Instead of finding correlated keypixels within the same layer, each query pixel is encouraged to attend to keypixels at multiple previous layers of the network. In order to efficientlyembed such attention design into neural network backbones, we propose a novelAdaptive Cross-Layer Attention (ACLA) module. Two adaptive designs are proposedfor ACLA: (1) adaptively selecting the keys for non-local attention at eachlayer; (2) automatically searching for the insertion locations for ACLAmodules. By these two adaptive designs, ACLA dynamically selects a flexiblenumber of keys to be aggregated for non-local attention at previous layer whilemaintaining a compact neural network with compelling performance. Extensiveexperiments on image restoration tasks, including single imagesuper-resolution, image denoising, image demosaicing, and image compressionartifacts reduction, validate the effectiveness and efficiency of ACLA. Thecode of ACLA is available at url{", "output": "Adaptive Cross-Layer Attention for Image Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of robust hypothesis testing is studied, where under the null andthe alternative hypotheses, the data-generating distributions are assumed to bein some uncertainty sets, and the goal is to design a test that performs wellunder the worst-case distributions over the uncertainty sets. In this paper,uncertainty sets are constructed in a data-driven manner using kernel method,i.e., they are centered around empirical distributions of training samples fromthe null and alternative hypotheses, respectively; and are constrained via thedistance between kernel mean embeddings of distributions in the reproducingkernel Hilbert space, i.e., maximum mean discrepancy (MMD). The Bayesiansetting and the Neyman-Pearson setting are investigated. For the Bayesiansetting where the goal is to minimize the worst-case error probability, anoptimal test is firstly obtained when the alphabet is finite. When the alphabetis infinite, a tractable approximation is proposed to quantify the worst-caseaverage error probability, and a kernel smoothing method is further applied todesign test that generalizes to unseen samples. A direct robust kernel test isalso proposed and proved to be exponentially consistent. For the Neyman-Pearsonsetting, where the goal is to minimize the worst-case probability of missdetection subject to a constraint on the worst-case probability of false alarm,an efficient robust kernel test is proposed and is shown to be asymptoticallyoptimal. Numerical results are provided to demonstrate the performance of theproposed robust tests.", "output": "Kernel Robust Hypothesis Testing."}]