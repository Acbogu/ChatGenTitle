[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Even in the most cutting-edge Artificial General Intelligence (AGI)endeavors, the disparity between humans and artificial systems is extremelyapparent. Although this difference fundamentally divides the capabilities ofeach, human-level intelligence (HLI) has remained the aim of AGI for decades.This paper opposes the binarity of the Turing Test, the foundation of thisintention and original establishment of a potentially intelligent machine. Itdiscusses how AI experts misinterpreted the Imitation Game as a means toanthropomorphize computer systems and asserts that HLI is a red herring thatdistracts current research from relevant problems. Despite the extensiveresearch on the potential design of an AGI application, there has been littleconsideration of how such a system will access and ingest data at a human-likelevel. Although current machines may emulate specific human attributes, AGI isdeveloped under the pretense that this can be easily scaled up to a generalintelligence level. This paper establishes contextual and rational attributesthat perpetuate the variation between human and AI data collection abilitiesand explores the characteristics that current AGI lacks. After asserting thatAGI should not be seeking HLI, its current state is analyzed, the Turing Testis reevaluated, and the future of AGI development is discussed within thisframework.", "output": "Analyzing the Contextual Shortcomings of Artificial General Intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce and illustrate the novel phenomenon of ConceptualEntanglement which emerges due to the representational manifoldness immanentwhile incrementally modelling domain ontologies step-by-step across thefollowing five levels: perception, labelling, semantic alignment, hierarchicalmodelling and intensional definition. In turn, we propose ConceptualDisentanglement, a multi-level conceptual modelling strategy which enforces andexplicates, via guiding principles, semantic bijections with respect to eachlevel of conceptual entanglement (across all the above five levels) paving theway for engineering conceptually disentangled domain ontologies. We alsobriefly argue why state-of-the-art ontology development methodologies andapproaches are insufficient with respect to our characterization.", "output": "Disentangling Domain Ontologies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A number of generalizations of stochastic and information-theoreticrandomness are known in the literature. However, they are not compatible withhandling meaning in vague and dynamic contexts of rough reasoning (andtherefore explainable artificial intelligence and machine learning). In thisresearch, new concepts of rough randomness that are neither stochastic norbased on properties of strings are introduced by the present author. Herconcepts are intended to capture a wide variety of rough processes (applicableto both static and dynamic data), construct related models, and explore thevalidity of other machine learning algorithms. The last mentioned is restrictedto soft/hard clustering algorithms in this paper. Two new computationallyefficient algebraically-justified algorithms for soft and hard clustervalidation that involve rough random functions are additionally proposed inthis research. A class of rough random functions termed large-minded reasonershave a central role in these.", "output": "Rough Randomness and its Application."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The challenges of using inadequate online recruitment systems can beaddressed with machine learning and software engineering techniques.Bi-directional personalization reinforcement learning-based architecture withactive learning can get recruiters to recommend qualified applicants and alsoenable applicants to receive personalized job recommendations. This paperfocuses on how machine learning techniques can enhance the recruitment processin the travel nursing industry by helping speed up data acquisition using amulti-model data service and then providing personalized recommendations usingbi-directional reinforcement learning with active learning. This need wasespecially evident when trying to respond to the overwhelming needs ofhealthcare facilities during the COVID-19 pandemic. The need for travelingnurses and other healthcare professionals was more evident during the lockdownperiod. A data service was architected for job feed processing using anorchestration of natural language processing (NLP) models that synthesizejob-related data into a database efficiently and accurately. The multi-modeldata service provided the data necessary to develop a bi-directionalpersonalization system using reinforcement learning with active learning thatcould recommend travel nurses and healthcare professionals to recruiters andprovide job recommendations to applicants using an internally developed smartmatch score as a basis. The bi-directional personalization reinforcementlearning-based architecture with active learning combines two personalizationsystems - one that runs forward to recommend qualified candidates for jobs andanother that runs backward and recommends jobs for applicants.", "output": "Bi-directional personalization reinforcement learning-based architecture with active learning using a multi-model data service for the travel nursing industry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in technology, particularly in machine learning (ML),deep learning (DL), and the metaverse, offer great potential forrevolutionizing surgical science. The combination of artificial intelligenceand extended reality (AI-XR) technologies has the potential to create asurgical metaverse, a virtual environment where surgeries can be planned andperformed. This paper aims to provide insight into the various potentialapplications of an AI-XR surgical metaverse and the challenges that must beaddressed to bring its full potential to fruition. It is important for thecommunity to focus on these challenges to fully realize the potential of theAI-XR surgical metaverses. Furthermore, to emphasize the need for secure androbust AI-XR surgical metaverses and to demonstrate the real-world implicationsof security threats to the AI-XR surgical metaverses, we present a case studyin which the ``an immersive surgical attack'' on incision point localization isperformed in the context of preoperative planning in a surgical metaverse.", "output": "Can We Revitalize Interventional Healthcare with AI-XR Surgical Metaverses?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) are revolutionizing several areas of ArtificialIntelligence. One of the most remarkable applications is creative writing,e.g., poetry or storytelling: the generated outputs are often of astonishingquality. However, a natural question arise: can LLMs really be consideredcreative? In this article we firstly analyze the development of LLMs under thelens of creativity theories, investigating the key open questions andchallenges. Then, we identify a set of \"easy\" and \"hard\" problems in machinecreativity, discussing them in relation to LLMs. Finally, we analyze thesocietal impact of these technologies with a particular focus on the creativeindustries.", "output": "On the Creativity of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of cooperative multi-agent reinforcement learning (MARL), thestandard paradigm is the use of centralised training and decentralisedexecution where a central critic conditions the policies of the cooperativeagents based on a central state. It has been shown, that in cases with largenumbers of redundant agents these methods become less effective. In a moregeneral case, there is likely to be a larger number of agents in an environmentthan is required to solve the task. These redundant agents reduce performanceby enlarging the dimensionality of both the state space and and increasing thesize of the joint policy used to solve the environment. We propose leveraginglayerwise relevance propagation (LRP) to instead separate the learning of thejoint value function and generation of local reward signals and create a newMARL algorithm: relevance decomposition network (RDN). We find that althoughthe performance of both baselines VDN and Qmix degrades with the number ofredundant agents, RDN is unaffected.", "output": "The challenge of redundancy on multi-agent value factorisation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The prevalence of memes on social media has created the need to sentimentanalyze their underlying meanings for censoring harmful content. Meme censoringsystems by machine learning raise the need for a semi-supervised learningsolution to take advantage of the large number of unlabeled memes available onthe internet and make the annotation process less challenging. Moreover, theapproach needs to utilize multimodal data as memes' meanings usually come fromboth images and texts. This research proposes a multimodal semi-supervisedlearning approach that outperforms other multimodal semi-supervised learningand supervised learning state-of-the-art models on two datasets, the MultimediaAutomatic Misogyny Identification and Hateful Memes dataset. Building on theinsights gained from Contrastive Language-Image Pre-training, which is aneffective multimodal learning technique, this research introduces SemiMemes, anovel training method that combines auto-encoder and classification task tomake use of the resourceful unlabeled data.", "output": "SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sequential decision-making agents struggle with long horizon tasks, sincesolving them requires multi-step reasoning. Most reinforcement learning (RL)algorithms address this challenge by improved credit assignment, introducingmemory capability, altering the agent's intrinsic motivation (i.e. exploration)or its worldview (i.e. knowledge representation). Many of these componentscould be learned from offline data. In this work, we follow the hypothesis thatexploration and representation learning can be improved by separately learningtwo different models from a single offline dataset. We show that learning astate representation using noise-contrastive estimation and a model ofauxiliary reward separately from a single collection of human demonstrationscan significantly improve the sample efficiency on the challenging NetHackbenchmark. We also ablate various components of our experimental setting andhighlight crucial insights.", "output": "Accelerating exploration and representation learning with offline pre-training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cyberlogic is an enabling logical foundation for building and analyzingdigital transactions that involve the exchange of digital forms of evidence. Itis based on an extension of (first-order) intuitionistic predicate logic withan attestation and a knowledge modality. The key ideas underlying Cyberlogicare extremely simple, as (1) public keys correspond to authorizations, (2)transactions are specified as distributed logic programs, and (3) verifiableevidence is collected by means of distributed proof search. Verifiableevidence, in particular, are constructed from extra-logical elements such assigned documents and cryptographic signatures. Despite this conceptualsimplicity of Cyberlogic, central features of authorization policies includingtrust, delegation, and revocation of authority are definable. An expressivetemporal-epistemic logic for specifying distributed authorization policies andprotocols is therefore definable in Cyberlogic using a trusted time source. Wedescribe the distributed execution of Cyberlogic programs based on thehereditary Harrop fragment in terms of distributed proof search, and weillustrate some fundamental issues in the distributed construction ofcertificates. The main principles of encoding and executing cryptographicprotocols in Cyberlogic are demonstrated. Finally, a functional encryptionscheme is proposed for checking certificates of evidential transactions whenpolicies are kept private.", "output": "Evidential Transactions with Cyberlogic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training has been shown to be reliable in improving robustnessagainst adversarial samples. However, the problem of adversarial training interms of fairness has not yet been properly studied, and the relationshipbetween fairness and accuracy attack still remains unclear. Can wesimultaneously improve robustness w.r.t. both fairness and accuracy? To tacklethis topic, in this paper, we study the problem of adversarial training andadversarial attack w.r.t. both metrics. We propose a unified structure forfairness attack which brings together common notions in group fairness, and wetheoretically prove the equivalence of fairness attack against differentnotions. Moreover, we show the alignment of fairness and accuracy attack, andtheoretically demonstrate that robustness w.r.t. one metric benefits fromrobustness w.r.t. the other metric. Our study suggests a novel way to unifyadversarial training and attack w.r.t. fairness and accuracy, and experimentalresults show that our proposed method achieves better performance in terms ofrobustness w.r.t. both metrics.", "output": "To be Robust and to be Fair: Aligning Fairness with Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article provides a curated review of selected papers published inprominent economics journals that use machine learning (ML) tools for researchand policy analysis. The review focuses on three key questions: (1) when ML isused in economics, (2) what ML models are commonly preferred, and (3) how theyare used for economic applications. The review highlights that ML isparticularly used in processing nontraditional and unstructured data, capturingstrong nonlinearity, and improving prediction accuracy. Deep learning modelsare suitable for nontraditional data, whereas ensemble learning models arepreferred for traditional datasets. While traditional econometric models maysuffice for analyzing low-complexity data, the increasing complexity ofeconomic data due to rapid digitalization and the growing literature suggestthat ML is becoming an essential addition to the econometrician's toolbox.", "output": "Machine Learning for Economics Research: When What and How?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vector-based retrieval systems have become a common staple for academic andindustrial search applications because they provide a simple and scalable wayof extending the search to leverage contextual representations for documentsand queries. As these vector-based systems rely on contextual language models,their usage commonly requires GPUs, which can be expensive and difficult tomanage. Given recent advances in introducing sparsity into language models forimproved inference efficiency, in this paper, we study how sparse languagemodels can be used for dense retrieval to improve inference efficiency. Usingthe popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQAdatasets, we find that sparse language models can be used as directreplacements with little to no drop in accuracy and up to 4.3x improvedinference speeds", "output": "Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Background: Alzheimer's Disease (AD) is the most common type of age-relateddementia, affecting 6.2 million people aged 65 or older according to CDC data.It is commonly agreed that discovering an effective AD diagnosis biomarkercould have enormous public health benefits, potentially preventing or delayingup to 40% of dementia cases. Tau neurofibrillary tangles are the primary driverof downstream neurodegeneration and subsequent cognitive impairment in AD,resulting in structural deformations such as hippocampal atrophy that can beobserved in magnetic resonance imaging (MRI) scans. Objective: To build asurface-based model to 1) detect differences between APOE subgroups in patternsof tau deposition and hippocampal atrophy, and 2) use the extractedsurface-based features to predict cognitive decline. Methods: Using dataobtained from different institutions, we develop a surface-based federated Chowtest model to study the synergistic effects of APOE, a previously reportedsignificant risk factor of AD, and tau on hippocampal surface morphometry.Results: We illustrate that the APOE-specific morphometry features correlatewith AD progression and better predict future AD conversion than other MRIbiomarkers. For example, a strong association between atrophy and abnormal tauwas identified in hippocampal subregion cornu ammonis 1 (CA1 subfield) andsubiculum in e4 homozygote cohort. Conclusion: Our model allows for identifyingMRI biomarkers for AD and cognitive decline prediction and may uncover a cornerof the neural mechanism of the influence of APOE and tau deposition onhippocampal morphology.", "output": "A Surface-Based Federated Chow Test Model for Integrating APOE Status, Tau Deposition Measure, and Hippocampal Surface Morphometry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anatomical consistency in biomarker segmentation is crucial for many medicalimage analysis tasks. A promising paradigm for achieving anatomicallyconsistent segmentation via deep networks is incorporating pixel connectivity,a basic concept in digital topology, to model inter-pixel relationships.However, previous works on connectivity modeling have ignored the richchannel-wise directional information in the latent space. In this work, wedemonstrate that effective disentanglement of directional sub-space from theshared latent space can significantly enhance the feature representation in theconnectivity-based network. To this end, we propose a directional connectivitymodeling scheme for segmentation that decouples, tracks, and utilizes thedirectional information across the network. Experiments on various publicmedical image segmentation benchmarks show the effectiveness of our model ascompared to the state-of-the-art methods. Code is available at", "output": "Directional Connectivity-based Segmentation of Medical Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In end-to-end (E2E) speech recognition models, a representationaltight-coupling inevitably emerges between the encoder and the decoder. We buildupon recent work that has begun to explore building encoders with modularencoded representations, such that encoders and decoders from different modelscan be stitched together in a zero-shot manner without further fine-tuning.While previous research only addresses full-context speech models, we explorethe problem in a streaming setting as well. Our framework builds on top ofexisting encoded representations, converting them to modular features, dubbedas Lego-Features, without modifying the pre-trained model. The features remaininterchangeable when the model is retrained with distinct initializations.Though sparse, we show that the Lego-Features are powerful when tested withRNN-T or LAS decoders, maintaining high-quality downstream performance. Theyare also rich enough to represent the first-pass prediction during two-passdeliberation. In this scenario, they outperform the N-best hypotheses, sincethey do not need to be supplemented with acoustic features to deliver the bestresults. Moreover, generating the Lego-Features does not require beam search orauto-regressive computation. Overall, they present a modular, powerful andcheap alternative to the standard encoder output, as well as the N-besthypotheses.", "output": "Lego-Features: Exporting modular encoder features for streaming and deliberation ASR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To advance automated detection of extreme weather events, which areincreasing in frequency and intensity with climate change, we exploremodifications to a novel light-weight Context Guided convolutional neuralnetwork architecture trained for semantic segmentation of tropical cyclones andatmospheric rivers in climate data. Our primary focus is on tropical cyclones,the most destructive weather events, for which current models show limitedperformance. We investigate feature engineering, data augmentation, learningrate modifications, alternative loss functions, and architectural changes. Incontrast to previous approaches optimizing for intersection over union, wespecifically seek to improve recall to penalize under-counting and prioritizeidentification of tropical cyclones. We report success through the use ofweighted loss functions to counter class imbalance for these rare events. Weconclude with directions for future research on extreme weather eventsdetection, a crucial task for prediction, mitigation, and equitable adaptationto the impacts of climate change.", "output": "Improving extreme weather events detection with light-weight neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Response ranking in dialogues plays a crucial role in retrieval-basedconversational systems. In a multi-turn dialogue, to capture the gist of aconversation, contextual information serves as essential knowledge to achievethis goal. In this paper, we present a flexible neural framework that canintegrate contextual information from multiple channels. Specifically for thecurrent task, our approach is to provide two information channels in parallel,Fusing Conversation history and domain knowledge extracted from Candidateprovenance (FCC), where candidate responses are curated, as contextualinformation to improve the performance of multi-turn dialogue response ranking.The proposed approach can be generalized as a module to incorporatemiscellaneous contextual features for other context-oriented tasks. We evaluateour model on the MSDialog dataset widely used for evaluating conversationalresponse ranking tasks. Our experimental results show that our frameworksignificantly outperforms the previous state-of-the-art models, improvingRecall@1 by 7% and MAP by 4%. Furthermore, we conduct ablation studies toevaluate the contributions of each information channel, and of the frameworkcomponents, to the overall ranking performance, providing additional insightsand directions for further improvements.", "output": "FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep generative models have the capacity to render high fidelity images ofcontent like human faces. Recently, there has been substantial progress inconditionally generating images with specific quantitative attributes, like theemotion conveyed by one's face. These methods typically require a user toexplicitly quantify the desired intensity of a visual attribute. A limitationof this method is that many attributes, like how \"angry\" a human face looks,are difficult for a user to precisely quantify. However, a user would be ableto reliably say which of two faces seems \"angrier\". Following this premise, wedevelop the $textit{PrefGen}$ system, which allows users to control therelative attributes of generated images by presenting them with simple pairedcomparison queries of the form \"do you prefer image $a$ or image $b$?\" Usinginformation from a sequence of query responses, we can estimate userpreferences over a set of image attributes and perform preference-guided imageediting and generation. Furthermore, to make preference localization feasibleand efficient, we apply an active query selection strategy. We demonstrate thesuccess of this approach using a StyleGAN2 generator on the task of human faceediting. Additionally, we demonstrate how our approach can be combined withCLIP, allowing a user to edit the relative intensity of attributes specified bytext prompts. Code at ", "output": "PrefGen: Preference Guided Image Generation with Relative Attributes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent text-to-image generation models like DreamBooth have made remarkableprogress in generating highly customized images of a target subject, byfine-tuning an ``expert model'' for a given subject from a few examples.However, this process is expensive, since a new expert model must be learnedfor each subject. In this paper, we present SuTI, a Subject-drivenText-to-Image generator that replaces subject-specific fine tuning withemph{in-context} learning. Given a few demonstrations of a new subject, SuTIcan instantly generate novel renditions of the subject in different scenes,without any subject-specific optimization. SuTI is powered by {emapprenticeship learning}, where a single apprentice model is learned from datagenerated by massive amount of subject-specific expert models. Specifically, wemine millions of image clusters from the Internet, each centered around aspecific visual subject. We adopt these clusters to train massive amount ofexpert models specialized on different subjects. The apprentice model SuTI thenlearns to mimic the behavior of these experts through the proposedapprenticeship learning algorithm. SuTI can generate high-quality andcustomized subject-specific images 20x faster than optimization-based SoTAmethods. On the challenging DreamBench and DreamBench-v2, our human evaluationshows that SuTI can significantly outperform existing approaches likeInstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen whileperforming on par with DreamBooth.", "output": "Subject-driven Text-to-Image Generation via Apprenticeship Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In human spatial awareness, information appears to be represented accordingto 3-D projective geometry. It structures information integration and actionplanning within an internal representation space. The way different firstperson perspectives of an agent relate to each other, through transformationsof a world model, defines a specific perception scheme for the agent. Inmathematics, this collection of transformations is called a `group' and itcharacterizes a geometric space by acting on it. We propose that imbuing worldmodels with a `geometric' structure, given by a group, is one way to capturedifferent perception schemes of agents. We explore how changing the geometricstructure of a world model impacts the behavior of an agent.In particular, we focus on how such geometrical operations transform theformal expression of epistemic value in active inference as driving an agent'scuriosity about its environment, and impact exploration behaviors accordingly.We used group action as a special class of policies for perspective-dependentcontrol. We compared the Euclidean versus projective groups. We formallydemonstrate that the groups induce distinct behaviors. The projective groupinduces nonlinear contraction and dilatation that transform entropy andepistemic value as a function of the choice of frame, which fosters explorationbehaviors. This contribution opens research avenues in which a geometrystructures textit{a priori} an agent's internal representation space forinformation integration and action planning.", "output": "Action of the Euclidean versus Projective group on an agent's internal space in curiosity driven exploration: a formal analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traffic congestion has been a major challenge in many urban road networks.Extensive research studies have been conducted to highlight traffic-relatedcongestion and address the issue using data-driven approaches. Currently, mosttraffic congestion analyses are done using simulation software that offerslimited insight due to the limitations in the tools and utilities being used torender various traffic congestion scenarios. All that impacts the formulationof custom business problems which vary from place to place and country tocountry. By exploiting the power of the knowledge graph, we model a trafficcongestion problem into the Neo4j graph and then use the load balancing,optimization algorithm to identify congestion-free road networks. We also showhow traffic propagates backward in case of congestion or accident scenarios andits overall impact on other segments of the roads. We also train a sequentialRNN-LSTM (Long Short-Term Memory) deep learning model on the real-time trafficdata to assess the accuracy of simulation results based on a road-specificcongestion. Our results show that graph-based traffic simulation, supplementedby AI ML-based traffic prediction can be more effective in estimating thecongestion level in a road network.", "output": "Leveraging Neo4j and deep learning for traffic congestion simulation & optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Relation prediction on knowledge graphs (KGs) is a key research topic.Dominant embedding-based methods mainly focus on the transductive setting andlack the inductive ability to generalize to new entities for inference.Existing methods for inductive reasoning mostly mine the connections betweenentities, i.e., relational paths, without considering the nature of head andtail entities contained in the relational context. This paper proposes a novelmethod that captures both connections between entities and the intrinsic natureof entities, by simultaneously aggregating RElational Paths and cOntext with aunified hieRarchical Transformer framework, namely REPORT. REPORT relies solelyon relation semantics and can naturally generalize to the fully-inductivesetting, where KGs for training and inference have no common entities. In theexperiments, REPORT performs consistently better than all baselines on almostall the eight version subsets of two fully-inductive datasets. Moreover. REPORTis interpretable by providing each element's contribution to the predictionresults.", "output": "Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Analyzing high resolution whole slide images (WSIs) with regard toinformation across multiple scales poses a significant challenge in digitalpathology. Multi-instance learning (MIL) is a common solution for working withhigh resolution images by classifying bags of objects (i.e. sets of smallerimage patches). However, such processing is typically performed at a singlescale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scaleinformation that is key to diagnoses by human pathologists. In this study, wepropose a novel cross-scale MIL algorithm to explicitly aggregate inter-scalerelationships into a single MIL network for pathological image diagnosis. Thecontribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL)algorithm that integrates the multi-scale information and the inter-scalerelationships is proposed; (2) A toy dataset with scale-specific morphologicalfeatures is created and released to examine and visualize differentialcross-scale attention; (3) Superior performance on both in-house and publicdatasets is demonstrated by our simple cross-scale MIL strategy. The officialimplementation is publicly available at ", "output": "Cross-scale Multi-instance Learning for Pathological Image Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper shows that Masking the Deep hierarchical features is an efficientself-supervised method, denoted as MaskDeep. MaskDeep treats each patch in therepresentation space as an independent instance. We mask part of patches in therepresentation space and then utilize sparse visible patches to reconstructhigh semantic image representation. The intuition of MaskDeep lies in the factthat models can reason from sparse visible patches semantic to the globalsemantic of the image. We further propose three designs in our framework: 1) aHierarchical Deep-Masking module to concern the hierarchical property of patchrepresentations, 2) a multi-group strategy to improve the efficiency withoutany extra computing consumption of the encoder and 3) a multi-target strategyto provide more description of the global semantic. Our MaskDeep brings decentimprovements. Trained on ResNet50 with 200 epochs, MaskDeep achievesstate-of-the-art results of 71.2% Top1 accuracy linear classification onImageNet. On COCO object detection tasks, MaskDeep outperforms theself-supervised method SoCo, which specifically designed for object detection.When trained with 100 epochs, MaskDeep achieves 69.6% Top1 accuracy, whichsurpasses current methods trained with 200 epochs, such as HCSC, by 0.4% .", "output": "Mask Hierarchical Features For Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of learning in a non-stationary reinforcementlearning (RL) environment, where the setting can be fully described by apiecewise stationary discrete-time Markov decision process (MDP). We introducea variant of the Restarted Bayesian Online Change-Point Detection algorithm(R-BOCPD) that operates on input streams originating from the more generalmultinomial distribution and provides near-optimal theoretical guarantees interms of false-alarm rate and detection delay. Based on this, we propose animproved version of the UCRL2 algorithm for MDPs with state transition kernelsampled from a multinomial distribution, which we call R-BOCPD-UCRL2. Weperform a finite-time performance analysis and show that R-BOCPD-UCRL2 enjoys afavorable regret bound of $Oleft(D O sqrt{A T K_T logleft (frac{T}{delta}right) + frac{K_T log frac{K_T}{delta}}{minlimits_ell :mathbf{KL}left({mathbf{theta}^{(ell+1)}}midmid{mathbf{theta}^{(ell)}}right)}}right)$,where $D$ is the largest MDP diameter from the set of MDPs defining thepiecewise stationary MDP setting, $O$ is the finite number of states (constantover all changes), $A$ is the finite number of actions (constant over allchanges), $K_T$ is the number of change points up to horizon $T$, and$mathbf{theta}^{(ell)}$ is the transition kernel during the interval$[c_ell, c_{ell+1})$, which we assume to be multinomially distributed overthe set of states $mathbb{O}$. Interestingly, the performance bound does notdirectly scale with the variation in MDP state transition distributions andrewards, ie. can also model abrupt changes. In practice, R-BOCPD-UCRL2outperforms the state-of-the-art in a variety of scenarios in syntheticenvironments. We provide a detailed experimental setup along with a coderepository (upon publication) that can be used to easily reproduce ourexperiments.", "output": "Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training deep neural network (DNN) models, which has become an important taskin today's software development, is often costly in terms of computationalresources and time. With the inspiration of software reuse, building DNN modelsthrough reusing existing ones has gained increasing attention recently. Priorapproaches to DNN model reuse have two main limitations: 1) reusing the entiremodel, while only a small part of the model's functionalities (labels) arerequired, would cause much overhead (e.g., computational and time costs forinference), and 2) model reuse would inherit the defects and weaknesses of thereused model, and hence put the new system under threats of security attack. Tosolve the above problem, we propose SeaM, a tool that re-engineers a trainedDNN model to improve its reusability. Specifically, given a target problem anda trained model, SeaM utilizes a gradient-based search method to search for themodel's weights that are relevant to the target problem. The re-engineeredmodel that only retains the relevant weights is then reused to solve the targetproblem. Evaluation results on widely-used models show that the re-engineeredmodels produced by SeaM only contain 10.11% weights of the original models,resulting 42.41% reduction in terms of inference time. For the target problem,the re-engineered models even outperform the original models in classificationaccuracy by 5.85%. Moreover, reusing the re-engineered models inherits anaverage of 57% fewer defects than reusing the entire model. We believe ourapproach to reducing reuse overhead and defect inheritance is one importantstep forward for practical model reuse.", "output": "Reusing Deep Neural Network Models through Model Re-engineering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stroke is the second leading cause of death worldwide. Machine learningclassification algorithms have been widely adopted for stroke prediction.However, these algorithms were evaluated using different datasets andevaluation metrics. Moreover, there is no comprehensive framework for strokedata analytics. This paper proposes an intelligent stroke prediction frameworkbased on a critical examination of machine learning prediction algorithms inthe literature. The five most used machine learning algorithms for strokeprediction are evaluated using a unified setup for objective comparison.Comparative analysis and numerical results reveal that the Random Forestalgorithm is best suited for stroke prediction.", "output": "From Conception to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and Performance Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A backdoor attack allows a malicious user to manipulate the environment orcorrupt the training data, thus inserting a backdoor into the trained agent.Such attacks compromise the RL system's reliability, leading to potentiallycatastrophic results in various key fields. In contrast, relatively limitedresearch has investigated effective defenses against backdoor attacks in RL.This paper proposes the Recovery Triggered States (RTS) method, a novelapproach that effectively protects the victim agents from backdoor attacks. RTSinvolves building a surrogate network to approximate the dynamics model.Developers can then recover the environment from the triggered state to a cleanstate, thereby preventing attackers from activating backdoors hidden in theagent by presenting the trigger. When training the surrogate to predict states,we incorporate agent action information to reduce the discrepancy between theactions taken by the agent on predicted states and the actions taken on realstates. RTS is the first approach to defend against backdoor attacks in asingle-agent setting. Our results show that using RTS, the cumulative rewardonly decreased by 1.41% under the backdoor attack.", "output": "Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As an intrinsic and fundamental property of big data, data heterogeneityexists in a variety of real-world applications, such as precision medicine,autonomous driving, financial applications, etc. For machine learningalgorithms, the ignorance of data heterogeneity will greatly hurt thegeneralization performance and the algorithmic fairness, since the predictionmechanisms among different sub-populations are likely to differ from eachother. In this work, we focus on the data heterogeneity that affects theprediction of machine learning models, and firstly propose the emph{usablepredictive heterogeneity}, which takes into account the model capacity andcomputational constraints. We prove that it can be reliably estimated fromfinite data with probably approximately correct (PAC) bounds. Additionally, wedesign a bi-level optimization algorithm to explore the usable predictiveheterogeneity from data. Empirically, the explored heterogeneity providesinsights for sub-population divisions in income prediction, crop yieldprediction and image classification tasks, and leveraging such heterogeneitybenefits the out-of-distribution generalization performance.", "output": "Predictive Heterogeneity: Measures and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Random label noises (or observational noises) widely exist in practicalmachine learning settings. While previous studies primarily focus on theaffects of label noises to the performance of learning, our work intends toinvestigate the implicit regularization effects of the label noises, undermini-batch sampling settings of stochastic gradient descent (SGD), withassumptions that label noises are unbiased. Specifically, we analyze thelearning dynamics of SGD over the quadratic loss with unbiased label noises,where we model the dynamics of SGD as a stochastic differentiable equation(SDE) with two diffusion terms (namely a Doubly Stochastic Model). While thefirst diffusion term is caused by mini-batch sampling over the(label-noiseless) loss gradients as many other works on SGD, our modelinvestigates the second noise term of SGD dynamics, which is caused bymini-batch sampling over the label noises, as an implicit regularizer. Ourtheoretical analysis finds such implicit regularizer would favor someconvergence points that could stabilize model outputs against perturbation ofparameters (namely inference stability). Though similar phenomenon have beeninvestigated, our work doesn't assume SGD as an Ornstein-Uhlenbeck like processand achieve a more generalizable result with convergence of approximationproved. To validate our analysis, we design two sets of empirical studies toanalyze the implicit regularizer of SGD with unbiased random label noises fordeep neural networks training and linear regression.", "output": "Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern robotics often involves multiple embodied agents operating within ashared environment. Path planning in these cases is considerably morechallenging than in single-agent scenarios. Although standard Sampling-basedAlgorithms (SBAs) can be used to search for solutions in the robots' jointspace, this approach quickly becomes computationally intractable as the numberof agents increases. To address this issue, we integrate the concept offactorization into sampling-based algorithms, which requires only minimalmodifications to existing methods. During the search for a solution we candecouple (i.e., factorize) different subsets of agents into independentlower-dimensional search spaces once we certify that their future solutionswill be independent of each other using a factorization heuristic.Consequently, we progressively construct a lean hypergraph where certain(hyper-)edges split the agents to independent subgraphs. In the best case, thisapproach can reduce the growth in dimensionality of the search space fromexponential to linear in the number of agents. On average, fewer samples areneeded to find high-quality solutions while preserving the optimality,completeness, and anytime properties of SBAs. We present a generalimplementation of a factorized SBA, derive an analytical gain in terms ofsample complexity for PRM*, and showcase empirical results for RRG.", "output": "Factorization of Multi-Agent Sampling-Based Motion Planning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline meta reinforcement learning (OMRL) aims to learn transferrableknowledge from offline datasets to facilitate the learning process for newtarget tasks. Context-based RL employs a context encoder to rapidly adapt theagent to new tasks by inferring about the task representation, and thenadjusting the acting policy based on the inferred task representation. Here weconsider context-based OMRL, in particular, the issue of task representationlearning for OMRL. We empirically demonstrate that the context encoder trainedon offline datasets could suffer from distribution shift between the contextsused for training and testing. To tackle this issue, we propose a hard samplingbased strategy for learning a robust task context encoder. Experimentalresults, based on distinct continuous control tasks, demonstrate that theutilization of our technique results in more robust task representations andbetter testing performance in terms of accumulated returns, compared withbaseline methods. Our code is available at", "output": "On Context Distribution Shift in Task Representation Learning for Offline Meta RL."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although pair trading is the simplest hedging strategy for an investor toeliminate market risk, it is still a great challenge for reinforcement learning(RL) methods to perform pair trading as human expertise. It requires RL methodsto make thousands of correct actions that nevertheless have no obviousrelations to the overall trading profit, and to reason over infinite states ofthe time-varying market most of which have never appeared in history. However,existing RL methods ignore the temporal connections between asset pricemovements and the risk of the performed trading. These lead to frequenttradings with high transaction costs and potential losses, which barely reachthe human expertise level of trading. Therefore, we introduce CREDIT, arisk-aware agent capable of learning to exploit long-term trading opportunitiesin pair trading similar to a human expert. CREDIT is the first to applybidirectional GRU along with the temporal attention mechanism to fully considerthe temporal correlations embedded in the states, which allows CREDIT tocapture long-term patterns of the price movements of two assets to earn higherprofit. We also design the risk-aware reward inspired by the economic theory,that models both the profit and risk of the tradings during the trading period.It helps our agent to master pair trading with a robust trading preference thatavoids risky trading with possible high returns and losses. Experiments showthat it outperforms existing reinforcement learning methods in pair trading andachieves a significant profit over five years of U.S. stock data.", "output": "Mastering Pair Trading with Risk-Aware Recurrent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncovering potential failure cases is a crucial step in the validation ofsafety critical systems such as autonomous vehicles. Failure search may be donethrough logging substantial vehicle miles in either simulation or real worldtesting. Due to the sparsity of failure events, naive random search approachesrequire significant amounts of vehicle operation hours to find potential systemweaknesses. As a result, adaptive searching techniques have been proposed toefficiently explore and uncover failure trajectories of an autonomous policy insimulation. Adaptive Stress Testing (AST) is one such method that poses theproblem of failure search as a Markov decision process and uses reinforcementlearning techniques to find high probability failures. However, thisformulation requires a probability model for the actions of all agents in theenvironment. In systems where the environment actions are discrete anddependencies among agents exist, it may be infeasible to fully characterize thedistribution or find a suitable proxy. This work proposes the use of a datadriven approach to learn a suitable classifier that tries to model how humansidentify {critical states and use this to guide failure search in AST. We showthat the incorporation of critical states into the AST framework generatesfailure scenarios with increased safety violations in an autonomous drivingpolicy with a discrete action space.", "output": "Adaptive Failure Search Using Critical States from Domain Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As advances in artificial intelligence enable increasingly capablelearning-based autonomous agents, it becomes more challenging for humanobservers to efficiently construct a mental model of the agent's behaviour. Inorder to successfully deploy autonomous agents, humans should not only be ableto understand the individual limitations of the agents but also have insight onhow they compare against one another. To do so, we need effective methods forgenerating human interpretable agent behaviour summaries. Single agentbehaviour summarization has been tackled in the past through methods thatgenerate explanations for why an agent chose to pick a particular action at asingle timestep. However, for complex tasks, a per-action explanation may notbe able to convey an agents global strategy. As a result, researchers havelooked towards multi-timestep summaries which can better help humans assess anagents overall capability. More recently, multi-step summaries have also beenused for generating contrasting examples to evaluate multiple agents. However,past approaches have largely relied on unstructured search methods to generatesummaries and require agents to have a discrete action space. In this paper wepresent an adaptive search method for efficiently generating contrastingbehaviour summaries with support for continuous state and action spaces. Weperform a user study to evaluate the effectiveness of the summaries for helpinghumans discern the superior autonomous agent for a given task. Our resultsindicate that adaptive search can efficiently identify informative contrastingscenarios that enable humans to accurately select the better performing agentwith a limited observation time budget.", "output": "Conveying Autonomous Robot Capabilities through Contrasting Behaviour Summaries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In computing, the aim of personalization is to train a model that caters to aspecific individual or group of people by optimizing one or more performancemetrics and adhering to specific constraints. In this paper, we discuss theneed for personalization in affective and personality computing (hereinafterreferred to as affective computing). We present a survey of state-of-the-artapproaches for personalization in affective computing. Our review spanstraining techniques and objectives towards the personalization of affectivecomputing models. We group existing approaches into seven categories: (1)Target-specific Models, (2) Group-specific Models, (3) Weighting-basedApproaches, (4) Fine-tuning Approaches, (5) Multitask Learning, (6)Generative-based Models, and (7) Feature Augmentation. Additionally, we providea statistical meta-analysis of the surveyed literature, analyzing theprevalence of different affective computing tasks, interaction modes,interaction contexts, and the level of personalization among the surveyedworks. Based on that, we provide a road-map for those who are interested inexploring this direction.", "output": "A Survey on Personalized Affective Computing in Human-Machine Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The cascade of 2D geometric transformations were exploited to model relationsbetween entities in a knowledge graph (KG), leading to an effective KGembedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space wasproposed as a new KGE model, Rotate3D, by leveraging its non-commutativeproperty. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometrictransformations, including translation, rotation, scaling, reflection, andshear and propose a family of KGE models, named CompoundE3D, in this work.CompoundE3D allows multiple design variants to match rich underlyingcharacteristics of a KG. Since each variant has its own advantages on a subsetof relations, an ensemble of multiple variants can yield superior performance.The effectiveness and flexibility of CompoundE3D are experimentally verified onfour popular link prediction datasets.", "output": "Knowledge Graph Embedding with 3D Compound Geometric Transformations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Highway merging scenarios featuring mixed traffic conditions pose significantmodeling and control challenges for connected and automated vehicles (CAVs)interacting with incoming on-ramp human-driven vehicles (HDVs). In this paper,we present an approach to learn an approximate information state model ofCAV-HDV interactions for a CAV to maneuver safely during highway merging. Inour approach, the CAV learns the behavior of an incoming HDV using approximateinformation states before generating a control strategy to facilitate merging.First, we validate the efficacy of this framework on real-world data by usingit to predict the behavior of an HDV in mixed traffic situations extracted fromthe Next-Generation Simulation repository. Then, we generate simulation datafor HDV-CAV interactions in a highway merging scenario using a standard inversereinforcement learning approach. Without assuming a prior knowledge of thegenerating model, we show that our approximate information state model learnsto predict the future trajectory of the HDV using only observations.Subsequently, we generate safe control policies for a CAV while merging withHDVs, demonstrating a spectrum of driving behaviors, from aggressive toconservative. We demonstrate the effectiveness of the proposed approach byperforming numerical simulations.", "output": "Connected and Automated Vehicles in Mixed-Traffic: Learning Human Driver Behavior for Effective On-Ramp Merging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Becoming a (super) hero is almost every kid's dream. During their shelteredchildhood, they do whatever it takes to grow up to be one. Work hard, play hard-- all day long. But as they're getting older, distractions are more and morelikely to occur. They're getting off track. They start discovering what isfeared as simple math. Finally, they end up as a researcher, writing boring,non-impressive papers all day long because they only rely on simplemathematics. No top-tier conferences, no respect, no groupies. Life's over.To finally put an end to this tragedy, we propose a fundamentally newalgorithm, dubbed zero2hero, that turns every research paper into a scientificmasterpiece. Given a LaTeX document containing ridiculously simple math, basedon next-generation large language models, our system automaticallyover-complicates every single equation so that no one, including yourself, isable to understand what the hell is going on. Future reviewers will be blownaway by the complexity of your equations, immediately leading to acceptance.zero2hero gets you back on track, because you deserve to be ahero$^{text{TM}}$. Code leaked aturl{", "output": "From Zero to Hero: Convincing with Extremely Complicated Math."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose and release a new vulnerable source code dataset. We curate thedataset by crawling security issue websites, extracting vulnerability-fixingcommits and source codes from the corresponding projects. Our new datasetcontains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerablefunctions extracted from 7,861 commits. Our dataset covers 305 more projectsthan all previous datasets combined. We show that increasing the diversity andvolume of training data improves the performance of deep learning models forvulnerability detection.Combining our new dataset with previous datasets, we present an analysis ofthe challenges and promising research directions of using deep learning fordetecting software vulnerabilities. We study 11 model architectures belongingto 4 families. Our results show that deep learning is still not ready forvulnerability detection, due to high false positive rate, low F1 score, anddifficulty of detecting hard CWEs. In particular, we demonstrate an importantgeneralization challenge for the deployment of deep learning-based models.However, we also identify hopeful future research directions. We demonstratethat large language models (LLMs) are the future for vulnerability detection,outperforming Graph Neural Networks (GNNs) with manual feature engineering.Moreover, developing source code specific pre-training objectives is apromising research direction to improve the vulnerability detectionperformance.", "output": "DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in large language models (LLMs) have led to the developmentof powerful AI chatbots capable of engaging in natural and human-likeconversations. However, these chatbots can be potentially harmful, exhibitingmanipulative, gaslighting, and narcissistic behaviors. We define Healthy AI tobe safe, trustworthy and ethical. To create healthy AI systems, we present theSafeguardGPT framework that uses psychotherapy to correct for these harmfulbehaviors in AI chatbots. The framework involves four types of AI agents: aChatbot, a \"User,\" a \"Therapist,\" and a \"Critic.\" We demonstrate theeffectiveness of SafeguardGPT through a working example of simulating a socialconversation. Our results show that the framework can improve the quality ofconversations between AI chatbots and humans. Although there are still severalchallenges and directions to be addressed in the future, SafeguardGPT providesa promising approach to improving the alignment between AI chatbots and humanvalues. By incorporating psychotherapy and reinforcement learning techniques,the framework enables AI chatbots to learn and adapt to human preferences andvalues in a safe and ethical way, contributing to the development of a morehuman-centric and responsible AI.", "output": "Towards Healthy AI: Large Language Models Need Therapists Too."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We answer the question: \"Does local progress (on batches) imply globalprogress (on the entire dataset) for mini-batch $k$-means?\". Specifically, weconsider mini-batch $k$-means which terminates only when the improvement in thequality of the clustering on the sampled batch is below some threshold.Although at first glance it appears that this algorithm might executeforever, we answer the above question in the affirmative and show that if thebatch is of size $tilde{Omega}((d/epsilon)^2)$, it must terminate within$O(d/epsilon)$ iterations with high probability, where $d$ is the dimension ofthe input, and $epsilon$ is a threshold parameter for termination. This istrue regardless of how the centers are initialized. When the algorithm isinitialized with the $k$-means++ initialization scheme, it achieves anapproximation ratio of $O(log k)$ (the same as the full-batch version).Finally, we show the applicability of our results to the mini-batch $k$-meansalgorithm implemented in the scikit-learn (sklearn) python library.", "output": "Mini-batch $k$-means terminates within $O(d/\\epsilon)$ iterations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Malicious perturbations embedded in input data, known as Trojan attacks, cancause neural networks to misbehave. However, the impact of a Trojan attack isreduced during fine-tuning of the model, which involves transferring knowledgefrom a pretrained large-scale model like visual question answering (VQA) to thetarget model. To mitigate the effects of a Trojan attack, replacing andfine-tuning multiple layers of the pretrained model is possible. This researchfocuses on sample efficiency, stealthiness and variation, and robustness tomodel fine-tuning. To address these challenges, we propose an instance-levelTrojan attack that generates diverse Trojans across input samples andmodalities. Adversarial learning establishes a correlation between a specifiedperturbation layer and the misbehavior of the fine-tuned model. We conductedextensive experiments on the VQA-v2 dataset using a range of metrics. Theresults show that our proposed method can effectively adapt to a fine-tunedmodel with minimal samples. Specifically, we found that a model with a singlefine-tuning layer can be compromised using a single shot of adversarialsamples, while a model with more fine-tuning layers can be compromised usingonly a few shots.", "output": "Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Macro-management is an important problem in StarCraft, which has been studiedfor a long time. Various datasets together with assorted methods have beenproposed in the last few years. But these datasets have some defects forboosting the academic and industrial research: 1) There're neither standardpreprocessing, parsing and feature extraction procedures nor predefinedtraining, validation and test set in some datasets. 2) Some datasets are onlyspecified for certain tasks in macro-management. 3) Some datasets are eithertoo small or don't have enough labeled data for modern machine learningalgorithms such as deep neural networks. So most previous methods are trainedwith various features, evaluated on different test sets from the same ordifferent datasets, making it difficult to be compared directly. To boost theresearch of macro-management in StarCraft, we release a new dataset MSC basedon the platform SC2LE. MSC consists of well-designed feature vectors,pre-defined high-level actions and final result of each match. We also splitMSC into training, validation and test set for the convenience of evaluationand comparison. Besides the dataset, we propose a baseline model and presentinitial baseline results for global state evaluation and build orderprediction, which are two of the key tasks in macro-management. Variousdownstream tasks and analyses of the dataset are also described for the sake ofresearch on macro-management in StarCraft II. Homepage:", "output": "MSC: A Dataset for Macro-Management in StarCraft II."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As progress in AI continues to advance, it is important to know how advancedsystems will make choices and in what ways they may fail. Machines can alreadyoutsmart humans in some domains, and understanding how to safely build oneswhich may have capabilities at or above the human level is of particularconcern. One might suspect that artificially generally intelligent (AGI) andartificially superintelligent (ASI) will be systems that humans cannot reliablyoutsmart. As a challenge to this assumption, this paper presents the AchillesHeel hypothesis which states that even a potentially superintelligent systemmay nonetheless have stable decision-theoretic delusions which cause them tomake irrational decisions in adversarial settings. In a survey of key dilemmasand paradoxes from the decision theory literature, a number of these potentialAchilles Heels are discussed in context of this hypothesis. Several novelcontributions are made toward understanding the ways in which these weaknessesmight be implanted into a system.", "output": "Achilles Heels for AGI/ASI via Decision Theoretic Adversaries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The stochastic approximation (SA) algorithm is a widely used probabilisticmethod for finding a zero or a fixed point of a vector-valued funtion, whenonly noisy measurements of the function are available. In the literature todate, one makes a distinction between ``synchronous'' updating, whereby everycomponent of the current guess is updated at each time, and ``asynchronous''updating, whereby only one component is updated. In this paper, we study anintermediate situation that we call ``batch asynchronous stochasticapproximation'' (BASA), in which, at each time instant, textit{some but notall} components of the current estimated solution are updated. BASA allows theuser to trade off memory requirements against time complexity. We develop ageneral methodology for proving that such algorithms converge to the fixedpoint of the map under study. These convergence proofs make use of weakerhypotheses than existing results. Specifically, existing convergence proofsrequire that the measurement noise is a zero-mean i.i.d sequence or amartingale difference sequence. In the present paper, we permit biasedmeasurements, that is, measurement noises that have nonzero conditional mean.Also, all convergence results to date assume that the stochastic step sizessatisfy a probabilistic analog of the well-known Robbins-Monro conditions. Wereplace this assumption by a purely deterministic condition on theirreducibility of the underlying Markov processes.As specific applications to Reinforcement Learning, we analyze the temporaldifference algorithm $TD(lambda)$ for value iteration, and the $Q$-learningalgorithm for finding the optimal action-value function. In both cases, weestablish the convergence of these algorithms, under milder conditions than inthe existing literature.", "output": "Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in Federated Learning (FL) have brought large-scalecollaborative machine learning opportunities for massively distributed clientswith performance and data privacy guarantees. However, most current works focuson the interest of the central controller in FL,and overlook the interests ofthe FL clients. This may result in unfair treatment of clients that discouragesthem from actively participating in the learning process and damages thesustainability of the FL ecosystem. Therefore, the topic of ensuring fairnessin FL is attracting a great deal of research interest. In recent years, diverseFairness-Aware FL (FAFL) approaches have been proposed in an effort to achievefairness in FL from different perspectives. However, there is no comprehensivesurvey that helps readers gain insight into this interdisciplinary field. Thispaper aims to provide such a survey. By examining the fundamental andsimplifying assumptions, as well as the notions of fairness adopted by existingliterature in this field, we propose a taxonomy of FAFL approaches coveringmajor steps in FL, including client selection, optimization, contributionevaluation and incentive distribution. In addition, we discuss the main metricsfor experimentally evaluating the performance of FAFL approaches, and suggestpromising future research directions towards FAFL.", "output": "Towards Fairness-Aware Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, as more and more systems achieve good performance in traditionalvoice conversion (VC) tasks, people's attention gradually turns to VC tasksunder extreme conditions. In this paper, we propose a novel method forzero-shot voice conversion. We aim to obtain intermediate representations forspeaker-content disentanglement of speech to better remove speaker informationand get pure content information. Accordingly, our proposed framework containsa module that removes the speaker information from the acoustic feature of thesource speaker. Moreover, speaker information control is added to our system tomaintain the voice cloning performance. The proposed system is evaluated bysubjective and objective metrics. Results show that our proposed systemsignificantly reduces the trade-off problem in zero-shot voice conversion,while it also manages to have high spoofing power to the speaker verificationsystem.", "output": "SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper aims to illustrate the concept-emerging phenomenon in a trainedDNN. Specifically, we find that the inference score of a DNN can bedisentangled into the effects of a few interactive concepts. These concepts canbe understood as causal patterns in a sparse, symbolic causal graph, whichexplains the DNN. The faithfulness of using such a causal graph to explain theDNN is theoretically guaranteed, because we prove that the causal graph canwell mimic the DNN's outputs on an exponential number of different maskedsamples. Besides, such a causal graph can be further simplified and re-writtenas an And-Or graph (AOG), without losing much explanation accuracy.", "output": "Defining and Quantifying the Emergence of Sparse Concepts in DNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fuzzy inference engine, as one of the most important components of fuzzysystems, can obtain some meaningful outputs from fuzzy sets on input space andfuzzy rule base using fuzzy logic inference methods. In order to enhance thecomputational efficiency of fuzzy inference engine inmulti-input-single-output(MISO) fuzzy systems,this paper aims mainly toinvestigate three MISO fuzzy hierarchial inference engines based on fuzzyimplications satisfying the law of importation with aggregation functions(LIA). We firstly find some aggregation functions for well-known fuzzyimplications such that they satisfy (LIA). For a given aggregation function,the fuzzy implication which satisfies (LIA) with this aggregation function isthen characterized. Finally, we construct three fuzzy hierarchical inferenceengines in MISO fuzzy systems applying aforementioned theoretical developments.", "output": "MISO hierarchical inference engine satisfying the law of importation with aggregation functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper resolves the open question of designing near-optimal algorithmsfor learning imperfect-information extensive-form games from bandit feedback.We present the first line of algorithms that require only$widetilde{mathcal{O}}((XA+YB)/varepsilon^2)$ episodes of play to find an$varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where$X,Y$ are the number of information sets and $A,B$ are the number of actionsfor the two players. This improves upon the best known sample complexity of$widetilde{mathcal{O}}((X^2A+Y^2B)/varepsilon^2)$ by a factor of$widetilde{mathcal{O}}(max{X, Y})$, and matches the information-theoreticlower bound up to logarithmic factors. We achieve this sample complexity by twonew algorithms: Balanced Online Mirror Descent, and Balanced CounterfactualRegret Minimization. Both algorithms rely on novel approaches of integratingemph{balanced exploration policies} into their classical counterparts. We alsoextend our results to learning Coarse Correlated Equilibria in multi-playergeneral-sum games.", "output": "Near-Optimal Learning of Extensive-Form Games with Imperfect Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While developments in machine learning led to impressive performance gains onbig data, many human subjects data are, in actuality, small and sparselylabeled. Existing methods applied to such data often do not easily generalizeto out-of-sample subjects. Instead, models must make predictions on test datathat may be drawn from a different distribution, a problem known astextit{zero-shot learning}. To address this challenge, we develop anend-to-end framework using a meta-learning approach, which enables the model torapidly adapt to a new prediction task with limited training data forout-of-sample test data. We use three real-world small-scale human subjectsdatasets (two randomized control studies and one observational study), forwhich we predict treatment outcomes for held-out treatment groups. Our modellearns the latent treatment effects of each intervention and, by design, cannaturally handle multi-task predictions. We show that our model performs thebest holistically for each held-out group and especially when the test group isdistinctly different from the training group. Our model has implications forimproved generalization of small-size human studies to the wider population.", "output": "Zero-shot meta-learning for small-scale data from human subjects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-and-language (V&amp;L) models pretrained on large-scale multimodal datahave demonstrated strong performance on various tasks such as image captioningand visual question answering (VQA). The quality of such models is commonlyassessed by measuring their performance on unseen data that typically comesfrom the same distribution as the training data. However, when evaluated underout-of-distribution (out-of-dataset) settings for VQA, we observe that thesemodels exhibit poor generalization. We comprehensively evaluate two pretrainedV&amp;L models under different settings (i.e. classification and open-ended textgeneration) by conducting cross-dataset evaluations. We find that these modelstend to learn to solve the benchmark, rather than learning the high-levelskills required by the VQA task. We also find that in most cases generativemodels are less susceptible to shifts in data distribution compared todiscriminative ones, and that multimodal pretraining is generally helpful forOOD generalization. Finally, we revisit assumptions underlying the use ofautomatic VQA evaluation metrics, and empirically show that their stringentnature repeatedly penalizes models for correct responses.", "output": "Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models have become the core architecture upon which mostmodern natural language processing (NLP) systems build. These models canconsistently deliver impressive accuracy and robustness across tasks anddomains, but their high computational overhead can make inference difficult andexpensive. To make using these models less costly, recent work has exploredleveraging structured and unstructured pruning, quantization, and distillationto improve inference speed and decrease size. This paper studies how modelspruned using Gradual Unstructured Magnitude Pruning can transfer betweendomains and tasks. Our experimentation shows that models that are pruned duringpretraining using general domain masked language models can transfer to noveldomains and tasks without extensive hyperparameter exploration or specializedapproaches. We demonstrate that our general sparse model Sparse*BERT can becomeSparseBioBERT simply by pretraining the compressed architecture on unstructuredbiomedical text. Moreover, we show that SparseBioBERT can match the quality ofBioBERT with only 10% of the parameters.", "output": "Sparse*BERT: Sparse Models Generalize To New tasks and Domains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Studies on benign overfitting provide insights for the success ofoverparameterized deep learning models. In this work, we examine whetheroverfitting is truly benign in real-world classification tasks. We start withthe observation that a ResNet model overfits benignly on Cifar10 but notbenignly on ImageNet. To understand why benign overfitting fails in theImageNet experiment, we theoretically analyze benign overfitting under a morerestrictive setup where the number of parameters is not significantly largerthan the number of data points. Under this mild overparameterization setup, ouranalysis identifies a phase change: unlike in the previous heavyoverparameterization settings, benign overfitting can now fail in the presenceof label noise. Our analysis explains our empirical observations, and isvalidated by a set of control experiments with ResNets. Our work highlights theimportance of understanding implicit bias in underfitting regimes as a futuredirection.", "output": "Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spiking neural networks (SNNs) have demonstrated excellent capabilities invarious intelligent scenarios. Most existing methods for training SNNs arebased on the concept of synaptic plasticity; however, learning in the realisticbrain also utilizes intrinsic non-synaptic mechanisms of neurons. The spikethreshold of biological neurons is a critical intrinsic neuronal feature thatexhibits rich dynamics on a millisecond timescale and has been proposed as anunderlying mechanism that facilitates neural information processing. In thisstudy, we develop a novel synergistic learning approach that involvessimultaneously training synaptic weights and spike thresholds in SNNs. SNNstrained with synapse-threshold synergistic learning~(STL-SNNs) achievesignificantly superior performance on various static and neuromorphic datasetsthan SNNs trained with two degenerated single-learning models. During training,the synergistic learning approach optimizes neural thresholds, providing thenetwork with stable signal transmission via appropriate firing rates. Furtheranalysis indicates that STL-SNNs are robust to noisy data and exhibit lowenergy consumption for deep network structures. Additionally, the performanceof STL-SNN can be further improved by introducing a generalized joint decisionframework. Overall, our findings indicate that biologically plausible synergiesbetween synaptic and intrinsic non-synaptic mechanisms may provide a promisingapproach for developing highly efficient SNN learning methods.", "output": "A Synapse-Threshold Synergistic Learning Approach for Spiking Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Measuring semantic similarity between job titles is an essentialfunctionality for automatic job recommendations. This task is usuallyapproached using supervised learning techniques, which requires training datain the form of equivalent job title pairs. In this paper, we instead propose anunsupervised representation learning method for training a job title similaritymodel using noisy skill labels. We show that it is highly effective for taskssuch as text ranking and job normalization.", "output": "Learning Job Titles Similarity from Noisy Skill Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Following work on joint object-action representations, functionalobject-oriented networks (FOON) were introduced as a knowledge graphrepresentation for robots. A FOON contains symbolic concepts useful to arobot's understanding of tasks and its environment for object-level planning.Prior to this work, little has been done to show how plans acquired from FOONcan be executed by a robot, as the concepts in a FOON are too abstract forexecution. We thereby introduce the idea of exploiting object-level knowledgeas a FOON for task planning and execution. Our approach automaticallytransforms FOON into PDDL and leverages off-the-shelf planners, actioncontexts, and robot skills in a hierarchical planning pipeline to generateexecutable task plans. We demonstrate our entire approach on long-horizon tasksin CoppeliaSim and show how learned action contexts can be extended tonever-before-seen scenarios.", "output": "Long-Horizon Planning and Execution with Functional Object-Oriented Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we will provide a quantitative analysis of a simple model ofthe Federal Disaster Assistance policy from the viewpoint of three differentstakeholders. This quantitative methodology is new and has applications toother areas such as business and healthcare processes. The stakeholders areinterested in process transparency but each has a different opinion onprecisely what constitutes transparency. We will also consider threemodifications to the Federal Disaster Assistance policy and analyse, from astakeholder viewpoint, how stakeholder satisfaction changes from process toprocess. This analysis is used to rank the favourability of four policies withrespect to all collective stakeholder preferences.", "output": "A simple declarative model of the Federal Disaster Assistance Policy -- modelling and measuring transparency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of representation learning on knowledge graphs (KGs), ahyper-relational fact consists of a main triple and several auxiliaryattribute-value descriptions, which is considered more comprehensive andspecific than a triple-based fact. However, currently availablehyper-relational KG embedding methods in a single view are limited inapplication because they weaken the hierarchical structure that represents theaffiliation between entities. To overcome this limitation, we propose adual-view hyper-relational KG structure (DH-KG) that contains ahyper-relational instance view for entities and a hyper-relational ontologyview for concepts that are abstracted hierarchically from the entities. Thispaper defines link prediction and entity typing tasks on DH-KG for the firsttime and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, andHTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embeddingmodel based on GRAN encoders, HGNNs, and joint learning. DHGE outperformsbaseline models on DH-KG, according to experimental results. Finally, weprovide an example of how this technology can be used to treat hypertension.Our model and new datasets are publicly available.", "output": "DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusion models represent a recent emerging topic in computervision, demonstrating remarkable results in the area of generative modeling. Adiffusion model is a deep generative model that is based on two stages, aforward diffusion stage and a reverse diffusion stage. In the forward diffusionstage, the input data is gradually perturbed over several steps by addingGaussian noise. In the reverse stage, a model is tasked at recovering theoriginal input data by learning to gradually reverse the diffusion process,step by step. Diffusion models are widely appreciated for the quality anddiversity of the generated samples, despite their known computational burdens,i.e. low speeds due to the high number of steps involved during sampling. Inthis survey, we provide a comprehensive review of articles on denoisingdiffusion models applied in vision, comprising both theoretical and practicalcontributions in the field. First, we identify and present three genericdiffusion modeling frameworks, which are based on denoising diffusionprobabilistic models, noise conditioned score networks, and stochasticdifferential equations. We further discuss the relations between diffusionmodels and other deep generative models, including variational auto-encoders,generative adversarial networks, energy-based models, autoregressive models andnormalizing flows. Then, we introduce a multi-perspective categorization ofdiffusion models applied in computer vision. Finally, we illustrate the currentlimitations of diffusion models and envision some interesting directions forfuture research.", "output": "Diffusion Models in Vision: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper revisits Deep Mutual Learning (DML), a simple yet effectivecomputing paradigm. We propose using R'{e}nyi divergence instead of the KLdivergence, which is more flexible and tunable, to improve vanilla DML. Thismodification is able to consistently improve performance over vanilla DML withlimited additional complexity. The convergence properties of the proposedparadigm are analyzed theoretically, and Stochastic Gradient Descent with aconstant learning rate is shown to converge with $mathcal{O}(1)$-bias in theworst case scenario for nonconvex optimization tasks. That is, learning willreach nearby local optima but continue searching within a bounded scope, whichmay help mitigate overfitting. Finally, our extensive empirical resultsdemonstrate the advantage of combining DML and R'{e}nyi divergence, whichfurther improves generalized models.", "output": "R\\'{e}nyi Divergence Deep Mutual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite great success in many applications, deep neural networks are notalways robust in practice. For instance, a convolutional neuron network (CNN)model for classification tasks often performs unsatisfactorily in classifyingsome particular classes of objects. In this work, we are concerned withpatching the weak part of a CNN model instead of improving it through thecostly retraining of the entire model. Inspired by the fundamental concepts ofmodularization and composition in software engineering, we propose a compressedmodularization approach, CNNSplitter, which decomposes a strong CNN model for$N$-class classification into $N$ smaller CNN modules. Each module is asub-model containing a part of the convolution kernels of the strong model. Topatch a weak CNN model that performs unsatisfactorily on a target class (TC),we compose the weak CNN model with the corresponding module obtained from astrong CNN model. The ability of the weak CNN model to recognize the TC canthus be improved through patching. Moreover, the ability to recognize non-TCsis also improved, as the samples misclassified as TC could be classified asnon-TCs correctly. Experimental results with two representative CNNs on threewidely-used datasets show that the averaged improvement on the TC in terms ofprecision and recall are 12.54% and 2.14%, respectively. Moreover, patchingimproves the accuracy of non-TCs by 1.18%. The results demonstrate thatCNNSplitter can patch a weak CNN model through modularization and composition,thus providing a new solution for developing robust CNN models.", "output": "Patching Weak Convolutional Neural Network Models through Modularization and Composition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Soft prompt learning has recently emerged as one of the methods of choice foradapting V&amp;L models to a downstream task using a few training examples.However, current methods significantly overfit the training data, sufferingfrom large accuracy degradation when tested on unseen classes from the samedomain. To this end, in this paper, we make the following 4 contributions: (1)To alleviate base class overfitting, we propose a novel Language-Aware SoftPrompting (LASP) learning method by means of a text-to-text cross-entropy lossthat maximizes the probability of the learned prompts to be correctlyclassified with respect to pre-defined hand-crafted textual prompts. (2) Toincrease the representation capacity of the prompts, we propose grouped LASPwhere each group of prompts is optimized with respect to a separate subset oftextual prompts. (3) We identify a visual-language misalignment introduced byprompt learning and LASP, and more importantly, propose a re-calibrationmechanism to address it. (4) We show that LASP is inherently amenable toincluding, during training, virtual classes, i.e. class names for which novisual samples are available, further increasing the robustness of the learnedprompts. Through evaluations on 11 datasets, we show that our approach (a)significantly outperforms all prior works on soft prompting, and (b) matchesand surpasses, for the first time, the accuracy on novel classes obtained byhand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be madeavailable at ", "output": "LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, imitation learning (IL) has been widely used in industry asthe core of autonomous vehicle (AV) planning modules. However, previous ILworks show sample inefficiency and low generalisation in safety-criticalscenarios, on which they are rarely tested. As a result, IL planners can reacha performance plateau where adding more training data ceases to improve thelearnt policy. First, our work presents an IL model using the splinecoefficient parameterisation and offline expert queries to enhance safety andtraining efficiency. Then, we expose the weakness of the learnt IL policy bysynthetically generating critical scenarios through optimisation of parametersof the driver's risk field (DRF), a parametric human driving behaviour modelimplemented in a multi-agent traffic simulator based on the Lyft PredictionDataset. To continuously improve the learnt policy, we retrain the IL modelwith augmented data. Thanks to the expressivity and interpretability of theDRF, the desired driving behaviours can be encoded and aggregated to theoriginal training data. Our work constitutes a full development cycle that canefficiently and continuously improve the learnt IL policies in closed-loop.Finally, we show that our IL planner developed with less training resourcestill has superior performance compared to the previous state-of-the-art.", "output": "Learning from Demonstrations of Critical Driving Behaviours Using Driver's Risk Field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The sixth-generation (6G) mobile networks are expected to feature theubiquitous deployment of machine learning and AI algorithms at the networkedge. With rapid advancements in edge AI, the time has come to realizeintelligence downloading onto edge devices (e.g., smartphones and sensors). Tomaterialize this version, we propose a novel technology in this article, calledin-situ model downloading, that aims to achieve transparent and real-timereplacement of on-device AI models by downloading from an AI library in thenetwork. Its distinctive feature is the adaptation of downloading totime-varying situations (e.g., application, location, and time), devices'heterogeneous storage-and-computing capacities, and channel states. A keycomponent of the presented framework is a set of techniques that dynamicallycompress a downloaded model at the depth-level, parameter-level, or bit-levelto support adaptive model downloading. We further propose a virtualized 6Gnetwork architecture customized for deploying in-situ model downloading withthe key feature of a three-tier (edge, local, and central) AI library.Furthermore, experiments are conducted to quantify 6G connectivity requirementsand research opportunities pertaining to the proposed technology are discussed.", "output": "In-situ Model Downloading to Realize Versatile Edge AI in 6G Mobile Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have demonstrated remarkable progress in image generationquality, especially when guidance is used to control the generative process.However, guidance requires a large amount of image-annotation pairs fortraining and is thus dependent on their availability, correctness andunbiasedness. In this paper, we eliminate the need for such annotation byinstead leveraging the flexibility of self-supervision signals to design aframework for self-guided diffusion models. By leveraging a feature extractionfunction and a self-annotation function, our method provides guidance signalsat various image granularities: from the level of holistic images to objectboxes and even segmentation masks. Our experiments on single-label andmulti-label image datasets demonstrate that self-labeled guidance alwaysoutperforms diffusion models without guidance and may even surpass guidancebased on ground-truth labels, especially on unbalanced data. When equipped withself-supervised box or mask proposals, our method further generates visuallydiverse yet semantically consistent images, without the need for any class,box, or segment label annotation. Self-guided diffusion is simple, flexible andexpected to profit from deployment at scale.", "output": "Self-Guided Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a decluttering problem where multiple rigid convex polygonalobjects rest in randomly placed positions and orientations on a planar surfaceand must be efficiently transported to a packing box using both single andmulti-object grasps. Prior work considered frictionless multi-object grasping.In this paper, we introduce friction to increase picks per hour. We train aneural network using real examples to plan robust multi-object grasps. Inphysical experiments, we find a 13.7% increase in success rate, a 1.6x increasein picks per hour, and a 6.3x decrease in grasp planning time compared to priorwork on multi-object grasping. Compared to single object grasping, we find a3.1x increase in picks per hour.", "output": "Learning to Efficiently Plan Robust Frictional Multi-Object Grasps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pre-trained language models have attracted increasing attention in thebiomedical domain, inspired by their great success in the general naturallanguage domain. Among the two main branches of pre-trained language models inthe general language domain, i.e., BERT (and its variants) and GPT (and itsvariants), the first one has been extensively studied in the biomedical domain,such as BioBERT and PubMedBERT. While they have achieved great success on avariety of discriminative downstream biomedical tasks, the lack of generationability constrains their application scope. In this paper, we propose BioGPT, adomain-specific generative Transformer language model pre-trained on largescale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks anddemonstrate that our model outperforms previous models on most tasks.Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDIend-to-end relation extraction tasks respectively, and 78.2% accuracy onPubMedQA, creating a new record. Our case study on text generation furtherdemonstrates the advantage of BioGPT on biomedical literature to generatefluent descriptions for biomedical terms. Code is available at", "output": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Communication and computation are often viewed as separate tasks. Thisapproach is very effective from the perspective of engineering as isolatedoptimizations can be performed. However, for many computation-orientedapplications, the main interest is a function of the local information at thedevices, rather than the local information itself. In such scenarios,information theoretical results show that harnessing the interference in amultiple access channel for computation, i.e., over-the-air computation (OAC),can provide a significantly higher achievable computation rate than separatingcommunication and computation tasks. Moreover, the gap between OAC andseparation in terms of computation rate increases with more participatingnodes. Given this motivation, in this study, we provide a comprehensive surveyon practical OAC methods. After outlining fundamentals related to OAC, wediscuss the available OAC schemes with their pros and cons. We provide anoverview of the enabling mechanisms for achieving reliable computation in thewireless channel. Finally, we summarize the potential applications of OAC andpoint out some future directions.", "output": "A Survey on Over-the-Air Computation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Social intelligence and Theory of Mind (ToM), i.e., the ability to reasonabout the different mental states, intents, and reactions of all peopleinvolved, allow humans to effectively navigate and understand everyday socialinteractions. As NLP systems are used in increasingly complex socialsituations, their ability to grasp social dynamics becomes crucial. In thiswork, we examine the open question of social intelligence and Theory of Mind inmodern NLP systems from an empirical and theory-based perspective. We show thatone of today's largest language models (GPT-3; Brown et al., 2020) lacks thiskind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap etal., 2019), which measures models' ability to understand intents and reactionsof participants of social interactions, and ToMi (Le et al., 2019), whichmeasures whether models can infer mental states and realities of participantsof situations. Our results show that models struggle substantially at theseTheory of Mind tasks, with well-below-human accuracies of 55% and 60% onSocialIQa and ToMi, respectively. To conclude, we draw on theories frompragmatics to contextualize this shortcoming of large language models, byexamining the limitations stemming from their data, neural architecture, andtraining paradigms. Challenging the prevalent narrative that only scale isneeded, we posit that person-centric NLP approaches might be more effectivetowards neural Theory of Mind.In our updated version, we also analyze newer instruction tuned and RLFHmodels for neural ToM. We find that even ChatGPT and GPT-4 do not displayemergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy onthe ToMi questions related to mental states and realities.", "output": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a smoothly broken power law functional form (referred to by us asa Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates thescaling behaviors of deep neural networks (i.e. how the evaluation metric ofinterest varies as the amount of compute used for training, number of modelparameters, training dataset size, model input size, number of training steps,or upstream performance varies) for various architectures and for each ofvarious tasks within a large and diverse set of upstream and downstream tasks,in zero-shot, prompted, and fine-tuned settings. This set includes large-scalevision, language, audio, video, diffusion, generative modeling, multimodallearning, contrastive learning, AI alignment, robotics, out-of-distribution(OOD) generalization, continual learning, transfer learning, uncertaintyestimation / calibration, out-of-distribution detection, adversarialrobustness, distillation, sparsity, retrieval, quantization, pruning, fairness,molecules, computer programming/coding, math word problems, \"emergent\" \"phasetransitions / changes\", arithmetic, unsupervised/self-supervised learning, &amp;reinforcement learning (single agent &amp; multi-agent). When compared to otherfunctional forms for neural scaling behavior, this functional form yieldsextrapolations of scaling behavior that are considerably more accurate on thisset. Moreover, this functional form accurately models &amp; extrapolates scalingbehavior that other functional forms are incapable of expressing such as thenon-monotonic transitions present in the scaling behavior of phenomena such asdouble descent &amp; the delayed, sharp inflection points present in the scalingbehavior of tasks such as arithmetic. Lastly, we use this functional form toglean insights about the limit of the predictability of scaling behavior. Codeis available at ", "output": "Broken Neural Scaling Laws."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic software generation based on some specification is known as programsynthesis. Most existing approaches formulate program synthesis as a searchproblem with discrete parameters. In this paper, we present a novel formulationof program synthesis as a continuous optimization problem and use astate-of-the-art evolutionary approach, known as Covariance Matrix AdaptationEvolution Strategy to solve the problem. We then propose a mapping scheme toconvert the continuous formulation into actual programs. We compare our system,called GENESYS, with several recent program synthesis techniques (in bothdiscrete and continuous domains) and show that GENESYS synthesizes moreprograms within a fixed time budget than those existing schemes. For example,for programs of length 10, GENESYS synthesizes 28% more programs than thoseexisting schemes within the same time budget.", "output": "Synthesizing Programs with Continuous Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Today's computer vision models achieve human or near-human level performanceacross a wide variety of vision tasks. However, their architectures, data, andlearning algorithms differ in numerous ways from those that give rise to humanvision. In this paper, we investigate the factors that affect the alignmentbetween the representations learned by neural networks and human mentalrepresentations inferred from behavioral responses. We find that model scaleand architecture have essentially no effect on the alignment with humanbehavioral responses, whereas the training dataset and objective function bothhave a much larger impact. These findings are consistent across three datasetsof human similarity judgments collected using two different tasks. Lineartransformations of neural network representations learned from behavioralresponses from one dataset substantially improve alignment with humansimilarity judgments on the other two datasets. In addition, we find that somehuman concepts such as food and animals are well-represented by neural networkswhereas others such as royal or sports-related objects are not. Overall,although models trained on larger, more diverse datasets achieve betteralignment with humans than models trained on ImageNet alone, our resultsindicate that scaling alone is unlikely to be sufficient to train neuralnetworks with conceptual representations that match those used by humans.", "output": "Human alignment of neural network representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Action advising is a knowledge transfer technique for reinforcement learningbased on the teacher-student paradigm. An expert teacher provides advice to astudent during training in order to improve the student's sample efficiency andpolicy performance. Such advice is commonly given in the form of state-actionpairs. However, it makes it difficult for the student to reason with and applyto novel states. We introduce Explainable Action Advising, in which the teacherprovides action advice as well as associated explanations indicating why theaction was chosen. This allows the student to self-reflect on what it haslearned, enabling advice generalization and leading to improved sampleefficiency and learning performance - even in environments where the teacher issub-optimal. We empirically show that our framework is effective in bothsingle-agent and multi-agent scenarios, yielding improved policy returns andconvergence rates when compared to state-of-the-art methods", "output": "Explainable Action Advising for Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mathematical notation makes up a large portion of STEM literature, yet,finding semantic representations for formulae remains a challenging problem.Because mathematical notation is precise, and its meaning changes significantlywith small character shifts, the methods that work for natural text do notnecessarily work well for mathematical expressions. In this work, we describean approach for representing mathematical expressions in a continuous vectorspace. We use the encoder of a sequence-to-sequence architecture, trained onvisually different but mathematically equivalent expressions, to generatevector representations (or embeddings). We compare this approach with anautoencoder and show that the former is better at capturing mathematicalsemantics. Finally, to expedite future research, we publish a corpus ofequivalent transcendental and algebraic expression pairs.", "output": "Semantic Representations of Mathematical Expressions in a Continuous Vector Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multivariate biosignals are prevalent in many medical domains, such aselectroencephalography, polysomnography, and electrocardiography. Modelingspatiotemporal dependencies in multivariate biosignals is challenging due to(1) long-range temporal dependencies and (2) complex spatial correlationsbetween the electrodes. To address these challenges, we propose representingmultivariate biosignals as time-dependent graphs and introduce GraphS4mer, ageneral graph neural network (GNN) architecture that improves performance onbiosignal classification tasks by modeling spatiotemporal dependencies inbiosignals. Specifically, (1) we leverage the Structured State Spacearchitecture, a state-of-the-art deep sequence model, to capture long-rangetemporal dependencies in biosignals and (2) we propose a graph structurelearning layer in GraphS4mer to learn dynamically evolving graph structures inthe data. We evaluate our proposed model on three distinct biosignalclassification tasks and show that GraphS4mer consistently improves overexisting models, including (1) seizure detection from electroencephalographysignals, outperforming a previous GNN with self-supervised pre-training by 3.1points in AUROC; (2) sleep staging from polysomnography signals, a 4.1 pointsimprovement in macro-F1 score compared to existing sleep staging models; and(3) 12-lead electrocardiogram classification, outperforming previousstate-of-the-art models by 2.7 points in macro-F1 score.", "output": "Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Complex query answering (CQA) is an essential task for multi-hop and logicalreasoning on knowledge graphs (KGs). Currently, most approaches are limited toqueries among binary relational facts and pay less attention to n-ary facts(n&gt;=2) containing more than two entities, which are more prevalent in the realworld. Moreover, previous CQA methods can only make predictions for a few giventypes of queries and cannot be flexibly extended to more complex logicalqueries, which significantly limits their applications. To overcome thesechallenges, in this work, we propose a novel N-ary Query Embedding (NQE) modelfor CQA over hyper-relational knowledge graphs (HKGs), which include massiven-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder andfuzzy logic theory to satisfy all n-ary FOL queries, including existentialquantifiers, conjunction, disjunction, and negation. We also propose a parallelprocessing algorithm that can train or predict arbitrary n-ary FOL queries in asingle batch, regardless of the kind of each query, with good flexibility andextensibility. In addition, we generate a new CQA dataset WD50K-NFOL, includingdiverse n-ary FOL queries over WD50K. Experimental results on WD50K-NFOL andother standard CQA datasets show that NQE is the state-of-the-art CQA methodover HKGs with good generalization capability. Our code and dataset arepublicly available.", "output": "NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In Deep Image Prior (DIP), a Convolutional Neural Network (CNN) is fitted tomap a latent space to a degraded (e.g. noisy) image but in the process learnsto reconstruct the clean image. This phenomenon is attributed to CNN's internalimage-prior. We revisit the DIP framework, examining it from the perspective ofa neural implicit representation. Motivated by this perspective, we replace therandom or learned latent with Fourier-Features (Positional Encoding). We showthat thanks to the Fourier features properties, we can replace the convolutionlayers with simple pixel-level MLPs. We name this scheme ``Positional EncodingImage Prior\" (PIP) and exhibit that it performs very similarly to DIP onvarious image-reconstruction tasks with much less parameters required.Additionally, we demonstrate that PIP can be easily extended to videos, where3D-DIP struggles and suffers from instability. Code and additional examples forall tasks, including videos, are available on the project page", "output": "PIP: Positional-encoding Image Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised image semantic segmentation(UISS) aims to match low-level visualfeatures with semantic-level representations without outer supervision. In thispaper, we address the critical properties from the view of feature alignmentsand feature uniformity for UISS models. We also make a comparison between UISSand image-wise representation learning. Based on the analysis, we argue thatthe existing MI-based methods in UISS suffer from representation collapse. Bythis, we proposed a robust network called Semantic Attention Network(SAN), inwhich a new module Semantic Attention(SEAT) is proposed to generate pixel-wiseand semantic features dynamically. Experimental results on multiple semanticsegmentation benchmarks show that our unsupervised segmentation frameworkspecializes in catching semantic representations, which outperforms all theunpretrained and even several pretrained methods.", "output": "Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformers are powerful visual learners, in large part due to theirconspicuous lack of manually-specified priors. This flexibility can beproblematic in tasks that involve multiple-view geometry, due to thenear-infinite possible variations in 3D shapes and viewpoints (requiringflexibility), and the precise nature of projective geometry (obeying rigidlaws). To resolve this conundrum, we propose a \"light touch\" approach, guidingvisual Transformers to learn multiple-view geometry but allowing them to breakfree when needed. We achieve this by using epipolar lines to guide theTransformer's cross-attention maps, penalizing attention values outside theepipolar lines and encouraging higher attention along these lines since theycontain geometrically plausible matches. Unlike previous methods, our proposaldoes not require any camera pose information at test-time. We focus onpose-invariant object instance retrieval, where standard Transformer networksstruggle, due to the large differences in viewpoint between query and retrievedimages. Experimentally, our method outperforms state-of-the-art approaches atobject retrieval, without needing pose information at test-time.", "output": "A Light Touch Approach to Teaching Transformers Multi-view Geometry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement Learning (RL) generally suffers from poor sample complexity,mostly due to the need to exhaustively explore the state-action space to findwell-performing policies. On the other hand, we postulate that expert knowledgeof the system often allows us to design simple rules we expect good policies tofollow at all times. In this work, we hence propose a simple yet effectivemodification of continuous actor-critic frameworks to incorporate such rulesand avoid regions of the state-action space that are known to be suboptimal,thereby significantly accelerating the convergence of RL agents. Concretely, wesaturate the actions chosen by the agent if they do not comply with ourintuition and, critically, modify the gradient update step of the policy toensure the learning process is not affected by the saturation step. On a roomtemperature control case study, it allows agents to converge to well-performingpolicies up to 6-7x faster than classical agents without computational overheadand while retaining good final performance.", "output": "Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative models have shown great promise in synthesizing photorealistic 3Dobjects, but they require large amounts of training data. We introduce SinGRAF,a 3D-aware generative model that is trained with a few input images of a singlescene. Once trained, SinGRAF generates different realizations of this 3D scenethat preserve the appearance of the input while varying scene layout. For thispurpose, we build on recent progress in 3D GAN architectures and introduce anovel progressive-scale patch discrimination approach during training. Withseveral experiments, we demonstrate that the results produced by SinGRAFoutperform the closest related works in both quality and diversity by a largemargin.", "output": "SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One objection to conventional AI ethics is that it slows innovation. Thispresentation responds by reconfiguring ethics as an innovation accelerator. Thecritical elements develop from a contrast between Stability AI's Diffusion andOpenAI's Dall-E. By analyzing the divergent values underlying their opposedstrategies for development and deployment, five conceptions are identified ascommon to acceleration ethics. Uncertainty is understood as positive andencouraging, rather than discouraging. Innovation is conceived as intrinsicallyvaluable, instead of worthwhile only as mediated by social effects. AI problemsare solved by more AI, not less. Permissions and restrictions governing AIemerge from a decentralized process, instead of a unified authority. The workof ethics is embedded in AI development and application, instead of functioningfrom outside. Together, these attitudes and practices remake ethics asprovoking rather than restraining artificial intelligence.", "output": "Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose an end-to-end Retrieval-Augmented Visual LanguageModel (REVEAL) that learns to encode world knowledge into a large-scale memory,and to retrieve from it to answer knowledge-intensive queries. REVEAL consistsof four key components: the memory, the encoder, the retriever and thegenerator. The large-scale memory encodes various sources of multimodal worldknowledge (e.g. image-text pairs, question answering pairs, knowledge graphtriplets, etc) via a unified encoder. The retriever finds the most relevantknowledge entries in the memory, and the generator fuses the retrievedknowledge with the input query to produce the output. A key novelty in ourapproach is that the memory, encoder, retriever and generator are allpre-trained end-to-end on a massive amount of data. Furthermore, our approachcan use a diverse set of multimodal knowledge sources, which is shown to resultin significant gains. We show that REVEAL achieves state-of-the-art results onvisual question answering and image captioning.", "output": "REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Function approximation (FA) has been a critical component in solving largezero-sum games. Yet, little attention has been given towards FA in solvingtextit{general-sum} extensive-form games, despite them being widely regardedas being computationally more challenging than their fully competitive orcooperative counterparts. A key challenge is that for many equilibria ingeneral-sum games, no simple analogue to the state value function used inMarkov Decision Processes and zero-sum games exists. In this paper, we proposelearning the textit{Enforceable Payoff Frontier} (EPF) -- a generalization ofthe state value function for general-sum games. We approximate the optimaltextit{Stackelberg extensive-form correlated equilibrium} by representing EPFswith neural networks and training them by using appropriate backup operationsand loss functions. This is the first method that applies FA to the Stackelbergsetting, allowing us to scale to much larger games while still enjoyingperformance guarantees based on FA error. Additionally, our proposed methodguarantees incentive compatibility and is easy to evaluate without having todepend on self-play or approximate best-response oracles.", "output": "Function Approximation for Solving Stackelberg Equilibrium in Large Perfect Information Games."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The role of data in building AI systems has recently been significantlymagnified by the emerging concept of data-centric AI (DCAI), which advocates afundamental shift from model advancements to ensuring data quality andreliability. Although our community has continuously invested efforts intoenhancing data in different aspects, they are often isolated initiatives onspecific tasks. To facilitate the collective initiative in our community andpush forward DCAI, we draw a big picture and bring together three generalmissions: training data development, inference data development, and datamaintenance. We provide a top-level discussion on representative DCAI tasks andshare perspectives. Finally, we list open challenges. More resources aresummarized at ", "output": "Data-centric AI: Perspectives and Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have seen a surge of interest in learning high-level causalrepresentations from low-level image pairs under interventions. Yet, existingefforts are largely limited to simple synthetic settings that are far away fromreal-world problems. In this paper, we present Causal Triplet, a causalrepresentation learning benchmark featuring not only visually more complexscenes, but also two crucial desiderata commonly overlooked in previous works:(i) an actionable counterfactual setting, where only certain object-levelvariables allow for counterfactual observations whereas others do not; (ii) aninterventional downstream task with an emphasis on out-of-distributionrobustness from the independent causal mechanisms principle. Through extensiveexperiments, we find that models built with the knowledge of disentangled orobject-centric representations significantly outperform their distributedcounterparts. However, recent causal representation learning methods stillstruggle to identify such latent structures, indicating substantial challengesand opportunities for future work. Our code and datasets will be available at", "output": "Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, a new perspective is suggested for unsupervised OntologyMatching (OM) or Ontology Alignment (OA) by treating it as a translation task.Ontologies are represented as graphs, and the translation is performed from anode in the source ontology graph to a path in the target ontology graph. Theproposed framework, Truveta Mapper (TM), leverages a multi-tasksequence-to-sequence transformer model to perform alignment across multipleontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enablesthe model to implicitly learn the relationship between different ontologies viatransfer-learning without requiring any explicit cross-ontology manuallylabeled data. This also enables the formulated framework to outperform existingsolutions for both runtime latency and alignment quality. The model ispre-trained and fine-tuned only on publicly available text corpus andinner-ontologies data. The proposed solution outperforms state-of-the-artapproaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presentednew OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offerslog-linear complexity in contrast to quadratic in the existing end-to-endmethods, and overall makes the OM task efficient and more straightforwardwithout much post-processing involving mapping extension or mapping repair.", "output": "Truveta Mapper: A Zero-shot Ontology Alignment Framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monocular Depth Estimation (MDE) is a critical component in applications suchas autonomous driving. There are various attacks against MDE networks. Theseattacks, especially the physical ones, pose a great threat to the security ofsuch systems. Traditional adversarial training method requires ground-truthlabels hence cannot be directly applied to self-supervised MDE that does nothave ground-truth depth. Some self-supervised model hardening techniques (e.g.,contrastive learning) ignore the domain knowledge of MDE and can hardly achieveoptimal performance. In this work, we propose a novel adversarial trainingmethod for self-supervised MDE models based on view synthesis without usingground-truth depth. We improve adversarial robustness against physical-worldattacks using L0-norm-bounded perturbation in training. We compare our methodwith supervised learning based and contrastive learning based methods that aretailored for MDE. Results on two representative MDE networks show that weachieve better robustness against various adversarial attacks with nearly nobenign performance degradation.", "output": "Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, arerapidly transforming the way we communicate, illustrate, and create. However,AI regulation, in the EU and beyond, has primarily focused on conventional AImodels, not LGAIMs. This paper will situate these new generative models in thecurrent debate on trustworthy AI regulation, and ask how the law can betailored to their capabilities. After laying technical foundations, the legalpart of the paper proceeds in four steps, covering (1) direct regulation, (2)data protection, (3) content moderation, and (4) policy proposals. It suggestsa novel terminology to capture the AI value chain in LGAIM settings bydifferentiating between LGAIM developers, deployers, professional andnon-professional users, as well as recipients of LGAIM output. We tailorregulatory duties to these different actors along the value chain and suggestfour strategies to ensure that LGAIMs are trustworthy and deployed for thebenefit of society at large. Rules in the AI Act and other direct regulationmust match the specificities of pre-trained models. In particular, regulationshould focus on concrete high-risk applications, and not the pre-trained modelitself, and should include (i) obligations regarding transparency and (ii) riskmanagement. Non-discrimination provisions (iii) may, however, apply to LGAIMdevelopers. Lastly, (iv) the core of the DSA content moderation rules should beexpanded to cover LGAIMs. This includes notice and action mechanisms, andtrusted flaggers. In all areas, regulators and lawmakers need to act fast tokeep track with the dynamics of ChatGPT et al.", "output": "Regulating ChatGPT and other Large Generative AI Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT)framework. It emphasizes spatio-temporal continuity and integrates both pastand future reasoning for tracked objects. Thus, we name it \"Past-and-Futurereasoning for Tracking\" (PF-Track). Specifically, our method adapts the\"tracking by attention\" framework and represents tracked instances coherentlyover time with object queries. To explicitly use historical cues, our \"PastReasoning\" module learns to refine the tracks and enhance the object featuresby cross-attending to queries from previous frames and other objects. The\"Future Reasoning\" module digests historical information and predicts robustfuture trajectories. In the case of long-term occlusions, our method maintainsthe object positions and enables re-association by integrating motionpredictions. On the nuScenes dataset, our method improves AMOTA by a largemargin and remarkably reduces ID-Switches by 90% compared to prior approaches,which is an order of magnitude less. The code and models are made available at", "output": "Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised object-centric representation (OCR) learning has recently drawnattention as a new paradigm of visual representation. This is because of itspotential of being an effective pre-training technique for various downstreamtasks in terms of sample efficiency, systematic generalization, and reasoning.Although image-based reinforcement learning (RL) is one of the most importantand thus frequently mentioned such downstream tasks, the benefit in RL hassurprisingly not been investigated systematically thus far. Instead, most ofthe evaluations have focused on rather indirect metrics such as segmentationquality and object property prediction accuracy. In this paper, we investigatethe effectiveness of OCR pre-training for image-based reinforcement learningvia empirical experiments. For systematic evaluation, we introduce a simpleobject-centric visual RL benchmark and conduct experiments to answer questionssuch as ``Does OCR pre-training improve performance on object-centric tasks?''and ``Can OCR pre-training help with out-of-distribution generalization?''. Ourresults provide empirical evidence for valuable insights into the effectivenessof OCR pre-training for RL and the potential limitations of its use in certainscenarios. Additionally, this study also examines the critical aspects ofincorporating OCR pre-training in RL, including performance in a visuallycomplex environment and the appropriate pooling layer to aggregate the objectrepresentations.", "output": "An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "IoU losses are surrogates that directly optimize the Jaccard index. Insemantic segmentation, leveraging IoU losses as part of the loss function isshown to perform better with respect to the Jaccard index measure thanoptimizing pixel-wise losses such as the cross-entropy loss alone. The mostnotable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss.However, these losses are incompatible with soft labels which are ubiquitous inmachine learning. In this paper, we propose Jaccard metric losses (JMLs), whichare identical to the soft Jaccard loss in a standard setting with hard labels,but are compatible with soft labels. With JMLs, we study two of the mostpopular use cases of soft labels: label smoothing and knowledge distillation.With a variety of architectures, our experiments show significant improvementsover the cross-entropy loss on three semantic segmentation datasets(Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approachoutperforms state-of-the-art knowledge distillation methods by a large margin.Code is available at:href{", "output": "Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) are often used for tasks involving the geometryof a given graph, such as molecular dynamics simulation. Although the distancematrix of a geometric graph contains complete geometric information, it hasbeen demonstrated that Message Passing Neural Networks (MPNNs) are insufficientfor learning this geometry. In this work, we expand on the families ofcounterexamples that MPNNs are unable to distinguish from their distancematrices, by constructing families of novel and symmetric geometric graphs. Wethen propose $k$-DisGNNs, which can effectively exploit the rich geometrycontained in the distance matrix. We demonstrate the high expressive power ofour models and prove that some existing well-designed geometric models can beunified by $k$-DisGNNs as special cases. Most importantly, we establish aconnection between geometric deep learning and traditional graph representationlearning, showing that those highly expressive GNN models originally designedfor graph structure learning can also be applied to geometric deep learningproblems with impressive performance, and that existing complex, equivariantmodels are not the only solution. Experimental results verify our theory.", "output": "Is Distance Matrix Enough for Geometric Deep Learning?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Well-performed deep neural networks (DNNs) generally require massive labelleddata and computational resources for training. Various watermarking techniquesare proposed to protect such intellectual properties (IPs), wherein the DNNproviders implant secret information into the model so that they can laterclaim IP ownership by retrieving their embedded watermarks with some dedicatedtrigger inputs. While promising results are reported in the literature,existing solutions suffer from watermark removal attacks, such as modelfine-tuning and model pruning.In this paper, we propose a novel DNN watermarking solution that caneffectively defend against the above attacks. Our key insight is to enhance thecoupling of the watermark and model functionalities such that removing thewatermark would inevitably degrade the model's performance on normal inputs. Tothis end, unlike previous methods relying on secret features learnt fromout-of-distribution data, our method only uses features learnt fromin-distribution data. Specifically, on the one hand, we propose to sampleinputs from the original training dataset and fuse them as watermark triggers.On the other hand, we randomly mask model weights during training so that theinformation of our embedded watermarks spreads in the network. By doing so,model fine-tuning/pruning would not forget our function-coupled watermarks.Evaluation results on various image classification tasks show a 100% watermarkauthentication success rate under aggressive watermark removal attacks,significantly outperforming existing solutions. Code is available:", "output": "On Function-Coupled Watermarks for Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The popularity of on-demand ride pooling is owing to the benefits offered tocustomers (lower prices), taxi drivers (higher revenue), environment (lowercarbon footprint due to fewer vehicles) and aggregation companies like Uber(higher revenue). To achieve these benefits, two key interlinked challengeshave to be solved effectively: (a) pricing -- setting prices to customerrequests for taxis; and (b) matching -- assignment of customers (that acceptedthe prices) to taxis/cars. Traditionally, both these challenges have beenstudied individually and using myopic approaches (considering only currentrequests), without considering the impact of current matching on addressingfuture requests. In this paper, we develop a novel framework that handles thepricing and matching problems together, while also considering the futureimpact of the pricing and matching decisions. In our experimental results on areal-world taxi dataset, we demonstrate that our framework can significantlyimprove revenue (up to 17% and on average 6.4%) in a sustainable manner byreducing the number of vehicles (up to 14% and on average 10.6%) required toobtain a given fixed revenue and the overall distance travelled by vehicles (upto 11.1% and on average 3.7%). That is to say, we are able to provide an idealwin-win scenario for all stakeholders (customers, drivers, aggregator,environment) involved by obtaining higher revenue for customers, drivers,aggregator (ride pooling company) while being good for the environment (due tofewer number of vehicles on the road and lesser fuel consumed).", "output": "Future Aware Pricing and Matching for Sustainable On-demand Ride Pooling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Smart cities operate on computational predictive frameworks that collect,aggregate, and utilize data from large-scale sensor networks. However, theseframeworks are prone to multiple sources of data and algorithmic bias, whichoften lead to unfair prediction results. In this work, we first demonstratethat bias persists at a micro-level both temporally and spatially by studyingreal city data from Chattanooga, TN. To alleviate the issue of such bias, weintroduce Fairguard, a micro-level temporal logic-based approach for fair smartcity policy adjustment and generation in complex temporal-spatial domains. TheFairguard framework consists of two phases: first, we develop a staticgenerator that is able to reduce data bias based on temporal logic conditionsby minimizing correlations between selected attributes. Then, to ensurefairness in predictive algorithms, we design a dynamic component to regulateprediction results and generate future fair predictions by harnessing logicrules. Evaluations show that logic-enabled static Fairguard can effectivelyreduce the biased correlations while dynamic Fairguard can guarantee fairnesson protected groups at run-time with minimal impact on overall performance.", "output": "Fairguard: Harness Logic-based Fairness Rules in Smart Cities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has experienced significant growth in recent years, resultingin increased energy consumption and carbon emission from the use of GPUs fortraining deep neural networks (DNNs). Answering the call for sustainability,conventional solutions have attempted to move training jobs to locations ortime frames with lower carbon intensity. However, moving jobs to otherlocations may not always be feasible due to large dataset sizes or dataregulations. Moreover, postponing training can negatively impact applicationservice quality because the DNNs backing the service are not updated in atimely fashion. In this work, we present a practical solution that reduces thecarbon footprint of DNN training without migrating or postponing jobs.Specifically, our solution observes real-time carbon intensity shifts duringtraining and controls the energy consumption of GPUs, thereby reducing carbonfootprint while maintaining training performance. Furthermore, in order toproactively adapt to shifting carbon intensity, we propose a lightweightmachine learning algorithm that predicts the carbon intensity of the upcomingtime frame. Our solution, Chase, reduces the total carbon footprint of trainingResNet-50 on ImageNet by 13.6% while only increasing training time by 2.5%.", "output": "Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, considering the balance of data/model privacy of model ownersand user needs, we propose a new setting called Back-Propagated Black-BoxAdaptation (BPBA) for users to better train their private models via theguidance of the back-propagated results of a Black-box foundation/source model.Our setting can ease the usage of foundation/source models as well as preventthe leakage and misuse of foundation/source models. Moreover, we also propose anew training strategy called Bootstrap The Original Latent (BTOL) to fullyutilize the foundation/source models. Our strategy consists of a domain adapterand a freeze-and-thaw strategy. We apply our BTOL under BPBA and Black-box UDAsettings on three different datasets. Experiments show that our strategy isefficient and robust in various settings without manual augmentations.", "output": "Bootstrap The Original Latent: Learning a Private Model from a Black-box Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series generation (TSG) studies have mainly focused on the use ofGenerative Adversarial Networks (GANs) combined with recurrent neural network(RNN) variants. However, the fundamental limitations and challenges of trainingGANs still remain. In addition, the RNN-family typically has difficulties withtemporal consistency between distant timesteps. Motivated by the successes inthe image generation (IMG) domain, we propose TimeVQVAE, the first work, to ourknowledge, that uses vector quantization (VQ) techniques to address the TSGproblem. Moreover, the priors of the discrete latent spaces are learned withbidirectional transformer models that can better capture global temporalconsistency. We also propose VQ modeling in a time-frequency domain, separatedinto low-frequency (LF) and high-frequency (HF). This allows us to retainimportant characteristics of the time series and, in turn, generate newsynthetic signals that are of better quality, with sharper changes inmodularity, than its competing TSG methods. Our experimental evaluation isconducted on all datasets from the UCR archive, using well-established metricsin the IMG literature, such as Fr'echet inception distance and inceptionscores. Our implementation on GitHub:url{", "output": "Vector Quantized Time Series Generation with a Bidirectional Prior Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This survey reviews text-to-image diffusion models in the context thatdiffusion models have emerged to be popular for a wide range of generativetasks. As a self-contained work, this survey starts with a brief introductionof how a basic diffusion model works for image synthesis, followed by howcondition or guidance improves learning. Based on that, we present a review ofstate-of-the-art methods on text-conditioned image synthesis, i.e.,text-to-image. We further summarize applications beyond text-to-imagegeneration: text-guided creative generation and text-guided image editing.Beyond the progress made so far, we discuss existing challenges and promisingfuture directions.", "output": "Text-to-image Diffusion Models in Generative AI: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) are capable to perform complex reasoning byin-context learning (ICL) when provided with a few input-output demonstrations(demos) and more powerful when intermediate reasoning steps (\"chain of thoughts(CoT)\") of the demos are given. Is it necessary to use multi-demo in ICL? Inthis paper, we study ICL using fewer demos for each test query on the tasksin~cite{wei2022chain}. Surprisingly, we do not observe significant degradationwhen using only one randomly chosen demo. To study this phenomenon, for eachtest query, we categorize demos into \"correct demos\" leading to the correctanswer, and \"wrong demos\" resulting in wrong answers. Our analysis reveals aninherent bias in those widely studied datasets: most demos are correct for amajority of test queries, which explains the good performance of using onerandom demo. Moreover, ICL (with and w/o CoT) using only one correct demosignificantly outperforms all-demo ICL adopted by most previous works,indicating the weakness of LLMs in finding correct demo(s) for input queries,which is difficult to evaluate on the biased datasets. Furthermore, we observea counterintuitive behavior of ICL using multi-demo, i.e., its accuracydegrades(improves) when given more correct(wrong) demos. This implies that ICLcan be easily misguided by interference among demos and their spuriouscorrelations. Our analyses highlight several fundamental challenges that needto be addressed in LLMs training, ICL, and benchmark design.", "output": "It Takes One to Tango but More Make Trouble? The Number of Demonstrations Needed for In-Context Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence (AI) is making a profound impact in almost everydomain. A vital enabler of its great success is the availability of abundantand high-quality data for building machine learning models. Recently, the roleof data in AI has been significantly magnified, giving rise to the emergingconcept of data-centric AI. The attention of researchers and practitioners hasgradually shifted from advancing model design to enhancing the quality andquantity of the data. In this survey, we discuss the necessity of data-centricAI, followed by a holistic view of three general data-centric goals (trainingdata development, inference data development, and data maintenance) and therepresentative methods. We also organize the existing literature fromautomation and collaboration perspectives, discuss the challenges, and tabulatethe benchmarks for various tasks. We believe this is the first comprehensivesurvey that provides a global view of a spectrum of tasks across various stagesof the data lifecycle. We hope it can help the readers efficiently grasp abroad picture of this field, and equip them with the techniques and furtherresearch ideas to systematically engineer data for building AI systems. Acompanion list of data-centric AI resources will be regularly updated on", "output": "Data-centric Artificial Intelligence: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "AI-induced societal harms mirror existing problems in domains where AIreplaces or complements traditional methodologies. However, trustworthy AIdiscourses postulate the homogeneity of AI, aim to derive common causesregarding the harms they generate, and demand uniform human interventions. SuchAI monism has spurred legislation for omnibus AI laws requiring any high-riskAI systems to comply with a full, uniform package of rules on fairness,transparency, accountability, human oversight, accuracy, robustness, andsecurity, as demonstrated by the EU AI Regulation and the U.S. draftAlgorithmic Accountability Act. However, it is irrational to require high-riskor critical AIs to comply with all the safety, fairness, accountability, andprivacy regulations when it is possible to separate AIs entailing safety risks,biases, infringements, and privacy problems. Legislators should gradually adaptexisting regulations by categorizing AI systems according to the types ofsocietal harms they induce. Accordingly, this paper proposes the followingcategorizations, subject to ongoing empirical reassessments. First, regardingintelligent agents, safety regulations must be adapted to address incrementalaccident risks arising from autonomous behavior. Second, regardingdiscriminative models, law must focus on the mitigation of allocative harms andthe disclosure of marginal effects of immutable features. Third, for generativemodels, law should optimize developer liability for data mining and contentgeneration, balancing potential social harms arising from infringing contentand the negative impact of excessive filtering and identify cases where itsnon-human identity should be disclosed. Lastly, for cognitive models, dataprotection law should be adapted to effectively address privacy, surveillance,and security problems and facilitate governance built on public-privatepartnerships.", "output": "Heterogeneity of AI-Induced Societal Harms and the Failure of Omnibus AI Laws."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prevalent predictive coding-based video compression methods rely on a heavyencoder to reduce temporal redundancy, which makes it challenging to deploythem on resource-constrained devices. Since the 1970s, distributed sourcecoding theory has indicated that independent encoding and joint decoding withside information (SI) can achieve high-efficient compression of correlatedsources. This has inspired a distributed coding architecture aiming at reducingthe encoding complexity. However, traditional distributed coding methods sufferfrom a substantial performance gap to predictive coding ones. Inspired by thegreat success of learning-based compression, we propose the first end-to-enddistributed deep video compression framework to improve the rate-distortionperformance. A key ingredient is an effective SI generation module at thedecoder, which helps to effectively exploit inter-frame correlations withoutcomputation-intensive encoder-side motion estimation and compensation.Experiments show that our method significantly outperforms conventionaldistributed video coding and H.264. Meanwhile, it enjoys 6-7x encoding speedupagainst DVC [1] with comparable compression performance. Code is released at", "output": "Low-complexity Deep Video Compression with A Distributed Coding Architecture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of repository-level code completion is to continue writing theunfinished code based on a broader context of the repository. While forautomated code completion tools, it is difficult to utilize the usefulinformation scattered in different files. We propose RepoCoder, a simple,generic, and effective framework to address the challenge. It streamlines therepository-level code completion process by incorporating a similarity-basedretriever and a pre-trained code language model, which allows for the effectiveutilization of repository-level information for code completion and grants theability to generate code at various levels of granularity. Furthermore,RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridgesthe gap between retrieval context and the intended completion target. We alsopropose a new benchmark RepoEval, which consists of the latest and high-qualityreal-world repositories covering line, API invocation, and function bodycompletion scenarios. We test the performance of RepoCoder by using variouscombinations of code retrievers and generators. Experimental results indicatethat RepoCoder significantly improves the zero-shot code completion baseline byover 10% in all settings and consistently outperforms the vanillaretrieval-augmented code completion approach. Furthermore, we validate theeffectiveness of RepoCoder through comprehensive analysis, providing valuableinsights for future research.", "output": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative AI has demonstrated impressive performance in various fields,among which speech synthesis is an interesting direction. With the diffusionmodel as the most popular generative model, numerous works have attempted twoactive tasks: text to speech and speech enhancement. This work conducts asurvey on audio diffusion model, which is complementary to existing surveysthat either lack the recent progress of diffusion-based speech synthesis orhighlight an overall picture of applying diffusion model in multiple fields.Specifically, this work first briefly introduces the background of audio anddiffusion model. As for the text-to-speech task, we divide the methods intothree categories based on the stage where diffusion model is adopted: acousticmodel, vocoder and end-to-end framework. Moreover, we categorize various speechenhancement tasks by either certain signals are removed or added into the inputspeech. Comparisons of experimental results and discussions are also covered inthis survey.", "output": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Planning as theorem proving in situation calculus was abandoned 50 years agoas an impossible project. But we have developed a Theorem Proving LiftedHeuristic (TPLH) planner that searches for a plan in a tree of situations usingthe A* search algorithm. It is controlled by a delete relaxation-based domainindependent heuristic. We compare TPLH with Fast Downward (FD) and Best FirstWidth Search (BFWS) planners over several standard benchmarks. Since ourimplementation of the heuristic function is not optimized, TPLH is slower thanFD and BFWS. But it computes shorter plans, and it explores fewer states. Wediscuss previous research on planning within KR&amp;R and identify relateddirections. Thus, we show that deductive lifted heuristic planning in situationcalculus is actually doable.", "output": "Planning as Theorem Proving with Heuristics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Catastrophic forgetting has been a major challenge in continual learning,where the model needs to learn new tasks with limited or no access to data frompreviously seen tasks. To tackle this challenge, methods based on knowledgedistillation in feature space have been proposed and shown to reduceforgetting. However, most feature distillation methods directly constrain thenew features to match the old ones, overlooking the need for plasticity. Toachieve a better stability-plasticity trade-off, we propose Backward FeatureProjection (BFP), a method for continual learning that allows the new featuresto change up to a learnable linear transformation of the old features. BFPpreserves the linear separability of the old classes while allowing theemergence of new feature directions to accommodate new classes. BFP can beintegrated with existing experience replay methods and boost performance by asignificant margin. We also demonstrate that BFP helps learn a betterrepresentation space, in which linear separability is well preserved duringcontinual learning and linear probing achieves high classification accuracy.The code can be found at ", "output": "Preserving Linear Separability in Continual Learning by Backward Feature Projection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Road network digital twins (RNDTs) play a critical role in the development ofnext-generation intelligent transportation systems, enabling more precisetraffic planning and control. To support just-in-time (JIT) decision making,RNDTs require a model that dynamically learns the traffic patterns from onlinesensor data and generates high-fidelity simulation results. Although currenttraffic prediction techniques based on graph neural networks have achievedstate-of-the-art performance, these techniques only predict future traffic bymining correlations in historical traffic data, disregarding the causes oftraffic generation, such as Origin-Destination (OD) demands and routeselection. Therefore, their performance is unreliable for JIT decision making.To fill this gap, we introduce a novel deep learning framework called TraffNetthat learns the causality of traffic volumes from vehicle trajectory data.First, we use a heterogeneous graph to represent the road network, allowing themodel to incorporate causal features of traffic volumes. Next, inspired by thetraffic domain knowledge, we propose a traffic causality learning method tolearn an embedding vector that encodes OD demands and path-level dependenciesfor each road segment. Then, we model temporal dependencies to match theunderlying process of traffic generation. Finally, the experiments verify theutility of TraffNet. The code of TraffNet is available at", "output": "TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The soft Dice loss (SDL) has taken a pivotal role in many automatedsegmentation pipelines in the medical imaging community. Over the last years,some reasons behind its superior functioning have been uncovered and furtheroptimizations have been explored. However, there is currently no implementationthat supports its direct use in settings with soft labels. Hence, a synergybetween the use of SDL and research leveraging the use of soft labels, also inthe context of model calibration, is still missing. In this work, we introduceDice semimetric losses (DMLs), which (i) are by design identical to SDL in astandard setting with hard labels, but (ii) can be used in settings with softlabels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirmthe potential synergy of DMLs with soft labels (e.g. averaging, labelsmoothing, and knowledge distillation) over hard labels (e.g. majority votingand random selection). As a result, we obtain superior Dice scores and modelcalibration, which supports the wider adoption of DMLs in practice. Code isavailable athref{", "output": "Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Safety-critical cyber-physical systems require control strategies whoseworst-case performance is robust against adversarial disturbances and modelinguncertainties. In this paper, we present a framework for approximate controland learning in partially observed systems to minimize the worst-casediscounted cost over an infinite time horizon. We model disturbances to thesystem as finite-valued uncertain variables with unknown probabilitydistributions. For problems with known system dynamics, we construct a dynamicprogramming (DP) decomposition to compute the optimal control strategy. Ourfirst contribution is to define information states that improve thecomputational tractability of this DP without loss of optimality. Then, wedescribe a simplification for a class of problems where the incurred cost isobservable at each time instance. Our second contribution is defining anapproximate information state that can be constructed or learned directly fromobserved data for problems with observable costs. We derive bounds on theperformance loss of the resulting approximate control strategy and illustratethe effectiveness of our approach in partially observed decision-makingproblems with a numerical example.", "output": "Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT is built on a large language model trained on an enormous corpus ofhuman text to emulate human conversation. Despite lacking any explicitprogramming regarding the laws of physics, recent work has demonstrated thatGPT-3.5 could pass an introductory physics course at some nominal level andregister something close to a minimal understanding of Newtonian Mechanics onthe Force Concept Inventory. This work replicates those results and alsodemonstrates that the latest version, GPT-4, has reached a much higher mark inthe latter context. Indeed, its responses come quite close to perfectlydemonstrating expert-level competence, with a few very notable exceptions andlimitations. We briefly comment on the implications of this for the future ofphysics education and pedagogy.", "output": "Advances in apparent conceptual physics reasoning in GPT-4."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently reported state-of-the-art results in visual speech recognition (VSR)often rely on increasingly large amounts of video data, while the publiclyavailable transcribed video datasets are limited in size. In this paper, forthe first time, we study the potential of leveraging synthetic visual data forVSR. Our method, termed SynthVSR, substantially improves the performance of VSRsystems with synthetic lip movements. The key idea behind SynthVSR is toleverage a speech-driven lip animation model that generates lip movementsconditioned on the input speech. The speech-driven lip animation model istrained on an unlabeled audio-visual dataset and could be further optimizedtowards a pre-trained VSR model when labeled videos are available. As plenty oftranscribed acoustic data and face images are available, we are able togenerate large-scale synthetic data using the proposed lip animation model forsemi-supervised VSR training. We evaluate the performance of our approach onthe largest public VSR benchmark - Lip Reading Sentences 3 (LRS3). SynthVSRachieves a WER of 43.3% with only 30 hours of real labeled data, outperformingoff-the-shelf approaches using thousands of hours of video. The WER is furtherreduced to 27.9% when using all 438 hours of labeled data from LRS3, which ison par with the state-of-the-art self-supervised AV-HuBERT method. Furthermore,when combined with large-scale pseudo-labeled audio-visual data SynthVSR yieldsa new state-of-the-art VSR WER of 16.9% using publicly available data only,surpassing the recent state-of-the-art approaches trained with 29 times morenon-public machine-transcribed video data (90,000 hours). Finally, we performextensive ablation studies to understand the effect of each component in ourproposed method.", "output": "SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an artificial intelligence system to remotely assess the motorperformance of individuals with Parkinson's disease (PD). Participantsperformed a motor task (i.e., tapping fingers) in front of a webcam, and datafrom 250 global participants were rated by three expert neurologists followingthe Movement Disorder Society Unified Parkinson's Disease Rating Scale(MDS-UPDRS). The neurologists' ratings were highly reliable, with anintra-class correlation coefficient (ICC) of 0.88. We developed computeralgorithms to obtain objective measurements that align with the MDS-UPDRSguideline and are strongly correlated with the neurologists' ratings. Ourmachine learning model trained on these measures outperformed an MDS-UPDRScertified rater, with a mean absolute error (MAE) of 0.59 compared to therater's MAE of 0.79. However, the model performed slightly worse than theexpert neurologists (0.53 MAE). The methodology can be replicated for similarmotor tasks, providing the possibility of evaluating individuals with PD andother movement disorders remotely, objectively, and in areas with limitedaccess to neurological care.", "output": "Using AI to Measure Parkinson's Disease Severity at Home."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solving complicated AI tasks with different domains and modalities is a keystep toward advanced artificial intelligence. While there are abundant AImodels available for different domains and modalities, they cannot handlecomplicated AI tasks. Considering large language models (LLMs) have exhibitedexceptional ability in language understanding, generation, interaction, andreasoning, we advocate that LLMs could act as a controller to manage existingAI models to solve complicated AI tasks and language could be a genericinterface to empower this. Based on this philosophy, we present HuggingGPT, aframework that leverages LLMs (e.g., ChatGPT) to connect various AI models inmachine learning communities (e.g., Hugging Face) to solve AI tasks.Specifically, we use ChatGPT to conduct task planning when receiving a userrequest, select models according to their function descriptions available inHugging Face, execute each subtask with the selected AI model, and summarizethe response according to the execution results. By leveraging the stronglanguage capability of ChatGPT and abundant AI models in Hugging Face,HuggingGPT is able to cover numerous sophisticated AI tasks in differentmodalities and domains and achieve impressive results in language, vision,speech, and other challenging tasks, which paves a new way towards advancedartificial intelligence.", "output": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a 3D object, kinematic motion prediction aims to identify the mobileparts as well as the corresponding motion parameters. Due to the largevariations in both topological structure and geometric details of 3D objects,this remains a challenging task and the lack of large scale labeled data alsoconstrain the performance of deep learning based approaches. In this paper, wetackle the task of object kinematic motion prediction problem in a semi-weaklysupervised manner. Our key observations are two-fold. First, although 3Ddataset with fully annotated motion labels is limited, there are existingdatasets and methods for object part semantic segmentation at large scale.Second, semantic part segmentation and mobile part segmentation is not alwaysconsistent but it is possible to detect the mobile parts from the underlying 3Dstructure. Towards this end, we propose a graph neural network to learn the mapbetween hierarchical part-level segmentation and mobile parts parameters, whichare further refined based on geometric alignment. This network can be firsttrained on PartNet-Mobility dataset with fully labeled mobility information andthen applied on PartNet dataset with fine-grained and hierarchical part-levelsegmentation. The network predictions yield a large scale of 3D objects withpseudo labeled mobility information and can further be used forweakly-supervised learning with pre-existing segmentation. Our experiments showthere are significant performance boosts with the augmented data for previousmethod designed for kinematic motion prediction on 3D partial scans.", "output": "Semi-Weakly Supervised Object Kinematic Motion Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Referring image segmentation (RIS) aims to find a segmentation mask given areferring expression grounded to a region of the input image. Collectinglabelled datasets for this task, however, is notoriously costly andlabor-intensive. To overcome this issue, we propose a simple yet effectivezero-shot referring image segmentation method by leveraging the pre-trainedcross-modal knowledge from CLIP. In order to obtain segmentation masks groundedto the input text, we propose a mask-guided visual encoder that captures globaland local contextual information of an input image. By utilizing instance masksobtained from off-the-shelf mask proposal techniques, our method is able tosegment fine-detailed Istance-level groundings. We also introduce aglobal-local text encoder where the global feature captures complexsentence-level semantics of the entire input expression while the local featurefocuses on the target noun phrase extracted by a dependency parser. In ourexperiments, the proposed method outperforms several zero-shot baselines of thetask and even the weakly supervised referring expression segmentation methodwith substantial margins. Our code is available at", "output": "Zero-shot Referring Image Segmentation with Global-Local Context Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised Continual learning involves updating a deep neural network (DNN)from an ever-growing stream of labeled data. While most work has focused onovercoming catastrophic forgetting, one of the major motivations behindcontinual learning is being able to efficiently update a network with newinformation, rather than retraining from scratch on the training dataset as itgrows over time. Despite recent continual learning methods largely solving thecatastrophic forgetting problem, there has been little attention paid to theefficiency of these algorithms. Here, we study recent methods for incrementalclass learning and illustrate that many are highly inefficient in terms ofcompute, memory, and storage. Some methods even require more compute thantraining from scratch! We argue that for continual learning to have real-worldapplicability, the research community cannot ignore the resources used by thesealgorithms. There is more to continual learning than mitigating catastrophicforgetting.", "output": "How Efficient Are Today's Continual Learning Algorithms?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The article discusses a method for automating the process of cutting ahoneycomb block, and specifically obtaining points and cutting angles for therequired faces. The following requirements are taken into account in thecalculations: the allowable location of the cut plane is 0.4 of the length ofthe cell face, the cut plane must be perpendicular to the cell wall. Thealgorithm itself consists of two main stages: determining the honeycombstructure and searching for cut points. In the absence of significant defectsin honeycomb blocks (deformation of the cell profile and a dent on the edges ofthe cells), the structure determination algorithm works without significantinaccuracies. The results of the cut point search algorithm can be consideredsatisfactory.", "output": "Determination of cutting positions of honeycomb blocks using computer vision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although end-to-end video text spotting methods based on Transformer canmodel long-range dependencies and simplify the train process, it will lead tolarge computation cost with the increase of the frame size in the input video.Therefore, considering the resolution of ICDAR 2023 DSText is 1080 * 1920 andslicing the video frame into several areas will destroy the spatial correlationof text, we divided the small and dense text spotting into two tasks, textdetection and tracking. For text detection, we adopt the PP-YOLOE-R which isproven effective in small object detection as our detection model. For textdetection, we use the sort algorithm for high inference speed. Experiments onDSText dataset demonstrate that our method is competitive on small and densetext spotting.", "output": "Video text tracking for dense and small text based on pp-yoloe-r and sort algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The prevalence of memes on social media has created the need to sentimentanalyze their underlying meanings for censoring harmful content. Meme censoringsystems by machine learning raise the need for a semi-supervised learningsolution to take advantage of the large number of unlabeled memes available onthe internet and make the annotation process less challenging. Moreover, theapproach needs to utilize multimodal data as memes' meanings usually come fromboth images and texts. This research proposes a multimodal semi-supervisedlearning approach that outperforms other multimodal semi-supervised learningand supervised learning state-of-the-art models on two datasets, the MultimediaAutomatic Misogyny Identification and Hateful Memes dataset. Building on theinsights gained from Contrastive Language-Image Pre-training, which is aneffective multimodal learning technique, this research introduces SemiMemes, anovel training method that combines auto-encoder and classification task tomake use of the resourceful unlabeled data.", "output": "SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the emergence of powerful computing resources and large-scaleannotated datasets, deep learning has seen wide applications in our daily life.However, most current methods require extensive data collection and retrainingwhen dealing with novel classes never seen before. On the other hand, we humanscan quickly recognize new classes by looking at a few samples, which motivatesthe recent popularity of few-shot learning (FSL) in machine learningcommunities. Most current FSL approaches work on 2D image domain, however, itsimplication in 3D perception is relatively under-explored. Not only needs torecognize the unseen examples as in 2D domain, 3D few-shot learning is morechallenging with unordered structures, high intra-class variances, and subtleinter-class differences. Moreover, different architectures and learningalgorithms make it difficult to study the effectiveness of existing 2D methodswhen migrating to the 3D domain. In this work, for the first time, we performsystematic and extensive studies of recent 2D FSL and 3D backbone networks forbenchmarking few-shot point cloud classification, and we suggest a strongbaseline and learning architectures for 3D FSL. Then, we propose a novelplug-and-play component called Cross-Instance Adaptation (CIA) module, toaddress the high intra-class variances and subtle inter-class differencesissues, which can be easily inserted into current baselines with significantperformance improvement. Extensive experiments on two newly introducedbenchmark datasets, ModelNet40-FS and ShapeNet70-FS, demonstrate thesuperiority of our proposed network for 3D FSL.", "output": "What Makes for Effective Few-shot Point Cloud Classification?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many real-world settings, the critical class is rare and a misseddetection carries a disproportionately high cost. For example, tumors are rareand a false negative diagnosis could have severe consequences on treatmentoutcomes; fraudulent banking transactions are rare and an undetected occurrencecould result in significant losses or legal penalties. In such contexts,systems are often operated at a high true positive rate, which may requiretolerating high false positives. In this paper, we present a novel approach toaddress the challenge of minimizing false positives for systems that need tooperate at a high true positive rate. We propose a ranking-based regularization(RankReg) approach that is easy to implement, and show empirically that it notonly effectively reduces false positives, but also complements conventionalimbalanced learning losses. With this novel technique in hand, we conduct aseries of experiments on three broadly explored datasets (CIFAR-10&amp;100 andMelanoma) and show that our approach lifts the previous state-of-the-artperformance by notable margins.", "output": "Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present a residual neural network-based method for pointset registration. Given a target and a reference point cloud, the goal is tolearn a minimal transformation that aligns the target to the reference underthe constraint that the topological structure of the target point cloud ispreserved. Similar to coherent point drift (CPD), the registration (alignment)problem is viewed as the movement of data points sampled from a targetdistribution along a regularized displacement vector field. While the coherenceconstraint in CPD is stated in terms of local motion coherence, the proposedregularization term relies on a global smoothness constraint as a proxy forpreserving local topology. This makes CPD less flexible when the deformation islocally rigid but globally non-rigid as in the case of multiple objects andarticulate pose registration. To mitigate these issues, a Jacobian-based costfunction along with geometric-aware statistical distances is proposed. Thelatter allows for measuring misalignment between the target and the reference.The justification for the kNN-graph preservation of target data, when theJacobian cost is used, is also provided. Further, a stochastic approximationfor high dimensional registration is introduced to make a high-dimensionalalignment feasible. The proposed method is tested on high-dimensional FlowCytometry to align two data distributions whilst preserving the kNN-graph ofthe data. The implementation of the proposed approach is available at under the MIT license.", "output": "kNN-Res: Residual Neural Network with kNN-Graph coherence for point cloud registration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dense 3D reconstruction from RGB images traditionally assumes static camerapose estimates. This assumption has endured, even as recent works haveincreasingly focused on real-time methods for mobile devices. However, theassumption of one pose per image does not hold for online execution: poses fromreal-time SLAM are dynamic and may be updated following events such as bundleadjustment and loop closure. This has been addressed in the RGB-D setting, byde-integrating past views and re-integrating them with updated poses, but itremains largely untreated in the RGB-only setting. We formalize this problem todefine the new task of online reconstruction from dynamically-posed images. Tosupport further research, we introduce a dataset called LivePose containing thedynamic poses from a SLAM system running on ScanNet. We select three recentreconstruction systems and apply a framework based on de-integration to adapteach one to the dynamic-pose setting. In addition, we propose a novel,non-linear de-integration module that learns to remove stale scene content. Weshow that responding to pose updates is critical for high-qualityreconstruction, and that our de-integration framework is an effective solution.", "output": "LivePose: Online 3D Reconstruction from Monocular Video with Dynamic Camera Poses."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive learning has shown promising potential for learning robustrepresentations by utilizing unlabeled data. However, constructing effectivepositive-negative pairs for contrastive learning on facial behavior datasetsremains challenging. This is because such pairs inevitably encode thesubject-ID information, and the randomly constructed pairs may push similarfacial images away due to the limited number of subjects in facial behaviordatasets. To address this issue, we propose to utilize activity descriptions,coarse-grained information provided in some datasets, which can providehigh-level semantic information about the image sequences but is oftenneglected in previous studies. More specifically, we introduce a two-stageContrastive Learning with Text-Embeded framework for Facial behaviorunderstanding (CLEF). The first stage is a weakly-supervised contrastivelearning method that learns representations from positive-negative pairsconstructed using coarse-grained activity information. The second stage aims totrain the recognition of facial expressions or facial action units bymaximizing the similarity between image and the corresponding text label names.The proposed CLEF achieves state-of-the-art performance on three in-the-labdatasets for AU recognition and three in-the-wild datasets for facialexpression recognition.", "output": "Weakly-Supervised Text-driven Contrastive Learning for Facial Behavior Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern image classifiers perform well on populated classes, while degradingconsiderably on tail classes with only a few instances. Humans, by contrast,effortlessly handle the long-tailed recognition challenge, since they can learnthe tail representation based on different levels of semantic abstraction,making the learned tail features more discriminative. This phenomenon motivatedus to propose SuperDisco, an algorithm that discovers super-classrepresentations for long-tailed recognition using a graph model. We learn toconstruct the super-class graph to guide the representation learning to dealwith long-tailed distributions. Through message passing on the super-classgraph, image representations are rectified and refined by attending to the mostrelevant entities based on the semantic similarity among their super-classes.Moreover, we propose to meta-learn the super-class graph under the supervisionof a prototype graph constructed from a small amount of imbalanced data. Bydoing so, we obtain a more robust super-class graph that further improves thelong-tailed recognition performance. The consistent state-of-the-artexperiments on the long-tailed CIFAR-100, ImageNet, Places and iNaturalistdemonstrate the benefit of the discovered super-class graph for dealing withlong-tailed distributions.", "output": "SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent quantitative parameter mapping methods including MR fingerprinting(MRF) collect a time series of images that capture the evolution ofmagnetization. The focus of this work is to introduce a novel approach termedas Deep Factor Model(DFM), which offers an efficient representation of themulti-contrast image time series. The higher efficiency of the representationenables the acquisition of the images in a highly undersampled fashion, whichtranslates to reduced scan time in 3D high-resolution multi-contrastapplications. The approach integrates motion estimation and compensation,making the approach robust to subject motion during the scan.", "output": "Deep Factor Model: A Novel Approach for Motion Compensated Multi-Dimensional MRI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anatomical consistency in biomarker segmentation is crucial for many medicalimage analysis tasks. A promising paradigm for achieving anatomicallyconsistent segmentation via deep networks is incorporating pixel connectivity,a basic concept in digital topology, to model inter-pixel relationships.However, previous works on connectivity modeling have ignored the richchannel-wise directional information in the latent space. In this work, wedemonstrate that effective disentanglement of directional sub-space from theshared latent space can significantly enhance the feature representation in theconnectivity-based network. To this end, we propose a directional connectivitymodeling scheme for segmentation that decouples, tracks, and utilizes thedirectional information across the network. Experiments on various publicmedical image segmentation benchmarks show the effectiveness of our model ascompared to the state-of-the-art methods. Code is available at", "output": "Directional Connectivity-based Segmentation of Medical Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new loss function for joint disparity and uncertainty estimationin deep stereo matching. Our work is motivated by the need for preciseuncertainty estimates and the observation that multi-task learning often leadsto improved performance in all tasks. We show that this can be achieved byrequiring the distribution of uncertainty to match the distribution ofdisparity errors via a KL divergence term in the network's loss function. Adifferentiable soft-histogramming technique is used to approximate thedistributions so that they can be used in the loss. We experimentally assessthe effectiveness of our approach and observe significant improvements in bothdisparity and uncertainty prediction on large datasets.", "output": "Learning the Distribution of Errors in Stereo Matching for Joint Disparity and Uncertainty Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To advance automated detection of extreme weather events, which areincreasing in frequency and intensity with climate change, we exploremodifications to a novel light-weight Context Guided convolutional neuralnetwork architecture trained for semantic segmentation of tropical cyclones andatmospheric rivers in climate data. Our primary focus is on tropical cyclones,the most destructive weather events, for which current models show limitedperformance. We investigate feature engineering, data augmentation, learningrate modifications, alternative loss functions, and architectural changes. Incontrast to previous approaches optimizing for intersection over union, wespecifically seek to improve recall to penalize under-counting and prioritizeidentification of tropical cyclones. We report success through the use ofweighted loss functions to counter class imbalance for these rare events. Weconclude with directions for future research on extreme weather eventsdetection, a crucial task for prediction, mitigation, and equitable adaptationto the impacts of climate change.", "output": "Improving extreme weather events detection with light-weight neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The left ventricular of ejection fraction is one of the most important metricof cardiac function. It is used by cardiologist to identify patients who areeligible for lifeprolonging therapies. However, the assessment of ejectionfraction suffers from inter-observer variability. To overcome this challenge,we propose a deep learning approach, based on hierarchical vision Transformers,to estimate the ejection fraction from echocardiogram videos. The proposedmethod can estimate ejection fraction without the need for left ventricesegmentation first, make it more efficient than other methods. We evaluated ourmethod on EchoNet-Dynamic dataset resulting 5.59, 7.59 and 0.59 for MAE, RMSEand R2 respectivelly. This results are better compared to the state-of-the-artmethod, Ultrasound Video Transformer (UVT). The source code is available on", "output": "Hierarchical Vision Transformers for Cardiac Ejection Fraction Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep generative models have the capacity to render high fidelity images ofcontent like human faces. Recently, there has been substantial progress inconditionally generating images with specific quantitative attributes, like theemotion conveyed by one's face. These methods typically require a user toexplicitly quantify the desired intensity of a visual attribute. A limitationof this method is that many attributes, like how \"angry\" a human face looks,are difficult for a user to precisely quantify. However, a user would be ableto reliably say which of two faces seems \"angrier\". Following this premise, wedevelop the $textit{PrefGen}$ system, which allows users to control therelative attributes of generated images by presenting them with simple pairedcomparison queries of the form \"do you prefer image $a$ or image $b$?\" Usinginformation from a sequence of query responses, we can estimate userpreferences over a set of image attributes and perform preference-guided imageediting and generation. Furthermore, to make preference localization feasibleand efficient, we apply an active query selection strategy. We demonstrate thesuccess of this approach using a StyleGAN2 generator on the task of human faceediting. Additionally, we demonstrate how our approach can be combined withCLIP, allowing a user to edit the relative intensity of attributes specified bytext prompts. Code at ", "output": "PrefGen: Preference Guided Image Generation with Relative Attributes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent text-to-image generation models like DreamBooth have made remarkableprogress in generating highly customized images of a target subject, byfine-tuning an ``expert model'' for a given subject from a few examples.However, this process is expensive, since a new expert model must be learnedfor each subject. In this paper, we present SuTI, a Subject-drivenText-to-Image generator that replaces subject-specific fine tuning withemph{in-context} learning. Given a few demonstrations of a new subject, SuTIcan instantly generate novel renditions of the subject in different scenes,without any subject-specific optimization. SuTI is powered by {emapprenticeship learning}, where a single apprentice model is learned from datagenerated by massive amount of subject-specific expert models. Specifically, wemine millions of image clusters from the Internet, each centered around aspecific visual subject. We adopt these clusters to train massive amount ofexpert models specialized on different subjects. The apprentice model SuTI thenlearns to mimic the behavior of these experts through the proposedapprenticeship learning algorithm. SuTI can generate high-quality andcustomized subject-specific images 20x faster than optimization-based SoTAmethods. On the challenging DreamBench and DreamBench-v2, our human evaluationshows that SuTI can significantly outperform existing approaches likeInstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen whileperforming on par with DreamBooth.", "output": "Subject-driven Text-to-Image Generation via Apprenticeship Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we investigate applications of no-collision transportation mapsintroduced in [Nurbekyan et. al., 2020] in manifold learning for image data.Recently, there has been a surge in applying transportation-based distances andfeatures for data representing motion-like or deformation-like phenomena.Indeed, comparing intensities at fixed locations often does not reveal the datastructure. No-collision maps and distances developed in [Nurbekyan et. al.,2020] are sensitive to geometric features similar to optimal transportation(OT) maps but much cheaper to compute due to the absence of optimization. Inthis work, we prove that no-collision distances provide an isometry betweentranslations (respectively dilations) of a single probability measure and thetranslation (respectively dilation) vectors equipped with a Euclidean distance.Furthermore, we prove that no-collision transportation maps, as well as OT andlinearized OT maps, do not in general provide an isometry for rotations. Thenumerical experiments confirm our theoretical findings and show thatno-collision distances achieve similar or better performance on severalmanifold learning tasks compared to other OT and Euclidean-based methods at afraction of a computational cost.", "output": "Applications of No-Collision Transportation Maps in Manifold Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fast adversarial training (FAT) is an efficient method to improve robustness.However, the original FAT suffers from catastrophic overfitting, whichdramatically and suddenly reduces robustness after a few training epochs.Although various FAT variants have been proposed to prevent overfitting, theyrequire high training costs. In this paper, we investigate the relationshipbetween adversarial example quality and catastrophic overfitting by comparingthe training processes of standard adversarial training and FAT. We find thatcatastrophic overfitting occurs when the attack success rate of adversarialexamples becomes worse. Based on this observation, we propose a positiveprior-guided adversarial initialization to prevent overfitting by improvingadversarial example quality without extra training costs. This initializationis generated by using high-quality adversarial perturbations from thehistorical training process. We provide theoretical analysis for the proposedinitialization and propose a prior-guided regularization method that boosts thesmoothness of the loss function. Additionally, we design a prior-guidedensemble FAT method that averages the different model weights of historicalmodels using different decay rates. Our proposed method, called FGSM-PGK,assembles the prior-guided knowledge, i.e., the prior-guided initialization andmodel weights, acquired during the historical training process. Evaluations offour datasets demonstrate the superiority of the proposed method.", "output": "Improving Fast Adversarial Training with Prior-Guided Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Road accidents involving autonomous vehicles commonly occur in situationswhere a (pedestrian) obstacle presents itself in the path of the moving vehicleat very sudden time intervals, leaving the robot even lesser time to react tothe change in scene.In order to tackle this issue, we propose a novel algorithmic implementationthat classifies the intent of a single arbitrarily chosen pedestrian in a twodimensional frame into logic states in a procedural manner using quaternionsgenerated from a MediaPipe pose estimation model. This bypasses the need toemploy any relatively high latency deep-learning algorithms primarily due tothe lack of necessity for depth perception as well as an implicit cap on thecomputational resources that most IoT edge devices present.The model was able to achieve an average testing accuracy of 83.56% with areliable variance of 0.0042 while operating with an average latency of 48milliseconds, demonstrating multiple notable advantages over the currentstandard of using spatio-temporal convolutional networks for these perceptivetasks.", "output": "Pedestrian Intention Classifier using ID3 Modelled Decision Trees for IoT Edge Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world medical image segmentation has tremendous long-tailed complexityof objects, among which tail conditions correlate with relatively rare diseasesand are clinically significant. A trustworthy medical AI algorithm shoulddemonstrate its effectiveness on tail conditions to avoid clinically dangerousdamage in these out-of-distribution (OOD) cases. In this paper, we adopt theconcept of object queries in Mask Transformers to formulate semanticsegmentation as a soft cluster assignment. The queries fit the feature-levelcluster centers of inliers during training. Therefore, when performinginference on a medical image in real-world scenarios, the similarity betweenpixels and the queries detects and localizes OOD regions. We term this OODlocalization as MaxQuery. Furthermore, the foregrounds of real-world medicalimages, whether OOD objects or inliers, are lesions. The difference betweenthem is less than that between the foreground and background, possiblymisleading the object queries to focus redundantly on the background. Thus, wepropose a query-distribution (QD) loss to enforce clear boundaries betweensegmentation targets and other regions at the query level, improving the inliersegmentation and OOD indication. Our proposed framework is tested on tworeal-world segmentation tasks, i.e., segmentation of pancreatic and livertumors, outperforming previous state-of-the-art algorithms by an average of7.39% on AUROC, 14.69% on AUPR, and 13.79% on FPR95 for OOD localization. Onthe other hand, our framework improves the performance of inlier segmentationby an average of 5.27% DSC when compared with the leading baseline nnUNet.", "output": "Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Analyzing high resolution whole slide images (WSIs) with regard toinformation across multiple scales poses a significant challenge in digitalpathology. Multi-instance learning (MIL) is a common solution for working withhigh resolution images by classifying bags of objects (i.e. sets of smallerimage patches). However, such processing is typically performed at a singlescale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scaleinformation that is key to diagnoses by human pathologists. In this study, wepropose a novel cross-scale MIL algorithm to explicitly aggregate inter-scalerelationships into a single MIL network for pathological image diagnosis. Thecontribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL)algorithm that integrates the multi-scale information and the inter-scalerelationships is proposed; (2) A toy dataset with scale-specific morphologicalfeatures is created and released to examine and visualize differentialcross-scale attention; (3) Superior performance on both in-house and publicdatasets is demonstrated by our simple cross-scale MIL strategy. The officialimplementation is publicly available at ", "output": "Cross-scale Multi-instance Learning for Pathological Image Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper shows that Masking the Deep hierarchical features is an efficientself-supervised method, denoted as MaskDeep. MaskDeep treats each patch in therepresentation space as an independent instance. We mask part of patches in therepresentation space and then utilize sparse visible patches to reconstructhigh semantic image representation. The intuition of MaskDeep lies in the factthat models can reason from sparse visible patches semantic to the globalsemantic of the image. We further propose three designs in our framework: 1) aHierarchical Deep-Masking module to concern the hierarchical property of patchrepresentations, 2) a multi-group strategy to improve the efficiency withoutany extra computing consumption of the encoder and 3) a multi-target strategyto provide more description of the global semantic. Our MaskDeep brings decentimprovements. Trained on ResNet50 with 200 epochs, MaskDeep achievesstate-of-the-art results of 71.2% Top1 accuracy linear classification onImageNet. On COCO object detection tasks, MaskDeep outperforms theself-supervised method SoCo, which specifically designed for object detection.When trained with 100 epochs, MaskDeep achieves 69.6% Top1 accuracy, whichsurpasses current methods trained with 200 epochs, such as HCSC, by 0.4% .", "output": "Mask Hierarchical Features For Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wires and powerlines are common visual distractions that often undermine theaesthetics of photographs. The manual process of precisely segmenting andremoving them is extremely tedious and may take up hours, especially onhigh-resolution photos where wires may span the entire space. In this paper, wepresent an automatic wire clean-up system that eases the process of wiresegmentation and removal/inpainting to within a few seconds. We observe severalunique challenges: wires are thin, lengthy, and sparse. These are rareproperties of subjects that common segmentation tasks cannot handle, especiallyin high-resolution images. We thus propose a two-stage method that leveragesboth global and local contexts to accurately segment wires in high-resolutionimages efficiently, and a tile-based inpainting strategy to remove the wiresgiven our predicted segmentation masks. We also introduce the first wiresegmentation benchmark dataset, WireSegHR. Finally, we demonstratequantitatively and qualitatively that our wire clean-up system enables fullyautomated wire removal with great generalization to various wire appearances.", "output": "Automatic High Resolution Wire Segmentation and Removal."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Siamese trackers based on 3D region proposal network (RPN) have shownremarkable success with deep Hough voting. However, using a single seed pointfeature as the cue for voting fails to produce high-quality 3D proposals.Additionally, the equal treatment of seed points in the voting process,regardless of their significance, exacerbates this limitation. To address thesechallenges, we propose a novel transformer-based voting scheme to generatebetter proposals. Specifically, a global-local transformer (GLT) module isdevised to integrate object- and patch-aware geometric priors into seed pointfeatures, resulting in robust and accurate cues for offset learning of seedpoints. To train the GLT module, we introduce an importance prediction branchthat learns the potential importance weights of seed points as a trainingconstraint. Incorporating this transformer-based voting scheme into 3D RPN, anovel Siamese method dubbed GLT-T is developed for 3D single object tracking onpoint clouds. Moreover, we identify that the highest-scored proposal in theSiamese paradigm may not be the most accurate proposal, which limits trackingperformance. Towards this concern, we approach the binary score prediction taskas a ranking problem, and design a target-aware ranking loss and alocalization-aware ranking loss to produce accurate ranking of proposals. Withthe ranking losses, we further present GLT-T++, an enhanced version of GLT-T.Extensive experiments on multiple benchmarks demonstrate that our GLT-T andGLT-T++ outperform state-of-the-art methods in terms of tracking accuracy whilemaintaining a real-time inference speed. The source code will be made availableat ", "output": "GLT-T++: Global-Local Transformer for 3D Siamese Tracking with Ranking Loss."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent detection transformer (DETR) has advanced object detection, butits application on resource-constrained devices requires massive computationand memory resources. Quantization stands out as a solution by representing thenetwork in low-bit parameters and operations. However, there is a significantperformance drop when performing low-bit quantized DETR (Q-DETR) with existingquantization methods. We find that the bottlenecks of Q-DETR come from thequery information distortion through our empirical analyses. This paperaddresses this problem based on a distribution rectification distillation(DRD). We formulate our DRD as a bi-level optimization problem, which can bederived by generalizing the information bottleneck (IB) principle to thelearning of Q-DETR. At the inner level, we conduct a distribution alignment forthe queries to maximize the self-information entropy. At the upper level, weintroduce a new foreground-aware query matching scheme to effectively transferthe teacher information to distillation-desired features to minimize theconditional information entropy. Extensive experimental results show that ourmethod performs much better than prior arts. For example, the 4-bit Q-DETR cantheoretically accelerate DETR with ResNet-50 backbone by 6.6x and achieve 39.4%AP, with only 2.6% performance gaps than its real-valued counterpart on theCOCO dataset.", "output": "Q-DETR: An Efficient Low-Bit Quantized Detection Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Localizing people and recognizing their actions from videos is a challengingtask towards high-level video understanding. Existing methods are mostlytwo-stage based, with one stage for person bounding box generation and theother stage for action recognition. However, such two-stage methods aregenerally with low efficiency. We observe that directly unifying detection andaction recognition normally suffers from (i) inferior learning due to differentdesired properties of context representation for detection and actionrecognition; (ii) optimization difficulty with insufficient training data. Inthis work, we present a decoupled one-stage network dubbed DOAD, to mitigateabove issues and improve the efficiency for spatio-temporal action detection.To achieve it, we decouple detection and action recognition into two branches.Specifically, one branch focuses on detection representation for actordetection, and the other one for action recognition. For the action branch, wedesign a transformer-based module (TransPC) to model pairwise relationshipsbetween people and context. Different from commonly used vector-based dotproduct in self-attention, it is built upon a novel matrix-based key and valuefor Hadamard attention to model person-context information. It not onlyexploits relationships between person pairs but also takes into account contextand relative position information. The results on AVA and UCF101-24 datasetsshow that our method is competitive with two-stage state-of-the-art methodswith significant efficiency improvement.", "output": "DOAD: Decoupled One Stage Action Detection Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Breast cancer is a significant public health concern and early detection iscritical for triaging high risk patients. Sequential screening mammograms canprovide important spatiotemporal information about changes in breast tissueover time. In this study, we propose a deep learning architecture calledRADIFUSION that utilizes sequential mammograms and incorporates a linear imageattention mechanism, radiomic features, a new gating mechanism to combinedifferent mammographic views, and bilateral asymmetry-based finetuning forbreast cancer risk assessment. We evaluate our model on a screening datasetcalled Cohort of Screen-Aged Women (CSAW) dataset. Based on results obtained onthe independent testing set consisting of 1,749 women, our approach achievedsuperior performance compared to other state-of-the-art models with area underthe receiver operating characteristic curves (AUCs) of 0.905, 0.872 and 0.866in the three respective metrics of 1-year AUC, 2-year AUC and &gt; 2-year AUC. Ourstudy highlights the importance of incorporating various deep learningmechanisms, such as image attention, radiomic features, gating mechanism, andbilateral asymmetry-based fine-tuning, to improve the accuracy of breast cancerrisk assessment. We also demonstrate that our model's performance was enhancedby leveraging spatiotemporal information from sequential mammograms. Ourfindings suggest that RADIFUSION can provide clinicians with a powerful toolfor breast cancer risk assessment.", "output": "RADIFUSION: A multi-radiomics deep learning based breast cancer risk prediction model using sequential mammographic images with image attention and bilateral asymmetry refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual Place Recognition (VPR) is the task of retrieving database imagessimilar to a query photo by comparing it to a large database of known images.In real-world applications, extreme illumination changes caused by query imagestaken at night pose a significant obstacle that VPR needs to overcome. However,a training set with day-night correspondence for city-scale, street-level VPRdoes not exist. To address this challenge, we propose a novel pipeline thatdivides VPR and conquers Nocturnal Place Recognition (NPR). Specifically, wefirst established a street-level day-night dataset, NightStreet, and used it totrain an unpaired image-to-image translation model. Then we used this model toprocess existing large-scale VPR datasets to generate the VPR-Night datasetsand demonstrated how to combine them with two popular VPR pipelines. Finally,we proposed a divide-and-conquer VPR framework and provided explanations at thetheoretical, experimental, and application levels. Under our framework,previous methods can significantly improve performance on two public datasets,including the top-ranked method.", "output": "NPR: Nocturnal Place Recognition in Street."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Currently, salience-based channel pruning makes continuous breakthroughs innetwork compression. In the realization, the salience mechanism is used as ametric of channel salience to guide pruning. Therefore, salience-based channelpruning can dynamically adjust the channel width at run-time, which provides aflexible pruning scheme. However, there are two problems emerging: a gatingfunction is often needed to truncate the specific salience entries to zero,which destabilizes the forward propagation; dynamic architecture brings morecost for indexing in inference which bottlenecks the inference speed. In thispaper, we propose a Progressive Channel-Shrinking (PCS) method to compress theselected salience entries at run-time instead of roughly approximating them tozero. We also propose a Running Shrinking Policy to provide a testing-staticpruning scheme that can reduce the memory access cost for filter indexing. Weevaluate our method on ImageNet and CIFAR10 datasets over two prevalentnetworks: ResNet and VGG, and demonstrate that our PCS outperforms allbaselines and achieves state-of-the-art in terms of compression-performancetradeoff. Moreover, we observe a significant and practical acceleration ofinference.", "output": "Progressive Channel-Shrinking Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformer models process input images by dividing them into aspatially regular grid of equal-size patches. Conversely, Transformers wereoriginally introduced over natural language sequences, where each tokenrepresents a subword - a chunk of raw data of arbitrary size. In this work, weapply this approach to Vision Transformers by introducing a novel imagetokenization scheme, replacing the standard uniform grid with amixed-resolution sequence of tokens, where each token represents a patch ofarbitrary size. Using the Quadtree algorithm and a novel saliency scorer, weconstruct a patch mosaic where low-saliency areas of the image are processed inlow resolution, routing more of the model's capacity to important imageregions. Using the same architecture as vanilla ViTs, our Quadformer modelsachieve substantial accuracy gains on image classification when controlling forthe computational budget. Code and models are publicly available at .", "output": "Vision Transformers with Mixed-Resolution Tokenization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a framework to use recently introduced Capsule Networks forsolving the problem of Optical Flow, one of the fundamental computer visiontasks. Most of the existing state of the art deep architectures either uses acorrelation oepration to match features from them. While correlation layer issensitive to the choice of hyperparameters and does not put a prior on theunderlying structure of the object, spatio temporal features will be limited bythe network's receptive field. Also, we as humans look at moving objects aswhole, something which cannot be encoded by correlation or spatio temporalfeatures. Capsules, on the other hand, are specialized to model seperateentities and their pose as a continuous matrix. Thus, we show that a simplerlinear operation over poses of the objects detected by the capsules in enoughto model flow. We show reslts on a small toy dataset where we outperformFlowNetC and PWC-Net models.", "output": "CapsFlow: Optical Flow Estimation with Capsule Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Whether by processing videos with fixed resolution from start to end orincorporating pooling and down-scaling strategies, existing video transformersprocess the whole video content throughout the network without speciallyhandling the large portions of redundant information. In this paper, we presenta Supertoken Video Transformer (SVT) that incorporates a Semantic PoolingModule (SPM) to aggregate latent representations along the depth of visualtransformer based on their semantics, and thus, reduces redundancy inherent invideo inputs.~Qualitative results show that our method can effectively reduceredundancy by merging latent representations with similar semantics and thusincrease the proportion of salient information for downstreamtasks.~Quantitatively, our method improves the performance of both ViT and MViTwhile requiring significantly less computations on the Kinectics andSomething-Something-V2 benchmarks.~More specifically, with our SPM, we improvethe accuracy of MAE-pretrained ViT-B and ViT-L by 1.5% with 33% less GFLOPs andby 0.2% with 55% less FLOPs, respectively, on the Kinectics-400 benchmark, andimprove the accuracy of MViTv2-B by 0.2% and 0.3% with 22% less GFLOPs onKinectics-400 and Something-Something-V2, respectively.", "output": "SVT: Supertoken Video Transformer for Efficient Video Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bullet time is a type of visual effect commonly used in film, television andgames that makes time seem to slow down or stop while still preserving dynamicdetails in the scene. It usually requires multiple sets of cameras to moveslowly with the subject and is synthesized using post-production techniques,which is costly and one-time. The dynamic scene perspective reconstructiontechnology based on neural rendering field can be used to solve thisrequirement, but most of the current methods are poor in reconstructionaccuracy due to the blurred input image and overfitting of dynamic and staticregions. Based on the NSFF algorithm, this paper reconstructed the common timespecial effects scenes in movies and television from a new perspective. Toimprove the accuracy of the reconstructed images, fuzzy kernel was added to thenetwork for reconstruction and analysis of the fuzzy process, and the clearperspective after analysis was input into the NSFF to improve the accuracy. Byusing the optical flow prediction information to suppress the dynamic networktimely, the network is forced to improve the reconstruction effect of dynamicand static networks independently, and the ability to understand andreconstruct dynamic and static scenes is improved. To solve the overfittingproblem of dynamic and static scenes, a new dynamic and static cross entropyloss is designed. Experimental results show that compared with original NSFFand other new perspective reconstruction algorithms of dynamic scenes, theimproved NSFF-RFCT improves the reconstruction accuracy and enhances theunderstanding ability of dynamic and static scenes.", "output": "Multi-view reconstruction of bullet time effect based on improved NSFF model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In order to produce facial-expression-specified talking head videos, previousaudio-driven one-shot talking head methods need to use a reference video with amatching speaking style (i.e., facial expressions). However, finding videoswith a desired style may not be easy, potentially restricting theirapplication. In this work, we propose an expression-controllable one-shottalking head method, dubbed TalkCLIP, where the expression in a speech isspecified by the natural language. This would significantly ease the difficultyof searching for a video with a desired speaking style. Here, we firstconstruct a text-video paired talking head dataset, in which each video hasalternative prompt-alike descriptions. Specifically, our descriptions involvecoarse-level emotion annotations and facial action unit (AU) based fine-grainedannotations. Then, we introduce a CLIP-based style encoder that first projectsnatural language descriptions to the CLIP text embedding space and then alignsthe textual embeddings to the representations of speaking styles. As extensivetextual knowledge has been encoded by CLIP, our method can even generalize toinfer a speaking style whose description has not been seen during training.Extensive experiments demonstrate that our method achieves the advancedcapability of generating photo-realistic talking heads with vivid facialexpressions guided by text descriptions.", "output": "TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study 3D point cloud attribute compression using a volumetric approach:given a target volumetric attribute function $f : mathbb{R}^3 rightarrowmathbb{R}$, we quantize and encode parameter vector $theta$ thatcharacterizes $f$ at the encoder, for reconstruction$f_{hat{theta}}(mathbf{x})$ at known 3D points $mathbf{x}$'s at thedecoder. Extending a previous work Region Adaptive Hierarchical Transform(RAHT) that employs piecewise constant functions to span a nested sequence offunction spaces, we propose a feedforward linear network that implementshigher-order B-spline bases spanning function spaces withouteigen-decomposition. Feedforward network architecture means that the system isamenable to end-to-end neural learning. The key to our network is space-varyingconvolution, similar to a graph operator, whose weights are computed from theknown 3D geometry for normalization. We show that the number of layers in thenormalization at the encoder is equivalent to the number of terms in a matrixinverse Taylor series. Experimental results on real-world 3D point clouds showup to 2-3 dB gain over RAHT in energy compaction and 20-30% bitrate reduction.", "output": "Volumetric Attribute Compression for 3D Point Clouds using Feedforward Network with Geometric Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a method that trains a neural radiance field (NeRF) to encode notonly the appearance of the scene but also semantic correlations between scenepoints, regions, or entities -- aiming to capture their mutual co-variationpatterns. In contrast to the traditional first-order photometric reconstructionobjective, our method explicitly regularizes the learning dynamics to align theJacobians of highly-correlated entities, which proves to maximize the mutualinformation between them under random scene perturbations. By paying attentionto this second-order information, we can shape a NeRF to express semanticallymeaningful synergies when the network weights are changed by a delta along thegradient of a single entity, region, or even a point. To demonstrate the meritof this mutual information modeling, we leverage the coordinated behavior ofscene entities that emerges from our shaping to perform label propagation forsemantic and instance segmentation. Our experiments show that a JacobiNeRF ismore efficient in propagating annotations among 2D pixels and 3D pointscompared to NeRFs without mutual information shaping, especially in extremelysparse label regimes -- thus reducing annotation burden. The same machinery canfurther be used for entity selection or scene modifications.", "output": "JacobiNeRF: NeRF Shaping with Mutual Information Gradients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the problem of clothed human reconstruction from a single image oruncalibrated multi-view images. Existing methods struggle with reconstructingdetailed geometry of a clothed human and often require a calibrated setting formulti-view reconstruction. We propose a flexible framework which, by leveragingthe parametric SMPL-X model, can take an arbitrary number of input images toreconstruct a clothed human model under an uncalibrated setting. At the core ofour framework is our novel self-evolved signed distance field (SeSDF) modulewhich allows the framework to learn to deform the signed distance field (SDF)derived from the fitted SMPL-X model, such that detailed geometry reflectingthe actual clothed human can be encoded for better reconstruction. Besides, wepropose a simple method for self-calibration of multi-view images via thefitted SMPL-X parameters. This lifts the requirement of tedious manualcalibration and largely increases the flexibility of our method. Further, weintroduce an effective occlusion-aware feature fusion strategy to account forthe most useful features to reconstruct the human model. We thoroughly evaluateour framework on public benchmarks, demonstrating significant superiority overthe state-of-the-arts both qualitatively and quantitatively.", "output": "SeSDF: Self-evolved Signed Distance Field for Implicit 3D Clothed Human Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In computing, the aim of personalization is to train a model that caters to aspecific individual or group of people by optimizing one or more performancemetrics and adhering to specific constraints. In this paper, we discuss theneed for personalization in affective and personality computing (hereinafterreferred to as affective computing). We present a survey of state-of-the-artapproaches for personalization in affective computing. Our review spanstraining techniques and objectives towards the personalization of affectivecomputing models. We group existing approaches into seven categories: (1)Target-specific Models, (2) Group-specific Models, (3) Weighting-basedApproaches, (4) Fine-tuning Approaches, (5) Multitask Learning, (6)Generative-based Models, and (7) Feature Augmentation. Additionally, we providea statistical meta-analysis of the surveyed literature, analyzing theprevalence of different affective computing tasks, interaction modes,interaction contexts, and the level of personalization among the surveyedworks. Based on that, we provide a road-map for those who are interested inexploring this direction.", "output": "A Survey on Personalized Affective Computing in Human-Machine Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prostate cancer is one of the leading causes of cancer-related death in menworldwide. Like many cancers, diagnosis involves expert integration ofheterogeneous patient information such as imaging, clinical risk factors, andmore. For this reason, there have been many recent efforts toward deepmultimodal fusion of image and non-image data for clinical decision tasks. Manyof these studies propose methods to fuse learned features from each patientmodality, providing significant downstream improvements with techniques likecross-modal attention gating, Kronecker product fusion, orthogonalityregularization, and more. While these enhanced fusion operations can improveupon feature concatenation, they often come with an extremely high learningcapacity, meaning they are likely to overfit when applied even to small orlow-dimensional datasets. Rather than designing a highly expressive fusionoperation, we propose three simple methods for improved multimodal fusion withsmall datasets that aid optimization by generating auxiliary sources ofsupervision during training: extra supervision, clinical prediction, and densefusion. We validate the proposed approaches on prostate cancer diagnosis frompaired histopathology imaging and tabular clinical features. The proposedmethods are straightforward to implement and can be applied to anyclassification task with paired image and non-image data.", "output": "Improved Multimodal Fusion for Small Datasets with Auxiliary Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised learning of skeleton sequence encoders for action recognition hasreceived significant attention in recent times. However, learning such encoderswithout labels continues to be a challenging problem. While prior works haveshown promising results by applying contrastive learning to pose sequences, thequality of the learned representations is often observed to be closely tied todata augmentations that are used to craft the positives. However, augmentingpose sequences is a difficult task as the geometric constraints among theskeleton joints need to be enforced to make the augmentations realistic forthat action. In this work, we propose a new contrastive learning approach totrain models for skeleton-based action recognition without labels. Our keycontribution is a simple module, HaLP - to Hallucinate Latent Positives forcontrastive learning. Specifically, HaLP explores the latent space of poses insuitable directions to generate new positives. To this end, we present a noveloptimization formulation to solve for the synthetic positives with an explicitcontrol on their hardness. We propose approximations to the objective, makingthem solvable in closed form with minimal overhead. We show via experimentsthat using these generated positives within a standard contrastive learningframework leads to consistent improvements across benchmarks such as NTU-60,NTU-120, and PKU-II on tasks like linear evaluation, transfer learning, and kNNevaluation. Our code will be made available at", "output": "HaLP: Hallucinating Latent Positives for Skeleton-based Self-Supervised Learning of Actions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Becoming a (super) hero is almost every kid's dream. During their shelteredchildhood, they do whatever it takes to grow up to be one. Work hard, play hard-- all day long. But as they're getting older, distractions are more and morelikely to occur. They're getting off track. They start discovering what isfeared as simple math. Finally, they end up as a researcher, writing boring,non-impressive papers all day long because they only rely on simplemathematics. No top-tier conferences, no respect, no groupies. Life's over.To finally put an end to this tragedy, we propose a fundamentally newalgorithm, dubbed zero2hero, that turns every research paper into a scientificmasterpiece. Given a LaTeX document containing ridiculously simple math, basedon next-generation large language models, our system automaticallyover-complicates every single equation so that no one, including yourself, isable to understand what the hell is going on. Future reviewers will be blownaway by the complexity of your equations, immediately leading to acceptance.zero2hero gets you back on track, because you deserve to be ahero$^{text{TM}}$. Code leaked aturl{", "output": "From Zero to Hero: Convincing with Extremely Complicated Math."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Arbitrary style transfer has been demonstrated to be efficient in artisticimage generation. Previous methods either globally modulate the content featureignoring local details, or overly focus on the local structure details leadingto style leakage. In contrast to the literature, we propose a new schemetextit{``style kernel\"} that learns {em spatially adaptive kernels} forper-pixel stylization, where the convolutional kernels are dynamicallygenerated from the global style-content aligned feature and then the learnedkernels are applied to modulate the content feature at each spatial position.This new scheme allows flexible both global and local interactions between thecontent and style features such that the wanted styles can be easilytransferred to the content image while at the same time the content structurecan be easily preserved. To further enhance the flexibility of our styletransfer method, we propose a Style Alignment Encoding (SAE) modulecomplemented with a Content-based Gating Modulation (CGM) module for learningthe dynamic style kernels in focusing regions. Extensive experiments stronglydemonstrate that our proposed method outperforms state-of-the-art methods andexhibits superior performance in terms of visual quality and efficiency.", "output": "Learning Dynamic Style Kernels for Artistic Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Single domain generalization aims to train a generalizable model with onlyone source domain to perform well on arbitrary unseen target domains. Imageaugmentation based on Random Convolutions (RandConv), consisting of oneconvolution layer randomly initialized for each mini-batch, enables the modelto learn generalizable visual representations by distorting local texturesdespite its simple and lightweight structure. However, RandConv has structurallimitations in that the generated image easily loses semantics as the kernelsize increases, and lacks the inherent diversity of a single convolutionoperation. To solve the problem, we propose a Progressive Random Convolution(Pro-RandConv) method that recursively stacks random convolution layers with asmall kernel size instead of increasing the kernel size. This progressiveapproach can not only mitigate semantic distortions by reducing the influenceof pixels away from the center in the theoretical receptive field, but alsocreate more effective virtual domains by gradually increasing the stylediversity. In addition, we develop a basic random convolution layer into arandom convolution block including deformable offsets and affine transformationto support texture and contrast diversification, both of which are alsorandomly initialized. Without complex generators or adversarial learning, wedemonstrate that our simple yet effective augmentation strategy outperformsstate-of-the-art methods on single domain generalization benchmarks.", "output": "Progressive Random Convolutions for Single Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot class-incremental learning (FSCIL) aims at learning to classify newclasses continually from limited samples without forgetting the old classes.The mainstream framework tackling FSCIL is first to adopt the cross-entropy(CE) loss for training at the base session, then freeze the feature extractorto adapt to new classes. However, in this work, we find that the CE loss is notideal for the base session training as it suffers poor class separation interms of representations, which further degrades generalization to novelclasses. One tempting method to mitigate this problem is to apply an additionalnaive supervised contrastive learning (SCL) in the base session. Unfortunately,we find that although SCL can create a slightly better representationseparation among different base classes, it still struggles to separate baseclasses and new classes. Inspired by the observations made, we proposeSemantic-Aware Virtual Contrastive model (SAVC), a novel method thatfacilitates separation between new classes and base classes by introducingvirtual classes to SCL. These virtual classes, which are generated viapre-defined transformations, not only act as placeholders for unseen classes inthe representation space, but also provide diverse semantic information. Bylearning to recognize and contrast in the fantasy space fostered by virtualclasses, our SAVC significantly boosts base class separation and novel classgeneralization, achieving new state-of-the-art performance on the threewidely-used FSCIL benchmark datasets. Code is available at:", "output": "Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Incomplete multi-view clustering is a hot and emerging topic. It is wellknown that unavoidable data incompleteness greatly weakens the effectiveinformation of multi-view data. To date, existing incomplete multi-viewclustering methods usually bypass unavailable views according to prior missinginformation, which is considered as a second-best scheme based on evasion.Other methods that attempt to recover missing information are mostly applicableto specific two-view datasets. To handle these problems, in this paper, wepropose an information recovery-driven deep incomplete multi-view clusteringnetwork, termed as RecFormer. Concretely, a two-stage autoencoder network withthe self-attention structure is built to synchronously extract high-levelsemantic representations of multiple views and recover the missing data.Besides, we develop a recurrent graph reconstruction mechanism that cleverlyleverages the restored views to promote the representation learning and thefurther data reconstruction. Visualization of recovery results are given andsufficient experimental results confirm that our RecFormer has obviousadvantages over other top methods.", "output": "Information Recovery-Driven Deep Incomplete Multi-view Clustering Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical imaging systems are often evaluated and optimized via objective, ortask-specific, measures of image quality (IQ) that quantify the performance ofan observer on a specific clinically-relevant task. The performance of theBayesian Ideal Observer (IO) sets an upper limit among all observers, numericalor human, and has been advocated for use as a figure-of-merit (FOM) forevaluating and optimizing medical imaging systems. However, the IO teststatistic corresponds to the likelihood ratio that is intractable to compute inthe majority of cases. A sampling-based method that employs Markov-Chain MonteCarlo (MCMC) techniques was previously proposed to estimate the IO performance.However, current applications of MCMC methods for IO approximation have beenlimited to a small number of situations where the considered distribution ofto-be-imaged objects can be described by a relatively simple stochastic objectmodel (SOM). As such, there remains an important need to extend the domain ofapplicability of MCMC methods to address a large variety of scenarios whereIO-based assessments are needed but the associated SOMs have not beenavailable. In this study, a novel MCMC method that employs a generativeadversarial network (GAN)-based SOM, referred to as MCMC-GAN, is described andevaluated. The MCMC-GAN method was quantitatively validated by use oftest-cases for which reference solutions were available. The resultsdemonstrate that the MCMC-GAN method can extend the domain of applicability ofMCMC methods for conducting IO analyses of medical imaging systems.", "output": "Ideal Observer Computation by Use of Markov-Chain Monte Carlo with Generative Adversarial Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Malicious perturbations embedded in input data, known as Trojan attacks, cancause neural networks to misbehave. However, the impact of a Trojan attack isreduced during fine-tuning of the model, which involves transferring knowledgefrom a pretrained large-scale model like visual question answering (VQA) to thetarget model. To mitigate the effects of a Trojan attack, replacing andfine-tuning multiple layers of the pretrained model is possible. This researchfocuses on sample efficiency, stealthiness and variation, and robustness tomodel fine-tuning. To address these challenges, we propose an instance-levelTrojan attack that generates diverse Trojans across input samples andmodalities. Adversarial learning establishes a correlation between a specifiedperturbation layer and the misbehavior of the fine-tuned model. We conductedextensive experiments on the VQA-v2 dataset using a range of metrics. Theresults show that our proposed method can effectively adapt to a fine-tunedmodel with minimal samples. Specifically, we found that a model with a singlefine-tuning layer can be compromised using a single shot of adversarialsamples, while a model with more fine-tuning layers can be compromised usingonly a few shots.", "output": "Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a Dual-Stream Pyramid Registration Network (referred asDual-PRNet) for unsupervised 3D medical image registration. Unlike recentCNN-based registration approaches, such as VoxelMorph, which explores asingle-stream encoder-decoder network to compute a registration fields from apair of 3D volumes, we design a two-stream architecture able to computemulti-scale registration fields from convolutional feature pyramids. Ourcontributions are two-fold: (i) we design a two-stream 3D encoder-decodernetwork which computes two convolutional feature pyramids separately for a pairof input volumes, resulting in strong deep representations that are meaningfulfor deformation estimation; (ii) we propose a pyramid registration module ableto predict multi-scale registration fields directly from the decoding featurepyramids. This allows it to refine the registration fields gradually in acoarse-to-fine manner via sequential warping, and enable the model with thecapability for handling significant deformations between two volumes, such aslarge displacements in spatial domain or slice space. The proposed Dual-PRNetis evaluated on two standard benchmarks for brain MRI registration, where itoutperforms the state-of-the-art approaches by a large margin, e.g., havingimprovements over recent VoxelMorph [2] with 0.683-&gt;0.778 on the LPBA40, and0.511-&gt;0.631 on the Mindboggle101, in term of average Dice score. Code isavailable at: ", "output": "Dual-Stream Pyramid Registration Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Shape-Text matching is an important task of high-level shape understanding.Current methods mainly represent a 3D shape as multiple 2D rendered views,which obviously can not be understood well due to the structural ambiguitycaused by self-occlusion in the limited number of views. To resolve this issue,we directly represent 3D shapes as point clouds, and propose to learn jointembedding of point clouds and texts by bidirectional matching between partsfrom shapes and words from texts. Specifically, we first segment the pointclouds into parts, and then leverage optimal transport method to match partsand words in an optimized feature space, where each part is represented byaggregating features of all points within it and each word is abstracted by itscontextual information. We optimize the feature space in order to enlarge thesimilarities between the paired training samples, while simultaneouslymaximizing the margin between the unpaired ones. Experiments demonstrate thatour method achieves a significant improvement in accuracy over the SOTAs onmulti-modal retrieval tasks under the Text2Shape dataset. Codes are availableat ", "output": "Parts2Words: Learning Joint Embedding of Point Clouds and Texts by Bidirectional Matching between Parts and Words."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The aortic vessel tree is composed of the aorta and its branching arteries,and plays a key role in supplying the whole body with blood. Aortic diseases,like aneurysms or dissections, can lead to an aortic rupture, whose treatmentwith open surgery is highly risky. Therefore, patients commonly undergo drugtreatment under constant monitoring, which requires regular inspections of thevessels through imaging. The standard imaging modality for diagnosis andmonitoring is computed tomography (CT), which can provide a detailed picture ofthe aorta and its branching vessels if completed with a contrast agent, calledCT angiography (CTA). Optimally, the whole aortic vessel tree geometry fromconsecutive CTAs is overlaid and compared. This allows not only detection ofchanges in the aorta, but also of its branches, caused by the primary pathologyor newly developed. When performed manually, this reconstruction requires sliceby slice contouring, which could easily take a whole day for a single aorticvessel tree, and is therefore not feasible in clinical practice. Automatic orsemi-automatic vessel tree segmentation algorithms, however, can complete thistask in a fraction of the manual execution time and run in parallel to theclinical routine of the clinicians. In this paper, we systematically reviewcomputing techniques for the automatic and semi-automatic segmentation of theaortic vessel tree. The review concludes with an in-depth discussion on howclose these state-of-the-art approaches are to an application in clinicalpractice and how active this research field is, taking into account the numberof publications, datasets and challenges.", "output": "AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper aims to illustrate the concept-emerging phenomenon in a trainedDNN. Specifically, we find that the inference score of a DNN can bedisentangled into the effects of a few interactive concepts. These concepts canbe understood as causal patterns in a sparse, symbolic causal graph, whichexplains the DNN. The faithfulness of using such a causal graph to explain theDNN is theoretically guaranteed, because we prove that the causal graph canwell mimic the DNN's outputs on an exponential number of different maskedsamples. Besides, such a causal graph can be further simplified and re-writtenas an And-Or graph (AOG), without losing much explanation accuracy.", "output": "Defining and Quantifying the Emergence of Sparse Concepts in DNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Registering point clouds of forest environments is an essential prerequisitefor LiDAR applications in precision forestry. State-of-the-art methods forforest point cloud registration require the extraction of individual treeattributes, and they have an efficiency bottleneck when dealing with pointclouds of real-world forests with dense trees. We propose an automatic, robust,and efficient method for the registration of forest point clouds. Our approachfirst locates tree stems from raw point clouds and then matches the stems basedon their relative spatial relationship to determine the registrationtransformation. The algorithm requires no extra individual tree attributes andhas quadratic complexity to the number of trees in the environment, allowing itto align point clouds of large forest environments. Extensive experiments onforest terrestrial point clouds have revealed that our method inherits theeffectiveness and robustness of the stem-based registration strategy whileexceedingly increasing its efficiency. Besides, we introduce a new benchmarkdataset that complements the very few existing open datasets for thedevelopment and evaluation of registration methods for forest point clouds. Thesource code of our method and the dataset are available at", "output": "GlobalMatch: Registration of Forest Terrestrial Point Clouds by Global Matching of Relative Stem Positions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prior work for articulated 3D shape reconstruction often relies onspecialized sensors (e.g., synchronized multi-camera systems), or pre-built 3Ddeformable models (e.g., SMAL or SMPL). Such methods are not able to scale todiverse sets of objects in the wild. We present BANMo, a method that requiresneither a specialized sensor nor a pre-defined template shape. BANMo buildshigh-fidelity, articulated 3D models (including shape and animatable skinningweights) from many monocular casual videos in a differentiable renderingframework. While the use of many videos provides more coverage of camera viewsand object articulations, they introduce significant challenges in establishingcorrespondence across scenes with different backgrounds, illuminationconditions, etc. Our key insight is to merge three schools of thought; (1)classic deformable shape models that make use of articulated bones and blendskinning, (2) volumetric neural radiance fields (NeRFs) that are amenable togradient-based optimization, and (3) canonical embeddings that generatecorrespondences between pixels and an articulated model. We introduce neuralblend skinning models that allow for differentiable and invertible articulateddeformations. When combined with canonical embeddings, such models allow us toestablish dense correspondences across videos that can be self-supervised withcycle consistency. On real and synthetic datasets, BANMo shows higher-fidelity3D reconstructions than prior works for humans and animals, with the ability torender realistic images from novel viewpoints and poses. Project webpage:banmo-www.github.io .", "output": "BANMo: Building Animatable 3D Neural Models from Many Casual Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional NeuralNetwork (CNN) for pose estimation of noncooperative spacecraft across domaingap. SPNv2 is a multi-scale, multi-task CNN which consists of a sharedmulti-scale feature encoder and multiple prediction heads that performdifferent tasks on a shared feature output. These tasks are all related todetection and pose estimation of a target spacecraft from an image, such asprediction of pre-defined satellite keypoints, direct pose regression, andbinary segmentation of the satellite foreground. It is shown that by jointlytraining on different yet related tasks with extensive data augmentations onsynthetic images only, the shared encoder learns features that are commonacross image domains that have fundamentally different visual characteristicscompared to synthetic images. This work also introduces Online DomainRefinement (ODR) which refines the parameters of the normalization layers ofSPNv2 on the target domain images online at deployment. Specifically, ODRperforms self-supervised entropy minimization of the predicted satelliteforeground, thereby improving the CNN's performance on the target domain imageswithout their pose labels and with minimal computational efforts. The GitHubrepository for SPNv2 is available at ", "output": "Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite recent advances of AI, story understanding remains an open andunder-investigated problem. We collect, preprocess, and publicly release avideo-language story dataset, Synopses of Movie Narratives (SYMON), containing5,193 video summaries of popular movies and TV series. SYMON capturesnaturalistic story-telling videos for human audience made by human creators. Asa prototypical and naturalistic story dataset, SYMON features high coverage ofmultimodal story events, abundant mental-state descriptions, and large semanticgaps between the visual and the textual modalities. We establish benchmarks onvideo-text retrieval and zero-shot alignment on movie summary videos, whichshowcase the importance of in-domain data in story understanding. With SYMON,we hope to lay the groundwork for progress in multimodal story understanding.", "output": "Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic image-based pavement distress detection and recognition are vitalfor pavement maintenance and management. However, existing deep learning-basedmethods largely omit the specific characteristics of pavement images, such ashigh image resolution and low distress area ratio, and are not end-to-endtrainable. In this paper, we present a series of simple yet effectiveend-to-end deep learning approaches named Weakly Supervised Patch LabelInference Networks (WSPLIN) for efficiently addressing these tasks undervarious application settings. WSPLIN transforms the fully supervised pavementimage classification problem into a weakly supervised pavement patchclassification problem for solutions. Specifically, WSPLIN first divides thepavement image under different scales into patches with different collectionstrategies and then employs a Patch Label Inference Network (PLIN) to infer thelabels of these patches to fully exploit the resolution and scale information.Notably, we design a patch label sparsity constraint based on the priorknowledge of distress distribution and leverage the Comprehensive DecisionNetwork (CDN) to guide the training of PLIN in a weakly supervised way.Therefore, the patch labels produced by PLIN provide interpretable intermediateinformation, such as the rough location and the type of distress. We evaluateour method on a large-scale bituminous pavement distress dataset named CQU-BPDDand the augmented Crack500 (Crack500-PDD) dataset, which is a newly constructedpavement distress detection dataset augmented from the Crack500. Extensiveresults demonstrate the superiority of our method over baselines in bothperformance and efficiency. The source codes of WSPLIN are released on", "output": "Weakly Supervised Patch Label Inference Networks for Efficient Pavement Distress Detection and Recognition in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The homogeneity of filter media is important for material selection andquality control, along with the specific weight (nominal grammage) and thedistribution of the local weight. Cloudiness or formation is a concept used todescribe deviations from homogeneity in filter media. We suggest to derive thecloudiness index from the power spectrum of the relative local areal weight,integrated over a selected frequency range. The power spectrum captures theenergy density in a broad spectral range. Moreover, under certain conditions,the structure of a nonwoven is fully characterized by the areal weight, thevariance of the local areal weight, and the power spectrum. Consequently, thepower spectrum is the parameter that exclusively reflects the cloudiness. Here,we address questions arising from practical application. The most prominent isthe choice of the spectral band. It certainly depends on the characteristic\"size of the clouds\", but is limited by the size and lateral resolution of theimages. We show that the cloudiness index based on the power spectrum of therelative local areal weight is theoretically well founded and can be robustlymeasured from image data. Choosing the spectral band allows to capture thecloudiness either visually perceived or found to be decisive for productproperties. It is thus well suited to build a technical standard on it.", "output": "Assessing cloudiness in nonwovens."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Early action prediction deals with inferring the ongoing action frompartially-observed videos, typically at the outset of the video. We propose abottleneck-based attention model that captures the evolution of the action,through progressive sampling over fine-to-coarse scales. Our proposed TemporalProgressive (TemPr) model is composed of multiple attention towers, one foreach scale. The predicted action label is based on the collective agreementconsidering confidences of these towers. Extensive experiments over four videodatasets showcase state-of-the-art performance on the task of Early ActionPrediction across a range of encoder architectures. We demonstrate theeffectiveness and consistency of TemPr through detailed ablations.", "output": "The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-and-language (V&amp;L) models pretrained on large-scale multimodal datahave demonstrated strong performance on various tasks such as image captioningand visual question answering (VQA). The quality of such models is commonlyassessed by measuring their performance on unseen data that typically comesfrom the same distribution as the training data. However, when evaluated underout-of-distribution (out-of-dataset) settings for VQA, we observe that thesemodels exhibit poor generalization. We comprehensively evaluate two pretrainedV&amp;L models under different settings (i.e. classification and open-ended textgeneration) by conducting cross-dataset evaluations. We find that these modelstend to learn to solve the benchmark, rather than learning the high-levelskills required by the VQA task. We also find that in most cases generativemodels are less susceptible to shifts in data distribution compared todiscriminative ones, and that multimodal pretraining is generally helpful forOOD generalization. Finally, we revisit assumptions underlying the use ofautomatic VQA evaluation metrics, and empirically show that their stringentnature repeatedly penalizes models for correct responses.", "output": "Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In strong adversarial attacks against deep neural networks (DNN), thegenerated adversarial example will mislead the DNN-implemented classifier bydestroying the output features of the last layer. To enhance the robustness ofthe classifier, in our paper, a textbf{F}eature textbf{A}nalysis andtextbf{C}onditional textbf{M}atching prediction distribution (FACM) model isproposed to utilize the features of intermediate layers to correct theclassification. Specifically, we first prove that the intermediate layers ofthe classifier can still retain effective features for the original category,which is defined as the correction property in our paper. According to this, wepropose the FACM model consisting of textbf{F}eature textbf{A}nalysis (FA)correction module, textbf{C}onditional textbf{M}atching textbf{P}redictiontextbf{D}istribution (CMPD) correction module and decision module. The FAcorrection module is the fully connected layers constructed with the output ofthe intermediate layers as the input to correct the classification of theclassifier. The CMPD correction module is a conditional auto-encoder, which cannot only use the output of intermediate layers as the condition to accelerateconvergence but also mitigate the negative effect of adversarial exampletraining with the Kullback-Leibler loss to match prediction distribution.Through the empirically verified diversity property, the correction modules canbe implemented synergistically to reduce the adversarial subspace. Hence, thedecision module is proposed to integrate the correction modules to enhance theDNN classifier's robustness. Specially, our model can be achieved byfine-tuning and can be combined with other model-specific defenses.", "output": "FACM: Intermediate Layer Still Retain Effective Features against Adversarial Examples."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classification has been the focal point of research on adversarial attacks,but only a few works investigate methods suited to denser prediction tasks,such as semantic segmentation. The methods proposed in these works do notaccurately solve the adversarial segmentation problem and, therefore,overestimate the size of the perturbations required to fool models. Here, wepropose a white-box attack for these models based on a proximal splitting toproduce adversarial perturbations with much smaller $ell_infty$ norms. Ourattack can handle large numbers of constraints within a nonconvex minimizationframework via an Augmented Lagrangian approach, coupled with adaptiveconstraint scaling and masking strategies. We demonstrate that our attacksignificantly outperforms previously proposed ones, as well as classificationattacks that we adapted for segmentation, providing a first comprehensivebenchmark for this dense task.", "output": "Proximal Splitting Adversarial Attacks for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Re-initializing a neural network during training has been observed to improvegeneralization in recent works. Yet it is neither widely adopted in deeplearning practice nor is it often used in state-of-the-art training protocols.This raises the question of when re-initialization works, and whether it shouldbe used together with regularization techniques such as data augmentation,weight decay and learning rate schedules. In this work, we conduct an extensiveempirical comparison of standard training with a selection of re-initializationmethods to answer this question, training over 15,000 models on a variety ofimage classification benchmarks. We first establish that such methods areconsistently beneficial for generalization in the absence of any otherregularization. However, when deployed alongside other carefully tunedregularization techniques, re-initialization methods offer little to no addedbenefit for generalization, although optimal generalization performance becomesless sensitive to the choice of learning rate and weight decay hyperparameters.To investigate the impact of re-initialization methods on noisy data, we alsoconsider learning under label noise. Surprisingly, in this case,re-initialization significantly improves upon standard training, even in thepresence of other carefully tuned regularization techniques.", "output": "When Does Re-initialization Work?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning based solutions are being succesfully implemented for a widevariety of applications. Most notably, clinical use-cases have gained anincreased interest and have been the main driver behind some of thecutting-edge data-driven algorithms proposed in the last years. Forapplications like sparse-view tomographic reconstructions, where the amount ofmeasurement data is small in order to keep acquisition time short and radiationdose low, reduction of the streaking artifacts has prompted the development ofdata-driven denoising algorithms with the main goal of obtaining diagnosticallyviable images with only a subset of a full-scan data. We propose WNet, adata-driven dual-domain denoising model which contains a trainablereconstruction layer for sparse-view artifact denoising. Two encoder-decodernetworks perform denoising in both sinogram- and reconstruction-domainsimultaneously, while a third layer implementing the Filtered Backprojectionalgorithm is sandwiched between the first two and takes care of thereconstruction operation. We investigate the performance of the network onsparse-view chest CT scans, and we highlight the added benefit of having atrainable reconstruction layer over the more conventional fixed ones. We trainand test our network on two clinically relevant datasets and we compare theobtained results with three different types of sparse-view CT denoising andreconstruction algorithms.", "output": "WNet: A data-driven dual-domain denoising model for sparse-view computed tomography with a trainable reconstruction layer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing long-tailed classification (LT) methods only focus on tackling theclass-wise imbalance that head classes have more samples than tail classes, butoverlook the attribute-wise imbalance. In fact, even if the class is balanced,samples within each class may still be long-tailed due to the varyingattributes. Note that the latter is fundamentally more ubiquitous andchallenging than the former because attributes are not just implicit for mostdatasets, but also combinatorially complex, thus prohibitively expensive to bebalanced. Therefore, we introduce a novel research problem: GeneralizedLong-Tailed classification (GLT), to jointly consider both kinds of imbalances.By \"generalized\", we mean that a GLT method should naturally solve thetraditional LT, but not vice versa. Not surprisingly, we find that mostclass-wise LT methods degenerate in our proposed two benchmarks: ImageNet-GLTand MSCOCO-GLT. We argue that it is because they over-emphasize the adjustmentof class distribution while neglecting to learn attribute-invariant features.To this end, we propose an Invariant Feature Learning (IFL) method as the firststrong baseline for GLT. IFL first discovers environments with divergentintra-class distributions from the imperfect predictions and then learnsinvariant features across them. Promisingly, as an improved feature backbone,IFL boosts all the LT line-up: one/two-stage re-balance, augmentation, andensemble. Codes and benchmarks are available on Github:", "output": "Invariant Feature Learning for Generalized Long-Tailed Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusion models represent a recent emerging topic in computervision, demonstrating remarkable results in the area of generative modeling. Adiffusion model is a deep generative model that is based on two stages, aforward diffusion stage and a reverse diffusion stage. In the forward diffusionstage, the input data is gradually perturbed over several steps by addingGaussian noise. In the reverse stage, a model is tasked at recovering theoriginal input data by learning to gradually reverse the diffusion process,step by step. Diffusion models are widely appreciated for the quality anddiversity of the generated samples, despite their known computational burdens,i.e. low speeds due to the high number of steps involved during sampling. Inthis survey, we provide a comprehensive review of articles on denoisingdiffusion models applied in vision, comprising both theoretical and practicalcontributions in the field. First, we identify and present three genericdiffusion modeling frameworks, which are based on denoising diffusionprobabilistic models, noise conditioned score networks, and stochasticdifferential equations. We further discuss the relations between diffusionmodels and other deep generative models, including variational auto-encoders,generative adversarial networks, energy-based models, autoregressive models andnormalizing flows. Then, we introduce a multi-perspective categorization ofdiffusion models applied in computer vision. Finally, we illustrate the currentlimitations of diffusion models and envision some interesting directions forfuture research.", "output": "Diffusion Models in Vision: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Universal domain adaptation (UniDA) aims to transfer the knowledge from alabeled source domain to an unlabeled target domain without any assumptions ofthe label sets, which requires distinguishing the unknown samples from theknown ones in the target domain. A main challenge of UniDA is that thenonidentical label sets cause the misalignment between the two domains.Moreover, the domain discrepancy and the supervised objectives in the sourcedomain easily lead the whole model to be biased towards the common classes andproduce overconfident predictions for unknown samples. To address the abovechallenging problems, we propose a new uncertainty-guided UniDA framework.Firstly, we introduce an empirical estimation of the probability of a targetsample belonging to the unknown class which fully exploits the distribution ofthe target samples in the latent space. Then, based on the estimation, wepropose a novel neighbors searching scheme in a linear subspace with a$delta$-filter to estimate the uncertainty score of a target sample anddiscover unknown samples. It fully utilizes the relationship between a targetsample and its neighbors in the source domain to avoid the influence of domainmisalignment. Secondly, this paper well balances the confidences of predictionsfor both known and unknown samples through an uncertainty-guided margin lossbased on the confidences of discovered unknown samples, which can reduce thegap between the intra-class variances of known classes with respect to theunknown class. Finally, experiments on three public datasets demonstrate thatour method significantly outperforms existing state-of-the-art methods.", "output": "Provably Uncertainty-Guided Universal Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Markerless human motion capture (mocap) from multiple RGB cameras is a widelystudied problem. Existing methods either need calibrated cameras or calibratethem relative to a static camera, which acts as the reference frame for themocap system. The calibration step has to be done a priori for every capturesession, which is a tedious process, and re-calibration is required whenevercameras are intentionally or accidentally moved. In this paper, we propose amocap method which uses multiple static and moving extrinsically uncalibratedRGB cameras. The key components of our method are as follows. First, since thecameras and the subject can move freely, we select the ground plane as a commonreference to represent both the body and the camera motions unlike existingmethods which represent bodies in the camera coordinate. Second, we learn aprobability distribution of short human motion sequences ($sim$1sec) relativeto the ground plane and leverage it to disambiguate between the camera andhuman motion. Third, we use this distribution as a motion prior in a novelmulti-stage optimization approach to fit the SMPL human body model and thecamera poses to the human body keypoints on the images. Finally, we show thatour method can work on a variety of datasets ranging from aerial cameras tosmartphones. It also gives more accurate results compared to thestate-of-the-art on the task of monocular human mocap with a static camera. Ourcode is available for research purposes on", "output": "SmartMocap: Joint Estimation of Human and Camera Motion using Uncalibrated RGB Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "PointGoal navigation in indoor environment is a fundamental task for personalrobots to navigate to a specified point. Recent studies solved this PointGoalnavigation task with near-perfect success rate in photo-realistically simulatedenvironments, under the assumptions with noiseless actuation and mostimportantly, perfect localization with GPS and compass sensors. However,accurate GPS signalis difficult to be obtained in real indoor environment. Toimprove the PointGoal navigation accuracy without GPS signal, we use visualodometry (VO) and propose a novel action integration module (AIM) trained inunsupervised manner. Sepecifically, unsupervised VO computes the relative poseof the agent from the re-projection error of two adjacent frames, and thenreplaces the accurate GPS signal with the path integration. The pseudo positionestimated by VO is used to train action integration which assists agent toupdate their internal perception of location and helps improve the success rateof navigation. The training and inference process only use RGB, depth,collision as well as self-action information. The experiments show that theproposed system achieves satisfactory results and outperforms the partiallysupervised learning algorithms on the popular Gibson dataset.", "output": "Unsupervised Visual Odometry and Action Integration for PointGoal Navigation in Indoor Environment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Group equivariant convolutional neural networks (G-CNNs) have beensuccessfully applied in geometric deep learning. Typically, G-CNNs have theadvantage over CNNs that they do not waste network capacity on trainingsymmetries that should have been hard-coded in the network. The recentlyintroduced framework of PDE-based G-CNNs (PDE-G-CNNs) generalises G-CNNs.PDE-G-CNNs have the core advantages that they simultaneously 1) reduce networkcomplexity, 2) increase classification performance, and 3) provide geometricinterpretability. Their implementations primarily consist of linear andmorphological convolutions with kernels.In this paper we show that the previously suggested approximativemorphological kernels do not always accurately approximate the exact kernelsaccurately. More specifically, depending on the spatial anisotropy of theRiemannian metric, we argue that one must resort to sub-Riemannianapproximations. We solve this problem by providing a new approximative kernelthat works regardless of the anisotropy. We provide new theorems with bettererror estimates of the approximative kernels, and prove that they all carry thesame reflectional symmetries as the exact ones.We test the effectiveness of multiple approximative kernels within thePDE-G-CNN framework on two datasets, and observe an improvement with the newapproximative kernels. We report that the PDE-G-CNNs again allow for aconsiderable reduction of network complexity while having comparable or betterperformance than G-CNNs and CNNs on the two datasets. Moreover, PDE-G-CNNs havethe advantage of better geometric interpretability over G-CNNs, as themorphological kernels are related to association fields from neurogeometry.", "output": "Analysis of (sub-)Riemannian PDE-G-CNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Soft prompt learning has recently emerged as one of the methods of choice foradapting V&amp;L models to a downstream task using a few training examples.However, current methods significantly overfit the training data, sufferingfrom large accuracy degradation when tested on unseen classes from the samedomain. To this end, in this paper, we make the following 4 contributions: (1)To alleviate base class overfitting, we propose a novel Language-Aware SoftPrompting (LASP) learning method by means of a text-to-text cross-entropy lossthat maximizes the probability of the learned prompts to be correctlyclassified with respect to pre-defined hand-crafted textual prompts. (2) Toincrease the representation capacity of the prompts, we propose grouped LASPwhere each group of prompts is optimized with respect to a separate subset oftextual prompts. (3) We identify a visual-language misalignment introduced byprompt learning and LASP, and more importantly, propose a re-calibrationmechanism to address it. (4) We show that LASP is inherently amenable toincluding, during training, virtual classes, i.e. class names for which novisual samples are available, further increasing the robustness of the learnedprompts. Through evaluations on 11 datasets, we show that our approach (a)significantly outperforms all prior works on soft prompting, and (b) matchesand surpasses, for the first time, the accuracy on novel classes obtained byhand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be madeavailable at ", "output": "LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pre-trained vision-language (V-L) models such as CLIP have shown excellentgeneralization ability to downstream tasks. However, they are sensitive to thechoice of input text prompts and require careful selection of prompt templatesto perform well. Inspired by the Natural Language Processing (NLP) literature,recent CLIP adaptation approaches learn prompts as the textual inputs tofine-tune CLIP for downstream tasks. We note that using prompting to adaptrepresentations in a single branch of CLIP (language or vision) is sub-optimalsince it does not allow the flexibility to dynamically adjust bothrepresentation spaces on a downstream task. In this work, we proposeMulti-modal Prompt Learning (MaPLe) for both vision and language branches toimprove alignment between the vision and language representations. Our designpromotes strong coupling between the vision-language prompts to ensure mutualsynergy and discourages learning independent uni-modal solutions. Further, welearn separate prompts across different early stages to progressively model thestage-wise feature relationships to allow rich context learning. We evaluatethe effectiveness of our approach on three representative tasks ofgeneralization to novel classes, new target datasets and unseen domain shifts.Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorableperformance and achieves an absolute gain of 3.45% on novel classes and 2.72%on overall harmonic-mean, averaged over 11 diverse image recognition datasets.Our code and pre-trained models are available at", "output": "MaPLe: Multi-modal Prompt Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-vocabulary semantic segmentation aims to segment an image into semanticregions according to text descriptions, which may not have been seen duringtraining. Recent two-stage methods first generate class-agnostic mask proposalsand then leverage pre-trained vision-language models, e.g., CLIP, to classifymasked regions. We identify the performance bottleneck of this paradigm to bethe pre-trained CLIP model, since it does not perform well on masked images. Toaddress this, we propose to finetune CLIP on a collection of masked imageregions and their corresponding text descriptions. We collect training data bymining an existing image-caption dataset (e.g., COCO Captions), using CLIP tomatch masked image regions to nouns in the image captions. Compared with themore precise and manually annotated segmentation labels with fixed classes(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retainCLIP's generalization ability. Along with finetuning the entire model, weutilize the \"blank\" areas in masked images using a method we dub mask prompttuning. Experiments demonstrate mask prompt tuning brings significantimprovement without modifying any weights of CLIP, and it can further improve afully finetuned model. In particular, when trained on COCO and evaluated onADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than theprevious state-of-the-art. For the first time, open-vocabulary generalistmodels match the performance of supervised specialist models in 2017 withoutdataset-specific adaptations.", "output": "Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have demonstrated remarkable progress in image generationquality, especially when guidance is used to control the generative process.However, guidance requires a large amount of image-annotation pairs fortraining and is thus dependent on their availability, correctness andunbiasedness. In this paper, we eliminate the need for such annotation byinstead leveraging the flexibility of self-supervision signals to design aframework for self-guided diffusion models. By leveraging a feature extractionfunction and a self-annotation function, our method provides guidance signalsat various image granularities: from the level of holistic images to objectboxes and even segmentation masks. Our experiments on single-label andmulti-label image datasets demonstrate that self-labeled guidance alwaysoutperforms diffusion models without guidance and may even surpass guidancebased on ground-truth labels, especially on unbalanced data. When equipped withself-supervised box or mask proposals, our method further generates visuallydiverse yet semantically consistent images, without the need for any class,box, or segment label annotation. Self-guided diffusion is simple, flexible andexpected to profit from deployment at scale.", "output": "Self-Guided Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While 6D object pose estimation has wide applications across computer visionand robotics, it remains far from being solved due to the lack of annotations.The problem becomes even more challenging when moving to category-level 6Dpose, which requires generalization to unseen instances. Current approaches arerestricted by leveraging annotations from simulation or collected from humans.In this paper, we overcome this barrier by introducing a self-supervisedlearning approach trained directly on large-scale real-world object videos forcategory-level 6D pose estimation in the wild. Our framework reconstructs thecanonical 3D shape of an object category and learns dense correspondencesbetween input images and the canonical shape via surface embedding. Fortraining, we propose novel geometrical cycle-consistency losses which constructcycles across 2D-3D spaces, across different instances and different timesteps. The learned correspondence can be applied for 6D pose estimation andother downstream tasks such as keypoint transfer. Surprisingly, our method,without any human annotations or simulators, can achieve on-par or even betterperformance than previous supervised or semi-supervised methods on in-the-wildimages. Our project page is:  .", "output": "Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic rib labeling and anatomical centerline extraction are commonprerequisites for various clinical applications. Prior studies either usein-house datasets that are inaccessible to communities, or focus on ribsegmentation that neglects the clinical significance of rib labeling. Toaddress these issues, we extend our prior dataset (RibSeg) on the binary ribsegmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CTscans (15,466 individual ribs in total) and annotations manually inspected byexperts for rib labeling and anatomical centerline extraction. Based on theRibSeg v2, we develop a pipeline including deep learning-based methods for riblabeling, and a skeletonization-based method for centerline extraction. Toimprove computational efficiency, we propose a sparse point cloudrepresentation of CT scans and compare it with standard dense voxel grids.Moreover, we design and analyze evaluation metrics to address the keychallenges of each task. Our dataset, code, and model are available online tofacilitate open research at ", "output": "RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Abductive reasoning aims to make the most likely inference for a given set ofincomplete observations. In this work, we propose a new task called abductiveaction inference, in which given a situation, the model answers the question`what actions were executed by the human in order to arrive in the currentstate?'. Given a state, we investigate three abductive inference problems:action set prediction, action sequence prediction, and abductive actionverification. We benchmark several SOTA models such as Transformers, Graphneural networks, CLIP, BLIP, end-to-end trained Slow-Fast, and Resnet50-3Dmodels. Our newly proposed object-relational BiGED model outperforms all othermethods on this challenging task on the Action Genome dataset. Codes will bemade available.", "output": "Abductive Action Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) have recently become the state-of-the-art acrossmany computer vision tasks. In contrast to convolutional networks (CNNs), ViTsenable global information sharing even within shallow layers of a network,i.e., among high-resolution features. However, this perk was later overlookedwith the success of pyramid architectures such as Swin Transformer, which showbetter performance-complexity trade-offs. In this paper, we present a simpleand efficient add-on component (termed GrafT) that considers globaldependencies and multi-scale information throughout the network, in both high-and low-resolution features alike. It has the flexibility of branching out atarbitrary depths and shares most of the parameters and computations of thebackbone. GrafT shows consistent gains over various well-known models whichincludes both hybrid and pure Transformer types, both homogeneous and pyramidstructures, and various self-attention methods. In particular, it largelybenefits mobile-size models by providing high-level semantics. On theImageNet-1k dataset, GrafT delivers +3.9%, +1.4%, and +1.9% top-1 accuracyimprovement to DeiT-T, Swin-T, and MobileViT-XXS, respectively. Our code andmodels will be made available.", "output": "Grafting Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating videos is a complex task that is accomplished by generating a setof temporally coherent images frame-by-frame. This limits the expressivity ofvideos to only image-based operations on the individual video frames needingnetwork designs to obtain temporally coherent trajectories in the underlyingimage space. We propose INR-V, a video representation network that learns acontinuous space for video-based generative tasks. INR-V parameterizes videosusing implicit neural representations (INRs), a multi-layered perceptron thatpredicts an RGB value for each input pixel location of the video. The INR ispredicted using a meta-network which is a hypernetwork trained on neuralrepresentations of multiple video instances. Later, the meta-network can besampled to generate diverse novel videos enabling many downstream video-basedgenerative tasks. Interestingly, we find that conditional regularization andprogressive weight initialization play a crucial role in obtaining INR-V. Therepresentation space learned by INR-V is more expressive than an image spaceshowcasing many interesting properties not possible with the existing works.For instance, INR-V can smoothly interpolate intermediate videos between knownvideo instances (such as intermediate identities, expressions, and poses inface videos). It can also in-paint missing portions in videos to recovertemporally coherent full videos. In this work, we evaluate the space learned byINR-V on diverse generative tasks such as video interpolation, novel videogeneration, video inversion, and video inpainting against the existingbaselines. INR-V significantly outperforms the baselines on several of thesedemonstrated tasks, clearly showcasing the potential of the proposedrepresentation space.", "output": "INR-V: A Continuous Representation Space for Video-based Generative Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Today's computer vision models achieve human or near-human level performanceacross a wide variety of vision tasks. However, their architectures, data, andlearning algorithms differ in numerous ways from those that give rise to humanvision. In this paper, we investigate the factors that affect the alignmentbetween the representations learned by neural networks and human mentalrepresentations inferred from behavioral responses. We find that model scaleand architecture have essentially no effect on the alignment with humanbehavioral responses, whereas the training dataset and objective function bothhave a much larger impact. These findings are consistent across three datasetsof human similarity judgments collected using two different tasks. Lineartransformations of neural network representations learned from behavioralresponses from one dataset substantially improve alignment with humansimilarity judgments on the other two datasets. In addition, we find that somehuman concepts such as food and animals are well-represented by neural networkswhereas others such as royal or sports-related objects are not. Overall,although models trained on larger, more diverse datasets achieve betteralignment with humans than models trained on ImageNet alone, our resultsindicate that scaling alone is unlikely to be sufficient to train neuralnetworks with conceptual representations that match those used by humans.", "output": "Human alignment of neural network representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ultimate goal of continuous sign language recognition(CSLR) is tofacilitate the communication between special people and normal people, whichrequires a certain degree of real-time and deploy-ability of the model.However, in the previous research on CSLR, little attention has been paid tothe real-time and deploy-ability. In order to improve the real-time anddeploy-ability of the model, this paper proposes a zero parameter, zerocomputation temporal superposition crossover module(TSCM), and combines it with2D convolution to form a \"TSCM+2D convolution\" hybrid convolution, whichenables 2D convolution to have strong spatial-temporal modelling capabilitywith zero parameter increase and lower deployment cost compared with otherspatial-temporal convolutions. The overall CSLR model based on TSCM is built onthe improved ResBlockT network in this paper. The hybrid convolution of\"TSCM+2D convolution\" is applied to the ResBlock of the ResNet network to formthe new ResBlockT, and random gradient stop and multi-level CTC loss areintroduced to train the model, which reduces the final recognition WER whilereducing the training memory usage, and extends the ResNet network from imageclassification task to video recognition task. In addition, this study is thefirst in CSLR to use only 2D convolution extraction of sign language videotemporal-spatial features for end-to-end learning for recognition. Experimentson two large-scale continuous sign language datasets demonstrate theeffectiveness of the proposed method and achieve highly competitive results.", "output": "Temporal superimposed crossover module for effective continuous sign language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dense image segmentation tasks e.g., semantic, panoptic) are useful for imageediting, but existing methods can hardly generalize well in an in-the-wildsetting where there are unrestricted image domains, classes, and imageresolution and quality variations. Motivated by these observations, weconstruct a new entity segmentation dataset, with a strong focus onhigh-quality dense segmentation in the wild. The dataset contains imagesspanning diverse image domains and entities, along with plentifulhigh-resolution images and high-quality mask annotations for training andtesting. Given the high-quality and -resolution nature of the dataset, wepropose CropFormer which is designed to tackle the intractability ofinstance-level segmentation on high-resolution images. It improves maskprediction by fusing high-res image crops that provide more fine-grained imagedetails and the full image. CropFormer is the first query-based Transformerarchitecture that can effectively fuse mask predictions from multiple imageviews, by learning queries that effectively associate the same entities acrossthe full image and its crop. With CropFormer, we achieve a significant AP gainof $1.9$ on the challenging entity segmentation task. Furthermore, CropFormerconsistently improves the accuracy of traditional segmentation tasks anddatasets. The dataset and code will be released at<a href=\" http URL</a>", "output": "High-Quality Entity Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous driving is a popular research area within the computer visionresearch community. Since autonomous vehicles are highly safety-critical,ensuring robustness is essential for real-world deployment. While severalpublic multimodal datasets are accessible, they mainly comprise two sensormodalities (camera, LiDAR) which are not well suited for adverse weather. Inaddition, they lack far-range annotations, making it harder to train neuralnetworks that are the base of a highway assistant function of an autonomousvehicle. Therefore, we introduce a multimodal dataset for robust autonomousdriving with long-range perception. The dataset consists of 176 scenes withsynchronized and calibrated LiDAR, camera, and radar sensors covering a360-degree field of view. The collected data was captured in highway, urban,and suburban areas during daytime, night, and rain and is annotated with 3Dbounding boxes with consistent identifiers across frames. Furthermore, wetrained unimodal and multimodal baseline models for 3D object detection. Dataare available at url{", "output": "aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) have shown impressive performance but stillrequire a high computation cost as compared to convolutional neural networks(CNNs), one reason is that ViTs' attention measures global similarities andthus has a quadratic complexity with the number of input tokens. Existingefficient ViTs adopt local attention (e.g., Swin) or linear attention (e.g.,Performer), which sacrifice ViTs' capabilities of capturing either global orlocal context. In this work, we ask an important research question: Can ViTslearn both global and local context while being more efficient duringinference? To this end, we propose a framework called Castling-ViT, whichtrains ViTs using both linear-angular attention and masked softmax-basedquadratic attention, but then switches to having only linear angular attentionduring ViT inference. Our Castling-ViT leverages angular kernels to measure thesimilarities between queries and keys via spectral angles. And we furthersimplify it with two techniques: (1) a novel linear-angular attentionmechanism: we decompose the angular kernels into linear terms and high-orderresiduals, and only keep the linear terms; and (2) we adopt two parameterizedmodules to approximate high-order residuals: a depthwise convolution and anauxiliary masked softmax attention to help learn both global and localinformation, where the masks for softmax attention are regularized to graduallybecome zeros and thus incur no overhead during ViT inference. Extensiveexperiments and ablation studies on three tasks consistently validate theeffectiveness of the proposed Castling-ViT, e.g., achieving up to a 1.8% higheraccuracy or 40% MACs reduction on ImageNet classification and 1.2 higher mAP onCOCO detection under comparable FLOPs, as compared to ViTs with vanillasoftmax-based attentions.", "output": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention During Vision Transformer Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video understanding is an important task in short video business platformsand it has a wide application in video recommendation and classification. Mostof the existing video understanding works only focus on the information thatappeared within the video content, including the video frames, audio and text.However, introducing common sense knowledge from the external Knowledge Graph(KG) dataset is essential for video understanding when referring to the contentwhich is less relevant to the video. Owing to the lack of video knowledge graphdataset, the work which integrates video understanding and KG is rare. In thispaper, we propose a heterogeneous dataset that contains the multi-modal videoentity and fruitful common sense relations. This dataset also provides multiplenovel video inference tasks like the Video-Relation-Tag (VRT) andVideo-Relation-Video (VRV) tasks. Furthermore, based on this dataset, wepropose an end-to-end model that jointly optimizes the video understandingobjective with knowledge graph embedding, which can not only better injectfactual knowledge into video understanding but also generate effectivemulti-modal entity embedding for KG. Comprehensive experiments indicate thatcombining video understanding embedding with factual knowledge benefits thecontent-based video retrieval performance. Moreover, it also helps the modelgenerate better knowledge graph embedding which outperforms traditionalKGE-based methods on VRT and VRV tasks with at least 42.36% and 17.73%improvement in HITS@10.", "output": "A Unified Model for Video Understanding and Knowledge Embedding with Heterogeneous Knowledge Graph Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Small on-device models have been successfully trained with user-leveldifferential privacy (DP) for next word prediction and image classificationtasks in the past. However, existing methods can fail when directly applied tolearn embedding models using supervised training data with a large class space.To achieve user-level DP for large image-to-embedding feature extractors, wepropose DP-FedEmb, a variant of federated learning algorithms with per-usersensitivity control and noise addition, to train from user-partitioned datacentralized in the datacenter. DP-FedEmb combines virtual clients, partialaggregation, private local fine-tuning, and public pretraining to achievestrong privacy utility trade-offs. We apply DP-FedEmb to train image embeddingmodels for faces, landmarks and natural species, and demonstrate its superiorutility under same privacy budget on benchmark datasets DigiFace, EMNIST, GLDand iNaturalist. We further illustrate it is possible to achieve stronguser-level DP guarantees of $epsilon&lt;4$ while controlling the utility dropwithin 5%, when millions of users can participate in training.", "output": "Learning to Generate Image Embeddings with User-level Differential Privacy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the last years, Denoising Diffusion Probabilistic Models (DDPMs) obtainedstate-of-the-art results in many generative tasks, outperforming GANs and otherclasses of generative models. In particular, they reached impressive results invarious image generation sub-tasks, among which conditional generation taskssuch as text-guided image synthesis. Given the success of DDPMs in 2Dgeneration, they have more recently been applied to 3D shape generation,outperforming previous approaches and reaching state-of-the-art results.However, these existing 3D DDPM works make little or no use of guidance, mainlybeing unconditional or class-conditional. In this work, we present IC3D, anImage-Conditioned 3D Diffusion model that generates 3D shapes by imageguidance. To guide our DDPM, we introduce CISP (Contrastive Image-ShapePre-training), a model jointly embedding images and shapes by contrastivepre-training, inspired by the literature on text-to-image DDPMs. Our generativediffusion model outperforms the state-of-the-art in 3D generation quality anddiversity. Furthermore, despite IC3D generative nature, we show that itsgenerated shapes are preferred by human evaluators to a SoTA single-view 3Dreconstruction model in terms of quality and coherence to the query image byrunning a side-by-side human evaluation. Ablation studies show the importanceof CISP for learning structural integrity properties, crucial for realisticgeneration. Such biases yield a regular embedding space and allow forinterpolation and conditioning on out-of-distribution images, while also makingIC3D capable of generating coherent but diverse completions of occluded viewsand enabling its adoption in controlled real-life applications.", "output": "IC3D: Image-Conditioned 3D Diffusion for Shape Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale Transformer models bring significant improvements for variousdownstream vision language tasks with a unified architecture. The performanceimprovements come with increasing model size, resulting in slow inference speedand increased cost for severing. While some certain predictions benefit fromthe full complexity of the large-scale model, not all of inputs need the sameamount of computation to conduct, potentially leading to computation resourcewaste. To handle this challenge, early exiting is proposed to adaptivelyallocate computational power in term of input complexity to improve inferenceefficiency. The existing early exiting strategies usually adopt outputconfidence based on intermediate layers as a proxy of input complexity to incurthe decision of skipping following layers. However, such strategies cannotapply to encoder in the widely-used unified architecture with both encoder anddecoder due to difficulty of output confidence estimation in the encoder. It issuboptimal in term of saving computation power to ignore the early exiting inencoder component. To handle this challenge, we propose a novel early exitingstrategy for unified visual language models, which allows dynamically skip thelayers in encoder and decoder simultaneously in term of input layer-wisesimilarities with multiple times of early exiting, namely textbf{MuE}. Bydecomposing the image and text modalities in the encoder, MuE is flexible andcan skip different layers in term of modalities, advancing the inferenceefficiency while minimizing performance drop. Experiments on the SNLI-VE and MSCOCO datasets show that the proposed approach MuE can reduce expected inferencetime by up to 50% and 40% while maintaining 99% and 96% performancerespectively.", "output": "You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Camera relocalization has various applications in autonomous driving.Previous camera pose regression models consider only ideal scenarios wherethere is little environmental perturbation. To deal with challenging drivingenvironments that may have changing seasons, weather, illumination, and thepresence of unstable objects, we propose RobustLoc, which derives itsrobustness against perturbations from neural differential equations. Our modeluses a convolutional neural network to extract feature maps from multi-viewimages, a robust neural differential equation diffusion block module to diffuseinformation interactively, and a branched pose decoder with multi-layertraining to estimate the vehicle poses. Experiments demonstrate that RobustLocsurpasses current state-of-the-art camera pose regression models and achievesrobust performance in various environments. Our code is released at:", "output": "RobustLoc: Robust Camera Pose Regression in Challenging Driving Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of finding a continuous and non-rigid matchingbetween a 2D contour and a 3D mesh. While such problems can be solved to globaloptimality by finding a shortest path in the product graph between both shapes,existing solutions heavily rely on unrealistic prior assumptions to avoiddegenerate solutions (e.g. knowledge to which region of the 3D shape each pointof the 2D contour is matched). To address this, we propose a novel 2D-3D shapematching formalism based on the conjugate product graph between the 2D contourand the 3D shape. Doing so allows us for the first time to considerhigher-order costs, i.e. defined for edge chains, as opposed to costs definedfor single edges. This offers substantially more flexibility, which we utiliseto incorporate a local rigidity prior. By doing so, we effectively circumventdegenerate solutions and thereby obtain smoother and more realistic matchings,even when using only a one-dimensional feature descriptor. Overall, our methodfinds globally optimal and continuous 2D-3D matchings, has the same asymptoticcomplexity as previous solutions, produces state-of-the-art results for shapematching and is even capable of matching partial shapes. Our code is publiclyavailable (", "output": "Conjugate Product Graphs for Globally Optimal 2D-3D Shape Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present ESLAM, an efficient implicit neural representation method forSimultaneous Localization and Mapping (SLAM). ESLAM reads RGB-D frames withunknown camera poses in a sequential manner and incrementally reconstructs thescene representation while estimating the current camera position in the scene.We incorporate the latest advances in Neural Radiance Fields (NeRF) into a SLAMsystem, resulting in an efficient and accurate dense visual SLAM method. Ourscene representation consists of multi-scale axis-aligned perpendicular featureplanes and shallow decoders that, for each point in the continuous space,decode the interpolated features into Truncated Signed Distance Field (TSDF)and RGB values. Our extensive experiments on three standard datasets, Replica,ScanNet, and TUM RGB-D show that ESLAM improves the accuracy of 3Dreconstruction and camera localization of state-of-the-art dense visual SLAMmethods by more than 50%, while it runs up to 10 times faster and does notrequire any pre-training.", "output": "ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In image denoising networks, feature scaling is widely used to enlarge thereceptive field size and reduce computational costs. This practice, however,also leads to the loss of high-frequency information and fails to considerwithin-scale characteristics. Recently, dynamic convolution has exhibitedpowerful capabilities in processing high-frequency information (e.g., edges,corners, textures), but previous works lack sufficient spatial contextualinformation in filter generation. To alleviate these issues, we propose toemploy dynamic convolution to improve the learning of high-frequency andmulti-scale features. Specifically, we design a spatially enhanced kernelgeneration (SEKG) module to improve dynamic convolution, enabling the learningof spatial context information with a very low computational complexity. Basedon the SEKG module, we propose a dynamic convolution block (DCB) and amulti-scale dynamic convolution block (MDCB). The former enhances thehigh-frequency information via dynamic convolution and preserves low-frequencyinformation via skip connections. The latter utilizes shared adaptive dynamickernels and the idea of dilated convolution to achieve efficient multi-scalefeature extraction. The proposed multi-dimension feature integration (MFI)mechanism further fuses the multi-scale features, providing precise andcontextually enriched feature representations. Finally, we build an efficientdenoising network with the proposed DCB and MDCB, named ADFNet. It achievesbetter performance with low computational complexity on real-world andsynthetic Gaussian noisy datasets. The source code is available at", "output": "Adaptive Dynamic Filtering Network for Image Denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Because of their high temporal resolution, increased resilience to motionblur, and very sparse output, event cameras have been shown to be ideal forlow-latency and low-bandwidth feature tracking, even in challenging scenarios.Existing feature tracking methods for event cameras are either handcrafted orderived from first principles but require extensive parameter tuning, aresensitive to noise, and do not generalize to different scenarios due tounmodeled effects. To tackle these deficiencies, we introduce the firstdata-driven feature tracker for event cameras, which leverages low-latencyevents to track features detected in a grayscale frame. We achieve robustperformance via a novel frame attention module, which shares information acrossfeature tracks. By directly transferring zero-shot from synthetic to real data,our data-driven tracker outperforms existing approaches in relative feature ageby up to 120% while also achieving the lowest latency. This performance gap isfurther increased to 130% by adapting our tracker to real data with a novelself-supervision strategy.", "output": "Data-driven Feature Tracking for Event Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reconstructing 3D shapes from planar cross-sections is a challenge inspiredby downstream applications like medical imaging and geographic informatics. Theinput is an in/out indicator function fully defined on a sparse collection ofplanes in space, and the output is an interpolation of the indicator functionto the entire volume. Previous works addressing this sparse and ill-posedproblem either produce low quality results, or rely on additional priors suchas target topology, appearance information, or input normal directions. In thispaper, we present OReX, a method for 3D shape reconstruction from slices alone,featuring a Neural Field as the interpolation prior. A modest neural network istrained on the input planes to return an inside/outside estimate for a given 3Dcoordinate, yielding a powerful prior that induces smoothness andself-similarities. The main challenge for this approach is high-frequencydetails, as the neural prior is overly smoothing. To alleviate this, we offeran iterative estimation architecture and a hierarchical input sampling schemethat encourage coarse-to-fine training, allowing the training process to focuson high frequencies at later stages. In addition, we identify and analyze aripple-like effect stemming from the mesh extraction step. We mitigate it byregularizing the spatial gradients of the indicator function around inputin/out boundaries during network training, tackling the problem at the root.Through extensive qualitative and quantitative experimentation, we demonstrateour method is robust, accurate, and scales well with the size of the input. Wereport state-of-the-art results compared to previous approaches and recentpotential solutions, and demonstrate the benefit of our individualcontributions through analysis and ablation studies.", "output": "OReX: Object Reconstruction from Planar Cross-sections Using Neural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Translating spoken languages into Sign languages is necessary for opencommunication between the hearing and hearing-impaired communities. To achievethis goal, we propose the first method for animating a text written inHamNoSys, a lexical Sign language notation, into signed pose sequences. AsHamNoSys is universal by design, our proposed method offers a generic solutioninvariant to the target Sign language. Our method gradually generates posepredictions using transformer encoders that create meaningful representationsof the text and poses while considering their spatial and temporal information.We use weak supervision for the training process and show that our methodsucceeds in learning from partial and inaccurate data. Additionally, we offer anew distance measurement that considers missing keypoints, to measure thedistance between pose sequences using DTW-MJE. We validate its correctnessusing AUTSL, a large-scale Sign language dataset, show that it measures thedistance between pose sequences more accurately than existing measurements, anduse it to assess the quality of our generated pose sequences. Code for the datapre-processing, the model, and the distance measurement is publicly releasedfor future research.", "output": "Ham2Pose: Animating Sign Language Notation into Pose Sequences."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In Deep Image Prior (DIP), a Convolutional Neural Network (CNN) is fitted tomap a latent space to a degraded (e.g. noisy) image but in the process learnsto reconstruct the clean image. This phenomenon is attributed to CNN's internalimage-prior. We revisit the DIP framework, examining it from the perspective ofa neural implicit representation. Motivated by this perspective, we replace therandom or learned latent with Fourier-Features (Positional Encoding). We showthat thanks to the Fourier features properties, we can replace the convolutionlayers with simple pixel-level MLPs. We name this scheme ``Positional EncodingImage Prior\" (PIP) and exhibit that it performs very similarly to DIP onvarious image-reconstruction tasks with much less parameters required.Additionally, we demonstrate that PIP can be easily extended to videos, where3D-DIP struggles and suffers from instability. Code and additional examples forall tasks, including videos, are available on the project page", "output": "PIP: Positional-encoding Image Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised image semantic segmentation(UISS) aims to match low-level visualfeatures with semantic-level representations without outer supervision. In thispaper, we address the critical properties from the view of feature alignmentsand feature uniformity for UISS models. We also make a comparison between UISSand image-wise representation learning. Based on the analysis, we argue thatthe existing MI-based methods in UISS suffer from representation collapse. Bythis, we proposed a robust network called Semantic Attention Network(SAN), inwhich a new module Semantic Attention(SEAT) is proposed to generate pixel-wiseand semantic features dynamically. Experimental results on multiple semanticsegmentation benchmarks show that our unsupervised segmentation frameworkspecializes in catching semantic representations, which outperforms all theunpretrained and even several pretrained methods.", "output": "Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperspectral image is unique and useful for its abundant spectral bands, butit subsequently requires extra elaborated treatments of the spatial-spectralcorrelation as well as the global correlation along the spectrum for building arobust and powerful HSI restoration algorithm. By considering such HSIcharacteristics, 3D Quasi-Recurrent Neural Network (QRNN3D) is one of the HSIdenoising networks that has been shown to achieve excellent performance andflexibility. In this paper, we show that with a few simple modifications, theperformance of QRNN3D could be substantially improved further. Ourmodifications are based on the finding that through QRNN3D is powerful formodeling spectral correlation, it neglects the proper treatment betweenfeatures from different sources and its training strategy is suboptimal. We,therefore, introduce an adaptive fusion module to replace its vanilla additiveskip connection to better fuse the features of the encoder and decoder. Weadditionally identify several important techniques to further enhance theperformance, which includes removing batch normalization, use of extrafrequency loss, and learning rate warm-up. Experimental results on variousnoise settings demonstrate the effectiveness and superior performance of ourmethod.", "output": "Improved Quasi-Recurrent Neural Network for Hyperspectral Image Denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformers are powerful visual learners, in large part due to theirconspicuous lack of manually-specified priors. This flexibility can beproblematic in tasks that involve multiple-view geometry, due to thenear-infinite possible variations in 3D shapes and viewpoints (requiringflexibility), and the precise nature of projective geometry (obeying rigidlaws). To resolve this conundrum, we propose a \"light touch\" approach, guidingvisual Transformers to learn multiple-view geometry but allowing them to breakfree when needed. We achieve this by using epipolar lines to guide theTransformer's cross-attention maps, penalizing attention values outside theepipolar lines and encouraging higher attention along these lines since theycontain geometrically plausible matches. Unlike previous methods, our proposaldoes not require any camera pose information at test-time. We focus onpose-invariant object instance retrieval, where standard Transformer networksstruggle, due to the large differences in viewpoint between query and retrievedimages. Experimentally, our method outperforms state-of-the-art approaches atobject retrieval, without needing pose information at test-time.", "output": "A Light Touch Approach to Teaching Transformers Multi-view Geometry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Virtual reality and augmented reality (XR) bring increasing demand for 3Dcontent. However, creating high-quality 3D content requires tedious work that ahuman expert must do. In this work, we study the challenging task of lifting asingle image to a 3D object and, for the first time, demonstrate the ability togenerate a plausible 3D object with 360{deg} views that correspond well withthe given reference image. By conditioning on the reference image, our modelcan fulfill the everlasting curiosity for synthesizing novel views of objectsfrom images. Our technique sheds light on a promising direction of easing theworkflows for 3D artists and XR designers. We propose a novel framework, dubbedNeuralLift-360, that utilizes a depth-aware neural radiance representation(NeRF) and learns to craft the scene guided by denoising diffusion models. Byintroducing a ranking loss, our NeuralLift-360 can be guided with rough depthestimation in the wild. We also adopt a CLIP-guided sampling strategy for thediffusion prior to provide coherent guidance. Extensive experiments demonstratethat our NeuralLift-360 significantly outperforms existing state-of-the-artbaselines. Project page: ", "output": "NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360{\\deg} Views."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an algorithm for taming Normalizing Flow models - changing theprobability that the model will produce a specific image or image category. Wefocus on Normalizing Flows because they can calculate the exact generationprobability likelihood for a given image. We demonstrate taming using modelsthat generate human faces, a subdomain with many interesting privacy and biasconsiderations. Our method can be used in the context of privacy, e.g.,removing a specific person from the output of a model, and also in the contextof debiasing by forcing a model to output specific image categories accordingto a given target distribution. Taming is achieved with a fast fine-tuningprocess without retraining the model from scratch, achieving the goal in amatter of minutes. We evaluate our method qualitatively and quantitatively,showing that the generation quality remains intact, while the desired changesare applied.", "output": "Taming Normalizing Flows."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative models have shown great promise in synthesizing photorealistic 3Dobjects, but they require large amounts of training data. We introduce SinGRAF,a 3D-aware generative model that is trained with a few input images of a singlescene. Once trained, SinGRAF generates different realizations of this 3D scenethat preserve the appearance of the input while varying scene layout. For thispurpose, we build on recent progress in 3D GAN architectures and introduce anovel progressive-scale patch discrimination approach during training. Withseveral experiments, we demonstrate that the results produced by SinGRAFoutperform the closest related works in both quality and diversity by a largemargin.", "output": "SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) have achieved overwhelming success, yet theysuffer from vulnerable resolution scalability, i.e., the performance dropsdrastically when presented with input resolutions that are unseen duringtraining. We introduce, ResFormer, a framework that is built upon the seminalidea of multi-resolution training for improved performance on a wide spectrumof, mostly unseen, testing resolutions. In particular, ResFormer operates onreplicated images of different resolutions and enforces a scale consistencyloss to engage interactive information across different scales. Moreimportantly, to alternate among varying resolutions effectively, especiallynovel ones in testing, we propose a global-local positional embedding strategythat changes smoothly conditioned on input sizes. We conduct extensiveexperiments for image classification on ImageNet. The results provide strongquantitative evidence that ResFormer has promising scaling abilities towards awide range of resolutions. For instance, ResFormer-B-MR achieves a Top-1accuracy of 75.86% and 81.72% when evaluated on relatively low and highresolutions respectively (i.e., 96 and 640), which are 48% and 7.49% betterthan DeiT-B. We also demonstrate, moreover, ResFormer is flexible and can beeasily extended to semantic segmentation, object detection and video actionrecognition. Code is available at ", "output": "ResFormer: Scaling ViTs with Multi-Resolution Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating 3D human motion from an egocentric video sequence plays a criticalrole in human behavior understanding and has various applications in VR/AR.However, naively learning a mapping between egocentric videos and human motionsis challenging, because the user's body is often unobserved by the front-facingcamera placed on the head of the user. In addition, collecting large-scale,high-quality datasets with paired egocentric videos and 3D human motionsrequires accurate motion capture devices, which often limit the variety ofscenes in the videos to lab-like environments. To eliminate the need for pairedegocentric video and human motions, we propose a new method, Ego-Body PoseEstimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the probleminto two stages, connected by the head motion as an intermediaterepresentation. EgoEgo first integrates SLAM and a learning approach toestimate accurate head motion. Subsequently, leveraging the estimated head poseas input, EgoEgo utilizes conditional diffusion to generate multiple plausiblefull-body motions. This disentanglement of head and body pose eliminates theneed for training datasets with paired egocentric videos and 3D human motion,enabling us to leverage large-scale egocentric video datasets and motioncapture datasets separately. Moreover, for systematic benchmarking, we developa synthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentricvideos and human motion. On both ARES and real data, our EgoEgo model performssignificantly better than the current state-of-the-art methods.", "output": "Ego-Body Pose Estimation via Ego-Head Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Zero-shot quantization is a promising approach for developing lightweightdeep neural networks when data is inaccessible owing to various reasons,including cost and issues related to privacy. By exploiting the learnedparameters ($mu$ and $sigma$) of batch normalization layers in anFP32-pre-trained model, zero-shot quantization schemes focus on generatingsynthetic data. Subsequently, they distill knowledge from the pre-trained model(teacher) to the quantized model (student) such that the quantized model can beoptimized with the synthetic dataset. However, thus far, zero-shot quantizationhas primarily been discussed in the context of quantization-aware trainingmethods, which require task-specific losses and long-term optimization as muchas retraining. We thus introduce a post-training quantization scheme forzero-shot quantization that produces high-quality quantized networks within afew hours. Furthermore, we propose a framework called genie~that generatesdata suited for quantization. With the data synthesized by Genie, we canproduce robust quantized models without real datasets, which is comparable tofew-shot quantization. We also propose a post-training quantization algorithmto enhance the performance of quantized models. By combining them, we canbridge the gap between zero-shot and few-shot quantization while significantlyimproving the quantization performance compared to that of existing approaches.In other words, we can obtain a unique state-of-the-art zero-shot quantizationapproach.", "output": "Genie: Show Me the Data for Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantitative cephalometric analysis is the most widely used clinical andresearch tool in modern orthodontics. Accurate localization of cephalometriclandmarks enables the quantification and classification of anatomicalabnormalities, however, the traditional manual way of marking these landmarksis a very tedious job. Endeavours have constantly been made to developautomated cephalometric landmark detection systems but they are inadequate fororthodontic applications. The fundamental reason for this is that the amount ofpublicly available datasets as well as the images provided for training inthese datasets are insufficient for an AI model to perform well. To facilitatethe development of robust AI solutions for morphometric analysis, we organisethe CEPHA29 Automatic Cephalometric Landmark Detection Challenge in conjunctionwith IEEE International Symposium on Biomedical Imaging (ISBI 2023). In thiscontext, we provide the largest known publicly available dataset, consisting of1000 cephalometric X-ray images. We hope that our challenge will not onlyderive forward research and innovation in automatic cephalometric landmarkidentification but will also signal the beginning of a new era in thediscipline.", "output": "CEPHA29: Automatic Cephalometric Landmark Detection Challenge 2023."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recognition capabilities of current state-of-the-art 3D models arelimited by datasets with a small number of annotated data and a pre-defined setof categories. In its 2D counterpart, recent advances have shown that similarproblems can be significantly alleviated by employing knowledge from othermodalities, such as language. Inspired by this, leveraging multimodalinformation for 3D modality could be promising to improve 3D understandingunder the restricted data regime, but this line of research is not wellstudied. Therefore, we introduce ULIP to learn a unified representation ofimages, texts, and 3D point clouds by pre-training with object triplets fromthe three modalities. To overcome the shortage of training triplets, ULIPleverages a pre-trained vision-language model that has already learned a commonvisual and textual space by training with massive image-text pairs. Then, ULIPlearns a 3D representation space aligned with the common image-text space,using a small number of automatically synthesized triplets. ULIP is agnostic to3D backbone networks and can easily be integrated into any 3D architecture.Experiments show that ULIP effectively improves the performance of multiplerecent 3D backbones by simply pre-training them on ShapeNet55 using ourframework, achieving state-of-the-art performance in both standard 3Dclassification and zero-shot 3D classification on ModelNet40 and ScanObjectNN.ULIP also improves the performance of PointMLP by around 3% in 3Dclassification on ScanObjectNN, and outperforms PointCLIP by 28.8% on top-1accuracy for zero-shot 3D classification on ModelNet40. Our code andpre-trained models are released at ", "output": "ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose an end-to-end Retrieval-Augmented Visual LanguageModel (REVEAL) that learns to encode world knowledge into a large-scale memory,and to retrieve from it to answer knowledge-intensive queries. REVEAL consistsof four key components: the memory, the encoder, the retriever and thegenerator. The large-scale memory encodes various sources of multimodal worldknowledge (e.g. image-text pairs, question answering pairs, knowledge graphtriplets, etc) via a unified encoder. The retriever finds the most relevantknowledge entries in the memory, and the generator fuses the retrievedknowledge with the input query to produce the output. A key novelty in ourapproach is that the memory, encoder, retriever and generator are allpre-trained end-to-end on a massive amount of data. Furthermore, our approachcan use a diverse set of multimodal knowledge sources, which is shown to resultin significant gains. We show that REVEAL achieves state-of-the-art results onvisual question answering and image captioning.", "output": "REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Recurrent Vision Transformers (RVTs), a novel backbone for objectdetection with event cameras. Event cameras provide visual information withsub-millisecond latency at a high-dynamic range and with strong robustnessagainst motion blur. These unique properties offer great potential forlow-latency object detection and tracking in time-critical scenarios. Priorwork in event-based vision has achieved outstanding detection performance butat the cost of substantial inference time, typically beyond 40 milliseconds. Byrevisiting the high-level design of recurrent vision backbones, we reduceinference time by a factor of 6 while retaining similar performance. To achievethis, we explore a multi-stage design that utilizes three key concepts in eachstage: First, a convolutional prior that can be regarded as a conditionalpositional embedding. Second, local and dilated global self-attention forspatial feature interaction. Third, recurrent temporal feature aggregation tominimize latency while retaining temporal information. RVTs can be trained fromscratch to reach state-of-the-art performance on event-based object detection -achieving an mAP of 47.2% on the Gen1 automotive dataset. At the same time,RVTs offer fast inference (&lt;12 ms on a T4 GPU) and favorable parameterefficiency (5 times fewer than prior art). Our study brings new insights intoeffective design choices that can be fruitful for research beyond event-basedvision.", "output": "Recurrent Vision Transformers for Object Detection with Event Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In object detection, post-processing methods like Non-maximum Suppression(NMS) are widely used. NMS can substantially reduce the number of falsepositive detections but may still keep some detections with low objectnessscores. In order to find the exact number of objects and their labels in theimage, we propose a post processing method called Detection Selection Algorithm(DSA) which is used after NMS or related methods. DSA greedily selects a subsetof detected bounding boxes, together with full object reconstructions that givethe interpretation of the whole image with highest likelihood, taking intoaccount object occlusions. The algorithm consists of four components. First, weadd an occlusion branch to Faster R-CNN to obtain occlusion relationshipsbetween objects. Second, we develop a single reconstruction algorithm which canreconstruct the whole appearance of an object given its visible part, based onthe optimization of latent variables of a trained generative network which wecall the decoder. Third, we propose a whole reconstruction algorithm whichgenerates the joint reconstruction of all objects in a hypothesizedinterpretation, taking into account occlusion ordering. Finally we propose agreedy algorithm that incrementally adds or removes detections from a list tomaximize the likelihood of the corresponding interpretation. DSA with NMS orSoft-NMS can achieve better results than NMS or Soft-NMS themselves, as isillustrated in our experiments on synthetic images with mutiple 3d objects.", "output": "Detection Selection Algorithm: A Likelihood based Optimization Method to Perform Post Processing for Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The two popular datasets ScanRefer [16] and ReferIt3D [3] connect naturallanguage to real-world 3D data. In this paper, we curate a large-scale andcomplementary dataset extending both the aforementioned ones by associating allobjects mentioned in a referential sentence to their underlying instancesinside a 3D scene. Specifically, our Scan Entities in 3D (ScanEnts3D) datasetprovides explicit correspondences between 369k objects across 84k naturalreferential sentences, covering 705 real-world scenes. Crucially, we show thatby incorporating intuitive losses that enable learning from this novel dataset,we can significantly improve the performance of several recently introducedneural listening architectures, including improving the SoTA in both the Nr3Dand ScanRefer benchmarks by 4.3% and 5.0%, respectively. Moreover, weexperiment with competitive baselines and recent methods for the task oflanguage generation and show that, as with neural listeners, 3D neural speakerscan also noticeably benefit by training with ScanEnts3D, including improvingthe SoTA by 13.2 CIDEr points on the Nr3D benchmark. Overall, our carefullyconducted experimental studies strongly support the conclusion that, bylearning on ScanEnts3D, commonly used visio-linguistic 3D architectures canbecome more efficient and interpretable in their generalization without needingto provide these newly collected annotations at test time. The project'swebpage is  .", "output": "ScanEnts3D: Exploiting Phrase-to-3D-Object Correspondences for Improved Visio-Linguistic Models in 3D Scenes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning classifiers provide the most accurate means of automaticallydiagnosing diabetic retinopathy (DR) based on optical coherence tomography(OCT) and its angiography (OCTA). The power of these models is attributable inpart to the inclusion of hidden layers that provide the complexity required toachieve a desired task. However, hidden layers also render algorithm outputsdifficult to interpret. Here we introduce a novel biomarker activation map(BAM) framework based on generative adversarial learning that allows cliniciansto verify and understand classifiers decision-making. A data set including 456macular scans were graded as non-referable or referable DR based on currentclinical standards. A DR classifier that was used to evaluate our BAM was firsttrained based on this data set. The BAM generation framework was designed bycombing two U-shaped generators to provide meaningful interpretability to thisclassifier. The main generator was trained to take referable scans as input andproduce an output that would be classified by the classifier as non-referable.The BAM is then constructed as the difference image between the output andinput of the main generator. To ensure that the BAM only highlightsclassifier-utilized biomarkers an assistant generator was trained to do theopposite, producing scans that would be classified as referable by theclassifier from non-referable scans. The generated BAMs highlighted knownpathologic features including nonperfusion area and retinal fluid. A fullyinterpretable classifier based on these highlights could help clinicians betterutilize and verify automated DR diagnosis.", "output": "Interpretable Diabetic Retinopathy Diagnosis based on Biomarker Activation Map."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training a Neural Radiance Field (NeRF) without pre-computed camera poses ischallenging. Recent advances in this direction demonstrate the possibility ofjointly optimising a NeRF and camera poses in forward-facing scenes. However,these methods still face difficulties during dramatic camera movement. Wetackle this challenging problem by incorporating undistorted monocular depthpriors. These priors are generated by correcting scale and shift parametersduring training, with which we are then able to constrain the relative posesbetween consecutive frames. This constraint is achieved using our proposednovel loss functions. Experiments on real-world indoor and outdoor scenes showthat our method can handle challenging camera trajectories and outperformsexisting methods in terms of novel view rendering quality and pose estimationaccuracy. Our project page is ", "output": "NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal models are becoming increasingly effective, in part due to unifiedcomponents, such as the Transformer architecture. However, multimodal modelsstill often consist of many task- and modality-specific pieces and trainingprocedures. For example, CLIP (Radford et al., 2021) trains independent textand image towers via a contrastive loss. We explore an additional unification:the use of a pure pixel-based model to perform image, text, and multimodaltasks. Our model is trained with contrastive loss alone, so we call itCLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes bothregular images and text rendered as images. CLIPPO performs image-based taskssuch as retrieval and zero-shot image classification almost as well asCLIP-style models, with half the number of parameters and no text-specifictower or embedding. When trained jointly via image-text contrastive learningand next-sentence contrastive learning, CLIPPO can perform well on naturallanguage understanding tasks, without any word-level loss (language modellingor masked language modelling), outperforming pixel-based prior work.Surprisingly, CLIPPO can obtain good accuracy in visual question answering,simply by rendering the question and image together. Finally, we exploit thefact that CLIPPO does not require a tokenizer to show that it can achievestrong performance on multilingual multimodal retrieval without modifications.", "output": "CLIPPO: Image-and-Language Understanding from Pixels Only."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The success of the Neural Radiance Fields (NeRF) in novel view synthesis hasinspired researchers to propose neural implicit scene reconstruction. However,most existing neural implicit reconstruction methods optimize per-sceneparameters and therefore lack generalizability to new scenes. We introduceVolRecon, a novel generalizable implicit reconstruction method with Signed RayDistance Function (SRDF). To reconstruct the scene with fine details and littlenoise, VolRecon combines projection features aggregated from multi-viewfeatures, and volume features interpolated from a coarse global feature volume.Using a ray transformer, we compute SRDF values of sampled points on a ray andthen render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS byabout 30% in sparse view reconstruction and achieves comparable accuracy asMVSNet in full view reconstruction. Furthermore, our approach exhibits goodgeneralization performance on the large-scale ETH3D benchmark.", "output": "VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Can we build continuous generative models which generalize across scales, canbe evaluated at any coordinate, admit calculation of exact derivatives, and areconceptually simple? Existing MLP-based architectures generate worse samplesthan the grid-based generators with favorable convolutional inductive biases.Models that focus on generating images at different scales do better, butemploy complex architectures not designed for continuous evaluation of imagesand derivatives. We take a signal-processing perspective and treat continuousimage generation as interpolation from samples. Indeed, correctly sampleddiscrete images contain all information about the low spatial frequencies. Thequestion is then how to extrapolate the spectrum in a data-driven way whilemeeting the above design criteria. Our answer is FunkNN -- a new convolutionalnetwork which learns how to reconstruct continuous images at arbitrarycoordinates and can be applied to any image dataset. Combined with a discretegenerative model it becomes a functional generator which can act as a prior incontinuous ill-posed inverse problems. We show that FunkNN generateshigh-quality continuous images and exhibits strong out-of-distributionperformance thanks to its patch-based design. We further showcase itsperformance in several stylized inverse problems with exact spatialderivatives.", "output": "FunkNN: Neural Interpolation for Functional Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent CLIP-guided 3D optimization methods, such as DreamFields andPureCLIPNeRF, have achieved impressive results in zero-shot text-to-3Dsynthesis. However, due to scratch training and random initialization withoutprior knowledge, these methods often fail to generate accurate and faithful 3Dstructures that conform to the input text. In this paper, we make the firstattempt to introduce explicit 3D shape priors into the CLIP-guided 3Doptimization process. Specifically, we first generate a high-quality 3D shapefrom the input text in the text-to-shape stage as a 3D shape prior. We then useit as the initialization of a neural radiance field and optimize it with thefull prompt. To address the challenging text-to-shape generation task, wepresent a simple yet effective approach that directly bridges the text andimage modalities with a powerful text-to-image diffusion model. To narrow thestyle domain gap between the images synthesized by the text-to-image diffusionmodel and shape renderings used to train the image-to-shape generator, wefurther propose to jointly optimize a learnable text prompt and fine-tune thetext-to-image diffusion model for rendering-style image generation. Our method,Dream3D, is capable of generating imaginative 3D content with superior visualquality and shape accuracy compared to state-of-the-art methods.", "output": "Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we consider enhancing medical visual-language pre-training(VLP) with domain-specific knowledge, by exploiting the paired image-textreports from the radiological daily practice. In particular, we make thefollowing contributions: First, unlike existing works that directly process theraw reports, we adopt a novel triplet extraction module to extract themedical-related information, avoiding unnecessary complexity from languagegrammar and enhancing the supervision signals; Second, we propose a noveltriplet encoding module with entity translation by querying a knowledge base,to exploit the rich domain knowledge in medical field, and implicitly buildrelationships between medical entities in the language embedding space; Third,we propose to use a Transformer-based fusion model for spatially aligning theentity description with visual signals at the image patch level, enabling theability for medical diagnosis; Fourth, we conduct thorough experiments tovalidate the effectiveness of our architecture, and benchmark on numerouspublic benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,COVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuningsettings, our model has demonstrated strong performance compared with theformer methods on disease classification and grounding.", "output": "MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Speech-driven 3D facial animation has been widely studied, yet there is stilla gap to achieving realism and vividness due to the highly ill-posed nature andscarcity of audio-visual data. Existing works typically formulate thecross-modal mapping into a regression task, which suffers from theregression-to-mean problem leading to over-smoothed facial motions. In thispaper, we propose to cast speech-driven facial animation as a code query taskin a finite proxy space of the learned codebook, which effectively promotes thevividness of the generated motions by reducing the cross-modal mappinguncertainty. The codebook is learned by self-reconstruction over real facialmotions and thus embedded with realistic facial motion priors. Over thediscrete motion space, a temporal autoregressive model is employed tosequentially synthesize facial motions from the input speech signal, whichguarantees lip-sync as well as plausible facial expressions. We demonstratethat our approach outperforms current state-of-the-art methods bothqualitatively and quantitatively. Also, a user study further justifies oursuperiority in perceptual quality.", "output": "CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, relying on convolutional neural networks (CNNs), many methods forsalient object detection in optical remote sensing images (ORSI-SOD) areproposed. However, most methods ignore the huge parameters and computationalcost brought by CNNs, and only a few pay attention to the portability andmobility. To facilitate practical applications, in this paper, we propose anovel lightweight network for ORSI-SOD based on semantic matching and edgealignment, termed SeaNet. Specifically, SeaNet includes a lightweightMobileNet-V2 for feature extraction, a dynamic semantic matching module (DSMM)for high-level features, an edge self-alignment module (ESAM) for low-levelfeatures, and a portable decoder for inference. First, the high-level featuresare compressed into semantic kernels. Then, semantic kernels are used toactivate salient object locations in two groups of high-level features throughdynamic convolution operations in DSMM. Meanwhile, in ESAM, cross-scale edgeinformation extracted from two groups of low-level features is self-alignedthrough L2 loss and used for detail enhancement. Finally, starting from thehighest-level features, the decoder infers salient objects based on theaccurate locations and fine details contained in the outputs of the twomodules. Extensive experiments on two public datasets demonstrate that ourlightweight SeaNet not only outperforms most state-of-the-art lightweightmethods but also yields comparable accuracy with state-of-the-art conventionalmethods, while having only 2.76M parameters and running with 1.7G FLOPs for288x288 inputs. Our code and results are available at", "output": "Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have seen a surge of interest in learning high-level causalrepresentations from low-level image pairs under interventions. Yet, existingefforts are largely limited to simple synthetic settings that are far away fromreal-world problems. In this paper, we present Causal Triplet, a causalrepresentation learning benchmark featuring not only visually more complexscenes, but also two crucial desiderata commonly overlooked in previous works:(i) an actionable counterfactual setting, where only certain object-levelvariables allow for counterfactual observations whereas others do not; (ii) aninterventional downstream task with an emphasis on out-of-distributionrobustness from the independent causal mechanisms principle. Through extensiveexperiments, we find that models built with the knowledge of disentangled orobject-centric representations significantly outperform their distributedcounterparts. However, recent causal representation learning methods stillstruggle to identify such latent structures, indicating substantial challengesand opportunities for future work. Our code and datasets will be available at", "output": "Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image-based multi-person reconstruction in wide-field large scenes iscritical for crowd analysis and security alert. However, existing methodscannot deal with large scenes containing hundreds of people, which encounterthe challenges of large number of people, large variations in human scale, andcomplex spatial distribution. In this paper, we propose Crowd3D, the firstframework to reconstruct the 3D poses, shapes and locations of hundreds ofpeople with global consistency from a single large-scene image. The core of ourapproach is to convert the problem of complex crowd localization into pixellocalization with the help of our newly defined concept, Human-scene VirtualInteraction Point (HVIP). To reconstruct the crowd with global consistency, wepropose a progressive reconstruction network based on HVIP by pre-estimating ascene-level camera and a ground plane. To deal with a large number of personsand various human sizes, we also design an adaptive human-centric croppingscheme. Besides, we contribute a benchmark dataset, LargeCrowd, for crowdreconstruction in a large scene. Experimental results demonstrate theeffectiveness of the proposed method. The code and datasets will be madepublic.", "output": "Crowd3D: Towards Hundreds of People Reconstruction from a Single Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an approach based on convex relaxations for certifiably optimalrobust multiview triangulation. To this end, we extend existing relaxationapproaches to non-robust multiview triangulation by incorporating a leastsquares cost function. We propose two formulations, one based on epipolarconstraints and one based on fractional reprojection constraints. The first islower dimensional and remains tight under moderate noise and outlier levels,while the second is higher dimensional and therefore slower but remains tighteven under extreme noise and outlier levels. We demonstrate through extensiveexperiments that the proposed approaches allow us to compute provably optimalreconstructions even under significant noise and a large percentage ofoutliers.", "output": "Semidefinite Relaxations for Robust Multiview Triangulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning models that are robust to distribution shifts is a key concern inthe context of their real-life applicability. Invariant Risk Minimization (IRM)is a popular framework that aims to learn robust models from multipleenvironments. The success of IRM requires an important assumption: theunderlying causal mechanisms/features remain invariant across environments.When not satisfied, we show that IRM can over-constrain the predictor and toremedy this, we propose a relaxation via $textit{partial invariance}$. In thiswork, we theoretically highlight the sub-optimality of IRM and then demonstratehow learning from a partition of training domains can help improve invariantmodels. Several experiments, conducted both in linear settings as well as withdeep neural networks on tasks over both language and image data, allow us toverify our conclusions.", "output": "Learning Optimal Features via Partial Invariance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monocular Depth Estimation (MDE) is a critical component in applications suchas autonomous driving. There are various attacks against MDE networks. Theseattacks, especially the physical ones, pose a great threat to the security ofsuch systems. Traditional adversarial training method requires ground-truthlabels hence cannot be directly applied to self-supervised MDE that does nothave ground-truth depth. Some self-supervised model hardening techniques (e.g.,contrastive learning) ignore the domain knowledge of MDE and can hardly achieveoptimal performance. In this work, we propose a novel adversarial trainingmethod for self-supervised MDE models based on view synthesis without usingground-truth depth. We improve adversarial robustness against physical-worldattacks using L0-norm-bounded perturbation in training. We compare our methodwith supervised learning based and contrastive learning based methods that aretailored for MDE. Results on two representative MDE networks show that weachieve better robustness against various adversarial attacks with nearly nobenign performance degradation.", "output": "Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain Generalization (DG) is a fundamental challenge for machine learningmodels, which aims to improve model generalization on various domains. Previousmethods focus on generating domain invariant features from various sourcedomains. However, we argue that the domain variantions also contain usefulinformation, ie, classification-aware information, for downstream tasks, whichhas been largely ignored. Different from learning domain invariant featuresfrom source domains, we decouple the input images into Domain Expert Featuresand noise. The proposed domain expert features lie in a learned latent spacewhere the images in each domain can be classified independently, enabling theimplicit use of classification-aware domain variations. Based on the analysis,we proposed a novel paradigm called Domain Disentanglement Network (DDN) todisentangle the domain expert features from the source domain images andaggregate the source domain expert features for representing the target testdomain. We also propound a new contrastive learning method to guide the domainexpert features to form a more balanced and separable feature space.Experiments on the widely-used benchmarks of PACS, VLCS, OfficeHome, DomainNet,and TerraIncognita demonstrate the competitive performance of our methodcompared to the recently proposed alternatives.", "output": "Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT)framework. It emphasizes spatio-temporal continuity and integrates both pastand future reasoning for tracked objects. Thus, we name it \"Past-and-Futurereasoning for Tracking\" (PF-Track). Specifically, our method adapts the\"tracking by attention\" framework and represents tracked instances coherentlyover time with object queries. To explicitly use historical cues, our \"PastReasoning\" module learns to refine the tracks and enhance the object featuresby cross-attending to queries from previous frames and other objects. The\"Future Reasoning\" module digests historical information and predicts robustfuture trajectories. In the case of long-term occlusions, our method maintainsthe object positions and enables re-association by integrating motionpredictions. On the nuScenes dataset, our method improves AMOTA by a largemargin and remarkably reduces ID-Switches by 90% compared to prior approaches,which is an order of magnitude less. The code and models are made available at", "output": "Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised object-centric representation (OCR) learning has recently drawnattention as a new paradigm of visual representation. This is because of itspotential of being an effective pre-training technique for various downstreamtasks in terms of sample efficiency, systematic generalization, and reasoning.Although image-based reinforcement learning (RL) is one of the most importantand thus frequently mentioned such downstream tasks, the benefit in RL hassurprisingly not been investigated systematically thus far. Instead, most ofthe evaluations have focused on rather indirect metrics such as segmentationquality and object property prediction accuracy. In this paper, we investigatethe effectiveness of OCR pre-training for image-based reinforcement learningvia empirical experiments. For systematic evaluation, we introduce a simpleobject-centric visual RL benchmark and conduct experiments to answer questionssuch as ``Does OCR pre-training improve performance on object-centric tasks?''and ``Can OCR pre-training help with out-of-distribution generalization?''. Ourresults provide empirical evidence for valuable insights into the effectivenessof OCR pre-training for RL and the potential limitations of its use in certainscenarios. Additionally, this study also examines the critical aspects ofincorporating OCR pre-training in RL, including performance in a visuallycomplex environment and the appropriate pooling layer to aggregate the objectrepresentations.", "output": "An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "IoU losses are surrogates that directly optimize the Jaccard index. Insemantic segmentation, leveraging IoU losses as part of the loss function isshown to perform better with respect to the Jaccard index measure thanoptimizing pixel-wise losses such as the cross-entropy loss alone. The mostnotable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss.However, these losses are incompatible with soft labels which are ubiquitous inmachine learning. In this paper, we propose Jaccard metric losses (JMLs), whichare identical to the soft Jaccard loss in a standard setting with hard labels,but are compatible with soft labels. With JMLs, we study two of the mostpopular use cases of soft labels: label smoothing and knowledge distillation.With a variety of architectures, our experiments show significant improvementsover the cross-entropy loss on three semantic segmentation datasets(Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approachoutperforms state-of-the-art knowledge distillation methods by a large margin.Code is available at:href{", "output": "Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class ofconditional diffusion models that directly learn the nonlinear diffusionprocesses between two given distributions. These diffusion bridges areparticularly useful for image restoration, as the degraded images arestructurally informative priors for reconstructing the clean images. I$^2$SBbelongs to a tractable class of Schr\"odinger bridge, the nonlinear extensionto score-based models, whose marginal distributions can be computedanalytically given boundary pairs. This results in a simulation-free frameworkfor nonlinear diffusions, where the I$^2$SB training becomes scalable byadopting practical techniques used in standard diffusion models. We validateI$^2$SB in solving various image restoration tasks, including inpainting,super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and showthat I$^2$SB surpasses standard conditional diffusion models with moreinterpretable generative processes. Moreover, I$^2$SB matches the performanceof inverse methods that additionally require the knowledge of the corruptionoperators. Our work opens up new algorithmic opportunities for developingefficient nonlinear diffusion models on a large scale. scale. Project page andcodes: ", "output": "I$^2$SB: Image-to-Image Schr\\\"odinger Bridge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Well-performed deep neural networks (DNNs) generally require massive labelleddata and computational resources for training. Various watermarking techniquesare proposed to protect such intellectual properties (IPs), wherein the DNNproviders implant secret information into the model so that they can laterclaim IP ownership by retrieving their embedded watermarks with some dedicatedtrigger inputs. While promising results are reported in the literature,existing solutions suffer from watermark removal attacks, such as modelfine-tuning and model pruning.In this paper, we propose a novel DNN watermarking solution that caneffectively defend against the above attacks. Our key insight is to enhance thecoupling of the watermark and model functionalities such that removing thewatermark would inevitably degrade the model's performance on normal inputs. Tothis end, unlike previous methods relying on secret features learnt fromout-of-distribution data, our method only uses features learnt fromin-distribution data. Specifically, on the one hand, we propose to sampleinputs from the original training dataset and fuse them as watermark triggers.On the other hand, we randomly mask model weights during training so that theinformation of our embedded watermarks spreads in the network. By doing so,model fine-tuning/pruning would not forget our function-coupled watermarks.Evaluation results on various image classification tasks show a 100% watermarkauthentication success rate under aggressive watermark removal attacks,significantly outperforming existing solutions. Code is available:", "output": "On Function-Coupled Watermarks for Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To ensure reliable object detection in autonomous systems, the detector mustbe able to adapt to changes in appearance caused by environmental factors suchas time of day, weather, and seasons. Continually adapting the detector toincorporate these changes is a promising solution, but it can becomputationally costly. Our proposed approach is to selectively adapt thedetector only when necessary, using new data that does not have the samedistribution as the current training data. To this end, we investigate threepopular metrics for domain gap evaluation and find that there is a correlationbetween the domain gap and detection accuracy. Therefore, we apply the domaingap as a criterion to decide when to adapt the detector. Our experiments showthat our approach has the potential to improve the efficiency of the detector'soperation in real-world scenarios, where environmental conditions change in acyclical manner, without sacrificing the overall performance of the detector.Our code is publicly available at ", "output": "Assessing Domain Gap for Continual Domain Adaptation in Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Single object tracking is a well-known and challenging research topic incomputer vision. Over the last two decades, numerous researchers have proposedvarious algorithms to solve this problem and achieved promising results.Recently, Transformer-based tracking approaches have ushered in a new era insingle object tracking due to their superior tracking robustness. Althoughseveral survey studies have been conducted to analyze the performance oftrackers, there is a need for another survey study after the introduction ofTransformers in single object tracking. In this survey, we aim to analyze theliterature and performances of Transformer tracking approaches. Therefore, weconduct an in-depth literature analysis of Transformer tracking approaches andevaluate their tracking robustness and computational efficiency on challengingbenchmark datasets. In addition, we have measured their performances ondifferent tracking scenarios to find their strength and weaknesses. Our surveyprovides insights into the underlying principles of Transformer trackingapproaches, the challenges they face, and their future directions.", "output": "Transformers in Single Object Tracking: An Experimental Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image-text retrieval which associates different modalities has drawn broadattention due to its excellent research value and broad real-world application.However, most of the existing methods haven't taken the high-level semanticrelationships (\"style embedding\") and common knowledge from multi-modalitiesinto full consideration. To this end, we introduce a novel style transformernetwork with common knowledge optimization (CKSTN) for image-text retrieval.The main module is the common knowledge adaptor (CKA) with both the styleembedding extractor (SEE) and the common knowledge optimization (CKO) modules.Specifically, the SEE uses the sequential update strategy to effectivelyconnect the features of different stages in SEE. The CKO module is introducedto dynamically capture the latent concepts of common knowledge from differentmodalities. Besides, to get generalized temporal common knowledge, we propose asequential update strategy to effectively integrate the features of differentlayers in SEE with previous common feature units. CKSTN demonstrates thesuperiorities of the state-of-the-art methods in image-text retrieval on MSCOCOand Flickr30K datasets. Moreover, CKSTN is constructed based on the lightweighttransformer which is more convenient and practical for the application of realscenes, due to the better performance and lower parameters.", "output": "The style transformer with common knowledge optimization for image-text retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Depth-aware video panoptic segmentation is a promising approach to camerabased scene understanding. However, the current state-of-the-art methodsrequire costly video annotations and use a complex training pipeline comparedto their image-based equivalents. In this paper, we present a new approachtitled Unified Perception that achieves state-of-the-art performance withoutrequiring video-based training. Our method employs a simple two-stage cascadedtracking algorithm that (re)uses object embeddings computed in an image-basednetwork. Experimental results on the Cityscapes-DVPS dataset demonstrate thatour method achieves an overall DVPQ of 57.1, surpassing state-of-the-artmethods. Furthermore, we show that our tracking strategies are effective forlong-term object association on KITTI-STEP, achieving an STQ of 59.1 whichexceeded the performance of state-of-the-art methods that employ the samebackbone network.Code is available at: ", "output": "Unified Perception: Efficient Depth-Aware Video Panoptic Segmentation with Minimal Annotation Costs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial intelligence-based methods have generated substantial interest innuclear medicine. An area of significant interest has been using deep-learning(DL)-based approaches for denoising images acquired with lower doses, shorteracquisition times, or both. Objective evaluation of these approaches isessential for clinical application. DL-based approaches for denoisingnuclear-medicine images have typically been evaluated using fidelity-basedfigures of merit (FoMs) such as RMSE and SSIM. However, these images areacquired for clinical tasks and thus should be evaluated based on theirperformance in these tasks. Our objectives were to (1) investigate whetherevaluation with these FoMs is consistent with objective clinical-task-basedevaluation; (2) provide a theoretical analysis for determining the impact ofdenoising on signal-detection tasks; (3) demonstrate the utility of virtualclinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate aDL-based method for denoising myocardial perfusion SPECT (MPS) images wasconducted. The impact of DL-based denoising was evaluated using fidelity-basedFoMs and AUC, which quantified performance on detecting perfusion defects inMPS images as obtained using a model observer with anthropomorphic channels.Based on fidelity-based FoMs, denoising using the considered DL-based methodled to significantly superior performance. However, based on ROC analysis,denoising did not improve, and in fact, often degraded detection-taskperformance. The results motivate the need for objective task-based evaluationof DL-based denoising approaches. Further, this study shows how VCTs provide amechanism to conduct such evaluations using VCTs. Finally, our theoreticaltreatment reveals insights into the reasons for the limited performance of thedenoising approach.", "output": "Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene flow estimation, which predicts the 3D motion of scene points frompoint clouds, is a core task in autonomous driving and many other 3D visionapplications. Existing methods either suffer from structure distortion due toignorance of rigid motion consistency or require explicit pose estimation and3D object segmentation. Errors of estimated poses and segmented objects wouldyield inaccurate rigidity constraints and in turn mislead scene flowestimation. In this paper, we propose a novel weight-sharing aggregation (WSA)method for feature and scene flow up-sampling. WSA does not rely on estimatedposes and segmented objects, and can implicitly enforce rigidity constraints toavoid structure distortion in scene flow estimation. To further exploitgeometric information and preserve local structure, we design a deformationdegree module aim to keep the local region invariance. We modify thePointPWC-Net and integrate the proposed WSA and deformation degree module intothe enhanced PointPWC-Net to derive an end-to-end scene flow estimationnetwork, called WSAFlowNet. Extensive experimental results on theFlyingThings3D and KITTI datasets demonstrate that our WSAFlowNet achieves thestate-of-the-art performance and outperforms previous methods by a largemargin. We will release the source code at", "output": "Exploiting Implicit Rigidity Constraints via Weight-Sharing Aggregation for Scene Flow Estimation from Point Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models are challenged by the distribution shift between thetraining data and test data. Recently, the large models pre-trained on diversedata have demonstrated unprecedented robustness to various distribution shifts.However, fine-tuning these models can lead to a trade-off betweenin-distribution (ID) performance and out-of-distribution (OOD) robustness.Existing methods for tackling this trade-off do not explicitly address the OODrobustness problem. In this paper, based on causal analysis of theaforementioned problems, we propose a novel fine-tuning method, which usesmasked images as counterfactual samples that help improve the robustness of thefine-tuning model. Specifically, we mask either the semantics-related orsemantics-unrelated patches of the images based on class activation map tobreak the spurious correlation, and refill the masked patches with patches fromother images. The resulting counterfactual samples are used in feature-baseddistillation with the pre-trained model. Extensive experiments verify thatregularizing the fine-tuning with the proposed masked images can achieve abetter trade-off between ID and OOD performance, surpassing previous methods onthe OOD performance. Our code is available at", "output": "Masked Images Are Counterfactual Samples for Robust Fine-tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, considering the balance of data/model privacy of model ownersand user needs, we propose a new setting called Back-Propagated Black-BoxAdaptation (BPBA) for users to better train their private models via theguidance of the back-propagated results of a Black-box foundation/source model.Our setting can ease the usage of foundation/source models as well as preventthe leakage and misuse of foundation/source models. Moreover, we also propose anew training strategy called Bootstrap The Original Latent (BTOL) to fullyutilize the foundation/source models. Our strategy consists of a domain adapterand a freeze-and-thaw strategy. We apply our BTOL under BPBA and Black-box UDAsettings on three different datasets. Experiments show that our strategy isefficient and robust in various settings without manual augmentations.", "output": "Bootstrap The Original Latent: Learning a Private Model from a Black-box Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This survey reviews text-to-image diffusion models in the context thatdiffusion models have emerged to be popular for a wide range of generativetasks. As a self-contained work, this survey starts with a brief introductionof how a basic diffusion model works for image synthesis, followed by howcondition or guidance improves learning. Based on that, we present a review ofstate-of-the-art methods on text-conditioned image synthesis, i.e.,text-to-image. We further summarize applications beyond text-to-imagegeneration: text-guided creative generation and text-guided image editing.Beyond the progress made so far, we discuss existing challenges and promisingfuture directions.", "output": "Text-to-image Diffusion Models in Generative AI: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the fact that text-to-video (TTV) model has recently achievedremarkable success, there have been few approaches on TTV for its extension tovideo editing. Motivated by approaches on TTV models adapting fromdiffusion-based text-to-image (TTI) models, we suggest the video editingframework given only a pretrained TTI model and a single &lt;text, video&gt; pair,which we term Edit-A-Video. The framework consists of two stages: (1) inflatingthe 2D model into the 3D model by appending temporal modules and tuning on thesource video (2) inverting the source video into the noise and editing withtarget text prompt and attention map injection. Each stage enables the temporalmodeling and preservation of semantic attributes of the source video. One ofthe key challenges for video editing include a background inconsistencyproblem, where the regions not included for the edit suffer from undesirableand inconsistent temporal alterations. To mitigate this issue, we alsointroduce a novel mask blending method, termed as sparse-causal blending (SCBlending). We improve previous mask blending methods to reflect the temporalconsistency so that the area where the editing is applied exhibits smoothtransition while also achieving spatio-temporal consistency of the uneditedregions. We present extensive experimental results over various types of textand videos, and demonstrate the superiority of the proposed method compared tobaselines in terms of background consistency, text alignment, and video editingquality.", "output": "Edit-A-Video: Single Video Editing with Object-Aware Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lithography is fundamental to integrated circuit fabrication, necessitatinglarge computation overhead. The advancement of machine learning (ML)-basedlithography models alleviates the trade-offs between manufacturing processexpense and capability. However, all previous methods regard the lithographysystem as an image-to-image black box mapping, utilizing network parameters tolearn by rote mappings from massive mask-to-aerial or mask-to-resist imagepairs, resulting in poor generalization capability. In this paper, we propose anew ML-based paradigm disassembling the rigorous lithographic model intonon-parametric mask operations and learned optical kernels containingdeterminant source, pupil, and lithography information. By optimizingcomplex-valued neural fields to perform optical kernel regression fromcoordinates, our method can accurately restore lithography system using asmall-scale training dataset with fewer parameters, demonstrating superiorgeneralization capability as well. Experiments show that our framework can use31% of parameters while achieving 69$times$ smaller mean squared error with1.3$times$ higher throughput than the state-of-the-art.", "output": "Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent attention in instance segmentation has focused on query-based models.Despite being non-maximum suppression (NMS)-free and end-to-end, thesuperiority of these models on high-accuracy real-time benchmarks has not beenwell demonstrated. In this paper, we show the strong potential of query-basedmodels on efficient instance segmentation algorithm designs. We presentFastInst, a simple, effective query-based framework for real-time instancesegmentation. FastInst can execute at a real-time speed (i.e., 32.5 FPS) whileyielding an AP of more than 40 (i.e., 40.5 AP) on COCO test-dev without bellsand whistles. Specifically, FastInst follows the meta-architecture of recentlyintroduced Mask2Former. Its key designs include instance activation-guidedqueries, dual-path update strategy, and ground truth mask-guided learning,which enable us to use lighter pixel decoders, fewer Transformer decoderlayers, while achieving better performance. The experiments show that FastInstoutperforms most state-of-the-art real-time counterparts, including strongfully convolutional baselines, in both speed and accuracy. Code can be found at .", "output": "FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many imaging inverse problems$unicode{x2014}$such as image-dependentin-painting and dehazing$unicode{x2014}$are challenging because their forwardmodels are unknown or depend on unknown latent parameters. While one can solvesuch problems by training a neural network with vast quantities of pairedtraining data, such paired training data is often unavailable. In this paper,we propose a generalized framework for training image reconstruction networkswhen paired training data is scarce. In particular, we demonstrate the abilityof image denoising algorithms and, by extension, denoising diffusion models tosupervise network training in the absence of paired training data.", "output": "SUD$^2$: Supervision by Denoising Diffusion Models for Image Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This report addresses the technical aspects of de-identification of medicalimages of human subjects and biospecimens, such that re-identification risk ofethical, moral, and legal concern is sufficiently reduced to allow unrestrictedpublic sharing for any purpose, regardless of the jurisdiction of the sourceand distribution sites. All medical images, regardless of the mode ofacquisition, are considered, though the primary emphasis is on those withaccompanying data elements, especially those encoded in formats in which thedata elements are embedded, particularly Digital Imaging and Communications inMedicine (DICOM). These images include image-like objects such asSegmentations, Parametric Maps, and Radiotherapy (RT) Dose objects. The scopealso includes related non-image objects, such as RT Structure Sets, Plans andDose Volume Histograms, Structured Reports, and Presentation States. Onlyde-identification of publicly released data is considered, and alternativeapproaches to privacy preservation, such as federated learning for artificialintelligence (AI) model development, are out of scope, as are issues of privacyleakage from AI model sharing. Only technical issues of public sharing areaddressed.", "output": "Report of the Medical Image De-Identification (MIDI) Task Group -- Best Practices and Recommendations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gun violence is a critical security problem, and it is imperative for thecomputer vision community to develop effective gun detection algorithms forreal-world scenarios, particularly in Closed Circuit Television (CCTV)surveillance data. Despite significant progress in visual object detection,detecting guns in real-world CCTV images remains a challenging andunder-explored task. Firearms, especially handguns, are typically very small insize, non-salient in appearance, and often severely occluded orindistinguishable from other small objects. Additionally, the lack ofprincipled benchmarks and difficulty collecting relevant datasets furtherhinder algorithmic development. In this paper, we present a meticulouslycrafted and annotated benchmark, called textbf{CCTV-Gun}, which addresses thechallenges of detecting handguns in real-world CCTV images. Our contribution isthree-fold. Firstly, we carefully select and analyze real-world CCTV imagesfrom three datasets, manually annotate handguns and their holders, and assigneach image with relevant challenge factors such as blur and occlusion.Secondly, we propose a new cross-dataset evaluation protocol in addition to thestandard intra-dataset protocol, which is vital for gun detection in practicalsettings. Finally, we comprehensively evaluate both classical andstate-of-the-art object detection algorithms, providing an in-depth analysis oftheir generalizing abilities. The benchmark will facilitate further researchand development on this topic and ultimately enhance security. Code,annotations, and trained models are available at", "output": "CCTV-Gun: Benchmarking Handgun Detection in CCTV Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a method that enables interactive geometry editingfor neural radiance fields manipulation. We use two proxy cages(inner cage andouter cage) to edit a scene. The inner cage defines the operation target, andthe outer cage defines the adjustment space. Various operations apply to thetwo cages. After cage selection, operations on the inner cage lead to thedesired transformation of the inner cage and adjustment of the outer cage.Users can edit the scene with translation, rotation, scaling, or combinations.The operations on the corners and edges of the cage are also supported. Ourmethod does not need any explicit 3D geometry representations. The interactivegeometry editing applies directly to the implicit neural radiance fields.Extensive experimental results demonstrate the effectiveness of our approach.", "output": "Interactive Geometry Editing of Neural Radiance Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prevalent predictive coding-based video compression methods rely on a heavyencoder to reduce temporal redundancy, which makes it challenging to deploythem on resource-constrained devices. Since the 1970s, distributed sourcecoding theory has indicated that independent encoding and joint decoding withside information (SI) can achieve high-efficient compression of correlatedsources. This has inspired a distributed coding architecture aiming at reducingthe encoding complexity. However, traditional distributed coding methods sufferfrom a substantial performance gap to predictive coding ones. Inspired by thegreat success of learning-based compression, we propose the first end-to-enddistributed deep video compression framework to improve the rate-distortionperformance. A key ingredient is an effective SI generation module at thedecoder, which helps to effectively exploit inter-frame correlations withoutcomputation-intensive encoder-side motion estimation and compensation.Experiments show that our method significantly outperforms conventionaldistributed video coding and H.264. Meanwhile, it enjoys 6-7x encoding speedupagainst DVC [1] with comparable compression performance. Code is released at", "output": "Low-complexity Deep Video Compression with A Distributed Coding Architecture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning models for camera-based physiological measurement can haveweak generalization due to a lack of representative training data. Body motionis one of the most significant sources of noise when attempting to recover thesubtle cardiac pulse from a video. We explore motion transfer as a form of dataaugmentation to introduce motion variation while preserving physiologicalchanges. We adapt a neural video synthesis approach to augment videos for thetask of remote photoplethysmography (PPG) and study the effects of motionaugmentation with respect to 1) the magnitude and 2) the type of motion. Aftertraining on motion-augmented versions of publicly available datasets, thepresented inter-dataset results on five benchmark datasets show improvements ofup to 75% over existing state-of-the-art results. Our findings illustrate theutility of motion transfer as a data augmentation technique for improving thegeneralization of models for camera-based physiological sensing. We release ourcode and pre-trained models for using motion transfer as a data augmentationtechnique on our project page: ", "output": "Motion Matters: Neural Motion Transfer for Better Camera Physiological Sensing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Calibration of deep learning models is crucial to their trustworthiness andsafe usage, and as such, has been extensively studied in supervisedclassification models, with methods crafted to decrease miscalibration.However, there has yet to be a comprehensive study of the calibration ofvision-language models that are used for zero-shot inference, like CLIP. Wemeasure calibration across relevant variables like prompt, dataset, andarchitecture, and find that zero-shot inference with CLIP is miscalibrated.Furthermore, we propose a modified version of temperature scaling that isaligned with the common use cases of CLIP as a zero-shot inference model, andshow that a single learned temperature generalizes for each specific CLIP model(defined by a chosen pre-training dataset and architecture) across inferencedataset and prompt choice.", "output": "Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training-time defenses, known as adversarial training, incur high trainingcosts and do not generalize to unseen attacks. Test-time defenses solve theseissues but most existing test-time defenses require adapting the model weights,therefore they do not work on frozen models and complicate model memorymanagement. The only test-time defense that does not adapt model weights aimsto adapt the input with self-supervision tasks. However, we empirically foundthese self-supervision tasks are not sensitive enough to detect adversarialattacks accurately. In this paper, we propose DRAM, a novel defense method todetect and repair adversarial samples at test time via Masked autoencoder(MAE). We demonstrate how to use MAE losses to build a Kolmogorov-Smirnov testto detect adversarial samples. Moreover, we use the MAE losses to calculateinput reversal vectors that repair adversarial samples resulting frompreviously unseen attacks. Results on large-scale ImageNet dataset show that,compared to all detection baselines evaluated, DRAM achieves the best detectionrate (82% on average) on all eight adversarial attacks evaluated. For attackrepair, DRAM improves the robust accuracy by 6% ~ 41% for standard ResNet50 and3% ~ 8% for robust ResNet50 compared with the baselines that use contrastivelearning and rotation prediction.", "output": "Test-time Detection and Repair of Adversarial Samples via Masked Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we investigate the problem of creating high-fidelity 3D contentfrom only a single image. This is inherently challenging: it essentiallyinvolves estimating the underlying 3D geometry while simultaneouslyhallucinating unseen textures. To address this challenge, we leverage priorknowledge from a well-trained 2D diffusion model to act as 3D-aware supervisionfor 3D creation. Our approach, Make-It-3D, employs a two-stage optimizationpipeline: the first stage optimizes a neural radiance field by incorporatingconstraints from the reference image at the frontal view and diffusion prior atnovel views; the second stage transforms the coarse model into textured pointclouds and further elevates the realism with diffusion prior while leveragingthe high-quality textures from the reference image. Extensive experimentsdemonstrate that our method outperforms prior works by a large margin,resulting in faithful reconstructions and impressive visual quality. Our methodpresents the first attempt to achieve high-quality 3D creation from a singleimage for general objects and enables various applications such as text-to-3Dcreation and texture editing.", "output": "Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Catastrophic forgetting has been a major challenge in continual learning,where the model needs to learn new tasks with limited or no access to data frompreviously seen tasks. To tackle this challenge, methods based on knowledgedistillation in feature space have been proposed and shown to reduceforgetting. However, most feature distillation methods directly constrain thenew features to match the old ones, overlooking the need for plasticity. Toachieve a better stability-plasticity trade-off, we propose Backward FeatureProjection (BFP), a method for continual learning that allows the new featuresto change up to a learnable linear transformation of the old features. BFPpreserves the linear separability of the old classes while allowing theemergence of new feature directions to accommodate new classes. BFP can beintegrated with existing experience replay methods and boost performance by asignificant margin. We also demonstrate that BFP helps learn a betterrepresentation space, in which linear separability is well preserved duringcontinual learning and linear probing achieves high classification accuracy.The code can be found at ", "output": "Preserving Linear Separability in Continual Learning by Backward Feature Projection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural view synthesis (NVS) is one of the most successful techniques forsynthesizing free viewpoint videos, capable of achieving high fidelity fromonly a sparse set of captured images. This success has led to many variants ofthe techniques, each evaluated on a set of test views typically using imagequality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of researchon how NVS methods perform with respect to perceived video quality. We presentthe first study on perceptual evaluation of NVS and NeRF variants. For thisstudy, we collected two datasets of scenes captured in a controlled labenvironment as well as in-the-wild. In contrast to existing datasets, thesescenes come with reference video sequences, allowing us to test for temporalartifacts and subtle distortions that are easily overlooked when viewing onlystatic images. We measured the quality of videos synthesized by several NVSmethods in a well-controlled perceptual quality assessment experiment as wellas with many existing state-of-the-art image/video quality metrics. We presenta detailed analysis of the results and recommendations for dataset and metricselection for NVS evaluation.", "output": "Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic radiology report generation has attracted enormous researchinterest due to its practical value in reducing the workload of radiologists.However, simultaneously establishing global correspondences between the image(e.g., Chest X-ray) and its related report and local alignments between imagepatches and keywords remains challenging. To this end, we propose an Unify,Align and then Refine (UAR) approach to learn multi-level cross-modalalignments and introduce three novel modules: Latent Space Unifier (LSU),Cross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR).Specifically, LSU unifies multimodal data into discrete tokens, making itflexible to learn common knowledge among modalities with a shared network. Themodality-agnostic CRA learns discriminative features via a set of orthonormalbasis and a dual-gate mechanism first and then globally aligns visual andtextual representations under a triplet contrastive loss. TIR booststoken-level local alignment via calibrating text-to-image attention with alearnable mask. Additionally, we design a two-stage training procedure to makeUAR gradually grasp cross-modal alignments at different levels, which imitatesradiologists' workflow: writing sentence by sentence first and then checkingword by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXRbenchmark datasets demonstrate the superiority of our UAR against variedstate-of-the-art methods.", "output": "Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Learning based diagnostics systems can provide accurate and robustquantitative analysis in digital pathology. These algorithms require largeamounts of annotated training data which is impractical in pathology due to thehigh resolution of histopathological images. Hence, self-supervised methodshave been proposed to learn features using ad-hoc pretext tasks. Theself-supervised training process is time consuming and often leads to subparfeature representation due to a lack of constrain on the learnt feature space,particularly prominent under data imbalance. In this work, we propose toactively sample the training set using a handful of labels and a small proxynetwork, decreasing sample requirement by 93% and training time by 99%.", "output": "Data Efficient Contrastive Learning in Histopathology using Active Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The soft Dice loss (SDL) has taken a pivotal role in many automatedsegmentation pipelines in the medical imaging community. Over the last years,some reasons behind its superior functioning have been uncovered and furtheroptimizations have been explored. However, there is currently no implementationthat supports its direct use in settings with soft labels. Hence, a synergybetween the use of SDL and research leveraging the use of soft labels, also inthe context of model calibration, is still missing. In this work, we introduceDice semimetric losses (DMLs), which (i) are by design identical to SDL in astandard setting with hard labels, but (ii) can be used in settings with softlabels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirmthe potential synergy of DMLs with soft labels (e.g. averaging, labelsmoothing, and knowledge distillation) over hard labels (e.g. majority votingand random selection). As a result, we obtain superior Dice scores and modelcalibration, which supports the wider adoption of DMLs in practice. Code isavailable athref{", "output": "Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the record-breaking performance in Text-to-Image (T2I) generation byStable Diffusion, less research attention is paid to its adversarialrobustness. In this work, we study the problem of adversarial attack generationfor Stable Diffusion and ask if an adversarial text prompt can be obtained evenin the absence of end-to-end model queries. We call the resulting problem'query-free attack generation'. To resolve this problem, we show that thevulnerability of T2I models is rooted in the lack of robustness of textencoders, e.g., the CLIP text encoder used for attacking Stable Diffusion.Based on such insight, we propose both untargeted and targeted query-freeattacks, where the former is built on the most influential dimensions in thetext embedding space, which we call steerable key dimensions. By leveraging theproposed attacks, we empirically show that only a five-character perturbationto the text prompt is able to cause the significant content shift ofsynthesized images using Stable Diffusion. Moreover, we show that the proposedtarget attack can precisely steer the diffusion model to scrub the targetedimage content without causing much change in untargeted image content. Our codeis available at ", "output": "A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently reported state-of-the-art results in visual speech recognition (VSR)often rely on increasingly large amounts of video data, while the publiclyavailable transcribed video datasets are limited in size. In this paper, forthe first time, we study the potential of leveraging synthetic visual data forVSR. Our method, termed SynthVSR, substantially improves the performance of VSRsystems with synthetic lip movements. The key idea behind SynthVSR is toleverage a speech-driven lip animation model that generates lip movementsconditioned on the input speech. The speech-driven lip animation model istrained on an unlabeled audio-visual dataset and could be further optimizedtowards a pre-trained VSR model when labeled videos are available. As plenty oftranscribed acoustic data and face images are available, we are able togenerate large-scale synthetic data using the proposed lip animation model forsemi-supervised VSR training. We evaluate the performance of our approach onthe largest public VSR benchmark - Lip Reading Sentences 3 (LRS3). SynthVSRachieves a WER of 43.3% with only 30 hours of real labeled data, outperformingoff-the-shelf approaches using thousands of hours of video. The WER is furtherreduced to 27.9% when using all 438 hours of labeled data from LRS3, which ison par with the state-of-the-art self-supervised AV-HuBERT method. Furthermore,when combined with large-scale pseudo-labeled audio-visual data SynthVSR yieldsa new state-of-the-art VSR WER of 16.9% using publicly available data only,surpassing the recent state-of-the-art approaches trained with 29 times morenon-public machine-transcribed video data (90,000 hours). Finally, we performextensive ablation studies to understand the effect of each component in ourproposed method.", "output": "SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networkshave proven to be highly effective, achieving state-of-the-art results. Thisstudy introduces a novel streaming architecture based toolflow for mapping suchmodels onto FPGAs considering the model's inherent characteristics and thefeatures of the targeted FPGA device. The HARFLOW3D toolflow takes as input a3D CNN in ONNX format and a description of the FPGA characteristics, generatinga design that minimizes the latency of the computation. The toolflow iscomprised of a number of parts, including i) a 3D CNN parser, ii) a performanceand resource model, iii) a scheduling algorithm for executing 3D models on thegenerated hardware, iv) a resource-aware optimization engine tailored for 3Dmodels, v) an automated mapping to synthesizable code for FPGAs. The ability ofthe toolflow to support a broad range of models and devices is shown through anumber of experiments on various 3D CNN and FPGA system pairs. Furthermore, thetoolflow has produced high-performing results for 3D CNN models that have notbeen mapped to FPGAs before, demonstrating the potential of FPGA-based systemsin this space. Overall, HARFLOW3D has demonstrated its ability to delivercompetitive latency compared to a range of state-of-the-art hand-tunedapproaches being able to achieve up to 5$times$ better performance compared tosome of the existing works.", "output": "HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The accurate representation of 3D building models in urban environments issignificantly hindered by challenges such as texture occlusion, blurring, andmissing details, which are difficult to mitigate through standardphotogrammetric texture mapping pipelines. Current image completion methodsoften struggle to produce structured results and effectively handle theintricate nature of highly-structured fac{c}ade textures with diversearchitectural styles. Furthermore, existing image synthesis methods encounterdifficulties in preserving high-frequency details and artificial regularstructures, which are essential for achieving realistic fac{c}ade texturesynthesis. To address these challenges, we introduce a novel approach forsynthesizing fac{c}ade texture images that authentically reflect thearchitectural style from a structured label map, guided by a ground-truthfac{c}ade image. In order to preserve fine details and regular structures, wepropose a regularity-aware multi-domain method that capitalizes on frequencyinformation and corner maps. We also incorporate SEAN blocks into our generatorto enable versatile style transfer. To generate plausible structured imageswithout undesirable regions, we employ image completion techniques to removeocclusions according to semantics prior to image inference. Our proposed methodis also capable of synthesizing texture images with specific styles forfac{c}ades that lack pre-existing textures, using manually annotated labels.Experimental results on publicly available fac{c}ade image and 3D modeldatasets demonstrate that our method yields superior results and effectivelyaddresses issues associated with flawed textures. The code and datasets will bemade publicly available for further research and development.", "output": "Semantic Image Translation for Repairing the Texture Defects of Building Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solving complicated AI tasks with different domains and modalities is a keystep toward advanced artificial intelligence. While there are abundant AImodels available for different domains and modalities, they cannot handlecomplicated AI tasks. Considering large language models (LLMs) have exhibitedexceptional ability in language understanding, generation, interaction, andreasoning, we advocate that LLMs could act as a controller to manage existingAI models to solve complicated AI tasks and language could be a genericinterface to empower this. Based on this philosophy, we present HuggingGPT, aframework that leverages LLMs (e.g., ChatGPT) to connect various AI models inmachine learning communities (e.g., Hugging Face) to solve AI tasks.Specifically, we use ChatGPT to conduct task planning when receiving a userrequest, select models according to their function descriptions available inHugging Face, execute each subtask with the selected AI model, and summarizethe response according to the execution results. By leveraging the stronglanguage capability of ChatGPT and abundant AI models in Hugging Face,HuggingGPT is able to cover numerous sophisticated AI tasks in differentmodalities and domains and achieve impressive results in language, vision,speech, and other challenging tasks, which paves a new way towards advancedartificial intelligence.", "output": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a 3D object, kinematic motion prediction aims to identify the mobileparts as well as the corresponding motion parameters. Due to the largevariations in both topological structure and geometric details of 3D objects,this remains a challenging task and the lack of large scale labeled data alsoconstrain the performance of deep learning based approaches. In this paper, wetackle the task of object kinematic motion prediction problem in a semi-weaklysupervised manner. Our key observations are two-fold. First, although 3Ddataset with fully annotated motion labels is limited, there are existingdatasets and methods for object part semantic segmentation at large scale.Second, semantic part segmentation and mobile part segmentation is not alwaysconsistent but it is possible to detect the mobile parts from the underlying 3Dstructure. Towards this end, we propose a graph neural network to learn the mapbetween hierarchical part-level segmentation and mobile parts parameters, whichare further refined based on geometric alignment. This network can be firsttrained on PartNet-Mobility dataset with fully labeled mobility information andthen applied on PartNet dataset with fine-grained and hierarchical part-levelsegmentation. The network predictions yield a large scale of 3D objects withpseudo labeled mobility information and can further be used forweakly-supervised learning with pre-existing segmentation. Our experiments showthere are significant performance boosts with the augmented data for previousmethod designed for kinematic motion prediction on 3D partial scans.", "output": "Semi-Weakly Supervised Object Kinematic Motion Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) have been shown to be effective in various visiontasks. However, resizing them to a mobile-friendly size leads to significantperformance degradation. Therefore, developing lightweight vision transformershas become a crucial area of research. This paper introduces CloFormer, alightweight vision transformer that leverages context-aware local enhancement.CloFormer explores the relationship between globally shared weights often usedin vanilla convolutional operators and token-specific context-aware weightsappearing in attention, then proposes an effective and straightforward moduleto capture high-frequency local information. In CloFormer, we introduceAttnConv, a convolution operator in attention's style. The proposed AttnConvuses shared weights to aggregate local information and deploys carefullydesigned context-aware weights to enhance local features. The combination ofthe AttnConv and vanilla attention which uses pooling to reduce FLOPs inCloFormer enables the model to perceive high-frequency and low-frequencyinformation. Extensive experiments were conducted in image classification,object detection, and semantic segmentation, demonstrating the superiority ofCloFormer.", "output": "Rethinking Local Perception in Lightweight Vision Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Referring image segmentation (RIS) aims to find a segmentation mask given areferring expression grounded to a region of the input image. Collectinglabelled datasets for this task, however, is notoriously costly andlabor-intensive. To overcome this issue, we propose a simple yet effectivezero-shot referring image segmentation method by leveraging the pre-trainedcross-modal knowledge from CLIP. In order to obtain segmentation masks groundedto the input text, we propose a mask-guided visual encoder that captures globaland local contextual information of an input image. By utilizing instance masksobtained from off-the-shelf mask proposal techniques, our method is able tosegment fine-detailed Istance-level groundings. We also introduce aglobal-local text encoder where the global feature captures complexsentence-level semantics of the entire input expression while the local featurefocuses on the target noun phrase extracted by a dependency parser. In ourexperiments, the proposed method outperforms several zero-shot baselines of thetask and even the weakly supervised referring expression segmentation methodwith substantial margins. Our code is available at", "output": "Zero-shot Referring Image Segmentation with Global-Local Context Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised Continual learning involves updating a deep neural network (DNN)from an ever-growing stream of labeled data. While most work has focused onovercoming catastrophic forgetting, one of the major motivations behindcontinual learning is being able to efficiently update a network with newinformation, rather than retraining from scratch on the training dataset as itgrows over time. Despite recent continual learning methods largely solving thecatastrophic forgetting problem, there has been little attention paid to theefficiency of these algorithms. Here, we study recent methods for incrementalclass learning and illustrate that many are highly inefficient in terms ofcompute, memory, and storage. Some methods even require more compute thantraining from scratch! We argue that for continual learning to have real-worldapplicability, the research community cannot ignore the resources used by thesealgorithms. There is more to continual learning than mitigating catastrophicforgetting.", "output": "How Efficient Are Today's Continual Learning Algorithms?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The challenges of using inadequate online recruitment systems can beaddressed with machine learning and software engineering techniques.Bi-directional personalization reinforcement learning-based architecture withactive learning can get recruiters to recommend qualified applicants and alsoenable applicants to receive personalized job recommendations. This paperfocuses on how machine learning techniques can enhance the recruitment processin the travel nursing industry by helping speed up data acquisition using amulti-model data service and then providing personalized recommendations usingbi-directional reinforcement learning with active learning. This need wasespecially evident when trying to respond to the overwhelming needs ofhealthcare facilities during the COVID-19 pandemic. The need for travelingnurses and other healthcare professionals was more evident during the lockdownperiod. A data service was architected for job feed processing using anorchestration of natural language processing (NLP) models that synthesizejob-related data into a database efficiently and accurately. The multi-modeldata service provided the data necessary to develop a bi-directionalpersonalization system using reinforcement learning with active learning thatcould recommend travel nurses and healthcare professionals to recruiters andprovide job recommendations to applicants using an internally developed smartmatch score as a basis. The bi-directional personalization reinforcementlearning-based architecture with active learning combines two personalizationsystems - one that runs forward to recommend qualified candidates for jobs andanother that runs backward and recommends jobs for applicants.", "output": "Bi-directional personalization reinforcement learning-based architecture with active learning using a multi-model data service for the travel nursing industry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It has become cognitive inertia to employ cross-entropy loss function inclassification related tasks. In the untargeted attacks on graph structure, thegradients derived from the attack objective are the attacker's basis forevaluating a perturbation scheme. Previous methods use negative cross-entropyloss as the attack objective in attacking node-level classification models.However, the suitability of the cross-entropy function for constructing theuntargeted attack objective has yet been discussed in previous works. Thispaper argues about the previous unreasonable attack objective from theperspective of budget allocation. We demonstrate theoretically and empiricallythat negative cross-entropy tends to produce more significant gradients fromnodes with lower confidence in the labeled classes, even if the predictedclasses of these nodes have been misled. To free up these inefficient attackbudgets, we propose a simple attack model for untargeted attacks on graphstructure based on a novel attack objective which generates unweightedgradients on graph structures that are not affected by the node confidence. Byconducting experiments in gray-box poisoning attack scenarios, we demonstratethat a reasonable budget allocation can significantly improve the effectivenessof gradient-based edge perturbations without any extra hyper-parameter.", "output": "Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Organ transplant is the essential treatment method for some end-stagediseases, such as liver failure. Analyzing the post-transplant cause of death(CoD) after organ transplant provides a powerful tool for clinical decisionmaking, including personalized treatment and organ allocation. However,traditional methods like Model for End-stage Liver Disease (MELD) score andconventional machine learning (ML) methods are limited in CoD analysis due totwo major data and model-related challenges. To address this, we propose anovel framework called CoD-MTL leveraging multi-task learning to model thesemantic relationships between various CoD prediction tasks jointly.Specifically, we develop a novel tree distillation strategy for multi-tasklearning, which combines the strength of both the tree model and multi-tasklearning. Experimental results are presented to show the precise and reliableCoD predictions of our framework. A case study is conducted to demonstrate theclinical importance of our method in the liver transplant.", "output": "Multi-Task Learning for Post-transplant Cause of Death Analysis: A Case Study on Liver Transplant."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inverse problems are mathematically ill-posed. Thus, given some (noisy) data,there is more than one solution that fits the data. In recent years, deepneural techniques that find the most appropriate solution, in the sense that itcontains a-priori information, were developed. However, they suffer fromseveral shortcomings. First, most techniques cannot guarantee that the solutionfits the data at inference. Second, while the derivation of the techniques isinspired by the existence of a valid scalar regularization function, suchtechniques do not in practice rely on such a function, and therefore veer awayfrom classical variational techniques. In this work we introduce a new familyof neural regularizers for the solution of inverse problems. These regularizersare based on a variational formulation and are guaranteed to fit the data. Wedemonstrate their use on a number of highly ill-posed problems, from imagedeblurring to limited angle tomography.", "output": "DRIP: Deep Regularizers for Inverse Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The prevalence of memes on social media has created the need to sentimentanalyze their underlying meanings for censoring harmful content. Meme censoringsystems by machine learning raise the need for a semi-supervised learningsolution to take advantage of the large number of unlabeled memes available onthe internet and make the annotation process less challenging. Moreover, theapproach needs to utilize multimodal data as memes' meanings usually come fromboth images and texts. This research proposes a multimodal semi-supervisedlearning approach that outperforms other multimodal semi-supervised learningand supervised learning state-of-the-art models on two datasets, the MultimediaAutomatic Misogyny Identification and Hateful Memes dataset. Building on theinsights gained from Contrastive Language-Image Pre-training, which is aneffective multimodal learning technique, this research introduces SemiMemes, anovel training method that combines auto-encoder and classification task tomake use of the resourceful unlabeled data.", "output": "SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a review of the field of reinforcement learning (RL),with a focus on providing a comprehensive overview of the key concepts,techniques, and algorithms for beginners. RL has a unique setting, jargon, andmathematics that can be intimidating for those new to the field or artificialintelligence more broadly. While many papers review RL in the context ofspecific applications, such as games, healthcare, finance, or robotics, thesepapers can be difficult for beginners to follow due to the inclusion ofnon-RL-related work and the use of algorithms customized to those specificapplications. To address these challenges, this paper provides a clear andconcise overview of the fundamental principles of RL and covers the differenttypes of RL algorithms. For each algorithm/method, we outline the mainmotivation behind its development, its inner workings, and its limitations. Thepresentation of the paper is aligned with the historical progress of the field,from the early 1980s Q-learning algorithm to the current state-of-the-artalgorithms such as TD3, PPO, and offline RL. Overall, this paper aims to serveas a valuable resource for beginners looking to construct a solid understandingof the fundamentals of RL and be aware of the historical progress of the field.It is intended to be a go-to reference for those interested in learning aboutRL without being distracted by the details of specific applications.", "output": "Understanding Reinforcement Learning Algorithms: The Progress from Basic Q-learning to Proximal Policy Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven method for Structural Health Monitoring (SHM), that mine thehidden structural performance from the correlations among monitored time seriesdata, has received widely concerns recently. However, missing datasignificantly impacts the conduction of this method. Missing data is afrequently encountered issue in time series data in SHM and many otherreal-world applications, that harms to the standardized data mining anddownstream tasks, such as condition assessment. Imputation approaches based onspatiotemporal relations among monitoring data are developed to handle thisissue, however, no additional information is added during imputation. Thispaper thus develops a robust method for damage identification that considersthe missing data occasions, based on long-short term memory (LSTM) model anddropout mechanism in the autoencoder (AE) framework. Inputs channels arerandomly dropped to simulate the missing data in training, and reconstructionerrors are used as the loss function and the damage indicator. Quasi-staticresponse (cable tension) of a cable-stayed bridge released in 1st IPC-SHM isemployed to verify this proposed method, and results show that the missing dataimputation and damage identification can be implemented together in a unifiedway.", "output": "A robust deep learning-based damage identification approach for SHM considering missing data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sequential decision-making agents struggle with long horizon tasks, sincesolving them requires multi-step reasoning. Most reinforcement learning (RL)algorithms address this challenge by improved credit assignment, introducingmemory capability, altering the agent's intrinsic motivation (i.e. exploration)or its worldview (i.e. knowledge representation). Many of these componentscould be learned from offline data. In this work, we follow the hypothesis thatexploration and representation learning can be improved by separately learningtwo different models from a single offline dataset. We show that learning astate representation using noise-contrastive estimation and a model ofauxiliary reward separately from a single collection of human demonstrationscan significantly improve the sample efficiency on the challenging NetHackbenchmark. We also ablate various components of our experimental setting andhighlight crucial insights.", "output": "Accelerating exploration and representation learning with offline pre-training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Allowing organizations to share their data for training of machine learning(ML) models without unintended information leakage is an open problem inpractice. A promising technique for this still-open problem is to train modelson the encoded data. Our approach, called Privately Encoded Open Datasets withPublic Labels (PEOPL), uses a certain class of randomly constructed transformsto encode sensitive data. Organizations publish their randomly encoded data andassociated raw labels for ML training, where training is done without knowledgeof the encoding realization. We investigate several important aspects of thisproblem: We introduce information-theoretic scores for privacy and utility,which quantify the average performance of an unfaithful user (e.g., adversary)and a faithful user (e.g., model developer) that have access to the publishedencoded data. We then theoretically characterize primitives in buildingfamilies of encoding schemes that motivate the use of random deep neuralnetworks. Empirically, we compare the performance of our randomized encodingscheme and a linear scheme to a suite of computational attacks, and we alsoshow that our scheme achieves competitive prediction accuracy to raw-samplebaselines. Moreover, we demonstrate that multiple institutions, usingindependent random encoders, can collaborate to train improved ML models.", "output": "PEOPL: Characterizing Privately Encoded Open Datasets with Public Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many real-world settings, the critical class is rare and a misseddetection carries a disproportionately high cost. For example, tumors are rareand a false negative diagnosis could have severe consequences on treatmentoutcomes; fraudulent banking transactions are rare and an undetected occurrencecould result in significant losses or legal penalties. In such contexts,systems are often operated at a high true positive rate, which may requiretolerating high false positives. In this paper, we present a novel approach toaddress the challenge of minimizing false positives for systems that need tooperate at a high true positive rate. We propose a ranking-based regularization(RankReg) approach that is easy to implement, and show empirically that it notonly effectively reduces false positives, but also complements conventionalimbalanced learning losses. With this novel technique in hand, we conduct aseries of experiments on three broadly explored datasets (CIFAR-10&amp;100 andMelanoma) and show that our approach lifts the previous state-of-the-artperformance by notable margins.", "output": "Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We improve upon previous oblivious sketching and turnstile streaming resultsfor $ell_1$ and logistic regression, giving a much smaller sketching dimensionachieving $O(1)$-approximation and yielding an efficient optimization problemin the sketch space. Namely, we achieve for any constant $c&gt;0$ a sketchingdimension of $tilde{O}(d^{1+c})$ for $ell_1$ regression and $tilde{O}(mud^{1+c})$ for logistic regression, where $mu$ is a standard measure thatcaptures the complexity of compressing the data. For $ell_1$-regression oursketching dimension is near-linear and improves previous work which eitherrequired $Omega(log d)$-approximation with this sketching dimension, orrequired a larger $operatorname{poly}(d)$ number of rows. Similarly, forlogistic regression previous work had worse $operatorname{poly}(mu d)$factors in its sketching dimension. We also give a tradeoff that yields a$1+varepsilon$ approximation in input sparsity time by increasing the totalsize to $(dlog(n)/varepsilon)^{O(1/varepsilon)}$ for $ell_1$ and to $(mudlog(n)/varepsilon)^{O(1/varepsilon)}$ for logistic regression. Finally, weshow that our sketch can be extended to approximate a regularized version oflogistic regression where the data-dependent regularizer corresponds to thevariance of the individual logistic losses.", "output": "Almost Linear Constant-Factor Sketching for $\\ell_1$ and Logistic Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training has been shown to be reliable in improving robustnessagainst adversarial samples. However, the problem of adversarial training interms of fairness has not yet been properly studied, and the relationshipbetween fairness and accuracy attack still remains unclear. Can wesimultaneously improve robustness w.r.t. both fairness and accuracy? To tacklethis topic, in this paper, we study the problem of adversarial training andadversarial attack w.r.t. both metrics. We propose a unified structure forfairness attack which brings together common notions in group fairness, and wetheoretically prove the equivalence of fairness attack against differentnotions. Moreover, we show the alignment of fairness and accuracy attack, andtheoretically demonstrate that robustness w.r.t. one metric benefits fromrobustness w.r.t. the other metric. Our study suggests a novel way to unifyadversarial training and attack w.r.t. fairness and accuracy, and experimentalresults show that our proposed method achieves better performance in terms ofrobustness w.r.t. both metrics.", "output": "To be Robust and to be Fair: Aligning Fairness with Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper addresses the challenge of efficiently solving the optimal powerflow problem in real-time electricity markets. The proposed solution, namedPhysics-Informed Market-Aware Active Set learning OPF (PIMA-AS-OPF), leveragesphysical constraints and market properties to ensure physical and economicfeasibility of market-clearing outcomes. Specifically, PIMA-AS-OPF employs theactive set learning technique and expands its capabilities to account forcurtailment in load or renewable power generation, which is a common challengein real-world power systems. The core of PIMA-AS-OPF is a fully-connectedneural network that takes the net load and the system topology as input. Theoutputs of this neural network include active constraints such as saturatedgenerators and transmission lines, as well as non-zero load shedding and windcurtailments. These outputs allow for reducing the original market-clearingoptimization to a system of linear equations, which can be solved efficientlyand yield both the dispatch decisions and the locational marginal prices(LMPs). The dispatch decisions and LMPs are then tested for their feasibilitywith respect to the requirements for efficient market-clearing results. Theaccuracy and scalability of the proposed method is tested on a realistic1814-bus NYISO system with current and future renewable energy penetrationlevels.", "output": "A Physics-Informed Machine Learning for Electricity Markets: A NYISO Case Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Incorporating machine learning (ML) components into software products raisesnew software-engineering challenges and exacerbates existing challenges. Manyresearchers have invested significant effort in understanding the challenges ofindustry practitioners working on building products with ML components, throughinterviews and surveys with practitioners. With the intention to aggregate andpresent their collective findings, we conduct a meta-summary study: We collect50 relevant papers that together interacted with over 4758 practitioners usingguidelines for systematic literature reviews. We then collected, grouped, andorganized the over 500 mentions of challenges within those papers. We highlightthe most commonly reported challenges and hope this meta-summary will be auseful resource for the research community to prioritize research and educationin this field.", "output": "A Meta-Summary of Challenges in Building Products with ML Components -- Collecting Experiences from 4758+ Practitioners."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The growing popularity of Machine Learning (ML) has led to its deployment invarious sensitive domains, which has resulted in significant research focusedon ML security and privacy. However, in some applications, such as autonomousdriving, integrity verification of the outsourced ML workload is morecritical-a facet that has not received much attention. Existing solutions, suchas multi-party computation and proof-based systems, impose significantcomputation overhead, which makes them unfit for real-time applications. Wepropose Fides, a novel framework for real-time validation of outsourced MLworkloads. Fides features a novel and efficient distillation technique-GreedyDistillation Transfer Learning-that dynamically distills and fine-tunes a spaceand compute-efficient verification model for verifying the correspondingservice model while running inside a trusted execution environment. Fidesfeatures a client-side attack detection model that uses statistical analysisand divergence measurements to identify, with a high likelihood, if the servicemodel is under attack. Fides also offers a re-classification functionality thatpredicts the original class whenever an attack is identified. We devised agenerative adversarial network framework for training the attack detection andre-classification models. The extensive evaluation shows that Fides achieves anaccuracy of up to 98% for attack detection and 94% for re-classification.", "output": "Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article provides a curated review of selected papers published inprominent economics journals that use machine learning (ML) tools for researchand policy analysis. The review focuses on three key questions: (1) when ML isused in economics, (2) what ML models are commonly preferred, and (3) how theyare used for economic applications. The review highlights that ML isparticularly used in processing nontraditional and unstructured data, capturingstrong nonlinearity, and improving prediction accuracy. Deep learning modelsare suitable for nontraditional data, whereas ensemble learning models arepreferred for traditional datasets. While traditional econometric models maysuffice for analyzing low-complexity data, the increasing complexity ofeconomic data due to rapid digitalization and the growing literature suggestthat ML is becoming an essential addition to the econometrician's toolbox.", "output": "Machine Learning for Economics Research: When What and How?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The expansion in technology and attainability of a large number of sensorshas led to a huge amount of real-time streaming data. The real-time data in theelectrical distribution system is collected through distribution-level phasormeasurement units referred to as $mu$PMU which report high-resolution phasormeasurements comprising various event signatures which provide situationalawareness and enable a level of visibility into the distribution system. Theseevents are infrequent, unschedule, and uncertain; it is a challenge toscrutinize, detect and predict the occurrence of such events. For electricaldistribution systems, it is challenging to explicitly identify evolutionfunctions that describe the complex, non-linear, and non-stationary signaturepatterns of events. In this paper, we seek to address this problem bydeveloping a physics dynamics-based approach to detect anomalies in the$mu$PMU streaming data and simultaneously predict the events using governingequations. We propose a data-driven approach based on the Hankel alternativeview of the Koopman (HAVOK) operator, called DynamoPMU, to analyze theunderlying dynamics of the distribution system by representing them in a linearintrinsic space. The key technical idea is that the proposed method separatesout the linear dynamical behaviour pattern and intermittent forcing (anomalousevents) in sequential data which turns out to be very useful for anomalydetection and simultaneous data prediction. We demonstrate the efficacy of ourproposed framework through analysis of real $mu$PMU data taken from the LBNLdistribution grid. DynamoPMU is suitable for real-time event detection as wellas prediction in an unsupervised way and adapts to varying statistics.", "output": "DynamoPMU: A Physics Informed Anomaly Detection and Prediction Methodology using non-linear dynamics from $\\mu$PMU Measurement Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the complexity of machine learning (ML) models increases and theapplications in different (and critical) domains grow, there is a strong demandfor more interpretable and trustworthy ML. One straightforward andmodel-agnostic way to interpret complex ML models is to train surrogate models,such as rule sets and decision trees, that sufficiently approximate theoriginal ones while being simpler and easier-to-explain. Yet, rule sets canbecome very lengthy, with many if-else statements, and decision tree depthgrows rapidly when accurately emulating complex ML models. In such cases, bothapproaches can fail to meet their core goal, providing users with modelinterpretability. We tackle this by proposing DeforestVis, a visual analyticstool that offers user-friendly summarization of the behavior of complex MLmodels by providing surrogate decision stumps (one-level decision trees)generated with the adaptive boosting (AdaBoost) technique. Our solution helpsusers to explore the complexity vs fidelity trade-off by incrementallygenerating more stumps, creating attribute-based explanations with weightedstumps to justify decision making, and analyzing the impact of rule overridingon training instance allocation between one or more stumps. An independent testset allows users to monitor the effectiveness of manual rule changes and formhypotheses based on case-by-case investigations. We show the applicability andusefulness of DeforestVis with two use cases and expert interviews with dataanalysts and model developers.", "output": "DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent developments in Machine Learning approaches for modelling physicalsystems have begun to mirror the past development of numerical methods in thecomputational sciences. In this survey, we begin by providing an example ofthis with the parallels between the development trajectories of graph neuralnetwork acceleration for physical simulations and particle-based approaches. Wethen give an overview of simulation approaches, which have not yet found theirway into state-of-the-art Machine Learning methods and hold the potential tomake Machine Learning approaches more accurate and more efficient. We concludeby presenting an outlook on the potential of these approaches for makingMachine Learning models for science more efficient.", "output": "On the Relationships between Graph Neural Networks for the Simulation of Physical Systems and Classical Numerical Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We contribute to the vastly growing field of machine learning for engineeringsystems by demonstrating that equivariant graph neural networks have thepotential to learn more accurate dynamic-interaction models than theirnon-equivariant counterparts. We benchmark two well-studied fluid flow systems,namely the 3D decaying Taylor-Green vortex and the 3D reverse Poiseuille flow,and compare equivariant graph neural networks to their non-equivariantcounterparts on different performance measures, such as kinetic energy orSinkhorn distance. Such measures are typically used in engineering to validatenumerical solvers. Our main findings are that while being rather slow to trainand evaluate, equivariant models learn more physically accurate interactions.This indicates opportunities for future work towards coarse-grained models forturbulent flows, and generalization across system dynamics and parameters.", "output": "E($3$) Equivariant Graph Neural Networks for Particle-Based Fluid Mechanics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider online reinforcement learning in episodic Markov decision process(MDP) with an unknown transition matrix and stochastic rewards drawn from afixed but unknown distribution. The learner aims to learn the optimal policyand minimize their regret over a finite time horizon through interacting withthe environment. We devise a simple and efficient model-based algorithm thatachieves $tilde{O}(LXsqrt{TA})$ regret with high probability, where $L$ isthe episode length, $T$ is the number of episodes, and $X$ and $A$ are thecardinalities of the state space and the action space, respectively. Theproposed algorithm, which is based on the concept of \"optimism in the face ofuncertainty\", maintains confidence sets of transition and reward functions anduses occupancy measures to connect the online MDP with linear programming. Itachieves a tighter regret bound compared to the existing works that use asimilar confidence sets framework and improves the computational effortcompared to those that use a different framework but with a slightly tighterregret bound.", "output": "Online Reinforcement Learning in Markov Decision Process Using Linear Programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Markov games model interactions among multiple players in a stochastic,dynamic environment. Each player in a Markov game maximizes its expected totaldiscounted reward, which depends upon the policies of the other players. Weformulate a class of Markov games, termed affine Markov games, where an affinereward function couples the players' actions. We introduce a novel solutionconcept, the soft-Bellman equilibrium, where each player is boundedly rationaland chooses a soft-Bellman policy rather than a purely rational policy as inthe well-known Nash equilibrium concept. We provide conditions for theexistence and uniqueness of the soft-Bellman equilibrium and propose anonlinear least squares algorithm to compute such an equilibrium in the forwardproblem. We then solve the inverse game problem of inferring the players'reward parameters from observed state-action trajectories via a projectedgradient algorithm. Experiments in a predator-prey OpenAI Gym environment showthat the reward parameters inferred by the proposed algorithm outperform thoseinferred by a baseline algorithm: they reduce the Kullback-Leibler divergencebetween the equilibrium policies and observed policies by at least two ordersof magnitude.", "output": "Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep generative models have the capacity to render high fidelity images ofcontent like human faces. Recently, there has been substantial progress inconditionally generating images with specific quantitative attributes, like theemotion conveyed by one's face. These methods typically require a user toexplicitly quantify the desired intensity of a visual attribute. A limitationof this method is that many attributes, like how \"angry\" a human face looks,are difficult for a user to precisely quantify. However, a user would be ableto reliably say which of two faces seems \"angrier\". Following this premise, wedevelop the $textit{PrefGen}$ system, which allows users to control therelative attributes of generated images by presenting them with simple pairedcomparison queries of the form \"do you prefer image $a$ or image $b$?\" Usinginformation from a sequence of query responses, we can estimate userpreferences over a set of image attributes and perform preference-guided imageediting and generation. Furthermore, to make preference localization feasibleand efficient, we apply an active query selection strategy. We demonstrate thesuccess of this approach using a StyleGAN2 generator on the task of human faceediting. Additionally, we demonstrate how our approach can be combined withCLIP, allowing a user to edit the relative intensity of attributes specified bytext prompts. Code at ", "output": "PrefGen: Preference Guided Image Generation with Relative Attributes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traffic congestion has been a major challenge in many urban road networks.Extensive research studies have been conducted to highlight traffic-relatedcongestion and address the issue using data-driven approaches. Currently, mosttraffic congestion analyses are done using simulation software that offerslimited insight due to the limitations in the tools and utilities being used torender various traffic congestion scenarios. All that impacts the formulationof custom business problems which vary from place to place and country tocountry. By exploiting the power of the knowledge graph, we model a trafficcongestion problem into the Neo4j graph and then use the load balancing,optimization algorithm to identify congestion-free road networks. We also showhow traffic propagates backward in case of congestion or accident scenarios andits overall impact on other segments of the roads. We also train a sequentialRNN-LSTM (Long Short-Term Memory) deep learning model on the real-time trafficdata to assess the accuracy of simulation results based on a road-specificcongestion. Our results show that graph-based traffic simulation, supplementedby AI ML-based traffic prediction can be more effective in estimating thecongestion level in a road network.", "output": "Leveraging Neo4j and deep learning for traffic congestion simulation & optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider perception-based control using state estimates that are obtainedfrom high-dimensional sensor measurements via learning-enabled perception maps.However, these perception maps are not perfect and result in state estimationerrors that can lead to unsafe system behavior. Stochastic sensor noise canmake matters worse and result in estimation errors that follow unknowndistributions. We propose a perception-based control framework that i)quantifies estimation uncertainty of perception maps, and ii) integrates theseuncertainty representations into the control design. To do so, we use conformalprediction to compute valid state estimation regions, which are sets thatcontain the unknown state with high probability. We then devise a sampled-datacontroller for continuous-time systems based on the notion of measurementrobust control barrier functions. Our controller uses idea from self-triggeredcontrol and enables us to avoid using stochastic calculus. Our framework isagnostic to the choice of the perception map, independent of the noisedistribution, and to the best of our knowledge the first to provideprobabilistic safety guarantees in such a setting. We demonstrate theeffectiveness of our proposed perception-based controller for a LiDAR-enabledF1/10th car.", "output": "Safe Perception-Based Control under Stochastic Sensor Uncertainty using Conformal Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A framework is proposed that casts relational learning in terms oftransformers, implementing binding between sensory states and abstract stateswith relational cross attention mechanisms.", "output": "Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores the problem of training a recurrent neural network fromnoisy data. While neural network based dynamic predictors perform well withnoise-free training data, prediction with noisy inputs during training phaseposes a significant challenge. Here a sequential training algorithm isdeveloped for an echo-state network (ESN) by incorporating noisy observationsusing an ensemble Kalman filter. The resultant Kalman-trained echo-statenetwork (KalT-ESN) outperforms the traditionally trained ESN with least squarealgorithm while still being computationally cheap. The proposed method isdemonstrated on noisy observations from three systems: two synthetic datasetsfrom chaotic dynamical systems and a set of real-time traffic data.", "output": "Sequential Learning from Noisy Data: Data-Assimilation Meets Echo-State Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we investigate applications of no-collision transportation mapsintroduced in [Nurbekyan et. al., 2020] in manifold learning for image data.Recently, there has been a surge in applying transportation-based distances andfeatures for data representing motion-like or deformation-like phenomena.Indeed, comparing intensities at fixed locations often does not reveal the datastructure. No-collision maps and distances developed in [Nurbekyan et. al.,2020] are sensitive to geometric features similar to optimal transportation(OT) maps but much cheaper to compute due to the absence of optimization. Inthis work, we prove that no-collision distances provide an isometry betweentranslations (respectively dilations) of a single probability measure and thetranslation (respectively dilation) vectors equipped with a Euclidean distance.Furthermore, we prove that no-collision transportation maps, as well as OT andlinearized OT maps, do not in general provide an isometry for rotations. Thenumerical experiments confirm our theoretical findings and show thatno-collision distances achieve similar or better performance on severalmanifold learning tasks compared to other OT and Euclidean-based methods at afraction of a computational cost.", "output": "Applications of No-Collision Transportation Maps in Manifold Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel diffusion map particle system (DMPS) for generativemodeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradientdescent (LAWGD). Diffusion maps are used to approximate the generator of theLangevin diffusion process from samples, and hence to learn the underlyingdata-generating manifold. On the other hand, LAWGD enables efficient samplingfrom the target distribution given a suitable choice of kernel, which weconstruct here via a spectral approximation of the generator, computed withdiffusion maps. Numerical experiments show that our method outperforms otherson synthetic datasets, including examples with manifold structure.", "output": "Diffusion map particle systems for generative modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fast adversarial training (FAT) is an efficient method to improve robustness.However, the original FAT suffers from catastrophic overfitting, whichdramatically and suddenly reduces robustness after a few training epochs.Although various FAT variants have been proposed to prevent overfitting, theyrequire high training costs. In this paper, we investigate the relationshipbetween adversarial example quality and catastrophic overfitting by comparingthe training processes of standard adversarial training and FAT. We find thatcatastrophic overfitting occurs when the attack success rate of adversarialexamples becomes worse. Based on this observation, we propose a positiveprior-guided adversarial initialization to prevent overfitting by improvingadversarial example quality without extra training costs. This initializationis generated by using high-quality adversarial perturbations from thehistorical training process. We provide theoretical analysis for the proposedinitialization and propose a prior-guided regularization method that boosts thesmoothness of the loss function. Additionally, we design a prior-guidedensemble FAT method that averages the different model weights of historicalmodels using different decay rates. Our proposed method, called FGSM-PGK,assembles the prior-guided knowledge, i.e., the prior-guided initialization andmodel weights, acquired during the historical training process. Evaluations offour datasets demonstrate the superiority of the proposed method.", "output": "Improving Fast Adversarial Training with Prior-Guided Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world medical image segmentation has tremendous long-tailed complexityof objects, among which tail conditions correlate with relatively rare diseasesand are clinically significant. A trustworthy medical AI algorithm shoulddemonstrate its effectiveness on tail conditions to avoid clinically dangerousdamage in these out-of-distribution (OOD) cases. In this paper, we adopt theconcept of object queries in Mask Transformers to formulate semanticsegmentation as a soft cluster assignment. The queries fit the feature-levelcluster centers of inliers during training. Therefore, when performinginference on a medical image in real-world scenarios, the similarity betweenpixels and the queries detects and localizes OOD regions. We term this OODlocalization as MaxQuery. Furthermore, the foregrounds of real-world medicalimages, whether OOD objects or inliers, are lesions. The difference betweenthem is less than that between the foreground and background, possiblymisleading the object queries to focus redundantly on the background. Thus, wepropose a query-distribution (QD) loss to enforce clear boundaries betweensegmentation targets and other regions at the query level, improving the inliersegmentation and OOD indication. Our proposed framework is tested on tworeal-world segmentation tasks, i.e., segmentation of pancreatic and livertumors, outperforming previous state-of-the-art algorithms by an average of7.39% on AUROC, 14.69% on AUPR, and 13.79% on FPR95 for OOD localization. Onthe other hand, our framework improves the performance of inlier segmentationby an average of 5.27% DSC when compared with the leading baseline nnUNet.", "output": "Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Relation prediction on knowledge graphs (KGs) is a key research topic.Dominant embedding-based methods mainly focus on the transductive setting andlack the inductive ability to generalize to new entities for inference.Existing methods for inductive reasoning mostly mine the connections betweenentities, i.e., relational paths, without considering the nature of head andtail entities contained in the relational context. This paper proposes a novelmethod that captures both connections between entities and the intrinsic natureof entities, by simultaneously aggregating RElational Paths and cOntext with aunified hieRarchical Transformer framework, namely REPORT. REPORT relies solelyon relation semantics and can naturally generalize to the fully-inductivesetting, where KGs for training and inference have no common entities. In theexperiments, REPORT performs consistently better than all baselines on almostall the eight version subsets of two fully-inductive datasets. Moreover. REPORTis interpretable by providing each element's contribution to the predictionresults.", "output": "Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Analyzing high resolution whole slide images (WSIs) with regard toinformation across multiple scales poses a significant challenge in digitalpathology. Multi-instance learning (MIL) is a common solution for working withhigh resolution images by classifying bags of objects (i.e. sets of smallerimage patches). However, such processing is typically performed at a singlescale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scaleinformation that is key to diagnoses by human pathologists. In this study, wepropose a novel cross-scale MIL algorithm to explicitly aggregate inter-scalerelationships into a single MIL network for pathological image diagnosis. Thecontribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL)algorithm that integrates the multi-scale information and the inter-scalerelationships is proposed; (2) A toy dataset with scale-specific morphologicalfeatures is created and released to examine and visualize differentialcross-scale attention; (3) Superior performance on both in-house and publicdatasets is demonstrated by our simple cross-scale MIL strategy. The officialimplementation is publicly available at ", "output": "Cross-scale Multi-instance Learning for Pathological Image Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning generally suffers from enormous computational resources andtime-consuming training processes. Broad Learning System (BLS) and itsconvolutional variants have been proposed to mitigate these issues and haveachieved superb performance in image classification. However, the existingconvolutional-based broad learning system (C-BLS) either lacks an efficienttraining method and incremental learning capability or suffers from poorperformance. To this end, we propose a convolutional broad learning system(ConvBLS) based on the spherical K-means (SKM) algorithm and two-stagemulti-scale (TSMS) feature fusion, which consists of the convolutional feature(CF) layer, convolutional enhancement (CE) layer, TSMS feature fusion layer,and output layer. First, unlike the current C-BLS, the simple yet efficient SKMalgorithm is utilized to learn the weights of CF layers. Compared with randomfilters, the SKM algorithm makes the CF layer learn more comprehensive spatialfeatures. Second, similar to the vanilla BLS, CE layers are established toexpand the feature space. Third, the TSMS feature fusion layer is proposed toextract more effective multi-scale features through the integration of CFlayers and CE layers. Thanks to the above design and the pseudo-inversecalculation of the output layer weights, our proposed ConvBLS method isunprecedentedly efficient and effective. Finally, the corresponding incrementallearning algorithms are presented for rapid remodeling if the model deems toexpand. Experiments and comparisons demonstrate the superiority of our method.", "output": "ConvBLS: An Effective and Efficient Incremental Convolutional Broad Learning System for Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of learning in a non-stationary reinforcementlearning (RL) environment, where the setting can be fully described by apiecewise stationary discrete-time Markov decision process (MDP). We introducea variant of the Restarted Bayesian Online Change-Point Detection algorithm(R-BOCPD) that operates on input streams originating from the more generalmultinomial distribution and provides near-optimal theoretical guarantees interms of false-alarm rate and detection delay. Based on this, we propose animproved version of the UCRL2 algorithm for MDPs with state transition kernelsampled from a multinomial distribution, which we call R-BOCPD-UCRL2. Weperform a finite-time performance analysis and show that R-BOCPD-UCRL2 enjoys afavorable regret bound of $Oleft(D O sqrt{A T K_T logleft (frac{T}{delta}right) + frac{K_T log frac{K_T}{delta}}{minlimits_ell :mathbf{KL}left({mathbf{theta}^{(ell+1)}}midmid{mathbf{theta}^{(ell)}}right)}}right)$,where $D$ is the largest MDP diameter from the set of MDPs defining thepiecewise stationary MDP setting, $O$ is the finite number of states (constantover all changes), $A$ is the finite number of actions (constant over allchanges), $K_T$ is the number of change points up to horizon $T$, and$mathbf{theta}^{(ell)}$ is the transition kernel during the interval$[c_ell, c_{ell+1})$, which we assume to be multinomially distributed overthe set of states $mathbb{O}$. Interestingly, the performance bound does notdirectly scale with the variation in MDP state transition distributions andrewards, ie. can also model abrupt changes. In practice, R-BOCPD-UCRL2outperforms the state-of-the-art in a variety of scenarios in syntheticenvironments. We provide a detailed experimental setup along with a coderepository (upon publication) that can be used to easily reproduce ourexperiments.", "output": "Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stroke is the second leading cause of death worldwide. Machine learningclassification algorithms have been widely adopted for stroke prediction.However, these algorithms were evaluated using different datasets andevaluation metrics. Moreover, there is no comprehensive framework for strokedata analytics. This paper proposes an intelligent stroke prediction frameworkbased on a critical examination of machine learning prediction algorithms inthe literature. The five most used machine learning algorithms for strokeprediction are evaluated using a unified setup for objective comparison.Comparative analysis and numerical results reveal that the Random Forestalgorithm is best suited for stroke prediction.", "output": "From Conception to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and Performance Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A backdoor attack allows a malicious user to manipulate the environment orcorrupt the training data, thus inserting a backdoor into the trained agent.Such attacks compromise the RL system's reliability, leading to potentiallycatastrophic results in various key fields. In contrast, relatively limitedresearch has investigated effective defenses against backdoor attacks in RL.This paper proposes the Recovery Triggered States (RTS) method, a novelapproach that effectively protects the victim agents from backdoor attacks. RTSinvolves building a surrogate network to approximate the dynamics model.Developers can then recover the environment from the triggered state to a cleanstate, thereby preventing attackers from activating backdoors hidden in theagent by presenting the trigger. When training the surrogate to predict states,we incorporate agent action information to reduce the discrepancy between theactions taken by the agent on predicted states and the actions taken on realstates. RTS is the first approach to defend against backdoor attacks in asingle-agent setting. Our results show that using RTS, the cumulative rewardonly decreased by 1.41% under the backdoor attack.", "output": "Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent developments in hardware and information technology have enabled theemergence of billions of connected, intelligent devices around the worldexchanging information with minimal human involvement. This paradigm, known asthe Internet of Things (IoT) is progressing quickly with an estimated 27billion devices by 2025. This growth in the number of IoT devices andsuccessful IoT services has generated a tremendous amount of data. However,this humongous volume of data poses growing concerns for user privacy. Thisintroductory chapter has presented a brief survey of some of the existing dataprivacy-preservation schemes proposed by researchers in the field of theInternet of Things.", "output": "Data Privacy Preservation on the Internet of Things."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A machine learning approach for improving monitoring in passive opticalnetworks with almost equidistant branches is proposed and experimentallyvalidated. It achieves a high diagnostic accuracy of 98.7% and an eventlocalization error of 0.5m", "output": "Branch Identification in Passive Optical Networks using Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Representation learning is an important step in the machine learningpipeline. Given the current biological sequencing data volume, learning anexplicit representation is prohibitive due to the dimensionality of theresulting feature vectors. Kernel-based methods, e.g., SVM, are a provenefficient and useful alternative for several machine learning (ML) tasks suchas sequence classification. Three challenges with kernel methods are (i) thecomputation time, (ii) the memory usage (storing an $ntimes n$ matrix), and(iii) the usage of kernel matrices limited to kernel-based ML methods(difficult to generalize on non-kernel classifiers). While (i) can be solvedusing approximate methods, challenge (ii) remains for typical kernel methods.Similarly, although non-kernel-based ML methods can be applied to kernelmatrices by extracting principal components (kernel PCA), it may result ininformation loss, while being computationally expensive. In this paper, wepropose a general-purpose representation learning approach that embodies kernelmethods' qualities while avoiding computation, memory, and generalizabilitychallenges. This involves computing a low-dimensional embedding of eachsequence, using random projections of its $k$-mer frequency vectors,significantly reducing the computation needed to compute the dot product andthe memory needed to store the resulting representation. Our proposed fast andalignment-free embedding method can be used as input to any distance (e.g., $k$nearest neighbors) and non-distance (e.g., decision tree) based ML method forclassification and clustering tasks. Using different forms of biologicalsequences as input, we perform a variety of real-world classification tasks,such as SARS-CoV-2 lineage and gene family classification, outperformingseveral state-of-the-art embedding and kernel methods in predictiveperformance.", "output": "BioSequence2Vec: Efficient Embedding Generation For Biological Sequences."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose {it Fair-CDA}, a fine-grained data augmentationstrategy for imposing fairness constraints. We use a feature disentanglementmethod to extract the features highly related to the sensitive attributes. Thenwe show that group fairness can be achieved by regularizing the models ontransition paths of sensitive features between groups. By adjusting theperturbation strength in the direction of the paths, our proposed augmentationis controllable and auditable. To alleviate the accuracy degradation caused byfairness constraints, we further introduce a calibrated model to impute labelsfor the augmented data. Our proposed method does not assume any data generativemodel and ensures good generalization for both accuracy and fairness.Experimental results show that Fair-CDA consistently outperformsstate-of-the-art methods on widely-used benchmarks, e.g., Adult, CelebA andMovieLens. Especially, Fair-CDA obtains an 86.3% relative improvement forfairness while maintaining the accuracy on the Adult dataset. Moreover, weevaluate Fair-CDA in an online recommendation system to demonstrate theeffectiveness of our method in terms of accuracy and fairness.", "output": "Fair-CDA: Continuous and Directional Augmentation for Group Fairness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As an intrinsic and fundamental property of big data, data heterogeneityexists in a variety of real-world applications, such as precision medicine,autonomous driving, financial applications, etc. For machine learningalgorithms, the ignorance of data heterogeneity will greatly hurt thegeneralization performance and the algorithmic fairness, since the predictionmechanisms among different sub-populations are likely to differ from eachother. In this work, we focus on the data heterogeneity that affects theprediction of machine learning models, and firstly propose the emph{usablepredictive heterogeneity}, which takes into account the model capacity andcomputational constraints. We prove that it can be reliably estimated fromfinite data with probably approximately correct (PAC) bounds. Additionally, wedesign a bi-level optimization algorithm to explore the usable predictiveheterogeneity from data. Empirically, the explored heterogeneity providesinsights for sub-population divisions in income prediction, crop yieldprediction and image classification tasks, and leveraging such heterogeneitybenefits the out-of-distribution generalization performance.", "output": "Predictive Heterogeneity: Measures and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The healthcare sector is an important pillar of every community, numerousresearch studies have been carried out in this context to optimize medicalprocesses and improve care quality and facilitate patient management. In thisarticle we have analyzed and examined different works concerning theexploitation of the most recent technologies such as big data, artificialintelligence, machine learning, and deep learning for the improvement of healthcare, which enabled us to propose our general approach concentrating on thecollection, preprocessing and clustering of medical data to facilitate access,after analysis, to the patients and health professionals to predict the mostfrequent pathologies with better precision within a notable timeframe.keywords: Healthcare, big data, artificial intelligence, automatic languageprocessing, data mining, predictive models.", "output": "Medical Pathologies Prediction : Systematic Review and Proposed Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Random label noises (or observational noises) widely exist in practicalmachine learning settings. While previous studies primarily focus on theaffects of label noises to the performance of learning, our work intends toinvestigate the implicit regularization effects of the label noises, undermini-batch sampling settings of stochastic gradient descent (SGD), withassumptions that label noises are unbiased. Specifically, we analyze thelearning dynamics of SGD over the quadratic loss with unbiased label noises,where we model the dynamics of SGD as a stochastic differentiable equation(SDE) with two diffusion terms (namely a Doubly Stochastic Model). While thefirst diffusion term is caused by mini-batch sampling over the(label-noiseless) loss gradients as many other works on SGD, our modelinvestigates the second noise term of SGD dynamics, which is caused bymini-batch sampling over the label noises, as an implicit regularizer. Ourtheoretical analysis finds such implicit regularizer would favor someconvergence points that could stabilize model outputs against perturbation ofparameters (namely inference stability). Though similar phenomenon have beeninvestigated, our work doesn't assume SGD as an Ornstein-Uhlenbeck like processand achieve a more generalizable result with convergence of approximationproved. To validate our analysis, we design two sets of empirical studies toanalyze the implicit regularizer of SGD with unbiased random label noises fordeep neural networks training and linear regression.", "output": "Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven modeling approaches can produce fast surrogates to studylarge-scale physics problems. Among them, graph neural networks (GNNs) thatoperate on mesh-based data are desirable because they possess inductive biasesthat promote physical faithfulness, but hardware limitations have precludedtheir application to large computational domains. We show that it istextit{possible} to train a class of GNN surrogates on 3D meshes. We scaleMeshGraphNets (MGN), a subclass of GNNs for mesh-based physics modeling, viaour domain decomposition approach to facilitate training that is mathematicallyequivalent to training on the whole domain under certain conditions. With this,we were able to train MGN on meshes with textit{millions} of nodes to generatecomputational fluid dynamics (CFD) simulations. Furthermore, we show how toenhance MGN via higher-order numerical integration, which can reduce MGN'serror and training time. We validated our methods on an accompanying dataset of3D $text{CO}_2$-capture CFD simulations on a 3.1M-node mesh. This workpresents a practical path to scaling MGN for real-world applications.", "output": "Scientific Computing Algorithms to Learn Enhanced Scalable Surrogates for Mesh Physics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline meta reinforcement learning (OMRL) aims to learn transferrableknowledge from offline datasets to facilitate the learning process for newtarget tasks. Context-based RL employs a context encoder to rapidly adapt theagent to new tasks by inferring about the task representation, and thenadjusting the acting policy based on the inferred task representation. Here weconsider context-based OMRL, in particular, the issue of task representationlearning for OMRL. We empirically demonstrate that the context encoder trainedon offline datasets could suffer from distribution shift between the contextsused for training and testing. To tackle this issue, we propose a hard samplingbased strategy for learning a robust task context encoder. Experimentalresults, based on distinct continuous control tasks, demonstrate that theutilization of our technique results in more robust task representations andbetter testing performance in terms of accumulated returns, compared withbaseline methods. Our code is available at", "output": "On Context Distribution Shift in Task Representation Learning for Offline Meta RL."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncovering potential failure cases is a crucial step in the validation ofsafety critical systems such as autonomous vehicles. Failure search may be donethrough logging substantial vehicle miles in either simulation or real worldtesting. Due to the sparsity of failure events, naive random search approachesrequire significant amounts of vehicle operation hours to find potential systemweaknesses. As a result, adaptive searching techniques have been proposed toefficiently explore and uncover failure trajectories of an autonomous policy insimulation. Adaptive Stress Testing (AST) is one such method that poses theproblem of failure search as a Markov decision process and uses reinforcementlearning techniques to find high probability failures. However, thisformulation requires a probability model for the actions of all agents in theenvironment. In systems where the environment actions are discrete anddependencies among agents exist, it may be infeasible to fully characterize thedistribution or find a suitable proxy. This work proposes the use of a datadriven approach to learn a suitable classifier that tries to model how humansidentify {critical states and use this to guide failure search in AST. We showthat the incorporation of critical states into the AST framework generatesfailure scenarios with increased safety violations in an autonomous drivingpolicy with a discrete action space.", "output": "Adaptive Failure Search Using Critical States from Domain Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a new approach to simulate forward and inverse problemsof moving loads using physics-informed machine learning (PIML).Physics-informed neural networks (PINNs) utilize the underlying physics ofmoving load problems and aim to predict the deflection of beams and themagnitude of the loads. The mathematical representation of the moving loadconsidered in this work involves a Dirac delta function, to capture the effectof the load moving across the structure. Approximating the Dirac delta functionwith PINNs is challenging because of its instantaneous change of output at asingle point, causing difficulty in the convergence of the loss function. Wepropose to approximate the Dirac delta function with a Gaussian function. Theincorporated Gaussian function physical equations are used in thephysics-informed neural architecture to simulate beam deflections and topredict the magnitude of the load. Numerical results show that PIML is aneffective method for simulating the forward and inverse problems for theconsidered model of a moving load.", "output": "Physics-informed machine learning for moving load problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In computing, the aim of personalization is to train a model that caters to aspecific individual or group of people by optimizing one or more performancemetrics and adhering to specific constraints. In this paper, we discuss theneed for personalization in affective and personality computing (hereinafterreferred to as affective computing). We present a survey of state-of-the-artapproaches for personalization in affective computing. Our review spanstraining techniques and objectives towards the personalization of affectivecomputing models. We group existing approaches into seven categories: (1)Target-specific Models, (2) Group-specific Models, (3) Weighting-basedApproaches, (4) Fine-tuning Approaches, (5) Multitask Learning, (6)Generative-based Models, and (7) Feature Augmentation. Additionally, we providea statistical meta-analysis of the surveyed literature, analyzing theprevalence of different affective computing tasks, interaction modes,interaction contexts, and the level of personalization among the surveyedworks. Based on that, we provide a road-map for those who are interested inexploring this direction.", "output": "A Survey on Personalized Affective Computing in Human-Machine Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The cascade of 2D geometric transformations were exploited to model relationsbetween entities in a knowledge graph (KG), leading to an effective KGembedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space wasproposed as a new KGE model, Rotate3D, by leveraging its non-commutativeproperty. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometrictransformations, including translation, rotation, scaling, reflection, andshear and propose a family of KGE models, named CompoundE3D, in this work.CompoundE3D allows multiple design variants to match rich underlyingcharacteristics of a KG. Since each variant has its own advantages on a subsetof relations, an ensemble of multiple variants can yield superior performance.The effectiveness and flexibility of CompoundE3D are experimentally verified onfour popular link prediction datasets.", "output": "Knowledge Graph Embedding with 3D Compound Geometric Transformations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated Program Repair (APR) aims to automatically generate patches forbuggy programs. Recent APR work has been focused on leveraging modern LargeLanguage Models (LLMs) to directly generate patches for APR. Such LLM-based APRtools work by first constructing an input prompt built using the original buggycode and then queries the LLM to generate patches. While the LLM-based APRtools are able to achieve state-of-the-art results, it still follows theclassic Generate and Validate repair paradigm of first generating lots ofpatches and then validating each one afterwards. This not only leads to manyrepeated patches that are incorrect but also miss the crucial information intest failures as well as in plausible patches.To address these limitations, we propose ChatRepair, the first fullyautomated conversation-driven APR approach that interleaves patch generationwith instant feedback to perform APR in a conversational style. ChatRepairfirst feeds the LLM with relevant test failure information to start with, andthen learns from both failures and successes of earlier patching attempts ofthe same bug for more powerful APR. For earlier patches that failed to pass alltests, we combine the incorrect patches with their corresponding relevant testfailure information to construct a new prompt for the LLM to generate the nextpatch. In this way, we can avoid making the same mistakes. For earlier patchesthat passed all the tests, we further ask the LLM to generate alternativevariations of the original plausible patches. In this way, we can further buildon and learn from earlier successes to generate more plausible patches toincrease the chance of having correct patches. While our approach is general,we implement ChatRepair using state-of-the-art dialogue-based LLM -- ChatGPT.By calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugsfor $0.42 each!", "output": "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We combine concepts from multilevel solvers for partial differentialequations (PDEs) with neural network based deep learning and propose a newmethodology for the efficient numerical solution of high-dimensional parametricPDEs. An in-depth theoretical analysis shows that the proposed architecture isable to approximate multigrid V-cycles to arbitrary precision with the numberof weights only depending logarithmically on the resolution of the finest mesh.As a consequence, approximation bounds for the solution of parametric PDEs byneural networks that are independent on the (stochastic) parameter dimensioncan be derived. The performance of the proposed method is illustrated onhigh-dimensional parametric linear elliptic PDEs that are common benchmarkproblems in uncertainty quantification. We find substantial improvements overstate-of-the-art deep learning-based solvers. As particularly challengingexamples, random conductivity with high-dimensional non-affine Gaussian fieldsin 100 parameter dimensions and a random cookie problem are examined. Due tothe multilevel structure of our method, the amount of training samples can bereduced on finer levels, hence significantly lowering the generation time fortraining data and the training time of our method.", "output": "Multilevel CNNs for Parametric PDEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive learning is an efficient approach to self-supervisedrepresentation learning. Although recent studies have made progress in thetheoretical understanding of contrastive learning, the investigation of how tocharacterize the clusters of the learned representations is still limited. Inthis paper, we aim to elucidate the characterization from theoreticalperspectives. To this end, we consider a kernel-based contrastive learningframework termed Kernel Contrastive Learning (KCL), where kernel functions playan important role when applying our theoretical results to other frameworks. Weintroduce a formulation of the similarity structure of learned representationsby utilizing a statistical dependency viewpoint. We investigate the theoreticalproperties of the kernel-based contrastive loss via this formulation. We firstprove that the formulation characterizes the structure of representationslearned with the kernel-based contrastive learning framework. We show a newupper bound of the classification error of a downstream task, which explainsthat our theory is consistent with the empirical success of contrastivelearning. We also establish a generalization error bound of KCL. Finally, weshow a guarantee for the generalization ability of KCL to the downstreamclassification task via a surrogate bound.", "output": "Towards Understanding the Mechanism of Contrastive Learning via Similarity Structure: A Theoretical Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Serverless cloud is an innovative cloud service model that frees customersfrom most cloud management duties. It also offers the same advantages as othercloud models but at much lower costs. As a result, the serverless cloud hasbeen increasingly employed in high-impact areas such as system security,banking, and health care. A big threat to the serverless cloud's performance iscold-start, which is when the time of provisioning the needed cloud resource toserve customers' requests incurs unacceptable costs to the service providersand/or the customers. This paper proposes a novel low-coupling, high-cohesionensemble policy that addresses the cold-start problem at infrastructure- andfunction-levels of the serverless cloud stack, while the state of the artpolicies have a more narrowed focus. This ensemble policy anchors on theprediction of function instance arrivals, 10 to 15 minutes into the future. Itis achievable by using the temporal convolutional network (TCN) deep-learningmethod. Bench-marking results on a real-world dataset from a large-scaleserverless cloud provider show that TCN out-performs other popular machinelearning algorithms for time series. Going beyond cold-start management, theproposed policy and publicly available codes can be adopted in solving othercloud problems such as optimizing the provisioning of virtual software-definednetwork assets.", "output": "Managing Cold-start in The Serverless Cloud with Temporal Convolutional Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Highway merging scenarios featuring mixed traffic conditions pose significantmodeling and control challenges for connected and automated vehicles (CAVs)interacting with incoming on-ramp human-driven vehicles (HDVs). In this paper,we present an approach to learn an approximate information state model ofCAV-HDV interactions for a CAV to maneuver safely during highway merging. Inour approach, the CAV learns the behavior of an incoming HDV using approximateinformation states before generating a control strategy to facilitate merging.First, we validate the efficacy of this framework on real-world data by usingit to predict the behavior of an HDV in mixed traffic situations extracted fromthe Next-Generation Simulation repository. Then, we generate simulation datafor HDV-CAV interactions in a highway merging scenario using a standard inversereinforcement learning approach. Without assuming a prior knowledge of thegenerating model, we show that our approximate information state model learnsto predict the future trajectory of the HDV using only observations.Subsequently, we generate safe control policies for a CAV while merging withHDVs, demonstrating a spectrum of driving behaviors, from aggressive toconservative. We demonstrate the effectiveness of the proposed approach byperforming numerical simulations.", "output": "Connected and Automated Vehicles in Mixed-Traffic: Learning Human Driver Behavior for Effective On-Ramp Merging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose and release a new vulnerable source code dataset. We curate thedataset by crawling security issue websites, extracting vulnerability-fixingcommits and source codes from the corresponding projects. Our new datasetcontains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerablefunctions extracted from 7,861 commits. Our dataset covers 305 more projectsthan all previous datasets combined. We show that increasing the diversity andvolume of training data improves the performance of deep learning models forvulnerability detection.Combining our new dataset with previous datasets, we present an analysis ofthe challenges and promising research directions of using deep learning fordetecting software vulnerabilities. We study 11 model architectures belongingto 4 families. Our results show that deep learning is still not ready forvulnerability detection, due to high false positive rate, low F1 score, anddifficulty of detecting hard CWEs. In particular, we demonstrate an importantgeneralization challenge for the deployment of deep learning-based models.However, we also identify hopeful future research directions. We demonstratethat large language models (LLMs) are the future for vulnerability detection,outperforming Graph Neural Networks (GNNs) with manual feature engineering.Moreover, developing source code specific pre-training objectives is apromising research direction to improve the vulnerability detectionperformance.", "output": "DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in large language models (LLMs) have led to the developmentof powerful AI chatbots capable of engaging in natural and human-likeconversations. However, these chatbots can be potentially harmful, exhibitingmanipulative, gaslighting, and narcissistic behaviors. We define Healthy AI tobe safe, trustworthy and ethical. To create healthy AI systems, we present theSafeguardGPT framework that uses psychotherapy to correct for these harmfulbehaviors in AI chatbots. The framework involves four types of AI agents: aChatbot, a \"User,\" a \"Therapist,\" and a \"Critic.\" We demonstrate theeffectiveness of SafeguardGPT through a working example of simulating a socialconversation. Our results show that the framework can improve the quality ofconversations between AI chatbots and humans. Although there are still severalchallenges and directions to be addressed in the future, SafeguardGPT providesa promising approach to improving the alignment between AI chatbots and humanvalues. By incorporating psychotherapy and reinforcement learning techniques,the framework enables AI chatbots to learn and adapt to human preferences andvalues in a safe and ethical way, contributing to the development of a morehuman-centric and responsible AI.", "output": "Towards Healthy AI: Large Language Models Need Therapists Too."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We answer the question: \"Does local progress (on batches) imply globalprogress (on the entire dataset) for mini-batch $k$-means?\". Specifically, weconsider mini-batch $k$-means which terminates only when the improvement in thequality of the clustering on the sampled batch is below some threshold.Although at first glance it appears that this algorithm might executeforever, we answer the above question in the affirmative and show that if thebatch is of size $tilde{Omega}((d/epsilon)^2)$, it must terminate within$O(d/epsilon)$ iterations with high probability, where $d$ is the dimension ofthe input, and $epsilon$ is a threshold parameter for termination. This istrue regardless of how the centers are initialized. When the algorithm isinitialized with the $k$-means++ initialization scheme, it achieves anapproximation ratio of $O(log k)$ (the same as the full-batch version).Finally, we show the applicability of our results to the mini-batch $k$-meansalgorithm implemented in the scikit-learn (sklearn) python library.", "output": "Mini-batch $k$-means terminates within $O(d/\\epsilon)$ iterations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the growing needs of online A/B testing to support the innovation inindustry, the opportunity cost of running an experiment becomes non-negligible.Therefore, there is an increasing demand for an efficient continuous monitoringservice that allows early stopping when appropriate. Classic statisticalmethods focus on hypothesis testing and are mostly developed for traditionalhigh-stake problems such as clinical trials, while experiments at onlineservice companies typically have very different features and focuses. Motivatedby the real needs, in this paper, we introduce a novel framework that wedeveloped in Amazon to maximize customer experience and control opportunitycost. We formulate the problem as a Bayesian optimal sequential decision makingproblem that has a unified utility function. We discuss extensively practicaldesign choices and considerations. We further introduce how to solve theoptimal decision rule via Reinforcement Learning and scale the solution. Weshow the effectiveness of this novel approach compared with existing methodsvia a large-scale meta-analysis on experiments in Amazon.", "output": "Experimentation Platforms Meet Reinforcement Learning: Bayesian Sequential Decision-Making for Continuous Monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The dynamics of systems of many degrees of freedom evolving on multiplescales are often modeled in terms of stochastic differential equations. Usuallythe structural form of these equations is unknown and the only manifestation ofthe system's dynamics are observations at discrete points in time. Despitetheir widespread use, accurately inferring these systems from sparse-in-timeobservations remains challenging. Conventional inference methods either focuson the temporal structure of observations, neglecting the geometry of thesystem's invariant density, or use geometric approximations of the invariantdensity, which are limited to conservative driving forces. To address theselimitations, here, we introduce a novel approach that reconciles these twoperspectives. We propose a path augmentation scheme that employs data-drivencontrol to account for the geometry of the invariant system's density.Non-parametric inference on the augmented paths, enables efficientidentification of the underlying deterministic forces of systems observed atlow sampling rates.", "output": "Geometric constraints improve inference of sparsely observed stochastic dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical imaging systems are often evaluated and optimized via objective, ortask-specific, measures of image quality (IQ) that quantify the performance ofan observer on a specific clinically-relevant task. The performance of theBayesian Ideal Observer (IO) sets an upper limit among all observers, numericalor human, and has been advocated for use as a figure-of-merit (FOM) forevaluating and optimizing medical imaging systems. However, the IO teststatistic corresponds to the likelihood ratio that is intractable to compute inthe majority of cases. A sampling-based method that employs Markov-Chain MonteCarlo (MCMC) techniques was previously proposed to estimate the IO performance.However, current applications of MCMC methods for IO approximation have beenlimited to a small number of situations where the considered distribution ofto-be-imaged objects can be described by a relatively simple stochastic objectmodel (SOM). As such, there remains an important need to extend the domain ofapplicability of MCMC methods to address a large variety of scenarios whereIO-based assessments are needed but the associated SOMs have not beenavailable. In this study, a novel MCMC method that employs a generativeadversarial network (GAN)-based SOM, referred to as MCMC-GAN, is described andevaluated. The MCMC-GAN method was quantitatively validated by use oftest-cases for which reference solutions were available. The resultsdemonstrate that the MCMC-GAN method can extend the domain of applicability ofMCMC methods for conducting IO analyses of medical imaging systems.", "output": "Ideal Observer Computation by Use of Markov-Chain Monte Carlo with Generative Adversarial Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series event detection methods are evaluated mainly by standardclassification metrics that focus solely on detection accuracy. However,inaccuracy in detecting an event can often result from its preceding or delayedeffects reflected in neighboring detections. These detections are valuable totrigger necessary actions or help mitigate unwelcome consequences. In thiscontext, current metrics are insufficient and inadequate for the context ofevent detection. There is a demand for metrics that incorporate both theconcept of time and temporal tolerance for neighboring detections. This paperintroduces SoftED metrics, a new set of metrics designed for soft evaluatingevent detection methods. They enable the evaluation of both detection accuracyand the degree to which their detections represent events. They improved eventdetection evaluation by associating events and their representative detections,incorporating temporal tolerance in over 36% of experiments compared to theusual classification metrics. SoftED metrics were validated by domainspecialists that indicated their contribution to detection evaluation andmethod selection.", "output": "SoftED: Metrics for Soft Evaluation of Time Series Event Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic modulation classification (AMC) is a crucial stage in the spectrummanagement, signal monitoring, and control of wireless communication systems.The accurate classification of the modulation format plays a vital role in thesubsequent decoding of the transmitted data. End-to-end deep learning methodshave been recently applied to AMC, outperforming traditional featureengineering techniques. However, AMC still has limitations in lowsignal-to-noise ratio (SNR) environments. To address the drawback, we propose anovel AMC-Net that improves recognition by denoising the input signal in thefrequency domain while performing multi-scale and effective feature extraction.Experiments on two representative datasets demonstrate that our model performsbetter in efficiency and effectiveness than the most current methods.", "output": "AMC-Net: An Effective Network for Automatic Modulation Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "dame-flame is a Python package for performing matching for observationalcausal inference on datasets containing discrete covariates. This packageimplements the Dynamic Almost Matching Exactly (DAME) and Fast Large-ScaleAlmost Matching Exactly (FLAME) algorithms, which match treatment and controlunits on subsets of the covariates. The resulting matched groups areinterpretable, because the matches are made on covariates, and high-quality,because machine learning is used to determine which covariates are important tomatch on. DAME solves an optimization problem that matches units on as manycovariates as possible, prioritizing matches on important covariates. FLAMEapproximates the solution found by DAME via a much faster backward featureselection procedure. The package provides several adjustable parameters toadapt the algorithms to specific applications, and can calculate treatmenteffect estimates after matching. Descriptions of these parameters, details onestimating treatment effects, and further examples, can be found in thedocumentation at", "output": "dame-flame: A Python Library Providing Fast Interpretable Matching for Causal Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We devise a theoretical framework and a numerical method to infertrajectories of a stochastic process from samples of its temporal marginals.This problem arises in the analysis of single cell RNA-sequencing data, whichprovide high dimensional measurements of cell states but cannot track thetrajectories of the cells over time. We prove that for a class of stochasticprocesses it is possible to recover the ground truth trajectories from limitedsamples of the temporal marginals at each time-point, and provide an efficientalgorithm to do so in practice. The method we develop, Global Waddington-OT(gWOT), boils down to a smooth convex optimization problem posed globally overall time-points involving entropy-regularized optimal transport. We demonstratethat this problem can be solved efficiently in practice and yields goodreconstructions, as we show on several synthetic and real datasets.", "output": "Towards a mathematical theory of trajectory inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We design, implement, and evaluate DeepEverest, a system for the efficientexecution of interpretation by example queries over the activation values of adeep neural network. DeepEverest consists of an efficient indexing techniqueand a query execution algorithm with various optimizations. We prove that theproposed query execution algorithm is instance optimal. Experiments with ourprototype show that DeepEverest, using less than 20% of the storage of fullmaterialization, significantly accelerates individual queries by up to 63x andconsistently outperforms other methods on multi-query workloads that simulateDNN interpretation processes.", "output": "DeepEverest: Accelerating Declarative Top-K Queries for Deep Neural Network Interpretation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the problem of causal effect estimation in the presence ofunobserved confounding, but where proxies for the latent confounder(s) areobserved. We propose two kernel-based methods for nonlinear causal effectestimation in this setting: (a) a two-stage regression approach, and (b) amaximum moment restriction approach. We focus on the proximal causal learningsetting, but our methods can be used to solve a wider class of inverse problemscharacterised by a Fredholm integral equation. In particular, we provide aunifying view of two-stage and moment restriction approaches for solving thisproblem in a nonlinear setting. We provide consistency guarantees for eachalgorithm, and we demonstrate these approaches achieve competitive results onsynthetic data and data simulating a real-world task. In particular, ourapproach outperforms earlier methods that are not suited to leveraging proxyvariables.", "output": "Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative adversarial networks (GANs) are among the most successful modelsfor learning high-complexity, real-world distributions. However, in theory, dueto the highly non-convex, non-concave landscape of the minmax trainingobjective, GAN remains one of the least understood deep learning models. Inthis work, we formally study how GANs can efficiently learn certainhierarchically generated distributions that are close to the distribution ofreal-life images. We prove that when a distribution has a structure that werefer to as Forward Super-Resolution, then simply training generativeadversarial networks using stochastic gradient descent ascent (SGDA) can learnthis distribution efficiently, both in sample and time complexities. We alsoprovide empirical evidence that our assumption \"forward super-resolution\" isvery natural in practice, and the underlying learning mechanisms that we studyin this paper (to allow us efficiently train GAN via SGDA in theory) simulatesthe actual learning process of GANs on real-world problems.", "output": "Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The difficulty in acquiring a sufficient amount of training data is a majorbottleneck for machine learning (ML) based data analytics. Recently,commoditizing ML models has been proposed as an economical and moderatesolution to ML-oriented data acquisition. However, existing model marketplacesassume that the broker can access data owners' private training data, which maynot be realistic in practice. In this paper, to promote trustworthy dataacquisition for ML tasks, we propose FL-Market, a locally private modelmarketplace that protects privacy not only against model buyers but alsoagainst the untrusted broker. FL-Market decouples ML from the need to centrallygather training data on the broker's side using federated learning, an emergingprivacy-preserving ML paradigm in which data owners collaboratively train an MLmodel by uploading local gradients (to be aggregated into a global gradient formodel updating). Then, FL-Market enables data owners to locally perturb theirgradients by local differential privacy and thus further prevents privacyrisks. To drive FL-Market, we propose a deep learning-empowered auctionmechanism for intelligently deciding the local gradients' perturbation levelsand an optimal aggregation mechanism for aggregating the perturbed gradients.Our auction and aggregation mechanisms can jointly maximize the globalgradient's accuracy, which optimizes model buyers' utility. Our experimentsverify the effectiveness of the proposed mechanisms.", "output": "FL-Market: Trading Private Models in Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider distributed stochastic variational inequalities (VIs) onunbounded domains with the problem data that is heterogeneous (non-IID) anddistributed across many devices. We make a very general assumption on thecomputational network that, in particular, covers the settings of fullydecentralized calculations with time-varying networks and centralizedtopologies commonly used in Federated Learning. Moreover, multiple localupdates on the workers can be made for reducing the communication frequencybetween the workers. We extend the stochastic extragradient method to this verygeneral setting and theoretically analyze its convergence rate in thestrongly-monotone, monotone, and non-monotone (when a Minty solution exists)settings. The provided rates explicitly exhibit the dependence on networkcharacteristics (e.g., mixing time), iteration counter, data heterogeneity,variance, number of devices, and other standard parameters. As a special case,our method and analysis apply to distributed stochastic saddle-point problems(SPP), e.g., to the training of Deep Generative Adversarial Networks (GANs) forwhich decentralized training has been reported to be extremely challenging. Inexperiments for the decentralized training of GANs we demonstrate theeffectiveness of our proposed approach.", "output": "Decentralized Local Stochastic Extra-Gradient for Variational Inequalities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We are interested in estimating the effect of a treatment applied toindividuals at multiple sites, where data is stored locally for each site. Dueto privacy constraints, individual-level data cannot be shared across sites;the sites may also have heterogeneous populations and treatment assignmentmechanisms. Motivated by these considerations, we develop federated methods todraw inference on the average treatment effects of combined data across sites.Our methods first compute summary statistics locally using propensity scoresand then aggregate these statistics across sites to obtain point and varianceestimators of average treatment effects. We show that these estimators areconsistent and asymptotically normal. To achieve these asymptotic properties,we find that the aggregation schemes need to account for the heterogeneity intreatment assignments and in outcomes across sites. We demonstrate the validityof our federated methods through a comparative study of two large medicalclaims databases.", "output": "Federated Causal Inference in Heterogeneous Observational Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The aortic vessel tree is composed of the aorta and its branching arteries,and plays a key role in supplying the whole body with blood. Aortic diseases,like aneurysms or dissections, can lead to an aortic rupture, whose treatmentwith open surgery is highly risky. Therefore, patients commonly undergo drugtreatment under constant monitoring, which requires regular inspections of thevessels through imaging. The standard imaging modality for diagnosis andmonitoring is computed tomography (CT), which can provide a detailed picture ofthe aorta and its branching vessels if completed with a contrast agent, calledCT angiography (CTA). Optimally, the whole aortic vessel tree geometry fromconsecutive CTAs is overlaid and compared. This allows not only detection ofchanges in the aorta, but also of its branches, caused by the primary pathologyor newly developed. When performed manually, this reconstruction requires sliceby slice contouring, which could easily take a whole day for a single aorticvessel tree, and is therefore not feasible in clinical practice. Automatic orsemi-automatic vessel tree segmentation algorithms, however, can complete thistask in a fraction of the manual execution time and run in parallel to theclinical routine of the clinicians. In this paper, we systematically reviewcomputing techniques for the automatic and semi-automatic segmentation of theaortic vessel tree. The review concludes with an in-depth discussion on howclose these state-of-the-art approaches are to an application in clinicalpractice and how active this research field is, taking into account the numberof publications, datasets and challenges.", "output": "AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The stochastic approximation (SA) algorithm is a widely used probabilisticmethod for finding a zero or a fixed point of a vector-valued funtion, whenonly noisy measurements of the function are available. In the literature todate, one makes a distinction between ``synchronous'' updating, whereby everycomponent of the current guess is updated at each time, and ``asynchronous''updating, whereby only one component is updated. In this paper, we study anintermediate situation that we call ``batch asynchronous stochasticapproximation'' (BASA), in which, at each time instant, textit{some but notall} components of the current estimated solution are updated. BASA allows theuser to trade off memory requirements against time complexity. We develop ageneral methodology for proving that such algorithms converge to the fixedpoint of the map under study. These convergence proofs make use of weakerhypotheses than existing results. Specifically, existing convergence proofsrequire that the measurement noise is a zero-mean i.i.d sequence or amartingale difference sequence. In the present paper, we permit biasedmeasurements, that is, measurement noises that have nonzero conditional mean.Also, all convergence results to date assume that the stochastic step sizessatisfy a probabilistic analog of the well-known Robbins-Monro conditions. Wereplace this assumption by a purely deterministic condition on theirreducibility of the underlying Markov processes.As specific applications to Reinforcement Learning, we analyze the temporaldifference algorithm $TD(lambda)$ for value iteration, and the $Q$-learningalgorithm for finding the optimal action-value function. In both cases, weestablish the convergence of these algorithms, under milder conditions than inthe existing literature.", "output": "Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce novel convergence results for asynchronous iterations thatappear in the analysis of parallel and distributed optimization algorithms. Theresults are simple to apply and give explicit estimates for how the degree ofasynchrony impacts the convergence rates of the iterates. Our results shorten,streamline and strengthen existing convergence proofs for several asynchronousoptimization methods and allow us to establish convergence guarantees forpopular algorithms that were thus far lacking a complete theoreticalunderstanding. Specifically, we use our results to derive better iterationcomplexity bounds for proximal incremental aggregated gradient methods, toobtain tighter guarantees depending on the average rather than maximum delayfor the asynchronous stochastic gradient descent method, to provide lessconservative analyses of the speedup conditions for asynchronousblock-coordinate implementations of Krasnoselskii-Mann iterations, and toquantify the convergence rates for totally asynchronous iterations undervarious assumptions on communication delays and update rates.", "output": "Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) is a privacy-preserving distributed machine learningparadigm that enables collaborative training among geographically distributedand heterogeneous devices without gathering their data. Extending FL beyond thesupervised learning models, federated reinforcement learning (FRL) was proposedto handle sequential decision-making problems in edge computing systems.However, the existing FRL algorithms directly combine model-free RL with FL,thus often leading to high sample complexity and lacking theoreticalguarantees. To address the challenges, we propose a novel FRL algorithm thateffectively incorporates model-based RL and ensemble knowledge distillationinto FL for the first time. Specifically, we utilise FL and knowledgedistillation to create an ensemble of dynamics models for clients, and thentrain the policy by solely using the ensemble model without interacting withthe environment. Furthermore, we theoretically prove that the monotonicimprovement of the proposed algorithm is guaranteed. The extensive experimentalresults demonstrate that our algorithm obtains much higher sample efficiencycompared to classic model-free FRL algorithms in the challenging continuouscontrol benchmark environments under edge computing settings. The results alsohighlight the significant impact of heterogeneous client data and local modelupdate steps on the performance of FRL, validating the insights obtained fromour theoretical analysis.", "output": "Federated Ensemble Model-based Reinforcement Learning in Edge Computing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent developments in Artificial Intelligence techniques have enabled theirsuccessful application across a spectrum of commercial and industrial settings.However, these techniques require large volumes of data to be aggregated in acentralized manner, forestalling their applicability to scenarios wherein thedata is sensitive or the cost of data transmission is prohibitive. FederatedLearning alleviates these problems by decentralizing model training, therebyremoving the need for data transfer and aggregation. To advance the adoption ofFederated Learning, more research and development needs to be conducted toaddress some important open questions. In this work, we propose OpenFed, anopen-source software framework for end-to-end Federated Learning. OpenFedreduces the barrier to entry for both researchers and downstream users ofFederated Learning by the targeted removal of existing pain points. Forresearchers, OpenFed provides a framework wherein new methods can be easilyimplemented and fairly evaluated against an extensive suite of benchmarks. Fordownstream users, OpenFed allows Federated Learning to be plugged and playwithin different subject-matter contexts, removing the need for deep expertisein Federated Learning.", "output": "OpenFed: A Comprehensive and Versatile Open-Source Federated Learning Framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Variational inequalities in general and saddle point problems in particularare increasingly relevant in machine learning applications, includingadversarial learning, GANs, transport and robust optimization. With increasingdata and problem sizes necessary to train high performing models across variousapplications, we need to rely on parallel and distributed computing. However,in distributed training, communication among the compute nodes is a keybottleneck during training, and this problem is exacerbated for highdimensional and over-parameterized models. Due to these considerations, it isimportant to equip existing methods with strategies that would allow to reducethe volume of transmitted information during training while obtaining a modelof comparable quality. In this paper, we present the first theoreticallygrounded distributed methods for solving variational inequalities and saddlepoint problems using compressed communication: MASHA1 and MASHA2. Our theoryand methods allow for the use of both unbiased (such as Rand$k$; MASHA1) andcontractive (such as Top$k$; MASHA2) compressors. New algorithms supportbidirectional compressions, and also can be modified for stochastic settingwith batches and for federated learning with partial participation of clients.We empirically validated our conclusions using two experimental setups: astandard bilinear min-max problem, and large-scale distributed adversarialtraining of transformers.", "output": "Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Schr\"odinger Bridge (SB) is an entropy-regularized optimal transport problemthat has received increasing attention in deep generative modeling for itsmathematical flexibility compared to the Scored-based Generative Model (SGM).However, it remains unclear whether the optimization principle of SB relates tothe modern training of deep generative models, which often rely on constructinglog-likelihood objectives.This raises questions on the suitability of SB modelsas a principled alternative for generative applications. In this work, wepresent a novel computational framework for likelihood training of SB modelsgrounded on Forward-Backward Stochastic Differential Equations Theory - amathematical methodology appeared in stochastic optimal control that transformsthe optimality condition of SB into a set of SDEs. Crucially, these SDEs can beused to construct the likelihood objectives for SB that, surprisingly,generalizes the ones for SGM as special cases. This leads to a new optimizationprinciple that inherits the same SB optimality yet without losing applicationsof modern generative training techniques, and we show that the resultingtraining algorithm achieves comparable results on generating realistic imageson MNIST, CelebA, and CIFAR10. Our code is available at", "output": "Likelihood Training of Schr\\\"odinger Bridge using Forward-Backward SDEs Theory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in Federated Learning (FL) have brought large-scalecollaborative machine learning opportunities for massively distributed clientswith performance and data privacy guarantees. However, most current works focuson the interest of the central controller in FL,and overlook the interests ofthe FL clients. This may result in unfair treatment of clients that discouragesthem from actively participating in the learning process and damages thesustainability of the FL ecosystem. Therefore, the topic of ensuring fairnessin FL is attracting a great deal of research interest. In recent years, diverseFairness-Aware FL (FAFL) approaches have been proposed in an effort to achievefairness in FL from different perspectives. However, there is no comprehensivesurvey that helps readers gain insight into this interdisciplinary field. Thispaper aims to provide such a survey. By examining the fundamental andsimplifying assumptions, as well as the notions of fairness adopted by existingliterature in this field, we propose a taxonomy of FAFL approaches coveringmajor steps in FL, including client selection, optimization, contributionevaluation and incentive distribution. In addition, we discuss the main metricsfor experimentally evaluating the performance of FAFL approaches, and suggestpromising future research directions towards FAFL.", "output": "Towards Fairness-Aware Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper aims to illustrate the concept-emerging phenomenon in a trainedDNN. Specifically, we find that the inference score of a DNN can bedisentangled into the effects of a few interactive concepts. These concepts canbe understood as causal patterns in a sparse, symbolic causal graph, whichexplains the DNN. The faithfulness of using such a causal graph to explain theDNN is theoretically guaranteed, because we prove that the causal graph canwell mimic the DNN's outputs on an exponential number of different maskedsamples. Besides, such a causal graph can be further simplified and re-writtenas an And-Or graph (AOG), without losing much explanation accuracy.", "output": "Defining and Quantifying the Emergence of Sparse Concepts in DNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The explosion of digital information and the growing involvement of people insocial networks led to enormous research activity to develop methods that canextract meaningful information from interaction data. Commonly, interactionsare represented by edges in a network or a graph, which implicitly assumes thatthe interactions are pairwise and static. However, real-world interactionsdeviate from these assumptions: (i) interactions can be multi-way, involvingmore than two nodes or individuals (e.g., family relationships, proteininteractions), and (ii) interactions can change over a period of time (e.g.,change of opinions and friendship status). While pairwise interactions havebeen studied in a dynamic network setting and multi-way interactions have beenstudied using hypergraphs in static networks, there exists no method, atpresent, that can predict multi-way interactions or hyperedges in dynamicsettings. Existing related methods cannot answer temporal queries like whattype of interaction will occur next and when it will occur. This paper proposesa temporal point process model for hyperedge prediction to address theseproblems. Our proposed model uses dynamic representation learning techniquesfor nodes in a neural point process framework to forecast hyperedges. Wepresent several experimental results and set benchmark results. As far as ourknowledge, this is the first work that uses the temporal point process toforecast hyperedges in dynamic networks.", "output": "Dynamic Representation Learning with Temporal Point Processes for Higher-Order Interaction Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning is gaining growing momentum in various recent models for thedynamic analysis of information flows in data communications networks. Thesepreliminary models often rely on off-the-shelf learning models to predict fromhistorical statistics while disregarding the physics governing the generatingbehaviors of these flows. This paper instead introduces Flow Neural Network(FlowNN) to improve the feature representation with learned physical bias. Thisis implemented by an induction layer, working upon the embedding layer, toimpose the physics connected data correlations, and a self-supervised learningstrategy with stop-gradient to make the learned physics universal. For theshort-timescale network prediction tasks, FlowNN achieves 17% - 71% of lossdecrease than the state-of-the-art baselines on both synthetic and real-worldnetworking datasets, which shows the strength of this new approach.", "output": "Physics Constrained Flow Neural Network for Short-Timescale Predictions in Data Communications Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Safety is a critical component of autonomous systems and remains a challengefor learning-based policies to be utilized in the real world. In particular,policies learned using reinforcement learning often fail to generalize to novelenvironments due to unsafe behavior. In this paper, we proposeSim-to-Lab-to-Real to bridge the reality gap with a probabilisticallyguaranteed safety-aware policy distribution. To improve safety, we apply a dualpolicy setup where a performance policy is trained using the cumulative taskreward and a backup (safety) policy is trained by solving the Safety BellmanEquation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Labtransfer, we apply a supervisory control scheme to shield unsafe actions duringexploration; in Lab-to-Real transfer, we leverage the Probably ApproximatelyCorrect (PAC)-Bayes framework to provide lower bounds on the expectedperformance and safety of policies in unseen environments. Additionally,inheriting from the HJ reachability analysis, the bound accounts for theexpectation over the worst-case safety in each environment. We empiricallystudy the proposed framework for ego-vision navigation in two types of indoorenvironments with varying degrees of photorealism. We also demonstrate stronggeneralization performance through hardware experiments in real indoor spaceswith a quadrupedal robot. See for supplementarymaterial.", "output": "Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Along with the flourish of the information age, massive amounts of data aregenerated day by day. Due to the large-scale and high-dimensionalcharacteristics of these data, it is often difficult to achieve betterdecision-making in practical applications. Therefore, an efficient big dataanalytics method is urgently needed. For feature engineering, feature selectionseems to be an important research content in which is anticipated to select\"excellent\" features from candidate ones. Different functions can be realizedthrough feature selection, such as dimensionality reduction, model effectimprovement, and model performance improvement. In many classification tasks,researchers found that data seem to be usually close to each other if they arefrom the same class; thus, local compactness is of great importance for theevaluation of a feature. In this manuscript, we propose a fast unsupervisedfeature selection method, named as, Compactness Score (CSUFS), to selectdesired features. To demonstrate the efficiency and accuracy, several data setsare chosen with extensive experiments being performed. Later, the effectivenessand superiority of our method are revealed through addressing clustering tasks.Here, the performance is indicated by several well-known evaluation metrics,while the efficiency is reflected by the corresponding running time. Asrevealed by the simulation results, our proposed algorithm seems to be moreaccurate and efficient compared with existing algorithms.", "output": "Compactness Score: A Fast Filter Method for Unsupervised Feature Selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper resolves the open question of designing near-optimal algorithmsfor learning imperfect-information extensive-form games from bandit feedback.We present the first line of algorithms that require only$widetilde{mathcal{O}}((XA+YB)/varepsilon^2)$ episodes of play to find an$varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where$X,Y$ are the number of information sets and $A,B$ are the number of actionsfor the two players. This improves upon the best known sample complexity of$widetilde{mathcal{O}}((X^2A+Y^2B)/varepsilon^2)$ by a factor of$widetilde{mathcal{O}}(max{X, Y})$, and matches the information-theoreticlower bound up to logarithmic factors. We achieve this sample complexity by twonew algorithms: Balanced Online Mirror Descent, and Balanced CounterfactualRegret Minimization. Both algorithms rely on novel approaches of integratingemph{balanced exploration policies} into their classical counterparts. We alsoextend our results to learning Coarse Correlated Equilibria in multi-playergeneral-sum games.", "output": "Near-Optimal Learning of Extensive-Form Games with Imperfect Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Variational inequalities are a formalism that includes games, minimization,saddle point, and equilibrium problems as special cases. Methods forvariational inequalities are therefore universal approaches for many appliedtasks, including machine learning problems. This work concentrates on thedecentralized setting, which is increasingly important but not well understood.In particular, we consider decentralized stochastic (sum-type) variationalinequalities over fixed and time-varying networks. We present lower complexitybounds for both communication and local iterations and construct optimalalgorithms that match these lower bounds. Our algorithms are the best among theavailable literature not only in the decentralized stochastic case, but also inthe decentralized deterministic and non-distributed stochastic cases.Experimental results confirm the effectiveness of the presented algorithms.", "output": "Optimal Algorithms for Decentralized Stochastic Variational Inequalities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work finds the analytical expression of the global minima of a deeplinear network with weight decay and stochastic neurons, a fundamental modelfor understanding the landscape of neural networks. Our result implies thatzero is a special point in deep neural network architecture. We show thatweight decay strongly interacts with the model architecture and can create badminima at zero in a network with more than $1$ hidden layer, qualitativelydifferent from a network with only $1$ hidden layer. Practically, our resultimplies that common deep learning initialization methods are insufficient toease the optimization of neural networks in general.", "output": "Exact Solutions of a Deep Linear Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional NeuralNetwork (CNN) for pose estimation of noncooperative spacecraft across domaingap. SPNv2 is a multi-scale, multi-task CNN which consists of a sharedmulti-scale feature encoder and multiple prediction heads that performdifferent tasks on a shared feature output. These tasks are all related todetection and pose estimation of a target spacecraft from an image, such asprediction of pre-defined satellite keypoints, direct pose regression, andbinary segmentation of the satellite foreground. It is shown that by jointlytraining on different yet related tasks with extensive data augmentations onsynthetic images only, the shared encoder learns features that are commonacross image domains that have fundamentally different visual characteristicscompared to synthetic images. This work also introduces Online DomainRefinement (ODR) which refines the parameters of the normalization layers ofSPNv2 on the target domain images online at deployment. Specifically, ODRperforms self-supervised entropy minimization of the predicted satelliteforeground, thereby improving the CNN's performance on the target domain imageswithout their pose labels and with minimal computational efforts. The GitHubrepository for SPNv2 is available at ", "output": "Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite recent advances of AI, story understanding remains an open andunder-investigated problem. We collect, preprocess, and publicly release avideo-language story dataset, Synopses of Movie Narratives (SYMON), containing5,193 video summaries of popular movies and TV series. SYMON capturesnaturalistic story-telling videos for human audience made by human creators. Asa prototypical and naturalistic story dataset, SYMON features high coverage ofmultimodal story events, abundant mental-state descriptions, and large semanticgaps between the visual and the textual modalities. We establish benchmarks onvideo-text retrieval and zero-shot alignment on movie summary videos, whichshowcase the importance of in-domain data in story understanding. With SYMON,we hope to lay the groundwork for progress in multimodal story understanding.", "output": "Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While developments in machine learning led to impressive performance gains onbig data, many human subjects data are, in actuality, small and sparselylabeled. Existing methods applied to such data often do not easily generalizeto out-of-sample subjects. Instead, models must make predictions on test datathat may be drawn from a different distribution, a problem known astextit{zero-shot learning}. To address this challenge, we develop anend-to-end framework using a meta-learning approach, which enables the model torapidly adapt to a new prediction task with limited training data forout-of-sample test data. We use three real-world small-scale human subjectsdatasets (two randomized control studies and one observational study), forwhich we predict treatment outcomes for held-out treatment groups. Our modellearns the latent treatment effects of each intervention and, by design, cannaturally handle multi-task predictions. We show that our model performs thebest holistically for each held-out group and especially when the test group isdistinctly different from the training group. Our model has implications forimproved generalization of small-size human studies to the wider population.", "output": "Zero-shot meta-learning for small-scale data from human subjects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most deep learning models for computational imaging regress a singlereconstructed image. In practice, however, ill-posedness, nonlinearity, modelmismatch, and noise often conspire to make such point estimates misleading orinsufficient. The Bayesian approach models images and (noisy) measurements asjointly distributed random vectors and aims to approximate the posteriordistribution of unknowns. Recent variational inference methods based onconditional normalizing flows are a promising alternative to traditional MCMCmethods, but they come with drawbacks: excessive memory and compute demands formoderate to high resolution images and underwhelming performance on hardnonlinear problems. In this work, we propose C-Trumpets -- conditionalinjective flows specifically designed for imaging problems, which greatlydiminish these challenges. Injectivity reduces memory footprint and trainingtime while low-dimensional latent space together with architectural innovationslike fixed-volume-change layers and skip-connection revnet layers, C-Trumpetsoutperform regular conditional flow models on a variety of imaging and imagerestoration tasks, including limited-view CT and nonlinear inverse scattering,with a lower compute and memory budget. C-Trumpets enable fast approximation ofpoint estimates like MMSE or MAP as well as physically-meaningful uncertaintyquantification.", "output": "Conditional Injective Flows for Bayesian Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning is increasingly used to discover diagnostic and prognosticbiomarkers from high-dimensional molecular data. However, a variety of factorsrelated to experimental design may affect the ability to learn generalizableand clinically applicable diagnostics. Here, we argue that a causal perspectiveimproves the identification of these challenges and formalizes their relationto the robustness and generalization of machine learning-based diagnostics. Tomake for a concrete discussion, we focus on a specific, recently establishedhigh-dimensional biomarker - adaptive immune receptor repertoires (AIRRs).Through simulations, we illustrate how major biological and experimentalfactors of the AIRR domain may influence the learned biomarkers. In conclusion,we argue that causal modeling improves machine learning-based biomarkerrobustness by identifying stable relations between variables and by guiding theadjustment of the relations and variables that vary between populations.", "output": "Improving generalization of machine learning-identified biomarkers with causal modeling: an investigation into immune receptor diagnostics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the proliferation of pump-and-dump schemes (P&amp;Ds) in the cryptocurrencymarket, it becomes imperative to detect such fraudulent activities in advanceto alert potentially susceptible investors. In this paper, we focus onpredicting the pump probability of all coins listed in the target exchangebefore a scheduled pump time, which we refer to as the target coin predictiontask. Firstly, we conduct a comprehensive study of the latest 709 P&amp;D eventsorganized in Telegram from Jan. 2019 to Jan. 2022. Our empirical analysisreveals some interesting patterns of P&amp;Ds, such as that pumped coins exhibitintra-channel homogeneity and inter-channel heterogeneity. Here channel refersa form of group in Telegram that is frequently used to coordinate P&amp;D events.This observation inspires us to develop a novel sequence-based neural network,dubbed SNN, which encodes a channel's P&amp;D event history into a sequencerepresentation via the positional attention mechanism to enhance the predictionaccuracy. Positional attention helps to extract useful information andalleviates noise, especially when the sequence length is long. Extensiveexperiments verify the effectiveness and generalizability of proposed methods.Additionally, we release the code and P&amp;D dataset on GitHub: andregularly update the dataset.", "output": "Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Storm surge is a major natural hazard in coastal regions, responsible bothfor significant property damage and loss of life. Accurate, efficient models ofstorm surge are needed both to assess long-term risk and to guide emergencymanagement decisions. While high-fidelity regional- and global-oceancirculation models such as the ADvanced CIRCulation (ADCIRC) model canaccurately predict storm surge, they are very computationally expensive. Herewe develop a novel surrogate model for peak storm surge prediction based on amulti-stage approach. In the first stage, points are classified as inundated ornot. In the second, the level of inundation is predicted . Additionally, wepropose a new formulation of the surrogate problem in which storm surge ispredicted independently for each point. This allows for predictions to be madedirectly for locations not present in the training data, and significantlyreduces the number of model parameters. We demonstrate our modeling frameworkon two study areas: the Texas coast and the northern portion of the Alaskancoast. For Texas, the model is trained with a database of 446 synthetichurricanes. The model is able to accurately match ADCIRC predictions on a testset of synthetic storms. We further present a test of the model on HurricanesIke (2008) and Harvey (2017). For Alaska, the model is trained on a dataset of109 historical surge events. We test the surrogate model on actual surge eventsincluding the recent Typhoon Merbok (2022) that take place after the events inthe training data. For both datasets, the surrogate model achieves similarperformance to ADCIRC on real events when compared to observational data. Inboth cases, the surrogate models are many orders of magnitude faster thanADCIRC.", "output": "A Framework for Flexible Peak Storm Surge Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-and-language (V&amp;L) models pretrained on large-scale multimodal datahave demonstrated strong performance on various tasks such as image captioningand visual question answering (VQA). The quality of such models is commonlyassessed by measuring their performance on unseen data that typically comesfrom the same distribution as the training data. However, when evaluated underout-of-distribution (out-of-dataset) settings for VQA, we observe that thesemodels exhibit poor generalization. We comprehensively evaluate two pretrainedV&amp;L models under different settings (i.e. classification and open-ended textgeneration) by conducting cross-dataset evaluations. We find that these modelstend to learn to solve the benchmark, rather than learning the high-levelskills required by the VQA task. We also find that in most cases generativemodels are less susceptible to shifts in data distribution compared todiscriminative ones, and that multimodal pretraining is generally helpful forOOD generalization. Finally, we revisit assumptions underlying the use ofautomatic VQA evaluation metrics, and empirically show that their stringentnature repeatedly penalizes models for correct responses.", "output": "Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The log odds ratio is a well-established metric for evaluating theassociation between binary outcome and exposure variables. Despite itswidespread use, there has been limited discussion on how to summarize the logodds ratio as a function of confounders through averaging. To address thisissue, we propose the Average Adjusted Association (AAA), which is a summarymeasure of association in a heterogeneous population, adjusted for observedconfounders. To facilitate the use of it, we also develop efficientdouble/debiased machine learning (DML) estimators of the AAA. Our DMLestimators use two equivalent forms of the efficient influence function, andare applicable in various sampling scenarios, including random sampling,outcome-based sampling, and exposure-based sampling. Through real data andsimulations, we demonstrate the practicality and effectiveness of our proposedestimators in measuring the AAA.", "output": "Average Adjusted Association: Efficient Estimation with High Dimensional Confounders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Studies on benign overfitting provide insights for the success ofoverparameterized deep learning models. In this work, we examine whetheroverfitting is truly benign in real-world classification tasks. We start withthe observation that a ResNet model overfits benignly on Cifar10 but notbenignly on ImageNet. To understand why benign overfitting fails in theImageNet experiment, we theoretically analyze benign overfitting under a morerestrictive setup where the number of parameters is not significantly largerthan the number of data points. Under this mild overparameterization setup, ouranalysis identifies a phase change: unlike in the previous heavyoverparameterization settings, benign overfitting can now fail in the presenceof label noise. Our analysis explains our empirical observations, and isvalidated by a set of control experiments with ResNets. Our work highlights theimportance of understanding implicit bias in underfitting regimes as a futuredirection.", "output": "Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classification has been the focal point of research on adversarial attacks,but only a few works investigate methods suited to denser prediction tasks,such as semantic segmentation. The methods proposed in these works do notaccurately solve the adversarial segmentation problem and, therefore,overestimate the size of the perturbations required to fool models. Here, wepropose a white-box attack for these models based on a proximal splitting toproduce adversarial perturbations with much smaller $ell_infty$ norms. Ourattack can handle large numbers of constraints within a nonconvex minimizationframework via an Augmented Lagrangian approach, coupled with adaptiveconstraint scaling and masking strategies. We demonstrate that our attacksignificantly outperforms previously proposed ones, as well as classificationattacks that we adapted for segmentation, providing a first comprehensivebenchmark for this dense task.", "output": "Proximal Splitting Adversarial Attacks for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Re-initializing a neural network during training has been observed to improvegeneralization in recent works. Yet it is neither widely adopted in deeplearning practice nor is it often used in state-of-the-art training protocols.This raises the question of when re-initialization works, and whether it shouldbe used together with regularization techniques such as data augmentation,weight decay and learning rate schedules. In this work, we conduct an extensiveempirical comparison of standard training with a selection of re-initializationmethods to answer this question, training over 15,000 models on a variety ofimage classification benchmarks. We first establish that such methods areconsistently beneficial for generalization in the absence of any otherregularization. However, when deployed alongside other carefully tunedregularization techniques, re-initialization methods offer little to no addedbenefit for generalization, although optimal generalization performance becomesless sensitive to the choice of learning rate and weight decay hyperparameters.To investigate the impact of re-initialization methods on noisy data, we alsoconsider learning under label noise. Surprisingly, in this case,re-initialization significantly improves upon standard training, even in thepresence of other carefully tuned regularization techniques.", "output": "When Does Re-initialization Work?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Analyzing the distribution shift of data is a growing research direction innowadays Machine Learning (ML), leading to emerging new benchmarks that focuson providing a suitable scenario for studying the generalization properties ofML models. The existing benchmarks are focused on supervised learning, and tothe best of our knowledge, there is none for unsupervised learning. Therefore,we introduce an unsupervised anomaly detection benchmark with data that shiftsover time, built over Kyoto-2006+, a traffic dataset for network intrusiondetection. This type of data meets the premise of shifting the inputdistribution: it covers a large time span ($10$ years), with naturallyoccurring changes over time (eg users modifying their behavior patterns, andsoftware updates). We first highlight the non-stationary nature of the data,using a basic per-feature analysis, t-SNE, and an Optimal Transport approachfor measuring the overall distribution distances between years. Next, wepropose AnoShift, a protocol splitting the data in IID, NEAR, and FAR testingsplits. We validate the performance degradation over time with diverse models,ranging from classical approaches to deep learning. Finally, we show that byacknowledging the distribution shift problem and properly addressing it, theperformance can be improved compared to the classical training which assumesindependent and identically distributed data (on average, by up to $3%$ forour approach). Dataset and code are available at", "output": "AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning based solutions are being succesfully implemented for a widevariety of applications. Most notably, clinical use-cases have gained anincreased interest and have been the main driver behind some of thecutting-edge data-driven algorithms proposed in the last years. Forapplications like sparse-view tomographic reconstructions, where the amount ofmeasurement data is small in order to keep acquisition time short and radiationdose low, reduction of the streaking artifacts has prompted the development ofdata-driven denoising algorithms with the main goal of obtaining diagnosticallyviable images with only a subset of a full-scan data. We propose WNet, adata-driven dual-domain denoising model which contains a trainablereconstruction layer for sparse-view artifact denoising. Two encoder-decodernetworks perform denoising in both sinogram- and reconstruction-domainsimultaneously, while a third layer implementing the Filtered Backprojectionalgorithm is sandwiched between the first two and takes care of thereconstruction operation. We investigate the performance of the network onsparse-view chest CT scans, and we highlight the added benefit of having atrainable reconstruction layer over the more conventional fixed ones. We trainand test our network on two clinically relevant datasets and we compare theobtained results with three different types of sparse-view CT denoising andreconstruction algorithms.", "output": "WNet: A data-driven dual-domain denoising model for sparse-view computed tomography with a trainable reconstruction layer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating the political leanings of social media users is a challenging andever more pressing problem given the increase in social media consumption. Weintroduce Retweet-BERT, a simple and scalable model to estimate the politicalleanings of Twitter users. Retweet-BERT leverages the retweet network structureand the language used in users' profile descriptions. Our assumptions stem frompatterns of networks and linguistics homophily among people who share similarideologies. Retweet-BERT demonstrates competitive performance against otherstate-of-the-art baselines, achieving 96%-97% macro-F1 on two recent Twitterdatasets (a COVID-19 dataset and a 2020 United States presidential electionsdataset). We also perform manual validation to validate the performance ofRetweet-BERT on users not in the training data. Finally, in a case study ofCOVID-19, we illustrate the presence of political echo chambers on Twitter andshow that it exists primarily among right-leaning users. Our code isopen-sourced and our data is publicly available.", "output": "Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of representation learning on knowledge graphs (KGs), ahyper-relational fact consists of a main triple and several auxiliaryattribute-value descriptions, which is considered more comprehensive andspecific than a triple-based fact. However, currently availablehyper-relational KG embedding methods in a single view are limited inapplication because they weaken the hierarchical structure that represents theaffiliation between entities. To overcome this limitation, we propose adual-view hyper-relational KG structure (DH-KG) that contains ahyper-relational instance view for entities and a hyper-relational ontologyview for concepts that are abstracted hierarchically from the entities. Thispaper defines link prediction and entity typing tasks on DH-KG for the firsttime and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, andHTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embeddingmodel based on GRAN encoders, HGNNs, and joint learning. DHGE outperformsbaseline models on DH-KG, according to experimental results. Finally, weprovide an example of how this technology can be used to treat hypertension.Our model and new datasets are publicly available.", "output": "DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bayesian optimization is a methodology for global optimization of unknown andexpensive objectives. It combines a surrogate Bayesian regression model with anacquisition function to decide where to evaluate the objective. Typicalregression models are given by Gaussian processes with stationary covariancefunctions. However, these functions are unable to express prior input-dependentinformation, including possible locations of the optimum. The ubiquity ofstationary models has led to the common practice of exploiting priorinformation via informative mean functions. In this paper, we highlight thatthese models can perform poorly, especially in high dimensions. We proposenovel informative covariance functions for optimization, leveragingnonstationarity to encode preferences for certain regions of the search spaceand adaptively promote local exploration during optimization. We demonstratethat the proposed functions can increase the sample efficiency of Bayesianoptimization in high dimensions, even under weak prior information.", "output": "Bayesian Optimization with Informative Covariance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently proposed BERT-based evaluation metrics for text generation performwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,relating to information correctness. We argue that this stems (in part) fromthe fact that they are models of semantic similarity. In contrast, we developevaluation metrics based on Natural Language Inference (NLI), which we deem amore appropriate modeling. We design a preference-based adversarial attackframework and show that our NLI based metrics are much more robust to theattacks than the recent BERT-based metrics. On standard benchmarks, our NLIbased metrics outperform existing summarization metrics, but perform below SOTAMT metrics. However, when combining existing metrics with our NLI metrics, weobtain both higher adversarial robustness (15%-30%) and higher quality metricsas measured on standard benchmarks (+5% to 30%).", "output": "MENLI: Robust Evaluation Metrics from Natural Language Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present CycleDance, a dance style transfer system to transform an existingmotion clip in one dance style to a motion clip in another dance style whileattempting to preserve motion context of the dance. Our method extends anexisting CycleGAN architecture for modeling audio sequences and integratesmultimodal transformer encoders to account for music context. We adopt sequencelength-based curriculum learning to stabilize training. Our approach capturesrich and long-term intra-relations between motion frames, which is a commonchallenge in motion transfer and synthesis work. We further introduce newmetrics for gauging transfer strength and content preservation in the contextof dance movements. We perform an extensive ablation study as well as a humanstudy including 30 participants with 5 or more years of dance experience. Theresults demonstrate that CycleDance generates realistic movements with thetarget style, significantly outperforming the baseline CycleGAN on naturalness,transfer strength, and content preservation.", "output": "Dance Style Transfer with Cross-modal Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Privacy and security challenges in Machine Learning (ML) have becomeincreasingly severe, along with ML's pervasive development and the recentdemonstration of large attack surfaces. As a mature system-oriented approach,Confidential Computing has been utilized in both academia and industry tomitigate privacy and security issues in various ML scenarios. In this paper,the conjunction between ML and Confidential Computing is investigated. Wesystematize the prior work on Confidential Computing-assisted ML techniquesthat provide i) confidentiality guarantees and ii) integrity assurances, anddiscuss their advanced features and drawbacks. Key challenges are furtheridentified, and we provide dedicated analyses of the limitations in existingTrusted Execution Environment (TEE) systems for ML use cases. Finally,prospective works are discussed, including grounded privacy definitions forclosed-loop protection, partitioned executions of efficient ML, dedicatedTEE-assisted designs for ML, TEE-aware ML, and ML full pipeline guarantees. Byproviding these potential solutions in our systematization of knowledge, we aimat building the bridge to help achieve a much strong TEE-enabled ML for privacyguarantees without introducing computation and system costs.", "output": "Machine Learning with Confidential Computing: A Systematization of Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies the problem of designing an optimal sequence ofinterventions in a causal graphical model to minimize cumulative regret withrespect to the best intervention in hindsight. This is, naturally, posed as acausal bandit problem. The focus is on causal bandits for linear structuralequation models (SEMs) and soft interventions. It is assumed that the graph'sstructure is known and has $N$ nodes. Two linear mechanisms, one softintervention and one observational, are assumed for each node, giving rise to$2^N$ possible interventions. Majority of the existing causal bandit algorithmsassume that at least the interventional distributions of the reward node'sparents are fully specified. However, there are $2^N$ such distributions (onecorresponding to each intervention), acquiring which becomes prohibitive evenin moderate-sized graphs. This paper dispenses with the assumption of knowingthese distributions or their marginals. Two algorithms are proposed for thefrequentist (UCB-based) and Bayesian (Thompson Sampling-based) settings. Thekey idea of these algorithms is to avoid directly estimating the $2^N$ rewarddistributions and instead estimate the parameters that fully specify the SEMs(linear in $N$) and use them to compute the rewards. In both algorithms, underboundedness assumptions on noise and the parameter space, the cumulativeregrets scale as $tilde{cal O} (d^{L+frac{1}{2}} sqrt{NT})$, where $d$ isthe graph's maximum degree, and $L$ is the length of its longest causal path.Additionally, a minimax lower of $Omega(d^{frac{L}{2}-2}sqrt{T})$ ispresented, which suggests that the achievable and lower bounds conform in theirscaling behavior with respect to the horizon $T$ and graph parameters $d$ and$L$.", "output": "Causal Bandits for Linear Structural Equation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper is a survey of methods for solving smooth (strongly) monotonestochastic variational inequalities. To begin with, we give the deterministicfoundation from which the stochastic methods eventually evolved. Then we reviewmethods for the general stochastic formulation, and look at the finite sumsetup. The last parts of the paper are devoted to various recent (notnecessarily stochastic) advances in algorithms for variational inequalities.", "output": "Smooth Monotone Stochastic Variational Inequalities and Saddle Point Problems: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While representation learning has been central to the rise of machinelearning and artificial intelligence, a key problem remains in making thelearnt representations meaningful. For this the typical approach is toregularize the learned representation through prior probability distributions.However such priors are usually unavailable or ad hoc. To deal with this, wepropose a dynamics-constrained representation learning framework. Instead ofusing predefined probabilities, we restrict the latent representation to followspecific dynamics, which is a more natural constraint for representationlearning in dynamical systems. Our belief stems from a fundamental observationin physics that though different systems can have different marginalizedprobability distributions, they typically obey the same dynamics, such asNewton's and Schrodinger's equations. We validate our framework for differentsystems including a real-world fluorescent DNA movie dataset. We show that ouralgorithm can uniquely identify an uncorrelated, isometric and meaningfullatent representation.", "output": "From latent dynamics to meaningful representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising diffusion models represent a recent emerging topic in computervision, demonstrating remarkable results in the area of generative modeling. Adiffusion model is a deep generative model that is based on two stages, aforward diffusion stage and a reverse diffusion stage. In the forward diffusionstage, the input data is gradually perturbed over several steps by addingGaussian noise. In the reverse stage, a model is tasked at recovering theoriginal input data by learning to gradually reverse the diffusion process,step by step. Diffusion models are widely appreciated for the quality anddiversity of the generated samples, despite their known computational burdens,i.e. low speeds due to the high number of steps involved during sampling. Inthis survey, we provide a comprehensive review of articles on denoisingdiffusion models applied in vision, comprising both theoretical and practicalcontributions in the field. First, we identify and present three genericdiffusion modeling frameworks, which are based on denoising diffusionprobabilistic models, noise conditioned score networks, and stochasticdifferential equations. We further discuss the relations between diffusionmodels and other deep generative models, including variational auto-encoders,generative adversarial networks, energy-based models, autoregressive models andnormalizing flows. Then, we introduce a multi-perspective categorization ofdiffusion models applied in computer vision. Finally, we illustrate the currentlimitations of diffusion models and envision some interesting directions forfuture research.", "output": "Diffusion Models in Vision: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum kernels are considered as potential resources to illustrate benefitsof quantum computing in machine learning. Considering the impact ofhyperparameters on the performance of a classical machine learning model, it isimperative to identify promising hyperparameters using quantum kernel methodsin order to achieve quantum advantages. In this work, we analyse and classifysentiments of textual data using a new quantum kernel based on linear and fullentangled circuits as hyperparameters for controlling the correlation amongwords. We also find that the use of linear and full entanglement furthercontrols the expressivity of the Quantum Support Vector Machine (QSVM). Inaddition, we also compare the efficiency of the proposed circuit with otherquantum circuits and classical machine learning algorithms. Our results showthat the proposed fully entangled circuit outperforms all other fully orlinearly entangled circuits in addition to classical algorithms for most of thefeatures. In fact, as the feature increases the efficiency of our proposedfully entangled model also increases significantly.", "output": "The role of entanglement for enhancing the efficiency of quantum kernels towards classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper revisits Deep Mutual Learning (DML), a simple yet effectivecomputing paradigm. We propose using R'{e}nyi divergence instead of the KLdivergence, which is more flexible and tunable, to improve vanilla DML. Thismodification is able to consistently improve performance over vanilla DML withlimited additional complexity. The convergence properties of the proposedparadigm are analyzed theoretically, and Stochastic Gradient Descent with aconstant learning rate is shown to converge with $mathcal{O}(1)$-bias in theworst case scenario for nonconvex optimization tasks. That is, learning willreach nearby local optima but continue searching within a bounded scope, whichmay help mitigate overfitting. Finally, our extensive empirical resultsdemonstrate the advantage of combining DML and R'{e}nyi divergence, whichfurther improves generalized models.", "output": "R\\'{e}nyi Divergence Deep Mutual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite great success in many applications, deep neural networks are notalways robust in practice. For instance, a convolutional neuron network (CNN)model for classification tasks often performs unsatisfactorily in classifyingsome particular classes of objects. In this work, we are concerned withpatching the weak part of a CNN model instead of improving it through thecostly retraining of the entire model. Inspired by the fundamental concepts ofmodularization and composition in software engineering, we propose a compressedmodularization approach, CNNSplitter, which decomposes a strong CNN model for$N$-class classification into $N$ smaller CNN modules. Each module is asub-model containing a part of the convolution kernels of the strong model. Topatch a weak CNN model that performs unsatisfactorily on a target class (TC),we compose the weak CNN model with the corresponding module obtained from astrong CNN model. The ability of the weak CNN model to recognize the TC canthus be improved through patching. Moreover, the ability to recognize non-TCsis also improved, as the samples misclassified as TC could be classified asnon-TCs correctly. Experimental results with two representative CNNs on threewidely-used datasets show that the averaged improvement on the TC in terms ofprecision and recall are 12.54% and 2.14%, respectively. Moreover, patchingimproves the accuracy of non-TCs by 1.18%. The results demonstrate thatCNNSplitter can patch a weak CNN model through modularization and composition,thus providing a new solution for developing robust CNN models.", "output": "Patching Weak Convolutional Neural Network Models through Modularization and Composition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Group equivariant convolutional neural networks (G-CNNs) have beensuccessfully applied in geometric deep learning. Typically, G-CNNs have theadvantage over CNNs that they do not waste network capacity on trainingsymmetries that should have been hard-coded in the network. The recentlyintroduced framework of PDE-based G-CNNs (PDE-G-CNNs) generalises G-CNNs.PDE-G-CNNs have the core advantages that they simultaneously 1) reduce networkcomplexity, 2) increase classification performance, and 3) provide geometricinterpretability. Their implementations primarily consist of linear andmorphological convolutions with kernels.In this paper we show that the previously suggested approximativemorphological kernels do not always accurately approximate the exact kernelsaccurately. More specifically, depending on the spatial anisotropy of theRiemannian metric, we argue that one must resort to sub-Riemannianapproximations. We solve this problem by providing a new approximative kernelthat works regardless of the anisotropy. We provide new theorems with bettererror estimates of the approximative kernels, and prove that they all carry thesame reflectional symmetries as the exact ones.We test the effectiveness of multiple approximative kernels within thePDE-G-CNN framework on two datasets, and observe an improvement with the newapproximative kernels. We report that the PDE-G-CNNs again allow for aconsiderable reduction of network complexity while having comparable or betterperformance than G-CNNs and CNNs on the two datasets. Moreover, PDE-G-CNNs havethe advantage of better geometric interpretability over G-CNNs, as themorphological kernels are related to association fields from neurogeometry.", "output": "Analysis of (sub-)Riemannian PDE-G-CNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Soft prompt learning has recently emerged as one of the methods of choice foradapting V&amp;L models to a downstream task using a few training examples.However, current methods significantly overfit the training data, sufferingfrom large accuracy degradation when tested on unseen classes from the samedomain. To this end, in this paper, we make the following 4 contributions: (1)To alleviate base class overfitting, we propose a novel Language-Aware SoftPrompting (LASP) learning method by means of a text-to-text cross-entropy lossthat maximizes the probability of the learned prompts to be correctlyclassified with respect to pre-defined hand-crafted textual prompts. (2) Toincrease the representation capacity of the prompts, we propose grouped LASPwhere each group of prompts is optimized with respect to a separate subset oftextual prompts. (3) We identify a visual-language misalignment introduced byprompt learning and LASP, and more importantly, propose a re-calibrationmechanism to address it. (4) We show that LASP is inherently amenable toincluding, during training, virtual classes, i.e. class names for which novisual samples are available, further increasing the robustness of the learnedprompts. Through evaluations on 11 datasets, we show that our approach (a)significantly outperforms all prior works on soft prompting, and (b) matchesand surpasses, for the first time, the accuracy on novel classes obtained byhand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be madeavailable at ", "output": "LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neurons in the brain are often finely tuned for specific task variables.Moreover, such disentangled representations are highly sought after in machinelearning. Here we mathematically prove that simple biological constraints onneurons, namely nonnegativity and energy efficiency in both activity andweights, promote such sought after disentangled representations by enforcingneurons to become selective for single factors of task variation. Wedemonstrate these constraints lead to disentanglement in a variety of tasks andarchitectures, including variational autoencoders. We also use this theory toexplain why the brain partitions its cells into distinct cell types such asgrid and object-vector cells, and also explain when the brain instead entanglesrepresentations in response to entangled task factors. Overall, this workprovides a mathematical understanding of why single neurons in the brain oftenrepresent single human-interpretable factors, and steps towards anunderstanding task structure shapes the structure of brain representation.", "output": "Disentanglement with Biological Constraints: A Theory of Functional Cell Types."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-vocabulary semantic segmentation aims to segment an image into semanticregions according to text descriptions, which may not have been seen duringtraining. Recent two-stage methods first generate class-agnostic mask proposalsand then leverage pre-trained vision-language models, e.g., CLIP, to classifymasked regions. We identify the performance bottleneck of this paradigm to bethe pre-trained CLIP model, since it does not perform well on masked images. Toaddress this, we propose to finetune CLIP on a collection of masked imageregions and their corresponding text descriptions. We collect training data bymining an existing image-caption dataset (e.g., COCO Captions), using CLIP tomatch masked image regions to nouns in the image captions. Compared with themore precise and manually annotated segmentation labels with fixed classes(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retainCLIP's generalization ability. Along with finetuning the entire model, weutilize the \"blank\" areas in masked images using a method we dub mask prompttuning. Experiments demonstrate mask prompt tuning brings significantimprovement without modifying any weights of CLIP, and it can further improve afully finetuned model. In particular, when trained on COCO and evaluated onADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than theprevious state-of-the-art. For the first time, open-vocabulary generalistmodels match the performance of supervised specialist models in 2017 withoutdataset-specific adaptations.", "output": "Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have demonstrated remarkable progress in image generationquality, especially when guidance is used to control the generative process.However, guidance requires a large amount of image-annotation pairs fortraining and is thus dependent on their availability, correctness andunbiasedness. In this paper, we eliminate the need for such annotation byinstead leveraging the flexibility of self-supervision signals to design aframework for self-guided diffusion models. By leveraging a feature extractionfunction and a self-annotation function, our method provides guidance signalsat various image granularities: from the level of holistic images to objectboxes and even segmentation masks. Our experiments on single-label andmulti-label image datasets demonstrate that self-labeled guidance alwaysoutperforms diffusion models without guidance and may even surpass guidancebased on ground-truth labels, especially on unbalanced data. When equipped withself-supervised box or mask proposals, our method further generates visuallydiverse yet semantically consistent images, without the need for any class,box, or segment label annotation. Self-guided diffusion is simple, flexible andexpected to profit from deployment at scale.", "output": "Self-Guided Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While 6D object pose estimation has wide applications across computer visionand robotics, it remains far from being solved due to the lack of annotations.The problem becomes even more challenging when moving to category-level 6Dpose, which requires generalization to unseen instances. Current approaches arerestricted by leveraging annotations from simulation or collected from humans.In this paper, we overcome this barrier by introducing a self-supervisedlearning approach trained directly on large-scale real-world object videos forcategory-level 6D pose estimation in the wild. Our framework reconstructs thecanonical 3D shape of an object category and learns dense correspondencesbetween input images and the canonical shape via surface embedding. Fortraining, we propose novel geometrical cycle-consistency losses which constructcycles across 2D-3D spaces, across different instances and different timesteps. The learned correspondence can be applied for 6D pose estimation andother downstream tasks such as keypoint transfer. Surprisingly, our method,without any human annotations or simulators, can achieve on-par or even betterperformance than previous supervised or semi-supervised methods on in-the-wildimages. Our project page is:  .", "output": "Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a decluttering problem where multiple rigid convex polygonalobjects rest in randomly placed positions and orientations on a planar surfaceand must be efficiently transported to a packing box using both single andmulti-object grasps. Prior work considered frictionless multi-object grasping.In this paper, we introduce friction to increase picks per hour. We train aneural network using real examples to plan robust multi-object grasps. Inphysical experiments, we find a 13.7% increase in success rate, a 1.6x increasein picks per hour, and a 6.3x decrease in grasp planning time compared to priorwork on multi-object grasping. Compared to single object grasping, we find a3.1x increase in picks per hour.", "output": "Learning to Efficiently Plan Robust Frictional Multi-Object Grasps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic rib labeling and anatomical centerline extraction are commonprerequisites for various clinical applications. Prior studies either usein-house datasets that are inaccessible to communities, or focus on ribsegmentation that neglects the clinical significance of rib labeling. Toaddress these issues, we extend our prior dataset (RibSeg) on the binary ribsegmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CTscans (15,466 individual ribs in total) and annotations manually inspected byexperts for rib labeling and anatomical centerline extraction. Based on theRibSeg v2, we develop a pipeline including deep learning-based methods for riblabeling, and a skeletonization-based method for centerline extraction. Toimprove computational efficiency, we propose a sparse point cloudrepresentation of CT scans and compare it with standard dense voxel grids.Moreover, we design and analyze evaluation metrics to address the keychallenges of each task. Our dataset, code, and model are available online tofacilitate open research at ", "output": "RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Message Passing Neural Networks (MPNNs) are a widely used class of GraphNeural Networks (GNNs). The limited representational power of MPNNs inspiresthe study of provably powerful GNN architectures. However, knowing one model ismore powerful than another gives little insight about what functions they canor cannot express. It is still unclear whether these models are able toapproximate specific functions such as counting certain graph substructures,which is essential for applications in biology, chemistry and social networkanalysis. Motivated by this, we propose to study the counting power of SubgraphMPNNs, a recent and popular class of powerful GNN models that extract rootedsubgraphs for each node, assign the root node a unique identifier and encodethe root node's representation within its rooted subgraph. Specifically, weprove that Subgraph MPNNs fail to count more-than-4-cycles at node level,implying that node representations cannot correctly encode the surroundingsubstructures like ring systems with more than four atoms. To overcome thislimitation, we propose I$^2$-GNNs to extend Subgraph MPNNs by assigningdifferent identifiers for the root node and its neighbors in each subgraph.I$^2$-GNNs' discriminative power is shown to be strictly stronger than SubgraphMPNNs and partially stronger than the 3-WL test. More importantly, I$^2$-GNNsare proven capable of counting all 3, 4, 5 and 6-cycles, covering commonsubstructures like benzene rings in organic chemistry, while still keepinglinear complexity. To the best of our knowledge, it is the first linear-timeGNN model that can count 6-cycles with theoretical guarantees. We validate itscounting power in cycle counting tasks and demonstrate its competitiveperformance in molecular prediction benchmarks.", "output": "Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes an algorithm for Federated Learning (FL) with a two-layerstructure that achieves both variance reduction and a faster convergence rateto an optimal solution in the setting where each agent has an arbitraryprobability of selection in each iteration. In distributed machine learning,when privacy matters, FL is a functional tool. Placing FL in an environmentwhere it has some irregular connections of agents (devices), reaching a trainedmodel in both an economical and quick way can be a demanding job. The firstlayer of our algorithm corresponds to the model parameter propagation acrossagents done by the server. In the second layer, each agent does its localupdate with a stochastic and variance-reduced technique called StochasticVariance Reduced Gradient (SVRG). We leverage the concept of variance reductionfrom stochastic optimization when the agents want to do their local update stepto reduce the variance caused by stochastic gradient descent (SGD). We providea convergence bound for our algorithm which improves the rate from$O(frac{1}{sqrt{K}})$ to $O(frac{1}{K})$ by using a constant step-size. Wedemonstrate the performance of our algorithm using numerical examples.", "output": "Federated Learning Using Variance Reduced Stochastic Gradient for Probabilistically Activated Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a smoothly broken power law functional form (referred to by us asa Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates thescaling behaviors of deep neural networks (i.e. how the evaluation metric ofinterest varies as the amount of compute used for training, number of modelparameters, training dataset size, model input size, number of training steps,or upstream performance varies) for various architectures and for each ofvarious tasks within a large and diverse set of upstream and downstream tasks,in zero-shot, prompted, and fine-tuned settings. This set includes large-scalevision, language, audio, video, diffusion, generative modeling, multimodallearning, contrastive learning, AI alignment, robotics, out-of-distribution(OOD) generalization, continual learning, transfer learning, uncertaintyestimation / calibration, out-of-distribution detection, adversarialrobustness, distillation, sparsity, retrieval, quantization, pruning, fairness,molecules, computer programming/coding, math word problems, \"emergent\" \"phasetransitions / changes\", arithmetic, unsupervised/self-supervised learning, &amp;reinforcement learning (single agent &amp; multi-agent). When compared to otherfunctional forms for neural scaling behavior, this functional form yieldsextrapolations of scaling behavior that are considerably more accurate on thisset. Moreover, this functional form accurately models &amp; extrapolates scalingbehavior that other functional forms are incapable of expressing such as thenon-monotonic transitions present in the scaling behavior of phenomena such asdouble descent &amp; the delayed, sharp inflection points present in the scalingbehavior of tasks such as arithmetic. Lastly, we use this functional form toglean insights about the limit of the predictability of scaling behavior. Codeis available at ", "output": "Broken Neural Scaling Laws."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Piano covers of pop music are enjoyed by many people. However, the task ofautomatically generating piano covers of pop music is still understudied. Thisis partly due to the lack of synchronized {Pop, Piano Cover} data pairs, whichmade it challenging to apply the latest data-intensive deep learning-basedmethods. To leverage the power of the data-driven approach, we make a largeamount of paired and synchronized {Pop, Piano Cover} data using an automatedpipeline. In this paper, we present Pop2Piano, a Transformer network thatgenerates piano covers given waveforms of pop music. To the best of ourknowledge, this is the first model to generate a piano cover directly from popaudio without using melody and chord extraction modules. We show thatPop2Piano, trained with our dataset, is capable of producing plausible pianocovers.", "output": "Pop2Piano : Pop Audio-based Piano Cover Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Today's computer vision models achieve human or near-human level performanceacross a wide variety of vision tasks. However, their architectures, data, andlearning algorithms differ in numerous ways from those that give rise to humanvision. In this paper, we investigate the factors that affect the alignmentbetween the representations learned by neural networks and human mentalrepresentations inferred from behavioral responses. We find that model scaleand architecture have essentially no effect on the alignment with humanbehavioral responses, whereas the training dataset and objective function bothhave a much larger impact. These findings are consistent across three datasetsof human similarity judgments collected using two different tasks. Lineartransformations of neural network representations learned from behavioralresponses from one dataset substantially improve alignment with humansimilarity judgments on the other two datasets. In addition, we find that somehuman concepts such as food and animals are well-represented by neural networkswhereas others such as royal or sports-related objects are not. Overall,although models trained on larger, more diverse datasets achieve betteralignment with humans than models trained on ImageNet alone, our resultsindicate that scaling alone is unlikely to be sufficient to train neuralnetworks with conceptual representations that match those used by humans.", "output": "Human alignment of neural network representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) is a distributed machine learning paradigm whereclients collaboratively train a model using their local (human-generated)datasets. While existing studies focus on FL algorithm development to tackledata heterogeneity across clients, the important issue of data quality (e.g.,label noise) in FL is overlooked. This paper aims to fill this gap by providinga quantitative study on the impact of label noise on FL. We derive an upperbound for the generalization error that is linear in the clients' label noiselevel. Then we conduct experiments on MNIST and CIFAR-10 datasets using variousFL algorithms. Our empirical results show that the global model accuracylinearly decreases as the noise level increases, which is consistent with ourtheoretical analysis. We further find that label noise slows down theconvergence of FL training, and the global model tends to overfit when thenoise level is high.", "output": "Quantifying the Impact of Label Noise on Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Action advising is a knowledge transfer technique for reinforcement learningbased on the teacher-student paradigm. An expert teacher provides advice to astudent during training in order to improve the student's sample efficiency andpolicy performance. Such advice is commonly given in the form of state-actionpairs. However, it makes it difficult for the student to reason with and applyto novel states. We introduce Explainable Action Advising, in which the teacherprovides action advice as well as associated explanations indicating why theaction was chosen. This allows the student to self-reflect on what it haslearned, enabling advice generalization and leading to improved sampleefficiency and learning performance - even in environments where the teacher issub-optimal. We empirically show that our framework is effective in bothsingle-agent and multi-agent scenarios, yielding improved policy returns andconvergence rates when compared to state-of-the-art methods", "output": "Explainable Action Advising for Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel method of using explainability techniques to designphysics-aware neural networks. We demonstrate our approach by developing aconvolutional neural network (CNN) for solving an inverse problem for shallowsubsurface imaging. Although CNNs have gained popularity in recent years acrossmany fields, the development of CNNs remains an art, as there are no clearguidelines regarding the selection of hyperparameters that will yield the bestnetwork. While optimization algorithms may be used to select hyperparametersautomatically, these methods focus on developing networks with high predictiveaccuracy while disregarding model explainability (descriptive accuracy).However, the field of Explainable Artificial Intelligence (XAI) addresses theabsence of model explainability by providing tools that allow developers toevaluate the internal logic of neural networks. In this study, we use theexplainability methods Score-CAM and Deep SHAP to select hyperparameters, suchas kernel sizes and network depth, to develop a physics-aware CNN for shallowsubsurface imaging. We begin with a relatively deep Encoder-Decoder network,which uses surface wave dispersion images as inputs and generates 2D shear wavevelocity subsurface images as outputs. Through model explanations, weultimately find that a shallow CNN using two convolutional layers with anatypical kernel size of 3x1 yields comparable predictive accuracy but withincreased descriptive accuracy. We also show that explainability methods can beused to evaluate the network's complexity and decision-making. We believe thismethod can be used to develop neural networks with high predictive accuracywhile also providing inherent explainability.", "output": "Using explainability to design physics-aware CNNs for solving subsurface inverse problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Small on-device models have been successfully trained with user-leveldifferential privacy (DP) for next word prediction and image classificationtasks in the past. However, existing methods can fail when directly applied tolearn embedding models using supervised training data with a large class space.To achieve user-level DP for large image-to-embedding feature extractors, wepropose DP-FedEmb, a variant of federated learning algorithms with per-usersensitivity control and noise addition, to train from user-partitioned datacentralized in the datacenter. DP-FedEmb combines virtual clients, partialaggregation, private local fine-tuning, and public pretraining to achievestrong privacy utility trade-offs. We apply DP-FedEmb to train image embeddingmodels for faces, landmarks and natural species, and demonstrate its superiorutility under same privacy budget on benchmark datasets DigiFace, EMNIST, GLDand iNaturalist. We further illustrate it is possible to achieve stronguser-level DP guarantees of $epsilon&lt;4$ while controlling the utility dropwithin 5%, when millions of users can participate in training.", "output": "Learning to Generate Image Embeddings with User-level Differential Privacy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale Transformer models bring significant improvements for variousdownstream vision language tasks with a unified architecture. The performanceimprovements come with increasing model size, resulting in slow inference speedand increased cost for severing. While some certain predictions benefit fromthe full complexity of the large-scale model, not all of inputs need the sameamount of computation to conduct, potentially leading to computation resourcewaste. To handle this challenge, early exiting is proposed to adaptivelyallocate computational power in term of input complexity to improve inferenceefficiency. The existing early exiting strategies usually adopt outputconfidence based on intermediate layers as a proxy of input complexity to incurthe decision of skipping following layers. However, such strategies cannotapply to encoder in the widely-used unified architecture with both encoder anddecoder due to difficulty of output confidence estimation in the encoder. It issuboptimal in term of saving computation power to ignore the early exiting inencoder component. To handle this challenge, we propose a novel early exitingstrategy for unified visual language models, which allows dynamically skip thelayers in encoder and decoder simultaneously in term of input layer-wisesimilarities with multiple times of early exiting, namely textbf{MuE}. Bydecomposing the image and text modalities in the encoder, MuE is flexible andcan skip different layers in term of modalities, advancing the inferenceefficiency while minimizing performance drop. Experiments on the SNLI-VE and MSCOCO datasets show that the proposed approach MuE can reduce expected inferencetime by up to 50% and 40% while maintaining 99% and 96% performancerespectively.", "output": "You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multivariate biosignals are prevalent in many medical domains, such aselectroencephalography, polysomnography, and electrocardiography. Modelingspatiotemporal dependencies in multivariate biosignals is challenging due to(1) long-range temporal dependencies and (2) complex spatial correlationsbetween the electrodes. To address these challenges, we propose representingmultivariate biosignals as time-dependent graphs and introduce GraphS4mer, ageneral graph neural network (GNN) architecture that improves performance onbiosignal classification tasks by modeling spatiotemporal dependencies inbiosignals. Specifically, (1) we leverage the Structured State Spacearchitecture, a state-of-the-art deep sequence model, to capture long-rangetemporal dependencies in biosignals and (2) we propose a graph structurelearning layer in GraphS4mer to learn dynamically evolving graph structures inthe data. We evaluate our proposed model on three distinct biosignalclassification tasks and show that GraphS4mer consistently improves overexisting models, including (1) seizure detection from electroencephalographysignals, outperforming a previous GNN with self-supervised pre-training by 3.1points in AUROC; (2) sleep staging from polysomnography signals, a 4.1 pointsimprovement in macro-F1 score compared to existing sleep staging models; and(3) 12-lead electrocardiogram classification, outperforming previousstate-of-the-art models by 2.7 points in macro-F1 score.", "output": "Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reconstructing 3D shapes from planar cross-sections is a challenge inspiredby downstream applications like medical imaging and geographic informatics. Theinput is an in/out indicator function fully defined on a sparse collection ofplanes in space, and the output is an interpolation of the indicator functionto the entire volume. Previous works addressing this sparse and ill-posedproblem either produce low quality results, or rely on additional priors suchas target topology, appearance information, or input normal directions. In thispaper, we present OReX, a method for 3D shape reconstruction from slices alone,featuring a Neural Field as the interpolation prior. A modest neural network istrained on the input planes to return an inside/outside estimate for a given 3Dcoordinate, yielding a powerful prior that induces smoothness andself-similarities. The main challenge for this approach is high-frequencydetails, as the neural prior is overly smoothing. To alleviate this, we offeran iterative estimation architecture and a hierarchical input sampling schemethat encourage coarse-to-fine training, allowing the training process to focuson high frequencies at later stages. In addition, we identify and analyze aripple-like effect stemming from the mesh extraction step. We mitigate it byregularizing the spatial gradients of the indicator function around inputin/out boundaries during network training, tackling the problem at the root.Through extensive qualitative and quantitative experimentation, we demonstrateour method is robust, accurate, and scales well with the size of the input. Wereport state-of-the-art results compared to previous approaches and recentpotential solutions, and demonstrate the benefit of our individualcontributions through analysis and ablation studies.", "output": "OReX: Object Reconstruction from Planar Cross-sections Using Neural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformers are powerful visual learners, in large part due to theirconspicuous lack of manually-specified priors. This flexibility can beproblematic in tasks that involve multiple-view geometry, due to thenear-infinite possible variations in 3D shapes and viewpoints (requiringflexibility), and the precise nature of projective geometry (obeying rigidlaws). To resolve this conundrum, we propose a \"light touch\" approach, guidingvisual Transformers to learn multiple-view geometry but allowing them to breakfree when needed. We achieve this by using epipolar lines to guide theTransformer's cross-attention maps, penalizing attention values outside theepipolar lines and encouraging higher attention along these lines since theycontain geometrically plausible matches. Unlike previous methods, our proposaldoes not require any camera pose information at test-time. We focus onpose-invariant object instance retrieval, where standard Transformer networksstruggle, due to the large differences in viewpoint between query and retrievedimages. Experimentally, our method outperforms state-of-the-art approaches atobject retrieval, without needing pose information at test-time.", "output": "A Light Touch Approach to Teaching Transformers Multi-view Geometry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Comparison-based learning addresses the problem of learning when, instead ofexplicit features or pairwise similarities, one only has access to comparisonsof the form: emph{Object $A$ is more similar to $B$ than to $C$.} Recently, ithas been shown that, in Hierarchical Clustering, single and complete linkagecan be directly implemented using only such comparisons while severalalgorithms have been proposed to emulate the behaviour of average linkage.Hence, finding hierarchies (or dendrograms) using only comparisons is a wellunderstood problem. However, evaluating their meaningfulness when noground-truth nor explicit similarities are available remains an open question.In this paper, we bridge this gap by proposing a new revenue function thatallows one to measure the goodness of dendrograms using only comparisons. Weshow that this function is closely related to Dasgupta's cost for hierarchicalclustering that uses pairwise similarities. On the theoretical side, we use theproposed revenue function to resolve the open problem of whether one canapproximately recover a latent hierarchy using few triplet comparisons. On thepractical side, we present principled algorithms for comparison-basedhierarchical clustering based on the maximisation of the revenue and weempirically compare them with existing methods.", "output": "A Revenue Function for Comparison-Based Hierarchical Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Causal phenomena associated with rare events occur across a wide range ofengineering problems, such as risk-sensitive safety analysis, accident analysisand prevention, and extreme value theory. However, current methods for causaldiscovery are often unable to uncover causal links, between random variables ina dynamic setting, that manifest only when the variables first experiencelow-probability realizations. To address this issue, we introduce a novelstatistical independence test on data collected from time-invariant dynamicalsystems in which rare but consequential events occur. In particular, we exploitthe time-invariance of the underlying data to construct a superimposed datasetof the system state before rare events happen at different timesteps. We thendesign a conditional independence test on the reorganized data. We providenon-asymptotic sample complexity bounds for the consistency of our method, andvalidate its performance across various simulated and real-world datasets,including incident data collected from the Caltrans Performance MeasurementSystem (PeMS). Code containing the datasets and experiments is publiclyavailable.", "output": "Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement Learning (RL) generally suffers from poor sample complexity,mostly due to the need to exhaustively explore the state-action space to findwell-performing policies. On the other hand, we postulate that expert knowledgeof the system often allows us to design simple rules we expect good policies tofollow at all times. In this work, we hence propose a simple yet effectivemodification of continuous actor-critic frameworks to incorporate such rulesand avoid regions of the state-action space that are known to be suboptimal,thereby significantly accelerating the convergence of RL agents. Concretely, wesaturate the actions chosen by the agent if they do not comply with ourintuition and, critically, modify the gradient update step of the policy toensure the learning process is not affected by the saturation step. On a roomtemperature control case study, it allows agents to converge to well-performingpolicies up to 6-7x faster than classical agents without computational overheadand while retaining good final performance.", "output": "Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative models have shown great promise in synthesizing photorealistic 3Dobjects, but they require large amounts of training data. We introduce SinGRAF,a 3D-aware generative model that is trained with a few input images of a singlescene. Once trained, SinGRAF generates different realizations of this 3D scenethat preserve the appearance of the input while varying scene layout. For thispurpose, we build on recent progress in 3D GAN architectures and introduce anovel progressive-scale patch discrimination approach during training. Withseveral experiments, we demonstrate that the results produced by SinGRAFoutperform the closest related works in both quality and diversity by a largemargin.", "output": "SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We shed light on a pitfall and an opportunity in physics-informed neuralnetworks (PINNs). We prove that a multilayer perceptron (MLP) only with ReLU(Rectified Linear Unit) or ReLU-like Lipschitz activation functions will alwayslead to a vanished Hessian. Such a network-imposed constraint contradicts anysecond- or higher-order partial differential equations (PDEs). Therefore, aReLU-based MLP cannot form a permissible function space for the approximationof their solutions. Inspired by this pitfall, we prove that a linear PDE up tothe $n$-th order can be strictly satisfied by an MLP with $C^n$ activationfunctions when the weights of its output layer lie on a certain hyperplane, ascalled the out-layer-hyperplane. An MLP equipped with the out-layer-hyperplanebecomes \"physics-enforced\", no longer requiring a loss function for the PDEitself (but only those for the initial and boundary conditions). Such ahyperplane exists not only for MLPs but for any network architecture tailed bya fully-connected hidden layer. To our knowledge, this should be the first PINNarchitecture that enforces point-wise correctness of PDEs. We show aclosed-form expression of the out-layer-hyperplane for second-order linearPDEs, which can be generalised to higher-order nonlinear PDEs.", "output": "On the Compatibility between Neural Networks and Partial Differential Equations for Physics-informed Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Zero-shot quantization is a promising approach for developing lightweightdeep neural networks when data is inaccessible owing to various reasons,including cost and issues related to privacy. By exploiting the learnedparameters ($mu$ and $sigma$) of batch normalization layers in anFP32-pre-trained model, zero-shot quantization schemes focus on generatingsynthetic data. Subsequently, they distill knowledge from the pre-trained model(teacher) to the quantized model (student) such that the quantized model can beoptimized with the synthetic dataset. However, thus far, zero-shot quantizationhas primarily been discussed in the context of quantization-aware trainingmethods, which require task-specific losses and long-term optimization as muchas retraining. We thus introduce a post-training quantization scheme forzero-shot quantization that produces high-quality quantized networks within afew hours. Furthermore, we propose a framework called genie~that generatesdata suited for quantization. With the data synthesized by Genie, we canproduce robust quantized models without real datasets, which is comparable tofew-shot quantization. We also propose a post-training quantization algorithmto enhance the performance of quantized models. By combining them, we canbridge the gap between zero-shot and few-shot quantization while significantlyimproving the quantization performance compared to that of existing approaches.In other words, we can obtain a unique state-of-the-art zero-shot quantizationapproach.", "output": "Genie: Show Me the Data for Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A household robot should be able to navigate to target objects withoutrequiring users to first annotate everything in their home. Most currentapproaches to object navigation do not test on real robots and rely solely onreconstructed scans of houses and their expensively labeled semantic 3D meshes.In this work, our goal is to build an agent that builds self-supervised modelsof the world via exploration, the same as a child might - thus we (1) eschewthe expense of labeled 3D mesh and (2) enable self-supervised in-situfinetuning in the real world. We identify a strong source of self-supervision(Location Consistency - LocCon) that can train all components of an ObjectNavagent, using unannotated simulated houses. Our key insight is that embodiedagents can leverage location consistency as a self-supervision signal -collecting images from different views/angles and applying contrastivelearning. We show that our agent can perform competitively in the real worldand simulation. Our results also indicate that supervised training with 3D meshannotations causes models to learn simulation artifacts, which are nottransferrable to the real world. In contrast, our LocCon shows the most robusttransfer in the real world among the set of models we compare to, and that thereal-world performance of all models can be further improved withself-supervised LocCon in-situ training.", "output": "Self-Supervised Object Goal Navigation with In-Situ Finetuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning classifiers provide the most accurate means of automaticallydiagnosing diabetic retinopathy (DR) based on optical coherence tomography(OCT) and its angiography (OCTA). The power of these models is attributable inpart to the inclusion of hidden layers that provide the complexity required toachieve a desired task. However, hidden layers also render algorithm outputsdifficult to interpret. Here we introduce a novel biomarker activation map(BAM) framework based on generative adversarial learning that allows cliniciansto verify and understand classifiers decision-making. A data set including 456macular scans were graded as non-referable or referable DR based on currentclinical standards. A DR classifier that was used to evaluate our BAM was firsttrained based on this data set. The BAM generation framework was designed bycombing two U-shaped generators to provide meaningful interpretability to thisclassifier. The main generator was trained to take referable scans as input andproduce an output that would be classified by the classifier as non-referable.The BAM is then constructed as the difference image between the output andinput of the main generator. To ensure that the BAM only highlightsclassifier-utilized biomarkers an assistant generator was trained to do theopposite, producing scans that would be classified as referable by theclassifier from non-referable scans. The generated BAMs highlighted knownpathologic features including nonperfusion area and retinal fluid. A fullyinterpretable classifier based on these highlights could help clinicians betterutilize and verify automated DR diagnosis.", "output": "Interpretable Diabetic Retinopathy Diagnosis based on Biomarker Activation Map."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Linear partial differential equations (PDEs) are an important, widely appliedclass of mechanistic models, describing physical processes such as heattransfer, electromagnetism, and wave propagation. In practice, specializednumerical methods based on discretization are used to solve PDEs. Theygenerally use an estimate of the unknown model parameters and, if available,physical measurements for initialization. Such solvers are often embedded intolarger scientific models with a downstream application and thus errorquantification plays a key role. However, by ignoring parameter and measurementuncertainty, classical PDE solvers may fail to produce consistent estimates oftheir inherent approximation error. In this work, we approach this problem in aprincipled fashion by interpreting solving linear PDEs as physics-informedGaussian process (GP) regression. Our framework is based on a keygeneralization of the Gaussian process inference theorem to observations madevia an arbitrary bounded linear operator. Crucially, this probabilisticviewpoint allows to (1) quantify the inherent discretization error; (2)propagate uncertainty about the model parameters to the solution; and (3)condition on noisy measurements. Demonstrating the strength of thisformulation, we prove that it strictly generalizes methods of weightedresiduals, a central class of PDE solvers including collocation, finite volume,pseudospectral, and (generalized) Galerkin methods such as finite element andspectral methods. This class can thus be directly equipped with a structurederror estimate. In summary, our results enable the seamless integration ofmechanistic models as modular building blocks into probabilistic models byblurring the boundaries between numerical analysis and Bayesian inference.", "output": "Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Can we build continuous generative models which generalize across scales, canbe evaluated at any coordinate, admit calculation of exact derivatives, and areconceptually simple? Existing MLP-based architectures generate worse samplesthan the grid-based generators with favorable convolutional inductive biases.Models that focus on generating images at different scales do better, butemploy complex architectures not designed for continuous evaluation of imagesand derivatives. We take a signal-processing perspective and treat continuousimage generation as interpolation from samples. Indeed, correctly sampleddiscrete images contain all information about the low spatial frequencies. Thequestion is then how to extrapolate the spectrum in a data-driven way whilemeeting the above design criteria. Our answer is FunkNN -- a new convolutionalnetwork which learns how to reconstruct continuous images at arbitrarycoordinates and can be applied to any image dataset. Combined with a discretegenerative model it becomes a functional generator which can act as a prior incontinuous ill-posed inverse problems. We show that FunkNN generateshigh-quality continuous images and exhibits strong out-of-distributionperformance thanks to its patch-based design. We further showcase itsperformance in several stylized inverse problems with exact spatialderivatives.", "output": "FunkNN: Neural Interpolation for Functional Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose the predictive forward-forward (PFF) algorithm for conductingcredit assignment in neural systems. Specifically, we design a novel, dynamicrecurrent neural system that learns a directed generative circuit jointly andsimultaneously with a representation circuit. Notably, the system integrateslearnable lateral competition, noise injection, and elements of predictivecoding, an emerging and viable neurobiological process theory of corticalfunction, with the forward-forward (FF) adaptation scheme. Furthermore, PFFefficiently learns to propagate learning signals and updates synapses withforward passes only, eliminating key structural and computational constraintsimposed by backpropagation-based schemes. Besides computational advantages, thePFF process could prove useful for understanding the learning mechanisms behindbiological neurons that use local signals despite missing feedback connections.We run experiments on image data and demonstrate that the PFF procedure worksas well as backpropagation, offering a promising brain-inspired algorithm forclassifying, reconstructing, and synthesizing data patterns.", "output": "The Predictive Forward-Forward Algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The role of data in building AI systems has recently been significantlymagnified by the emerging concept of data-centric AI (DCAI), which advocates afundamental shift from model advancements to ensuring data quality andreliability. Although our community has continuously invested efforts intoenhancing data in different aspects, they are often isolated initiatives onspecific tasks. To facilitate the collective initiative in our community andpush forward DCAI, we draw a big picture and bring together three generalmissions: training data development, inference data development, and datamaintenance. We provide a top-level discussion on representative DCAI tasks andshare perspectives. Finally, we list open challenges. More resources aresummarized at ", "output": "Data-centric AI: Perspectives and Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have seen a surge of interest in learning high-level causalrepresentations from low-level image pairs under interventions. Yet, existingefforts are largely limited to simple synthetic settings that are far away fromreal-world problems. In this paper, we present Causal Triplet, a causalrepresentation learning benchmark featuring not only visually more complexscenes, but also two crucial desiderata commonly overlooked in previous works:(i) an actionable counterfactual setting, where only certain object-levelvariables allow for counterfactual observations whereas others do not; (ii) aninterventional downstream task with an emphasis on out-of-distributionrobustness from the independent causal mechanisms principle. Through extensiveexperiments, we find that models built with the knowledge of disentangled orobject-centric representations significantly outperform their distributedcounterparts. However, recent causal representation learning methods stillstruggle to identify such latent structures, indicating substantial challengesand opportunities for future work. Our code and datasets will be available at", "output": "Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, a new perspective is suggested for unsupervised OntologyMatching (OM) or Ontology Alignment (OA) by treating it as a translation task.Ontologies are represented as graphs, and the translation is performed from anode in the source ontology graph to a path in the target ontology graph. Theproposed framework, Truveta Mapper (TM), leverages a multi-tasksequence-to-sequence transformer model to perform alignment across multipleontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enablesthe model to implicitly learn the relationship between different ontologies viatransfer-learning without requiring any explicit cross-ontology manuallylabeled data. This also enables the formulated framework to outperform existingsolutions for both runtime latency and alignment quality. The model ispre-trained and fine-tuned only on publicly available text corpus andinner-ontologies data. The proposed solution outperforms state-of-the-artapproaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presentednew OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offerslog-linear complexity in contrast to quadratic in the existing end-to-endmethods, and overall makes the OM task efficient and more straightforwardwithout much post-processing involving mapping extension or mapping repair.", "output": "Truveta Mapper: A Zero-shot Ontology Alignment Framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning models that are robust to distribution shifts is a key concern inthe context of their real-life applicability. Invariant Risk Minimization (IRM)is a popular framework that aims to learn robust models from multipleenvironments. The success of IRM requires an important assumption: theunderlying causal mechanisms/features remain invariant across environments.When not satisfied, we show that IRM can over-constrain the predictor and toremedy this, we propose a relaxation via $textit{partial invariance}$. In thiswork, we theoretically highlight the sub-optimality of IRM and then demonstratehow learning from a partition of training domains can help improve invariantmodels. Several experiments, conducted both in linear settings as well as withdeep neural networks on tasks over both language and image data, allow us toverify our conclusions.", "output": "Learning Optimal Features via Partial Invariance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In up-to-date machine learning (ML) applications on cloud or edge computingplatforms, batching is an important technique for providing efficient andeconomical services at scale. In particular, parallel computing resources onthe platforms, such as graphics processing units (GPUs), have highercomputational and energy efficiency with larger batch sizes. However, largerbatch sizes may also result in longer response time, and thus it requires ajudicious design. This paper aims to provide a dynamic batching policy thatstrikes a balance between efficiency and latency. The GPU-based inferenceservice is modeled as a batch service queue with batch-size dependentprocessing time. Then, the design of dynamic batching is a continuous-timeaverage-cost problem, and is formulated as a semi-Markov decision process(SMDP) with the objective of minimizing the weighted sum of average responsetime and average power consumption. The optimal policy is acquired by solvingan associated discrete-time Markov decision process (MDP) problem with finitestate approximation and \"discretization\". By introducing an abstract cost toreflect the impact of \"tail\" states, the space complexity and the timecomplexity of the procedure can decrease by 63.5% and 98%, respectively. Ourresults show that the optimal policies potentially possess a control limitstructure. Numerical results also show that SMDP-based batching policies canadapt to different traffic intensities and outperform other benchmark policies.Furthermore, the proposed solution has notable flexibility in balancing powerconsumption and latency.", "output": "SMDP-Based Dynamic Batching for Efficient Inference on GPU-Based Platforms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an algorithm for minimizing an objective with hard-to-computegradients by using a related, easier-to-access function as a proxy. Ouralgorithm is based on approximate proximal point iterations on the proxycombined with relatively few stochastic gradients from the objective. When thedifference between the objective and the proxy is $delta$-smooth, ouralgorithm guarantees convergence at a rate matching stochastic gradient descenton a $delta$-smooth objective, which can lead to substantially better sampleefficiency. Our algorithm has many potential applications in machine learning,and provides a principled means of leveraging synthetic data, physicssimulators, mixed public and private data, and more.", "output": "Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT)framework. It emphasizes spatio-temporal continuity and integrates both pastand future reasoning for tracked objects. Thus, we name it \"Past-and-Futurereasoning for Tracking\" (PF-Track). Specifically, our method adapts the\"tracking by attention\" framework and represents tracked instances coherentlyover time with object queries. To explicitly use historical cues, our \"PastReasoning\" module learns to refine the tracks and enhance the object featuresby cross-attending to queries from previous frames and other objects. The\"Future Reasoning\" module digests historical information and predicts robustfuture trajectories. In the case of long-term occlusions, our method maintainsthe object positions and enables re-association by integrating motionpredictions. On the nuScenes dataset, our method improves AMOTA by a largemargin and remarkably reduces ID-Switches by 90% compared to prior approaches,which is an order of magnitude less. The code and models are made available at", "output": "Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised object-centric representation (OCR) learning has recently drawnattention as a new paradigm of visual representation. This is because of itspotential of being an effective pre-training technique for various downstreamtasks in terms of sample efficiency, systematic generalization, and reasoning.Although image-based reinforcement learning (RL) is one of the most importantand thus frequently mentioned such downstream tasks, the benefit in RL hassurprisingly not been investigated systematically thus far. Instead, most ofthe evaluations have focused on rather indirect metrics such as segmentationquality and object property prediction accuracy. In this paper, we investigatethe effectiveness of OCR pre-training for image-based reinforcement learningvia empirical experiments. For systematic evaluation, we introduce a simpleobject-centric visual RL benchmark and conduct experiments to answer questionssuch as ``Does OCR pre-training improve performance on object-centric tasks?''and ``Can OCR pre-training help with out-of-distribution generalization?''. Ourresults provide empirical evidence for valuable insights into the effectivenessof OCR pre-training for RL and the potential limitations of its use in certainscenarios. Additionally, this study also examines the critical aspects ofincorporating OCR pre-training in RL, including performance in a visuallycomplex environment and the appropriate pooling layer to aggregate the objectrepresentations.", "output": "An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "IoU losses are surrogates that directly optimize the Jaccard index. Insemantic segmentation, leveraging IoU losses as part of the loss function isshown to perform better with respect to the Jaccard index measure thanoptimizing pixel-wise losses such as the cross-entropy loss alone. The mostnotable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss.However, these losses are incompatible with soft labels which are ubiquitous inmachine learning. In this paper, we propose Jaccard metric losses (JMLs), whichare identical to the soft Jaccard loss in a standard setting with hard labels,but are compatible with soft labels. With JMLs, we study two of the mostpopular use cases of soft labels: label smoothing and knowledge distillation.With a variety of architectures, our experiments show significant improvementsover the cross-entropy loss on three semantic segmentation datasets(Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approachoutperforms state-of-the-art knowledge distillation methods by a large margin.Code is available at:href{", "output": "Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) are often used for tasks involving the geometryof a given graph, such as molecular dynamics simulation. Although the distancematrix of a geometric graph contains complete geometric information, it hasbeen demonstrated that Message Passing Neural Networks (MPNNs) are insufficientfor learning this geometry. In this work, we expand on the families ofcounterexamples that MPNNs are unable to distinguish from their distancematrices, by constructing families of novel and symmetric geometric graphs. Wethen propose $k$-DisGNNs, which can effectively exploit the rich geometrycontained in the distance matrix. We demonstrate the high expressive power ofour models and prove that some existing well-designed geometric models can beunified by $k$-DisGNNs as special cases. Most importantly, we establish aconnection between geometric deep learning and traditional graph representationlearning, showing that those highly expressive GNN models originally designedfor graph structure learning can also be applied to geometric deep learningproblems with impressive performance, and that existing complex, equivariantmodels are not the only solution. Experimental results verify our theory.", "output": "Is Distance Matrix Enough for Geometric Deep Learning?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class ofconditional diffusion models that directly learn the nonlinear diffusionprocesses between two given distributions. These diffusion bridges areparticularly useful for image restoration, as the degraded images arestructurally informative priors for reconstructing the clean images. I$^2$SBbelongs to a tractable class of Schr\"odinger bridge, the nonlinear extensionto score-based models, whose marginal distributions can be computedanalytically given boundary pairs. This results in a simulation-free frameworkfor nonlinear diffusions, where the I$^2$SB training becomes scalable byadopting practical techniques used in standard diffusion models. We validateI$^2$SB in solving various image restoration tasks, including inpainting,super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and showthat I$^2$SB surpasses standard conditional diffusion models with moreinterpretable generative processes. Moreover, I$^2$SB matches the performanceof inverse methods that additionally require the knowledge of the corruptionoperators. Our work opens up new algorithmic opportunities for developingefficient nonlinear diffusion models on a large scale. scale. Project page andcodes: ", "output": "I$^2$SB: Image-to-Image Schr\\\"odinger Bridge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The linear response of a photomultiplier tube (PMT) is a required propertyfor photon counting and reconstruction of the neutrino energy. The linearityvalid region and the saturation response of PMT were investigated using alinear-alkyl-benzene (LAB)-based liquid scintillator. A correlation wasobserved between the two different saturation responses, with pulse-shapedistortion and pulse-area decrease. The observed pulse-shape provides usefulinformation for the estimation of the linearity region relative to thepulse-area. This correlation-based diagnosis allows an ${in}$-${situ}$estimation of the linearity range, which was previously challenging. Themeasured correlation between the two saturation responses was employed to trainan artificial-neural-network (ANN) to predict the decrease in pulse-area fromthe observed pulse-shape. The ANN-predicted pulse-area decrease enables theprediction of the ideal number of photoelectrons irrelevant to the saturationbehavior. This pulse-shape-based machine learning technique offers a novelmethod for restoring the saturation response of PMTs.", "output": "Restoring the saturation response of a PMT using pulse-shape and artificial-neural-networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we combine the strengths of distance-based andclassification-based approaches for the task of identifying extended realityusers by their movements. For this we present an embedding-based approach thatleverages deep metric learning. We train the model on a dataset of usersplaying the VR game ``Half-Life: Alyx'' and conduct multiple experiments andanalyses using a state of the art classification-based model as baseline. Theresults show that the embedding-based method 1) is able to identify new usersfrom non-specific movements using only a few minutes of enrollment data, 2) canenroll new users within seconds, while retraining the baseline approach takesalmost a day, 3) is more reliable than the baseline approach when only littleenrollment data is available, 4) can be used to identify new users from anotherdataset recorded with different VR devices.Altogether, our solution is a foundation for easily extensible XR useridentification systems, applicable to a wide range of user motions. It alsopaves the way for production-ready models that could be used by XRpractitioners without the requirements of expertise, hardware, or data fortraining deep learning models.", "output": "Extensible Motion-based Identification of XR Users using Non-Specific Motion Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The dynamic scheduling of ultra-reliable and low-latency traffic (URLLC) inthe uplink can significantly enhance the efficiency of coexisting services,such as enhanced mobile broadband (eMBB) devices, by only allocating resourceswhen necessary. The main challenge is posed by the uncertainty in the processof URLLC packet generation, which mandates the use of predictors for URLLCtraffic in the coming frames. In practice, such prediction may overestimate orunderestimate the amount of URLLC data to be generated, yielding either anexcessive or an insufficient amount of resources to be pre-emptively allocatedfor URLLC packets. In this paper, we introduce a novel scheduler for URLLCpackets that provides formal guarantees on reliability and latency irrespectiveof the quality of the URLLC traffic predictor. The proposed method leveragesrecent advances in online conformal prediction (CP), and follows the principleof dynamically adjusting the amount of allocated resources so as to meetreliability and latency requirements set by the designer.", "output": "Guaranteed Dynamic Scheduling of Ultra-Reliable Low-Latency Traffic via Conformal Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contemporary graph learning algorithms are not well-defined for largemolecules since they do not consider the hierarchical interactions among theatoms, which are essential to determine the molecular properties ofmacromolecules. In this work, we propose Multiresolution Graph Transformers(MGT), the first graph transformer architecture that can learn to representlarge molecules at multiple scales. MGT can learn to produce representationsfor the atoms and group them into meaningful functional groups or repeatingunits. We also introduce Wavelet Positional Encoding (WavePE), a new positionalencoding method that can guarantee localization in both spectral and spatialdomains. Our proposed model achieves competitive results on two macromoleculedatasets consisting of polymers and peptides, and one drug-like moleculedataset. Importantly, our model outperforms other state-of-the-art methods andachieves chemical accuracy in estimating molecular properties (e.g., GAP, HOMOand LUMO) calculated by Density Functional Theory (DFT) in the polymersdataset. Furthermore, the visualizations, including clustering results onmacromolecules and low-dimensional spaces of their representations, demonstratethe capability of our methodology in learning to represent long-range andhierarchical structures. Our PyTorch implementation is publicly available at", "output": "Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Well-performed deep neural networks (DNNs) generally require massive labelleddata and computational resources for training. Various watermarking techniquesare proposed to protect such intellectual properties (IPs), wherein the DNNproviders implant secret information into the model so that they can laterclaim IP ownership by retrieving their embedded watermarks with some dedicatedtrigger inputs. While promising results are reported in the literature,existing solutions suffer from watermark removal attacks, such as modelfine-tuning and model pruning.In this paper, we propose a novel DNN watermarking solution that caneffectively defend against the above attacks. Our key insight is to enhance thecoupling of the watermark and model functionalities such that removing thewatermark would inevitably degrade the model's performance on normal inputs. Tothis end, unlike previous methods relying on secret features learnt fromout-of-distribution data, our method only uses features learnt fromin-distribution data. Specifically, on the one hand, we propose to sampleinputs from the original training dataset and fuse them as watermark triggers.On the other hand, we randomly mask model weights during training so that theinformation of our embedded watermarks spreads in the network. By doing so,model fine-tuning/pruning would not forget our function-coupled watermarks.Evaluation results on various image classification tasks show a 100% watermarkauthentication success rate under aggressive watermark removal attacks,significantly outperforming existing solutions. Code is available:", "output": "On Function-Coupled Watermarks for Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial intelligence-based methods have generated substantial interest innuclear medicine. An area of significant interest has been using deep-learning(DL)-based approaches for denoising images acquired with lower doses, shorteracquisition times, or both. Objective evaluation of these approaches isessential for clinical application. DL-based approaches for denoisingnuclear-medicine images have typically been evaluated using fidelity-basedfigures of merit (FoMs) such as RMSE and SSIM. However, these images areacquired for clinical tasks and thus should be evaluated based on theirperformance in these tasks. Our objectives were to (1) investigate whetherevaluation with these FoMs is consistent with objective clinical-task-basedevaluation; (2) provide a theoretical analysis for determining the impact ofdenoising on signal-detection tasks; (3) demonstrate the utility of virtualclinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate aDL-based method for denoising myocardial perfusion SPECT (MPS) images wasconducted. The impact of DL-based denoising was evaluated using fidelity-basedFoMs and AUC, which quantified performance on detecting perfusion defects inMPS images as obtained using a model observer with anthropomorphic channels.Based on fidelity-based FoMs, denoising using the considered DL-based methodled to significantly superior performance. However, based on ROC analysis,denoising did not improve, and in fact, often degraded detection-taskperformance. The results motivate the need for objective task-based evaluationof DL-based denoising approaches. Further, this study shows how VCTs provide amechanism to conduct such evaluations using VCTs. Finally, our theoreticaltreatment reveals insights into the reasons for the limited performance of thedenoising approach.", "output": "Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning has experienced significant growth in recent years, resultingin increased energy consumption and carbon emission from the use of GPUs fortraining deep neural networks (DNNs). Answering the call for sustainability,conventional solutions have attempted to move training jobs to locations ortime frames with lower carbon intensity. However, moving jobs to otherlocations may not always be feasible due to large dataset sizes or dataregulations. Moreover, postponing training can negatively impact applicationservice quality because the DNNs backing the service are not updated in atimely fashion. In this work, we present a practical solution that reduces thecarbon footprint of DNN training without migrating or postponing jobs.Specifically, our solution observes real-time carbon intensity shifts duringtraining and controls the energy consumption of GPUs, thereby reducing carbonfootprint while maintaining training performance. Furthermore, in order toproactively adapt to shifting carbon intensity, we propose a lightweightmachine learning algorithm that predicts the carbon intensity of the upcomingtime frame. Our solution, Chase, reduces the total carbon footprint of trainingResNet-50 on ImageNet by 13.6% while only increasing training time by 2.5%.", "output": "Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models are challenged by the distribution shift between thetraining data and test data. Recently, the large models pre-trained on diversedata have demonstrated unprecedented robustness to various distribution shifts.However, fine-tuning these models can lead to a trade-off betweenin-distribution (ID) performance and out-of-distribution (OOD) robustness.Existing methods for tackling this trade-off do not explicitly address the OODrobustness problem. In this paper, based on causal analysis of theaforementioned problems, we propose a novel fine-tuning method, which usesmasked images as counterfactual samples that help improve the robustness of thefine-tuning model. Specifically, we mask either the semantics-related orsemantics-unrelated patches of the images based on class activation map tobreak the spurious correlation, and refill the masked patches with patches fromother images. The resulting counterfactual samples are used in feature-baseddistillation with the pre-trained model. Extensive experiments verify thatregularizing the fine-tuning with the proposed masked images can achieve abetter trade-off between ID and OOD performance, surpassing previous methods onthe OOD performance. Our code is available at", "output": "Masked Images Are Counterfactual Samples for Robust Fine-tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series generation (TSG) studies have mainly focused on the use ofGenerative Adversarial Networks (GANs) combined with recurrent neural network(RNN) variants. However, the fundamental limitations and challenges of trainingGANs still remain. In addition, the RNN-family typically has difficulties withtemporal consistency between distant timesteps. Motivated by the successes inthe image generation (IMG) domain, we propose TimeVQVAE, the first work, to ourknowledge, that uses vector quantization (VQ) techniques to address the TSGproblem. Moreover, the priors of the discrete latent spaces are learned withbidirectional transformer models that can better capture global temporalconsistency. We also propose VQ modeling in a time-frequency domain, separatedinto low-frequency (LF) and high-frequency (HF). This allows us to retainimportant characteristics of the time series and, in turn, generate newsynthetic signals that are of better quality, with sharper changes inmodularity, than its competing TSG methods. Our experimental evaluation isconducted on all datasets from the UCR archive, using well-established metricsin the IMG literature, such as Fr'echet inception distance and inceptionscores. Our implementation on GitHub:url{", "output": "Vector Quantized Time Series Generation with a Bidirectional Prior Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider feasibility and constrained optimization problems defined oversmooth and/or strongly convex sets. These notions mirror their popular functioncounterparts but are much less explored in the first-order optimizationliterature. We propose new scalable, projection-free, accelerated first-ordermethods in these settings. Our methods avoid linear optimization or projectionoracles, only using cheap one-dimensional linesearches and normal vectorcomputations. Despite this, we derive optimal accelerated convergenceguarantees of $O(1/T)$ for strongly convex problems, $O(1/T^2)$ for smoothproblems, and accelerated linear convergence given both. Our algorithms andanalysis are based on novel characterizations of the Minkowski gauge of smoothand/or strongly convex sets, which may be of independent interest: although thegauge is neither smooth nor strongly convex, we show the gauge squared inheritsany structure present in the set.", "output": "Gauges and Accelerated Optimization over Smooth and/or Strongly Convex Sets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tensor decomposition is now being used for data analysis, informationcompression, and knowledge recovery. However, the mathematical property oftensor decomposition is not yet fully clarified because it is one of singularlearning machines. In this paper, we give the upper bound of its real logcanonical threshold (RLCT) of the tensor decomposition by using an algebraicgeometrical method and derive its Bayesian generalization error theoretically.We also give considerations about its mathematical property through numericalexperiments.", "output": "Upper Bound of Real Log Canonical Threshold of Tensor Decomposition and its Application to Bayesian Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rendering and inverse-rendering algorithms that drive conventional computergraphics have recently been superseded by neural representations (NR). NRs haverecently been used to learn the geometric and the material properties of thescenes and use the information to synthesize photorealistic imagery, therebypromising a replacement for traditional rendering algorithms with scalablequality and predictable performance. In this work we ask the question: Doesneural graphics (NG) need hardware support? We studied representative NGapplications showing that, if we want to render 4k res. at 60FPS there is a gapof 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,there is an even larger gap of 2-4 OOM between the desired performance and therequired system power. We identify that the input encoding and the MLP kernelsare the performance bottlenecks, consuming 72%,60% and 59% of application timefor multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,respectively. We propose a NG processing cluster, a scalable and flexiblehardware architecture that directly accelerates the input encoding and MLPkernels through dedicated engines and supports a wide range of NG applications.We also accelerate the rest of the kernels by fusing them together in Vulkan,which leads to 9.94X kernel-level performance improvement compared to un-fusedimplementation of the pre-processing and the post-processing kernels. Ourresults show that, NGPC gives up to 58X end-to-end application-levelperformance improvement, for multi res. hashgrid encoding on average across thefour NG applications, the performance benefits are 12X,20X,33X and 39X for thescaling factor of 8,16,32 and 64, respectively. Our results show that withmulti res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPSfor NeRF and 8k res. at 120FPS for all our other NG applications.", "output": "Hardware Acceleration of Neural Graphics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While the acquisition of time series has become more straightforward,developing dynamical models from time series is still a challenging andevolving problem domain. Within the last several years, to address thisproblem, there has been a merging of machine learning tools with what is calledthe dynamic mode decomposition (DMD). This general approach has been shown tobe an especially promising avenue for accurate model development. Building onthis prior body of work, we develop a deep learning DMD based method whichmakes use of the fundamental insight of Takens' Embedding Theorem to build anadaptive learning scheme that better approximates higher dimensional andchaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). Welikewise explore how our method learns mappings which tend, after successfultraining, to significantly change the mutual information between dimensions inthe dynamics. This appears to be a key feature in enhancing the DMD overall,and it should help provide further insight for developing other deep learningmethods for time series analysis and model generation.", "output": "Machine Learning Enhanced Hankel Dynamic-Mode Decomposition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Partial differential equations (PDEs) are central to describing and modellingcomplex physical systems that arise in many disciplines across science andengineering. However, in many realistic applications PDE modelling provides anincomplete description of the physics of interest. PDE-based machine learningtechniques are designed to address this limitation. In this approach, the PDEis used as an inductive bias enabling the coupled model to rely on fundamentalphysical laws while requiring less training data. The deployment ofhigh-performance simulations coupling PDEs and machine learning to complexproblems necessitates the composition of capabilities provided by machinelearning and PDE-based frameworks. We present a simple yet effective couplingbetween the machine learning framework PyTorch and the PDE system Firedrakethat provides researchers, engineers and domain specialists with a highproductive way of specifying coupled models while only requiring trivialchanges to existing code.", "output": "Physics-driven machine learning models coupling PyTorch and Firedrake."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Protein-protein interactions are essential in biochemical processes. Accurateprediction of the protein-protein interaction sites (PPIs) deepens ourunderstanding of biological mechanism and is crucial for new drug design.However, conventional experimental methods for PPIs prediction are costly andtime-consuming so that many computational approaches, especially ML-basedmethods, have been developed recently. Although these approaches have achievedgratifying results, there are still two limitations: (1) Most models haveexcavated some useful input features, but failed to take coevolutionaryfeatures into account, which could provide clues for inter-residuerelationships; (2) The attention-based models only allocate attention weightsfor neighboring residues, instead of doing it globally, neglecting that someresidues being far away from the target residues might also matter.We propose a coevolution-enhanced global attention neural network, asequence-based deep learning model for PPIs prediction, called CoGANPPIS. Itutilizes three layers in parallel for feature extraction: (1) Local-levelrepresentation aggregation layer, which aggregates the neighboring residues'features; (2) Global-level representation learning layer, which employs a novelcoevolution-enhanced global attention mechanism to allocate attention weightsto all the residues on the same protein sequences; (3) Coevolutionaryinformation learning layer, which applies CNN &amp; pooling to coevolutionaryinformation to obtain the coevolutionary profile representation. Then, thethree outputs are concatenated and passed into several fully connected layersfor the final prediction. Application on two benchmark datasets demonstrated astate-of-the-art performance of our model. The source code is publiclyavailable at ", "output": "CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Digital Twins (DTs) for physical wireless environments have been recentlyproposed as accurate virtual representations of the propagation environmentthat can enable multi-layer decisions at the physical communication equipment.At high frequency bands, DTs can help to overcome the challenges emerging inthe high mobility conditions featuring vehicular environments. In this paper,we propose a novel data-driven workflow for the creation of the DT of aVehicle-to-Everything (V2X) communication scenario and a multi-modal simulationframework for the generation of realistic sensor data and accuratemmWave/sub-THz wireless channels. The proposed method leverages an automotivesimulation and testing framework based on the Unreal Engine game engine and anaccurate ray-tracing channel simulator. Simulations over an urban scenario showthe achievable realistic sensor and channel modelling both at theinfrastructure and at an ego-vehicle.", "output": "A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X Communications in Dynamic Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning methods are commonly evaluated and compared by theirperformance on data sets from public repositories. This allows for multiplemethods, oftentimes several thousands, to be evaluated under identicalconditions and across time. The highest ranked performance on a problem isreferred to as state-of-the-art (SOTA) performance, and is used, among otherthings, as a reference point for publication of new methods. Using thehighest-ranked performance as an estimate for SOTA is a biased estimator,giving overly optimistic results. The mechanisms at play are those ofmultiplicity, a topic that is well-studied in the context of multiplecomparisons and multiple testing, but has, as far as the authors are aware of,been nearly absent from the discussion regarding SOTA estimates. The optimisticstate-of-the-art estimate is used as a standard for evaluating new methods, andmethods with substantial inferior results are easily overlooked. In thisarticle, we provide a probability distribution for the case of multipleclassifiers so that known analyses methods can be engaged and a better SOTAestimate can be provided. We demonstrate the impact of multiplicity through asimulated example with independent classifiers. We show how classifierdependency impacts the variance, but also that the impact is limited when theaccuracy is high. Finally, we discuss a real-world example; a Kagglecompetition from 2020.", "output": "What is the state of the art? Accounting for multiplicity in machine learning benchmark performance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This survey reviews text-to-image diffusion models in the context thatdiffusion models have emerged to be popular for a wide range of generativetasks. As a self-contained work, this survey starts with a brief introductionof how a basic diffusion model works for image synthesis, followed by howcondition or guidance improves learning. Based on that, we present a review ofstate-of-the-art methods on text-conditioned image synthesis, i.e.,text-to-image. We further summarize applications beyond text-to-imagegeneration: text-guided creative generation and text-guided image editing.Beyond the progress made so far, we discuss existing challenges and promisingfuture directions.", "output": "Text-to-image Diffusion Models in Generative AI: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We compared the efficiency of the FlyHash model, an insect-inspired sparseneural network (Dasgupta et al., 2017), to similar but non-sparse models in anembodied navigation task. This requires a model to control steering bycomparing current visual inputs to memories stored along a training route. Weconcluded the FlyHash model is more efficient than others, especially in termsof data encoding.", "output": "Vision-based route following by an embodied insect-inspired sparse neural network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lithography is fundamental to integrated circuit fabrication, necessitatinglarge computation overhead. The advancement of machine learning (ML)-basedlithography models alleviates the trade-offs between manufacturing processexpense and capability. However, all previous methods regard the lithographysystem as an image-to-image black box mapping, utilizing network parameters tolearn by rote mappings from massive mask-to-aerial or mask-to-resist imagepairs, resulting in poor generalization capability. In this paper, we propose anew ML-based paradigm disassembling the rigorous lithographic model intonon-parametric mask operations and learned optical kernels containingdeterminant source, pupil, and lithography information. By optimizingcomplex-valued neural fields to perform optical kernel regression fromcoordinates, our method can accurately restore lithography system using asmall-scale training dataset with fewer parameters, demonstrating superiorgeneralization capability as well. Experiments show that our framework can use31% of parameters while achieving 69$times$ smaller mean squared error with1.3$times$ higher throughput than the state-of-the-art.", "output": "Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the popularity of the recent Transformer-based models represented byBERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a rangeof natural language processing tasks. However, the massive computations, hugememory footprint, and thus high latency of Transformer-based models is aninevitable challenge for the cloud with high real-time requirement. To tacklethe issue, we propose BBCT, a method of block-wise bit-compression fortransformer without retraining. Our method achieves more fine-grainedcompression of the whole transformer, including embedding, matrixmultiplication, GELU, softmax, layer normalization, and all the intermediateresults. As a case, we compress an efficient BERT with the method of BBCT. Ourbenchmark test results on General Language Understanding Evaluation (GLUE) showthat BBCT can achieve less than 1% accuracy drop in most tasks.", "output": "Block-wise Bit-Compression of Transformer-based Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many imaging inverse problems$unicode{x2014}$such as image-dependentin-painting and dehazing$unicode{x2014}$are challenging because their forwardmodels are unknown or depend on unknown latent parameters. While one can solvesuch problems by training a neural network with vast quantities of pairedtraining data, such paired training data is often unavailable. In this paper,we propose a generalized framework for training image reconstruction networkswhen paired training data is scarce. In particular, we demonstrate the abilityof image denoising algorithms and, by extension, denoising diffusion models tosupervise network training in the absence of paired training data.", "output": "SUD$^2$: Supervision by Denoising Diffusion Models for Image Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence (AI) is making a profound impact in almost everydomain. A vital enabler of its great success is the availability of abundantand high-quality data for building machine learning models. Recently, the roleof data in AI has been significantly magnified, giving rise to the emergingconcept of data-centric AI. The attention of researchers and practitioners hasgradually shifted from advancing model design to enhancing the quality andquantity of the data. In this survey, we discuss the necessity of data-centricAI, followed by a holistic view of three general data-centric goals (trainingdata development, inference data development, and data maintenance) and therepresentative methods. We also organize the existing literature fromautomation and collaboration perspectives, discuss the challenges, and tabulatethe benchmarks for various tasks. We believe this is the first comprehensivesurvey that provides a global view of a spectrum of tasks across various stagesof the data lifecycle. We hope it can help the readers efficiently grasp abroad picture of this field, and equip them with the techniques and furtherresearch ideas to systematically engineer data for building AI systems. Acompanion list of data-centric AI resources will be regularly updated on", "output": "Data-centric Artificial Intelligence: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated systems enable collaborative training on highly heterogeneous datathrough model personalization, which can be facilitated by employing multi-tasklearning algorithms. However, significant variation in device computingcapabilities may result in substantial degradation in the convergence rate oftraining. To accelerate the learning procedure for diverse participants in amulti-task federated setting, more efficient and robust methods need to bedeveloped. In this paper, we design an efficient iterative distributed methodbased on the alternating direction method of multipliers (ADMM) for supportvector machines (SVMs), which tackles federated classification and regression.The proposed method utilizes efficient computations and model exchange in anetwork of heterogeneous nodes and allows personalization of the learning modelin the presence of non-i.i.d. data. To further enhance privacy, we introduce arandom mask procedure that helps avoid data inversion. Finally, we analyze theimpact of the proposed privacy mechanisms and participant hardware and dataheterogeneity on the system performance.", "output": "Multi-Task Model Personalization for Federated Supervised SVM in Heterogeneous Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Calibration of deep learning models is crucial to their trustworthiness andsafe usage, and as such, has been extensively studied in supervisedclassification models, with methods crafted to decrease miscalibration.However, there has yet to be a comprehensive study of the calibration ofvision-language models that are used for zero-shot inference, like CLIP. Wemeasure calibration across relevant variables like prompt, dataset, andarchitecture, and find that zero-shot inference with CLIP is miscalibrated.Furthermore, we propose a modified version of temperature scaling that isaligned with the common use cases of CLIP as a zero-shot inference model, andshow that a single learned temperature generalizes for each specific CLIP model(defined by a chosen pre-training dataset and architecture) across inferencedataset and prompt choice.", "output": "Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training-time defenses, known as adversarial training, incur high trainingcosts and do not generalize to unseen attacks. Test-time defenses solve theseissues but most existing test-time defenses require adapting the model weights,therefore they do not work on frozen models and complicate model memorymanagement. The only test-time defense that does not adapt model weights aimsto adapt the input with self-supervision tasks. However, we empirically foundthese self-supervision tasks are not sensitive enough to detect adversarialattacks accurately. In this paper, we propose DRAM, a novel defense method todetect and repair adversarial samples at test time via Masked autoencoder(MAE). We demonstrate how to use MAE losses to build a Kolmogorov-Smirnov testto detect adversarial samples. Moreover, we use the MAE losses to calculateinput reversal vectors that repair adversarial samples resulting frompreviously unseen attacks. Results on large-scale ImageNet dataset show that,compared to all detection baselines evaluated, DRAM achieves the best detectionrate (82% on average) on all eight adversarial attacks evaluated. For attackrepair, DRAM improves the robust accuracy by 6% ~ 41% for standard ResNet50 and3% ~ 8% for robust ResNet50 compared with the baselines that use contrastivelearning and rotation prediction.", "output": "Test-time Detection and Repair of Adversarial Samples via Masked Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative AI has demonstrated impressive performance in various fields,among which speech synthesis is an interesting direction. With the diffusionmodel as the most popular generative model, numerous works have attempted twoactive tasks: text to speech and speech enhancement. This work conducts asurvey on audio diffusion model, which is complementary to existing surveysthat either lack the recent progress of diffusion-based speech synthesis orhighlight an overall picture of applying diffusion model in multiple fields.Specifically, this work first briefly introduces the background of audio anddiffusion model. As for the text-to-speech task, we divide the methods intothree categories based on the stage where diffusion model is adopted: acousticmodel, vocoder and end-to-end framework. Moreover, we categorize various speechenhancement tasks by either certain signals are removed or added into the inputspeech. Comparisons of experimental results and discussions are also covered inthis survey.", "output": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Regular physics-informed neural networks (PINNs) predict the solution ofpartial differential equations using sparse labeled data but only over a singledomain. On the other hand, fully supervised learning models are first trainedusually over a few thousand domains with known solutions (i.e., labeled data)and then predict the solution over a few hundred unseen domains.Physics-informed PointNet (PIPN) is primarily designed to fill this gap betweenPINNs (as weakly supervised learning models) and fully supervised learningmodels. In this article, we demonstrate that PIPN predicts the solution ofdesired partial differential equations over a few hundred domainssimultaneously, while it only uses sparse labeled data. This framework benefitsfast geometric designs in the industry when only sparse labeled data areavailable. Particularly, we show that PIPN predicts the solution of a planestress problem over more than 500 domains with different geometries,simultaneously. Moreover, we pioneer implementing the concept of remarkablebatch size (i.e., the number of geometries fed into PIPN at each sub-epoch)into PIPN. Specifically, we try batch sizes of 7, 14, 19, 38, 76, and 133.Additionally, the effect of the PIPN size, symmetric function in the PIPNarchitecture, and static and dynamic weights for the component of the sparselabeled data in the loss function are investigated.", "output": "Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of data attribution is to trace model predictions back to trainingdata. Despite a long line of work towards this goal, existing approaches todata attribution tend to force users to choose between computationaltractability and efficacy. That is, computationally tractable methods canstruggle with accurately attributing model predictions in non-convex settings(e.g., in the context of deep neural networks), while methods that areeffective in such regimes require training thousands of models, which makesthem impractical for large models or datasets.In this work, we introduce TRAK (Tracing with the Randomly-projected AfterKernel), a data attribution method that is both effective and computationallytractable for large-scale, differentiable models. In particular, by leveragingonly a handful of trained models, TRAK can match the performance of attributionmethods that require training thousands of models. We demonstrate the utilityof TRAK across various modalities and scales: image classifiers trained onImageNet, vision-language models (CLIP), and language models (BERT and mT5). Weprovide code for using TRAK (and reproducing our work) at .", "output": "TRAK: Attributing Model Behavior at Scale."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Catastrophic forgetting has been a major challenge in continual learning,where the model needs to learn new tasks with limited or no access to data frompreviously seen tasks. To tackle this challenge, methods based on knowledgedistillation in feature space have been proposed and shown to reduceforgetting. However, most feature distillation methods directly constrain thenew features to match the old ones, overlooking the need for plasticity. Toachieve a better stability-plasticity trade-off, we propose Backward FeatureProjection (BFP), a method for continual learning that allows the new featuresto change up to a learnable linear transformation of the old features. BFPpreserves the linear separability of the old classes while allowing theemergence of new feature directions to accommodate new classes. BFP can beintegrated with existing experience replay methods and boost performance by asignificant margin. We also demonstrate that BFP helps learn a betterrepresentation space, in which linear separability is well preserved duringcontinual learning and linear probing achieves high classification accuracy.The code can be found at ", "output": "Preserving Linear Separability in Continual Learning by Backward Feature Projection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) has recently emerged as a popular framework, whichallows resource-constrained discrete clients to cooperatively learn the globalmodel under the orchestration of a central server while storingprivacy-sensitive data locally. However, due to the difference in equipment anddata divergence of heterogeneous clients, there will be parameter deviationbetween local models, resulting in a slow convergence rate and a reduction ofthe accuracy of the global model. The current FL algorithms use the staticclient learning strategy pervasively and can not adapt to the dynamic trainingparameters of different clients. In this paper, by considering the deviationbetween different local model parameters, we propose an adaptive learning ratescheme for each client based on entropy theory to alleviate the deviationbetween heterogeneous clients and achieve fast convergence of the global model.It's difficult to design the optimal dynamic learning rate for each client asthe local information of other clients is unknown, especially during the localtraining epochs without communications between local clients and the centralserver. To enable a decentralized learning rate design for each client, wefirst introduce mean-field schemes to estimate the terms related to otherclients' local model parameters. Then the decentralized adaptive learning ratefor each client is obtained in closed form by constructing the Hamiltonequation. Moreover, we prove that there exist fixed point solutions for themean-field estimators, and an algorithm is proposed to obtain them. Finally,extensive experimental results on real datasets show that our algorithm caneffectively eliminate the deviation between local model parameters compared toother recent FL algorithms.", "output": "Adaptive Federated Learning via New Entropy Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) is a novel machine learning framework, which enablesmultiple distributed devices cooperatively to train a shared model scheduled bya central server while protecting private data locally. However, thenon-independent-and-identically-distributed (Non-IID) data samples and frequentcommunication across participants may significantly slow down the convergentrate and increase communication costs. To achieve fast convergence, weameliorate the conventional local updating rule by introducing the aggregatedgradients at each local update epoch, and propose an adaptive learning ratealgorithm that further takes the deviation of local parameter and globalparameter into consideration. The above adaptive learning rate design requiresall clients' local information including the local parameters and gradients,which is challenging as there is no communication during the local updateepochs. To obtain a decentralized adaptive learning rate for each client, weutilize the mean field approach by introducing two mean field terms to estimatethe average local parameters and gradients respectively, which does not requirethe clients to exchange their local information with each other at each localepoch. Numerical results show that our proposed framework is superior to thestate-of-art FL schemes in both model accuracy and convergent rate for IID andNon-IID datasets.", "output": "Fast Convergent Federated Learning with Aggregated Gradients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Road network digital twins (RNDTs) play a critical role in the development ofnext-generation intelligent transportation systems, enabling more precisetraffic planning and control. To support just-in-time (JIT) decision making,RNDTs require a model that dynamically learns the traffic patterns from onlinesensor data and generates high-fidelity simulation results. Although currenttraffic prediction techniques based on graph neural networks have achievedstate-of-the-art performance, these techniques only predict future traffic bymining correlations in historical traffic data, disregarding the causes oftraffic generation, such as Origin-Destination (OD) demands and routeselection. Therefore, their performance is unreliable for JIT decision making.To fill this gap, we introduce a novel deep learning framework called TraffNetthat learns the causality of traffic volumes from vehicle trajectory data.First, we use a heterogeneous graph to represent the road network, allowing themodel to incorporate causal features of traffic volumes. Next, inspired by thetraffic domain knowledge, we propose a traffic causality learning method tolearn an embedding vector that encodes OD demands and path-level dependenciesfor each road segment. Then, we model temporal dependencies to match theunderlying process of traffic generation. Finally, the experiments verify theutility of TraffNet. The code of TraffNet is available at", "output": "TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this article, we consider the manifold learning problem when the data setis invariant under the action of a compact Lie group $K$. Our approach consistsin augmenting the data-induced graph Laplacian by integrating over the$K$-orbits of the existing data points, which yields a $K$-invariant graphLaplacian $L$. We prove that $L$ can be diagonalized by using the unitaryirreducible representation matrices of $K$, and we provide an explicit formulafor computing its eigenvalues and eigenfunctions. In addition, we show that thenormalized Laplacian operator $L_N$ converges to the Laplace-Beltrami operatorof the data manifold with an improved convergence rate, where the improvementgrows with the dimension of the symmetry group $K$. This work extends thesteerable graph Laplacian framework of Landa and Shkolnisky from the case of$operatorname{SO}(2)$ to arbitrary compact Lie groups.", "output": "Diffusion Maps for Group-Invariant Manifolds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep ensemble is a simple and straightforward approach for approximatingBayesian inference and has been successfully applied to many classificationtasks. This study aims to comprehensively investigate this approach in themulti-output regression task to predict the aerodynamic performance of amissile configuration. By scrutinizing the effect of the number of neuralnetworks used in the ensemble, an obvious trend toward underconfidence inestimated uncertainty is observed. In this context, we propose the deepensemble framework that applies the post-hoc calibration method, and itsimproved uncertainty quantification performance is demonstrated. It is comparedwith Gaussian process regression, the most prevalent model for uncertaintyquantification in engineering, and is proven to have superior performance interms of regression accuracy, reliability of estimated uncertainty, andtraining efficiency. Finally, the impact of the suggested framework on theresults of Bayesian optimization is examined, showing that whether or not thedeep ensemble is calibrated can result in completely different explorationcharacteristics. This framework can be seamlessly applied and extended to anyregression task, as no special assumptions have been made for the specificproblem used in this study.", "output": "Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The soft Dice loss (SDL) has taken a pivotal role in many automatedsegmentation pipelines in the medical imaging community. Over the last years,some reasons behind its superior functioning have been uncovered and furtheroptimizations have been explored. However, there is currently no implementationthat supports its direct use in settings with soft labels. Hence, a synergybetween the use of SDL and research leveraging the use of soft labels, also inthe context of model calibration, is still missing. In this work, we introduceDice semimetric losses (DMLs), which (i) are by design identical to SDL in astandard setting with hard labels, but (ii) can be used in settings with softlabels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirmthe potential synergy of DMLs with soft labels (e.g. averaging, labelsmoothing, and knowledge distillation) over hard labels (e.g. majority votingand random selection). As a result, we obtain superior Dice scores and modelcalibration, which supports the wider adoption of DMLs in practice. Code isavailable athref{", "output": "Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, graph pre-training has attracted wide research attention, whichaims to learn transferable knowledge from unlabeled graph data so as to improvedownstream performance. Despite these recent attempts, the negative transfer isa major issue when applying graph pre-trained models to downstream tasks.Existing works made great efforts on the issue of what to pre-train and how topre-train by designing a number of graph pre-training and fine-tuningstrategies. However, there are indeed cases where no matter how advanced thestrategy is, the \"pre-train and fine-tune\" paradigm still cannot achieve clearbenefits. This paper introduces a generic framework W2PGNN to answer thecrucial question of when to pre-train (i.e., in what situations could we takeadvantage of graph pre-training) before performing effortful pre-training orfine-tuning. We start from a new perspective to explore the complex generativemechanisms from the pre-training data to downstream data. In particular, W2PGNNfirst fits the pre-training data into graphon bases, each element of graphonbasis (i.e., a graphon) identifies a fundamental transferable pattern shared bya collection of pre-training graphs. All convex combinations of graphon basesgive rise to a generator space, from which graphs generated form the solutionspace for those downstream data that can benefit from pre-training. In thismanner, the feasibility of pre-training can be quantified as the generationprobability of the downstream data from any generator in the generator space.W2PGNN provides three broad applications, including providing the applicationscope of graph pre-trained models, quantifying the feasibility of performingpre-training, and helping select pre-training data to enhance downstreamperformance. We give a theoretically sound solution for the first applicationand extensive empirical justifications for the latter two applications.", "output": "When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Owing to their remarkable learning (and relearning) capabilities, deep neuralnetworks (DNNs) find use in numerous real-world applications. However, thelearning of these data-driven machine learning models is generally as good asthe data available to them for training. Hence, training datasets withlong-tail distribution pose a challenge for DNNs, since the DNNs trained onthem may provide a varying degree of classification performance acrossdifferent output classes. While the overall bias of such networks is alreadyhighlighted in existing works, this work identifies the node bias that leads toa varying sensitivity of the nodes for different output classes. To the best ofour knowledge, this is the first work highlighting this unique challenge inDNNs, discussing its probable causes, and providing open challenges for thisnew research direction. We support our reasoning using an empirical case studyof the networks trained on a real-world dataset.", "output": "Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution in trained DNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networkshave proven to be highly effective, achieving state-of-the-art results. Thisstudy introduces a novel streaming architecture based toolflow for mapping suchmodels onto FPGAs considering the model's inherent characteristics and thefeatures of the targeted FPGA device. The HARFLOW3D toolflow takes as input a3D CNN in ONNX format and a description of the FPGA characteristics, generatinga design that minimizes the latency of the computation. The toolflow iscomprised of a number of parts, including i) a 3D CNN parser, ii) a performanceand resource model, iii) a scheduling algorithm for executing 3D models on thegenerated hardware, iv) a resource-aware optimization engine tailored for 3Dmodels, v) an automated mapping to synthesizable code for FPGAs. The ability ofthe toolflow to support a broad range of models and devices is shown through anumber of experiments on various 3D CNN and FPGA system pairs. Furthermore, thetoolflow has produced high-performing results for 3D CNN models that have notbeen mapped to FPGAs before, demonstrating the potential of FPGA-based systemsin this space. Overall, HARFLOW3D has demonstrated its ability to delivercompetitive latency compared to a range of state-of-the-art hand-tunedapproaches being able to achieve up to 5$times$ better performance compared tosome of the existing works.", "output": "HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an artificial intelligence system to remotely assess the motorperformance of individuals with Parkinson's disease (PD). Participantsperformed a motor task (i.e., tapping fingers) in front of a webcam, and datafrom 250 global participants were rated by three expert neurologists followingthe Movement Disorder Society Unified Parkinson's Disease Rating Scale(MDS-UPDRS). The neurologists' ratings were highly reliable, with anintra-class correlation coefficient (ICC) of 0.88. We developed computeralgorithms to obtain objective measurements that align with the MDS-UPDRSguideline and are strongly correlated with the neurologists' ratings. Ourmachine learning model trained on these measures outperformed an MDS-UPDRScertified rater, with a mean absolute error (MAE) of 0.59 compared to therater's MAE of 0.79. However, the model performed slightly worse than theexpert neurologists (0.53 MAE). The methodology can be replicated for similarmotor tasks, providing the possibility of evaluating individuals with PD andother movement disorders remotely, objectively, and in areas with limitedaccess to neurological care.", "output": "Using AI to Measure Parkinson's Disease Severity at Home."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solving complicated AI tasks with different domains and modalities is a keystep toward advanced artificial intelligence. While there are abundant AImodels available for different domains and modalities, they cannot handlecomplicated AI tasks. Considering large language models (LLMs) have exhibitedexceptional ability in language understanding, generation, interaction, andreasoning, we advocate that LLMs could act as a controller to manage existingAI models to solve complicated AI tasks and language could be a genericinterface to empower this. Based on this philosophy, we present HuggingGPT, aframework that leverages LLMs (e.g., ChatGPT) to connect various AI models inmachine learning communities (e.g., Hugging Face) to solve AI tasks.Specifically, we use ChatGPT to conduct task planning when receiving a userrequest, select models according to their function descriptions available inHugging Face, execute each subtask with the selected AI model, and summarizethe response according to the execution results. By leveraging the stronglanguage capability of ChatGPT and abundant AI models in Hugging Face,HuggingGPT is able to cover numerous sophisticated AI tasks in differentmodalities and domains and achieve impressive results in language, vision,speech, and other challenging tasks, which paves a new way towards advancedartificial intelligence.", "output": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce an adaptive refinement procedure for smart, and scalableabstraction of dynamical systems. Our technique relies on partitioning thestate space depending on the observation of future outputs. However, thisknowledge is dynamically constructed in an adaptive, asymmetric way. In orderto learn the optimal structure, we define a Kantorovich-inspired metric betweenMarkov chains, and we use it as a loss function. Our technique is prone todata-driven frameworks, but not restricted to.We also study properties of the above mentioned metric between Markov chains,which we believe could be of application for wider purpose. We propose analgorithm to approximate it, and we show that our method yields a much bettercomputational complexity than using classical linear programming techniques.", "output": "Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version]."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a methodology to align a medium-sized GPT model,originally trained in English for an open domain, to a small closed domain inSpanish. The application for which the model is finely tuned is the questionanswering task. To achieve this we also needed to train and implement anotherneural network (which we called the reward model) that could score anddetermine whether an answer is appropriate for a given question. This componentserved to improve the decoding and generation of the answers of the system.Numerical metrics such as BLEU and perplexity were used to evaluate the model,and human judgment was also used to compare the decoding technique with others.Finally, the results favored the proposed method, and it was determined that itis feasible to use a reward model to align the generation of responses.", "output": "Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a second-order method, the Natural Gradient Descent (NGD) has the abilityto accelerate training of neural networks. However, due to the prohibitivecomputational and memory costs of computing and inverting the FisherInformation Matrix (FIM), efficient approximations are necessary to make NGDscalable to Deep Neural Networks (DNNs). Many such approximations have beenattempted. The most sophisticated of these is KFAC, which approximates the FIMas a block-diagonal matrix, where each block corresponds to a layer of theneural network. By doing so, KFAC ignores the interactions between differentlayers. In this work, we investigate the interest of restoring somelow-frequency interactions between the layers by means of two-level methods.Inspired from domain decomposition, several two-level corrections to KFAC usingdifferent coarse spaces are proposed and assessed. The obtained results showthat incorporating the layer interactions in this fashion does not reallyimprove the performance of KFAC. This suggests that it is safe to discard theoff-diagonal blocks of the FIM, since the block-diagonal approach issufficiently robust, accurate and economical in computation time.", "output": "Analysis and Comparison of Two-Level KFAC Methods for Training Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised Continual learning involves updating a deep neural network (DNN)from an ever-growing stream of labeled data. While most work has focused onovercoming catastrophic forgetting, one of the major motivations behindcontinual learning is being able to efficiently update a network with newinformation, rather than retraining from scratch on the training dataset as itgrows over time. Despite recent continual learning methods largely solving thecatastrophic forgetting problem, there has been little attention paid to theefficiency of these algorithms. Here, we study recent methods for incrementalclass learning and illustrate that many are highly inefficient in terms ofcompute, memory, and storage. Some methods even require more compute thantraining from scratch! We argue that for continual learning to have real-worldapplicability, the research community cannot ignore the resources used by thesealgorithms. There is more to continual learning than mitigating catastrophicforgetting.", "output": "How Efficient Are Today's Continual Learning Algorithms?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data privacy and ownership are significant in social data science, raisinglegal and ethical concerns. Sharing and analyzing data is difficult whendifferent parties own different parts of it. An approach to this challenge isto apply de-identification or anonymization techniques to the data beforecollecting it for analysis. However, this can reduce data utility and increasethe risk of re-identification. To address these limitations, we present PADME,a distributed analytics tool that federates model implementation and training.PADME uses a federated approach where the model is implemented and deployed byall parties and visits each data location incrementally for training. Thisenables the analysis of data across locations while still allowing the model tobe trained as if all data were in a single location. Training the model on datain its original location preserves data ownership. Furthermore, the results arenot provided until the analysis is completed on all data locations to ensureprivacy and avoid bias in the results.", "output": "PADME-SoSci: A Platform for Analytics and Distributed Machine Learning for the Social Sciences."}]