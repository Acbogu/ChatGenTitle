[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Aortic stenosis (AS) is a degenerative valve condition that causessubstantial morbidity and mortality. This condition is under-diagnosed andunder-treated. In clinical practice, AS is diagnosed with expert review oftransthoracic echocardiography, which produces dozens of ultrasound images ofthe heart. Only some of these views show the aortic valve. To automatescreening for AS, deep networks must learn to mimic a human expert's ability toidentify views of the aortic valve then aggregate across these relevant imagesto produce a study-level diagnosis. We find previous approaches to AS detectionyield insufficient accuracy due to relying on inflexible averages acrossimages. We further find that off-the-shelf attention-based multiple instancelearning (MIL) performs poorly. We contribute a new end-to-end MIL approachwith two key methodological innovations. First, a supervised attentiontechnique guides the learned attention mechanism to favor relevant views.Second, a novel self-supervised pretraining strategy applies contrastivelearning on the representation of the whole study instead of individual imagesas commonly done in prior literature. Experiments on an open-access dataset andan external validation set show that our approach yields higher accuracy whilereducing model size.", "output": "Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One prevalent property we find empirically in real-world graph anomalydetection (GAD) datasets is a one-class homophily, i.e., normal nodes tend tohave strong connection/affinity with each other, while the homophily inabnormal nodes is significantly weaker than normal nodes. However, thisanomaly-discriminative property is ignored by existing GAD methods that aretypically built using a conventional anomaly detection objective, such as datareconstruction. In this work, we explore this property to introduce a novelunsupervised anomaly scoring measure for GAD -- local node affinity -- thatassigns a larger anomaly score to nodes that are less affiliated with theirneighbors, with the affinity defined as similarity on nodeattributes/representations. We further propose Truncated Affinity Maximization(TAM) that learns tailored node representations for our anomaly measure bymaximizing the local affinity of nodes to their neighbors. Optimizing on theoriginal graph structure can be biased by non-homophily edges (i.e., edgesconnecting normal and abnormal nodes). Thus, TAM is instead optimized ontruncated graphs where non-homophily edges are removed iteratively to mitigatethis bias. The learned representations result in significantly stronger localaffinity for normal nodes than abnormal nodes. Extensive empirical results onsix real-world GAD datasets show that TAM substantially outperforms sevencompeting models, achieving over 10% increase in AUROC/AUPRC compared to thebest contenders on challenging datasets. Our code will be made available at //github.com/mala-lab/TAM-master/.", "output": "Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Simplicial map neural networks (SMNNs) are topology-based neural networkswith interesting properties such as universal approximation capability androbustness to adversarial examples under appropriate conditions. However, SMNNspresent some bottlenecks for their possible application in high dimensions.First, no SMNN training process has been defined so far. Second, SMNNs requirethe construction of a convex polytope surrounding the input dataset. In thispaper, we propose a SMNN training procedure based on a support subset of thegiven dataset and a method based on projection to a hypersphere as areplacement for the convex polytope construction. In addition, theexplainability capacity of SMNNs is also introduced for the first time in thispaper.", "output": "Explainability in Simplicial Map Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the era of big data, there has been a surge in the availability of datacontaining rich spatial and temporal information, offering valuable insightsinto dynamic systems and processes for applications such as weatherforecasting, natural disaster management, intelligent transport systems, andprecision agriculture. Graph neural networks (GNNs) have emerged as a powerfultool for modeling and understanding data with dependencies to each other suchas spatial and temporal dependencies. There is a large amount of existing workthat focuses on addressing the complex spatial and temporal dependencies inspatiotemporal data using GNNs. However, the strong interdisciplinary nature ofspatiotemporal data has created numerous GNNs variants specifically designedfor distinct application domains. Although the techniques are generallyapplicable across various domains, cross-referencing these methods remainsessential yet challenging due to the absence of a comprehensive literaturereview on GNNs for spatiotemporal data. This article aims to provide asystematic and comprehensive overview of the technologies and applications ofGNNs in the spatiotemporal domain. First, the ways of constructing graphs fromspatiotemporal data are summarized to help domain experts understand how togenerate graphs from various types of spatiotemporal data. Then, a systematiccategorization and summary of existing spatiotemporal GNNs are presented toenable domain experts to identify suitable techniques and to support modeldevelopers in advancing their research. Moreover, a comprehensive overview ofsignificant applications in the spatiotemporal domain is offered to introduce abroader range of applications to model developers and domain experts, assistingthem in exploring potential research topics and enhancing the impact of theirwork. Finally, open challenges and future directions are discussed.", "output": "Graph Neural Network for spatiotemporal data: methods and applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Label errors have been found to be prevalent in popular text, vision, andaudio datasets, which heavily influence the safe development and evaluation ofmachine learning algorithms. Despite increasing efforts towards improving thequality of generic data types, such as images and texts, the problem ofmislabel detection in graph data remains underexplored. To bridge the gap, weexplore mislabelling issues in popular real-world graph datasets and proposeGraphCleaner, a post-hoc method to detect and correct these mislabelled nodesin graph datasets. GraphCleaner combines the novel ideas of 1) SyntheticMislabel Dataset Generation, which seeks to generate realistic mislabels; and2) Neighborhood-Aware Mislabel Detection, where neighborhood dependency isexploited in both labels and base classifier predictions. Empirical evaluationson 6 datasets and 6 experimental settings demonstrate that GraphCleaneroutperforms the closest baseline, with an average improvement of 0.14 in F1score, and 0.16 in MCC. On real-data case studies, GraphCleaner detects realand previously unknown mislabels in popular graph benchmarks: PubMed, Cora,CiteSeer and OGB-arxiv; we find that at least 6.91% of PubMed data ismislabelled or ambiguous, and simply removing these mislabelled data can boostevaluation performance from 86.71% to 89.11%.", "output": "GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Discrete choice models (DCM) are widely employed in travel demand analysis asa powerful theoretical econometric framework for understanding and predictingchoice behaviors. DCMs are formed as random utility models (RUM), with theirkey advantage of interpretability. However, a core requirement for theestimation of these models is a priori specification of the associated utilityfunctions, making them sensitive to modelers' subjective beliefs. Recently,machine learning (ML) approaches have emerged as a promising avenue forlearning unobserved non-linear relationships in DCMs. However, ML models areconsidered \"black box\" and may not correspond with expected relationships. Thispaper proposes a framework that expands the potential of data-driven approachesfor DCM by supporting the development of interpretable models that incorporatedomain knowledge and prior beliefs through constraints. The proposed frameworkincludes pseudo data samples that represent required relationships and a lossfunction that measures their fulfillment, along with observed data, for modeltraining. The developed framework aims to improve model interpretability bycombining ML's specification flexibility with econometrics and interpretablebehavioral analysis. A case study demonstrates the potential of this frameworkfor discrete choice analysis.", "output": "Incorporating Domain Knowledge in Deep Neural Networks for Discrete Choice Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have achieved a milestone that undenia-blychanged many held beliefs in artificial intelligence (AI). However, thereremains many limitations of these LLMs when it comes to true languageunderstanding, limitations that are a byproduct of the under-lying architectureof deep neural networks. Moreover, and due to their subsymbolic nature,whatever knowledge these models acquire about how language works will always beburied in billions of microfeatures (weights), none of which is meaningful onits own, making such models hopelessly unexplainable. To address theselimitations, we suggest com-bining the strength of symbolic representationswith what we believe to be the key to the success of LLMs, namely a successfulbottom-up re-verse engineering of language at scale. As such we argue for abottom-up reverse engineering of language in a symbolic setting. Hints on whatthis project amounts to have been suggested by several authors, and we discussin some detail here how this project could be accomplished.", "output": "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have shown remarkable capabilities across abroad range of tasks involving question answering and the generation ofcoherent text and code. Comprehensively understanding the strengths andweaknesses of LLMs is beneficial for safety, downstream applications andimproving performance. In this work, we investigate the degree to which GPT-4has acquired factual geographic knowledge and is capable of using thisknowledge for interpretative reasoning, which is especially important forapplications that involve geographic data, such as geospatial analysis, supplychain management, and disaster response. To this end, we design and conduct aseries of diverse experiments, starting from factual tasks such as location,distance and elevation estimation to more complex questions such as generatingcountry outlines and travel networks, route finding under constraints andsupply chain analysis. We provide a broad characterisation of what GPT-4(without plugins or Internet access) knows about the world, highlighting bothpotentially surprising capabilities but also limitations.", "output": "GPT4GEO: How a Language Model Sees the World's Geography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There have been remarkable breakthroughs in Machine Learning and ArtificialIntelligence, notably in the areas of Natural Language Processing and DeepLearning. Additionally, hate speech detection in dialogues has been gainingpopularity among Natural Language Processing researchers with the increased useof social media. However, as evidenced by the recent trends, the need for thedimensions of explainability and interpretability in AI models has been deeplyrealised. Taking note of the factors above, the research goal of this paper isto bridge the gap between hate speech prediction and the explanations generatedby the system to support its decision. This has been achieved by firstpredicting the classification of a text and then providing a posthoc, modelagnostic and surrogate interpretability approach for explainability and toprevent model bias. The bidirectional transformer model BERT has been used forprediction because of its state of the art efficiency over other MachineLearning models. The model agnostic algorithm LIME generates explanations forthe output of a trained classifier and predicts the features that influence themodel decision. The predictions generated from the model were evaluatedmanually, and after thorough evaluation, we observed that the model performsefficiently in predicting and explaining its prediction. Lastly, we suggestfurther directions for the expansion of the provided research work.", "output": "Explaining Hate Speech Classification with Model Agnostic Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Currently, many researchers and analysts are working toward medical diagnosisenhancement for various diseases. Heart disease is one of the common diseasesthat can be considered a significant cause of mortality worldwide. Earlydetection of heart disease significantly helps in reducing the risk of heartfailure. Consequently, the Centers for Disease Control and Prevention (CDC)conducts a health-related telephone survey yearly from over 400,000participants. However, several concerns arise regarding the reliability of thedata in predicting heart disease and whether all of the survey questions arestrongly related. This study aims to utilize several machine learningtechniques, such as support vector machines and logistic regression, toinvestigate the accuracy of the CDC's heart disease survey in the UnitedStates. Furthermore, we use various feature selection methods to identify themost relevant subset of questions that can be utilized to forecast heartconditions. To reach a robust conclusion, we perform stability analysis byrandomly sampling the data 300 times. The experimental results show that thesurvey data can be useful up to 80% in terms of predicting heart disease, whichsignificantly improves the diagnostic process before bloodwork and tests. Inaddition, the amount of time spent conducting the survey can be reduced by 77%while maintaining the same level of performance.", "output": "Predicting Heart Disease and Reducing Survey Time Using Machine Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Operationalizing AI fairness at LinkedIn's scale is challenging not onlybecause there are multiple mutually incompatible definitions of fairness butalso because determining what is fair depends on the specifics and context ofthe product where AI is deployed. Moreover, AI practitioners need clarity onwhat fairness expectations need to be addressed at the AI level. In this paper,we present the evolving AI fairness framework used at LinkedIn to address thesethree challenges. The framework disentangles AI fairness by separating outequal treatment and equitable product expectations. Rather than imposing atrade-off between these two commonly opposing interpretations of fairness, theframework provides clear guidelines for operationalizing equal AI treatmentcomplemented with a product equity strategy. This paper focuses on the equal AItreatment component of LinkedIn's AI fairness framework, shares the principlesthat support it, and illustrates their application through a case study. Wehope this paper will encourage other big tech companies to join us in sharingtheir approach to operationalizing AI fairness at scale, so that together wecan keep advancing this constantly evolving field.", "output": "Disentangling and Operationalizing AI Fairness at LinkedIn."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Code intelligence plays a key role in transforming modern softwareengineering. Recently, deep learning-based models, especially Transformer-basedlarge language models (LLMs), have demonstrated remarkable potential intackling these tasks by leveraging massive open-source code data andprogramming language features. However, the development and deployment of suchmodels often require expertise in both machine learning and softwareengineering, creating a barrier for the model adoption. In this paper, wepresent CodeTF, an open-source Transformer-based library for state-of-the-artCode LLMs and code intelligence. Following the principles of modular design andextensible framework, we design CodeTF with a unified interface to enable rapidaccess and development across different types of models, datasets and tasks.Our library supports a collection of pretrained Code LLM models and popularcode benchmarks, including a standardized interface to train and serve codeLLMs efficiently, and data features such as language-specific parsers andutility functions for extracting code attributes. In this paper, we describethe design principles, the architecture, key modules and components, andcompare with other related library tools. Finally, we hope CodeTF is able tobridge the gap between machine learning/generative AI and software engineering,providing a comprehensive open-source solution for developers, researchers, andpractitioners.", "output": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robot design aims at learning to create robots that can be easily controlledand perform tasks efficiently. Previous works on robot design have proven itsability to generate robots for various tasks. However, these works searched therobots directly from the vast design space and ignored common structures,resulting in abnormal robots and poor performance. To tackle this problem, wepropose a Symmetry-Aware Robot Design (SARD) framework that exploits thestructure of the design space by incorporating symmetry searching into therobot design process. Specifically, we represent symmetries with the subgroupsof the dihedral group and search for the optimal symmetry in structuredsubgroups. Then robots are designed under the searched symmetry. In this way,SARD can design efficient symmetric robots while covering the original designspace, which is theoretically analyzed. We further empirically evaluate SARD onvarious tasks, and the results show its superior efficiency andgeneralizability.", "output": "Symmetry-Aware Robot Design with Structured Subgroups."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Twitter as one of the most popular social networks, offers a means forcommunication and online discourse, which unfortunately has been the target ofbots and fake accounts, leading to the manipulation and spreading of falseinformation. Towards this end, we gather a challenging, multilingual dataset ofsocial discourse on Twitter, originating from 9M users regarding the recentRusso-Ukrainian war, in order to detect the bot accounts and the conversationinvolving them. We collect the ground truth for our dataset through the TwitterAPI suspended accounts collection, containing approximately 343K of botaccounts and 8M of normal users. Additionally, we use a dataset provided byBotometer-V3 with 1,777 Varol, 483 German accounts, and 1,321 US accounts.Besides the publicly available datasets, we also manage to collect 2independent datasets around popular discussion topics of the 2022 energy crisisand the 2022 conspiracy discussions. Both of the datasets were labeledaccording to the Twitter suspension mechanism. We build a novel ML model forbot detection using the state-of-the-art XGBoost model. We combine the modelwith a high volume of labeled tweets according to the Twitter suspensionmechanism ground truth. This requires a limited set of profile featuresallowing labeling of the dataset in different time periods from the collection,as it is independent of the Twitter API. In comparison with Botometer ourmethodology achieves an average 11% higher ROC-AUC score over two real-casescenario datasets.", "output": "BotArtist: Twitter bot detection Machine Learning model based on Twitter suspension."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a Federated Learning Code Smell Detection (FedCSD)approach that allows organizations to collaboratively train federated ML modelswhile preserving their data privacy. These assertions have been supported bythree experiments that have significantly leveraged three manually validateddatasets aimed at detecting and examining different code smell scenarios. Inexperiment 1, which was concerned with a centralized training experiment,dataset two achieved the lowest accuracy (92.30%) with fewer smells, whiledatasets one and three achieved the highest accuracy with a slight difference(98.90% and 99.5%, respectively). This was followed by experiment 2, which wasconcerned with cross-evaluation, where each ML model was trained using onedataset, which was then evaluated over the other two datasets. Results fromthis experiment show a significant drop in the model's accuracy (lowestaccuracy: 63.80%) where fewer smells exist in the training dataset, which hasa noticeable reflection (technical debt) on the model's performance. Finally,the last and third experiments evaluate our approach by splitting the datasetinto 10 companies. The ML model was trained on the company's site, then allmodel-updated weights were transferred to the server. Ultimately, an accuracyof 98.34% was achieved by the global model that has been trained using 10companies for 100 training rounds. The results reveal a slight difference inthe global model's accuracy compared to the highest accuracy of the centralizedmodel, which can be ignored in favour of the global model's comprehensiveknowledge, lower training cost, preservation of data privacy, and avoidance ofthe technical debt problem.", "output": "FedCSD: A Federated Learning Based Approach for Code-Smell Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A key component of automated algorithm selection and configuration, which inmost cases are performed using supervised machine learning (ML) methods is agood-performing predictive model. The predictive model uses the featurerepresentation of a set of problem instances as input data and predicts thealgorithm performance achieved on them. Common machine learning models struggleto make predictions for instances with feature representations not covered bythe training data, resulting in poor generalization to unseen problems. In thisstudy, we propose a workflow to estimate the generalizability of a predictivemodel for algorithm performance, trained on one benchmark suite to another. Theworkflow has been tested by training predictive models across benchmark suitesand the results show that generalizability patterns in the landscape featurespace are reflected in the performance space.", "output": "Assessing the Generalizability of a Performance Predictive Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most metaheuristic algorithms rely on a few searched solutions to guide latersearches during the convergence process for a simple reason: the limitedcomputing resource of a computer makes it impossible to retain all the searchedsolutions. This also reveals that each search of most metaheuristic algorithmsis just like a ballpark guess. To help address this issue, we present a novelmetaheuristic algorithm called space net optimization (SNO). It is equippedwith a new mechanism called space net; thus, making it possible for ametaheuristic algorithm to use most information provided by all searchedsolutions to depict the landscape of the solution space. With the space net, ametaheuristic algorithm is kind of like having a ``vision'' on the solutionspace. Simulation results show that SNO outperforms all the other metaheuristicalgorithms compared in this study for a set of well-known single objectivebound constrained problems in most cases.", "output": "Space Net Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Is the lottery ticket phenomenon an idiosyncrasy of gradient-based trainingor does it generalize to evolutionary optimization? In this paper we establishthe existence of highly sparse trainable initializations for evolutionstrategies (ES) and characterize qualitative differences compared to gradientdescent (GD)-based sparse training. We introduce a novel signal-to-noiseiterative pruning procedure, which incorporates loss curvature information intothe network pruning step. This can enable the discovery of even sparsertrainable network initializations when using black-box evolution as compared toGD-based optimization. Furthermore, we find that these initializations encodean inductive bias, which transfers across different ES, related tasks and evento GD-based training. Finally, we compare the local optima resulting from thedifferent optimization paradigms and sparsity levels. In contrast to GD, ESexplore diverse and flat local optima and do not preserve linear modeconnectivity across sparsity levels and independent runs. The results highlightqualitative differences between evolution and gradient-based learning dynamics,which can be uncovered by the study of iterative pruning procedures.", "output": "Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum machine learning is often highlighted as one of the most promisinguses for a quantum computer to solve practical problems. However, a majorobstacle to the widespread use of quantum machine learning models in practiceis that these models, even once trained, still require access to a quantumcomputer in order to be evaluated on new data. To solve this issue, we suggestthat following the training phase of a quantum model, a quantum computer couldbe used to generate what we call a classical shadow of this model, i.e., aclassically computable approximation of the learned function. While recentworks already explore this idea and suggest approaches to construct such shadowmodels, they also raise the possibility that a completely classical model couldbe trained instead, thus circumventing the need for a quantum computer in thefirst place. In this work, we take a novel approach to define shadow modelsbased on the frameworks of quantum linear models and classical shadowtomography. This approach allows us to show that there exist shadow modelswhich can solve certain learning tasks that are intractable for fully classicalmodels, based on widely-believed cryptography assumptions. We also discuss the(un)likeliness that all quantum models could be shadowfiable, based on commonassumptions in complexity theory.", "output": "Shadows of quantum machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "AI-based text-to-image generation has undergone a significant leap in theproduction of visually comprehensive and aesthetic imagery over the past year,to the point where differentiating between a man-made piece of art and anAI-generated image is becoming more difficult. Generative Models such as StableDiffusion, Midjourney and others are expected to affect several majorindustries in technological and ethical aspects. Striking the balance betweenraising human standard of life and work vs exploiting one group of people toenrich another is a complex and crucial part of the discussion. Due to therapid growth of this technology, the way in which its models operate, and grayarea legalities, visual and artistic domains - including the video gameindustry, are at risk of being taken over from creators by AI infrastructureowners. This paper is a literature review examining the concerns facing both AIdevelopers and users today, including identity theft, data laundering and more.It discusses legalization challenges and ethical concerns, and concludes withhow AI generative models can be tremendously useful in streamlining the processof visual creativity in both static and interactive media given properregulation.Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AIEthics, Game Design, Digital Art, Data Laundering", "output": "AI Imagery and the Overton Window."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has recently emerged as a promising paradigmfor training generalisable models on large-scale data in the fields of vision,text, and speech. Although SSL has been proven effective in speech and audio,its application to music audio has yet to be thoroughly explored. This isprimarily due to the distinctive challenges associated with modelling musicalknowledge, particularly its tonal and pitched characteristics of music. Toaddress this research gap, we propose an acoustic Music undERstanding modelwith large-scale self-supervised Training (MERT), which incorporates teachermodels to provide pseudo labels in the masked language modelling (MLM) styleacoustic pre-training. In our exploration, we identified a superior combinationof teacher models, which outperforms conventional speech and audio approachesin terms of performance. This combination includes an acoustic teacher based onResidual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musicalteacher based on the Constant-Q Transform (CQT). These teachers effectivelyguide our student model, a BERT-style transformer encoder, to better modelmusic audio. In addition, we introduce an in-batch noise mixture augmentationto enhance the representation robustness. Furthermore, we explore a wide rangeof settings to overcome the instability in acoustic language modelpre-training, which allows our designed paradigm to scale from 95M to 330Mparameters. Experimental results indicate that our model can generalise andperform well on 14 music understanding tasks and attains state-of-the-art(SOTA) overall scores. The code and models are online:", "output": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating music from text descriptions is a user-friendly mode since thetext is a relatively easy interface for user engagement. While some approachesutilize texts to control music audio generation, editing musical elements ingenerated audio is challenging for users. In contrast, symbolic music offersease of editing, making it more accessible for users to manipulate specificmusical elements. In this paper, we propose MuseCoco, which generates symbolicmusic from text descriptions with musical attributes as the bridge to breakdown the task into text-to-attribute understanding and attribute-to-musicgeneration stages. MuseCoCo stands for Music Composition Copilot that empowersmusicians to generate music directly from given text descriptions, offering asignificant improvement in efficiency compared to creating music entirely fromscratch. The system has two main advantages: Firstly, it is data efficient. Inthe attribute-to-music generation stage, the attributes can be directlyextracted from music sequences, making the model training self-supervised. Inthe text-to-attribute understanding stage, the text is synthesized and refinedby ChatGPT based on the defined attribute templates. Secondly, the system canachieve precise control with specific attributes in text descriptions andoffers multiple control options through attribute-conditioned ortext-conditioned approaches. MuseCoco outperforms baseline systems in terms ofmusicality, controllability, and overall score by at least 1.27, 1.08, and 1.32respectively. Besides, there is a notable enhancement of about 20% in objectivecontrol accuracy. In addition, we have developed a robust large-scale modelwith 1.2 billion parameters, showcasing exceptional controllability andmusicality.", "output": "MuseCoco: Generating Symbolic Music from Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monitoring land cover using remote sensing is vital for studyingenvironmental changes and ensuring global food security through crop yieldforecasting. Specifically, multitemporal remote sensing imagery providesrelevant information about the dynamics of a scene, which has proven to lead tobetter land cover classification results. Nevertheless, few studies havebenefited from high spatial and temporal resolution data due to the difficultyof accessing reliable, fine-grained and high-quality annotated samples tosupport their hypotheses. Therefore, we introduce a temporal patch-baseddataset of Canadian croplands, enriched with labels retrieved from the CanadianAnnual Crop Inventory. The dataset contains 78,536 manually verified andcurated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10crop classes collected over four crop production years (2017-2020) and fivemonths (June-October). Each instance contains 12 spectral bands, an RGB image,and additional vegetation index bands. Individually, each category contains atleast 4,800 images. Moreover, as a benchmark, we provide models and source codethat allow a user to predict the crop class using a single image (ResNet,DenseNet, EfficientNet) or a sequence of images (LRCN, 3D-CNN) from the samelocation. In perspective, we expect this evolving dataset to propel thecreation of robust agro-environmental models that can accelerate thecomprehension of complex agricultural regions by providing accurate andcontinuous monitoring of land cover.", "output": "The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In 1989 George Cybenko proved in a landmark paper that wide shallow neuralnetworks can approximate arbitrary continuous functions on a compact set. Thisuniversal approximation theorem sparked a lot of follow-up research.Shen, Yang and Zhang determined optimal approximation rates for ReLU-networksin $L^p$-norms with $p in [1,infty)$. Kidger and Lyons proved a universalapproximation theorem for deep narrow ReLU-networks. Telgarsky gave an exampleof a deep narrow ReLU-network that cannot be approximated by a wide shallowReLU-network unless it has exponentially many neurons.However, there are even more questions that still remain unresolved. Arethere any wide shallow ReLU-networks that cannot be approximated well by deepnarrow ReLU-networks? Is the universal approximation theorem still true forother norms like the Sobolev norm $W^{1,1}$? Do these results hold foractivation functions other than ReLU?We will answer all of those questions and more with a framework of twoexpressive powers. The first one is well-known and counts the maximal number oflinear regions of a function calculated by a ReLU-network. We will improve thebest known bounds for this expressive power. The second one is entirely new.", "output": "On the Expressive Power of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This tutorial paper provides a general overview of symbolic regression (SR)with specific focus on standards of interpretability. We posit thatinterpretable modeling, although its definition is still disputed in theliterature, is a practical way to support the evaluation of successfulinformation fusion. In order to convey the benefits of SR as a modelingtechnique, we demonstrate an application within the field of health andnutrition using publicly available National Health and Nutrition ExaminationSurvey (NHANES) data from the Centers for Disease Control and Prevention (CDC),fusing together anthropometric markers into a simple mathematical expression toestimate body fat percentage. We discuss the advantages and challengesassociated with SR modeling and provide qualitative and quantitative analysesof the learned models.", "output": "Information Fusion via Symbolic Regression: A Tutorial in the Context of Human Health."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When formulated using Bayesian networks, two standard decision algorithms(Evidential Decision Theory and Causal Decision Theory) can be shown to failsystematically when faced with aspects of the prisoner's dilemma and so-called\"Newcomblike\" problems. We describe a new form of decision algorithm, calledTimeless Decision Theory, which consistently wins on these problems.", "output": "A Comparison of Decision Algorithms on Newcomblike Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative large language models (LLMs) can be a powerful tool for augmentingtext annotation procedures, but their performance varies across annotationtasks due to prompt quality, text data idiosyncrasies, and conceptualdifficulty. Because these challenges will persist even as LLM technologyimproves, we argue that any automated annotation process using an LLM mustvalidate the LLM's performance against labels generated by humans. To this end,we outline a workflow to harness the annotation potential of LLMs in aprincipled, efficient way. Using GPT-4, we validate this approach byreplicating 27 annotation tasks across 11 datasets from recent social sciencearticles in high-impact journals. We find that LLM performance for textannotation is promising but highly contingent on both the dataset and the typeof annotation task, which reinforces the necessity to validate on atask-by-task basis. We make available easy-to-use software designed toimplement our workflow and streamline the deployment of LLMs for automatedannotation.", "output": "Automated Annotation with Generative AI Requires Validation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reconstruction of 3D neural fields from posed images has emerged as apromising method for self-supervised representation learning. The key challengepreventing the deployment of these 3D scene learners on large-scale video datais their dependence on precise camera poses from structure-from-motion, whichis prohibitively expensive to run at scale. We propose a method that jointlyreconstructs camera poses and 3D neural scene representations online and in asingle forward pass. We estimate poses by first lifting frame-to-frame opticalflow to 3D scene flow via differentiable rendering, preserving locality andshift-equivariance of the image processing backbone. SE(3) camera poseestimation is then performed via a weighted least-squares fit to the scene flowfield. This formulation enables us to jointly supervise pose estimation and ageneralizable neural scene representation via re-rendering the input video, andthus, train end-to-end and fully self-supervised on real-world video datasets.We demonstrate that our method performs robustly on diverse, real-world video,notably on sequences traditionally challenging to optimization-based poseestimation techniques.", "output": "FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Representations learned by pre-training a neural network on a large datasetare increasingly used successfully to perform a variety of downstream tasks. Inthis work, we take a closer look at how features are encoded in suchpre-trained representations. We find that learned representations in a givenlayer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subsetof neurons in the layer that is larger than a threshold size shares a largedegree of similarity with the full layer and is able to perform similarly asthe whole layer on a variety of downstream tasks. For example, a linear probetrained on $20%$ of randomly picked neurons from a ResNet50 pre-trained onImageNet1k achieves an accuracy within $5%$ of a linear probe trained on thefull layer of neurons for downstream CIFAR10 classification. We conductexperiments on different neural architectures (including CNNs and Transformers)pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety ofdownstream tasks taken from the VTAB benchmark. We find that the loss &amp; datasetused during pre-training largely govern the degree of diffuse redundancy andthe \"critical mass\" of neurons needed often depends on the downstream task,suggesting that there is a task-inherent redundancy-performance Paretofrontier. Our findings shed light on the nature of representations learned bypre-trained deep neural networks and suggest that entire layers might not benecessary to perform many downstream tasks. We investigate the potential forexploiting this redundancy to achieve efficient generalization for downstreamtasks and also draw caution to certain possible unintended consequences.", "output": "Diffused Redundancy in Pre-trained Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven methods have shown tremendous progress in medical image analysis.In this context, deep learning-based supervised methods are widely popular.However, they require a large amount of training data and face issues ingeneralisability to unseen datasets that hinder clinical translation.Endoscopic imaging data incorporates large inter- and intra-patient variabilitythat makes these models more challenging to learn representative features fordownstream tasks. Thus, despite the publicly available datasets and datasetsthat can be generated within hospitals, most supervised models stillunderperform. While self-supervised learning has addressed this problem to someextent in natural scene data, there is a considerable performance gap in themedical image domain. In this paper, we propose to explore patch-levelinstance-group discrimination and penalisation of inter-class variation usingadditive angular margin within the cosine similarity metrics. Our novelapproach enables models to learn to cluster similar representative patches,thereby improving their ability to provide better separation between differentclasses. Our results demonstrate significant improvement on all metrics overthe state-of-the-art (SOTA) methods on the test set from the same and diversedatasets. We evaluated our approach for classification, detection, andsegmentation. SSL-CPCD achieves 79.77% on Top 1 accuracy for ulcerative colitisclassification, 88.62% on mAP for polyp detection, and 82.32% on dicesimilarity coefficient for segmentation tasks are nearly over 4%, 2%, and 3%,respectively, compared to the baseline architectures. We also demonstrate thatour method generalises better than all SOTA methods to unseen datasets,reporting nearly 7% improvement in our generalisability assessment.", "output": "SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While stochastic gradient descent (SGD) is still the most popularoptimization algorithm in deep learning, adaptive algorithms such as Adam haveestablished empirical advantages over SGD in some deep learning applicationssuch as training transformers. However, it remains a question that why Adamconverges significantly faster than SGD in these scenarios. In this paper, wepropose one explanation of why Adam converges faster than SGD using a newconcept directional sharpness. We argue that the performance of optimizationalgorithms is closely related to the directional sharpness of the update steps,and show SGD has much worse directional sharpness compared to adaptivealgorithms. We further observe that only a small fraction of the coordinatescauses the bad sharpness and slow convergence of SGD, and propose to usecoordinate-wise clipping as a solution to SGD and other optimizationalgorithms. We demonstrate the effect of coordinate-wise clipping on sharpnessreduction and speeding up the convergence of optimization algorithms undervarious settings. We show that coordinate-wise clipping improves the local lossreduction when only a small fraction of the coordinates has bad sharpness. Weconclude that the sharpness reduction effect of adaptive coordinate-wisescaling is the reason for Adam's success in practice and suggest the use ofcoordinate-wise clipping as a universal technique to speed up deep learningoptimization.", "output": "Toward Understanding Why Adam Converges Faster Than SGD for Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised pre-trained models extract general-purpose representationsfrom data, and quantifying how reliable they are is crucial because manydownstream models use these representations as input for their own tasks. Tothis end, we first introduce a formal definition of representation reliability:the representation for a given test input is considered to be reliable if thedownstream models built on top of that representation can consistently generateaccurate predictions for that test point. It is desired to estimate therepresentation reliability without knowing the downstream tasks a priori. Weprovide a negative result showing that existing frameworks for uncertaintyquantification in supervised learning are not suitable for this purpose. As analternative, we propose an ensemble-based method for quantifying representationreliability, based on the concept of neighborhood consistency in therepresentation spaces across various pre-trained models. More specifically, thekey insight is to use shared neighboring points as anchors to align differentrepresentation spaces. We demonstrate through comprehensive numericalexperiments that our method is capable of predicting representation reliabilitywith high accuracy.", "output": "Representation Reliability and Its Impact on Downstream Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We examine online safe multi-agent reinforcement learning using constrainedMarkov games in which agents compete by maximizing their expected total rewardsunder a constraint on expected total utilities. Our focus is confined to anepisodic two-player zero-sum constrained Markov game with independenttransition functions that are unknown to agents, adversarial reward functions,and stochastic utility functions. For such a Markov game, we employ an approachbased on the occupancy measure to formulate it as an online constrainedsaddle-point problem with an explicit constraint. We extend the Lagrangemultiplier method in constrained optimization to handle the constraint bycreating a generalized Lagrangian with minimax decision primal variables and adual variable. Next, we develop an upper confidence reinforcement learningalgorithm to solve this Lagrangian problem while balancing exploration andexploitation. Our algorithm updates the minimax decision primal variables viaonline mirror descent and the dual variable via projected gradient step and weprove that it enjoys sublinear rate $ O((|X|+|Y|) L sqrt{T(|A|+|B|)}))$ forboth regret and constraint violation after playing $T$ episodes of the game.Here, $L$ is the horizon of each episode, $(|X|,|A|)$ and $(|Y|,|B|)$ are thestate/action space sizes of the min-player and the max-player, respectively. Tothe best of our knowledge, we provide the first provably efficient online safereinforcement learning algorithm in constrained Markov games.", "output": "Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image generative models have made remarkable advancements ingenerating high-quality images. However, generated images often containundesirable artifacts or other errors due to model limitations. Existingtechniques to fine-tune generated images are time-consuming (manual editing),produce poorly-integrated results (inpainting), or result in unexpected changesacross the entire image (variation selection and prompt fine-tuning). In thiswork, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool toefficiently fine-tune desired regions within an AI-synthesized image. Ourmethod introduces new random noise patterns at targeted regions during thereverse diffusion process, enabling the model to efficiently make changes tothe specified regions while preserving the original context for the rest of theimage. We evaluate our method's usability and effectiveness through a userstudy with artists, comparing our technique against other state-of-the-artimage inpainting techniques and editing software for fine-tuning AI-generatedimagery.", "output": "Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual Question Answering is a challenging task, as it requires seamlessinteraction between perceptual, linguistic, and background knowledge systems.While the recent progress of visual and natural language models like BLIP hasled to improved performance on this task, we lack understanding of the abilityof such models to perform on different kinds of questions and reasoning types.As our initial analysis of BLIP-family models revealed difficulty withanswering fine-detail questions, we investigate the following question: Canvisual cropping be employed to improve the performance of state-of-the-artvisual question answering models on fine-detail questions? Given the recentsuccess of the BLIP-family models, we study a zero-shot and a fine-tuned BLIPmodel. We define three controlled subsets of the popular VQA-v2 benchmark tomeasure whether cropping can help model performance. Besides human cropping, wedevise two automatic cropping strategies based on multi-modal embedding by CLIPand BLIP visual QA model gradients. Our experiments demonstrate that theperformance of BLIP model variants can be significantly improved through humancropping, and automatic cropping methods can produce comparable benefits. Adeeper dive into our findings indicates that the performance enhancement ismore pronounced in zero-shot models than in fine-tuned models and more salientwith smaller bounding boxes than larger ones. We perform case studies toconnect quantitative differences with qualitative observations across questiontypes and datasets. Finally, we see that the cropping enhancement is robust, aswe gain an improvement of 4.59% (absolute) in the general VQA-random task bysimply inputting a concatenation of the original and gradient-based croppedimages. We make our code available to facilitate further innovation on visualcropping methods for question answering.", "output": "Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sequential models that encode user activity for next action prediction havebecome a popular design choice for building web-scale personalizedrecommendation systems. Traditional methods of sequential recommendation eitherutilize end-to-end learning on realtime user actions, or learn userrepresentations separately in an offline batch-generated manner. This paper (1)presents Pinterest's ranking architecture for Homefeed, our personalizedrecommendation product and the largest engagement surface; (2) proposesTransAct, a sequential model that extracts users' short-term preferences fromtheir realtime activities; (3) describes our hybrid approach to ranking, whichcombines end-to-end sequential modeling via TransAct with batch-generated userembeddings. The hybrid approach allows us to combine the advantages ofresponsiveness from learning directly on realtime user activity with thecost-effectiveness of batch user representations learned over a longer timeperiod. We describe the results of ablation studies, the challenges we facedduring productionization, and the outcome of an online A/B experiment, whichvalidates the effectiveness of our hybrid ranking model. We further demonstratethe effectiveness of TransAct on other surfaces such as contextualrecommendations and search. Our model has been deployed to production inHomefeed, Related Pins, Notifications, and Search at Pinterest.", "output": "TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world planning problems$unicode{x2014}$including autonomous driving andsustainable energy applications like carbon storage and resourceexploration$unicode{x2014}$have recently been modeled as partially observableMarkov decision processes (POMDPs) and solved using approximate methods. Tosolve high-dimensional POMDPs in practice, state-of-the-art methods use onlineplanning with problem-specific heuristics to reduce planning horizons and makethe problems tractable. Algorithms that learn approximations to replaceheuristics have recently found success in large-scale problems in the fullyobservable domain. The key insight is the combination of online Monte Carlotree search with offline neural network approximations of the optimal policyand value function. In this work, we bring this insight to partially observeddomains and propose BetaZero, a belief-state planning algorithm for POMDPs.BetaZero learns offline approximations based on accurate belief models toenable online decision making in long-horizon problems. We address severalchallenges inherent in large-scale partially observable domains; namelychallenges of transitioning in stochastic environments, prioritizing actionbranching with limited search budget, and representing beliefs as input to thenetwork. We apply BetaZero to various well-established benchmark POMDPs foundin the literature. As a real-world case study, we test BetaZero on thehigh-dimensional geological problem of critical mineral exploration.Experiments show that BetaZero outperforms state-of-the-art POMDP solvers on avariety of tasks.", "output": "BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-training is an important technique for solving semi-supervised learningproblems. It leverages unlabeled data by generating pseudo-labels and combiningthem with a limited labeled dataset for training. The effectiveness ofself-training heavily relies on the accuracy of these pseudo-labels. In thispaper, we introduce doubly robust self-training, a novel semi-supervisedalgorithm that provably balances between two extremes. When the pseudo-labelsare entirely incorrect, our method reduces to a training process solely usinglabeled data. Conversely, when the pseudo-labels are completely accurate, ourmethod transforms into a training process utilizing all pseudo-labeled data andlabeled data, thus increasing the effective sample size. Through empiricalevaluations on both the ImageNet dataset for image classification and thenuScenes autonomous driving dataset for 3D object detection, we demonstrate thesuperiority of the doubly robust loss over the standard self-training baseline.", "output": "Doubly Robust Self-Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural architecture search (NAS) has allowed for the automatic creation ofnew and effective neural network architectures, offering an alternative to thelaborious process of manually designing complex architectures. However,traditional NAS algorithms are slow and require immense amounts of computingpower. Recent research has investigated training-free NAS metrics for imageclassification architectures, drastically speeding up search algorithms. Inthis paper, we investigate training-free NAS metrics for recurrent neuralnetwork (RNN) and BERT-based transformer architectures, targeted towardslanguage modeling tasks. First, we develop a new training-free metric, namedhidden covariance, that predicts the trained performance of an RNN architectureand significantly outperforms existing training-free metrics. We experimentallyevaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLPbenchmark. Second, we find that the current search space paradigm fortransformer architectures is not optimized for training-free neuralarchitecture search. Instead, a simple qualitative analysis can effectivelyshrink the search space to the best performing architectures. This conclusionis based on our investigation of existing training-free metrics and new metricsdeveloped from recent transformer pruning literature, evaluated on our ownbenchmark of trained BERT architectures. Ultimately, our analysis shows thatthe architecture search space and the training-free metric must be developedtogether in order to achieve effective results.", "output": "Training-free Neural Architecture Search for RNNs and Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We can usually assume others have goals analogous to our own. This assumptioncan also, at times, be applied to multi-agent games - e.g. Agent 1's attractionto green pellets is analogous to Agent 2's attraction to red pellets. This\"analogy\" assumption is tied closely to the cognitive process known as empathy.Inspired by empathy, we design a simple and explainable architecture to modelanother agent's action-value function. This involves learning an \"ImaginationNetwork\" to transform the other agent's observed state in order to produce ahuman-interpretable \"empathetic state\" which, when presented to the learningagent, produces behaviours that mimic the other agent. Our approach isapplicable to multi-agent scenarios consisting of a single learning agent andother (independent) agents acting according to fixed policies. Thisarchitecture is particularly beneficial for (but not limited to) algorithmsusing a composite value or reward function. We show our method produces betterperformance in multi-agent games, where it robustly estimates the other's modelin different environment configurations. Additionally, we show that theempathetic states are human interpretable, and thus verifiable.", "output": "EMOTE: An Explainable architecture for Modelling the Other Through Empathy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Motivated by the striking ability of transformers for in-context learning,several works demonstrate that transformers can implement algorithms likegradient descent. By a careful construction of weights, these works show thatmultiple layers of transformers are expressive enough to simulate gradientdescent iterations. Going beyond the question of expressivity, we ask: Cantransformers learn to implement such algorithms by training over random probleminstances? To our knowledge, we make the first theoretical progress toward thisquestion via analysis of the loss landscape for linear transformers trainedover random instances of linear regression. For a single attention layer, weprove the global minimum of the training objective implements a singleiteration of preconditioned gradient descent. Notably, the preconditioningmatrix not only adapts to the input distribution but also to the varianceinduced by data inadequacy. For a transformer with $k$ attention layers, weprove certain critical points of the training objective implement $k$iterations of preconditioned gradient descent. Our results call for futuretheoretical studies on learning algorithms by training transformers.", "output": "Transformers learn to implement preconditioned gradient descent for in-context learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models have been used in creating various effective imageclassification applications. However, they are vulnerable to adversarialattacks that seek to misguide the models into predicting incorrect classes. Ourstudy of major adversarial attack models shows that they all specificallytarget and exploit the neural networking structures in their designs. Thisunderstanding makes us develop a hypothesis that most classical machinelearning models, such as Random Forest (RF), are immune to adversarial attackmodels because they do not rely on neural network design at all. Ourexperimental study of classical machine learning models against popularadversarial attacks supports this hypothesis. Based on this hypothesis, wepropose a new adversarial-aware deep learning system by using a classicalmachine learning model as the secondary verification system to complement theprimary deep learning model in image classification. Although the secondaryclassical machine learning model has less accurate output, it is only used forverification purposes, which does not impact the output accuracy of the primarydeep learning model, and at the same time, can effectively detect anadversarial attack when a clear mismatch occurs. Our experiments based onCIFAR-100 dataset show that our proposed approach outperforms currentstate-of-the-art adversarial defense systems.", "output": "Adversarial-Aware Deep Learning System based on a Secondary Classical Machine Learning Verification Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Post-training quantization (PTQ) has been gaining popularity for thedeployment of deep neural networks on resource-limited devices since unlikequantization-aware training, neither a full training dataset nor end-to-endtraining is required at all. As PTQ schemes based on reconstructing each layeror block output turn out to be effective to enhance quantized modelperformance, recent works have developed algorithms to devise and learn a newweight-rounding scheme so as to better reconstruct each layer or block output.In this work, we propose a simple yet effective new weight-rounding mechanismfor PTQ, coined FlexRound, based on element-wise division instead of typicalelement-wise addition such that FlexRound enables jointly learning a commonquantization grid size as well as a different scale for each pre-trainedweight. Thanks to the reciprocal rule of derivatives induced by element-wisedivision, FlexRound is inherently able to exploit pre-trained weights whenupdating their corresponding scales, and thus, flexibly quantize pre-trainedweights depending on their magnitudes. We empirically validate the efficacy ofFlexRound on a wide range of models and tasks. To the best of our knowledge,our work is the first to carry out comprehensive experiments on not only imageclassification and natural language understanding but also natural languagegeneration, assuming a per-tensor uniform PTQ setting. Moreover, wedemonstrate, for the first time, that large language models can be efficientlyquantized, with only a negligible impact on performance compared tohalf-precision baselines, achieved by reconstructing the output in ablock-by-block manner.", "output": "FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language is often considered a key aspect of human thinking, providing uswith exceptional abilities to generalize, explore, plan, replan, and adapt tonew situations. However, Reinforcement Learning (RL) agents are far fromhuman-level performance in any of these abilities. We hypothesize one reasonfor such cognitive deficiencies is that they lack the benefits of thinking inlanguage and that we can improve AI agents by training them to think likehumans do. We introduce a novel Imitation Learning framework, Thought Cloning,where the idea is to not just clone the behaviors of human demonstrators, butalso the thoughts humans have as they perform these behaviors. While we expectThought Cloning to truly shine at scale on internet-sized datasets of humansthinking out loud while acting (e.g. online videos with transcripts), here weconduct experiments in a domain where the thinking and action data aresynthetically generated. Results reveal that Thought Cloning learns much fasterthan Behavioral Cloning and its performance advantage grows the further out ofdistribution test tasks are, highlighting its ability to better handle novelsituations. Thought Cloning also provides important benefits for AI Safety andInterpretability, and makes it easier to debug and improve AI. Because we canobserve the agent's thoughts, we can (1) more easily diagnose why things aregoing wrong, making it easier to fix the problem, (2) steer the agent bycorrecting its thinking, or (3) prevent it from doing unsafe things it plans todo. Overall, by training agents how to think as well as behave, Thought Cloningcreates safer, more powerful agents.", "output": "Thought Cloning: Learning to Think while Acting by Imitating Human Thinking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a multi-dimensional structured state space (S4) approach to speechenhancement. To better capture the spectral dependencies across the frequencyaxis, we focus on modifying the multi-dimensional S4 layer with whiteningtransformation to build new small-footprint models that also achieve goodperformance. We explore several S4-based deep architectures in time (T) andtime-frequency (TF) domains. The 2-D S4 layer can be considered a particularconvolutional layer with an infinite receptive field although it utilizes fewerparameters than a conventional convolutional layer. Evaluated on theVoiceBank-DEMAND data set, when compared with the conventional U-net modelbased on convolutional layers, the proposed TF-domain S4-based model is 78.6%smaller in size, yet it still achieves competitive results with a PESQ score of3.15 with data augmentation. By increasing the model size, we can even reach aPESQ score of 3.18.", "output": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exact inference of marginals in probabilistic graphical models (PGM) is knownto be intractable, necessitating the use of approximate methods. Most of theexisting variational techniques perform iterative message passing in loopygraphs which is slow to converge for many benchmarks. In this paper, we proposea new algorithm for marginal inference that is based on the incrementalbuild-infer-approximate (IBIA) paradigm. Our algorithm converts the PGM into asequence of linked clique tree forests (SLCTF) with bounded clique sizes, andthen uses a heuristic belief update algorithm to infer the marginals. For thespecial case of Bayesian networks, we show that if the incremental build stepin IBIA uses the topological order of variables then (a) the prior marginalsare consistent in all CTFs in the SLCTF and (b) the posterior marginals areconsistent once all evidence variables are added to the SLCTF. In our approach,the belief propagation step is non-iterative and the accuracy-complexitytrade-off is controlled using user-defined clique size bounds. Results forseveral benchmark sets from recent UAI competitions show that our method giveseither better or comparable accuracy than existing variational and samplingbased methods, with smaller runtimes.", "output": "Approximate inference of marginals using the IBIA framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Works on implicit regularization have studied gradient trajectories duringthe optimization process to explain why deep networks favor certain kinds ofsolutions over others. In deep linear networks, it has been shown that gradientdescent implicitly regularizes toward low-rank solutions on matrixcompletion/factorization tasks. Adding depth not only improves performance onthese tasks but also acts as an accelerative pre-conditioning that furtherenhances this bias towards low-rankedness. Inspired by this, we propose anexplicit penalty to mirror this implicit bias which only takes effect withcertain adaptive gradient optimizers (e.g. Adam). This combination can enable adegenerate single-layer network to achieve low-rank approximations withgeneralization error comparable to deep linear networks, making depth no longernecessary for learning. The single-layer network also performs competitively orout-performs various approaches for matrix completion over a range of parameterand data regimes despite its simplicity. Together with an optimizer's inductivebias, our findings suggest that explicit regularization can play a role indesigning different, desirable forms of regularization and that a more nuancedunderstanding of this interplay may be necessary.", "output": "Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion-based generative models have achieved remarkable success in variousdomains. It trains a model on denoising tasks that encompass different noiselevels simultaneously, representing a form of multi-task learning (MTL).However, analyzing and improving diffusion models from an MTL perspectiveremains under-explored. In particular, MTL can sometimes lead to the well-knownphenomenon of $textit{negative transfer}$, which results in the performancedegradation of certain tasks due to conflicts between tasks. In this paper, weaim to analyze diffusion training from an MTL standpoint, presenting two keyobservations: $textbf{(O1)}$ the task affinity between denoising tasksdiminishes as the gap between noise levels widens, and $textbf{(O2)}$ negativetransfer can arise even in the context of diffusion training. Building uponthese observations, our objective is to enhance diffusion training bymitigating negative transfer. To achieve this, we propose leveraging existingMTL methods, but the presence of a huge number of denoising tasks makes thiscomputationally expensive to calculate the necessary per-task loss or gradient.To address this challenge, we propose clustering the denoising tasks into smalltask clusters and applying MTL methods to them. Specifically, based on$textbf{(O2)}$, we employ interval clustering to enforce temporal proximityamong denoising tasks within clusters. We show that interval clustering can besolved with dynamic programming and utilize signal-to-noise ratio, timestep,and task affinity for clustering objectives. Through this, our approachaddresses the issue of negative transfer in diffusion models by allowing forefficient computation of MTL methods. We validate the proposed clustering andits integration with MTL methods through various experiments, demonstratingimproved sample quality of diffusion models.", "output": "Addressing Negative Transfer in Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Datasets often have their intrinsic symmetries, and particular deep-learningmodels called equivariant or invariant models have been developed to exploitthese symmetries. However, if some or all of these symmetries are onlyapproximate, which frequently happens in practice, these models may besuboptimal due to the architectural restrictions imposed on them. We tacklethis issue of approximate symmetries in a setup where symmetries are mixed,i.e., they are symmetries of not single but multiple different types and thedegree of approximation varies across these types. Instead of proposing a newarchitectural restriction as in most of the previous approaches, we present aregularizer-based method for building a model for a dataset with mixedapproximate symmetries. The key component of our method is what we callequivariance regularizer for a given type of symmetries, which measures howmuch a model is equivariant with respect to the symmetries of the type. Ourmethod is trained with these regularizers, one per each symmetry type, and thestrength of the regularizers is automatically tuned during training, leading tothe discovery of the approximation levels of some candidate symmetry typeswithout explicit supervision. Using synthetic function approximation and motionforecasting tasks, we demonstrate that our method achieves better accuracy thanprior approaches while discovering the approximate symmetry levels correctly.", "output": "Regularizing Towards Soft Equivariance Under Mixed Symmetries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The emergence of various notions of ``consistency'' in diffusion models hasgarnered considerable attention and helped achieve improved sample quality,likelihood estimation, and accelerated sampling. Although similar concepts havebeen proposed in the literature, the precise relationships among them remainunclear. In this study, we establish theoretical connections between threerecent ``consistency'' notions designed to enhance diffusion models fordistinct objectives. Our insights offer the potential for a more comprehensiveand encompassing framework for consistency-type models.", "output": "On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a method to control the attributes of Language Models (LMs) forthe text generation task using Causal Average Treatment Effect (ATE) scores andcounterfactual augmentation. We explore this method, in the context of LMdetoxification, and propose the Causally Fair Language (CFL) architecture fordetoxifying pre-trained LMs in a plug-and-play manner. Our architecture isbased on a Structural Causal Model (SCM) that is mathematically transparent andcomputationally efficient as compared with many existing detoxificationtechniques. We also propose several new metrics that aim to better understandthe behaviour of LMs in the context of toxic text generation. Further, weachieve state of the art performance for toxic degeneration, which are computedusing RTP (RTP) benchmark. Our experiments show that CFL achieves such adetoxification without much impact on the model perplexity. We also show thatCFL mitigates the unintended bias problem through experiments on the BOLDdataset.", "output": "CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cyber Security is one of the most arising disciplines in our modern society.We work on Cybersecurity domain and in this the topic we chose is CyberSecurity Ontologies. In this we gather all latest and previous ontologies andcompare them on the basis of different analyzing factors to get best of them.Reason to select this topic is to assemble different ontologies from differentera of time. Because, researches that included in this SLR is mostly studiedsingle ontology. If any researcher wants to study ontologies, he has to studyevery single ontology and select which one is best for his research. So, weassemble different types of ontology and compare them against each other to getbest of them. A total 24 papers between years 2010-2020 are carefully selectedthrough systematic process and classified accordingly. Lastly, this SLR havebeen presented to provide the researchers promising future directions in thedomain of cybersecurity ontologies.", "output": "Developing and Building Ontologies in Cyber Security."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When making strategic decisions, we are often confronted with overwhelminginformation to process. The situation can be further complicated when somepieces of evidence are contradicted each other or paradoxical. The challengethen becomes how to determine which information is useful and which ones shouldbe eliminated. This process is known as meta-decision. Likewise, when it comesto using Artificial Intelligence (AI) systems for strategic decision-making,placing trust in the AI itself becomes a meta-decision, given that many AIsystems are viewed as opaque \"black boxes\" that process large amounts of data.Trusting an opaque system involves deciding on the level of Trustworthy AI(TAI). We propose a new approach to address this issue by introducing a noveltaxonomy or framework of TAI, which encompasses three crucial domains:articulate, authentic, and basic for different levels of trust. To underpinthese domains, we create ten dimensions to measure trust:explainability/transparency, fairness/diversity, generalizability, privacy,data governance, safety/robustness, accountability, reproducibility,reliability, and sustainability. We aim to use this taxonomy to conduct acomprehensive survey and explore different TAI approaches from a strategicdecision-making perspective.", "output": "The Survey, Taxonomy, and Future Directions of Trustworthy AI: A Meta Decision of Strategic Decisions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rise in popularity of video-based social media, new categories ofvideos are constantly being generated, creating an urgent need for robustincremental learning techniques for video understanding. One of the biggestchallenges in this task is catastrophic forgetting, where the network tends toforget previously learned data while learning new categories. To overcome thisissue, knowledge distillation is a widely used technique for rehearsal-basedvideo incremental learning that involves transferring important information onsimilarities among different categories to enhance the student model.Therefore, it is preferable to have a strong teacher model to guide thestudents. However, the limited performance of the network itself and theoccurrence of catastrophic forgetting can result in the teacher network makinginaccurate predictions for some memory exemplars, ultimately limiting thestudent network's performance. Based on these observations, we propose ateacher agent capable of generating stable and accurate soft labels to replacethe output of the teacher model. This method circumvents the problem ofknowledge misleading caused by inaccurate predictions of the teacher model andavoids the computational overhead of loading the teacher model for knowledgedistillation. Extensive experiments demonstrate the advantages of our method,yielding significant performance improvements while utilizing only half theresolution of video clips in the incremental phases as input compared to recentstate-of-the-art methods. Moreover, our method surpasses the performance ofjoint training when employing four times the number of samples in episodicmemory.", "output": "Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As information filtering services, recommender systems have extremelyenriched our daily life by providing personalized suggestions and facilitatingpeople in decision-making, which makes them vital and indispensable to humansociety in the information era. However, as people become more dependent onthem, recent studies show that recommender systems potentially ownunintentional impacts on society and individuals because of their unfairness(e.g., gender discrimination in job recommendations). To develop trustworthyservices, it is crucial to devise fairness-aware recommender systems that canmitigate these bias issues. In this survey, we summarise existing methodologiesand practices of fairness in recommender systems. Firstly, we present conceptsof fairness in different recommendation scenarios, comprehensively categorizecurrent advances, and introduce typical methods to promote fairness indifferent stages of recommender systems. Next, after introducing datasets andevaluation metrics applied to assess the fairness of recommender systems, wewill delve into the significant influence that fairness-aware recommendersystems exert on real-world industrial applications. Subsequently, we highlightthe connection between fairness and other principles of trustworthy recommendersystems, aiming to consider trustworthiness principles holistically whileadvocating for fairness. Finally, we summarize this review, spotlightingpromising opportunities in comprehending concepts, frameworks, the balancebetween accuracy and fairness, and the ties with trustworthiness, with theultimate goal of fostering the development of fairness-aware recommendersystems.", "output": "A Survey on Fairness-aware Recommender Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating realistic and controllable motions for virtual characters is achallenging task in computer animation, and its implications extend to games,simulations, and virtual reality. Recent studies have drawn inspiration fromthe success of diffusion models in image generation, demonstrating thepotential for addressing this task. However, the majority of these studies havebeen limited to offline applications that target at sequence-level generationthat generates all steps simultaneously. To enable real-time motion synthesiswith diffusion models in response to time-varying control signals, we proposethe framework of the Controllable Motion Diffusion Model (COMODO). Ourframework begins with an auto-regressive motion diffusion model (A-MDM), whichgenerates motion sequences step by step. In this way, simply using the standardDDPM algorithm without any additional complexity, our framework is able togenerate high-fidelity motion sequences over extended periods with differenttypes of control signals. Then, we propose our reinforcement learning-basedcontroller and controlling strategies on top of the A-MDM model, so that ourframework can steer the motion synthesis process across multiple tasks,including target reaching, joystick-based control, goal-oriented control, andtrajectory following. The proposed framework enables the real-time generationof diverse motions that react adaptively to user commands on-the-fly, therebyenhancing the overall user experience. Besides, it is compatible with theinpainting-based editing methods and can predict much more diverse motionswithout additional fine-tuning of the basic motion generation models. Weconduct comprehensive experiments to evaluate the effectiveness of ourframework in performing various tasks and compare its performance againststate-of-the-art methods.", "output": "Controllable Motion Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, aspect sentiment quad prediction has received widespread attentionin the field of aspect-based sentiment analysis. Existing studies extractquadruplets via pre-trained generative language models to paraphrase theoriginal sentence into a templated target sequence. However, previous worksonly focus on what to generate but ignore what not to generate. We argue thatconsidering the negative samples also leads to potential benefits. In thiswork, we propose a template-agnostic method to control the token-levelgeneration, which boosts original learning and reduces mistakes simultaneously.Specifically, we introduce Monte Carlo dropout to understand the built-inuncertainty of pre-trained language models, acquiring the noises and errors. Wefurther propose marginalized unlikelihood learning to suppress theuncertainty-aware mistake tokens. Finally, we introduce minimization entropy tobalance the effects of marginalized unlikelihood learning. Extensiveexperiments on four public datasets demonstrate the effectiveness of ourapproach on various generation templates1.", "output": "Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence Generated Content (AIGC) is one of the latestachievements in AI development. The content generated by related applications,such as text, images and audio, has sparked a heated discussion. Variousderived AIGC applications are also gradually entering all walks of life,bringing unimaginable impact to people's daily lives. However, the rapiddevelopment of such generative tools has also raised concerns about privacy andsecurity issues, and even copyright issues in AIGC. We note that advancedtechnologies such as blockchain and privacy computing can be combined with AIGCtools, but no work has yet been done to investigate their relevance andprospect in a systematic and detailed way. Therefore it is necessary toinvestigate how they can be used to protect the privacy and security of data inAIGC by fully exploring the aforementioned technologies. In this paper, wefirst systematically review the concept, classification and underlyingtechnologies of AIGC. Then, we discuss the privacy and security challengesfaced by AIGC from multiple perspectives and purposefully list thecountermeasures that currently exist. We hope our survey will help researchersand industry to build a more secure and robust AIGC system.", "output": "Challenges and Remedies to Privacy and Security in AIGC: Exploring the Potential of Privacy Computing, Blockchain, and Beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning (CL) is an important technique to allow artificial neuralnetworks to work in open environments. CL enables a system to learn new taskswithout severe interference to its performance on old tasks, i.e., overcome theproblems of catastrophic forgetting. In joint learning, it is well known thatthe out-of-distribution (OOD) problem caused by intentional attacks orenvironmental perturbations will severely impair the ability of networks togeneralize. In this work, we reported a special form of catastrophic forgettingraised by the OOD problem in continual learning settings, and we named itout-of-distribution forgetting (OODF). In continual image classification tasks,we found that for a given category, introducing an intra-class distributionshift significantly impaired the recognition accuracy of CL methods for thatcategory during subsequent learning. Interestingly, this phenomenon is specialfor CL as the same level of distribution shift had only negligible effects inthe joint learning scenario. We verified that CL methods without dedicatingsubnetworks for individual tasks are all vulnerable to OODF. Moreover, OODFdoes not depend on any specific way of shifting the distribution, suggesting itis a risk for CL in a wide range of circumstances. Taken together, our workidentified an under-attended risk during CL, highlighting the importance ofdeveloping approaches that can overcome OODF.", "output": "Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a set of deterministic algorithms for Russian inflection andautomated text synthesis. These algorithms are implemented in a publiclyavailable web-service www.passare.ru. This service provides functions forinflection of single words, word matching and synthesis of grammaticallycorrect Russian text. Selected code and datasets are available at Performance of the inflectionalfunctions has been tested against the annotated corpus of Russian languageOpenCorpora, compared with that of other solutions, and used for estimating themorphological variability and complexity of different parts of speech inRussian.", "output": "A big data approach towards sarcasm detection in Russian."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)has emerged as a highly successful approach, with training only a small numberof parameters without sacrificing performance and becoming the de-factolearning paradigm with the increasing size of PLMs. However, existing PEFTmethods are not memory-efficient, because they still require caching most ofthe intermediate activations for the gradient calculation, akin to fine-tuning.One effective way to reduce the activation memory is to apply a reversiblemodel, so the intermediate activations are not necessary to be cached and canbe recomputed. Nevertheless, modifying a PLM to its reversible variant withPEFT is not straightforward, since the reversible model has a distinctarchitecture from the currently released PLMs. In this paper, we firstinvestigate what is a key factor for the success of existing PEFT methods, andrealize that it's essential to preserve the PLM's starting point wheninitializing a PEFT method. With this finding, we propose memory-efficientfine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM'sstarting point and making it reversible without additional pre-training. Weevaluate MEFT on the GLUE benchmark and five question-answering tasks withvarious backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces theactivation memory up to 84% of full fine-tuning with a negligible amount oftrainable parameters. Moreover, MEFT achieves the same score on GLUE and acomparable score on the question-answering tasks as full fine-tuning.", "output": "Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Parallel neurosymbolic architectures have been applied effectively in NLP bydistilling knowledge from a logic theory into a deep model.However, prior artfaces several limitations including supporting restricted forms of logictheories and relying on the assumption of independence between the logic andthe deep network. We present Concordia, a framework overcoming the limitationsof prior art. Concordia is agnostic both to the deep network and the logictheory offering support for a wide range of probabilistic theories. Ourframework can support supervised training of both components and unsupervisedtraining of the neural component. Concordia has been successfully applied totasks beyond NLP and data classification, improving the accuracy ofstate-of-the-art on collective activity detection, entity linking andrecommendation tasks.", "output": "Parallel Neurosymbolic Integration with Concordia."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of non-monotonic logics, the notion of Rational Closure (RC) isacknowledged as a prominent approach. In recent years, RC has gained even morepopularity in the context of Description Logics (DLs), the logic underpinningthe semantic web standard ontology language OWL 2, whose main ingredients areclasses and roles. In this work, we show how to integrate RC within the triplelanguage RDFS, which together with OWL2 are the two major standard semantic webontology languages. To do so, we start from $rho df$, which is the logicbehind RDFS, and then extend it to $rho df_bot$, allowing to state that twoentities are incompatible. Eventually, we propose defeasible $rho df_bot$ viaa typical RC construction. The main features of our approach are: (i) unlikemost other approaches that add an extra non-monotone rule layer on top ofmonotone RDFS, defeasible $rho df_bot$ remains syntactically a triplelanguage and is a simple extension of $rho df_bot$ by introducing some newpredicate symbols with specific semantics. In particular, any RDFSreasoner/store may handle them as ordinary terms if it does not want to takeaccount for the extra semantics of the new predicate symbols; (ii) thedefeasible $rho df_bot$ entailment decision procedure is build on top of the$rho df_bot$ entailment decision procedure, which in turn is an extension ofthe one for $rho df$ via some additional inference rules favouring anpotential implementation; and (iii) defeasible $rho df_bot$ entailment can bedecided in polynomial time.", "output": "Defeasible RDFS via Rational Closure."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks have become increasingly of interest in dynamical systemprediction, but out-of-distribution generalization and long-term stabilitystill remains challenging. In this work, we treat the domain parameters ofdynamical systems as factors of variation of the data generating process. Byleveraging ideas from supervised disentanglement and causal factorization, weaim to separate the domain parameters from the dynamics in the latent space ofgenerative models. In our experiments we model dynamics both in phase space andin video sequences and conduct rigorous OOD evaluations. Results indicate thatdisentangled VAEs adapt better to domain parameters spaces that were notpresent in the training data. At the same time, disentanglement can improve thelong-term and out-of-distribution predictions of state-of-the-art models invideo sequences.", "output": "Disentangled Generative Models for Robust Prediction of System Dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Network representation learning aims to generate an embedding for each nodein a network, which facilitates downstream machine learning tasks such as nodeclassification and link prediction. Current work mainly focuses on transductivenetwork representation learning, i.e. generating fixed node embeddings, whichis not suitable for real-world applications. Therefore, we propose a newinductive network representation learning method called MNCI by miningneighborhood and community influences in temporal networks. We propose anaggregator function that integrates neighborhood influence with communityinfluence to generate node embeddings at any time. We conduct extensiveexperiments on several real-world datasets and compare MNCI with severalstate-of-the-art baseline methods on various tasks, including nodeclassification and network visualization. The experimental results show thatMNCI achieves better performance than baselines.", "output": "Inductive Representation Learning in Temporal Networks via Mining Neighborhood and Community Influences."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) haverecently emerged as a promising direction for tuning Deep Learning methods.However, existing methods suffer from a sub-optimal allocation of the HPObudget to the hyperparameter configurations. In this work, we introduce DyHPO,a Bayesian Optimization method that learns to decide which hyperparameterconfiguration to train further in a dynamic race among all feasibleconfigurations. We propose a new deep kernel for Gaussian Processes that embedsthe learning curve dynamics, and an acquisition function that incorporatesmulti-budget information. We demonstrate the significant superiority of DyHPOagainst state-of-the-art hyperparameter optimization methods throughlarge-scale experiments comprising 50 datasets (Tabular, Image, NLP) anddiverse architectures (MLP, CNN/NAS, RNN).", "output": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wider adoption of neural networks in many critical domains such as financeand healthcare is being hindered by the need to explain their predictions andto impose additional constraints on them. Monotonicity constraint is one of themost requested properties in real-world scenarios and is the focus of thispaper. One of the oldest ways to construct a monotonic fully connected neuralnetwork is to constrain signs on its weights. Unfortunately, this constructiondoes not work with popular non-saturated activation functions as it can onlyapproximate convex functions. We show this shortcoming can be fixed byconstructing two additional activation functions from a typical unsaturatedmonotonic activation function and employing each of them on the part ofneurons. Our experiments show this approach of building monotonic neuralnetworks has better accuracy when compared to other state-of-the-art methods,while being the simplest one in the sense of having the least number ofparameters, and not requiring any modifications to the learning procedure orpost-learning steps. Finally, we prove it can approximate any continuousmonotone function on a compact subset of $mathbb{R}^n$.", "output": "Constrained Monotonic Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to converse with humans and follow natural language commands iscrucial for intelligent unmanned aerial vehicles (a.k.a. drones). It canrelieve people's burden of holding a controller all the time, allowmultitasking, and make drone control more accessible for people withdisabilities or with their hands occupied. To this end, we introduce AerialVision-and-Dialog Navigation (AVDN), to navigate a drone via natural languageconversation. We build a drone simulator with a continuous photorealisticenvironment and collect a new AVDN dataset of over 3k recorded navigationtrajectories with asynchronous human-human dialogs between commanders andfollowers. The commander provides initial navigation instruction and furtherguidance by request, while the follower navigates the drone in the simulatorand asks questions when needed. During data collection, followers' attention onthe drone's visual observation is also recorded. Based on the AVDN dataset, westudy the tasks of aerial navigation from (full) dialog history and propose aneffective Human Attention Aided Transformer model (HAA-Transformer), whichlearns to predict both navigation waypoints and human attention.", "output": "Aerial Vision-and-Dialog Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Toward combining inductive reasoning with perception abilities, we developtechniques for neurosymbolic program synthesis where perceptual input is firstparsed by neural nets into a low-dimensional interpretable representation,which is then processed by a synthesized program. We explore several techniquesfor relaxing the problem and jointly learning all modules end-to-end withgradient descent: multitask learning; amortized inference;overparameterization; and a differentiable strategy for penalizing lengthyprograms. Collectedly this toolbox improves the stability of gradient-guidedprogram search, and suggests ways of learning both how to perceive input asdiscrete abstractions, and how to symbolically process those abstractions asprograms.", "output": "From Perception to Programs: Regularize, Overparameterize, and Amortize."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nonlinear independent component analysis (ICA) aims to recover the underlyingindependent latent sources from their observable nonlinear mixtures. How tomake the nonlinear ICA model identifiable up to certain trivial indeterminaciesis a long-standing problem in unsupervised learning. Recent breakthroughsreformulate the standard independence assumption of sources as conditionalindependence given some auxiliary variables (e.g., class labels and/ordomain/time indexes) as weak supervision or inductive bias. However, nonlinearICA with unconditional priors cannot benefit from such developments. We explorean alternative path and consider only assumptions on the mixing process, suchas Structural Sparsity. We show that under specific instantiations of suchconstraints, the independent latent sources can be identified from theirnonlinear mixtures up to a permutation and a component-wise transformation,thus achieving nontrivial identifiability of nonlinear ICA without auxiliaryvariables. We provide estimation methods and validate the theoretical resultsexperimentally. The results on image data suggest that our conditions may holdin a number of practical data generating processes.", "output": "On the Identifiability of Nonlinear ICA: Sparsity and Beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tiny machine learning (TinyML), executing AI workloads on resource and powerstrictly restricted systems, is an important and challenging topic. This brieffirstly presents an extremely tiny backbone to construct high efficiency CNNmodels for various visual tasks. Then, a specially designed neural co-processor(NCP) is interconnected with MCU to build an ultra-low power TinyML system,which stores all features and weights on chip and completely removes both oflatency and power consumption in off-chip memory access. Furthermore, anapplication specific instruction-set is further presented for realizing agiledevelopment and rapid deployment. Extensive experiments demonstrate that theproposed TinyML system based on our model, NCP and instruction set yieldsconsiderable accuracy and achieves a record ultra-low power of 160mW whileimplementing object detection and recognition at 30FPS. The demo video isavailable on url{", "output": "An Ultra-low Power TinyML System for Real-time Visual Processing at Edge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Claim-Dissector: a novel latent variable model for fact-checkingand analysis, which given a claim and a set of retrieved evidences jointlylearns to identify: (i) the relevant evidences to the given claim, (ii) theveracity of the claim. We propose to disentangle the per-evidence relevanceprobability and its contribution to the final veracity probability in aninterpretable way -- the final veracity probability is proportional to a linearensemble of per-evidence relevance probabilities. In this way, the individualcontributions of evidences towards the final predicted probability can beidentified. In per-evidence relevance probability, our model can furtherdistinguish whether each relevant evidence is supporting (S) or refuting (R)the claim. This allows to quantify how much the S/R probability contributes tothe final verdict or to detect disagreeing evidence.Despite its interpretable nature, our system achieves results competitivewith state-of-the-art on the FEVER dataset, as compared to typical two-stagesystem pipelines, while using significantly fewer parameters. It also sets newstate-of-the-art on FAVIQ and RealFC datasets. Furthermore, our analysis showsthat our model can learn fine-grained relevance cues while using coarse-grainedsupervision, and we demonstrate it in 2 ways. (i) We show that our model canachieve competitive sentence recall while using only paragraph-level relevancesupervision. (ii) Traversing towards the finest granularity of relevance, weshow that our model is capable of identifying relevance at the token level. Todo this, we present a new benchmark TLR-FEVER focusing on token-levelinterpretability -- humans annotate tokens in relevant evidences theyconsidered essential when making their judgment. Then we measure how similarare these annotations to the tokens our model is focusing on.", "output": "Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An inherent problem of reinforcement learning is performing exploration of anenvironment through random actions, of which a large portion can beunproductive. Instead, exploration can be improved by initializing the learningpolicy with an existing (previously learned or hard-coded) oracle policy,offline data, or demonstrations. In the case of using an oracle policy, it canbe unclear how best to incorporate the oracle policy's experience into thelearning policy in a way that maximizes learning sample efficiency. In thispaper, we propose a method termed Critic Confidence Guided Exploration (CCGE)for incorporating such an oracle policy into standard actor-criticreinforcement learning algorithms. More specifically, CCGE takes in the oraclepolicy's actions as suggestions and incorporates this information into thelearning scheme when uncertainty is high, while ignoring it when theuncertainty is low. CCGE is agnostic to methods of estimating uncertainty, andwe show that it is equally effective with two different techniques.Empirically, we evaluate the effect of CCGE on various benchmark reinforcementlearning tasks, and show that this idea can lead to improved sample efficiencyand final performance. Furthermore, when evaluated on sparse rewardenvironments, CCGE is able to perform competitively against adjacent algorithmsthat also leverage an oracle policy. Our experiments show that it is possibleto utilize uncertainty as a heuristic to guide exploration using an oracle inreinforcement learning. We expect that this will inspire more research in thisdirection, where various heuristics are used to determine the direction ofguidance provided to learning.", "output": "Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans can attribute beliefs to others. However, it is unknown to what extentthis ability results from an innate biological endowment or from experienceaccrued through child development, particularly exposure to language describingothers' mental states. We test the viability of the language exposurehypothesis by assessing whether models exposed to large quantities of humanlanguage display sensitivity to the implied knowledge states of characters inwritten passages. In pre-registered analyses, we present a linguistic versionof the False Belief Task to both human participants and a Large Language Model,GPT-3. Both are sensitive to others' beliefs, but while the language modelsignificantly exceeds chance behavior, it does not perform as well as thehumans, nor does it explain the full extent of their behavior -- despite beingexposed to more language than a human would in a lifetime. This suggests thatwhile statistical learning from language exposure may in part explain howhumans develop the ability to reason about the mental states of others, othermechanisms are also responsible.", "output": "Do Large Language Models know what humans know?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpretable machine learning seeks to understand the reasoning process ofcomplex black-box systems that are long notorious for lack of explainability.One flourishing approach is through counterfactual explanations, which providesuggestions on what a user can do to alter an outcome. Not only must acounterfactual example counter the original prediction from the black-boxclassifier but it should also satisfy various constraints for practicalapplications. Diversity is one of the critical constraints that however remainsless discussed. While diverse counterfactuals are ideal, it is computationallychallenging to simultaneously address some other constraints. Furthermore,there is a growing privacy concern over the released counterfactual data. Tothis end, we propose a feature-based learning framework that effectivelyhandles the counterfactual constraints and contributes itself to the limitedpool of private explanation models. We demonstrate the flexibility andeffectiveness of our method in generating diverse counterfactuals ofactionability and plausibility. Our counterfactual engine is more efficientthan counterparts of the same capacity while yielding the lowestre-identification risks.", "output": "Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To generate accurate videos, algorithms have to understand the spatial andtemporal dependencies in the world. Current algorithms enable accuratepredictions over short horizons but tend to suffer from temporalinconsistencies. When generated content goes out of view and is laterrevisited, the model invents different content instead. Despite this severelimitation, no established benchmarks on complex data exist for rigorouslyevaluating video generation with long temporal dependencies. In this paper, wecurate 3 challenging video datasets with long-range dependencies by renderingwalks through 3D scenes of procedural mazes, Minecraft worlds, and indoorscans. We perform a comprehensive evaluation of current models and observetheir limitations in temporal consistency. Moreover, we introduce theTemporally Consistent Transformer (TECO), a generative model that substantiallyimproves long-term consistency while also reducing sampling time. Bycompressing its input sequence into fewer embeddings, applying a temporaltransformer, and expanding back using a spatial MaskGit, TECO outperformsexisting models across many metrics. Videos are available on the website:", "output": "Temporally Consistent Transformers for Video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the class of location-scale or heteroscedastic noise models (LSNMs),in which the effect $Y$ can be written as a function of the cause $X$ and anoise source $N$ independent of $X$, which may be scaled by a positive function$g$ over the cause, i.e., $Y = f(X) + g(X)N$. Despite the generality of themodel class, we show the causal direction is identifiable up to somepathological cases. To empirically validate these theoretical findings, wepropose two estimators for LSNMs: an estimator based on (non-linear) featuremaps, and one based on neural networks. Both model the conditional distributionof $Y$ given $X$ as a Gaussian parameterized by its natural parameters. Whenthe feature maps are correctly specified, we prove that our estimator isjointly concave, and a consistent estimator for the cause-effect identificationtask. Although the the neural network does not inherit those guarantees, it canfit functions of arbitrary complexity, and reaches state-of-the-art performanceacross benchmarks.", "output": "On the Identifiability and Estimation of Causal Location-Scale Noise Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixup is a data augmentation technique that relies on training using randomconvex combinations of data points and their labels. In recent years, Mixup hasbecome a standard primitive used in the training of state-of-the-art imageclassification models due to its demonstrated benefits over empirical riskminimization with regards to generalization and robustness. In this work, wetry to explain some of this success from a feature learning perspective. Wefocus our attention on classification problems in which each class may havemultiple associated features (or views) that can be used to predict the classcorrectly. Our main theoretical results demonstrate that, for a non-trivialclass of data distributions with two features per class, training a 2-layerconvolutional network using empirical risk minimization can lead to learningonly one feature for almost all classes while training with a specificinstantiation of Mixup succeeds in learning both features for every class. Wealso show empirically that these theoretical insights extend to the practicalsettings of image benchmarks modified to have multiple features.", "output": "Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Streaming models are an essential component of real-time speech enhancementtools. The streaming regime constrains speech enhancement models to use only atiny context of future information. As a result, the low-latency streamingsetup is generally considered a challenging task and has a significant negativeimpact on the model's quality. However, the sequential nature of streaminggeneration offers a natural possibility for autoregression, that is, utilizingprevious predictions while making current ones. The conventional method fortraining autoregressive models is teacher forcing, but its primary drawbacklies in the training-inference mismatch that can lead to a substantialdegradation in quality. In this study, we propose a straightforward yeteffective alternative technique for training autoregressive low-latency speechenhancement models. We demonstrate that the proposed approach leads to stableimprovement across diverse architectures and training scenarios.", "output": "Iterative autoregression: a novel trick to improve your low-latency speech enhancement model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There have been several meta-evaluation studies on the correlation betweenhuman ratings and offline machine translation (MT) evaluation metrics such asBLEU, chrF2, BertScore and COMET. These metrics have been used to evaluatesimultaneous speech translation (SST) but their correlations with human ratingsof SST, which has been recently collected as Continuous Ratings (CR), areunclear. In this paper, we leverage the evaluations of candidate systemssubmitted to the English-German SST task at IWSLT 2022 and conduct an extensivecorrelation analysis of CR and the aforementioned metrics. Our study revealsthat the offline metrics are well correlated with CR and can be reliably usedfor evaluating machine translation in simultaneous mode, with some limitationson the test set size. We conclude that given the current quality levels of SST,these metrics can be used as proxies for CR, alleviating the need for largescale human evaluation. Additionally, we observe that correlations of themetrics with translation as a reference is significantly higher than withsimultaneous interpreting, and thus we recommend the former for reliableevaluation.", "output": "MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To align conditional text generation model outputs with desired behaviors,there has been an increasing focus on training the model using reinforcementlearning (RL) with reward functions learned from human annotations. Under thisframework, we identify three common cases where high rewards are incorrectlyassigned to undesirable patterns: noise-induced spurious correlation, naturallyoccurring spurious correlation, and covariate shift. We show that even thoughlearned metrics achieve high performance on the distribution of the data usedto train the reward function, the undesirable patterns may be amplified duringRL training of the text generation model. While there has been discussion aboutreward gaming in the RL or safety community, in this discussion piece, we wouldlike to highlight reward gaming in the natural language generation (NLG)community using concrete conditional text generation examples and discusspotential fixes and areas for future work.", "output": "Reward Gaming in Conditional Text Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Behavior constrained policy optimization has been demonstrated to be asuccessful paradigm for tackling Offline Reinforcement Learning. By exploitinghistorical transitions, a policy is trained to maximize a learned valuefunction while constrained by the behavior policy to avoid a significantdistributional shift. In this paper, we propose our closed-form policyimprovement operators. We make a novel observation that the behavior constraintnaturally motivates the use of first-order Taylor approximation, leading to alinear approximation of the policy objective. Additionally, as practicaldatasets are usually collected by heterogeneous policies, we model the behaviorpolicies as a Gaussian Mixture and overcome the induced optimizationdifficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality,giving rise to a closed-form policy improvement operator. We instantiateoffline RL algorithms with our novel policy improvement operators andempirically demonstrate their effectiveness over state-of-the-art algorithms onthe standard D4RL benchmark. Our code is available at", "output": "Offline Reinforcement Learning with Closed-Form Policy Improvement Operators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Neighborhood Search (LNS) is a popular heuristic algorithm for solvingcombinatorial optimization problems (COP). It starts with an initial solutionto the problem and iteratively improves it by searching a large neighborhoodaround the current best solution. LNS relies on heuristics to selectneighborhoods to search in. In this paper, we focus on designing effective andefficient heuristics in LNS for integer linear programs (ILP) since a widerange of COPs can be represented as ILPs. Local Branching (LB) is a heuristicthat selects the neighborhood that leads to the largest improvement over thecurrent solution in each iteration of LNS. LB is often slow since it needs tosolve an ILP of the same size as input. Our proposed heuristics, LB-RELAX andits variants, use the linear programming relaxation of LB to selectneighborhoods. Empirically, LB-RELAX and its variants compute as effectiveneighborhoods as LB but run faster. They achieve state-of-the-art anytimeperformance on several ILP benchmarks.", "output": "Local Branching Relaxation Heuristics for Integer Linear Programs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning, as an essential ability for complex problem-solving, can provideback-end support for various real-world applications, such as medicaldiagnosis, negotiation, etc. This paper provides a comprehensive survey ofcutting-edge research on reasoning with language model prompting. We introduceresearch works with comparisons and summaries and provide systematic resourcesto help beginners. We also discuss the potential reasons for emerging suchreasoning abilities and highlight future research directions. Resources areavailable at  (updatedperiodically).", "output": "Reasoning with Language Model Prompting: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the increasing ability of large language models (LLMs), in-contextlearning (ICL) has become a new paradigm for natural language processing (NLP),where LLMs make predictions only based on contexts augmented with a fewexamples. It has been a new trend to explore ICL to evaluate and extrapolatethe ability of LLMs. In this paper, we aim to survey and summarize the progressand challenges of ICL. We first present a formal definition of ICL and clarifyits correlation to related studies. Then, we organize and discuss advancedtechniques, including training strategies, demonstration designing strategies,as well as related analysis. Finally, we discuss the challenges of ICL andprovide potential directions for further research. We hope that our work canencourage more research on uncovering how ICL works and improving ICL.", "output": "A Survey on In-context Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We provide a theoretical framework for Reinforcement Learning with HumanFeedback (RLHF). Our analysis shows that when the true reward function islinear, the widely used maximum likelihood estimator (MLE) converges under boththe Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However,we show that when training a policy based on the learned reward model, MLEfails while a pessimistic MLE provides policies with improved performance undercertain coverage assumptions. Additionally, we demonstrate that under the PLmodel, the true MLE and an alternative MLE that splits the $K$-wise comparisoninto pairwise comparisons both converge. Moreover, the true MLE isasymptotically more efficient. Our results validate the empirical success ofexisting RLHF algorithms in InstructGPT and provide new insights for algorithmdesign. Furthermore, our results unify the problem of RLHF and max-entropyInverse Reinforcement Learning (IRL), and provide the first sample complexitybound for max-entropy IRL.", "output": "Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial learners often behave differently from human learners in thecontext of neural agent-based simulations of language emergence and change. Acommon explanation is the lack of appropriate cognitive biases in theselearners. However, it has also been proposed that more naturalistic settings oflanguage learning and use could lead to more human-like results. We investigatethis latter account focusing on the word-order/case-marking trade-off, a widelyattested language universal that has proven particularly hard to simulate. Wepropose a new Neural-agent Language Learning and Communication framework(NeLLCom) where pairs of speaking and listening agents first learn a miniaturelanguage via supervised learning, and then optimize it for communication viareinforcement learning. Following closely the setup of earlier humanexperiments, we succeed in replicating the trade-off with the new frameworkwithout hard-coding specific biases in the agents. We see this as an essentialstep towards the investigation of language universals with neural learners.", "output": "Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Demographic parity is the most widely recognized measure of group fairness inmachine learning, which ensures equal treatment of different demographicgroups. Numerous works aim to achieve demographic parity by pursuing thecommonly used metric $Delta DP$. Unfortunately, in this paper, we reveal thatthe fairness metric $Delta DP$ can not precisely measure the violation ofdemographic parity, because it inherently has the following drawbacks: i)zero-value $Delta DP$ does not guarantee zero violation of demographic parity,ii) $Delta DP$ values can vary with different classification thresholds. Tothis end, we propose two new fairness metrics, Area Between Probability densityfunction Curves (ABPC) and Area Between Cumulative density function Curves(ABCC), to precisely measure the violation of demographic parity at thedistribution level. The new fairness metrics directly measure the differencebetween the distributions of the prediction probability for differentdemographic groups. Thus our proposed new metrics enjoy: i) zero-valueABCC/ABPC guarantees zero violation of demographic parity; ii) ABCC/ABPCguarantees demographic parity while the classification thresholds are adjusted.We further re-evaluate the existing fair models with our proposed fairnessmetrics and observe different fairness behaviors of those models under the newmetrics. The code is available at", "output": "Retiring $\\Delta$DP: New Distribution-Level Metrics for Demographic Parity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Retrosynthesis, which aims to find a route to synthesize a target moleculefrom commercially available starting materials, is a critical task in drugdiscovery and materials design. Recently, the combination of ML-basedsingle-step reaction predictors with multi-step planners has led to promisingresults. However, the single-step predictors are mostly trained offline tooptimize the single-step accuracy, without considering complete routes. Here,we leverage reinforcement learning (RL) to improve the single-step predictor,by using a tree-shaped MDP to optimize complete routes. Specifically, wepropose a novel online training algorithm, called Planning with Dual ValueNetworks (PDVN), which alternates between the planning phase and updatingphase. In PDVN, we construct two separate value networks to predict thesynthesizability and cost of molecules, respectively. To maintain thesingle-step accuracy, we design a two-branch network structure for thesingle-step predictor. On the widely-used USPTO dataset, our PDVN algorithmimproves the search success rate of existing multi-step planners (e.g.,increasing the success rate from 85.79% to 98.95% for Retro*, and reducing thenumber of model calls by half while solving 99.47% molecules for RetroGraph).Additionally, PDVN helps find shorter synthesis routes (e.g., reducing theaverage route length from 5.76 to 4.83 for Retro*, and from 5.63 to 4.78 forRetroGraph).", "output": "Retrosynthetic Planning with Dual Value Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an efficient method to ground pretrained text-only language modelsto the visual domain, enabling them to process arbitrarily interleavedimage-and-text data, and generate text interleaved with retrieved images. Ourmethod leverages the abilities of language models learnt from large scaletext-only pretraining, such as in-context learning and free-form textgeneration. We keep the language model frozen, and finetune input and outputlinear layers to enable cross-modality interactions. This allows our model toprocess arbitrarily interleaved image-and-text inputs, and generate free-formtext interleaved with retrieved images. We achieve strong zero-shot performanceon grounded tasks such as contextual image retrieval and multimodal dialogue,and showcase compelling interactive abilities. Our approach works with anyoff-the-shelf language model and paves the way towards an effective, generalsolution for leveraging pretrained language models in visually groundedsettings.", "output": "Grounding Language Models to Images for Multimodal Inputs and Outputs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weight oscillation is an undesirable side effect of quantization-awaretraining, in which quantized weights frequently jump between two quantizedlevels, resulting in training instability and a sub-optimal final model. Wediscover that the learnable scaling factor, a widely-used $textit{de facto}$setting in quantization aggravates weight oscillation. In this study, weinvestigate the connection between the learnable scaling factor and quantizedweight oscillation and use ViT as a case driver to illustrate the findings andremedies. In addition, we also found that the interdependence between quantizedweights in $textit{query}$ and $textit{key}$ of a self-attention layer makesViT vulnerable to oscillation. We, therefore, propose three techniquesaccordingly: statistical weight quantization ($rm StatsQ$) to improvequantization robustness compared to the prevalent learnable-scale-based method;confidence-guided annealing ($rm CGA$) that freezes the weights with$textit{high confidence}$ and calms the oscillating weights; and$textit{query}$-$textit{key}$ reparameterization ($rm QKR$) to resolve thequery-key intertwined oscillation and mitigate the resulting gradientmisestimation. Extensive experiments demonstrate that these proposed techniquessuccessfully abate weight oscillation and consistently achieve substantialaccuracy improvement on ImageNet. Specifically, our 2-bit DeiT-T/DeiT-Salgorithms outperform the previous state-of-the-art by 9.8% and 7.7%,respectively. Code and models are available at:url{", "output": "Oscillation-free Quantization for Low-bit Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has emerged as a powerful framework to learnrepresentations from raw data without supervision. Yet in practice, engineersface issues such as instability in tuning optimizers and collapse ofrepresentations during training. Such challenges motivate the need for a theoryto shed light on the complex interplay between the choice of data augmentation,network architecture, and training algorithm. We study such an interplay with aprecise analysis of generalization performance on both pretraining anddownstream tasks in a theory friendly setup, and highlight several insights forSSL practitioners that arise from our theory.", "output": "The SSL Interplay: Augmentations, Inductive Bias, and Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the advance of language models, privacy protection is receiving moreattention. Training data extraction is therefore of great importance, as it canserve as a potential tool to assess privacy leakage. However, due to thedifficulty of this task, most of the existing methods are proof-of-concept andstill not effective enough. In this paper, we investigate and benchmark tricksfor improving training data extraction using a publicly available dataset.Because most existing extraction methods use a pipeline ofgenerating-then-ranking, i.e., generating text candidates as potential trainingdata and then ranking them based on specific criteria, our research focuses onthe tricks for both text generation (e.g., sampling strategy) and text ranking(e.g., token-level criteria). The experimental results show that severalpreviously overlooked tricks can be crucial to the success of training dataextraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricksoutperform the baseline by a large margin in most cases, providing a muchstronger baseline for future research. The code is available at", "output": "Bag of Tricks for Training Data Extraction from Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It has been recognized that the data generated by the denoising diffusionprobabilistic model (DDPM) improves adversarial training. After two years ofrapid development in diffusion models, a question naturally arises: can betterdiffusion models further improve adversarial training? This paper gives anaffirmative answer by employing the most recent diffusion model which hashigher efficiency ($sim 20$ sampling steps) and image quality (lower FIDscore) compared with DDPM. Our adversarially trained models achievestate-of-the-art performance on RobustBench using only generated data (noexternal datasets). Under the $ell_infty$-norm threat model with$epsilon=8/255$, our models achieve $70.69%$ and $42.67%$ robust accuracy onCIFAR-10 and CIFAR-100, respectively, i.e. improving upon previousstate-of-the-art models by $+4.58%$ and $+8.03%$. Under the $ell_2$-normthreat model with $epsilon=128/255$, our models achieve $84.86%$ on CIFAR-10($+4.44%$). These results also beat previous works that use external data. Wealso provide compelling results on the SVHN and TinyImageNet datasets. Our codeis available at ", "output": "Better Diffusion Models Further Improve Adversarial Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While Reinforcement Learning (RL) achieves tremendous success in sequentialdecision-making problems of many domains, it still faces key challenges of datainefficiency and the lack of interpretability. Interestingly, many researchershave leveraged insights from the causality literature recently, bringing forthflourishing works to unify the merits of causality and address well thechallenges from RL. As such, it is of great necessity and significance tocollate these Causal Reinforcement Learning (CRL) works, offer a review of CRLmethods, and investigate the potential functionality from causality toward RL.In particular, we divide existing CRL approaches into two categories accordingto whether their causality-based information is given in advance or not. Wefurther analyze each category in terms of the formalization of differentmodels, ranging from the Markov Decision Process (MDP), Partially ObservedMarkov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic TreatmentRegime (DTR). Moreover, we summarize the evaluation matrices and open sourceswhile we discuss emerging applications, along with promising prospects for thefuture development of CRL.", "output": "A Survey on Causal Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) are powerful tools, capable of leveraging theirtraining on natural language to write stories, generate code, and answerquestions. But can they generate functional video game levels? Game levels,with their complex functional constraints and spatial relationships in morethan one dimension, are very different from the kinds of data an LLM typicallysees during training. Datasets of game levels are also hard to come by,potentially taxing the abilities of these data-hungry models. We investigatethe use of LLMs to generate levels for the game Sokoban, finding that LLMs areindeed capable of doing so, and that their performance scales dramatically withdataset size. We also perform preliminary experiments on controlling LLM levelgenerators and discuss promising areas for future work.", "output": "Level Generation Through Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work aims to push the limits of agility for bipedal robots by enabling atorque-controlled bipedal robot to perform robust and versatile dynamic jumpsin the real world. We present a reinforcement learning framework for training arobot to accomplish a large variety of jumping tasks, such as jumping todifferent locations and directions. To improve performance on these challengingtasks, we develop a new policy structure that encodes the robot's long-terminput/output (I/O) history while also providing direct access to a short-termI/O history. In order to train a versatile jumping policy, we utilize amulti-stage training scheme that includes different training stages fordifferent objectives. After multi-stage training, the policy can be directlytransferred to a real bipedal Cassie robot. Training on different tasks andexploring more diverse scenarios lead to highly robust policies that canexploit the diverse set of learned maneuvers to recover from perturbations orpoor landings during real-world deployment. Such robustness in the proposedpolicy enables Cassie to succeed in completing a variety of challenging jumptasks in the real world, such as standing long jumps, jumping onto elevatedplatforms, and multi-axes jumps.", "output": "Robust and Versatile Bipedal Jumping Control through Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning agent behaviors from observational data has shown to improve ourunderstanding of their decision-making processes, advancing our ability toexplain their interactions with the environment and other agents. Whilemultiple learning techniques have been proposed in the literature, there is oneparticular setting that has not been explored yet: multi agent systems whereagent identities remain anonymous. For instance, in financial markets labeleddata that identifies market participant strategies is typically proprietary,and only the anonymous state-action pairs that result from the interaction ofmultiple market participants are publicly available. As a result, sequences ofagent actions are not observable, restricting the applicability of existingwork. In this paper, we propose a Policy Clustering algorithm, called K-SHAP,that learns to group anonymous state-action pairs according to the agentpolicies. We frame the problem as an Imitation Learning (IL) task, and we learna world-policy able to mimic all the agent behaviors upon differentenvironmental states. We leverage the world-policy to explain each anonymousobservation through an additive feature attribution method called SHAP (SHapleyAdditive exPlanations). Finally, by clustering the explanations we show that weare able to identify different agent policies and group observationsaccordingly. We evaluate our approach on simulated synthetic market data and areal-world financial dataset. We show that our proposal significantly andconsistently outperforms the existing methods, identifying different agentstrategies.", "output": "K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sparse coding, which refers to modeling a signal as sparse linearcombinations of the elements of a learned dictionary, has proven to be asuccessful (and interpretable) approach in applications such as signalprocessing, computer vision, and medical imaging. While this success hasspurred much work on provable guarantees for dictionary recovery when thelearned dictionary is the same size as the ground-truth dictionary, work on thesetting where the learned dictionary is larger (or over-realized) with respectto the ground truth is comparatively nascent. Existing theoretical results inthis setting have been constrained to the case of noise-less data. We show inthis work that, in the presence of noise, minimizing the standard dictionarylearning objective can fail to recover the elements of the ground-truthdictionary in the over-realized regime, regardless of the magnitude of thesignal in the data-generating process. Furthermore, drawing from the growingbody of work on self-supervised learning, we propose a novel masking objectivefor which recovering the ground-truth dictionary is in fact optimal as thesignal increases for a large class of data-generating processes. We corroborateour theoretical results with experiments across several parameter regimesshowing that our proposed objective also enjoys better empirical performancethan the standard reconstruction objective.", "output": "Hiding Data Helps: On the Benefits of Masking for Sparse Coding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The emergence of an AI-powered chatbot that can generate human-like sentencesand write coherent essays has caught the world's attention. This paperdiscusses the historical overview of chatbots and the technology behind ChatGenerative Pre-trained Transformer, better known as ChatGPT. Moreover,potential applications of ChatGPT in various domains, including healthcare,education, and research, are highlighted. Despite promising results, there areseveral privacy and ethical concerns surrounding ChatGPT. In addition, wehighlight some of the important limitations of the current version of ChatGPT.We also ask ChatGPT to provide its point of view and present its responses toseveral questions we attempt to answer.", "output": "Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of relative representations for latent embeddings has shown potentialin enabling latent space communication and zero-shot model stitching across awide range of applications. Nevertheless, relative representations rely on acertain amount of parallel anchors to be given as input, which can beimpractical to obtain in certain scenarios. To overcome this limitation, wepropose an optimization-based method to discover new parallel anchors from alimited known set (seed). Our approach can be used to find semanticcorrespondence between different domains, align their relative spaces, andachieve competitive results in several tasks.", "output": "Bootstrapping Parallel Anchors for Relative Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-language models have recently demonstrated impressive zero-shotcapabilities in a variety of natural language tasks such as summarization,dialogue generation, and question-answering. Despite many promisingapplications in clinical medicine, adoption of these models in real-worldsettings has been largely limited by their tendency to generate incorrect andsometimes even toxic statements. In this study, we develop Almanac, a largelanguage model framework augmented with retrieval capabilities for medicalguideline and treatment recommendations. Performance on a novel dataset ofclinical scenarios (n = 130) evaluated by a panel of 5 board-certified andresident physicians demonstrates significant increases in factuality (mean of18% at p-value &lt; 0.05) across all specialties, with improvements incompleteness and safety. Our results demonstrate the potential for largelanguage models to be effective tools in the clinical decision-making process,while also emphasizing the importance of careful testing and deployment tomitigate their shortcomings.", "output": "Almanac: Retrieval-Augmented Language Models for Clinical Medicine."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Query-focused meeting summarization (QFMS) aims to generate summaries frommeeting transcripts in response to a given query. Previous works typicallyconcatenate the query with meeting transcripts and implicitly model the queryrelevance only at the token level with attention mechanism. However, due to thedilution of key query-relevant information caused by long meeting transcripts,the original transformer-based model is insufficient to highlight the key partsrelated to the query. In this paper, we propose a query-aware framework withjoint modeling token and utterance based on Query-Utterance Attention. Itcalculates the utterance-level relevance to the query with a dense retrievalmodule. Then both token-level query relevance and utterance-level queryrelevance are combined and incorporated into the generation process withattention mechanism explicitly. We show that the query relevance of differentgranularities contributes to generating a summary more related to the query.Experimental results on the QMSum dataset show that the proposed model achievesnew state-of-the-art performance.", "output": "Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many real-world applications based on online learning produce streaming datathat is haphazard in nature, i.e., contains missing features, features becomingobsolete in time, the appearance of new features at later points in time and alack of clarity on the total number of input features. These challenges make ithard to build a learnable system for such applications, and almost no workexists in deep learning that addresses this issue. In this paper, we presentAux-Drop, an auxiliary dropout regularization strategy for online learning thathandles the haphazard input features in an effective manner. Aux-Drop adaptsthe conventional dropout regularization scheme for the haphazard input featurespace ensuring that the final output is minimally impacted by the chaoticappearance of such features. It helps to prevent the co-adaptation ofespecially the auxiliary and base features, as well as reduces the strongdependence of the output on any of the auxiliary inputs of the model. Thishelps in better learning for scenarios where certain features disappear in timeor when new features are to be modelled. The efficacy of Aux-Drop has beendemonstrated through extensive numerical experiments on SOTA benchmarkingdatasets that include Italy Power Demand, HIGGS, SUSY and multiple UCIdatasets. The code is available at ", "output": "Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classifier guidance -- using the gradients of an image classifier to steerthe generations of a diffusion model -- has the potential to dramaticallyexpand the creative control over image generation and editing. However,currently classifier guidance requires either training new noise-aware modelsto obtain accurate gradients or using a one-step denoising approximation of thefinal generation, which leads to misaligned gradients and sub-optimal control.We highlight this approximation's shortcomings and propose a novel guidancemethod: Direct Optimization of Diffusion Latents (DOODL), which enablesplug-and-play guidance by optimizing diffusion latents w.r.t. the gradients ofa pre-trained classifier on the true generated pixels, using an invertiblediffusion process to achieve memory-efficient backpropagation. Showcasing thepotential of more precise guidance, DOODL outperforms one-step classifierguidance on computational and human evaluation metrics across different formsof guidance: using CLIP guidance to improve generations of complex prompts fromDrawBench, using fine-grained visual classifiers to expand the vocabulary ofStable Diffusion, enabling image-conditioned generation with a CLIP visualencoder, and improving image aesthetics using an aesthetic scoring network.Code at ", "output": "End-to-End Diffusion Latent Optimization Improves Classifier Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We are developing a virtual coaching system that helps patients adhere tobehavior change interventions (BCI). Our proposed system predicts whether apatient will perform the targeted behavior and uses counterfactual exampleswith feature control to guide personalizsation of BCI. We use simulated patientdata with varying levels of receptivity to intervention to arrive at the studydesign which would enable evaluation of our system.", "output": "Personalising Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy and data-efficient online time series prediction for predictingevolving dynamical systems are critical in several fields, especially edge AIapplications that need to update continuously based on streaming data. However,current DNN-based supervised online learning models require a large amount oftraining data and cannot quickly adapt when the underlying system changes.Moreover, these models require continuous retraining with incoming data makingthem highly inefficient. To solve these issues, we present a novel ContinuousLearning-based Unsupervised Recurrent Spiking Neural Network Model (CLURSNN),trained with spike timing dependent plasticity (STDP). CLURSNN makes onlinepredictions by reconstructing the underlying dynamical system using RandomDelay Embedding by measuring the membrane potential of neurons in the recurrentlayer of the RSNN with the highest betweenness centrality. We also usetopological data analysis to propose a novel methodology using the WassersteinDistance between the persistence homologies of the predicted and observed timeseries as a loss function. We show that the proposed online time seriesprediction methodology outperforms state-of-the-art DNN models when predictingan evolving Lorenz63 dynamical system.", "output": "Brain-Inspired Spiking Neural Network for Online Unsupervised Time Series Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantitatively profiling a scholar's scientific impact is important to modernresearch society. Current practices with bibliometric indicators (e.g.,h-index), lists, and networks perform well at scholar ranking, but do notprovide structured context for scholar-centric, analytical tasks such asprofile reasoning and understanding. This work presents GeneticFlow (GF), asuite of novel graph-based scholar profiles that fulfill three essentialrequirements: structured-context, scholar-centric, and evolution-rich. Wepropose a framework to compute GF over large-scale academic data sources withmillions of scholars. The framework encompasses a new unsupervisedadvisor-advisee detection algorithm, a well-engineered citation type classifierusing interpretable features, and a fine-tuned graph neural network (GNN)model. Evaluations are conducted on the real-world task of scientific awardinference. Experiment outcomes show that the F1 score of best GF profilesignificantly outperforms alternative methods of impact indicators andbibliometric networks in all the 6 computer science fields considered.Moreover, the core GF profiles, with 63.6%-66.5% nodes and 12.5%-29.9% edges ofthe full profile, still significantly outrun existing methods in 5 out of 6fields studied. Visualization of GF profiling result also reveals humanexplainable patterns for high-impact scholars.", "output": "Impact-Oriented Contextual Scholar Profiling using Self-Citation Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many recent advances in natural language generation have been fueled bytraining large language models on internet-scale data. However, this paradigmcan lead to models that generate toxic, inaccurate, and unhelpful content, andautomatic evaluation metrics often fail to identify these behaviors. As modelsbecome more capable, human feedback is an invaluable signal for evaluating andimproving models. This survey aims to provide an overview of the recentresearch that has leveraged human feedback to improve natural languagegeneration. First, we introduce an encompassing formalization of feedback, andidentify and organize existing research into a taxonomy following thisformalization. Next, we discuss how feedback can be described by its format andobjective, and cover the two approaches proposed to use feedback (either fortraining or decoding): directly using the feedback or training feedback models.We also discuss existing datasets for human-feedback data collection, andconcerns surrounding feedback collection. Finally, we provide an overview ofthe nascent field of AI feedback, which exploits large language models to makejudgments based on a set of principles and minimize the need for humanintervention.", "output": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scaling language models have revolutionized widespread NLP tasks, yet littlecomprehensively explored few-shot relation extraction with large languagemodels. In this paper, we investigate principal methodologies, in-contextlearning and data generation, for few-shot relation extraction via GPT-3.5through exhaustive experiments. To enhance few-shot performance, we furtherpropose task-related instructions and schema-constrained data generation. Weobserve that in-context learning can achieve performance on par with previousprompt learning approaches, and data generation with the large language modelcan boost previous solutions to obtain new state-of-the-art few-shot results onfour widely-studied relation extraction datasets. We hope our work can inspirefuture research for the capabilities of large language models in few-shotrelation extraction. Code is available in", "output": "How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While some powerful neural network architectures (e.g., Transformer, GraphNeural Networks) have achieved improved performance in sequentialrecommendation with high-order item dependency modeling, they may suffer frompoor representation capability in label scarcity scenarios. To address theissue of insufficient labels, Contrastive Learning (CL) has attracted muchattention in recent methods to perform data augmentation through embeddingcontrasting for self-supervision. However, due to the hand-crafted property oftheir contrastive view generation strategies, existing CL-enhanced models i)can hardly yield consistent performance on diverse sequential recommendationtasks; ii) may not be immune to user behavior data noise. In light of this, wepropose a simple yet effective Graph Masked AutoEncoder-enhanced sequentialRecommender system (MAERec) that adaptively and dynamically distills globalitem transitional information for self-supervised augmentation. It naturallyavoids the above issue of heavy reliance on constructing high-quality embeddingcontrastive views. Instead, an adaptive data reconstruction paradigm isdesigned to be integrated with the long-range item dependency modeling, forinformative augmentation in sequential recommendation. Extensive experimentsdemonstrate that our method significantly outperforms state-of-the-art baselinemodels and can learn more accurate representations against data noise andsparsity. Our implemented model code is available at", "output": "Graph Masked Autoencoder for Sequential Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continuous-time reinforcement learning tasks commonly use discrete steps offixed cycle times for actions. As practitioners need to choose the action-cycletime for a given task, a significant concern is whether the hyper-parameters ofthe learning algorithm need to be re-tuned for each choice of the cycle time,which is prohibitive for real-world robotics. In this work, we investigate thewidely-used baseline hyper-parameter values of two policy gradient algorithms-- PPO and SAC -- across different cycle times. Using a benchmark task wherethe baseline hyper-parameters of both algorithms were shown to work well, wereveal that when a cycle time different than the task default is chosen, PPOwith baseline hyper-parameters fails to learn. Moreover, both PPO and SAC withtheir baseline hyper-parameters perform substantially worse than their tunedvalues for each cycle time. We propose novel approaches for setting thesehyper-parameters based on the cycle time. In our experiments on simulated andreal-world robotic tasks, the proposed approaches performed at least as well asthe baseline hyper-parameters, with significantly better performance for mostchoices of the cycle time, and did not result in learning failure for any cycletime. Hyper-parameter tuning still remains a significant barrier for real-worldrobotics, as our approaches require some initial tuning on a new task, eventhough it is negligible compared to an extensive tuning for each cycle time.Our approach requires no additional tuning after the cycle time is changed fora given task and is a step toward avoiding extensive and costly hyper-parametertuning for real-world policy optimization.", "output": "Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human-Object Interaction (HOI) detection is a task to localize humans andobjects in an image and predict the interactions in human-object pairs. Inreal-world scenarios, HOI detection models are required systematicgeneralization, i.e., generalization to novel combinations of objects andinteractions, because the train data are expected to cover a limited portion ofall possible combinations. However, to our knowledge, no open benchmarks orprevious work exist for evaluating the systematic generalization performance ofHOI detection models. To address this issue, we created two new sets of HOIdetection data splits named HICO-DET-SG and V-COCO-SG based on the HICO-DET andV-COCO datasets, respectively. When evaluated on the new data splits, therepresentative HOI detection models performed much more poorly than whenevaluated on the original splits. This reveals that systematic generalizationis a challenging goal in HOI detection. By analyzing the evaluation results, wealso gain insights for improving the systematic generalization performance andidentify four possible future research directions. We hope that our new datasplits and presented analysis will encourage further research on systematicgeneralization in HOI detection.", "output": "HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We apply convolutional neural networks (CNN) to the problem of imageorientation detection in the context of determining the correct orientation(from 0, 90, 180, and 270 degrees) of a consumer photo. The problem isespecially important for digitazing analog photographs. We substantiallyimprove on the published state of the art in terms of the performance on one ofthe standard datasets, and test our system on a more difficult large dataset ofconsumer photos. We use Guided Backpropagation to obtain insights into how ourCNN detects photo orientation, and to explain its mistakes.", "output": "Automatic Photo Orientation Detection with Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of root cause analysis (RCA) is to identify the root causes ofsystem faults/failures by analyzing system monitoring data. Efficient RCA cangreatly accelerate system failure recovery and mitigate system damages orfinancial losses. However, previous research has mostly focused on developingoffline RCA algorithms, which often require manually initiating the RCAprocess, a significant amount of time and data to train a robust model, andthen being retrained from scratch for a new system fault.In this paper, we propose CORAL, a novel online RCA framework that canautomatically trigger the RCA process and incrementally update the RCA model.CORAL consists of Trigger Point Detection, Incremental Disentangled CausalGraph Learning, and Network Propagation-based Root Cause Localization. TheTrigger Point Detection component aims to detect system state transitionsautomatically and in near-real-time. To achieve this, we develop an onlinetrigger point detection approach based on multivariate singular spectrumanalysis and cumulative sum statistics. To efficiently update the RCA model, wepropose an incremental disentangled causal graph learning approach to decouplethe state-invariant and state-dependent information. After that, CORAL appliesa random walk with restarts to the updated causal graph to accurately identifyroot causes. The online RCA process terminates when the causal graph and thegenerated root cause list converge. Extensive experiments on three real-worlddatasets with case studies demonstrate the effectiveness and superiority of theproposed framework.", "output": "Disentangled Causal Graph Learning forOnline Unsupervised Root Cause Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While multilingual neural machine translation has achieved great success, itsuffers from the off-target issue, where the translation is in the wronglanguage. This problem is more pronounced on zero-shot translation tasks. Inthis work, we find that failing in encoding discriminative target languagesignal will lead to off-target and a closer lexical distance (i.e.,KL-divergence) between two languages' vocabularies is related with a higheroff-target rate. We also find that solely isolating the vocab of differentlanguages in the decoder can alleviate the problem. Motivated by the findings,we propose Language Aware Vocabulary Sharing (LAVS), a simple and effectivealgorithm to construct the multilingual vocabulary, that greatly alleviates theoff-target problem of the translation model by increasing the KL-divergencebetween languages. We conduct experiments on a multilingual machine translationbenchmark in 11 languages. Experiments show that the off-target rate for 90translation tasks is reduced from 29% to 8%, while the overall BLEU score isimproved by an average of 1.9 points without extra training cost or sacrificingthe supervised directions' performance. We release the code athref{ reproduction.", "output": "On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the paradigm of AI-generated content (AIGC), there has been increasingattention in extending pre-trained text-to-image (T2I) models to text-to-video(T2V) generation. Despite their effectiveness, these frameworks face challengesin maintaining consistent narratives and handling rapid shifts in scenecomposition or object placement from a single user prompt. This paperintroduces a new framework, dubbed DirecT2V, which leverages instruction-tunedlarge language models (LLMs) to generate frame-by-frame descriptions from asingle abstract user prompt. DirecT2V utilizes LLM directors to divide userinputs into separate prompts for each frame, enabling the inclusion oftime-varying content and facilitating consistent video generation. To maintaintemporal consistency and prevent object collapse, we propose a novel valuemapping method and dual-softmax filtering. Extensive experimental resultsvalidate the effectiveness of the DirecT2V framework in producing visuallycoherent and consistent videos from abstract user prompts, addressing thechallenges of zero-shot video generation.", "output": "Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The adoption of Deep Neural Networks (DNNs) has greatly benefited NaturalLanguage Processing (NLP) during the past decade. However, the demands of longdocument analysis are quite different from those of shorter texts, while theever increasing size of documents uploaded on-line renders automatedunderstanding of long texts a critical area of research. This article has twogoals: a) it overviews the relevant neural building blocks, thus serving as ashort tutorial, and b) it surveys the state-of-the-art in long document NLP,mainly focusing on two central tasks: document classification and documentsummarization. Sentiment analysis for long texts is also covered, since it istypically treated as a particular case of document classification.Additionally, this article discusses the main challenges, issues and currentsolutions related to long document NLP. Finally, the relevant, publiclyavailable, annotated datasets are presented, in order to facilitate furtherresearch.", "output": "Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As natural language processing (NLP) has recently seen an unprecedented levelof excitement, and more people are eager to enter the field, it is unclearwhether current research reproducibility efforts are sufficient for this groupof beginners to apply the latest developments. To understand their needs, weconducted a study with 93 students in an introductory NLP course, wherestudents reproduced the results of recent NLP papers. Surprisingly, we findthat their programming skill and comprehension of research papers have alimited impact on their effort spent completing the exercise. Instead, we findaccessibility efforts by research authors to be the key to success, includingcomplete documentation, better coding practice, and easier access to datafiles. Going forward, we recommend that NLP researchers pay close attention tothese simple aspects of open-sourcing their work, and use insights frombeginners' feedback to provide actionable ideas on how to better support them.", "output": "NLP Reproducibility For All: Understanding Experiences of Beginners."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The captivating realm of Minecraft has attracted substantial researchinterest in recent years, serving as a rich platform for developing intelligentagents capable of functioning in open-world environments. However, the currentresearch landscape predominantly focuses on specific objectives, such as thepopular \"ObtainDiamond\" task, and has not yet shown effective generalization toa broader spectrum of tasks. Furthermore, the current leading success rate forthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations ofReinforcement Learning (RL) based controllers used in existing methods. Totackle these challenges, we introduce Ghost in the Minecraft (GITM), a novelframework integrates Large Language Models (LLMs) with text-based knowledge andmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. Theseagents, equipped with the logic and common sense capabilities of LLMs, canskillfully navigate complex, sparse-reward environments with text-basedinteractions. We develop a set of structured actions and leverage LLMs togenerate action plans for the agents to execute. The resulting LLM-based agentmarkedly surpasses previous methods, achieving a remarkable improvement of+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superiorrobustness compared to traditional RL-based controllers. Notably, our agent isthe first to procure all items in the Minecraft Overworld technology tree,demonstrating its extensive capabilities. GITM does not need any GPU fortraining, but a single CPU node with 32 CPU cores is enough. This researchshows the potential of LLMs in developing capable agents for handlinglong-horizon, complex tasks and adapting to uncertainties in open-worldenvironments. See the project website at ", "output": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The emergent capabilities of Large Language Models (LLMs) have made itcrucial to align their values with those of humans. Current methodologiestypically attempt alignment with a homogeneous human value and requires humanverification, yet lack consensus on the desired aspect and depth of alignmentand resulting human biases. In this paper, we propose A2EHV, an AutomatedAlignment Evaluation with a Heterogeneous Value system that (1) is automated tominimize individual human biases, and (2) allows assessments against varioustarget values to foster heterogeneous agents. Our approach pivots on theconcept of value rationality, which represents the ability for agents toexecute behaviors that satisfy a target value the most. The quantification ofvalue rationality is facilitated by the Social Value Orientation framework fromsocial psychology, which partitions the value space into four categories toassess social preferences from agents' behaviors. We evaluate the valuerationality of eight mainstream LLMs and observe that large models are moreinclined to align neutral values compared to those with strong personal values.By examining the behavior of these LLMs, we contribute to a deeperunderstanding of value alignment within a heterogeneous value system.", "output": "Heterogeneous Value Evaluation for Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attention has become a common ingredient in deep learning architectures. Itadds a dynamical selection of information on top of the static selection ofinformation supported by weights. In the same way, we can imagine ahigher-order informational filter built on top of attention: an AttentionSchema (AS), namely, a descriptive and predictive model of attention. Incognitive neuroscience, Attention Schema Theory (AST) supports this idea ofdistinguishing attention from AS. A strong prediction of this theory is that anagent can use its own AS to also infer the states of other agents' attentionand consequently enhance coordination with other agents. As such, multi-agentreinforcement learning would be an ideal setting to experimentally test thevalidity of AST. We explore different ways in which attention and AS interactwith each other. Our preliminary results indicate that agents that implementthe AS as a recurrent internal control achieve the best performance. Ingeneral, these exploratory experiments suggest that equipping artificial agentswith a model of attention can enhance their social intelligence.", "output": "Attention Schema in Neural Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning (DL) has emerged as a powerful subset of machine learning (ML)and artificial intelligence (AI), outperforming traditional ML methods,especially in handling unstructured and large datasets. Its impact spans acrossvarious domains, including speech recognition, healthcare, autonomous vehicles,cybersecurity, predictive analytics, and more. However, the complexity anddynamic nature of real-world problems present challenges in designing effectivedeep learning models. Consequently, several deep learning models have beendeveloped to address different problems and applications. In this article, weconduct a comprehensive survey of various deep learning models, includingConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),Generative Models, Deep Reinforcement Learning (DRL), and Deep TransferLearning. We examine the structure, applications, benefits, and limitations ofeach model. Furthermore, we perform an analysis using three publicly availabledatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowneddeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),Bidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.", "output": "A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Root causes of disease intuitively correspond to root vertices that increasethe likelihood of a diagnosis. This description of a root cause neverthelesslacks the rigorous mathematical formulation needed for the development ofcomputer algorithms designed to automatically detect root causes from data.Prior work defined patient-specific root causes of disease using aninterventionalist account that only climbs to the second rung of Pearl's Ladderof Causation. In this theoretical piece, we climb to the third rung byproposing a counterfactual definition matching clinical intuition based onfixed factual data alone. We then show how to assign a root causal contributionscore to each variable using Shapley values from explainable artificialintelligence. The proposed counterfactual formulation of patient-specific rootcauses of disease accounts for noisy labels, adapts to disease prevalence andadmits fast computation without the need for counterfactual simulation.", "output": "Counterfactual Formulation of Patient-Specific Root Causes of Disease."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a novel approach to evaluating deep learning models'capacity for in-diagram logic interpretation. Leveraging the intriguing realmof visual illusions, we establish a unique dataset, InDL, designed torigorously test and benchmark these models. Deep learning has witnessedremarkable progress in domains such as computer vision and natural languageprocessing. However, models often stumble in tasks requiring logical reasoningdue to their inherent 'black box' characteristics, which obscure thedecision-making process. Our work presents a new lens to understand thesemodels better by focusing on their handling of visual illusions -- a complexinterplay of perception and logic. We utilize six classic geometric opticalillusions to create a comparative framework between human and machine visualperception. This methodology offers a quantifiable measure to rank models,elucidating potential weaknesses and providing actionable insights for modelimprovements. Our experimental results affirm the efficacy of our benchmarkingstrategy, demonstrating its ability to effectively rank models based on theirlogic interpretation ability. As part of our commitment to reproducibleresearch, the source code and datasets will be made publicly available here:href{", "output": "InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A hyper-relational knowledge graph has been recently studied where a tripletis associated with a set of qualifiers; a qualifier is composed of a relationand an entity, providing auxiliary information for a triplet. While existinghyper-relational knowledge graph embedding methods assume that the entities arediscrete objects, some information should be represented using numeric values,e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, OxfordUniv.) can be associated with a qualifier such as (start time, 1911). In thispaper, we propose a unified framework named HyNT that learns representations ofa hyper-relational knowledge graph containing numeric literals in eithertriplets or qualifiers. We define a context transformer and a predictiontransformer to learn the representations based not only on the correlationsbetween a triplet and its qualifiers but also on the numeric information. Bylearning compact representations of triplets and qualifiers and feeding theminto the transformers, we reduce the computation cost of using transformers.Using HyNT, we can predict missing numeric values in addition to missingentities or relations in a hyper-relational knowledge graph. Experimentalresults show that HyNT significantly outperforms state-of-the-art methods onreal-world datasets.", "output": "Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative neural models hold great promise in enhancing programmingeducation by synthesizing new content for students. We seek to design neuralmodels that can automatically generate programming tasks for a givenspecification in the context of visual programming domains. Despite the recentsuccesses of large generative models like GPT-4, our initial results show thatthese models are ineffective in synthesizing visual programming tasks andstruggle with logical and spatial reasoning. We propose a novel neuro-symbolictechnique, NeurTaskSyn, that can synthesize programming tasks for aspecification given in the form of desired programming concepts exercised byits solution code and constraints on the visual task. NeurTaskSyn has twocomponents: the first component is trained via imitation learning procedure togenerate possible solution codes, and the second component is trained viareinforcement learning procedure to guide an underlying symbolic executionengine that generates visual tasks for these codes. We demonstrate theeffectiveness of NeurTaskSyn through an extensive empirical evaluation and aqualitative study on reference tasks taken from the Hour of Code: Classic Mazechallenge by Code-dot-org and the Intro to Programming with Karel course byCodeHS-dot-com.", "output": "Neural Task Synthesis for Visual Programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trustworthy Federated Learning (TFL) typically leverages protectionmechanisms to guarantee privacy. However, protection mechanisms inevitablyintroduce utility loss or efficiency reduction while protecting data privacy.Therefore, protection mechanisms and their parameters should be carefullychosen to strike an optimal tradeoff between textit{privacy leakage},textit{utility loss}, and textit{efficiency reduction}. To this end,federated learning practitioners need tools to measure the three factors andoptimize the tradeoff between them to choose the protection mechanism that ismost appropriate to the application at hand. Motivated by this requirement, wepropose a framework that (1) formulates TFL as a problem of finding aprotection mechanism to optimize the tradeoff between privacy leakage, utilityloss, and efficiency reduction and (2) formally defines bounded measurements ofthe three factors. We then propose a meta-learning algorithm to approximatethis optimization problem and find optimal protection parameters forrepresentative protection mechanisms, including Randomization, HomomorphicEncryption, Secret Sharing, and Compression. We further design estimationalgorithms to quantify these found optimal protection parameters in a practicalhorizontal federated learning setting and provide a theoretical analysis of theestimation error.", "output": "A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Path of Destruction (PoD) is a self-supervised method for learning iterativegenerators. The core idea is to produce a training set by destroying a set ofartifacts, and for each destructive step create a training instance based onthe corresponding repair action. A generator trained on this dataset can thengenerate new artifacts by repairing from arbitrary states. The PoD method isvery data-efficient in terms of original training examples and well-suited tofunctional artifacts composed of categorical data, such as game levels anddiscrete 3D structures. In this paper, we extend the Path of Destruction methodto allow designer control over aspects of the generated artifacts.Controllability is introduced by adding conditional inputs to the state-actionpairs that make up the repair trajectories. We test the controllable PoD methodin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.", "output": "Controllable Path of Destruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a recent surge in the development of spatio-temporal forecastingmodels in the transportation domain. Long-range traffic forecasting, however,remains a challenging task due to the intricate and extensive spatio-temporalcorrelations observed in traffic networks. Current works primarily rely on roadnetworks with graph structures and learn representations using graph neuralnetworks (GNNs), but this approach suffers from over-smoothing problem in deeparchitectures. To tackle this problem, recent methods introduced thecombination of GNNs with residual connections or neural ordinary differentialequations (ODE). However, current graph ODE models face two key limitations infeature extraction: (1) they lean towards global temporal patterns, overlookinglocal patterns that are important for unexpected events; and (2) they lackdynamic semantic edges in their architectural design. In this paper, we proposea novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-ODE)which is designed with multiple connective ODE-GNN modules to learn betterrepresentations by capturing different views of complex local and globaldynamic spatio-temporal dependencies. We also add some techniques like sharedweights and divergence constraints into the intermediate layers of distinctODE-GNN modules to further improve their communication towards the forecastingtask. Our extensive set of experiments conducted on six real-world datasetsdemonstrate the superior performance of GRAM-ODE compared with state-of-the-artbaselines as well as the contribution of different components to the overallperformance. The code is available at ", "output": "Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although Graph Neural Networks (GNNs) have been successful in nodeclassification tasks, their performance heavily relies on the availability of asufficient number of labeled nodes per class. In real-world situations, not allclasses have many labeled nodes and there may be instances where the modelneeds to classify new classes, making manual labeling difficult. To solve thisproblem, it is important for GNNs to be able to classify nodes with a limitednumber of labeled nodes, known as few-shot node classification. Previousepisodic meta-learning based methods have demonstrated success in few-shot nodeclassification, but our findings suggest that optimal performance can only beachieved with a substantial amount of diverse training meta-tasks. To addressthis challenge of meta-learning based few-shot learning (FSL), we propose a newapproach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEGframework enables the model to learn transferable task-adaptation strategiesusing a limited number of training meta-tasks, allowing it to acquiremeta-knowledge for a wide range of meta-tasks. By incorporating equivariantneural networks, TEG can utilize their strong generalization abilities to learnhighly adaptable task-specific strategies. As a result, TEG achievesstate-of-the-art performance with limited training meta-tasks. Our experimentson various benchmark datasets demonstrate TEG's superiority in terms ofaccuracy and generalization ability, even when using minimal meta-trainingdata, highlighting the effectiveness of our proposed approach in addressing thechallenges of meta-learning based few-shot node classification. Our code isavailable at the following link: ", "output": "Task-Equivariant Graph Few-shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The multi-criteria (MC) recommender system, which leverages MC ratinginformation in a wide range of e-commerce areas, is ubiquitous nowadays.Surprisingly, although graph neural networks (GNNs) have been widely applied todevelop various recommender systems due to GNN's high expressive capability inlearning graph representations, it has been still unexplored how to design MCrecommender systems with GNNs. In light of this, we make the first attempttowards designing a GNN-aided MC recommender system. Specifically, rather thanstraightforwardly adopting existing GNN-based recommendation methods, we devisea novel criteria preference-aware light graph convolution CPA-LGC method, whichis capable of precisely capturing the criteria preference of users as well asthe collaborative signal in complex high-order connectivities. To this end, wefirst construct an MC expansion graph that transforms user--item MC ratingsinto an expanded bipartite graph to potentially learn from the collaborativesignal in MC ratings. Next, to strengthen the capability of criteria preferenceawareness, CPA-LGC incorporates newly characterized embeddings, includinguser-specific criteria-preference embeddings and item-specific criterionembeddings, into our graph convolution model. Through comprehensive evaluationsusing four real-world datasets, we demonstrate (a) the superiority overbenchmark MC recommendation methods and benchmark recommendation methods usingGNNs with tremendous gains, (b) the effectiveness of core components inCPA-LGC, and (c) the computational efficiency.", "output": "Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study aims to develop a single integrated spoofing-aware speakerverification (SASV) embeddings that satisfy two aspects. First, rejectingnon-target speakers' input as well as target speakers' spoofed inputs should beaddressed. Second, competitive performance should be demonstrated compared tothe fusion of automatic speaker verification (ASV) and countermeasure (CM)embeddings, which outperformed single embedding solutions by a large margin inthe SASV2022 challenge. We analyze that the inferior performance of single SASVembeddings comes from insufficient amount of training data and distinct natureof ASV and CM tasks. To this end, we propose a novel framework that includesmulti-stage training and a combination of loss functions. Copy synthesis,combined with several vocoders, is also exploited to address the lack ofspoofed data. Experimental results show dramatic improvements, achieving aSASV-EER of 1.06% on the evaluation protocol of the SASV2022 challenge.", "output": "Towards single integrated spoofing-aware speaker verification embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating graphs from a target distribution is a significant challengeacross many domains, including drug discovery and social network analysis. Inthis work, we introduce a novel graph generation method leveraging $K^2$-treerepresentation which was originally designed for lossless graph compression.Our motivation stems from the ability of the $K^2$-trees to enable compactgeneration while concurrently capturing the inherent hierarchical structure ofa graph. In addition, we make further contributions by (1) presenting asequential $K^2$-tree representation that incorporates pruning, flattening, andtokenization processes and (2) introducing a Transformer-based architecturedesigned to generate the sequence by incorporating a specialized treepositional encoding scheme. Finally, we extensively evaluate our algorithm onfour general and two molecular graph datasets to confirm its superiority forgraph generation.", "output": "Hierarchical Graph Generation with $K^2$-trees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent offline meta-reinforcement learning (meta-RL) methods typicallyutilize task-dependent behavior policies (e.g., training RL agents on eachindividual task) to collect a multi-task dataset. However, these methods alwaysrequire extra information for fast adaptation, such as offline context fortesting tasks. To address this problem, we first formally characterize a uniquechallenge in offline meta-RL: transition-reward distribution shift betweenoffline datasets and online adaptation. Our theory finds thatout-of-distribution adaptation episodes may lead to unreliable policyevaluation and that online adaptation with in-distribution episodes can ensureadaptation performance guarantee. Based on these theoretical insights, wepropose a novel adaptation framework, called In-Distribution online Adaptationwith uncertainty Quantification (IDAQ), which generates in-distribution contextusing a given uncertainty quantification and performs effective task beliefinference to address new tasks. We find a return-based uncertaintyquantification for IDAQ that performs effectively. Experiments show that IDAQachieves state-of-the-art performance on the Meta-World ML1 benchmark comparedto baselines with/without offline adaptation.", "output": "Offline Meta Reinforcement Learning with In-Distribution Online Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extensive research has been conducted on fault diagnosis of planetarygearboxes using vibration signals and deep learning (DL) approaches. However,DL-based methods are susceptible to the domain shift problem caused by varyingoperating conditions of the gearbox. Although domain adaptation and datasynthesis methods have been proposed to overcome such domain shifts, they areoften not directly applicable in real-world situations where only healthy datais available in the target domain. To tackle the challenge of extreme domainshift scenarios where only healthy data is available in the target domain, thispaper proposes two novel domain knowledge-informed data synthesis methodsutilizing the health data map (HDMap). The two proposed approaches are referredto as scaled CutPaste and FaultPaste. The HDMap is used to physically representthe vibration signal of the planetary gearbox as an image-like matrix, allowingfor visualization of fault-related features. CutPaste and FaultPaste are thenapplied to generate faulty samples based on the healthy data in the targetdomain, using domain knowledge and fault signatures extracted from the sourcedomain, respectively. In addition to generating realistic faults, the proposedmethods introduce scaling of fault signatures for controlled synthesis offaults with various severity levels. A case study is conducted on a planetarygearbox testbed to evaluate the proposed approaches. The results show that theproposed methods are capable of accurately diagnosing faults, even in cases ofextreme domain shift, and can estimate the severity of faults that have notbeen previously observed in the target domain.", "output": "Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in text-to-image diffusion models have achieved remarkablesuccess in generating high-quality, realistic images from given text prompts.However, previous methods fail to perform accurate modality alignment betweentext concepts and generated images due to the lack of fine-level semanticguidance that successfully diagnoses the modality discrepancy. In this paper,we propose FineRewards to improve the alignment between text and images intext-to-image diffusion models by introducing two new fine-grained semanticrewards: the caption reward and the Semantic Segment Anything (SAM) reward.From the global semantic view, the caption reward generates a correspondingdetailed caption that depicts all important contents in the synthetic image viaa BLIP-2 model and then calculates the reward score by measuring the similaritybetween the generated caption and the given prompt. From the local semanticview, the SAM reward segments the generated images into local parts withcategory labels, and scores the segmented parts by measuring the likelihood ofeach category appearing in the prompted scene via a large language model, i.e.,Vicuna-7B. Additionally, we adopt an assemble reward-ranked learning strategyto enable the integration of multiple reward functions to jointly guide themodel training. Adapting results of text-to-image models on the MS-COCObenchmark show that the proposed semantic reward outperforms other baselinereward functions with a considerable margin on both visual quality and semanticsimilarity with the input prompt. Moreover, by adopting the assemblereward-ranked learning strategy, we further demonstrate that model performanceis further improved when adapting under the unifying of the proposed semanticreward with the current image rewards.", "output": "Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have emerged as powerful tools in the field ofNatural Language Processing (NLP) and have recently gained significantattention in the domain of Recommendation Systems (RS). These models, trainedon massive amounts of data using self-supervised learning, have demonstratedremarkable success in learning universal representations and have the potentialto enhance various aspects of recommendation systems by some effective transfertechniques such as fine-tuning and prompt tuning, and so on. The crucial aspectof harnessing the power of language models in enhancing recommendation qualityis the utilization of their high-quality representations of textual featuresand their extensive coverage of external knowledge to establish correlationsbetween items and users. To provide a comprehensive understanding of theexisting LLM-based recommendation systems, this survey presents a taxonomy thatcategorizes these models into two major paradigms, respectively DiscriminativeLLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation(GLLM4Rec), with the latter being systematically sorted out for the first time.Furthermore, we systematically review and analyze existing LLM-basedrecommendation systems within each paradigm, providing insights into theirmethodologies, techniques, and performance. Additionally, we identify keychallenges and several valuable findings to provide researchers andpractitioners with inspiration. We have also created a GitHub repository toindex relevant papers on LLMs for recommendation,", "output": "A Survey on Large Language Models for Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inductive knowledge graph completion has been considered as the task ofpredicting missing triplets between new entities that are not observed duringtraining. While most inductive knowledge graph completion methods assume thatall entities can be new, they do not allow new relations to appear at inferencetime. This restriction prohibits the existing methods from appropriatelyhandling real-world knowledge graphs where new entities accompany newrelations. In this paper, we propose an INductive knowledge GRAph eMbeddingmethod, InGram, that can generate embeddings of new relations as well as newentities at inference time. Given a knowledge graph, we define a relation graphas a weighted graph consisting of relations and the affinity weights betweenthem. Based on the relation graph and the original knowledge graph, InGramlearns how to aggregate neighboring embeddings to generate relation and entityembeddings using an attention mechanism. Experimental results show that InGramoutperforms 14 different state-of-the-art methods on varied inductive learningscenarios.", "output": "InGram: Inductive Knowledge Graph Embedding via Relation Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendlyframework to extend Recursive Neural Networks (RvNNs) with beam search forlatent structure induction. We further extend this framework by proposing arelaxation of the hard top-k operators in beam search for better propagation ofgradient signals. We evaluate our proposed models in differentout-of-distribution splits in both synthetic and realistic data. Ourexperiments show that BTCell achieves near-perfect performance on severalchallenging structure-sensitive synthetic tasks like ListOps and logicalinference while maintaining comparable performance in realistic data againstother RvNN-based models. Additionally, we identify a previously unknown failurecase for neural models in generalization to unseen number of arguments inListOps. The code is available at:", "output": "Beam Tree Recursive Cells."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We describe a class of tasks called decision-oriented dialogues, in which AIassistants must collaborate with one or more humans via natural language tohelp them make complex decisions. We formalize three domains in which usersface everyday decisions: (1) choosing an assignment of reviewers to conferencepapers, (2) planning a multi-step itinerary in a city, and (3) negotiatingtravel plans for a group of friends. In each of these settings, AI assistantsand users have disparate abilities that they must combine to arrive at the bestdecision: assistants can access and process large amounts of information, whileusers have preferences and constraints external to the system. For each task,we build a dialogue environment where agents receive a reward based on thequality of the final decision they reach. Using these environments, we collecthuman-human dialogues with humans playing the role of assistant. To compare howcurrent AI assistants communicate in these settings, we present baselines usinglarge language models in self-play. Finally, we highlight a number ofchallenges models face in decision-oriented dialogues, ranging from efficientcommunication to reasoning and optimization, and release our environments as atestbed for future modeling work.", "output": "Decision-Oriented Dialogue for Human-AI Collaboration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a highly flexible, quantized, memory-efficient, andultra-lightweight object detection network, called TinyissimoYOLO. It aims toenable object detection on microcontrollers in the power domain of milliwatts,with less than 0.5MB memory available for storing convolutional neural network(CNN) weights. The proposed quantized network architecture with 422kparameters, enables real-time object detection on embedded microcontrollers,and it has been evaluated to exploit CNN accelerators. In particular, theproposed network has been deployed on the MAX78000 microcontroller achievinghigh frame-rate of up to 180fps and an ultra-low energy consumption of only196{mu}J per inference with an inference efficiency of more than 106MAC/Cycle. TinyissimoYOLO can be trained for any multi-object detection.However, considering the small network size, adding object detection classeswill increase the size and memory consumption of the network, thus objectdetection with up to 3 classes is demonstrated. Furthermore, the network istrained using quantization-aware training and deployed with 8-bit quantizationon different microcontrollers, such as STM32H7A3, STM32L4R9, Apollo4b and onthe MAX78000's CNN accelerator. Performance evaluations are presented in thispaper.", "output": "TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object Detection Network for Low Power Microcontrollers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Aortic stenosis (AS) is a degenerative valve condition that causessubstantial morbidity and mortality. This condition is under-diagnosed andunder-treated. In clinical practice, AS is diagnosed with expert review oftransthoracic echocardiography, which produces dozens of ultrasound images ofthe heart. Only some of these views show the aortic valve. To automatescreening for AS, deep networks must learn to mimic a human expert's ability toidentify views of the aortic valve then aggregate across these relevant imagesto produce a study-level diagnosis. We find previous approaches to AS detectionyield insufficient accuracy due to relying on inflexible averages acrossimages. We further find that off-the-shelf attention-based multiple instancelearning (MIL) performs poorly. We contribute a new end-to-end MIL approachwith two key methodological innovations. First, a supervised attentiontechnique guides the learned attention mechanism to favor relevant views.Second, a novel self-supervised pretraining strategy applies contrastivelearning on the representation of the whole study instead of individual imagesas commonly done in prior literature. Experiments on an open-access dataset andan external validation set show that our approach yields higher accuracy whilereducing model size.", "output": "Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating the number of clusters and underlying cluster structure in adataset is a crucial task. Real-world data are often unlabeled, complex andhigh-dimensional, which makes it difficult for traditional clusteringalgorithms to perform well. In recent years, a matrix reordering basedalgorithm, called \"visual assessment of tendency\" (VAT), and its variants haveattracted many researchers from various domains to estimate the number ofclusters and inherent cluster structure present in the data. However, thesealgorithms fail when applied to high-dimensional data due to the curse ofdimensionality, as they rely heavily on the notions of closeness and farnessbetween data points. To address this issue, we propose a deep-learning basedframework for cluster structure assessment in complex, image datasets. First,our framework generates representative embeddings for complex data using aself-supervised deep neural network, and then, these low-dimensional embeddingsare fed to VAT/iVAT algorithms to estimate the underlying cluster structure. Inthis process, we ensured not to use any prior knowledge for the number ofclusters (i.e k). We present our results on four real-life image datasets, andour findings indicate that our framework outperforms state-of-the-art VAT/iVATalgorithms in terms of clustering accuracy and normalized mutual information(NMI).", "output": "A Self-Supervised Approach for Cluster Assessment of High-Dimensional Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Out of the estimated few trillion galaxies, only around a million have beendetected through radio frequencies, and only a tiny fraction, approximately athousand, have been manually classified. We have addressed this disparitybetween labeled and unlabeled images of radio galaxies by employing asemi-supervised learning approach to classify them into the knownFanaroff-Riley Type I (FRI) and Type II (FRII) categories. A Group EquivariantConvolutional Neural Network (G-CNN) was used as an encoder of thestate-of-the-art self-supervised methods SimCLR (A Simple Framework forContrastive Learning of Visual Representations) and BYOL (Bootstrap Your OwnLatent). The G-CNN preserves the equivariance for the Euclidean Group E(2),enabling it to effectively learn the representation of globally orientedfeature maps. After representation learning, we trained a fully-connectedclassifier and fine-tuned the trained encoder with labeled data. Our findingsdemonstrate that our semi-supervised approach outperforms existingstate-of-the-art methods across several metrics, including cluster quality,convergence rate, accuracy, precision, recall, and the F1-score. Moreover,statistical significance testing via a t-test revealed that our methodsurpasses the performance of a fully supervised G-CNN. This study emphasizesthe importance of semi-supervised learning in radio galaxy classification,where labeled data are still scarce, but the prospects for discovery areimmense.", "output": "Morphological Classification of Radio Galaxies using Semi-Supervised Group Equivariant CNNs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cancer is one of the most life-threatening diseases worldwide, and head andneck (H&amp;N) cancer is a prevalent type with hundreds of thousands of new casesrecorded each year. Clinicians use medical imaging modalities such as computedtomography and positron emission tomography to detect the presence of a tumor,and they combine that information with clinical data for patient prognosis. Theprocess is mostly challenging and time-consuming. Machine learning and deeplearning can automate these tasks to help clinicians with highly promisingresults. This work studies two approaches for H&amp;N tumor segmentation: (i)exploration and comparison of vision transformer (ViT)-based and convolutionalneural network-based models; and (ii) proposal of a novel 2D perspective toworking with 3D data. Furthermore, this work proposes two new architectures forthe prognosis task. An ensemble of several models predicts patient outcomes(which won the HECKTOR 2021 challenge prognosis task), and a ViT-basedframework concurrently performs patient outcome prediction and tumorsegmentation, which outperforms the ensemble model.", "output": "Diagnosis and Prognosis of Head and Neck Cancer Patients using Artificial Intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial neural networks are prone to being fooled by carefully perturbedinputs which cause an egregious misclassification. These textit{adversarial}attacks have been the focus of extensive research. Likewise, there has been anabundance of research in ways to detect and defend against them. We introduce anovel approach of detection and interpretation of adversarial attacks from agraph perspective. For an image, benign or adversarial, we study how a neuralnetwork's architecture can induce an associated graph. We study this graph andintroduce specific measures used to predict and interpret adversarial attacks.We show that graphs-based approaches help to investigate the inner workings ofadversarial attacks.", "output": "Graph-based methods coupled with specific distributional distances for adversarial attack detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-class cell segmentation in high-resolution Giga-pixel whole slideimages (WSI) is critical for various clinical applications. Training such an AImodel typically requires labor-intensive pixel-wise manual annotation fromexperienced domain experts (e.g., pathologists). Moreover, such annotation iserror-prone when differentiating fine-grained cell types (e.g., podocyte andmesangial cells) via the naked human eye. In this study, we assess thefeasibility of democratizing pathological AI deployment by only using layannotators (annotators without medical domain knowledge). The contribution ofthis paper is threefold: (1) We proposed a molecular-empowered learning schemefor multi-class cell segmentation using partial labels from lay annotators; (2)The proposed method integrated Giga-pixel level molecular-morphologycross-modality registration, molecular-informed annotation, andmolecular-oriented segmentation model, so as to achieve significantly superiorperformance via 3 lay annotators as compared with 2 experienced pathologists;(3) A deep corrective learning (learning with imperfect label) method isproposed to further improve the segmentation performance using partiallyannotated noisy data. From the experimental results, our learning methodachieved F1 = 0.8496 using molecular-informed annotations from lay annotators,which is better than conventional morphology-based annotations (F1 = 0.7051)from experienced pathologists. Our method democratizes the development of apathological segmentation deep model to the lay annotator level, whichconsequently scales up the learning process similar to a non-medical computervision task. The official implementation and cell annotations are publiclyavailable at ", "output": "Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-empowered Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Road traffic scene reconstruction from videos has been desirable by roadsafety regulators, city planners, researchers, and autonomous drivingtechnology developers. However, it is expensive and unnecessary to cover everymile of the road with cameras mounted on the road infrastructure. This paperpresents a method that can process aerial videos to vehicle trajectory data sothat a traffic scene can be automatically reconstructed and accuratelyre-simulated using computers. On average, the vehicle localization error isabout 0.1 m to 0.3 m using a consumer-grade drone flying at 120 meters. Thisproject also compiles a dataset of 50 reconstructed road traffic scenes fromabout 100 hours of aerial videos to enable various downstream traffic analysisapplications and facilitate further road traffic related research. The datasetis available at ", "output": "CAROM Air -- Vehicle Localization and Traffic Scene Reconstruction from Aerial Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Two-Tower Vision-Language (VL) models have shown promising improvements onvarious downstream VL tasks. Although the most advanced work improvesperformance by building bridges between encoders, it suffers from ineffectivelayer-by-layer utilization of uni-modal representations and cannot flexiblyexploit different levels of uni-modal semantic knowledge. In this work, wepropose ManagerTower, a novel VL model architecture that gathers and combinesthe insights of pre-trained uni-modal experts at different levels. The managersintroduced in each cross-modal layer can adaptively aggregate uni-modalsemantic knowledge to facilitate more comprehensive cross-modal alignment andfusion. ManagerTower outperforms previous strong baselines both with andwithout Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTowerachieves superior performances on various downstream VL tasks, especially79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K.Code and checkpoints are available at ", "output": "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a new way to identify additional positive pairs for BYOL,a state-of-the-art (SOTA) self-supervised learning framework, to improve itsrepresentation learning ability. Unlike conventional BYOL which relies on onlyone positive pair generated by two augmented views of the same image, we arguethat information from different images with the same label can bring morediversity and variations to the target features, thus benefiting representationlearning. To identify such pairs without any label, we investigate TracIn, aninstance-based and computationally efficient influence function, for BYOLtraining. Specifically, TracIn is a gradient-based method that reveals theimpact of a training sample on a test sample in supervised learning. We extendit to the self-supervised learning setting and propose an efficient batch-wiseper-sample gradient computation method to estimate the pairwise TracIn torepresent the similarity of samples in the mini-batch during training. For eachimage, we select the most similar sample from other images as the additionalpositive and pull their features together with BYOL loss. Experimental resultson two public medical datasets (i.e., ISIC 2019 and ChestX-ray) demonstratethat the proposed method can improve the classification performance compared toother competitive baselines in both semi-supervised and transfer learningsettings.", "output": "Additional Positive Enables Better Representation Learning for Medical Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monitoring land cover using remote sensing is vital for studyingenvironmental changes and ensuring global food security through crop yieldforecasting. Specifically, multitemporal remote sensing imagery providesrelevant information about the dynamics of a scene, which has proven to lead tobetter land cover classification results. Nevertheless, few studies havebenefited from high spatial and temporal resolution data due to the difficultyof accessing reliable, fine-grained and high-quality annotated samples tosupport their hypotheses. Therefore, we introduce a temporal patch-baseddataset of Canadian croplands, enriched with labels retrieved from the CanadianAnnual Crop Inventory. The dataset contains 78,536 manually verified andcurated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10crop classes collected over four crop production years (2017-2020) and fivemonths (June-October). Each instance contains 12 spectral bands, an RGB image,and additional vegetation index bands. Individually, each category contains atleast 4,800 images. Moreover, as a benchmark, we provide models and source codethat allow a user to predict the crop class using a single image (ResNet,DenseNet, EfficientNet) or a sequence of images (LRCN, 3D-CNN) from the samelocation. In perspective, we expect this evolving dataset to propel thecreation of robust agro-environmental models that can accelerate thecomprehension of complex agricultural regions by providing accurate andcontinuous monitoring of land cover.", "output": "The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human vision demonstrates higher robustness than current AI algorithms underout-of-distribution scenarios. It has been conjectured such robustness benefitsfrom performing analysis-by-synthesis. Our paper formulates triple vision tasksin a consistent manner using approximate analysis-by-synthesis byrender-and-compare algorithms on neural features. In this work, we introduceNeural Textured Deformable Meshes, which involve the object model withdeformable geometry that allows optimization on both camera parameters andobject geometries. The deformable mesh is parameterized as a neural field, andcovered by whole-surface neural texture maps, which are trained to have spatialdiscriminability. During inference, we extract the feature map of the testimage and subsequently optimize the 3D pose and shape parameters of our modelusing differentiable rendering to best reconstruct the target feature map. Weshow that our analysis-by-synthesis is much more robust than conventionalneural networks when evaluated on real-world images and even in challengingout-of-distribution scenarios, such as occlusion and domain shift. Ouralgorithms are competitive with standard algorithms when tested on conventionalperformance measures.", "output": "Neural Textured Deformable Meshes for Robust Analysis-by-Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object pose estimation is important for object manipulation and sceneunderstanding. In order to improve the general applicability of poseestimators, recent research focuses on providing estimates for novel objects,that is objects unseen during training. Such works use deep template matchingstrategies to retrieve the closest template connected to a query image. Thistemplate retrieval implicitly provides object class and pose. Despite therecent success and improvements of Vision Transformers over CNNs for manyvision tasks, the state of the art uses CNN-based approaches for novel objectpose estimation. This work evaluates and demonstrates the differences betweenself-supervised CNNs and Vision Transformers for deep template matching. Indetail, both types of approaches are trained using contrastive learning tomatch training images against rendered templates of isolated objects. At testtime, such templates are matched against query images of known and novelobjects under challenging settings, such as clutter, occlusion and objectsymmetries, using masked cosine similarity. The presented results not onlydemonstrate that Vision Transformers improve in matching accuracy over CNNs,but also that for some cases pre-trained Vision Transformers do not needfine-tuning to do so. Furthermore, we highlight the differences in optimizationand network architecture when comparing these two types of network for deeptemplate matching.", "output": "Self-supervised Vision Transformers for 3D Pose Estimation of Novel Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Public imaging datasets are critical for the development and evaluation ofautomated tools in cancer imaging. Unfortunately, many do not includeannotations or image-derived features, complicating their downstream analysis.Artificial intelligence-based annotation tools have been shown to achieveacceptable performance and thus can be used to automatically annotate largedatasets. As part of the effort to enrich public data available within NCIImaging Data Commons (IDC), here we introduce AI-generated annotations for twocollections of computed tomography images of the chest, NSCLC-Radiomics, andthe National Lung Screening Trial. Using publicly available AI algorithms wederived volumetric annotations of thoracic organs at risk, their correspondingradiomics features, and slice-level annotations of anatomical landmarks andregions. The resulting annotations are publicly available within IDC, where theDICOM format is used to harmonize the data and achieve FAIR principles. Theannotations are accompanied by cloud-enabled notebooks demonstrating their use.This study reinforces the need for large, publicly accessible curated datasetsand demonstrates how AI can be used to aid in cancer imaging.", "output": "Enrichment of the NLST and NSCLC-Radiomics computed tomography collections with AI-derived annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reconstruction of 3D neural fields from posed images has emerged as apromising method for self-supervised representation learning. The key challengepreventing the deployment of these 3D scene learners on large-scale video datais their dependence on precise camera poses from structure-from-motion, whichis prohibitively expensive to run at scale. We propose a method that jointlyreconstructs camera poses and 3D neural scene representations online and in asingle forward pass. We estimate poses by first lifting frame-to-frame opticalflow to 3D scene flow via differentiable rendering, preserving locality andshift-equivariance of the image processing backbone. SE(3) camera poseestimation is then performed via a weighted least-squares fit to the scene flowfield. This formulation enables us to jointly supervise pose estimation and ageneralizable neural scene representation via re-rendering the input video, andthus, train end-to-end and fully self-supervised on real-world video datasets.We demonstrate that our method performs robustly on diverse, real-world video,notably on sequences traditionally challenging to optimization-based poseestimation techniques.", "output": "FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a growing gap between the impressive results of deep imagegenerative models and classical algorithms that offer theoretical guarantees.The former suffer from mode collapse or memorization issues, limiting theirapplication to scientific data. The latter require restrictive assumptions suchas log-concavity to escape the curse of dimensionality. We partially bridgethis gap by introducing conditionally strongly log-concave (CSLC) models, whichfactorize the data distribution into a product of conditional probabilitydistributions that are strongly log-concave. This factorization is obtainedwith orthogonal projectors adapted to the data distribution. It leads toefficient parameter estimation and sampling algorithms, with theoreticalguarantees, although the data distribution is not globally log-concave. We showthat several challenging multiscale processes are conditionally log-concaveusing wavelet packet orthogonal projectors. Numerical results are shown forphysical fields such as the $varphi^4$ model and weak lensing convergence mapswith higher resolution than in previous works.", "output": "Conditionally Strongly Log-Concave Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep reinforcement learning(DRL) is increasingly being explored in medicalimaging. However, the environments for medical imaging tasks are constantlyevolving in terms of imaging orientations, imaging sequences, and pathologies.To that end, we developed a Lifelong DRL framework, SERIL to continually learnnew tasks in changing imaging environments without catastrophic forgetting.SERIL was developed using selective experience replay based lifelong learningtechnique for the localization of five anatomical landmarks in brain MRI on asequence of twenty-four different imaging environments. The performance ofSERIL, when compared to two baseline setups: MERT(multi-environment-best-case)and SERT(single-environment-worst-case) demonstrated excellent performance withan average distance of $9.90pm7.35$ pixels from the desired landmark acrossall 120 tasks, compared to $10.29pm9.07$ for MERT and $36.37pm22.41$ forSERT($p&lt;0.05$), demonstrating the excellent potential for continuously learningmultiple tasks across dynamically changing imaging environments.", "output": "Multi-environment lifelong deep reinforcement learning for medical imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven methods have shown tremendous progress in medical image analysis.In this context, deep learning-based supervised methods are widely popular.However, they require a large amount of training data and face issues ingeneralisability to unseen datasets that hinder clinical translation.Endoscopic imaging data incorporates large inter- and intra-patient variabilitythat makes these models more challenging to learn representative features fordownstream tasks. Thus, despite the publicly available datasets and datasetsthat can be generated within hospitals, most supervised models stillunderperform. While self-supervised learning has addressed this problem to someextent in natural scene data, there is a considerable performance gap in themedical image domain. In this paper, we propose to explore patch-levelinstance-group discrimination and penalisation of inter-class variation usingadditive angular margin within the cosine similarity metrics. Our novelapproach enables models to learn to cluster similar representative patches,thereby improving their ability to provide better separation between differentclasses. Our results demonstrate significant improvement on all metrics overthe state-of-the-art (SOTA) methods on the test set from the same and diversedatasets. We evaluated our approach for classification, detection, andsegmentation. SSL-CPCD achieves 79.77% on Top 1 accuracy for ulcerative colitisclassification, 88.62% on mAP for polyp detection, and 82.32% on dicesimilarity coefficient for segmentation tasks are nearly over 4%, 2%, and 3%,respectively, compared to the baseline architectures. We also demonstrate thatour method generalises better than all SOTA methods to unseen datasets,reporting nearly 7% improvement in our generalisability assessment.", "output": "SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transferring the pose of a reference avatar to stylized 3D characters ofvarious shapes is a fundamental task in computer graphics. Existing methodseither require the stylized characters to be rigged, or they use the stylizedcharacter in the desired pose as ground truth at training. We present azero-shot approach that requires only the widely available deformednon-stylized avatars in training, and deforms stylized characters ofsignificantly different shapes at inference. Classical methods achieve stronggeneralization by deforming the mesh at the triangle level, but this requireslabelled correspondences. We leverage the power of local deformation, butwithout requiring explicit correspondence labels. We introduce asemi-supervised shape-understanding module to bypass the need for explicitcorrespondences at test time, and an implicit pose deformation module thatdeforms individual surface points to match the target pose. Furthermore, toencourage realistic and accurate deformation of stylized characters, weintroduce an efficient volume-based test-time training procedure. Because itdoes not need rigging, nor the deformed stylized character at training time,our model generalizes to categories with scarce annotation, such as stylizedquadrupeds. Extensive experiments demonstrate the effectiveness of the proposedmethod compared to the state-of-the-art approaches trained with comparable ormore supervision. Our project page is available at", "output": "Zero-shot Pose Transfer for Unrigged Stylized 3D Characters."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning (DL) techniques are highly effective for defect detection fromimages. Training DL classification models, however, requires vast amounts oflabeled data which is often expensive to collect. In many cases, not only theavailable training data is limited but may also imbalanced. In this paper, wepropose a novel domain adaptation (DA) approach to address the problem oflabeled training data scarcity for a target learning task by transferringknowledge gained from an existing source dataset used for a similar learningtask. Our approach works for scenarios where the source dataset and the datasetavailable for the target learning task have same or different feature spaces.We combine our DA approach with an autoencoder-based data augmentation approachto address the problem of imbalanced target datasets. We evaluate our combinedapproach using image data for wafer defect prediction. The experiments show itssuperior performance against other algorithms when the number of labeledsamples in the target dataset is significantly small and the target dataset isimbalanced.", "output": "Building Manufacturing Deep Learning Models with Minimal and Imbalanced Training Data Using Domain Adaptation and Data Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image generative models have made remarkable advancements ingenerating high-quality images. However, generated images often containundesirable artifacts or other errors due to model limitations. Existingtechniques to fine-tune generated images are time-consuming (manual editing),produce poorly-integrated results (inpainting), or result in unexpected changesacross the entire image (variation selection and prompt fine-tuning). In thiswork, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool toefficiently fine-tune desired regions within an AI-synthesized image. Ourmethod introduces new random noise patterns at targeted regions during thereverse diffusion process, enabling the model to efficiently make changes tothe specified regions while preserving the original context for the rest of theimage. We evaluate our method's usability and effectiveness through a userstudy with artists, comparing our technique against other state-of-the-artimage inpainting techniques and editing software for fine-tuning AI-generatedimagery.", "output": "Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual Question Answering is a challenging task, as it requires seamlessinteraction between perceptual, linguistic, and background knowledge systems.While the recent progress of visual and natural language models like BLIP hasled to improved performance on this task, we lack understanding of the abilityof such models to perform on different kinds of questions and reasoning types.As our initial analysis of BLIP-family models revealed difficulty withanswering fine-detail questions, we investigate the following question: Canvisual cropping be employed to improve the performance of state-of-the-artvisual question answering models on fine-detail questions? Given the recentsuccess of the BLIP-family models, we study a zero-shot and a fine-tuned BLIPmodel. We define three controlled subsets of the popular VQA-v2 benchmark tomeasure whether cropping can help model performance. Besides human cropping, wedevise two automatic cropping strategies based on multi-modal embedding by CLIPand BLIP visual QA model gradients. Our experiments demonstrate that theperformance of BLIP model variants can be significantly improved through humancropping, and automatic cropping methods can produce comparable benefits. Adeeper dive into our findings indicates that the performance enhancement ismore pronounced in zero-shot models than in fine-tuned models and more salientwith smaller bounding boxes than larger ones. We perform case studies toconnect quantitative differences with qualitative observations across questiontypes and datasets. Finally, we see that the cropping enhancement is robust, aswe gain an improvement of 4.59% (absolute) in the general VQA-random task bysimply inputting a concatenation of the original and gradient-based croppedimages. We make our code available to facilitate further innovation on visualcropping methods for question answering.", "output": "Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Forensic science heavily relies on analyzing latent fingerprints, which arecrucial for criminal investigations. However, various challenges, such asbackground noise, overlapping prints, and contamination, make theidentification process difficult. Moreover, limited access to real crime sceneand laboratory-generated databases hinders the development of efficientrecognition algorithms. This study aims to develop a fast method, which we callULPrint, to enhance various latent fingerprint types, including those obtainedfrom real crime scenes and laboratory-created samples, to boost fingerprintrecognition system performance. In closed-set identification accuracyexperiments, the enhanced image was able to improve the performance of theMSU-AFIS from 61.56% to 75.19% in the NIST SD27 database, from 67.63% to77.02% in the MSP Latent database, and from 46.90% to 52.12% in the NISTSD302 database. Our contributions include (1) the development of a two-steplatent fingerprint enhancement method that combines Ridge Segmentation withUNet and Mix Visual Transformer (MiT) SegFormer-B5 encoder architecture, (2)the implementation of multiple dilated convolutions in the UNet architecture tocapture intricate, non-local patterns better and enhance ridge segmentation,and (3) the guided blending of the predicted ridge mask with the latentfingerprint. This novel approach, ULPrint, streamlines the enhancement process,addressing challenges across diverse latent fingerprint types to improveforensic investigations and criminal justice outcomes.", "output": "A Universal Latent Fingerprint Enhancer Using Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern deep learning approaches usually transform inputs into amodality-specific form. For example, the most common deep learning approach toimage classification involves decoding image file bytes into an RGB tensorwhich is passed into a neural network. Instead, we investigate performingclassification directly on file bytes, without the need for decoding files atinference time. Using file bytes as model inputs enables the development ofmodels which can operate on multiple input modalities. Our model,emph{ByteFormer}, achieves an ImageNet Top-1 classification accuracy of$77.33%$ when training and testing directly on TIFF file bytes using atransformer backbone with configuration similar to DeiT-Ti ($72.2%$ accuracywhen operating on RGB images). Without modifications or hyperparameter tuning,ByteFormer achieves $95.42%$ classification accuracy when operating on WAVfiles from the Speech Commands v2 dataset (compared to state-of-the-artaccuracy of $98.7%$). Additionally, we demonstrate that ByteFormer hasapplications in privacy-preserving inference. ByteFormer is capable ofperforming inference on particular obfuscated input representations with noloss of accuracy. We also demonstrate ByteFormer's ability to perform inferencewith a hypothetical privacy-preserving camera which avoids forming full imagesby consistently masking $90%$ of pixel channels, while still achieving$71.35%$ accuracy on ImageNet. Our code will be made available at", "output": "Bytes Are All You Need: Transformers Operating Directly On File Bytes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The exploration of the latent space in StyleGANs and GAN inversion exemplifyimpressive real-world image editing, yet the trade-off between reconstructionquality and editing quality remains an open problem. In this study, we revisitStyleGANs' hyperspherical prior $mathcal{Z}$ and $mathcal{Z}^+$ and integratethem into seminal GAN inversion methods to improve editing quality. Besidesfaithful reconstruction, our extensions achieve sophisticated editing qualitywith the aid of the StyleGAN prior. We project the real images into theproposed space to obtain the inverted codes, by which we then move along$mathcal{Z}^{+}$, enabling semantic editing without sacrificing image quality.Comprehensive experiments show that $mathcal{Z}^{+}$ can replace the mostcommonly-used $mathcal{W}$, $mathcal{W}^{+}$, and $mathcal{S}$ spaces whilepreserving reconstruction quality, resulting in reduced distortion of editedimages.", "output": "Balancing Reconstruction and Editing Quality of GAN Inversion for Real Image Editing with StyleGAN Prior Latent Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Much of the previous work towards digital agents for graphical userinterfaces (GUIs) has relied on text-based representations (derived from HTMLor other structured data sources), which are not always readily available.These input representations have been often coupled with custom, task-specificaction spaces. This paper focuses on creating agents that interact with thedigital world using the same conceptual interface that humans commonly use --via pixel-based screenshots and a generic action space corresponding tokeyboard and mouse actions. Building upon recent progress in pixel-basedpretraining, we show, for the first time, that it is possible for such agentsto outperform human crowdworkers on the MiniWob++ benchmark of GUI-basedinstruction following tasks.", "output": "From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The monetary value of a given piece of real estate, a parcel, is oftenreadily available from a geographic information system. However, for manyapplications, such as insurance and urban planning, it is useful to haveestimates of property value at much higher spatial resolutions. We propose amethod to estimate the distribution over property value at the pixel level fromremote sensing imagery. We evaluate on a real-world dataset of a major urbanarea. Our results show that the proposed approaches are capable of generatingfine-level estimates of property values, significantly improving upon a diversecollection of baseline approaches.", "output": "Fine-Grained Property Value Assessment using Probabilistic Disaggregation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain adaptation (DA) adapts a training dataset from a source domain for usein a learning task in a target domain in combination with data available at thetarget. One popular approach for DA is to create a domain-independentrepresentation (DIRep) learned by a generator from all input samples and thentrain a classifier on top of it using all labeled samples. A domaindiscriminator is added to train the generator adversarially to exclude domainspecific features from the DIRep. However, this approach tends to generateinsufficient information for accurate classification learning. In this paper,we present a novel approach that integrates the adversarial model with avariational autoencoder. In addition to the DIRep, we introduce adomain-dependent representation (DDRep) such that information from both DIRepand DDRep is sufficient to reconstruct samples from both domains. We furtherpenalize the size of the DDRep to drive as much information as possible to theDIRep, which maximizes the accuracy of the classifier in labeling samples inboth domains. We empirically evaluate our model using synthetic datasets anddemonstrate that spurious class-related features introduced in the sourcedomain are successfully absorbed by the DDRep. This leaves a rich and cleanDIRep for accurate transfer learning in the target domain. We furtherdemonstrate its superior performance against other algorithms for a number ofcommon image datasets. We also show we can take advantage of pretrained models.", "output": "Maximal Domain Independent Representations Improve Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-training is an important technique for solving semi-supervised learningproblems. It leverages unlabeled data by generating pseudo-labels and combiningthem with a limited labeled dataset for training. The effectiveness ofself-training heavily relies on the accuracy of these pseudo-labels. In thispaper, we introduce doubly robust self-training, a novel semi-supervisedalgorithm that provably balances between two extremes. When the pseudo-labelsare entirely incorrect, our method reduces to a training process solely usinglabeled data. Conversely, when the pseudo-labels are completely accurate, ourmethod transforms into a training process utilizing all pseudo-labeled data andlabeled data, thus increasing the effective sample size. Through empiricalevaluations on both the ImageNet dataset for image classification and thenuScenes autonomous driving dataset for 3D object detection, we demonstrate thesuperiority of the doubly robust loss over the standard self-training baseline.", "output": "Doubly Robust Self-Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This document presents a preliminary approach to latent fingerprintenhancement, fundamentally designed around a mixed Unet architecture. Itcombines the capabilities of the Resnet-101 network and Unet encoder, aiming toform a potentially powerful composite. This combination, enhanced withattention mechanisms and forward skip connections, is intended to optimize theenhancement of ridge and minutiae features in fingerprints. One innovativeelement of this approach includes a novel Fingerprint Enhancement Gabor layer,specifically designed for GPU computations. This illustrates how moderncomputational resources might be harnessed to expedite enhancement. Given itspotential functionality as either a CNN or Transformer layer, this Gabor layercould offer improved agility and processing speed to the system. However, it isimportant to note that this approach is still in the early stages ofdevelopment and has not yet been fully validated through rigorous experiments.As such, it may require additional time and testing to establish its robustnessand usability in the field of latent fingerprint enhancement. This includesimprovements in processing speed, enhancement adaptability with distinct latentfingerprint types, and full validation in experimental approaches such asopen-set (identification 1:N) and open-set validation, fingerprint qualityevaluation, among others.", "output": "Accelerated Fingerprint Enhancement: A GPU-Optimized Mixed Architecture Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural network approaches are machine learning methods that are widely usedin various domains, such as healthcare and cybersecurity. Neural networks areespecially renowned for their ability to deal with image datasets. During thetraining process with images, various fundamental mathematical operations areperformed in the neural network. These operations include several algebraic andmathematical functions, such as derivatives, convolutions, and matrixinversions and transpositions. Such operations demand higher processing powerthan what is typically required for regular computer usage. Since CPUs arebuilt with serial processing, they are not appropriate for handling large imagedatasets. On the other hand, GPUs have parallel processing capabilities and canprovide higher speed. This paper utilizes advanced neural network techniques,such as VGG16, Resnet50, Densenet, Inceptionv3, Xception, Mobilenet, XGBOOSTVGG16, and our proposed models, to compare CPU and GPU resources. Weimplemented a system for classifying Autism disease using face images ofautistic and non-autistic children to compare performance during testing. Weused evaluation matrices such as Accuracy, F1 score, Precision, Recall, andExecution time. It was observed that GPU outperformed CPU in all testsconducted. Moreover, the performance of the neural network models in terms ofaccuracy increased on GPU compared to CPU.", "output": "Autism Disease Detection Using Transfer Learning Techniques: Performance Comparison Between Central Processing Unit vs Graphics Processing Unit Functions for Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The spreading of attention has been proposed as a mechanism for how humansgroup features to segment objects. However, such a mechanism has not yet beenimplemented and tested in naturalistic images. Here, we leverage the featuremaps from self-supervised vision Transformers and propose a model of humanobject-based attention spreading and segmentation. Attention spreads within anobject through the feature affinity signal between different patches of theimage. We also collected behavioral data on people grouping objects in naturalimages by judging whether two dots are on the same object or on two differentobjects. We found that our models of affinity spread that were built on featuremaps from the self-supervised Transformers showed significant improvement overbaseline and CNN based models on predicting reaction time patterns of humans,despite not being trained on the task or with any other object labels. Our workprovides new benchmarks for evaluating models of visual representation learningincluding Transformers.", "output": "Affinity-based Attention in Self-supervised Transformers Predicts Dynamics of Object Grouping in Humans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper surveys and evaluates some popular state of the art methods foralgorithmic curvature and normal estimation. In addition to surveying existingmethods we also propose a new method for robust curvature estimation andevaluate it against existing methods thus demonstrating its superiority toexisting methods in the case of significant data noise. Throughout this paperwe are concerned with computation in low dimensional spaces (N &lt; 10) andprimarily focus on the computation of the Weingarten map and quantities thatmay be derived from this; however, the algorithms discussed are theoreticallyapplicable in any dimension. One thing that is common to all these methods istheir basis in an estimated graph structure. For any of these methods to workthe local geometry of the manifold must be exploited; however, in the case ofpoint cloud data it is often difficult to discover a robust manifold structureunderlying the data, even in simple cases, which can greatly influence theresults of these algorithms. We hope that in pushing these algorithms to theirlimits we are able to discover, and perhaps resolve, many major pitfalls thatmay affect potential users and future researchers hoping to improve thesemethods", "output": "Robust Estimation of Surface Curvature Information from Point Cloud Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The deep learning, which is a dominating technique in artificialintelligence, has completely changed the image understanding over the pastdecade. As a consequence, the sea ice extraction (SIE) problem has reached anew era. We present a comprehensive review of four important aspects of SIE,including algorithms, datasets, applications, and the future trends. Our reviewfocuses on researches published from 2016 to the present, with a specific focuson deep learning-based approaches in the last five years. We divided allrelegated algorithms into 3 categories, including classical image segmentationapproach, machine learning-based approach and deep learning-based methods. Wereviewed the accessible ice datasets including SAR-based datasets, theoptical-based datasets and others. The applications are presented in 4 aspectsincluding climate research, navigation, geographic information systems (GIS)production and others. It also provides insightful observations and inspiringfuture research directions.", "output": "Sea Ice Extraction via Remote Sensed Imagery: Algorithms, Datasets, Applications and Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have achieved promising results in image restoration tasks,yet suffer from time-consuming, excessive computational resource consumption,and unstable restoration. To address these issues, we propose a robust andefficient Diffusion-based Low-Light image enhancement approach, dubbed DiffLL.Specifically, we present a wavelet-based conditional diffusion model (WCDM)that leverages the generative power of diffusion models to produce results withsatisfactory perceptual fidelity. Additionally, it also takes advantage of thestrengths of wavelet transformation to greatly accelerate inference and reducecomputational resource usage without sacrificing information. To avoid chaoticcontent and diversity, we perform both forward diffusion and reverse denoisingin the training phase of WCDM, enabling the model to achieve stable denoisingand reduce randomness during inference. Moreover, we further design ahigh-frequency restoration module (HFRM) that utilizes the vertical andhorizontal details of the image to complement the diagonal information forbetter fine-grained restoration. Extensive experiments on publicly availablereal-world benchmarks demonstrate that our method outperforms the existingstate-of-the-art methods both quantitatively and visually, and it achievesremarkable improvements in efficiency compared to previous diffusion-basedmethods. In addition, we empirically show that the application for low-lightface detection also reveals the latent practical values of our method.", "output": "Low-Light Image Enhancement with Wavelet-based Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate whether prompts learned independently for different tasks canbe later combined through prompt algebra to obtain a model that supportscomposition of tasks. We consider Visual Language Models (VLM) with prompttuning as our base classifier and formally define the notion of prompt algebra.We propose constrained prompt tuning to improve performance of the compositeclassifier. In the proposed scheme, prompts are constrained to appear in thelower dimensional subspace spanned by the basis vectors of the pre-trainedvocabulary. Further regularization is added to ensure that the learned promptis grounded correctly to the existing pre-trained vocabulary. We demonstratethe effectiveness of our method on object classification and object-attributeclassification datasets. On average, our composite model obtains classificationaccuracy within 2.5% of the best base model. On UTZappos it improvesclassification accuracy over the best base model by 8.45% on average.", "output": "Prompt Algebra for Task Composition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Perception is crucial in the realm of autonomous driving systems, wherebird's eye view (BEV)-based architectures have recently reachedstate-of-the-art performance. The desirability of self-supervisedrepresentation learning stems from the expensive and laborious process ofannotating 2D and 3D data. Although previous research has investigatedpretraining methods for both LiDAR and camera-based 3D object detection, aunified pretraining framework for multimodal BEV perception is missing. In thisstudy, we introduce CALICO, a novel framework that applies contrastiveobjectives to both LiDAR and camera backbones. Specifically, CALICOincorporates two stages: point-region contrast (PRC) and region-awaredistillation (RAD). PRC better balances the region- and scene-levelrepresentation learning on the LiDAR modality and offers significantperformance improvement compared to existing methods. RAD effectively achievescontrastive distillation on our self-trained teacher model. CALICO's efficacyis substantiated by extensive evaluations on 3D object detection and BEV mapsegmentation tasks, where it delivers significant performance improvements.Notably, CALICO outperforms the baseline method by 10.5% and 8.6% on NDS andmAP. Moreover, CALICO boosts the robustness of multimodal 3D object detectionagainst adversarial attacks and corruption. Additionally, our framework can betailored to different backbones and heads, positioning it as a promisingapproach for multimodal BEV perception.", "output": "CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion-based generative models have achieved remarkable success in variousdomains. It trains a model on denoising tasks that encompass different noiselevels simultaneously, representing a form of multi-task learning (MTL).However, analyzing and improving diffusion models from an MTL perspectiveremains under-explored. In particular, MTL can sometimes lead to the well-knownphenomenon of $textit{negative transfer}$, which results in the performancedegradation of certain tasks due to conflicts between tasks. In this paper, weaim to analyze diffusion training from an MTL standpoint, presenting two keyobservations: $textbf{(O1)}$ the task affinity between denoising tasksdiminishes as the gap between noise levels widens, and $textbf{(O2)}$ negativetransfer can arise even in the context of diffusion training. Building uponthese observations, our objective is to enhance diffusion training bymitigating negative transfer. To achieve this, we propose leveraging existingMTL methods, but the presence of a huge number of denoising tasks makes thiscomputationally expensive to calculate the necessary per-task loss or gradient.To address this challenge, we propose clustering the denoising tasks into smalltask clusters and applying MTL methods to them. Specifically, based on$textbf{(O2)}$, we employ interval clustering to enforce temporal proximityamong denoising tasks within clusters. We show that interval clustering can besolved with dynamic programming and utilize signal-to-noise ratio, timestep,and task affinity for clustering objectives. Through this, our approachaddresses the issue of negative transfer in diffusion models by allowing forefficient computation of MTL methods. We validate the proposed clustering andits integration with MTL methods through various experiments, demonstratingimproved sample quality of diffusion models.", "output": "Addressing Negative Transfer in Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional Neural Networks (ConvNets) usually rely on edge/shapeinformation to classify images. Visualization methods developed over the lastdecade confirm that ConvNets rely on edge information. We investigatesituations where the ConvNet needs to rely on image intensity in addition toshape. We show that the ConvNet relies on image intensity information usingvisualization.", "output": "How Do ConvNets Understand Image Intensity?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamical systems with complex behaviours, e.g. immune system cellsinteracting with a pathogen, are commonly modelled by splitting the behaviourinto different regimes, or modes, each with simpler dynamics, and then learningthe switching behaviour from one mode to another. Switching Dynamical Systems(SDS) are a powerful tool that automatically discovers these modes andmode-switching behaviour from time series data. While effective, these methodsfocus on independent objects, where the modes of one object are independent ofthe modes of the other objects. In this paper, we focus on the more generalinteracting object setting for switching dynamical systems, where theper-object dynamics also depends on an unknown and dynamically changing subsetof other objects and their modes. To this end, we propose a novel graph-basedapproach for switching dynamical systems, GRAph Switching dynamical Systems(GRASS), in which we use a dynamic graph to characterize interactions betweenobjects and learn both intra-object and inter-object mode-switching behaviour.We introduce two new datasets for this setting, a synthesized ODE-drivenparticles dataset and a real-world Salsa Couple Dancing dataset. Experimentsshow that GRASS can consistently outperforms previous state-of-the-art methods.", "output": "Graph Switching Dynamical Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present GenMM, a generative model that \"mines\" as many diverse motions aspossible from a single or few example sequences. In stark contrast to existingdata-driven methods, which typically require long offline training time, areprone to visual artifacts, and tend to fail on large and complex skeletons,GenMM inherits the training-free nature and the superior quality of thewell-known Motion Matching method. GenMM can synthesize a high-quality motionwithin a fraction of a second, even with highly complex and large skeletalstructures. At the heart of our generative framework lies the generative motionmatching module, which utilizes the bidirectional visual similarity as agenerative cost function to motion matching, and operates in a multi-stageframework to progressively refine a random guess using exemplar motion matches.In addition to diverse motion generation, we show the versatility of ourgenerative framework by extending it to a number of scenarios that are notpossible with motion matching alone, including motion completion, keyframe-guided generation, infinite looping, and motion reassembly. Code and datafor this paper are at ", "output": "Example-based Motion Synthesis via Generative Motion Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "E-commerce websites (e.g. Amazon) have a plethora of structured andunstructured information (text and images) present on the product pages.Sellers often either don't label or mislabel values of the attributes (e.g.color, size etc.) for their products. Automatically identifying these attributevalues from an eCommerce product page that contains both text and images is achallenging task, especially when the attribute value is not explicitlymentioned in the catalog. In this paper, we present a scalable solution forthis problem where we pose attribute extraction problem as a question-answeringtask, which we solve using textbf{MXT}, consisting of three key components:(i) textbf{M}AG (Multimodal Adaptation Gate), (ii) textbf{X}ception network,and (iii) textbf{T}5 encoder-decoder. Our system consists of a generativemodel that emph{generates} attribute-values for a given product by using bothtextual and visual characteristics (e.g. images) of the product. We show thatour system is capable of handling zero-shot attribute prediction (whenattribute value is not seen in training data) and value-absent prediction (whenattribute value is not mentioned in the text) which are missing in traditionalclassification-based and NER-based models respectively. We have trained ourmodels using distant supervision, removing dependency on human labeling, thusmaking them practical for real-world applications. With this framework, we areable to train a single model for 1000s of (product-type, attribute) pairs, thusreducing the overhead of training and maintaining separate models. Extensiveexperiments on two real world datasets show that our framework improves theabsolute recall@90P by 10.16% and 6.9% from the existing state of the artmodels. In a popular e-commerce store, we have deployed our models for 1000s of(product-type, attribute) pairs.", "output": "Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of learning-based hyperspectral image compression methods hasrecently attracted great attention in remote sensing. Such methods require ahigh number of hyperspectral images to be used during training to optimize allparameters and reach a high compression performance. However, existinghyperspectral datasets are not sufficient to train and evaluate learning-basedcompression methods, which hinders the research in this field. To address thisproblem, in this paper we present HySpecNet-11k that is a large-scalehyperspectral benchmark dataset made up of 11,483 nonoverlapping image patches.Each patch is a portion of 128 $times$ 128 pixels with 224 spectral bands anda ground sample distance of 30 m. We exploit HySpecNet-11k to benchmark thecurrent state of the art in learning-based hyperspectral image compression byfocussing our attention on various 1D, 2D and 3D convolutional autoencoderarchitectures. Nevertheless, HySpecNet-11k can be used for any unsupervisedlearning task in the framework of hyperspectral image analysis. The dataset,our code and the pre-trained weights are publicly available at", "output": "HySpecNet-11k: A Large-Scale Hyperspectral Dataset for Benchmarking Learning-Based Hyperspectral Image Compression Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Color-guided depth super-resolution (DSR) is an encouraging paradigm thatenhances a low-resolution (LR) depth map guided by an extra high-resolution(HR) RGB image from the same scene. Existing methods usually use interpolationto upscale the depth maps before feeding them into the network and transfer thehigh-frequency information extracted from HR RGB images to guide thereconstruction of depth maps. However, the extracted high-frequency informationusually contains textures that are not present in depth maps in the existenceof the cross-modality gap, and the noises would be further aggravated byinterpolation due to the resolution gap between the RGB and depth images. Totackle these challenges, we propose a novel Symmetric Uncertainty-aware FeatureTransmission (SUFT) for color-guided DSR. (1) For the resolution gap, SUFTbuilds an iterative up-and-down sampling pipeline, which makes depth featuresand RGB features spatially consistent while suppressing noise amplification andblurring by replacing common interpolated pre-upsampling. (2) For thecross-modality gap, we propose a novel Symmetric Uncertainty scheme to removeparts of RGB information harmful to the recovery of HR depth maps. Extensiveexperiments on benchmark datasets and challenging real-world settings suggestthat our method achieves superior performance compared to state-of-the-artmethods. Our code and models are available at", "output": "Symmetric Uncertainty-Aware Feature Transmission for Depth Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rise in popularity of video-based social media, new categories ofvideos are constantly being generated, creating an urgent need for robustincremental learning techniques for video understanding. One of the biggestchallenges in this task is catastrophic forgetting, where the network tends toforget previously learned data while learning new categories. To overcome thisissue, knowledge distillation is a widely used technique for rehearsal-basedvideo incremental learning that involves transferring important information onsimilarities among different categories to enhance the student model.Therefore, it is preferable to have a strong teacher model to guide thestudents. However, the limited performance of the network itself and theoccurrence of catastrophic forgetting can result in the teacher network makinginaccurate predictions for some memory exemplars, ultimately limiting thestudent network's performance. Based on these observations, we propose ateacher agent capable of generating stable and accurate soft labels to replacethe output of the teacher model. This method circumvents the problem ofknowledge misleading caused by inaccurate predictions of the teacher model andavoids the computational overhead of loading the teacher model for knowledgedistillation. Extensive experiments demonstrate the advantages of our method,yielding significant performance improvements while utilizing only half theresolution of video clips in the incremental phases as input compared to recentstate-of-the-art methods. Moreover, our method surpasses the performance ofjoint training when employing four times the number of samples in episodicmemory.", "output": "Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in vision backbones have significantly improved theirperformance by simultaneously modeling images' local and global contexts.However, the bidirectional interaction between these two contexts has not beenwell explored and exploited, which is important in the human visual system.This paper proposes a Fully Adaptive Self-Attention (FASA) mechanism for visiontransformer to model the local and global information as well as thebidirectional interaction between them in context-aware ways. Specifically,FASA employs self-modulated convolutions to adaptively extract localrepresentation while utilizing self-attention in down-sampled space to extractglobal representation. Subsequently, it conducts a bidirectional adaptationprocess between local and global representation to model their interaction. Inaddition, we introduce a fine-grained downsampling strategy to enhance thedown-sampled self-attention mechanism for finer-grained global perceptioncapability. Based on FASA, we develop a family of lightweight vision backbones,Fully Adaptive Transformer (FAT) family. Extensive experiments on multiplevision tasks demonstrate that FAT achieves impressive performance. Notably, FATaccomplishes a 77.6% accuracy on ImageNet-1K using only 4.5M parameters and0.7G FLOPs, which surpasses the most advanced ConvNets and Transformers withsimilar model size and computational costs. Moreover, our model exhibits fasterspeed on modern GPU compared to other models. Code will be available at", "output": "Lightweight Vision Transformer with Bidirectional Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the huge success of deep convolutional neural networks in facerecognition (FR) tasks, current methods lack explainability for theirpredictions because of their \"black-box\" nature. In recent years, studies havebeen carried out to give an interpretation of the decision of a deep FR system.However, the affinity between the input facial image and the extracted deepfeatures has not been explored. This paper contributes to the problem ofexplainable face recognition by first conceiving a face reconstruction-basedexplanation module, which reveals the correspondence between the deep featureand the facial regions. To further interpret the decision of an FR model, anovel visual saliency explanation algorithm has been proposed. It providesinsightful explanation by producing visual saliency maps that represent similarand dissimilar regions between input faces. A detailed analysis has beenpresented for the generated visual explanation to show the effectiveness of theproposed method.", "output": "Discriminative Deep Feature Visualization for Explainable Face Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One tough problem of image inpainting is to restore complex structures in thecorrupted regions. It motivates interactive image inpainting which leveragesadditional hints, e.g., sketches, to assist the inpainting process. Sketch issimple and intuitive to end users, but meanwhile has free forms with muchrandomness. Such randomness may confuse the inpainting models, and incur severeartifacts in completed images. To address this problem, we propose a two-stageimage inpainting method termed SketchRefiner. In the first stage, we proposeusing a cross-correlation loss function to robustly calibrate and refine theuser-provided sketches in a coarse-to-fine fashion. In the second stage, welearn to extract informative features from the abstracted sketches in thefeature space and modulate the inpainting process. We also propose an algorithmto simulate real sketches automatically and build a test protocol withdifferent applications. Experimental results on public datasets demonstratethat SketchRefiner effectively utilizes sketch information and eliminates theartifacts due to the free-form sketches. Our method consistently outperformsthe state-of-the-art ones both qualitatively and quantitatively, meanwhilerevealing great potential in real-world applications. Our code and dataset areavailable.", "output": "Towards Interactive Image Inpainting via Sketch Refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pre-trained language models (PLMs) have played an increasing role inmultimedia research. In terms of vision-language (VL) tasks, they often serveas a language encoder and still require an additional fusion network for VLreasoning, resulting in excessive memory overhead. In this paper, we focus onexploring PLMs as a stand-alone model for VL reasoning tasks. Inspired by therecently popular prompt tuning, we first prove that the processed visualfeatures can be also projected onto the semantic space of PLMs and act asprompt tokens to bridge the gap between single- and multi-modal learning.However, this solution exhibits obvious redundancy in visual information andmodel inference, and the placement of prompt tokens also greatly affects thefinal performance. Based on these observations, we further propose a noveltransfer learning approach for PLMs, termed Dynamic Visual Prompting (DVP).Concretely, DVP first deploys a cross-attention module to obtain text-relatedand compact visual prompt tokens, thereby greatly reducing the input length ofPLMs. To obtain the optimal placement, we also equip DVP with areinforcement-learning based search algorithm, which can automatically mergeDVP with PLMs for different VL tasks via a very short search process. Inaddition, we also experiment DVP with the recently popular adapter approach tokeep the most parameters of PLMs intact when adapting to VL tasks, helping PLMsachieve a quick shift between single- and multi-modal tasks. We apply DVP totwo representative PLMs, namely BERT and T5, and conduct extensive experimentson a set of VL reasoning benchmarks including VQA2.0, GQA and SNLIVE. Theexperimental results not only show the advantage of DVP on efficiency andperformance, but also confirm its superiority in adapting pre-trained languagemodels to VL tasks.", "output": "Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating realistic and controllable motions for virtual characters is achallenging task in computer animation, and its implications extend to games,simulations, and virtual reality. Recent studies have drawn inspiration fromthe success of diffusion models in image generation, demonstrating thepotential for addressing this task. However, the majority of these studies havebeen limited to offline applications that target at sequence-level generationthat generates all steps simultaneously. To enable real-time motion synthesiswith diffusion models in response to time-varying control signals, we proposethe framework of the Controllable Motion Diffusion Model (COMODO). Ourframework begins with an auto-regressive motion diffusion model (A-MDM), whichgenerates motion sequences step by step. In this way, simply using the standardDDPM algorithm without any additional complexity, our framework is able togenerate high-fidelity motion sequences over extended periods with differenttypes of control signals. Then, we propose our reinforcement learning-basedcontroller and controlling strategies on top of the A-MDM model, so that ourframework can steer the motion synthesis process across multiple tasks,including target reaching, joystick-based control, goal-oriented control, andtrajectory following. The proposed framework enables the real-time generationof diverse motions that react adaptively to user commands on-the-fly, therebyenhancing the overall user experience. Besides, it is compatible with theinpainting-based editing methods and can predict much more diverse motionswithout additional fine-tuning of the basic motion generation models. Weconduct comprehensive experiments to evaluate the effectiveness of ourframework in performing various tasks and compare its performance againststate-of-the-art methods.", "output": "Controllable Motion Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical imaging informatics is a rapidly growing field that combines theprinciples of medical imaging and informatics to improve the acquisition,management, and interpretation of medical images. This chapter introduces thebasic concepts of medical imaging informatics, including image processing,feature engineering, and machine learning. It also discusses the recentadvancements in computer vision and deep learning technologies and how they areused to develop new quantitative image markers and prediction models fordisease detection, diagnosis, and prognosis prediction. By covering the basicknowledge of medical imaging informatics, this chapter provides a foundationfor understanding the role of informatics in medicine and its potential impacton patient care.", "output": "Introduction to Medical Imaging Informatics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate knowledge retrieval with multi-modal queries, i.e. queriescontaining information split across image and text inputs, a challenging taskthat differs from previous work on cross-modal retrieval. We curate a newdataset called ReMuQ for benchmarking progress on this task. ReMuQ requires asystem to retrieve knowledge from a large corpus by integrating contents fromboth text and image queries. We introduce a retriever model ``ReViz'' that candirectly process input text and images to retrieve relevant knowledge in anend-to-end fashion without being dependent on intermediate modules such asobject detectors or caption generators. We introduce a new pretraining taskthat is effective for learning knowledge retrieval with multimodal queries andalso improves performance on downstream tasks. We demonstrate superiorperformance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shotsettings as well as further improvements when finetuned on these datasets.", "output": "End-to-end Knowledge Retrieval with Multi-modal Queries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning (CL) is an important technique to allow artificial neuralnetworks to work in open environments. CL enables a system to learn new taskswithout severe interference to its performance on old tasks, i.e., overcome theproblems of catastrophic forgetting. In joint learning, it is well known thatthe out-of-distribution (OOD) problem caused by intentional attacks orenvironmental perturbations will severely impair the ability of networks togeneralize. In this work, we reported a special form of catastrophic forgettingraised by the OOD problem in continual learning settings, and we named itout-of-distribution forgetting (OODF). In continual image classification tasks,we found that for a given category, introducing an intra-class distributionshift significantly impaired the recognition accuracy of CL methods for thatcategory during subsequent learning. Interestingly, this phenomenon is specialfor CL as the same level of distribution shift had only negligible effects inthe joint learning scenario. We verified that CL methods without dedicatingsubnetworks for individual tasks are all vulnerable to OODF. Moreover, OODFdoes not depend on any specific way of shifting the distribution, suggesting itis a risk for CL in a wide range of circumstances. Taken together, our workidentified an under-attended risk during CL, highlighting the importance ofdeveloping approaches that can overcome OODF.", "output": "Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Underwater object detection (UOD) is crucial for marine economic development,environmental protection, and the planet's sustainable development. The mainchallenges of this task arise from low-contrast, small objects, and mimicry ofaquatic organisms. The key to addressing these challenges is to focus the modelon obtaining more discriminative information. We observe that the edges ofunderwater objects are highly unique and can be distinguished from low-contrastor mimicry environments based on their edges. Motivated by this observation, wepropose an Edge-guided Representation Learning Network, termed ERL-Net, thataims to achieve discriminative representation learning and aggregation underthe guidance of edge cues. Firstly, we introduce an edge-guided attentionmodule to model the explicit boundary information, which generates morediscriminative features. Secondly, a feature aggregation module is proposed toaggregate the multi-scale discriminative features by regrouping them into threelevels, effectively aggregating global and local information for locating andrecognizing underwater objects. Finally, we propose a wide and asymmetricreceptive field block to enable features to have a wider receptive field,allowing the model to focus on more small object information. Comprehensiveexperiments on three challenging underwater datasets show that our methodachieves superior performance on the UOD task.", "output": "Edge-guided Representation Learning for Underwater Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, \"U-shaped\" neural networks featuring encoder and decoderstructures have gained popularity in the field of medical image segmentation.Various variants of this model have been developed. Nevertheless, theevaluation of these models has received less attention compared to modeldevelopment. In response, we propose a comprehensive method for evaluatingmedical image segmentation models for multi-indicator and multi-organ (namedMIMO). MIMO allows models to generate independent thresholds which are thencombined with multi-indicator evaluation and confidence estimation to screenand measure each organ. As a result, MIMO offers detailed information on thesegmentation of each organ in each sample, thereby aiding developers inanalyzing and improving the model. Additionally, MIMO can produce conciseusability and comprehensiveness scores for different models. Models with higherscores are deemed to be excellent models, which is convenient for clinicalevaluation. Our research tests eight different medical image segmentationmodels on two abdominal multi-organ datasets and evaluates them from fourperspectives: correctness, confidence estimation, Usable Region and MIMO.Furthermore, robustness experiments are tested. Experimental resultsdemonstrate that MIMO offers novel insights into multi-indicator andmulti-organ medical image evaluation and provides a specific and concisemeasure for the usability and comprehensiveness of the model. Code:", "output": "Evaluation of Multi-indicator And Multi-organ Medical Image Segmentation Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation is a crucial task in computer vision that involvessegmenting images into semantically meaningful regions at the pixel level.However, existing approaches often rely on expensive human annotations assupervision for model training, limiting their scalability to large, unlabeleddatasets. To address this challenge, we present ZeroSeg, a novel method thatleverages the existing pretrained vision-language (VL) model (e.g. CLIP) totrain open-vocabulary zero-shot semantic segmentation models. Although acquiredextensive knowledge of visual concepts, it is non-trivial to exploit knowledgefrom these VL models to the task of semantic segmentation, as they are usuallytrained at an image level. ZeroSeg overcomes this by distilling the visualconcepts learned by VL models into a set of segment tokens, each summarizing alocalized region of the target image. We evaluate ZeroSeg on multiple popularsegmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO,in a zero-shot manner (i.e., no training or adaption on target segmentationdatasets). Our approach achieves state-of-the-art performance when compared toother zero-shot segmentation methods under the same training data, while alsoperforming competitively compared to strongly supervised methods. Finally, wealso demonstrated the effectiveness of ZeroSeg on open-vocabulary segmentation,through both human studies and qualitative visualizations.", "output": "Exploring Open-Vocabulary Semantic Segmentation without Human Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fully-supervised polyp segmentation has accomplished significant triumphsover the years in advancing the early diagnosis of colorectal cancer. However,label-efficient solutions from weak supervision like scribbles are rarelyexplored yet primarily meaningful and demanding in medical practice due to theexpensiveness and scarcity of densely-annotated polyp data. Besides, variousdeployment issues, including data shifts and corruption, put forward furtherrequests for model generalization and robustness. To address these concerns, wedesign a framework of Spatial-Spectral Dual-branch Mutual Teaching andEntropy-guided Pseudo Label Ensemble Learning (S$^2$ME). Concretely, for thefirst time in weakly-supervised medical image segmentation, we promote thedual-branch co-teaching framework by leveraging the intrinsic complementarityof features extracted from the spatial and spectral domains and encouragingcross-space consistency through collaborative optimization. Furthermore, toproduce reliable mixed pseudo labels, which enhance the effectiveness ofensemble learning, we introduce a novel adaptive pixel-wise fusion techniquebased on the entropy guidance from the spatial and spectral branches. Ourstrategy efficiently mitigates the deleterious effects of uncertainty and noisepresent in pseudo labels and surpasses previous alternatives in terms ofefficacy. Ultimately, we formulate a holistic optimization objective to learnfrom the hybrid supervision of scribbles and pseudo labels. Extensiveexperiments and evaluation on four public datasets demonstrate the superiorityof our method regarding in-distribution accuracy, out-of-distributiongeneralization, and robustness, highlighting its promising clinicalsignificance. Our code is available at ", "output": "S$^2$ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-supervised Polyp Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "MindBigData 2023 MNIST-8B is the largest, to date (June 1st 2023), brainsignals open dataset created for Machine Learning, based on EEG signals from asingle subject captured using a custom 128 channels device, replicating thefull 70,000 digits from Yaan LeCun et all MNIST dataset. The brain signals werecaptured while the subject was watching the pixels of the original digits oneby one on a screen and listening at the same time to the spoken number 0 to 9from the real label. The data, collection procedures, hardware and softwarecreated are described in detail, background extra information and other relateddatasets can be found at our previous paper MindBigData 2022: A Large Datasetof Brain Signals.", "output": "MindBigData 2023 MNIST-8B The 8 billion datapoints Multimodal Dataset of Brain Signals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Structural MRI(S-MRI) is one of the most versatile imaging modality thatrevolutionized the anatomical study of brain in past decades. The corpuscallosum (CC) is the principal white matter fibre tract, enabling all kinds ofinter-hemispheric communication. Thus, subtle changes in CC might be associatedwith various neurological disorders. The present work proposes the potential ofYOLOv5-based CC detection framework to differentiate atypical Parkinsoniandisorders (PD) from healthy controls (HC). With 3 rounds of hold-outvalidation, mean classification accuracy of 92% is obtained using the proposedmethod on a proprietary dataset consisting of 20 healthy subjects and 20 casesof APDs, with an improvement of 5% over SOTA methods (CC morphometry and visualtexture analysis) that used the same dataset. Subsequently, in order toincorporate the explainability of YOLO predictions, Eigen CAM based heatmap isgenerated for identifying the most important sub-region in CC that leads to theclassification. The result of Eigen CAM showed CC mid-body as the mostdistinguishable sub-region in classifying APDs and HC, which is in-line withSOTA methodologies and the current prevalent understanding in medicine.", "output": "Interpretable simultaneous localization of MRI corpus callosum and classification of atypical Parkinsonian disorders using YOLOv5."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-time perception requires planned resource utilization. Computationalplanning in real-time perception is governed by two considerations -- accuracyand latency. There exist run-time decisions (e.g. choice of input resolution)that induce tradeoffs affecting performance on a given hardware, arising fromintrinsic (content, e.g. scene clutter) and extrinsic (system, e.g. resourcecontention) characteristics.Earlier runtime execution frameworks employed rule-based decision algorithmsand operated with a fixed algorithm latency budget to balance these concerns,which is sub-optimal and inflexible. We propose Chanakya, a learned approximateexecution framework that naturally derives from the streaming perceptionparadigm, to automatically learn decisions induced by these tradeoffs instead.Chanakya is trained via novel rewards balancing accuracy and latencyimplicitly, without approximating either objectives. Chanakya simultaneouslyconsiders intrinsic and extrinsic context, and predicts decisions in a flexiblemanner. Chanakya, designed with low overhead in mind, outperformsstate-of-the-art static and dynamic execution policies on public datasets onboth server GPUs and edge devices.", "output": "Learning Runtime Decisions for Adaptive Real-Time Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Consistent segmentation of COVID-19 patient's CT scans across multiple timepoints is essential to assess disease progression and response to therapyaccurately. Existing automatic and interactive segmentation models for medicalimages only use data from a single time point (static). However, valuablesegmentation information from previous time points is often not used to aid thesegmentation of a patient's follow-up scans. Also, fully automatic segmentationtechniques frequently produce results that would need further editing forclinical use. In this work, we propose a new single network model forinteractive segmentation that fully utilizes all available past information torefine the segmentation of follow-up scans. In the first segmentation round,our model takes 3D volumes of medical images from two-time points (target andreference) as concatenated slices with the additional reference time pointsegmentation as a guide to segment the target scan. In subsequent segmentationrefinement rounds, user feedback in the form of scribbles that correct thesegmentation and the target's previous segmentation results are additionallyfed into the model. This ensures that the segmentation information fromprevious refinement rounds is retained. Experimental results on our in-housemulticlass longitudinal COVID-19 dataset show that the proposed modeloutperforms its static version and can assist in localizing COVID-19 infectionsin patient's follow-up scans.", "output": "Interactive Segmentation for COVID-19 Infection Quantification on Longitudinal CT scans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent times, deep neural networks achieved outstanding predictiveperformance on various classification and pattern recognition tasks. However,many real-world prediction problems have ordinal response variables, and thisordering information is ignored by conventional classification losses such asthe multi-category cross-entropy. Ordinal regression methods for deep neuralnetworks address this. One such method is the CORAL method, which is based onan earlier binary label extension framework and achieves rank consistency amongits output layer tasks by imposing a weight-sharing constraint. However, whileearlier experiments showed that CORAL's rank consistency is beneficial forperformance, it is limited by a weight-sharing constraint in a neural network'sfully connected output layer, which may restrict the expressiveness andcapacity of a network trained using CORAL. We propose a new method forrank-consistent ordinal regression without this limitation. Our rank-consistentordinal regression framework (CORN) achieves rank consistency by a noveltraining scheme. This training scheme uses conditional training sets to obtainthe unconditional rank probabilities through applying the chain rule forconditional probability distributions. Experiments on various datasetsdemonstrate the efficacy of the proposed method to utilize the ordinal targetinformation, and the absence of the weight-sharing restriction improves theperformance substantially compared to the CORAL reference approach.Additionally, the suggested CORN method is not tied to any specificarchitecture and can be utilized with any deep neural network classifier totrain it for ordinal regression tasks.", "output": "Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent work on image anonymization has shown that generative adversarialnetworks (GANs) can generate near-photorealistic faces to anonymizeindividuals. However, scaling up these networks to the entire human body hasremained a challenging and yet unsolved task. We propose a new anonymizationmethod that generates realistic humans for in-the-wild images. A key part ofour design is to guide adversarial nets by dense pixel-to-surfacecorrespondences between an image and a canonical 3D surface. We introduceVariational Surface-Adaptive Modulation (V-SAM) that embeds surface informationthroughout the generator. Combining this with our novel discriminator surfacesupervision loss, the generator can synthesize high quality humans with diverseappearances in complex and varying scenes. We demonstrate that surface guidancesignificantly improves image quality and diversity of samples, yielding ahighly practical generator. Finally, we show that our method preserves datausability without infringing privacy when collecting image datasets fortraining computer vision models. Source code and appendix is available at:href{", "output": "Realistic Full-Body Anonymization with Surface-Guided GANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reliable object detection using cameras plays a crucial role in enablingautonomous vehicles to perceive their surroundings. However, existingcamera-based object detection approaches for autonomous driving lack theability to provide comprehensive feedback on detection performance forindividual frames. To address this limitation, we propose a novel evaluationmetric, named as the detection quality index (DQI), which assesses theperformance of camera-based object detection algorithms and providesframe-by-frame feedback on detection quality. The DQI is generated by combiningthe intensity of the fine-grained saliency map with the output results of theobject detection algorithm. Additionally, we have developed a superpixel-basedattention network (SPA-NET) that utilizes raw image pixels and superpixels asinput to predict the proposed DQI evaluation metric. To validate our approach,we conducted experiments on three open-source datasets. The results demonstratethat the proposed evaluation metric accurately assesses the detection qualityof camera-based systems in autonomous driving environments. Furthermore, theproposed SPA-NET outperforms other popular image-based quality regressionmodels. This highlights the effectiveness of the DQI in evaluating a camera'sability to perceive visual scenes. Overall, our work introduces a valuableself-evaluation tool for camera-based object detection in autonomous vehicles.", "output": "A Quality Index Metric and Method for Online Self-Assessment of Autonomous Vehicles Sensory Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": ". Continuous Sign Language Recognition (CSLR) is a long challenging task inComputer Vision due to the difficulties in detecting the explicit boundariesbetween the words in a sign sentence. To deal with this challenge, we propose atwo-stage model. In the first stage, the predictor model, which includes acombination of CNN, SVD, and LSTM, is trained with the isolated signs. In thesecond stage, we apply a post-processing algorithm to the Softmax outputsobtained from the first part of the model in order to separate the isolatedsigns in the continuous signs. While the proposed model is trained on theisolated sign classes with similar frame numbers, it is evaluated on thecontinuous sign videos with a different frame length per each isolated signclass. Due to the lack of a large dataset, including both the sign sequencesand the corresponding isolated signs, two public datasets in Isolated SignLanguage Recognition (ISLR), RKS-PERSIANSIGN and ASLLVD, are used forevaluation. Results of the continuous sign videos confirm the efficiency of theproposed model to deal with isolated sign boundaries detection.", "output": "Word separation in continuous sign language using isolated signs and post-processing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to converse with humans and follow natural language commands iscrucial for intelligent unmanned aerial vehicles (a.k.a. drones). It canrelieve people's burden of holding a controller all the time, allowmultitasking, and make drone control more accessible for people withdisabilities or with their hands occupied. To this end, we introduce AerialVision-and-Dialog Navigation (AVDN), to navigate a drone via natural languageconversation. We build a drone simulator with a continuous photorealisticenvironment and collect a new AVDN dataset of over 3k recorded navigationtrajectories with asynchronous human-human dialogs between commanders andfollowers. The commander provides initial navigation instruction and furtherguidance by request, while the follower navigates the drone in the simulatorand asks questions when needed. During data collection, followers' attention onthe drone's visual observation is also recorded. Based on the AVDN dataset, westudy the tasks of aerial navigation from (full) dialog history and propose aneffective Human Attention Aided Transformer model (HAA-Transformer), whichlearns to predict both navigation waypoints and human attention.", "output": "Aerial Vision-and-Dialog Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Metal implants and other high-density objects in patients introduce severestreaking artifacts in CT images, compromising image quality and diagnosticperformance. Although various methods were developed for CT metal artifactreduction over the past decades, including the latest dual-domain deepnetworks, remaining metal artifacts are still clinically challenging in manycases. Here we extend the state-of-the-art dual-domain deep network approachinto a quad-domain counterpart so that all the features in the sinogram, image,and their corresponding Fourier domains are synergized to eliminate metalartifacts optimally without compromising structural subtleties. Our proposedquad-domain network for MAR, referred to as Quad-Net, takes little additionalcomputational cost since the Fourier transform is highly efficient, and worksacross the four receptive fields to learn both global and local features aswell as their relations. Specifically, we first design a Sinogram-FourierRestoration Network (SFR-Net) in the sinogram domain and its Fourier space tofaithfully inpaint metal-corrupted traces. Then, we couple SFR-Net with anImage-Fourier Refinement Network (IFR-Net) which takes both an image and itsFourier spectrum to improve a CT image reconstructed from the SFR-Net outputusing cross-domain contextual information. Quad-Net is trained on clinicaldatasets to minimize a composite loss function. Quad-Net does not requireprecise metal masks, which is of great importance in clinical practice. Ourexperimental results demonstrate the superiority of Quad-Net over thestate-of-the-art MAR methods quantitatively, visually, and statistically. TheQuad-Net code is publicly available at", "output": "Quad-Net: Quad-domain Network for CT Metal Artifact Reduction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked visual modeling (MVM) has been recently proven effective for visualpre-training. While similar reconstructive objectives on video inputs (e.g.,masked frame modeling) have been explored in video-language (VidL)pre-training, previous studies fail to find a truly effective MVM strategy thatcan largely benefit the downstream performance. In this work, we systematicallyexamine the potential of MVM in the context of VidL learning. Specifically, webase our study on a fully end-to-end VIdeO-LanguagE Transformer (VIOLET), wherethe supervision from MVM training can be backpropagated to the video pixelspace. In total, eight different reconstructive targets of MVM are explored,from low-level pixel values and oriented gradients to high-level depth maps,optical flow, discrete visual tokens, and latent visual features. We conductcomprehensive experiments and provide insights into the factors leading toeffective MVM training, resulting in an enhanced model VIOLETv2. Empirically,we show VIOLETv2 pre-trained with MVM objective achieves notable improvementson 13 VidL benchmarks, ranging from video question answering, video captioning,to text-to-video retrieval.", "output": "An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robotic perception is currently at a cross-roads between modern methods,which operate in an efficient latent space, and classical methods, which aremathematically founded and provide interpretable, trustworthy results. In thispaper, we introduce a Convolutional Bayesian Kernel Inference (ConvBKI) layerwhich learns to perform explicit Bayesian inference within a depthwiseseparable convolution layer to maximize efficency while maintaining reliabilitysimultaneously. We apply our layer to the task of real-time 3D semanticmapping, where we learn semantic-geometric probability distributions for LiDARsensor information and incorporate semantic predictions into a global map. Weevaluate our network against state-of-the-art semantic mapping algorithms onthe KITTI data set, demonstrating improved latency with comparable semanticlabel inference results.", "output": "Convolutional Bayesian Kernel Inference for 3D Semantic Mapping."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress in generative models, especially in text-guided diffusionmodels, has enabled the production of aesthetically-pleasing imagery resemblingthe works of professional human artists. However, one has to carefully composethe textual description, called the prompt, and augment it with a set ofclarifying keywords. Since aesthetics are challenging to evaluatecomputationally, human feedback is needed to determine the optimal promptformulation and keyword combination. In this paper, we present ahuman-in-the-loop approach to learning the most useful combination of promptkeywords using a genetic algorithm. We also show how such an approach canimprove the aesthetic appeal of images depicting the same descriptions.", "output": "Best Prompts for Text-to-Image Models and How to Find Them."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To generate accurate videos, algorithms have to understand the spatial andtemporal dependencies in the world. Current algorithms enable accuratepredictions over short horizons but tend to suffer from temporalinconsistencies. When generated content goes out of view and is laterrevisited, the model invents different content instead. Despite this severelimitation, no established benchmarks on complex data exist for rigorouslyevaluating video generation with long temporal dependencies. In this paper, wecurate 3 challenging video datasets with long-range dependencies by renderingwalks through 3D scenes of procedural mazes, Minecraft worlds, and indoorscans. We perform a comprehensive evaluation of current models and observetheir limitations in temporal consistency. Moreover, we introduce theTemporally Consistent Transformer (TECO), a generative model that substantiallyimproves long-term consistency while also reducing sampling time. Bycompressing its input sequence into fewer embeddings, applying a temporaltransformer, and expanding back using a spatial MaskGit, TECO outperformsexisting models across many metrics. Videos are available on the website:", "output": "Temporally Consistent Transformers for Video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixup is a data augmentation technique that relies on training using randomconvex combinations of data points and their labels. In recent years, Mixup hasbecome a standard primitive used in the training of state-of-the-art imageclassification models due to its demonstrated benefits over empirical riskminimization with regards to generalization and robustness. In this work, wetry to explain some of this success from a feature learning perspective. Wefocus our attention on classification problems in which each class may havemultiple associated features (or views) that can be used to predict the classcorrectly. Our main theoretical results demonstrate that, for a non-trivialclass of data distributions with two features per class, training a 2-layerconvolutional network using empirical risk minimization can lead to learningonly one feature for almost all classes while training with a specificinstantiation of Mixup succeeds in learning both features for every class. Wealso show empirically that these theoretical insights extend to the practicalsettings of image benchmarks modified to have multiple features.", "output": "Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rapid development of 3D vision, point cloud has become anincreasingly popular 3D visual media content. Due to the irregular structure,point cloud has posed novel challenges to the related research, such ascompression, transmission, rendering and quality assessment. In these latestresearches, point cloud quality assessment (PCQA) has attracted wide attentiondue to its significant role in guiding practical applications, especially inmany cases where the reference point cloud is unavailable. However, currentno-reference metrics which based on prevalent deep neural network have apparentdisadvantages. For example, to adapt to the irregular structure of point cloud,they require preprocessing such as voxelization and projection that introduceextra distortions, and the applied grid-kernel networks, such as ConvolutionalNeural Networks, fail to extract effective distortion-related features.Besides, they rarely consider the various distortion patterns and thephilosophy that PCQA should exhibit shifting, scaling, and rotationalinvariance. In this paper, we propose a novel no-reference PCQA metric namedthe Graph convolutional PCQA network (GPA-Net). To extract effective featuresfor PCQA, we propose a new graph convolution kernel, i.e., GPAConv, whichattentively captures the perturbation of structure and texture. Then, wepropose the multi-task framework consisting of one main task (qualityregression) and two auxiliary tasks (distortion type and degree predictions).Finally, we propose a coordinate normalization module to stabilize the resultsof GPAConv under shift, scale and rotation transformations. Experimentalresults on two independent databases show that GPA-Net achieves the bestperformance compared to the state-of-the-art no-reference PCQA metrics, evenbetter than some full-reference metrics in some cases.", "output": "GPA-Net:No-Reference Point Cloud Quality Assessment with Multi-task Graph Convolutional Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study neural network loss landscapes through the lens of modeconnectivity, the observation that minimizers of neural networks retrieved viatraining on a dataset are connected via simple paths of low loss. Specifically,we ask the following question: are minimizers that rely on different mechanismsfor making their predictions connected via simple paths of low loss? We providea definition of mechanistic similarity as shared invariances to inputtransformations and demonstrate that lack of linear connectivity between twomodels implies they use dissimilar mechanisms for making their predictions.Relevant to practice, this result helps us demonstrate that naive fine-tuningon a downstream dataset can fail to alter a model's mechanisms, e.g.,fine-tuning can fail to eliminate a model's reliance on spurious attributes.Our analysis also motivates a method for targeted alteration of a model'smechanisms, named connectivity-based fine-tuning (CBFT), which we analyze usingseveral synthetic datasets for the task of reducing a model's reliance onspurious attributes.", "output": "Mechanistic Mode Connectivity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present in this paper a new architecture, the Pattern AttentionTransformer (PAT), that is composed of the new doughnut kernel. Compared withtokens in the NLP field, Transformer in computer vision has the problem ofhandling the high resolution of pixels in images. In ViT, an image is cut intosquare-shaped patches. As the follow-up of ViT, Swin Transformer proposes anadditional step of shifting to decrease the existence of fixed boundaries,which also incurs 'two connected Swin Transformer blocks' as the minimum unitof the model. Inheriting the patch/window idea, our doughnut kernel enhancesthe design of patches further. It replaces the line-cut boundaries with twotypes of areas: sensor and updating, which is based on the comprehension ofself-attention (named QKVA grid). The doughnut kernel also brings a new topicabout the shape of kernels beyond square. To verify its performance on imageclassification, PAT is designed with Transformer blocks of regular octagonshape doughnut kernels. Its architecture is lighter: the minimum patternattention layer is only one for each stage. Under similar complexity ofcomputation, its performances on ImageNet 1K reach higher throughput (+10%) andsurpass Swin Transformer (+0.8 acc1).", "output": "Pattern Attention Transformer with Doughnut Kernel."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We perform an empirical study of the behaviour of deep networks when fullylinearizing some of its feature channels through a sparsity prior on theoverall number of nonlinear units in the network. In experiments on imageclassification and machine translation tasks, we investigate how much we cansimplify the network function towards linearity before performance collapses.First, we observe a significant performance gap when reducing nonlinearity inthe network function early on as opposed to late in training, in-line withrecent observations on the time-evolution of the data-dependent NTK. Second, wefind that after training, we are able to linearize a significant number ofnonlinear units while maintaining a high performance, indicating that much of anetwork's expressivity remains unused but helps gradient descent in earlystages of training. To characterize the depth of the resulting partiallylinearized network, we introduce a measure called average path length,representing the average number of active nonlinearities encountered along apath in the network graph. Under sparsity pressure, we find that the remainingnonlinear units organize into distinct structures, forming core-networks ofnear constant effective depth and width, which in turn depend on taskdifficulty.", "output": "Nonlinear Advantage: Trained Networks Might Not Be As Complex as You Think."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual Question Answering (VQA) models often perform poorly onout-of-distribution data and struggle on domain generalization. Due to themulti-modal nature of this task, multiple factors of variation are intertwined,making generalization difficult to analyze. This motivates us to introduce avirtual benchmark, Super-CLEVR, where different factors in VQA domain shiftscan be isolated in order that their effects can be studied independently. Fourfactors are considered: visual complexity, question redundancy, conceptdistribution and concept compositionality. With controllably generated data,Super-CLEVR enables us to test VQA methods in situations where the test datadiffers from the training data along each of these axes. We study four existingmethods, including two neural symbolic methods NSCL and NSVQA, and twonon-symbolic methods FiLM and mDETR; and our proposed method, probabilisticNSVQA (P-NSVQA), which extends NSVQA with uncertainty reasoning. P-NSVQAoutperforms other methods on three of the four domain shift factors. Ourresults suggest that disentangling reasoning and perception, combined withprobabilistic uncertainty, form a strong VQA model that is more robust todomain shifts. The dataset and code are released at", "output": "Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Surrogate models are necessary to optimize meaningful quantities in physicaldynamics as their recursive numerical resolutions are often prohibitivelyexpensive. It is mainly the case for fluid dynamics and the resolution ofNavier-Stokes equations. However, despite the fast-growing field of data-drivenmodels for physical systems, reference datasets representing real-worldphenomena are lacking. In this work, we develop AirfRANS, a dataset forstudying the two-dimensional incompressible steady-state Reynolds-AveragedNavier-Stokes equations over airfoils at a subsonic regime and for differentangles of attacks. We also introduce metrics on the stress forces at thesurface of geometries and visualization of boundary layers to assess thecapabilities of models to accurately predict the meaningful information of theproblem. Finally, we propose deep learning baselines on four machine learningtasks to study AirfRANS under different constraints for generalizationconsiderations: big and scarce data regime, Reynolds number, and angle ofattack extrapolation.", "output": "AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier-Stokes Solutions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning, as an essential ability for complex problem-solving, can provideback-end support for various real-world applications, such as medicaldiagnosis, negotiation, etc. This paper provides a comprehensive survey ofcutting-edge research on reasoning with language model prompting. We introduceresearch works with comparisons and summaries and provide systematic resourcesto help beginners. We also discuss the potential reasons for emerging suchreasoning abilities and highlight future research directions. Resources areavailable at  (updatedperiodically).", "output": "Reasoning with Language Model Prompting: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "According to WHO statistics, the number of visually impaired people isincreasing annually. One of the most critical necessities for visually impairedpeople is the ability to navigate safely. This paper proposes a navigationsystem based on the visual slam and Yolo algorithm using monocular cameras. Theproposed system consists of three steps: obstacle distance estimation, pathdeviation detection, and next-step prediction. Using the ORB-SLAM algorithm,the proposed method creates a map from a predefined route and guides the usersto stay on the route while notifying them if they deviate from it.Additionally, the system utilizes the YOLO algorithm to detect obstacles alongthe route and alert the user. The experimental results, obtained by using alaptop camera, show that the proposed system can run in 30 frame per secondwhile guiding the user within predefined routes of 11 meters in indoors andoutdoors. The accuracy of the positioning system is 8cm, and the systemnotifies the users if they deviate from the predefined route by more than 60cm.", "output": "Implementation of a Blind navigation method in outdoors/indoors areas."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An increasing number of public datasets have shown a marked impact onautomated organ segmentation and tumor detection. However, due to the smallsize and partially labeled problem of each dataset, as well as a limitedinvestigation of diverse types of tumors, the resulting models are oftenlimited to segmenting specific organs/tumors and ignore the semantics ofanatomical structures, nor can they be extended to novel domains. To addressthese issues, we propose the CLIP-Driven Universal Model, which incorporatestext embedding learned from Contrastive Language-Image Pre-training (CLIP) tosegmentation models. This CLIP-based label encoding captures anatomicalrelationships, enabling the model to learn a structured feature embedding andsegment 25 organs and 6 types of tumors. The proposed model is developed froman assembly of 14 datasets, using a total of 3,410 CT scans for training andthen evaluated on 6,162 external CT scans from 3 additional datasets. We rankfirst on the Medical Segmentation Decathlon (MSD) public leaderboard andachieve state-of-the-art results on Beyond The Cranial Vault (BTCV).Additionally, the Universal Model is computationally more efficient (6x faster)compared with dataset-specific models, generalized better to CT scans fromvarying sites, and shows stronger transfer learning performance on novel tasks.", "output": "CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Barlow Twins and VICReg are self-supervised representation learning modelsthat use regularizers to decorrelate features. Although these models are aseffective as conventional representation learning models, their training can becomputationally demanding if the dimension d of the projected embeddings ishigh. As the regularizers are defined in terms of individual elements of across-correlation or covariance matrix, computing the loss for n samples takesO(n d^2) time. In this paper, we propose a relaxed decorrelating regularizerthat can be computed in O(n d log d) time by Fast Fourier Transform. We alsopropose an inexpensive technique to mitigate undesirable local minima thatdevelop with the relaxation. The proposed regularizer exhibits accuracycomparable to that of existing regularizers in downstream tasks, whereas theirtraining requires less memory and is faster for large d. The source code isavailable.", "output": "Learning Decorrelated Representations Efficiently Using Fast Fourier Transform."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Climate change results in an increased probability of extreme weather eventsthat put societies and businesses at risk on a global scale. Therefore, nearreal-time mapping of natural hazards is an emerging priority for the support ofnatural disaster relief, risk management, and informing governmental policydecisions. Recent methods to achieve near real-time mapping increasinglyleverage deep learning (DL). However, DL-based approaches are designed for onespecific task in a single geographic region based on specific frequency bandsof satellite data. Therefore, DL models used to map specific natural hazardsstruggle with their generalization to other types of natural hazards in unseenregions. In this work, we propose a methodology to significantly improve thegeneralizability of DL natural hazards mappers based on pre-training on asuitable pre-task. Without access to any data from the target domain, wedemonstrate this improved generalizability across four U-Net architectures forthe segmentation of unseen natural hazards. Importantly, our method isinvariant to geographic differences and differences in the type of frequencybands of satellite data. By leveraging characteristics of unlabeled images fromthe target domain that are publicly available, our approach is able to furtherimprove the generalization behavior without fine-tuning. Thereby, our approachsupports the development of foundation models for earth monitoring with theobjective of directly segmenting unseen natural hazards across novel geographicregions given different sources of satellite imagery.", "output": "Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present D$^2$CSG, a neural model composed of two dual and complementarynetwork branches, with dropouts, for unsupervised learning of compactconstructive solid geometry (CSG) representations of 3D CAD shapes. Our networkis trained to reconstruct a 3D shape by a fixed-order assembly of quadricprimitives, with both branches producing a union of primitive intersections orinverses. A key difference between D$^2$CSG and all prior neural CSG models isits dedicated residual branch to assemble the potentially complex shapecomplement, which is subtracted from an overall shape modeled by the coverbranch. With the shape complements, our network is provably general, while theweight dropout further improves compactness of the CSG tree by removingredundant primitives. We demonstrate both quantitatively and qualitatively thatD$^2$CSG produces compact CSG reconstructions with superior quality and morenatural primitives than all existing alternatives, especially over complex andhigh-genus CAD shapes.", "output": "D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot classification consists of a training phase where a model is learnedon a relatively large dataset and an adaptation phase where the learned modelis adapted to previously-unseen tasks with limited labeled samples. In thispaper, we empirically prove that the training algorithm and the adaptationalgorithm can be completely disentangled, which allows algorithm analysis anddesign to be done individually for each phase. Our meta-analysis for each phasereveals several interesting insights that may help better understand keyaspects of few-shot classification and connections with other fields such asvisual representation learning and transfer learning. We hope the insights andresearch challenges revealed in this paper can inspire future work in relateddirections. Code and pre-trained models (in PyTorch) are available at", "output": "A Closer Look at Few-shot Classification Again."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Minerals are indispensable for a functioning modern society. Yet, theirsupply is limited causing a need for optimizing their exploration andextraction both from ores and recyclable materials. Typically, these processesmust be meticulously adapted to the precise properties of the processedparticles, an extensive characterization of their shapes, appearances as wellas the overall material composition. Current approaches perform this analysisbased on bulk segmentation and characterization of particles imaged with amicro CT, and rely on rudimentary postprocessing techniques to separatetouching particles. However, due to their inability to reliably perform thisseparation as well as the need to retrain or reconfigure methods for each newimage, these approaches leave untapped potential to be leveraged. Here, wepropose ParticleSeg3D, an instance segmentation method that is able to extractindividual particles from large micro CT images taken from mineral samplesembedded in an epoxy matrix. Our approach is based on the powerful nnU-Netframework, introduces a particle size normalization, makes use of a border-corerepresentation to enable instance segmentation and is trained with a largedataset containing particles of numerous different materials and minerals. Wedemonstrate that ParticleSeg3D can be applied out-of-the box to a large varietyof particle types, including materials and appearances that have not been partof the training set. Thus, no further manual annotations and retraining arerequired when applying the method to new mineral samples, enablingsubstantially higher scalability of experiments than existing methods. Our codeand dataset are made publicly available.", "output": "[Work in progress] Scalable, out-of-the box segmentation of individual particles from mineral samples acquired with micro CT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an efficient method to ground pretrained text-only language modelsto the visual domain, enabling them to process arbitrarily interleavedimage-and-text data, and generate text interleaved with retrieved images. Ourmethod leverages the abilities of language models learnt from large scaletext-only pretraining, such as in-context learning and free-form textgeneration. We keep the language model frozen, and finetune input and outputlinear layers to enable cross-modality interactions. This allows our model toprocess arbitrarily interleaved image-and-text inputs, and generate free-formtext interleaved with retrieved images. We achieve strong zero-shot performanceon grounded tasks such as contextual image retrieval and multimodal dialogue,and showcase compelling interactive abilities. Our approach works with anyoff-the-shelf language model and paves the way towards an effective, generalsolution for leveraging pretrained language models in visually groundedsettings.", "output": "Grounding Language Models to Images for Multimodal Inputs and Outputs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weight oscillation is an undesirable side effect of quantization-awaretraining, in which quantized weights frequently jump between two quantizedlevels, resulting in training instability and a sub-optimal final model. Wediscover that the learnable scaling factor, a widely-used $textit{de facto}$setting in quantization aggravates weight oscillation. In this study, weinvestigate the connection between the learnable scaling factor and quantizedweight oscillation and use ViT as a case driver to illustrate the findings andremedies. In addition, we also found that the interdependence between quantizedweights in $textit{query}$ and $textit{key}$ of a self-attention layer makesViT vulnerable to oscillation. We, therefore, propose three techniquesaccordingly: statistical weight quantization ($rm StatsQ$) to improvequantization robustness compared to the prevalent learnable-scale-based method;confidence-guided annealing ($rm CGA$) that freezes the weights with$textit{high confidence}$ and calms the oscillating weights; and$textit{query}$-$textit{key}$ reparameterization ($rm QKR$) to resolve thequery-key intertwined oscillation and mitigate the resulting gradientmisestimation. Extensive experiments demonstrate that these proposed techniquessuccessfully abate weight oscillation and consistently achieve substantialaccuracy improvement on ImageNet. Specifically, our 2-bit DeiT-T/DeiT-Salgorithms outperform the previous state-of-the-art by 9.8% and 7.7%,respectively. Code and models are available at:url{", "output": "Oscillation-free Quantization for Low-bit Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It has been recognized that the data generated by the denoising diffusionprobabilistic model (DDPM) improves adversarial training. After two years ofrapid development in diffusion models, a question naturally arises: can betterdiffusion models further improve adversarial training? This paper gives anaffirmative answer by employing the most recent diffusion model which hashigher efficiency ($sim 20$ sampling steps) and image quality (lower FIDscore) compared with DDPM. Our adversarially trained models achievestate-of-the-art performance on RobustBench using only generated data (noexternal datasets). Under the $ell_infty$-norm threat model with$epsilon=8/255$, our models achieve $70.69%$ and $42.67%$ robust accuracy onCIFAR-10 and CIFAR-100, respectively, i.e. improving upon previousstate-of-the-art models by $+4.58%$ and $+8.03%$. Under the $ell_2$-normthreat model with $epsilon=128/255$, our models achieve $84.86%$ on CIFAR-10($+4.44%$). These results also beat previous works that use external data. Wealso provide compelling results on the SVHN and TinyImageNet datasets. Our codeis available at ", "output": "Better Diffusion Models Further Improve Adversarial Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation is an effective method for model compression. However,it is still a challenging topic to apply knowledge distillation to detectiontasks. There are two key points resulting in poor distillation performance fordetection tasks. One is the serious imbalance between foreground and backgroundfeatures, another one is that small object lacks enough feature representation.To solve the above issues, we propose a new distillation method named dualrelation knowledge distillation (DRKD), including pixel-wise relationdistillation and instance-wise relation distillation. The pixel-wise relationdistillation embeds pixel-wise features in the graph space and applies graphconvolution to capture the global pixel relation. By distilling the globalpixel relation, the student detector can learn the relation between foregroundand background features, and avoid the difficulty of distilling featuresdirectly for the feature imbalance issue. Besides, we find that instance-wiserelation supplements valuable knowledge beyond independent features for smallobjects. Thus, the instance-wise relation distillation is designed, whichcalculates the similarity of different instances to obtain a relation matrix.More importantly, a relation filter module is designed to highlight valuableinstance relations. The proposed dual relation knowledge distillation isgeneral and can be easily applied for both one-stage and two-stage detectors.Our method achieves state-of-the-art performance, which improves Faster R-CNNbased on ResNet50 from 38.4% to 41.6% mAP and improves RetinaNet based onResNet50 from 37.4% to 40.3% mAP on COCO 2017.", "output": "Dual Relation Knowledge Distillation for Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Leveraging privileged information (PI), or features available during trainingbut not at test time, has recently been shown to be an effective method foraddressing label noise. However, the reasons for its effectiveness are not wellunderstood. In this study, we investigate the role played by differentproperties of the PI in explaining away label noise. Through experiments onmultiple datasets with real PI (CIFAR-N/H) and a new large-scale benchmarkImageNet-PI, we find that PI is most helpful when it allows networks to easilydistinguish clean from noisy data, while enabling a learning shortcut tomemorize the noisy examples. Interestingly, when PI becomes too predictive ofthe target label, PI methods often perform worse than their no-PI baselines.Based on these findings, we propose several enhancements to thestate-of-the-art PI methods and demonstrate the potential of PI as a means oftackling label noise. Finally, we show how we can easily combine the resultingPI approaches with existing no-PI techniques designed to deal with label noise.", "output": "When does Privileged Information Explain Away Label Noise?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce VOCALExplore, a system designed to support users in buildingdomain-specific models over video datasets. VOCALExplore supports interactivelabeling sessions and trains models using user-supplied labels. VOCALExploremaximizes model quality by automatically deciding how to select samples basedon observed skew in the collected labels. It also selects the optimal videorepresentations to use when training models by casting feature selection as arising bandit problem. Finally, VOCALExplore implements optimizations toachieve low latency without sacrificing model performance. We demonstrate thatVOCALExplore achieves close to the best possible model quality given candidateacquisition functions and feature extractors, and it does so with low visiblelatency (~1 second per iteration) and no expensive preprocessing.", "output": "VOCALExplore: Pay-as-You-Go Video Data Exploration and Model Building [Technical Report]."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a method for editing NeRF scenes with text-instructions. Given aNeRF of a scene and the collection of images used to reconstruct it, our methoduses an image-conditioned diffusion model (InstructPix2Pix) to iteratively editthe input images while optimizing the underlying scene, resulting in anoptimized 3D scene that respects the edit instruction. We demonstrate that ourproposed method is able to edit large-scale, real-world scenes, and is able toaccomplish more realistic, targeted edits than prior work.", "output": "Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classifier guidance -- using the gradients of an image classifier to steerthe generations of a diffusion model -- has the potential to dramaticallyexpand the creative control over image generation and editing. However,currently classifier guidance requires either training new noise-aware modelsto obtain accurate gradients or using a one-step denoising approximation of thefinal generation, which leads to misaligned gradients and sub-optimal control.We highlight this approximation's shortcomings and propose a novel guidancemethod: Direct Optimization of Diffusion Latents (DOODL), which enablesplug-and-play guidance by optimizing diffusion latents w.r.t. the gradients ofa pre-trained classifier on the true generated pixels, using an invertiblediffusion process to achieve memory-efficient backpropagation. Showcasing thepotential of more precise guidance, DOODL outperforms one-step classifierguidance on computational and human evaluation metrics across different formsof guidance: using CLIP guidance to improve generations of complex prompts fromDrawBench, using fine-grained visual classifiers to expand the vocabulary ofStable Diffusion, enabling image-conditioned generation with a CLIP visualencoder, and improving image aesthetics using an aesthetic scoring network.Code at ", "output": "End-to-End Diffusion Latent Optimization Improves Classifier Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Considering the ill-posed nature, contrastive regularization has beendeveloped for single image dehazing, introducing the information from negativeimages as a lower bound. However, the contrastive samples are nonconsensual, asthe negatives are usually represented distantly from the clear (i.e., positive)image, leaving the solution space still under-constricted. Moreover, theinterpretability of deep dehazing models is underexplored towards the physicsof the hazing process. In this paper, we propose a novel curricular contrastiveregularization targeted at a consensual contrastive space as opposed to anon-consensual one. Our negatives, which provide better lower-boundconstraints, can be assembled from 1) the hazy image, and 2) correspondingrestorations by other existing methods. Further, due to the differentsimilarities between the embeddings of the clear image and negatives, thelearning difficulty of the multiple components is intrinsically imbalanced. Totackle this issue, we customize a curriculum learning strategy to reweight theimportance of different negatives. In addition, to improve the interpretabilityin the feature space, we build a physics-aware dual-branch unit according tothe atmospheric scattering model. With the unit, as well as curricularcontrastive regularization, we establish our dehazing network, named C2PNet.Extensive experiments demonstrate that our C2PNet significantly outperformsstate-of-the-art methods, with extreme PSNR boosts of 3.94dB and 1.50dB,respectively, on SOTS-indoor and SOTS-outdoor datasets.", "output": "Curricular Contrastive Regularization for Physics-aware Single Image Dehazing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision Transformers (ViTs) have been shown to be effective in various visiontasks. However, resizing them to a mobile-friendly size leads to significantperformance degradation. Therefore, developing lightweight vision transformershas become a crucial area of research. This paper introduces CloFormer, alightweight vision transformer that leverages context-aware local enhancement.CloFormer explores the relationship between globally shared weights often usedin vanilla convolutional operators and token-specific context-aware weightsappearing in attention, then proposes an effective and straightforward moduleto capture high-frequency local information. In CloFormer, we introduceAttnConv, a convolution operator in attention's style. The proposed AttnConvuses shared weights to aggregate local information and deploys carefullydesigned context-aware weights to enhance local features. The combination ofthe AttnConv and vanilla attention which uses pooling to reduce FLOPs inCloFormer enables the model to perceive high-frequency and low-frequencyinformation. Extensive experiments were conducted in image classification,object detection, and semantic segmentation, demonstrating the superiority ofCloFormer. The code is available at url{", "output": "Rethinking Local Perception in Lightweight Vision Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-Based Person Searching (TBPS) aims to identify the images of pedestriantargets from a large-scale gallery with given textual caption. For cross-modalTBPS task, it is critical to obtain well-distributed representation in thecommon embedding space to reduce the inter-modal gap. Furthermore, it is alsoessential to learn detailed image-text correspondence efficiently todiscriminate similar targets and enable fine-grained target search. To addressthese challenges, we present a simple yet effective method that calibratescross-modal features from these two perspectives. Our method consists of twonovel losses to provide fine-grained cross-modal features. The Sew calibrationloss takes the quality of textual captions as guidance and aligns featuresbetween image and text modalities. On the other hand, the Masking CaptionModeling (MCM) loss leverages a masked captions prediction task to establishdetailed and generic relationships between textual and visual parts. Theproposed method is cost-effective and can easily retrieve specific persons withtextual captions. The architecture has only a dual-encoder without multi-levelbranches or extra interaction modules, making a high-speed inference. Ourmethod achieves top results on three popular benchmarks with 73.81%, 74.25% and57.35% Rank1 accuracy on the CUHK-PEDES, ICFG-PEDES, and RSTPReID,respectively. We hope our scalable method will serve as a solid baseline andhelp ease future research in TBPS. The code will be publicly available.", "output": "Calibrating Cross-modal Features for Text-Based Person Searching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper investigates the application of unsupervised learning methods forcomputed tomography (CT) reconstruction. To motivate our work, we reviewseveral existing priors, namely the truncated Gaussian prior, the $l_1$ prior,the total variation prior, and the deep image prior (DIP). We find that DIPoutperforms the other three priors in terms of representational capability andvisual performance. However, the performance of DIP deteriorates when thenumber of iterations exceeds a certain threshold due to overfitting. To addressthis issue, we propose a novel method (MCDIP-ADMM) based on Multi-Code DeepImage Prior and plug-and-play Alternative Direction Method of Multipliers.Specifically, MCDIP utilizes multiple latent codes to generate a series offeature maps at an intermediate layer within a generator model. These maps arethen composed with trainable weights, representing the complete image prior.Experimental results demonstrate the superior performance of the proposedMCDIP-ADMM compared to three existing competitors. In the case of parallel beamprojection with Gaussian noise, MCDIP-ADMM achieves an average improvement of4.3 dB over DIP, 1.7 dB over ADMM DIP-WTV, and 1.2 dB over PnP-DIP in terms ofPSNR. Similarly, for fan-beam projection with Poisson noise, MCDIP-ADMMachieves an average improvement of 3.09 dB over DIP, 1.86 dB over ADMM DIP-WTV,and 0.84 dB over PnP-DIP in terms of PSNR.", "output": "MCDIP-ADMM: Overcoming Overfitting in DIP-based CT reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has made remarkable progress in visualrepresentation learning. Some studies combine SSL with knowledge distillation(SSL-KD) to boost the representation learning performance of small models. Inthis study, we propose a Multi-mode Online Knowledge Distillation method (MOKD)to boost self-supervised visual representation learning. Different fromexisting SSL-KD methods that transfer knowledge from a static pre-trainedteacher to a student, in MOKD, two different models learn collaboratively in aself-supervised manner. Specifically, MOKD consists of two distillation modes:self-distillation and cross-distillation modes. Among them, self-distillationperforms self-supervised learning for each model independently, whilecross-distillation realizes knowledge interaction between different models. Incross-distillation, a cross-attention feature search strategy is proposed toenhance the semantic feature alignment between different models. As a result,the two models can absorb knowledge from each other to boost theirrepresentation learning performance. Extensive experimental results ondifferent backbones and datasets demonstrate that two heterogeneous models canbenefit from MOKD and outperform their independently trained baseline. Inaddition, MOKD also outperforms existing SSL-KD methods for both the studentand teacher models.", "output": "Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate and efficient classification of different types of cancer iscritical for early detection and effective treatment. In this paper, we presentthe results of our experiments using the EfficientNet algorithm forclassification of brain tumor, breast cancer mammography, chest cancer, andskin cancer. We used publicly available datasets and preprocessed the images toensure consistency and comparability. Our experiments show that theEfficientNet algorithm achieved high accuracy, precision, recall, and F1 scoreson each of the cancer datasets, outperforming other state-of-the-art algorithmsin the literature. We also discuss the strengths and weaknesses of theEfficientNet algorithm and its potential applications in clinical practice. Ourresults suggest that the EfficientNet algorithm is well-suited forclassification of different types of cancer and can be used to improve theaccuracy and efficiency of cancer diagnosis.", "output": "EfficientNet Algorithm for Classification of Different Types of Cancer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, the Segment Anything Model (SAM) emerged as a powerful visionfoundation model which is capable to segment anything in 2D images. This paperaims to generalize SAM to segment 3D objects. Rather than replicating the dataacquisition and annotation procedure which is costly in 3D, we design anefficient solution, leveraging the Neural Radiance Field (NeRF) as a cheap andoff-the-shelf prior that connects multi-view 2D images to the 3D space. Werefer to the proposed solution as SA3D, for Segment Anything in 3D. It is onlyrequired to provide a manual segmentation prompt (e.g., rough points) for thetarget object in a single view, which is used to generate its 2D mask in thisview with SAM. Next, SA3D alternately performs mask inverse rendering andcross-view self-prompting across various views to iteratively complete the 3Dmask of the target object constructed with voxel grids. The former projects the2D mask obtained by SAM in the current view onto 3D mask with guidance of thedensity distribution learned by the NeRF; The latter extracts reliable promptsautomatically as the input to SAM from the NeRF-rendered 2D mask in anotherview. We show in experiments that SA3D adapts to various scenes and achieves 3Dsegmentation within minutes. Our research offers a generic and efficientmethodology to lift a 2D vision foundation model to 3D, as long as the 2D modelcan steadily address promptable segmentation across multiple views. The projectpage is at ", "output": "Segment Anything in 3D with NeRFs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we are dedicated to leveraging the denoising diffusion models'success and formulating feature refinement as the autoencoder-formed diffusionprocess, which is a mask-and-predict scheme. The state-of-the-art CSLRframework consists of a spatial module, a visual module, a sequence module, anda sequence learning function. However, this framework has faced sequence moduleoverfitting caused by the objective function and small-scale availablebenchmarks, resulting in insufficient model training. To overcome theoverfitting problem, some CSLR studies enforce the sequence module to learnmore visual temporal information or be guided by more informative supervisionto refine its representations. In this work, we propose a novelautoencoder-formed conditional diffusion feature refinement~(ACDR) to refinethe sequence representations to equip desired properties by learning theencoding-decoding optimization process in an end-to-end way. Specifically, forthe ACDR, a noising Encoder is proposed to progressively add noise equippedwith semantic conditions to the sequence representations. And a denoisingDecoder is proposed to progressively denoise the noisy sequence representationswith semantic conditions. Therefore, the sequence representations can be imbuedwith the semantics of provided semantic conditions. Further, a semanticconstraint is employed to prevent the denoised sequence representations fromsemantic corruption. Extensive experiments are conducted to validate theeffectiveness of our ACDR, benefiting state-of-the-art methods and achieving anotable gain on three benchmarks.", "output": "Conditional Diffusion Feature Refinement for Continuous Sign Language Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale multi-modal contrastive learning frameworks like CLIP typicallyrequire a large amount of image-text samples for training. However, thesesamples are always collected continuously in real scenarios. This paperdiscusses the feasibility of continual CLIP training using streaming data.Unlike continual learning based on self-supervised learning methods for pureimages, which is empirically robust against catastrophic forgetting, CLIP'sperformance degeneration in the continual setting is significant andnon-neglectable. By analyzing the changes in the model's representation spaceduring continual CLIP training from a spatial geometry perspective, we exploreand summarize these spatial variations as Spatial Disorder (SD), which can bedivided into Intra-modal Rotation and Inter-modal Deviation. Moreover, weempirically and theoretically demonstrate how SD leads to a performance declinefor CLIP on cross-modal retrieval tasks. To alleviate SD, we propose a newcontinual vision-language representation learning framework Mod-X: Maintainoff-diagonal information-matriX. By selectively aligning the off-diagonalinformation distribution of contrastive matrices, the Mod-X improves thecapability of the multi-modal model by maintaining the multi-modalrepresentation space alignment on the old data domain during continuouslyfitting the new training data domain. Experiments on commonly used datasetswith different scales and scopes have demonstrated the effectiveness of ourmethod.", "output": "Continual Vision-Language Representation Learning with Off-Diagonal Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human-Object Interaction (HOI) detection is a task to localize humans andobjects in an image and predict the interactions in human-object pairs. Inreal-world scenarios, HOI detection models are required systematicgeneralization, i.e., generalization to novel combinations of objects andinteractions, because the train data are expected to cover a limited portion ofall possible combinations. However, to our knowledge, no open benchmarks orprevious work exist for evaluating the systematic generalization performance ofHOI detection models. To address this issue, we created two new sets of HOIdetection data splits named HICO-DET-SG and V-COCO-SG based on the HICO-DET andV-COCO datasets, respectively. When evaluated on the new data splits, therepresentative HOI detection models performed much more poorly than whenevaluated on the original splits. This reveals that systematic generalizationis a challenging goal in HOI detection. By analyzing the evaluation results, wealso gain insights for improving the systematic generalization performance andidentify four possible future research directions. We hope that our new datasplits and presented analysis will encourage further research on systematicgeneralization in HOI detection.", "output": "HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We apply convolutional neural networks (CNN) to the problem of imageorientation detection in the context of determining the correct orientation(from 0, 90, 180, and 270 degrees) of a consumer photo. The problem isespecially important for digitazing analog photographs. We substantiallyimprove on the published state of the art in terms of the performance on one ofthe standard datasets, and test our system on a more difficult large dataset ofconsumer photos. We use Guided Backpropagation to obtain insights into how ourCNN detects photo orientation, and to explain its mistakes.", "output": "Automatic Photo Orientation Detection with Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a lot of research effort into developing different techniques forneural networks compression. However, the community lacks standardisedevaluation metrics, which are key to identifying the most suitable compressiontechnique for different applications. This paper reviews existing neuralnetwork compression evaluation metrics and implements them into astandardisation framework called NetZIP. We introduce two novel metrics tocover existing gaps of evaluation in the literature: 1) Compression andHardware Agnostic Theoretical Speed (CHATS) and 2) Overall Compression Success(OCS). We demonstrate the use of NetZIP using two case studies focusing onobject classification and object detection.", "output": "Evaluation Metrics for DNNs Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models, either with or without text condition, have demonstratedimpressive capability in synthesizing photorealistic images given a few or evenno words. These models may not fully satisfy user need, as normal users orartists intend to control the synthesized images with specific guidance, likeoverall layout, color, structure, object shape, and so on. To adapt diffusionmodels for controllable image synthesis, several methods have been proposed toincorporate the required conditions as regularization upon the intermediatefeatures of the diffusion denoising network. These methods, known asearly-constraint ones in this paper, have difficulties in handling multipleconditions with a single solution. They intend to train separate models foreach specific condition, which require much training cost and result innon-generalizable solutions. To address these difficulties, we propose a newapproach namely late-constraint: we leave the diffusion networks unchanged, butconstrain its output to be aligned with the required conditions. Specifically,we train a lightweight condition adapter to establish the correlation betweenexternal conditions and internal representations of diffusion models. Duringthe iterative denoising process, the conditional guidance is sent intocorresponding condition adapter to manipulate the sampling process with theestablished correlation. We further equip the introduced late-constraintstrategy with a timestep resampling method and an early stopping technique,which boost the quality of synthesized image meanwhile complying with theguidance. Our method outperforms the existing early-constraint methods andgeneralizes better to unseen condition. Our code would be available.", "output": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the paradigm of AI-generated content (AIGC), there has been increasingattention in extending pre-trained text-to-image (T2I) models to text-to-video(T2V) generation. Despite their effectiveness, these frameworks face challengesin maintaining consistent narratives and handling rapid shifts in scenecomposition or object placement from a single user prompt. This paperintroduces a new framework, dubbed DirecT2V, which leverages instruction-tunedlarge language models (LLMs) to generate frame-by-frame descriptions from asingle abstract user prompt. DirecT2V utilizes LLM directors to divide userinputs into separate prompts for each frame, enabling the inclusion oftime-varying content and facilitating consistent video generation. To maintaintemporal consistency and prevent object collapse, we propose a novel valuemapping method and dual-softmax filtering. Extensive experimental resultsvalidate the effectiveness of the DirecT2V framework in producing visuallycoherent and consistent videos from abstract user prompts, addressing thechallenges of zero-shot video generation.", "output": "Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces Deceptive-NeRF, a new method for enhancing the qualityof reconstructed NeRF models using synthetically generated pseudo-observations,capable of handling sparse input and removing floater artifacts. Our proposedmethod involves three key steps: 1) reconstruct a coarse NeRF model from sparseinputs; 2) generate pseudo-observations based on the coarse model; 3) refinethe NeRF model using pseudo-observations to produce a high-qualityreconstruction. To generate photo-realistic pseudo-observations that faithfullypreserve the identity of the reconstructed scene while remaining consistentwith the sparse inputs, we develop a rectification latent diffusion model thatgenerates images conditional on a coarse RGB image and depth map, which arederived from the coarse NeRF and latent text embedding from input images.Extensive experiments show that our method is effective and can generateperceptually high-quality NeRF even with very sparse inputs.", "output": "Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image (T2I) research has grown explosively in the past year, owing tothe large-scale pre-trained diffusion models and many emerging personalizationand editing approaches. Yet, one pain point persists: the text promptengineering, and searching high-quality text prompts for customized results ismore art than science. Moreover, as commonly argued: \"an image is worth athousand words\" - the attempt to describe a desired image with texts often endsup being ambiguous and cannot comprehensively cover delicate visual details,hence necessitating more additional controls from the visual domain. In thispaper, we take a bold step forward: taking \"Text\" out of a pre-trained T2Idiffusion model, to reduce the burdensome prompt engineering efforts for users.Our proposed framework, Prompt-Free Diffusion, relies on only visual inputs togenerate new images: it takes a reference image as \"context\", an optional imagestructural conditioning, and an initial noise, with absolutely no text prompt.The core architecture behind the scene is Semantic Context Encoder (SeeCoder),substituting the commonly used CLIP-based or LLM-based text encoder. Thereusability of SeeCoder also makes it a convenient drop-in component: one canalso pre-train a SeeCoder in one T2I model and reuse it for another. Throughextensive experiments, Prompt-Free Diffusion is experimentally found to (i)outperform prior exemplar-based image synthesis approaches; (ii) perform on parwith state-of-the-art T2I models using prompts following the best practice; and(iii) be naturally extensible to other downstream applications such as animefigure generation and virtual try-on, with promising quality. Our code andmodels are open-sourced at ", "output": "Prompt-Free Diffusion: Taking \"Text\" out of Text-to-Image Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Controllable scene synthesis aims to create interactive environments forvarious industrial use cases. Scene graphs provide a highly suitable interfaceto facilitate these applications by abstracting the scene context in a compactmanner. Existing methods, reliant on retrieval from extensive databases orpre-trained shape embeddings, often overlook scene-object and object-objectrelationships, leading to inconsistent results due to their limited generationcapacity. To address this issue, we present CommonScenes, a fully generativemodel that converts scene graphs into corresponding controllable 3D scenes,which are semantically realistic and conform to commonsense. Our pipelineconsists of two branches, one predicting the overall scene layout via avariational auto-encoder and the other generating compatible shapes via latentdiffusion, capturing global scene-object and local inter-object relationshipswhile preserving shape diversity. The generated scenes can be manipulated byediting the input scene graph and sampling the noise in the diffusion model.Due to lacking a scene graph dataset offering high-quality object-level mesheswith relations, we also construct SG-FRONT, enriching the off-the-shelf indoordataset 3D-FRONT with additional scene graph labels. Extensive experiments areconducted on SG-FRONT where CommonScenes shows clear advantages over othermethods regarding generation consistency, quality, and diversity. Codes and thedataset will be released upon acceptance.", "output": "CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Out-of-distribution (OOD) detection is a critical requirement for thedeployment of deep neural networks. This paper introduces the HEAT model, a newpost-hoc OOD detection method estimating the density of in-distribution (ID)samples using hybrid energy-based models (EBM) in the feature space of apre-trained backbone. HEAT complements prior density estimators of the IDdensity, e.g. parametric models like the Gaussian Mixture Model (GMM), toprovide an accurate yet robust density estimation. A second contribution is toleverage the EBM framework to provide a unified density estimation and tocompose several energy terms. Extensive experiments demonstrate thesignificance of the two contributions. HEAT sets new state-of-the-art OODdetection results on the CIFAR-10 / CIFAR-100 benchmark as well as on thelarge-scale Imagenet benchmark. The code is available at:", "output": "Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The captivating realm of Minecraft has attracted substantial researchinterest in recent years, serving as a rich platform for developing intelligentagents capable of functioning in open-world environments. However, the currentresearch landscape predominantly focuses on specific objectives, such as thepopular \"ObtainDiamond\" task, and has not yet shown effective generalization toa broader spectrum of tasks. Furthermore, the current leading success rate forthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations ofReinforcement Learning (RL) based controllers used in existing methods. Totackle these challenges, we introduce Ghost in the Minecraft (GITM), a novelframework integrates Large Language Models (LLMs) with text-based knowledge andmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. Theseagents, equipped with the logic and common sense capabilities of LLMs, canskillfully navigate complex, sparse-reward environments with text-basedinteractions. We develop a set of structured actions and leverage LLMs togenerate action plans for the agents to execute. The resulting LLM-based agentmarkedly surpasses previous methods, achieving a remarkable improvement of+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superiorrobustness compared to traditional RL-based controllers. Notably, our agent isthe first to procure all items in the Minecraft Overworld technology tree,demonstrating its extensive capabilities. GITM does not need any GPU fortraining, but a single CPU node with 32 CPU cores is enough. This researchshows the potential of LLMs in developing capable agents for handlinglong-horizon, complex tasks and adapting to uncertainties in open-worldenvironments. See the project website at ", "output": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building generalizable AI models is one of the primary challenges in thehealthcare domain. While radiologists rely on generalizable descriptive rulesof abnormality, Neural Network (NN) models suffer even with a slight shift ininput distribution (e.g., scanner type). Fine-tuning a model to transferknowledge from one domain to another requires a significant amount of labeleddata in the target domain. In this paper, we develop an interpretable modelthat can be efficiently fine-tuned to an unseen target domain with minimalcomputational cost. We assume the interpretable component of NN to beapproximately domain-invariant. However, interpretable models typicallyunderperform compared to their Blackbox (BB) variants. We start with a BB inthe source domain and distill it into a emph{mixture} of shallow interpretablemodels using human-understandable concepts. As each interpretable model coversa subset of data, a mixture of interpretable models achieves comparableperformance as BB. Further, we use the pseudo-labeling technique fromsemi-supervised learning (SSL) to learn the concept classifier in the targetdomain, followed by fine-tuning the interpretable models in the target domain.We evaluate our model using a real-life large-scale chest-X-ray (CXR)classification dataset. The code is available at:url{", "output": "Distilling BlackBox to Interpretable models for Efficient Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a novel approach to evaluating deep learning models'capacity for in-diagram logic interpretation. Leveraging the intriguing realmof visual illusions, we establish a unique dataset, InDL, designed torigorously test and benchmark these models. Deep learning has witnessedremarkable progress in domains such as computer vision and natural languageprocessing. However, models often stumble in tasks requiring logical reasoningdue to their inherent 'black box' characteristics, which obscure thedecision-making process. Our work presents a new lens to understand thesemodels better by focusing on their handling of visual illusions -- a complexinterplay of perception and logic. We utilize six classic geometric opticalillusions to create a comparative framework between human and machine visualperception. This methodology offers a quantifiable measure to rank models,elucidating potential weaknesses and providing actionable insights for modelimprovements. Our experimental results affirm the efficacy of our benchmarkingstrategy, demonstrating its ability to effectively rank models based on theirlogic interpretation ability. As part of our commitment to reproducibleresearch, the source code and datasets will be made publicly available here:href{", "output": "InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Incorporation of physical information in machine learning frameworks areopening and transforming many application domains. Here the learning process isaugmented through the induction of fundamental knowledge and governing physicallaws. In this work we explore their utility for computer vision tasks ininterpreting and understanding visual data. We present a systematic literaturereview of formulation and approaches to computer vision tasks guided byphysical laws. We begin by decomposing the popular computer vision pipelineinto a taxonomy of stages and investigate approaches to incorporate governingphysical equations in each stage. Existing approaches in each task are analyzedwith regard to what governing physical processes are modeled, formulated andhow they are incorporated, i.e. modify data (observation bias), modify networks(inductive bias), and modify losses (learning bias). The taxonomy offers aunified view of the application of the physics-informed capability,highlighting where physics-informed learning has been conducted and where thegaps and opportunities are. Finally, we highlight open problems and challengesto inform future research. While still in its early days, the study ofphysics-informed computer vision has the promise to develop better computervision models that can improve physical plausibility, accuracy, data efficiencyand generalization in increasingly realistic applications.", "output": "Physics-Informed Computer Vision: A Review and Perspectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Field (NeRF) research has attracted significant attentionrecently, with 3D modelling, virtual/augmented reality, and visual effectsdriving its application. While current NeRF implementations can produce highquality visual results, there is a conspicuous lack of reliable methods forevaluating them. Conventional image quality assessment methods and analyticalmetrics (e.g. PSNR, SSIM, LPIPS etc.) only provide approximate indicators ofperformance since they generalise the ability of the entire NeRF pipeline.Hence, in this paper, we propose a new test framework which isolates the neuralrendering network from the NeRF pipeline and then performs a parametricevaluation by training and evaluating the NeRF on an explicit radiance fieldrepresentation. We also introduce a configurable approach for generatingrepresentations specifically for evaluation purposes. This employs ray-castingto transform mesh models into explicit NeRF samples, as well as to \"shade\"these representations. Combining these two approaches, we demonstrate howdifferent \"tasks\" (scenes with different visual effects or learning strategies)and types of networks (NeRFs and depth-wise implicit neural representations(INRs)) can be evaluated within this framework. Additionally, we propose anovel metric to measure task complexity of the framework which accounts for thevisual parameters and the distribution of the spatial data. Our approach offersthe potential to create a comparative objective evaluation framework for NeRFmethods.", "output": "Towards a Robust Framework for NeRF Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Super-resolution (SR) techniques designed for real-world applicationscommonly encounter two primary challenges: generalization performance andrestoration accuracy. We demonstrate that when methods are trained usingcomplex, large-range degradations to enhance generalization, a decline inaccuracy is inevitable. However, since the degradation in a certain real-worldapplications typically exhibits a limited variation range, it becomes feasibleto strike a trade-off between generalization performance and testing accuracywithin this scope. In this work, we introduce a novel approach to crafttraining degradation distributions using a small set of reference images. Ourstrategy is founded upon the binned representation of the degradation space andthe Fr'echet distance between degradation distributions. Our results indicatethat the proposed technique significantly improves the performance of testimages while preserving generalization capabilities in real-world applications.", "output": "Crafting Training Degradation Distribution for the Accuracy-Generalization Trade-off in Real-World Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shownexceptional performance across various tasks. Although parameter-efficientfine-tuning (PEFT) has emerged to cheaply fine-tune these large models ondownstream tasks, their deployment is still hindered by the vast model scaleand computational costs. Neural network pruning offers a solution for modelcompression by removing redundant parameters, but most existing methods rely oncomputing parameter gradients. However, obtaining the gradients iscomputationally prohibitive for LPMs, which necessitates the exploration ofalternative approaches. To this end, we propose a unified framework forefficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first designa PEFT-aware pruning criterion, which utilizes the values and gradients ofLow-Rank Adaption (LoRA), rather than the gradients of pre-trained parametersfor importance estimation. We then propose an iterative pruning procedure toremove redundant parameters while maximizing the advantages of PEFT. Thus, ourLoRAPrune delivers an accurate, compact model for efficient inference in ahighly cost-effective manner. Experimental results on various tasks demonstratethat our method achieves state-of-the-art results. For instance, in the VTAB-1kbenchmark, LoRAPrune utilizes only 0.76% of the trainable parameters andoutperforms magnitude and movement pruning methods by a significant margin,achieving a mean Top-1 accuracy that is 5.7% and 4.3% higher, respectively.Moreover, our approach achieves comparable performance to PEFT methods,highlighting its efficacy in delivering high-quality results while benefitingfrom the advantages of pruning.", "output": "Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The paper introduces PaintSeg, a new unsupervised method for segmentingobjects without any training. We propose an adversarial masked contrastivepainting (AMCP) process, which creates a contrast between the original imageand a painted image in which a masked area is painted using off-the-shelfgenerative models. During the painting process, inpainting and outpainting arealternated, with the former masking the foreground and filling in thebackground, and the latter masking the background while recovering the missingpart of the foreground object. Inpainting and outpainting, also referred to asI-step and O-step, allow our method to gradually advance the targetsegmentation mask toward the ground truth without supervision or training.PaintSeg can be configured to work with a variety of prompts, e.g. coarsemasks, boxes, scribbles, and points. Our experimental results demonstrate thatPaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, andpoint-prompt segmentation tasks, providing a training-free solution suitablefor unsupervised segmentation.", "output": "PaintSeg: Training-free Segmentation via Painting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision and Language (VL) models offer an effective method for aligningrepresentation spaces of images and text, leading to numerous applications suchas cross-modal retrieval, visual question answering, captioning, and more.However, the aligned image-text spaces learned by all the popular VL models arestill suffering from the so-called `object bias' - their representations behaveas `bags of nouns', mostly ignoring or downsizing the attributes, relations,and states of objects described/appearing in texts/images. Although some greatattempts at fixing these `compositional reasoning' issues were proposed in therecent literature, the problem is still far from being solved. In this paper,we uncover two factors limiting the VL models' compositional reasoningperformance. These two factors are properties of the paired VL dataset used forfinetuning and pre-training the VL model: (i) the caption quality, or in otherwords `image-alignment', of the texts; and (ii) the `density' of the captionsin the sense of mentioning all the details appearing on the image. We propose afine-tuning approach for automatically treating these factors leveraging astandard VL dataset (CC3M). Applied to CLIP, we demonstrate its significantcompositional reasoning performance increase of up to $sim27%$ over the basemodel, up to $sim20%$ over the strongest baseline, and by $6.7%$ on average.", "output": "Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in text-to-image diffusion models have achieved remarkablesuccess in generating high-quality, realistic images from given text prompts.However, previous methods fail to perform accurate modality alignment betweentext concepts and generated images due to the lack of fine-level semanticguidance that successfully diagnoses the modality discrepancy. In this paper,we propose FineRewards to improve the alignment between text and images intext-to-image diffusion models by introducing two new fine-grained semanticrewards: the caption reward and the Semantic Segment Anything (SAM) reward.From the global semantic view, the caption reward generates a correspondingdetailed caption that depicts all important contents in the synthetic image viaa BLIP-2 model and then calculates the reward score by measuring the similaritybetween the generated caption and the given prompt. From the local semanticview, the SAM reward segments the generated images into local parts withcategory labels, and scores the segmented parts by measuring the likelihood ofeach category appearing in the prompted scene via a large language model, i.e.,Vicuna-7B. Additionally, we adopt an assemble reward-ranked learning strategyto enable the integration of multiple reward functions to jointly guide themodel training. Adapting results of text-to-image models on the MS-COCObenchmark show that the proposed semantic reward outperforms other baselinereward functions with a considerable margin on both visual quality and semanticsimilarity with the input prompt. Moreover, by adopting the assemblereward-ranked learning strategy, we further demonstrate that model performanceis further improved when adapting under the unifying of the proposed semanticreward with the current image rewards.", "output": "Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Geometry and color information provided by the point clouds are both crucialfor 3D scene understanding. Two pieces of information characterize thedifferent aspects of point clouds, but existing methods lack an elaboratedesign for the discrimination and relevance. Hence we explore a 3Dself-supervised paradigm that can better utilize the relations of point cloudinformation. Specifically, we propose a universal 3D scene pre-trainingframework via Geometry-Color Contrast (Point-GCC), which aligns geometry andcolor information using a Siamese network. To take care of actual applicationtasks, we design (i) hierarchical supervision with point-level contrast andreconstruct and object-level contrast based on the novel deep clustering moduleto close the gap between pre-training and downstream tasks; (ii)architecture-agnostic backbone to adapt for various downstream models.Benefiting from the object-level representation associated with downstreamtasks, Point-GCC can directly evaluate model performance and the resultdemonstrates the effectiveness of our methods. Transfer learning results on awide range of tasks also show consistent improvements across all datasets.e.g., new state-of-the-art object detection results on SUN RGB-D and S3DISdatasets. Codes will be released at ", "output": "Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative diffusion models have recently emerged as a leading approach forgenerating high-dimensional data. In this paper, we show that the dynamics ofthese models exhibit a spontaneous symmetry breaking that divides thegenerative dynamics into two distinct phases: 1) A linear steady-state dynamicsaround a central fixed-point and 2) an attractor dynamics directed towards thedata manifold. These two \"phases\" are separated by the change in stability ofthe central fixed-point, with the resulting window of instability beingresponsible for the diversity of the generated samples. Using both theoreticaland empirical evidence, we show that an accurate simulation of the earlydynamics does not significantly contribute to the final generation, since earlyfluctuations are reverted to the central fixed point. To leverage this insight,we propose a Gaussian late initialization scheme, which significantly improvesmodel performance, achieving up to 3x FID improvements on fast samplers, whilealso increasing sample diversity (e.g., racial composition of generated CelebAimages). Our work offers a new way to understand the generative dynamics ofdiffusion models that has the potential to bring about higher performance andless biased fast-samplers.", "output": "Spontaneous symmetry breaking in generative diffusion models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gait recognition is an emerging biological recognition technology thatidentifies and verifies individuals based on their walking patterns. However,many current methods are limited in their use of temporal information. In orderto fully harness the potential of gait recognition, it is crucial to considertemporal features at various granularities and spans. Hence, in this paper, wepropose a novel framework named GaitGS, which aggregates temporal features inthe granularity dimension and span dimension simultaneously. Specifically,Multi-Granularity Feature Extractor (MGFE) is proposed to focus on capturingthe micro-motion and macro-motion information at the frame level and unit levelrespectively. Moreover, we present Multi-Span Feature Learning (MSFL) module togenerate global and local temporal representations. On three popular gaitdatasets, extensive experiments demonstrate the state-of-the-art performance ofour method. Our method achieves the Rank-1 accuracies of 92.9% (+0.5%), 52.0%(+1.4%), and 97.5% (+0.8%) on CASIA-B, GREW, and OU-MVLP respectively. Thesource code will be released soon.", "output": "GaitGS: Temporal Feature Learning in Granularity and Span Dimension for Gait Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The spiking neural network (SNN), as a promising brain-inspired computationalmodel with binary spike information transmission mechanism, richspatially-temporal dynamics, and event-driven characteristics, has receivedextensive attention. However, its intricately discontinuous spike mechanismbrings difficulty to the optimization of the deep SNN. Since the surrogategradient method can greatly mitigate the optimization difficulty and showsgreat potential in directly training deep SNNs, a variety of directlearning-based deep SNN works have been proposed and achieved satisfyingprogress in recent years. In this paper, we present a comprehensive survey ofthese direct learning-based deep SNN works, mainly categorized into accuracyimprovement methods, efficiency improvement methods, and temporal dynamicsutilization methods. In addition, we also divide these categorizations intofiner granularities further to better organize and introduce them. Finally, thechallenges and trends that may be faced in future research are prospected.", "output": "Direct Learning-Based Deep Spiking Neural Networks: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image-language learning has made unprecedented progress in visualunderstanding. These developments have come at high costs, as contemporaryvision-language models require large model scales and amounts of data. We herepropose a much easier recipe for image-language learning, which produceseffective models, outperforming bigger and more expensive ones, often trainedon orders of magnitude larger datasets. Our key finding is the joint learningof a compact vision and language representation, which adaptively anditeratively fuses the multi-modal features. This results in a more effectiveimage-language learning, greatly lowering the FLOPs by combining and reducingthe number of tokens for both text and images, e.g. a 33% reduction in FLOPsis achieved, compared to baseline fusion techniques used by popularimage-language models, while improving performance. This also allows the modelto scale without a large increase in FLOPs or memory. In addition, we proposeadaptive pre-training data sampling which improves the data efficiency. Theproposed approach achieves competitive performance compared to much largermodels, and does so with significantly less data and FLOPs. With only 40Mtraining examples and with 39 GFLOPs our lightweight model outperforms manytimes larger state-of-the-art models of 2-20x more FLOPs and using biggerdatasets some of which with close to 1B training examples.", "output": "Joint Adaptive Representations for Image-Language Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Watermarking the outputs of generative models is a crucial technique fortracing copyright and preventing potential harm from AI-generated content. Inthis paper, we introduce a novel technique called Tree-Ring Watermarking thatrobustly fingerprints diffusion model outputs. Unlike existing methods thatperform post-hoc modifications to images after sampling, Tree-Ring Watermarkingsubtly influences the entire sampling process, resulting in a model fingerprintthat is invisible to humans. The watermark embeds a pattern into the initialnoise vector used for sampling. These patterns are structured in Fourier spaceso that they are invariant to convolutions, crops, dilations, flips, androtations. After image generation, the watermark signal is detected byinverting the diffusion process to retrieve the noise vector, which is thenchecked for the embedded signal. We demonstrate that this technique can beeasily applied to arbitrary diffusion models, including text-conditioned StableDiffusion, as a plug-in with negligible loss in FID. Our watermark issemantically hidden in the image space and is far more robust than watermarkingalternatives that are currently deployed. Code is available at", "output": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Perceptual metrics, like the Fr'echet Inception Distance (FID), are widelyused to assess the similarity between synthetically generated and ground truth(real) images. The key idea behind these metrics is to compute errors in a deepfeature space that captures perceptually and semantically rich image features.Despite their popularity, the effect that different deep features and theirdesign choices have on a perceptual metric has not been well studied. In thiswork, we perform a causal analysis linking differences in semantic attributesand distortions between face image distributions to Fr'echet distances (FD)using several popular deep feature spaces. A key component of our analysis isthe creation of synthetic counterfactual faces using deep face generators. Ourexperiments show that the FD is heavily influenced by its feature space'straining dataset and objective function. For example, FD using featuresextracted from ImageNet-trained models heavily emphasize hats over regions likethe eyes and mouth. Moreover, FD using features from a face gender classifieremphasize hair length more than distances in an identity (recognition) featurespace. Finally, we evaluate several popular face generation models acrossfeature spaces and find that StyleGAN2 consistently ranks higher than otherface generators, except with respect to identity (recognition) features. Thissuggests the need for considering multiple feature spaces when evaluatinggenerative models and using feature spaces that are tuned to nuances of thedomain of interest.", "output": "F?D: On understanding the role of deep feature spaces on face generation evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper examines the problems of severe image-text misalignment and highredundancy in the widely-used large-scale Vision-Language Pre-Training (VLP)datasets. To address these issues, we propose an efficient and straightforwardVision-Language learning algorithm called TL;DR, which aims to compress theexisting large VLP data into a small, high-quality set. Our approach consistsof two major steps. First, a codebook-based encoder-decoder captioner isdeveloped to select representative samples. Second, a new caption is generatedto complement the original captions for selected samples, mitigating thetext-image misalignment problem while maintaining uniqueness. As the result,TL;DR enables us to reduce the large dataset into a small set of high-qualitydata, which can serve as an alternative pre-training dataset. This algorithmsignificantly speeds up the time-consuming pretraining process. Specifically,TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reducewell-cleaned CC3M dataset from 2.82M to 0.67M ($sim$24%) and noisy YFCC15Mfrom 15M to 2.5M ($sim$16.7%). Extensive experiments with three popular VLPmodels over seven downstream tasks show that VLP model trained on thecompressed dataset provided by TL;DR can perform similar or even better resultscompared with training on the full-scale dataset. The code will be madeavailable at url{", "output": "Too Large; Data Reduction for Vision-Language Pre-Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We explore the use of graph networks to deal with irregular-geometrydetectors in the context of particle reconstruction. Thanks to theirrepresentation-learning capabilities, graph networks can exploit the fulldetector granularity, while natively managing the event sparsity andarbitrarily complex detector geometries. We introduce two distance-weightedgraph network architectures, dubbed GarNet and GravNet layers, and apply themto a typical particle reconstruction task. The performance of the newarchitectures is evaluated on a data set of simulated particle interactions ona toy model of a highly granular calorimeter, loosely inspired by the endcapcalorimeter to be installed in the CMS detector for the High-Luminosity LHCphase. We study the clustering of energy depositions, which is the basis forcalorimetric particle reconstruction, and provide a quantitative comparison toalternative approaches. The proposed algorithms provide an interestingalternative to existing methods, offering equally performing or lessresource-demanding solutions with less underlying assumptions on the detectorgeometry and, consequently, the possibility to generalize to other detectors.", "output": "Learning representations of irregular particle-detector geometry with distance-weighted graph networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Aortic stenosis (AS) is a degenerative valve condition that causessubstantial morbidity and mortality. This condition is under-diagnosed andunder-treated. In clinical practice, AS is diagnosed with expert review oftransthoracic echocardiography, which produces dozens of ultrasound images ofthe heart. Only some of these views show the aortic valve. To automatescreening for AS, deep networks must learn to mimic a human expert's ability toidentify views of the aortic valve then aggregate across these relevant imagesto produce a study-level diagnosis. We find previous approaches to AS detectionyield insufficient accuracy due to relying on inflexible averages acrossimages. We further find that off-the-shelf attention-based multiple instancelearning (MIL) performs poorly. We contribute a new end-to-end MIL approachwith two key methodological innovations. First, a supervised attentiontechnique guides the learned attention mechanism to favor relevant views.Second, a novel self-supervised pretraining strategy applies contrastivelearning on the representation of the whole study instead of individual imagesas commonly done in prior literature. Experiments on an open-access dataset andan external validation set show that our approach yields higher accuracy whilereducing model size.", "output": "Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One prevalent property we find empirically in real-world graph anomalydetection (GAD) datasets is a one-class homophily, i.e., normal nodes tend tohave strong connection/affinity with each other, while the homophily inabnormal nodes is significantly weaker than normal nodes. However, thisanomaly-discriminative property is ignored by existing GAD methods that aretypically built using a conventional anomaly detection objective, such as datareconstruction. In this work, we explore this property to introduce a novelunsupervised anomaly scoring measure for GAD -- local node affinity -- thatassigns a larger anomaly score to nodes that are less affiliated with theirneighbors, with the affinity defined as similarity on nodeattributes/representations. We further propose Truncated Affinity Maximization(TAM) that learns tailored node representations for our anomaly measure bymaximizing the local affinity of nodes to their neighbors. Optimizing on theoriginal graph structure can be biased by non-homophily edges (i.e., edgesconnecting normal and abnormal nodes). Thus, TAM is instead optimized ontruncated graphs where non-homophily edges are removed iteratively to mitigatethis bias. The learned representations result in significantly stronger localaffinity for normal nodes than abnormal nodes. Extensive empirical results onsix real-world GAD datasets show that TAM substantially outperforms sevencompeting models, achieving over 10% increase in AUROC/AUPRC compared to thebest contenders on challenging datasets. Our code will be made available at //github.com/mala-lab/TAM-master/.", "output": "Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Brazilian judiciary has a large workload, resulting in a long time tofinish legal proceedings. Brazilian National Council of Justice has establishedin Resolution 469/2022 formal guidance for document and process digitalizationopening up the possibility of using automatic techniques to help with everydaytasks in the legal field, particularly in a large number of texts yielded onthe routine of law procedures. Notably, Artificial Intelligence (AI) techniquesallow for processing and extracting useful information from textual data,potentially speeding up the process. However, datasets from the legal domainrequired by several AI techniques are scarce and difficult to obtain as theyneed labels from experts. To address this challenge, this article contributeswith four datasets from the legal domain, two with documents and metadata butunlabeled, and another two labeled with a heuristic aiming at its use intextual semantic similarity tasks. Also, to evaluate the effectiveness of theproposed heuristic label process, this article presents a small ground truthdataset generated from domain expert annotations. The analysis of ground truthlabels highlights that semantic analysis of domain text can be challenging evenfor domain experts. Also, the comparison between ground truth and heuristiclabels shows that heuristic labels are useful.", "output": "Datasets for Portuguese Legal Semantic Textual Similarity: Comparing weak supervision and an annotation process approaches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformers are central to recent successes in natural language processingand computer vision. Transformers have a mostly uniform backbone where layersalternate between feed-forward and self-attention in order to build a deepnetwork. Here we investigate this design choice and find that more complexblocks that have different permutations of layer primitives can be moreefficient. Using this insight, we develop a complex block, named Brainformer,that consists of a diverse sets of layers such as sparsely gated feed-forwardlayers, dense feed-forward layers, attention layers, and various forms of layernormalization and activation functions. Brainformer consistently outperformsthe state-of-the-art dense and sparse Transformers, in terms of both qualityand efficiency. A Brainformer model with 8 billion activated parameters pertoken demonstrates 2x faster training convergence and 5x faster step timecompared to its GLaM counterpart. In downstream task evaluation, Brainformeralso demonstrates a 3% higher SuperGLUE score with fine-tuning compared to GLaMwith a similar number of activated parameters. Finally, Brainformer largelyoutperforms a Primer dense model derived with NAS with similar computation pertoken on fewshot evaluations.", "output": "Brainformers: Trading Simplicity for Efficiency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There are roughly three stages in real industrial recommendation systems,candidates generation (retrieval), ranking and reranking. Individual-leveldiversity and system-level diversity are both important for industrialrecommender systems. The former focus on each single user's experience, whilethe latter focus on the difference among users. Graph-based retrievalstrategies are inevitably hijacked by heavy users and popular items, leading tothe convergence of candidates for users and the lack of system-level diversity.Meanwhile, in the reranking phase, Determinantal Point Process (DPP) isdeployed to increase individual-level diverisity. Heavily relying on thesemantic information of items, DPP suffers from clickbait and inaccurateattributes. Besides, most studies only focus on one of the two levels ofdiversity, and ignore the mutual influence among different stages in realrecommender systems. We argue that individual-level diversity and system-leveldiversity should be viewed as an integrated problem, and we provide anefficient and deployable solution for web-scale recommenders. Generally, wepropose to employ the retrieval graph information in diversity-based reranking,by which to weaken the hidden similarity of items exposed to users, andconsequently gain more graph explorations to improve the system-leveldiveristy. Besides, we argue that users' propensity for diversity changes overtime in content feed recommendation. Therefore, with the explored graph, wealso propose to capture the user's real-time personalized propensity to thediversity. We implement and deploy the combined system in WeChat App's TopStories used by hundreds of millions of users. Offline simulations and onlineA/B tests show our solution can effectively improve both user engagement andsystem revenue.", "output": "Graph Exploration Matters: Improving both individual-level and system-level diversity in WeChat Feed Recommender."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Simplicial map neural networks (SMNNs) are topology-based neural networkswith interesting properties such as universal approximation capability androbustness to adversarial examples under appropriate conditions. However, SMNNspresent some bottlenecks for their possible application in high dimensions.First, no SMNN training process has been defined so far. Second, SMNNs requirethe construction of a convex polytope surrounding the input dataset. In thispaper, we propose a SMNN training procedure based on a support subset of thegiven dataset and a method based on projection to a hypersphere as areplacement for the convex polytope construction. In addition, theexplainability capacity of SMNNs is also introduced for the first time in thispaper.", "output": "Explainability in Simplicial Map Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating the number of clusters and underlying cluster structure in adataset is a crucial task. Real-world data are often unlabeled, complex andhigh-dimensional, which makes it difficult for traditional clusteringalgorithms to perform well. In recent years, a matrix reordering basedalgorithm, called \"visual assessment of tendency\" (VAT), and its variants haveattracted many researchers from various domains to estimate the number ofclusters and inherent cluster structure present in the data. However, thesealgorithms fail when applied to high-dimensional data due to the curse ofdimensionality, as they rely heavily on the notions of closeness and farnessbetween data points. To address this issue, we propose a deep-learning basedframework for cluster structure assessment in complex, image datasets. First,our framework generates representative embeddings for complex data using aself-supervised deep neural network, and then, these low-dimensional embeddingsare fed to VAT/iVAT algorithms to estimate the underlying cluster structure. Inthis process, we ensured not to use any prior knowledge for the number ofclusters (i.e k). We present our results on four real-life image datasets, andour findings indicate that our framework outperforms state-of-the-art VAT/iVATalgorithms in terms of clustering accuracy and normalized mutual information(NMI).", "output": "A Self-Supervised Approach for Cluster Assessment of High-Dimensional Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the era of big data, there has been a surge in the availability of datacontaining rich spatial and temporal information, offering valuable insightsinto dynamic systems and processes for applications such as weatherforecasting, natural disaster management, intelligent transport systems, andprecision agriculture. Graph neural networks (GNNs) have emerged as a powerfultool for modeling and understanding data with dependencies to each other suchas spatial and temporal dependencies. There is a large amount of existing workthat focuses on addressing the complex spatial and temporal dependencies inspatiotemporal data using GNNs. However, the strong interdisciplinary nature ofspatiotemporal data has created numerous GNNs variants specifically designedfor distinct application domains. Although the techniques are generallyapplicable across various domains, cross-referencing these methods remainsessential yet challenging due to the absence of a comprehensive literaturereview on GNNs for spatiotemporal data. This article aims to provide asystematic and comprehensive overview of the technologies and applications ofGNNs in the spatiotemporal domain. First, the ways of constructing graphs fromspatiotemporal data are summarized to help domain experts understand how togenerate graphs from various types of spatiotemporal data. Then, a systematiccategorization and summary of existing spatiotemporal GNNs are presented toenable domain experts to identify suitable techniques and to support modeldevelopers in advancing their research. Moreover, a comprehensive overview ofsignificant applications in the spatiotemporal domain is offered to introduce abroader range of applications to model developers and domain experts, assistingthem in exploring potential research topics and enhancing the impact of theirwork. Finally, open challenges and future directions are discussed.", "output": "Graph Neural Network for spatiotemporal data: methods and applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "According to the World Health Organization (WHO), cancer is the secondleading cause of death globally. Scientific research on different types ofcancers grows at an ever-increasing rate, publishing large volumes of researcharticles every year. The insight information and the knowledge of the drug,diagnostics, risk, symptoms, treatments, etc., related to genes are significantfactors that help explore and advance the cancer research progression. Manualscreening of such a large volume of articles is very laborious andtime-consuming to formulate any hypothesis. The study uses the two mostnon-trivial NLP, Natural Language Processing functions, Entity Recognition, andtext classification to discover knowledge from biomedical literature. NamedEntity Recognition (NER) recognizes and extracts the predefined entitiesrelated to cancer from unstructured text with the support of a user-friendlyinterface and built-in dictionaries. Text classification helps to explore theinsights into the text and simplifies data categorization, querying, andarticle screening. Machine learning classifiers are also used to build theclassification model and Structured Query Languages (SQL) is used to identifythe hidden relations that may lead to significant predictions.", "output": "Machine Learning Approach for Cancer Entities Association and Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While transformer-based pre-trained language models (PLMs) have dominated anumber of NLP applications, these models are heavy to deploy and expensive touse. Therefore, effectively compressing large-scale PLMs becomes anincreasingly important problem. Quantization, which represents high-precisiontensors with low-bit fix-point format, is a viable solution. However, mostexisting quantization methods are task-specific, requiring customized trainingand quantization with a large number of trainable parameters on each individualtask. Inspired by the observation that the over-parameterization nature of PLMsmakes it possible to freeze most of the parameters during the fine-tuningstage, in this work, we propose a novel ``quantize before fine-tuning''framework, PreQuant, that differs from both quantization-aware training andpost-training quantization. PreQuant is compatible with various quantizationstrategies, with outlier-aware parameter-efficient fine-tuning incorporated tocorrect the induced quantization error. We demonstrate the effectiveness ofPreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide anempirical investigation into the workflow of PreQuant, which sheds light on itsefficacy.", "output": "PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Label errors have been found to be prevalent in popular text, vision, andaudio datasets, which heavily influence the safe development and evaluation ofmachine learning algorithms. Despite increasing efforts towards improving thequality of generic data types, such as images and texts, the problem ofmislabel detection in graph data remains underexplored. To bridge the gap, weexplore mislabelling issues in popular real-world graph datasets and proposeGraphCleaner, a post-hoc method to detect and correct these mislabelled nodesin graph datasets. GraphCleaner combines the novel ideas of 1) SyntheticMislabel Dataset Generation, which seeks to generate realistic mislabels; and2) Neighborhood-Aware Mislabel Detection, where neighborhood dependency isexploited in both labels and base classifier predictions. Empirical evaluationson 6 datasets and 6 experimental settings demonstrate that GraphCleaneroutperforms the closest baseline, with an average improvement of 0.14 in F1score, and 0.16 in MCC. On real-data case studies, GraphCleaner detects realand previously unknown mislabels in popular graph benchmarks: PubMed, Cora,CiteSeer and OGB-arxiv; we find that at least 6.91% of PubMed data ismislabelled or ambiguous, and simply removing these mislabelled data can boostevaluation performance from 86.71% to 89.11%.", "output": "GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Discrete choice models (DCM) are widely employed in travel demand analysis asa powerful theoretical econometric framework for understanding and predictingchoice behaviors. DCMs are formed as random utility models (RUM), with theirkey advantage of interpretability. However, a core requirement for theestimation of these models is a priori specification of the associated utilityfunctions, making them sensitive to modelers' subjective beliefs. Recently,machine learning (ML) approaches have emerged as a promising avenue forlearning unobserved non-linear relationships in DCMs. However, ML models areconsidered \"black box\" and may not correspond with expected relationships. Thispaper proposes a framework that expands the potential of data-driven approachesfor DCM by supporting the development of interpretable models that incorporatedomain knowledge and prior beliefs through constraints. The proposed frameworkincludes pseudo data samples that represent required relationships and a lossfunction that measures their fulfillment, along with observed data, for modeltraining. The developed framework aims to improve model interpretability bycombining ML's specification flexibility with econometrics and interpretablebehavioral analysis. A case study demonstrates the potential of this frameworkfor discrete choice analysis.", "output": "Incorporating Domain Knowledge in Deep Neural Networks for Discrete Choice Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have shown remarkable capabilities across abroad range of tasks involving question answering and the generation ofcoherent text and code. Comprehensively understanding the strengths andweaknesses of LLMs is beneficial for safety, downstream applications andimproving performance. In this work, we investigate the degree to which GPT-4has acquired factual geographic knowledge and is capable of using thisknowledge for interpretative reasoning, which is especially important forapplications that involve geographic data, such as geospatial analysis, supplychain management, and disaster response. To this end, we design and conduct aseries of diverse experiments, starting from factual tasks such as location,distance and elevation estimation to more complex questions such as generatingcountry outlines and travel networks, route finding under constraints andsupply chain analysis. We provide a broad characterisation of what GPT-4(without plugins or Internet access) knows about the world, highlighting bothpotentially surprising capabilities but also limitations.", "output": "GPT4GEO: How a Language Model Sees the World's Geography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Currently, many researchers and analysts are working toward medical diagnosisenhancement for various diseases. Heart disease is one of the common diseasesthat can be considered a significant cause of mortality worldwide. Earlydetection of heart disease significantly helps in reducing the risk of heartfailure. Consequently, the Centers for Disease Control and Prevention (CDC)conducts a health-related telephone survey yearly from over 400,000participants. However, several concerns arise regarding the reliability of thedata in predicting heart disease and whether all of the survey questions arestrongly related. This study aims to utilize several machine learningtechniques, such as support vector machines and logistic regression, toinvestigate the accuracy of the CDC's heart disease survey in the UnitedStates. Furthermore, we use various feature selection methods to identify themost relevant subset of questions that can be utilized to forecast heartconditions. To reach a robust conclusion, we perform stability analysis byrandomly sampling the data 300 times. The experimental results show that thesurvey data can be useful up to 80% in terms of predicting heart disease, whichsignificantly improves the diagnostic process before bloodwork and tests. Inaddition, the amount of time spent conducting the survey can be reduced by 77%while maintaining the same level of performance.", "output": "Predicting Heart Disease and Reducing Survey Time Using Machine Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extracting patient information from unstructured text is a critical task inhealth decision-support and clinical research. Large language models (LLMs)have shown the potential to accelerate clinical curation via few-shotin-context learning, in contrast to supervised learning which requires muchmore costly human annotations. However, despite drastic advances in modern LLMssuch as GPT-4, they still struggle with issues regarding accuracy andinterpretability, especially in mission-critical domains such as health. Here,we explore a general mitigation framework using self-verification, whichleverages the LLM to provide provenance for its own extraction and check itsown outputs. This is made possible by the asymmetry between verification andgeneration, where the latter is often much easier than the former. Experimentalresults show that our method consistently improves accuracy for various LLMs instandard clinical information extraction tasks. Additionally, self-verificationyields interpretations in the form of a short text span corresponding to eachoutput, which makes it very efficient for human experts to audit the results,paving the way towards trustworthy extraction of clinical information inresource-constrained scenarios. To facilitate future research in thisdirection, we release our code and prompts.", "output": "Self-Verification Improves Few-Shot Clinical Information Extraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While traditional distributionally robust optimization (DRO) aims to minimizethe maximal risk over a set of distributions, Agarwal and Zhang (2022) recentlyproposed a variant that replaces risk with excess risk. Compared to DRO, thenew formulation -- minimax excess risk optimization (MERO) has the advantage ofsuppressing the effect of heterogeneous noise in different distributions.However, the choice of excess risk leads to a very challenging minimaxoptimization problem, and currently there exists only an inefficient algorithmfor empirical MERO. In this paper, we develop efficient stochasticapproximation approaches which directly target MERO. Specifically, we leveragetechniques from stochastic convex optimization to estimate the minimal risk ofevery distribution, and solve MERO as a stochastic convex-concave optimization(SCCO) problem with biased gradients. The presence of bias makes existingtheoretical guarantees of SCCO inapplicable, and fortunately, we demonstratethat the bias, caused by the estimation error of the minimal risk, isunder-control. Thus, MERO can still be optimized with a nearly optimalconvergence rate. Moreover, we investigate a practical scenario where thequantity of samples drawn from each distribution may differ, and propose astochastic approach that delivers distribution-dependent convergence rates.", "output": "Efficient Stochastic Approximation of Minimax Excess Risk Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An important problem in reinforcement learning is designing agents that learnto solve tasks safely in an environment. A common solution is for a humanexpert to define either a penalty in the reward function or a cost to beminimised when reaching unsafe states. However, this is non-trivial, since toosmall a penalty may lead to agents that reach unsafe states, while too large apenalty increases the time to convergence. Additionally, the difficulty indesigning reward or cost functions can increase with the complexity of theproblem. Hence, for a given environment with a given set of unsafe states, weare interested in finding the upper bound of rewards at unsafe states whoseoptimal policies minimise the probability of reaching those unsafe states,irrespective of task rewards. We refer to this exact upper bound as the \"Minmaxpenalty\", and show that it can be obtained by taking into account both thecontrollability and diameter of an environment. We provide a simple practicalmodel-free algorithm for an agent to learn this Minmax penalty while learningthe task policy, and demonstrate that using it leads to agents that learn safepolicies in high-dimensional continuous control environments.", "output": "ROSARL: Reward-Only Safe Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Twitter as one of the most popular social networks, offers a means forcommunication and online discourse, which unfortunately has been the target ofbots and fake accounts, leading to the manipulation and spreading of falseinformation. Towards this end, we gather a challenging, multilingual dataset ofsocial discourse on Twitter, originating from 9M users regarding the recentRusso-Ukrainian war, in order to detect the bot accounts and the conversationinvolving them. We collect the ground truth for our dataset through the TwitterAPI suspended accounts collection, containing approximately 343K of botaccounts and 8M of normal users. Additionally, we use a dataset provided byBotometer-V3 with 1,777 Varol, 483 German accounts, and 1,321 US accounts.Besides the publicly available datasets, we also manage to collect 2independent datasets around popular discussion topics of the 2022 energy crisisand the 2022 conspiracy discussions. Both of the datasets were labeledaccording to the Twitter suspension mechanism. We build a novel ML model forbot detection using the state-of-the-art XGBoost model. We combine the modelwith a high volume of labeled tweets according to the Twitter suspensionmechanism ground truth. This requires a limited set of profile featuresallowing labeling of the dataset in different time periods from the collection,as it is independent of the Twitter API. In comparison with Botometer ourmethodology achieves an average 11% higher ROC-AUC score over two real-casescenario datasets.", "output": "BotArtist: Twitter bot detection Machine Learning model based on Twitter suspension."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a Federated Learning Code Smell Detection (FedCSD)approach that allows organizations to collaboratively train federated ML modelswhile preserving their data privacy. These assertions have been supported bythree experiments that have significantly leveraged three manually validateddatasets aimed at detecting and examining different code smell scenarios. Inexperiment 1, which was concerned with a centralized training experiment,dataset two achieved the lowest accuracy (92.30%) with fewer smells, whiledatasets one and three achieved the highest accuracy with a slight difference(98.90% and 99.5%, respectively). This was followed by experiment 2, which wasconcerned with cross-evaluation, where each ML model was trained using onedataset, which was then evaluated over the other two datasets. Results fromthis experiment show a significant drop in the model's accuracy (lowestaccuracy: 63.80%) where fewer smells exist in the training dataset, which hasa noticeable reflection (technical debt) on the model's performance. Finally,the last and third experiments evaluate our approach by splitting the datasetinto 10 companies. The ML model was trained on the company's site, then allmodel-updated weights were transferred to the server. Ultimately, an accuracyof 98.34% was achieved by the global model that has been trained using 10companies for 100 training rounds. The results reveal a slight difference inthe global model's accuracy compared to the highest accuracy of the centralizedmodel, which can be ignored in favour of the global model's comprehensiveknowledge, lower training cost, preservation of data privacy, and avoidance ofthe technical debt problem.", "output": "FedCSD: A Federated Learning Based Approach for Code-Smell Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A key component of automated algorithm selection and configuration, which inmost cases are performed using supervised machine learning (ML) methods is agood-performing predictive model. The predictive model uses the featurerepresentation of a set of problem instances as input data and predicts thealgorithm performance achieved on them. Common machine learning models struggleto make predictions for instances with feature representations not covered bythe training data, resulting in poor generalization to unseen problems. In thisstudy, we propose a workflow to estimate the generalizability of a predictivemodel for algorithm performance, trained on one benchmark suite to another. Theworkflow has been tested by training predictive models across benchmark suitesand the results show that generalizability patterns in the landscape featurespace are reflected in the performance space.", "output": "Assessing the Generalizability of a Performance Predictive Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The identification and discovery of drug-target Interaction (DTI) is animportant step in the field of Drug research and development, which can helpscientists discover new drugs and accelerate the development process.KnowledgeGraph and the related knowledge graph Embedding (KGE) model developrapidly and show good performance in the field of drug discovery in recentyears. In the task of drug target identification, the lack of authenticity andaccuracy of the model will lead to the increase of misjudgment rate and the lowefficiency of drug development. To solve the above problems, this study focusedon the problem of drug target link prediction with knowledge mapping as thecore technology, and adopted the confidence measurement method based on causalintervention to measure the triplet score, so as to improve the accuracy ofdrug target interaction prediction model. By comparing with the traditionalSoftmax and Sigmod confidence measurement methods on different KGE models, theresults show that the confidence measurement method based on causalintervention can effectively improve the accuracy of DTI link prediction,especially for high-precision models. The predicted results are more conduciveto guiding the design and development of followup experiments of drugdevelopment, so as to improve the efficiency of drug development.", "output": "Research And Implementation Of Drug Target Interaction Confidence Measurement Method Based On Causal Intervention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial neural networks are prone to being fooled by carefully perturbedinputs which cause an egregious misclassification. These textit{adversarial}attacks have been the focus of extensive research. Likewise, there has been anabundance of research in ways to detect and defend against them. We introduce anovel approach of detection and interpretation of adversarial attacks from agraph perspective. For an image, benign or adversarial, we study how a neuralnetwork's architecture can induce an associated graph. We study this graph andintroduce specific measures used to predict and interpret adversarial attacks.We show that graphs-based approaches help to investigate the inner workings ofadversarial attacks.", "output": "Graph-based methods coupled with specific distributional distances for adversarial attack detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Shortcut learning, or `Clever Hans effect` refers to situations where alearning agent (e.g., deep neural networks) learns spurious correlationspresent in data, resulting in biased models. We focus on finding shortcuts indeep learning based spoofing countermeasures (CMs) that predict whether a givenutterance is spoofed or not. While prior work has addressed specific dataartifacts, such as silence, no general normative framework has been exploredfor analyzing shortcut learning in CMs. In this study, we propose a genericapproach to identifying shortcuts by introducing systematic interventions onthe training and test sides, including the boundary cases of `near-perfect` and`worse than coin flip` (label flip). By using three different models, rangingfrom classic to state-of-the-art, we demonstrate the presence of shortcutlearning in five simulated conditions. We analyze the results using aregression model to understand how biases affect the class-conditional scorestatistics.", "output": "How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Is the lottery ticket phenomenon an idiosyncrasy of gradient-based trainingor does it generalize to evolutionary optimization? In this paper we establishthe existence of highly sparse trainable initializations for evolutionstrategies (ES) and characterize qualitative differences compared to gradientdescent (GD)-based sparse training. We introduce a novel signal-to-noiseiterative pruning procedure, which incorporates loss curvature information intothe network pruning step. This can enable the discovery of even sparsertrainable network initializations when using black-box evolution as compared toGD-based optimization. Furthermore, we find that these initializations encodean inductive bias, which transfers across different ES, related tasks and evento GD-based training. Finally, we compare the local optima resulting from thedifferent optimization paradigms and sparsity levels. In contrast to GD, ESexplore diverse and flat local optima and do not preserve linear modeconnectivity across sparsity levels and independent runs. The results highlightqualitative differences between evolution and gradient-based learning dynamics,which can be uncovered by the study of iterative pruning procedures.", "output": "Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum machine learning is often highlighted as one of the most promisinguses for a quantum computer to solve practical problems. However, a majorobstacle to the widespread use of quantum machine learning models in practiceis that these models, even once trained, still require access to a quantumcomputer in order to be evaluated on new data. To solve this issue, we suggestthat following the training phase of a quantum model, a quantum computer couldbe used to generate what we call a classical shadow of this model, i.e., aclassically computable approximation of the learned function. While recentworks already explore this idea and suggest approaches to construct such shadowmodels, they also raise the possibility that a completely classical model couldbe trained instead, thus circumventing the need for a quantum computer in thefirst place. In this work, we take a novel approach to define shadow modelsbased on the frameworks of quantum linear models and classical shadowtomography. This approach allows us to show that there exist shadow modelswhich can solve certain learning tasks that are intractable for fully classicalmodels, based on widely-believed cryptography assumptions. We also discuss the(un)likeliness that all quantum models could be shadowfiable, based on commonassumptions in complexity theory.", "output": "Shadows of quantum machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Whenever a binary classifier is used to provide decision support, ittypically provides both a label prediction and a confidence value. Then, thedecision maker is supposed to use the confidence value to calibrate how much totrust the prediction. In this context, it has been often argued that theconfidence value should correspond to a well calibrated estimate of theprobability that the predicted label matches the ground truth label. However,multiple lines of empirical evidence suggest that decision makers havedifficulties at developing a good sense on when to trust a prediction usingthese confidence values. In this paper, our goal is first to understand why andthen investigate how to construct more useful confidence values. We first arguethat, for a broad class of utility functions, there exist data distributionsfor which a rational decision maker is, in general, unlikely to discover theoptimal decision policy using the above confidence values -- an optimaldecision maker would need to sometimes place more (less) trust on predictionswith lower (higher) confidence values. However, we then show that, if theconfidence values satisfy a natural alignment property with respect to thedecision maker's confidence on her own predictions, there always exists anoptimal decision policy under which the level of trust the decision maker wouldneed to place on predictions is monotone on the confidence values, facilitatingits discoverability. Further, we show that multicalibration with respect to thedecision maker's confidence on her own predictions is a sufficient conditionfor alignment. Experiments on four different AI-assisted decision making taskswhere a classifier provides decision support to real human experts validate ourtheoretical results and suggest that alignment may lead to better decisions.", "output": "Human-Aligned Calibration for AI-Assisted Decision Making."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the task of \"Social Rearrangement\", consisting of cooperativeeveryday tasks like setting up the dinner table, tidying a house or unpackinggroceries in a simulated multi-agent environment. In Social Rearrangement, tworobots coordinate to complete a long-horizon task, using onboard sensing andegocentric observations, and no privileged information about the environment.We study zero-shot coordination (ZSC) in this task, where an agent collaborateswith a new partner, emulating a scenario where a robot collaborates with a newhuman partner. Prior ZSC approaches struggle to generalize in our complex andvisually rich setting, and on further analysis, we find that they fail togenerate diverse coordination behaviors at training time. To counter this, wepropose Behavior Diversity Play (BDP), a novel ZSC approach that encouragesdiversity through a discriminability objective. Our results demonstrate thatBDP learns adaptive agents that can tackle visual coordination, and zero-shotgeneralize to new partners in unseen environments, achieving 35% higher successand 32% higher efficiency compared to baselines.", "output": "Adaptive Coordination in Social Embodied Rearrangement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The relational data model was designed to facilitate large-scale datamanagement and analytics. We consider the problem of how to differentiatecomputations expressed relationally. We show experimentally that a relationalengine running an auto-differentiated relational algorithm can easily scale tovery large datasets, and is competitive with state-of-the-art, special-purposesystems for large-scale distributed machine learning.", "output": "Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reductive Lie Groups, such as the orthogonal groups, the Lorentz group, orthe unitary groups, play essential roles across scientific fields as diverse ashigh energy physics, quantum mechanics, quantum chromodynamics, moleculardynamics, computer vision, and imaging. In this paper, we present a generalEquivariant Neural Network architecture capable of respecting the symmetries ofthe finite-dimensional representations of any reductive Lie Group G. Ourapproach generalizes the successful ACE and MACE architectures for atomisticpoint clouds to any data equivariant to a reductive Lie group action. We alsointroduce the lie-nn software library, which provides all the necessary toolsto develop and implement such general G-equivariant neural networks. Itimplements routines for the reduction of generic tensor products ofrepresentations into irreducible representations, making it easy to apply ourarchitecture to a wide range of problems and groups. The generality andperformance of our approach are demonstrated by applying it to the tasks of topquark decay tagging (Lorentz group) and shape recognition (orthogonal group).", "output": "A General Framework for Equivariant Neural Networks on Reductive Lie Groups."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider Pareto front identification for linear bandits (PFILin) where thegoal is to identify a set of arms whose reward vectors are not dominated by anyof the others when the mean reward vector is a linear function of the context.PFILin includes the best arm identification problem and multi-objective activelearning as special cases. The sample complexity of our proposed algorithm is$tilde{O}(d/Delta^2)$, where $d$ is the dimension of contexts and $Delta$ isa measure of problem complexity. Our sample complexity is optimal up to alogarithmic factor. A novel feature of our algorithm is that it uses thecontexts of all actions. In addition to efficiently identifying the Paretofront, our algorithm also guarantees $tilde{O}(sqrt{d/t})$ bound forinstantaneous Pareto regret when the number of samples is larger than$Omega(dlog dL)$ for $L$ dimensional vector rewards. By using the contexts ofall arms, our proposed algorithm simultaneously provides efficient Pareto frontidentification and regret minimization. Numerical experiments demonstrate thatthe proposed algorithm successfully identifies the Pareto front whileminimizing the regret.", "output": "Pareto Front Identification with Regret Minimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Two-Tower Vision-Language (VL) models have shown promising improvements onvarious downstream VL tasks. Although the most advanced work improvesperformance by building bridges between encoders, it suffers from ineffectivelayer-by-layer utilization of uni-modal representations and cannot flexiblyexploit different levels of uni-modal semantic knowledge. In this work, wepropose ManagerTower, a novel VL model architecture that gathers and combinesthe insights of pre-trained uni-modal experts at different levels. The managersintroduced in each cross-modal layer can adaptively aggregate uni-modalsemantic knowledge to facilitate more comprehensive cross-modal alignment andfusion. ManagerTower outperforms previous strong baselines both with andwithout Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTowerachieves superior performances on various downstream VL tasks, especially79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K.Code and checkpoints are available at ", "output": "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has recently emerged as a promising paradigmfor training generalisable models on large-scale data in the fields of vision,text, and speech. Although SSL has been proven effective in speech and audio,its application to music audio has yet to be thoroughly explored. This isprimarily due to the distinctive challenges associated with modelling musicalknowledge, particularly its tonal and pitched characteristics of music. Toaddress this research gap, we propose an acoustic Music undERstanding modelwith large-scale self-supervised Training (MERT), which incorporates teachermodels to provide pseudo labels in the masked language modelling (MLM) styleacoustic pre-training. In our exploration, we identified a superior combinationof teacher models, which outperforms conventional speech and audio approachesin terms of performance. This combination includes an acoustic teacher based onResidual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musicalteacher based on the Constant-Q Transform (CQT). These teachers effectivelyguide our student model, a BERT-style transformer encoder, to better modelmusic audio. In addition, we introduce an in-batch noise mixture augmentationto enhance the representation robustness. Furthermore, we explore a wide rangeof settings to overcome the instability in acoustic language modelpre-training, which allows our designed paradigm to scale from 95M to 330Mparameters. Experimental results indicate that our model can generalise andperform well on 14 music understanding tasks and attains state-of-the-art(SOTA) overall scores. The code and models are online:", "output": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language models (LLMs) can be induced to solve non-trivial problemswith \"few-shot\" prompts including illustrative problem-solution examples. Nowif the few-shots also include \"chain of thought\" (CoT) explanations, which areof the form problem-explanation-solution, LLMs will generate a \"explained\"solution, and perform even better. Recently an exciting, substantially bettertechnique, self-consistency [1] (S-C) has emerged, based on the intuition thatthere are many plausible explanations for the right solution; when the LLM issampled repeatedly to generate a pool of explanation-solution pairs, for agiven problem, the most frequently occurring solutions in the pool (ignoringthe explanations) tend to be even more likely to be correct! Unfortunately, theuse of this highly-performant S-C (or even CoT) approach in softwareengineering settings is hampered by the lack of explanations; most softwaredatasets lack explanations. In this paper, we describe an application of theS-C approach to program repair, using the commit log on the fix as theexplanation, only in the illustrative few-shots. We achieve state-of-the artresults, beating previous approaches to prompting-based program repair, on theMODIT dataset; we also find evidence suggesting that the correct commitmessages are helping the LLM learn to produce better patches.", "output": "Majority Rule: better patching via Self-Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating music from text descriptions is a user-friendly mode since thetext is a relatively easy interface for user engagement. While some approachesutilize texts to control music audio generation, editing musical elements ingenerated audio is challenging for users. In contrast, symbolic music offersease of editing, making it more accessible for users to manipulate specificmusical elements. In this paper, we propose MuseCoco, which generates symbolicmusic from text descriptions with musical attributes as the bridge to breakdown the task into text-to-attribute understanding and attribute-to-musicgeneration stages. MuseCoCo stands for Music Composition Copilot that empowersmusicians to generate music directly from given text descriptions, offering asignificant improvement in efficiency compared to creating music entirely fromscratch. The system has two main advantages: Firstly, it is data efficient. Inthe attribute-to-music generation stage, the attributes can be directlyextracted from music sequences, making the model training self-supervised. Inthe text-to-attribute understanding stage, the text is synthesized and refinedby ChatGPT based on the defined attribute templates. Secondly, the system canachieve precise control with specific attributes in text descriptions andoffers multiple control options through attribute-conditioned ortext-conditioned approaches. MuseCoco outperforms baseline systems in terms ofmusicality, controllability, and overall score by at least 1.27, 1.08, and 1.32respectively. Besides, there is a notable enhancement of about 20% in objectivecontrol accuracy. In addition, we have developed a robust large-scale modelwith 1.2 billion parameters, showcasing exceptional controllability andmusicality.", "output": "MuseCoco: Generating Symbolic Music from Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monitoring land cover using remote sensing is vital for studyingenvironmental changes and ensuring global food security through crop yieldforecasting. Specifically, multitemporal remote sensing imagery providesrelevant information about the dynamics of a scene, which has proven to lead tobetter land cover classification results. Nevertheless, few studies havebenefited from high spatial and temporal resolution data due to the difficultyof accessing reliable, fine-grained and high-quality annotated samples tosupport their hypotheses. Therefore, we introduce a temporal patch-baseddataset of Canadian croplands, enriched with labels retrieved from the CanadianAnnual Crop Inventory. The dataset contains 78,536 manually verified andcurated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10crop classes collected over four crop production years (2017-2020) and fivemonths (June-October). Each instance contains 12 spectral bands, an RGB image,and additional vegetation index bands. Individually, each category contains atleast 4,800 images. Moreover, as a benchmark, we provide models and source codethat allow a user to predict the crop class using a single image (ResNet,DenseNet, EfficientNet) or a sequence of images (LRCN, 3D-CNN) from the samelocation. In perspective, we expect this evolving dataset to propel thecreation of robust agro-environmental models that can accelerate thecomprehension of complex agricultural regions by providing accurate andcontinuous monitoring of land cover.", "output": "The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop an analytical framework to characterize the set of optimal ReLUneural networks by reformulating the non-convex training problem as a convexprogram. We show that the global optima of the convex parameterization aregiven by a polyhedral set and then extend this characterization to the optimalset of the non-convex training objective. Since all stationary points of theReLU training problem can be represented as optima of sub-sampled convexprograms, our work provides a general expression for all critical points of thenon-convex objective. We then leverage our results to provide an optimalpruning algorithm for computing minimal networks, establish conditions for theregularization path of ReLU networks to be continuous, and develop sensitivityresults for minimal ReLU networks.", "output": "Optimal Sets and Solution Paths of ReLU Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In Federated Learning (FL) and many other distributed training frameworks,collaborators can hold their private data locally and only share the networkweights trained with the local data after multiple iterations. Gradientinversion is a family of privacy attacks that recovers data from its generatedgradients. Seemingly, FL can provide a degree of protection against gradientinversion attacks on weight updates, since the gradient of a single step isconcealed by the accumulation of gradients over multiple local iterations. Inthis work, we propose a principled way to extend gradient inversion attacks toweight updates in FL, thereby better exposing weaknesses in the presumedprivacy protection inherent in FL. In particular, we propose a surrogate modelmethod based on the characteristic of two-dimensional gradient flow andlow-rank property of local updates. Our method largely boosts the ability ofgradient inversion attacks on weight updates containing many iterations andachieves state-of-the-art (SOTA) performance. Additionally, our method runs upto $100times$ faster than the SOTA baseline in the common FL scenario. Ourwork re-evaluates and highlights the privacy risk of sharing network weights.Our code is available at", "output": "Surrogate Model Extension (SME): A Fast and Accurate Weight Update Attack on Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Canary exposure, introduced in Carlini et al. is frequently used toempirically evaluate, or audit, the privacy of machine learning model training.The goal of this note is to provide some intuition on how to interpret canaryexposure, including by relating it to membership inference attacks anddifferential privacy.", "output": "A Note On Interpreting Canary Exposure."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a technique for tuning the learning rate scale factor of anybase optimization algorithm and schedule automatically, which we calltextsc{mechanic}. Our method provides a practical realization of recenttheoretical reductions for accomplishing a similar goal in online convexoptimization. We rigorously evaluate textsc{mechanic} on a range of largescale deep learning tasks with varying batch sizes, schedules, and baseoptimization algorithms. These experiments demonstrate that depending on theproblem, textsc{mechanic} either comes very close to, matches or even improvesupon manual tuning of learning rates.", "output": "Mechanic: A Learning Rate Tuner."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In 1989 George Cybenko proved in a landmark paper that wide shallow neuralnetworks can approximate arbitrary continuous functions on a compact set. Thisuniversal approximation theorem sparked a lot of follow-up research.Shen, Yang and Zhang determined optimal approximation rates for ReLU-networksin $L^p$-norms with $p in [1,infty)$. Kidger and Lyons proved a universalapproximation theorem for deep narrow ReLU-networks. Telgarsky gave an exampleof a deep narrow ReLU-network that cannot be approximated by a wide shallowReLU-network unless it has exponentially many neurons.However, there are even more questions that still remain unresolved. Arethere any wide shallow ReLU-networks that cannot be approximated well by deepnarrow ReLU-networks? Is the universal approximation theorem still true forother norms like the Sobolev norm $W^{1,1}$? Do these results hold foractivation functions other than ReLU?We will answer all of those questions and more with a framework of twoexpressive powers. The first one is well-known and counts the maximal number oflinear regions of a function calculated by a ReLU-network. We will improve thebest known bounds for this expressive power. The second one is entirely new.", "output": "On the Expressive Power of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion model-based approaches have shown promise in data-driven planning,but there are no safety guarantees, thus making it hard to be applied forsafety-critical applications. To address these challenges, we propose a newmethod, called SafeDiffuser, to ensure diffusion probabilistic models satisfyspecifications by using a class of control barrier functions. The key idea ofour approach is to embed the proposed finite-time diffusion invariance into thedenoising diffusion procedure, which enables trustworthy diffusion datageneration. Moreover, we demonstrate that our finite-time diffusion invariancemethod through generative models not only maintains generalization performancebut also creates robustness in safe data generation. We test our method on aseries of safe planning tasks, including maze path generation, legged robotlocomotion, and 3D space manipulation, with results showing the advantages ofrobustness and guarantees over vanilla diffusion models.", "output": "SafeDiffuser: Safe Planning with Diffusion Probabilistic Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper considers distributed online convex optimization with adversarialconstraints. In this setting, a network of agents makes decisions at eachround, and then only a portion of the loss function and a coordinate block ofthe constraint function are privately revealed to each agent. The loss andconstraint functions are convex and can vary arbitrarily across rounds. Theagents collaborate to minimize network regret and cumulative constraintviolation. A novel distributed online algorithm is proposed and it achieves an$mathcal{O}(T^{max{c,1-c}})$ network regret bound and an$mathcal{O}(T^{1-c/2})$ network cumulative constraint violation bound, where$T$ is the number of rounds and $cin(0,1)$ is a user-defined trade-offparameter. When Slater's condition holds (i.e, there is a point that strictlysatisfies the inequality constraints), the network cumulative constraintviolation bound is reduced to $mathcal{O}(T^{1-c})$. Moreover, if the lossfunctions are strongly convex, then the network regret bound is reduced to$mathcal{O}(log(T))$, and the network cumulative constraint violation boundis reduced to $mathcal{O}(sqrt{log(T)T})$ and $mathcal{O}(log(T))$ withoutand with Slater's condition, respectively. To the best of our knowledge, thispaper is the first to achieve reduced (network) cumulative constraint violationbounds for (distributed) online convex optimization with adversarialconstraints under Slater's condition. Finally, the theoretical results areverified through numerical simulations.", "output": "Distributed Online Convex Optimization with Adversarial Constraints: Reduced Cumulative Constraint Violation Bounds under Slater's Condition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clustering (or community detection) on multilayer graphs poses severaladditional complications with respect to standard graphs as different layersmay be characterized by different structures and types of information. One ofthe major challenges is to establish the extent to which each layer contributesto the cluster assignment in order to effectively take advantage of themultilayer structure and improve upon the classification obtained using theindividual layers or their union. However, making an informed a-prioriassessment about the clustering information content of the layers can be verycomplicated. In this work, we assume a semi-supervised learning setting, wherethe class of a small percentage of nodes is initially provided, and we proposea parameter-free Laplacian-regularized model that learns an optimal nonlinearcombination of the different layers from the available input labels. Thelearning algorithm is based on a Frank-Wolfe optimization scheme with inexactgradient, combined with a modified Label Propagation iteration. We provide adetailed convergence analysis of the algorithm and extensive experiments onsynthetic and real-world datasets, showing that the proposed method comparesfavourably with a variety of baselines and outperforms each individual layerwhen used in isolation.", "output": "Learning the Right Layers: a Data-Driven Layer-Aggregation Strategy for Semi-Supervised Learning on Multilayer Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This tutorial paper provides a general overview of symbolic regression (SR)with specific focus on standards of interpretability. We posit thatinterpretable modeling, although its definition is still disputed in theliterature, is a practical way to support the evaluation of successfulinformation fusion. In order to convey the benefits of SR as a modelingtechnique, we demonstrate an application within the field of health andnutrition using publicly available National Health and Nutrition ExaminationSurvey (NHANES) data from the Centers for Disease Control and Prevention (CDC),fusing together anthropometric markers into a simple mathematical expression toestimate body fat percentage. We discuss the advantages and challengesassociated with SR modeling and provide qualitative and quantitative analysesof the learned models.", "output": "Information Fusion via Symbolic Regression: A Tutorial in the Context of Human Health."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Audio-Visual Lightweight ITerative model (AVLIT), an effective andlightweight neural network that uses Progressive Learning (PL) to performaudio-visual speech separation in noisy environments. To this end, we adopt theAsynchronous Fully Recurrent Convolutional Neural Network (A-FRCNN), which hasshown successful results in audio-only speech separation. Our architectureconsists of an audio branch and a video branch, with iterative A-FRCNN blockssharing weights for each modality. We evaluated our model in a controlledenvironment using the NTCD-TIMIT dataset and in-the-wild using a syntheticdataset that combines LRS3 and WHAM!. The experiments demonstrate thesuperiority of our model in both settings with respect to various audio-onlyand audio-visual baselines. Furthermore, the reduced footprint of our modelmakes it suitable for low resource applications.", "output": "Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As deep neural networks are highly expressive, it is important to findsolutions with small generalization gap (the difference between the performanceon the training data and unseen data). Focusing on the stochastic nature oftraining, we first present a theoretical analysis in which the bound ofgeneralization gap depends on what we call inconsistency and instability ofmodel outputs, which can be estimated on unlabeled data. Our empirical studybased on this analysis shows that instability and inconsistency are stronglypredictive of generalization gap in various settings. In particular, ourfinding indicates that inconsistency is a more reliable indicator ofgeneralization gap than the sharpness of the loss landscape. Furthermore, weshow that algorithmic reduction of inconsistency leads to superior performance.The results also provide a theoretical basis for existing methods such asco-distillation and ensemble.", "output": "Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many problems, such as online ad display, can be formulated as onlinebipartite matching. The crucial challenge lies in the nature ofsequentially-revealed online item information, based on which we makeirreversible matching decisions at each step. While numerous expert onlinealgorithms have been proposed with bounded worst-case competitive ratios, theymay not offer satisfactory performance in average cases. On the other hand,reinforcement learning (RL) has been applied to improve the averageperformance, but it lacks robustness and can perform arbitrarily poorly. Inthis paper, we propose a novel RL-based approach to edge-weighted onlinebipartite matching with robustness guarantees (LOMAR), achieving both goodaverage-case and worst-case performance. The key novelty of LOMAR is a newonline switching operation which, based on a judicious condition to hedgeagainst future uncertainties, decides whether to follow the expert's decisionor the RL decision for each online item. We prove that for any $rhoin[0,1]$,LOMAR is $rho$-competitive against any given expert online algorithm. Toimprove the average performance, we train the RL policy by explicitlyconsidering the online switching operation. Finally, we run empiricalexperiments to demonstrate the advantages of LOMAR compared to existingbaselines. Our code is available at: ", "output": "Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reconstruction of 3D neural fields from posed images has emerged as apromising method for self-supervised representation learning. The key challengepreventing the deployment of these 3D scene learners on large-scale video datais their dependence on precise camera poses from structure-from-motion, whichis prohibitively expensive to run at scale. We propose a method that jointlyreconstructs camera poses and 3D neural scene representations online and in asingle forward pass. We estimate poses by first lifting frame-to-frame opticalflow to 3D scene flow via differentiable rendering, preserving locality andshift-equivariance of the image processing backbone. SE(3) camera poseestimation is then performed via a weighted least-squares fit to the scene flowfield. This formulation enables us to jointly supervise pose estimation and ageneralizable neural scene representation via re-rendering the input video, andthus, train end-to-end and fully self-supervised on real-world video datasets.We demonstrate that our method performs robustly on diverse, real-world video,notably on sequences traditionally challenging to optimization-based poseestimation techniques.", "output": "FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a growing gap between the impressive results of deep imagegenerative models and classical algorithms that offer theoretical guarantees.The former suffer from mode collapse or memorization issues, limiting theirapplication to scientific data. The latter require restrictive assumptions suchas log-concavity to escape the curse of dimensionality. We partially bridgethis gap by introducing conditionally strongly log-concave (CSLC) models, whichfactorize the data distribution into a product of conditional probabilitydistributions that are strongly log-concave. This factorization is obtainedwith orthogonal projectors adapted to the data distribution. It leads toefficient parameter estimation and sampling algorithms, with theoreticalguarantees, although the data distribution is not globally log-concave. We showthat several challenging multiscale processes are conditionally log-concaveusing wavelet packet orthogonal projectors. Numerical results are shown forphysical fields such as the $varphi^4$ model and weak lensing convergence mapswith higher resolution than in previous works.", "output": "Conditionally Strongly Log-Concave Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Representations learned by pre-training a neural network on a large datasetare increasingly used successfully to perform a variety of downstream tasks. Inthis work, we take a closer look at how features are encoded in suchpre-trained representations. We find that learned representations in a givenlayer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subsetof neurons in the layer that is larger than a threshold size shares a largedegree of similarity with the full layer and is able to perform similarly asthe whole layer on a variety of downstream tasks. For example, a linear probetrained on $20%$ of randomly picked neurons from a ResNet50 pre-trained onImageNet1k achieves an accuracy within $5%$ of a linear probe trained on thefull layer of neurons for downstream CIFAR10 classification. We conductexperiments on different neural architectures (including CNNs and Transformers)pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety ofdownstream tasks taken from the VTAB benchmark. We find that the loss &amp; datasetused during pre-training largely govern the degree of diffuse redundancy andthe \"critical mass\" of neurons needed often depends on the downstream task,suggesting that there is a task-inherent redundancy-performance Paretofrontier. Our findings shed light on the nature of representations learned bypre-trained deep neural networks and suggest that entire layers might not benecessary to perform many downstream tasks. We investigate the potential forexploiting this redundancy to achieve efficient generalization for downstreamtasks and also draw caution to certain possible unintended consequences.", "output": "Diffused Redundancy in Pre-trained Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep reinforcement learning(DRL) is increasingly being explored in medicalimaging. However, the environments for medical imaging tasks are constantlyevolving in terms of imaging orientations, imaging sequences, and pathologies.To that end, we developed a Lifelong DRL framework, SERIL to continually learnnew tasks in changing imaging environments without catastrophic forgetting.SERIL was developed using selective experience replay based lifelong learningtechnique for the localization of five anatomical landmarks in brain MRI on asequence of twenty-four different imaging environments. The performance ofSERIL, when compared to two baseline setups: MERT(multi-environment-best-case)and SERT(single-environment-worst-case) demonstrated excellent performance withan average distance of $9.90pm7.35$ pixels from the desired landmark acrossall 120 tasks, compared to $10.29pm9.07$ for MERT and $36.37pm22.41$ forSERT($p&lt;0.05$), demonstrating the excellent potential for continuously learningmultiple tasks across dynamically changing imaging environments.", "output": "Multi-environment lifelong deep reinforcement learning for medical imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the infinite-horizon restless bandit problem with the average rewardcriterion, under both discrete-time and continuous-time settings. A fundamentalquestion is how to design computationally efficient policies that achieve adiminishing optimality gap as the number of arms, $N$, grows large. Existingresults on asymptotical optimality all rely on the uniform global attractorproperty (UGAP), a complex and challenging-to-verify assumption. In this paper,we propose a general, simulation-based framework that converts any single-armedpolicy into a policy for the original $N$-armed problem. This is accomplishedby simulating the single-armed policy on each arm and carefully steering thereal state towards the simulated state. Our framework can be instantiated toproduce a policy with an $O(1/sqrt{N})$ optimality gap. In the discrete-timesetting, our result holds under a simpler synchronization assumption, whichcovers some problem instances that do not satisfy UGAP. More notably, in thecontinuous-time setting, our result does not require any additional assumptionsbeyond the standard unichain condition. In both settings, we establish thefirst asymptotic optimality result that does not require UGAP.", "output": "Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven methods have shown tremendous progress in medical image analysis.In this context, deep learning-based supervised methods are widely popular.However, they require a large amount of training data and face issues ingeneralisability to unseen datasets that hinder clinical translation.Endoscopic imaging data incorporates large inter- and intra-patient variabilitythat makes these models more challenging to learn representative features fordownstream tasks. Thus, despite the publicly available datasets and datasetsthat can be generated within hospitals, most supervised models stillunderperform. While self-supervised learning has addressed this problem to someextent in natural scene data, there is a considerable performance gap in themedical image domain. In this paper, we propose to explore patch-levelinstance-group discrimination and penalisation of inter-class variation usingadditive angular margin within the cosine similarity metrics. Our novelapproach enables models to learn to cluster similar representative patches,thereby improving their ability to provide better separation between differentclasses. Our results demonstrate significant improvement on all metrics overthe state-of-the-art (SOTA) methods on the test set from the same and diversedatasets. We evaluated our approach for classification, detection, andsegmentation. SSL-CPCD achieves 79.77% on Top 1 accuracy for ulcerative colitisclassification, 88.62% on mAP for polyp detection, and 82.32% on dicesimilarity coefficient for segmentation tasks are nearly over 4%, 2%, and 3%,respectively, compared to the baseline architectures. We also demonstrate thatour method generalises better than all SOTA methods to unseen datasets,reporting nearly 7% improvement in our generalisability assessment.", "output": "SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Controlled generation refers to the problem of creating text that containsstylistic or semantic attributes of interest. Many approaches reduce thisproblem to training a predictor of the desired attribute. For example,researchers hoping to deploy a large language model to produce non-toxiccontent may use a toxicity classifier to filter generated text. In practice,the generated text to classify, which is determined by user prompts, may comefrom a wide range of distributions. In this paper, we show that the performanceof controlled generation may be poor if the distributions of text in responseto user prompts differ from the distribution the predictor was trained on. Toaddress this problem, we cast controlled generation under distribution shift asan invariant learning problem: the most effective predictor should be invariantacross multiple text environments. We then discuss a natural solution thatarises from this characterization and propose heuristics for selecting naturalenvironments. We study this characterization and the proposed methodempirically using both synthetic and real data. Experiments demonstrate boththe challenge of distribution shift in controlled generation and the potentialof invariance methods in this setting.", "output": "An Invariant Learning Characterization of Controlled Text Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a new class of online learning algorithms, generalized implicitFollow-The-Regularized-Leader (FTRL), that expands the scope of FTRL framework.Generalized implicit FTRL can recover known algorithms, as FTRL with linearizedlosses and implicit FTRL, and it allows the design of new update rules, asextensions of aProx and Mirror-Prox to FTRL. Our theory is constructive in thesense that it provides a simple unifying framework to design updates thatdirectly improve the worst-case upper bound on the regret. The key idea issubstituting the linearization of the losses with a Fenchel-Young inequality.We show the flexibility of the framework by proving that some known algorithms,like the Mirror-Prox updates, are instantiations of the generalized implicitFTRL. Finally, the new framework allows us to recover the temporal variationbound of implicit OMD, with the same computational complexity.", "output": "Generalized Implicit Follow-The-Regularized-Leader."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning (DL) techniques are highly effective for defect detection fromimages. Training DL classification models, however, requires vast amounts oflabeled data which is often expensive to collect. In many cases, not only theavailable training data is limited but may also imbalanced. In this paper, wepropose a novel domain adaptation (DA) approach to address the problem oflabeled training data scarcity for a target learning task by transferringknowledge gained from an existing source dataset used for a similar learningtask. Our approach works for scenarios where the source dataset and the datasetavailable for the target learning task have same or different feature spaces.We combine our DA approach with an autoencoder-based data augmentation approachto address the problem of imbalanced target datasets. We evaluate our combinedapproach using image data for wafer defect prediction. The experiments show itssuperior performance against other algorithms when the number of labeledsamples in the target dataset is significantly small and the target dataset isimbalanced.", "output": "Building Manufacturing Deep Learning Models with Minimal and Imbalanced Training Data Using Domain Adaptation and Data Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While stochastic gradient descent (SGD) is still the most popularoptimization algorithm in deep learning, adaptive algorithms such as Adam haveestablished empirical advantages over SGD in some deep learning applicationssuch as training transformers. However, it remains a question that why Adamconverges significantly faster than SGD in these scenarios. In this paper, wepropose one explanation of why Adam converges faster than SGD using a newconcept directional sharpness. We argue that the performance of optimizationalgorithms is closely related to the directional sharpness of the update steps,and show SGD has much worse directional sharpness compared to adaptivealgorithms. We further observe that only a small fraction of the coordinatescauses the bad sharpness and slow convergence of SGD, and propose to usecoordinate-wise clipping as a solution to SGD and other optimizationalgorithms. We demonstrate the effect of coordinate-wise clipping on sharpnessreduction and speeding up the convergence of optimization algorithms undervarious settings. We show that coordinate-wise clipping improves the local lossreduction when only a small fraction of the coordinates has bad sharpness. Weconclude that the sharpness reduction effect of adaptive coordinate-wisescaling is the reason for Adam's success in practice and suggest the use ofcoordinate-wise clipping as a universal technique to speed up deep learningoptimization.", "output": "Toward Understanding Why Adam Converges Faster Than SGD for Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised pre-trained models extract general-purpose representationsfrom data, and quantifying how reliable they are is crucial because manydownstream models use these representations as input for their own tasks. Tothis end, we first introduce a formal definition of representation reliability:the representation for a given test input is considered to be reliable if thedownstream models built on top of that representation can consistently generateaccurate predictions for that test point. It is desired to estimate therepresentation reliability without knowing the downstream tasks a priori. Weprovide a negative result showing that existing frameworks for uncertaintyquantification in supervised learning are not suitable for this purpose. As analternative, we propose an ensemble-based method for quantifying representationreliability, based on the concept of neighborhood consistency in therepresentation spaces across various pre-trained models. More specifically, thekey insight is to use shared neighboring points as anchors to align differentrepresentation spaces. We demonstrate through comprehensive numericalexperiments that our method is capable of predicting representation reliabilitywith high accuracy.", "output": "Representation Reliability and Its Impact on Downstream Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The remarkable growth and significant success of machine learning haveexpanded its applications into programming languages and program analysis.However, a key challenge in adopting the latest machine learning methods is therepresentation of programming languages, which directly impacts the ability ofmachine learning methods to reason about programs. The absence of numericalawareness, composite data structure information, and improper way of presentingvariables in previous representation works have limited their performances. Toovercome the limitations and challenges of current program representations, wepropose a novel graph-based program representation called PERFOGRAPH.PERFOGRAPH can capture numerical information and the composite data structureby introducing new nodes and edges. Furthermore, we propose an adaptedembedding method to incorporate numerical awareness. These enhancements makePERFOGRAPH a highly flexible and scalable representation that can effectivelycapture program intricate dependencies and semantics. Consequently, it servesas a powerful tool for various applications such as program analysis,performance optimization, and parallelism discovery. Our experimental resultsdemonstrate that PERFOGRAPH outperforms existing representations and sets newstate-of-the-art results by reducing the error rate by 7.4% (AMD dataset) and10% (NVIDIA dataset) in the well-known Device Mapping challenge. It also setsnew state-of-the-art results in various performance optimization tasks likeParallelism Discovery and Numa and Prefetchers Configuration prediction.", "output": "PERFOGRAPH: A Numerical Aware Program Graph Representation for Performance Optimization and Program Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We examine online safe multi-agent reinforcement learning using constrainedMarkov games in which agents compete by maximizing their expected total rewardsunder a constraint on expected total utilities. Our focus is confined to anepisodic two-player zero-sum constrained Markov game with independenttransition functions that are unknown to agents, adversarial reward functions,and stochastic utility functions. For such a Markov game, we employ an approachbased on the occupancy measure to formulate it as an online constrainedsaddle-point problem with an explicit constraint. We extend the Lagrangemultiplier method in constrained optimization to handle the constraint bycreating a generalized Lagrangian with minimax decision primal variables and adual variable. Next, we develop an upper confidence reinforcement learningalgorithm to solve this Lagrangian problem while balancing exploration andexploitation. Our algorithm updates the minimax decision primal variables viaonline mirror descent and the dual variable via projected gradient step and weprove that it enjoys sublinear rate $ O((|X|+|Y|) L sqrt{T(|A|+|B|)}))$ forboth regret and constraint violation after playing $T$ episodes of the game.Here, $L$ is the horizon of each episode, $(|X|,|A|)$ and $(|Y|,|B|)$ are thestate/action space sizes of the min-player and the max-player, respectively. Tothe best of our knowledge, we provide the first provably efficient online safereinforcement learning algorithm in constrained Markov games.", "output": "Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image generative models have made remarkable advancements ingenerating high-quality images. However, generated images often containundesirable artifacts or other errors due to model limitations. Existingtechniques to fine-tune generated images are time-consuming (manual editing),produce poorly-integrated results (inpainting), or result in unexpected changesacross the entire image (variation selection and prompt fine-tuning). In thiswork, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool toefficiently fine-tune desired regions within an AI-synthesized image. Ourmethod introduces new random noise patterns at targeted regions during thereverse diffusion process, enabling the model to efficiently make changes tothe specified regions while preserving the original context for the rest of theimage. We evaluate our method's usability and effectiveness through a userstudy with artists, comparing our technique against other state-of-the-artimage inpainting techniques and editing software for fine-tuning AI-generatedimagery.", "output": "Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent surge of interest in physics-informed neural network (PINN)methods has led to a wave of studies that attest to their potential for solvingpartial differential equations (PDEs) and predicting the dynamics of physicalsystems. However, the predictive limitations of PINNs have not been thoroughlyinvestigated. We look at the flow around a 2D cylinder and find that data-freePINNs are unable to predict vortex shedding. Data-driven PINN exhibits vortexshedding only while the training data (from a traditional CFD solver) isavailable, but reverts to the steady state solution when the data flow stops.We conducted dynamic mode decomposition and analyze the Koopman modes in thesolutions obtained with PINNs versus a traditional fluid solver (PetIBM). Thedistribution of the Koopman eigenvalues on the complex plane suggests that PINNis numerically dispersive and diffusive. The PINN method reverts to the steadysolution possibly as a consequence of spectral bias. This case study reaisesconcerns about the ability of PINNs to predict flows with instabilities,specifically vortex shedding. Our computational study supports the need formore theoretical work to analyze the numerical properties of PINN methods. Theresults in this paper are transparent and reproducible, with all data and codeavailable in public repositories and persistent archives; links are provided inthe paper repository at url{ anda Reproducibility Statement within the paper.", "output": "Predictive Limitations of Physics-Informed Neural Networks in Vortex Shedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a contextual combinatorial bandit problem where in each round alearning agent selects a subset of arms and receives feedback on the selectedarms according to their scores. The score of an arm is an unknown function ofthe arm's feature. Approximating this unknown score function with deep neuralnetworks, we propose algorithms: Combinatorial Neural UCB ($texttt{CN-UCB}$)and Combinatorial Neural Thompson Sampling ($texttt{CN-TS}$). We prove that$texttt{CN-UCB}$ achieves $tilde{mathcal{O}}(tilde{d} sqrt{T})$ or$tilde{mathcal{O}}(sqrt{tilde{d} T K})$ regret, where $tilde{d}$ is theeffective dimension of a neural tangent kernel matrix, $K$ is the size of asubset of arms, and $T$ is the time horizon. For $texttt{CN-TS}$, we adapt anoptimistic sampling technique to ensure the optimism of the sampledcombinatorial action, achieving a worst-case (frequentist) regret of$tilde{mathcal{O}}(tilde{d} sqrt{TK})$. To the best of our knowledge, theseare the first combinatorial neural bandit algorithms with regret performanceguarantees. In particular, $texttt{CN-TS}$ is the first Thompson samplingalgorithm with the worst-case regret guarantees for the general contextualcombinatorial bandit problem. The numerical experiments demonstrate thesuperior performances of our proposed algorithms.", "output": "Combinatorial Neural Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Much of the previous work towards digital agents for graphical userinterfaces (GUIs) has relied on text-based representations (derived from HTMLor other structured data sources), which are not always readily available.These input representations have been often coupled with custom, task-specificaction spaces. This paper focuses on creating agents that interact with thedigital world using the same conceptual interface that humans commonly use --via pixel-based screenshots and a generic action space corresponding tokeyboard and mouse actions. Building upon recent progress in pixel-basedpretraining, we show, for the first time, that it is possible for such agentsto outperform human crowdworkers on the MiniWob++ benchmark of GUI-basedinstruction following tasks.", "output": "From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Decentralized Stochastic Gradient Descent (SGD) is an emerging neural networktraining approach that enables multiple agents to train a model collaborativelyand simultaneously. Rather than using a central parameter server to collectgradients from all the agents, each agent keeps a copy of the model parametersand communicates with a small number of other agents to exchange model updates.Their communication, governed by the communication topology and gossip weightmatrices, facilitates the exchange of model updates. The state-of-the-artapproach uses the dynamic one-peer exponential-2 topology, achieving fastertraining times and improved scalability than the ring, grid, torus, andhypercube topologies. However, this approach requires a power-of-2 number ofagents, which is impractical at scale. In this paper, we remove thisrestriction and propose underline{D}ecentralized underline{SGD} withunderline{C}ommunication-optimal underline{E}xact underline{C}onsensusunderline{A}lgorithm (DSGD-CECA), which works for any number of agents whilestill achieving state-of-the-art properties. In particular, DSGD-CECA incurs aunit per-iteration communication overhead and an $tilde{O}(n^3)$ transientiteration complexity. Our proof is based on newly discovered properties ofgossip weight matrices and a novel approach to combine them with DSGD'sconvergence analysis. Numerical experiments show the efficiency of DSGD-CECA.", "output": "DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pre-trained machine learning (ML) models have shown great performance for awide range of applications, in particular in natural language processing (NLP)and computer vision (CV). Here, we study how pre-training could be used forscientific machine learning (SciML) applications, specifically in the contextof transfer learning. We study the transfer behavior of these models as (i) thepre-trained model size is scaled, (ii) the downstream training dataset size isscaled, (iii) the physics parameters are systematically pushed out ofdistribution, and (iv) how a single model pre-trained on a mixture of differentphysics problems can be adapted to various downstream applications. We findthat-when fine-tuned appropriately-transfer learning can help reach desiredaccuracy levels with orders of magnitude fewer downstream examples (acrossdifferent tasks that can even be out-of-distribution) than training fromscratch, with consistent behavior across a wide range of downstream examples.We also find that fine-tuning these models yields more performance gains asmodel size increases, compared to training from scratch on new downstreamtasks. These results hold for a broad range of PDE learning tasks. All in all,our results demonstrate the potential of the \"pre-train and fine-tune\" paradigmfor SciML problems, demonstrating a path towards building SciML foundationmodels. We open-source our code for reproducibility.", "output": "Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-training is an important technique for solving semi-supervised learningproblems. It leverages unlabeled data by generating pseudo-labels and combiningthem with a limited labeled dataset for training. The effectiveness ofself-training heavily relies on the accuracy of these pseudo-labels. In thispaper, we introduce doubly robust self-training, a novel semi-supervisedalgorithm that provably balances between two extremes. When the pseudo-labelsare entirely incorrect, our method reduces to a training process solely usinglabeled data. Conversely, when the pseudo-labels are completely accurate, ourmethod transforms into a training process utilizing all pseudo-labeled data andlabeled data, thus increasing the effective sample size. Through empiricalevaluations on both the ImageNet dataset for image classification and thenuScenes autonomous driving dataset for 3D object detection, we demonstrate thesuperiority of the doubly robust loss over the standard self-training baseline.", "output": "Doubly Robust Self-Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate how pair-wise data augmentation techniques like Mixup affectthe sample complexity of finding optimal decision boundaries in a binary linearclassification problem. For a family of data distributions with a separabilityconstant $kappa$, we analyze how well the optimal classifier in terms oftraining loss aligns with the optimal one in test accuracy (i.e., Bayes optimalclassifier). For vanilla training without augmentation, we uncover aninteresting phenomenon named the curse of separability. As we increase $kappa$to make the data distribution more separable, the sample complexity of vanillatraining increases exponentially in $kappa$; perhaps surprisingly, the task offinding optimal decision boundaries becomes harder for more separabledistributions. For Mixup training, we show that Mixup mitigates this problem bysignificantly reducing the sample complexity. To this end, we develop newconcentration results applicable to $n^2$ pair-wise augmented data pointsconstructed from $n$ independent data, by carefully dealing with dependenciesbetween overlapping pairs. Lastly, we study other masking-based Mixup-styletechniques and show that they can distort the training loss and make itsminimizer converge to a suboptimal classifier in terms of test accuracy.", "output": "Provable Benefit of Mixup for Finding Optimal Decision Boundaries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) is a decentralized learning framework wherein aparameter server (PS) and a collection of clients collaboratively train a modelvia minimizing a global objective. Communication bandwidth is a scarceresource; in each round, the PS aggregates the updates from a subset of clientsonly. In this paper, we focus on non-convex minimization that is vulnerable tonon-uniform and time-varying communication failures between the PS and theclients. Specifically, in each round $t$, the link between the PS and client$i$ is active with probability $p_i^t$, which is $textit{unknown}$ to both thePS and the clients. This arises when the channel conditions are heterogeneousacross clients and are changing over time.We show that when the $p_i^t$'s are not uniform, $textit{Federated Average}$(FedAvg) -- the most widely adopted FL algorithm -- fails to minimize theglobal objective. Observing this, we propose $textit{Federated PostponedBroadcast}$ (FedPBC) which is a simple variant of FedAvg. It differs fromFedAvg in that the PS postpones broadcasting the global model till the end ofeach round. We show that FedPBC converges to a stationary point of the originalobjective. The introduced staleness is mild and there is no noticeableslowdown. Both theoretical analysis and numerical results are provided. On thetechnical front, postponing the global model broadcasts enables implicitgossiping among the clients with active links at round $t$. Despite $p_i^t$'sare time-varying, we are able to bound the perturbation of the global modeldynamics via the techniques of controlling the gossip-type information mixingerrors.", "output": "Towards Bias Correction of FedAvg over Nonuniform and Time-Varying Communications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper investigates a combinational creativity approach to transferlearning to improve the performance of deep neural network-based models formusic generation on out-of-distribution (OOD) genres. We identify Iranian folkmusic as an example of such an OOD genre for MusicVAE, a large generative musicmodel. We find that a combinational creativity transfer learning approach canefficiently adapt MusicVAE to an Iranian folk music dataset, indicatingpotential for generating underrepresented music genres in the future.", "output": "Transfer Learning for Underrepresented Music Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum machine learning (QML) is an emerging field of research thatleverages quantum computing to improve the classical machine learning approachto solve complex real world problems. QML has the potential to addresscybersecurity related challenges. Considering the novelty and complexarchitecture of QML, resources are not yet explicitly available that can pavecybersecurity learners to instill efficient knowledge of this emergingtechnology. In this research, we design and develop QML-based ten learningmodules covering various cybersecurity topics by adopting student centeringcase-study based learning approach. We apply one subtopic of QML on acybersecurity topic comprised of pre-lab, lab, and post-lab activities towardsproviding learners with hands-on QML experiences in solving real-world securityproblems. In order to engage and motivate students in a learning environmentthat encourages all students to learn, pre-lab offers a brief introduction toboth the QML subtopic and cybersecurity problem. In this paper, we utilizequantum support vector machine (QSVM) for malware classification and protectionwhere we use open source Pennylane QML framework on the drebin215 dataset. Wedemonstrate our QSVM model and achieve an accuracy of 95% in malwareclassification and protection. We will develop all the modules and introducethem to the cybersecurity community in the coming days.", "output": "Case Study-Based Approach of Quantum Machine Learning in Cybersecurity: Quantum Support Vector Machine for Malware Classification and Protection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural architecture search (NAS) has allowed for the automatic creation ofnew and effective neural network architectures, offering an alternative to thelaborious process of manually designing complex architectures. However,traditional NAS algorithms are slow and require immense amounts of computingpower. Recent research has investigated training-free NAS metrics for imageclassification architectures, drastically speeding up search algorithms. Inthis paper, we investigate training-free NAS metrics for recurrent neuralnetwork (RNN) and BERT-based transformer architectures, targeted towardslanguage modeling tasks. First, we develop a new training-free metric, namedhidden covariance, that predicts the trained performance of an RNN architectureand significantly outperforms existing training-free metrics. We experimentallyevaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLPbenchmark. Second, we find that the current search space paradigm fortransformer architectures is not optimized for training-free neuralarchitecture search. Instead, a simple qualitative analysis can effectivelyshrink the search space to the best performing architectures. This conclusionis based on our investigation of existing training-free metrics and new metricsdeveloped from recent transformer pruning literature, evaluated on our ownbenchmark of trained BERT architectures. Ultimately, our analysis shows thatthe architecture search space and the training-free metric must be developedtogether in order to achieve effective results.", "output": "Training-free Neural Architecture Search for RNNs and Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We can usually assume others have goals analogous to our own. This assumptioncan also, at times, be applied to multi-agent games - e.g. Agent 1's attractionto green pellets is analogous to Agent 2's attraction to red pellets. This\"analogy\" assumption is tied closely to the cognitive process known as empathy.Inspired by empathy, we design a simple and explainable architecture to modelanother agent's action-value function. This involves learning an \"ImaginationNetwork\" to transform the other agent's observed state in order to produce ahuman-interpretable \"empathetic state\" which, when presented to the learningagent, produces behaviours that mimic the other agent. Our approach isapplicable to multi-agent scenarios consisting of a single learning agent andother (independent) agents acting according to fixed policies. Thisarchitecture is particularly beneficial for (but not limited to) algorithmsusing a composite value or reward function. We show our method produces betterperformance in multi-agent games, where it robustly estimates the other's modelin different environment configurations. Additionally, we show that theempathetic states are human interpretable, and thus verifiable.", "output": "EMOTE: An Explainable architecture for Modelling the Other Through Empathy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Motivated by the striking ability of transformers for in-context learning,several works demonstrate that transformers can implement algorithms likegradient descent. By a careful construction of weights, these works show thatmultiple layers of transformers are expressive enough to simulate gradientdescent iterations. Going beyond the question of expressivity, we ask: Cantransformers learn to implement such algorithms by training over random probleminstances? To our knowledge, we make the first theoretical progress toward thisquestion via analysis of the loss landscape for linear transformers trainedover random instances of linear regression. For a single attention layer, weprove the global minimum of the training objective implements a singleiteration of preconditioned gradient descent. Notably, the preconditioningmatrix not only adapts to the input distribution but also to the varianceinduced by data inadequacy. For a transformer with $k$ attention layers, weprove certain critical points of the training objective implement $k$iterations of preconditioned gradient descent. Our results call for futuretheoretical studies on learning algorithms by training transformers.", "output": "Transformers learn to implement preconditioned gradient descent for in-context learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While deep-learning models have been shown to perform well on image-to-textdatasets, it is difficult to use them in practice for captioning images. Thisis because textit{captions} traditionally tend to be context-dependent andoffer complementary information about an image, while models tend to producetextit{descriptions} that describe the visual features of the image. Priorresearch in caption generation has explored the use of models that generatecaptions when provided with the images alongside their respective descriptionsor contexts. We propose and evaluate a new approach, which leverages existinglarge language models to generate captions from textual descriptions andcontext alone, without ever processing the image directly. We demonstrate thatafter fine-tuning, our approach outperforms current state-of-the-art image-textalignment models like OSCAR-VinVL on this task on the CIDEr metric.", "output": "CapText: Large Language Model-based Caption Generation From Image Context and Description."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We derive an (almost) guaranteed upper bound on the error of deep neuralnetworks under distribution shift using unlabeled test data. Prior methodseither give bounds that are vacuous in practice or give estimates that areaccurate on average but heavily underestimate error for a sizeable fraction ofshifts. In particular, the latter only give guarantees based on complexcontinuous measures such as test calibration -- which cannot be identifiedwithout labels -- and are therefore unreliable. Instead, our bound requires asimple, intuitive condition which is well justified by prior empirical worksand holds in practice effectively 100% of the time. The bound is inspired by$mathcal{H}Deltamathcal{H}$-divergence but is easier to evaluate andsubstantially tighter, consistently providing non-vacuous guarantees.Estimating the bound requires optimizing one multiclass classifier to disagreewith another, for which some prior works have used sub-optimal proxy losses; wedevise a \"disagreement loss\" which is theoretically justified and performsbetter in practice. We expect this loss can serve as a drop-in replacement forfuture methods which require maximizing multiclass disagreement. Across a widerange of benchmarks, our method gives valid error bounds while achievingaverage accuracy comparable to competitive estimation baselines. Code ispublicly available at  .", "output": "(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a key component in online marketing, uplift modeling aims to accuratelycapture the degree to which different treatments motivate different users, suchas coupons or discounts, also known as the estimation of individual treatmenteffect (ITE). In an actual business scenario, the options for treatment may benumerous and complex, and there may be correlations between differenttreatments. In addition, each marketing instance may also have rich user andcontextual features. However, existing methods still fall short in both fullyexploiting treatment information and mining features that are sensitive to aparticular treatment. In this paper, we propose an explicit featureinteraction-aware uplift network (EFIN) to address these two problems. Our EFINincludes four customized modules: 1) a feature encoding module encodes not onlythe user and contextual features, but also the treatment features; 2) aself-interaction module aims to accurately model the user's natural responsewith all but the treatment features; 3) a treatment-aware interaction moduleaccurately models the degree to which a particular treatment motivates a userthrough interactions between the treatment features and other features, i.e.,ITE; and 4) an intervention constraint module is used to balance the ITEdistribution of users between the control and treatment groups so that themodel would still achieve a accurate uplift ranking on data collected from anon-random intervention marketing scenario. We conduct extensive experiments ontwo public datasets and one product dataset to verify the effectiveness of ourEFIN. In addition, our EFIN has been deployed in a credit card bill paymentscenario of a large online financial platform with a significant improvement.", "output": "Explicit Feature Interaction-aware Uplift Network for Online Marketing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Post-training quantization (PTQ) has been gaining popularity for thedeployment of deep neural networks on resource-limited devices since unlikequantization-aware training, neither a full training dataset nor end-to-endtraining is required at all. As PTQ schemes based on reconstructing each layeror block output turn out to be effective to enhance quantized modelperformance, recent works have developed algorithms to devise and learn a newweight-rounding scheme so as to better reconstruct each layer or block output.In this work, we propose a simple yet effective new weight-rounding mechanismfor PTQ, coined FlexRound, based on element-wise division instead of typicalelement-wise addition such that FlexRound enables jointly learning a commonquantization grid size as well as a different scale for each pre-trainedweight. Thanks to the reciprocal rule of derivatives induced by element-wisedivision, FlexRound is inherently able to exploit pre-trained weights whenupdating their corresponding scales, and thus, flexibly quantize pre-trainedweights depending on their magnitudes. We empirically validate the efficacy ofFlexRound on a wide range of models and tasks. To the best of our knowledge,our work is the first to carry out comprehensive experiments on not only imageclassification and natural language understanding but also natural languagegeneration, assuming a per-tensor uniform PTQ setting. Moreover, wedemonstrate, for the first time, that large language models can be efficientlyquantized, with only a negligible impact on performance compared tohalf-precision baselines, achieved by reconstructing the output in ablock-by-block manner.", "output": "FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Heuristic Blending (HUBL), a simple performance-improvingtechnique for a broad class of offline RL algorithms based on valuebootstrapping. HUBL modifies Bellman operators used in these algorithms,partially replacing the bootstrapped values with Monte-Carlo returns asheuristics. For trajectories with higher returns, HUBL relies more onheuristics and less on bootstrapping; otherwise, it leans more heavily onbootstrapping. We show that this idea can be easily implemented by relabelingthe offline datasets with adjusted rewards and discount factors, making HUBLreadily usable by many existing offline RL implementations. We theoreticallyprove that HUBL reduces offline RL's complexity and thus improves itsfinite-sample performance. Furthermore, we empirically demonstrate that HUBLconsistently improves the policy quality of four state-of-the-artbootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9%on average over 27 datasets of the D4RL and Meta-World benchmarks.", "output": "Improving Offline RL by Blending Heuristics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language is often considered a key aspect of human thinking, providing uswith exceptional abilities to generalize, explore, plan, replan, and adapt tonew situations. However, Reinforcement Learning (RL) agents are far fromhuman-level performance in any of these abilities. We hypothesize one reasonfor such cognitive deficiencies is that they lack the benefits of thinking inlanguage and that we can improve AI agents by training them to think likehumans do. We introduce a novel Imitation Learning framework, Thought Cloning,where the idea is to not just clone the behaviors of human demonstrators, butalso the thoughts humans have as they perform these behaviors. While we expectThought Cloning to truly shine at scale on internet-sized datasets of humansthinking out loud while acting (e.g. online videos with transcripts), here weconduct experiments in a domain where the thinking and action data aresynthetically generated. Results reveal that Thought Cloning learns much fasterthan Behavioral Cloning and its performance advantage grows the further out ofdistribution test tasks are, highlighting its ability to better handle novelsituations. Thought Cloning also provides important benefits for AI Safety andInterpretability, and makes it easier to debug and improve AI. Because we canobserve the agent's thoughts, we can (1) more easily diagnose why things aregoing wrong, making it easier to fix the problem, (2) steer the agent bycorrecting its thinking, or (3) prevent it from doing unsafe things it plans todo. Overall, by training agents how to think as well as behave, Thought Cloningcreates safer, more powerful agents.", "output": "Thought Cloning: Learning to Think while Acting by Imitating Human Thinking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fairness plays a crucial role in various multi-agent systems (e.g.,communication networks, financial markets, etc.). Many multi-agent dynamicalinteractions can be cast as Markov Decision Processes (MDPs). While existingresearch has focused on studying fairness in known environments, theexploration of fairness in such systems for unknown environments remains open.In this paper, we propose a Reinforcement Learning (RL) approach to achievefairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing thesum of individual agents' value functions, we introduce a fairness functionthat ensures equitable rewards across agents. Since the classical Bellman'sequation does not hold when the sum of individual value functions is notmaximized, we cannot use traditional approaches. Instead, in order to explore,we maintain a confidence bound of the unknown environment and then propose anonline convex optimization based approach to obtain a policy constrained tothis confidence region. We show that such an approach achieves sub-linearregret in terms of the number of episodes. Additionally, we provide a probablyapproximately correct (PAC) guarantee based on the obtained regret bound. Wealso propose an offline RL algorithm and bound the optimality gap with respectto the optimal fair solution. To mitigate computational complexity, weintroduce a policy-gradient type method for the fair objective. Simulationexperiments also demonstrate the efficacy of our approach.", "output": "Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In a recent work, Laforgue et al. introduce the model of last switchdependent (LSD) bandits, in an attempt to capture nonstationary phenomenainduced by the interaction between the player and the environment. Examplesinclude satiation, where consecutive plays of the same action lead to decreasedperformance, or deprivation, where the payoff of an action increases after aninterval of inactivity. In this work, we take a step towards understanding theapproximability of planning LSD bandits, namely, the (NP-hard) problem ofcomputing an optimal arm-pulling strategy under complete knowledge of themodel. In particular, we design the first efficient constant approximationalgorithm for the problem and show that, under a natural monotonicityassumption on the payoffs, its approximation guarantee (almost) matches thestate-of-the-art for the special and well-studied class of recharging bandits(also known as delay-dependent). In this attempt, we develop new tools andinsights for this class of problems, including a novel higher-dimensionalrelaxation and the technique of mirroring the evolution of virtual states. Webelieve that these novel elements could potentially be used for approachingricher classes of action-induced nonstationary bandits (e.g., special instancesof restless bandits). In the case where the model parameters are initiallyunknown, we develop an online learning adaptation of our algorithm for which weprovide sublinear regret guarantees against its full-information counterpart.", "output": "Last Switch Dependent Bandits with Monotone Payoff Functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Works on implicit regularization have studied gradient trajectories duringthe optimization process to explain why deep networks favor certain kinds ofsolutions over others. In deep linear networks, it has been shown that gradientdescent implicitly regularizes toward low-rank solutions on matrixcompletion/factorization tasks. Adding depth not only improves performance onthese tasks but also acts as an accelerative pre-conditioning that furtherenhances this bias towards low-rankedness. Inspired by this, we propose anexplicit penalty to mirror this implicit bias which only takes effect withcertain adaptive gradient optimizers (e.g. Adam). This combination can enable adegenerate single-layer network to achieve low-rank approximations withgeneralization error comparable to deep linear networks, making depth no longernecessary for learning. The single-layer network also performs competitively orout-performs various approaches for matrix completion over a range of parameterand data regimes despite its simplicity. Together with an optimizer's inductivebias, our findings suggest that explicit regularization can play a role indesigning different, desirable forms of regularization and that a more nuancedunderstanding of this interplay may be necessary.", "output": "Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many scientific and industrial applications require joint optimization ofmultiple, potentially competing objectives. Multi-objective Bayesianoptimization (MOBO) is a sample-efficient framework for identifyingPareto-optimal solutions. We show a natural connection between non-dominatedsolutions and the highest multivariate rank, which coincides with the outermostlevel line of the joint cumulative distribution function (CDF). We propose theCDF indicator, a Pareto-compliant metric for evaluating the quality ofapproximate Pareto sets that complements the popular hypervolume indicator. Atthe heart of MOBO is the acquisition function, which determines the nextcandidate to evaluate by navigating the best compromises among the objectives.Multi-objective acquisition functions that rely on box decomposition of theobjective space, such as the expected hypervolume improvement (EHVI) andentropy search, scale poorly to a large number of objectives. We propose anacquisition function, called BOtied, based on the CDF indicator. BOtied can beimplemented efficiently with copulas, a statistical tool for modeling complex,high-dimensional distributions. We benchmark BOtied against common acquisitionfunctions, including EHVI and random scalarization (ParEGO), in a series ofsynthetic and real-data experiments. BOtied performs on par with the baselinesacross datasets and metrics while being computationally efficient.", "output": "BOtied: Multi-objective Bayesian optimization with tied multivariate ranks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The class imbalance problem can cause machine learning models to produce anundesirable performance on the minority class as well as the whole dataset.Using data augmentation techniques to increase the number of samples is one wayto tackle this problem. We introduce a novel counterfactual data augmentationby verb replacement for the identification of medical claims. In addition, weinvestigate the impact of this method and compare it with 3 other dataaugmentation techniques, showing that the proposed method can result in asignificant (relative) improvement in the minority class.", "output": "CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating Class Imbalance in Causal Claim Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Perception is crucial in the realm of autonomous driving systems, wherebird's eye view (BEV)-based architectures have recently reachedstate-of-the-art performance. The desirability of self-supervisedrepresentation learning stems from the expensive and laborious process ofannotating 2D and 3D data. Although previous research has investigatedpretraining methods for both LiDAR and camera-based 3D object detection, aunified pretraining framework for multimodal BEV perception is missing. In thisstudy, we introduce CALICO, a novel framework that applies contrastiveobjectives to both LiDAR and camera backbones. Specifically, CALICOincorporates two stages: point-region contrast (PRC) and region-awaredistillation (RAD). PRC better balances the region- and scene-levelrepresentation learning on the LiDAR modality and offers significantperformance improvement compared to existing methods. RAD effectively achievescontrastive distillation on our self-trained teacher model. CALICO's efficacyis substantiated by extensive evaluations on 3D object detection and BEV mapsegmentation tasks, where it delivers significant performance improvements.Notably, CALICO outperforms the baseline method by 10.5% and 8.6% on NDS andmAP. Moreover, CALICO boosts the robustness of multimodal 3D object detectionagainst adversarial attacks and corruption. Additionally, our framework can betailored to different backbones and heads, positioning it as a promisingapproach for multimodal BEV perception.", "output": "CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop the theory of Energy Conserving Descent (ECD) and introduceECDSep, a gradient-based optimization algorithm able to tackle convex andnon-convex optimization problems. The method is based on the novel ECDframework of optimization as physical evolution of a suitable chaoticenergy-conserving dynamical system, enabling analytic control of thedistribution of results - dominated at low loss - even for generichigh-dimensional problems with no symmetries. Compared to previous realizationsof this idea, we exploit the theoretical control to improve both the dynamicsand chaos-inducing elements, enhancing performance while simplifying thehyper-parameter tuning of the optimization algorithm targeted to differentclasses of problems. We empirically compare with popular optimization methodssuch as SGD, Adam and AdamW on a wide range of machine learning problems,finding competitive or improved performance compared to the best among them oneach task. We identify limitations in our analysis pointing to possibilitiesfor additional improvements.", "output": "Improving Energy Conserving Descent for Machine Learning: Theory and Practice."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we introduce a novel, probabilistic viewpoint on adversarialexamples, achieved through box-constrained Langevin Monte Carlo (LMC).Proceeding from this perspective, we develop an innovative approach forgenerating semantics-aware adversarial examples in a principled manner. Thismethodology transcends the restriction imposed by geometric distance, insteadopting for semantic constraints. Our approach empowers individuals toincorporate their personal comprehension of semantics into the model. Throughhuman evaluation, we validate that our semantics-aware adversarial examplesmaintain their inherent meaning. Experimental findings on the MNIST and SVHNdatasets demonstrate that our semantics-aware adversarial examples caneffectively circumvent robust adversarial training methods tailored fortraditional adversarial attacks.", "output": "Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion-based generative models have achieved remarkable success in variousdomains. It trains a model on denoising tasks that encompass different noiselevels simultaneously, representing a form of multi-task learning (MTL).However, analyzing and improving diffusion models from an MTL perspectiveremains under-explored. In particular, MTL can sometimes lead to the well-knownphenomenon of $textit{negative transfer}$, which results in the performancedegradation of certain tasks due to conflicts between tasks. In this paper, weaim to analyze diffusion training from an MTL standpoint, presenting two keyobservations: $textbf{(O1)}$ the task affinity between denoising tasksdiminishes as the gap between noise levels widens, and $textbf{(O2)}$ negativetransfer can arise even in the context of diffusion training. Building uponthese observations, our objective is to enhance diffusion training bymitigating negative transfer. To achieve this, we propose leveraging existingMTL methods, but the presence of a huge number of denoising tasks makes thiscomputationally expensive to calculate the necessary per-task loss or gradient.To address this challenge, we propose clustering the denoising tasks into smalltask clusters and applying MTL methods to them. Specifically, based on$textbf{(O2)}$, we employ interval clustering to enforce temporal proximityamong denoising tasks within clusters. We show that interval clustering can besolved with dynamic programming and utilize signal-to-noise ratio, timestep,and task affinity for clustering objectives. Through this, our approachaddresses the issue of negative transfer in diffusion models by allowing forefficient computation of MTL methods. We validate the proposed clustering andits integration with MTL methods through various experiments, demonstratingimproved sample quality of diffusion models.", "output": "Addressing Negative Transfer in Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Datasets often have their intrinsic symmetries, and particular deep-learningmodels called equivariant or invariant models have been developed to exploitthese symmetries. However, if some or all of these symmetries are onlyapproximate, which frequently happens in practice, these models may besuboptimal due to the architectural restrictions imposed on them. We tacklethis issue of approximate symmetries in a setup where symmetries are mixed,i.e., they are symmetries of not single but multiple different types and thedegree of approximation varies across these types. Instead of proposing a newarchitectural restriction as in most of the previous approaches, we present aregularizer-based method for building a model for a dataset with mixedapproximate symmetries. The key component of our method is what we callequivariance regularizer for a given type of symmetries, which measures howmuch a model is equivariant with respect to the symmetries of the type. Ourmethod is trained with these regularizers, one per each symmetry type, and thestrength of the regularizers is automatically tuned during training, leading tothe discovery of the approximation levels of some candidate symmetry typeswithout explicit supervision. Using synthetic function approximation and motionforecasting tasks, we demonstrate that our method achieves better accuracy thanprior approaches while discovering the approximate symmetry levels correctly.", "output": "Regularizing Towards Soft Equivariance Under Mixed Symmetries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce an efficient and robust auto-tuning framework for hyperparameterselection in dimension reduction (DR) algorithms, focusing on large-scaledatasets and arbitrary performance metrics. By leveraging Bayesian optimization(BO) with a surrogate model, our approach enables efficient hyperparameterselection with multi-objective trade-offs and allows us to perform data-drivensensitivity analysis. By incorporating normalization and subsampling, theproposed framework demonstrates versatility and efficiency, as shown inapplications to visualization techniques such as t-SNE and UMAP. We evaluateour results on various synthetic and real-world datasets using multiple qualitymetrics, providing a robust and efficient solution for hyperparameter selectionin DR algorithms.", "output": "Efficient and Robust Bayesian Selection of Hyperparameters in Dimension Reduction for Visualization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional Neural Networks (ConvNets) usually rely on edge/shapeinformation to classify images. Visualization methods developed over the lastdecade confirm that ConvNets rely on edge information. We investigatesituations where the ConvNet needs to rely on image intensity in addition toshape. We show that the ConvNet relies on image intensity information usingvisualization.", "output": "How Do ConvNets Understand Image Intensity?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we develop the randomized Sharded Bayesian Additive RegressionTrees (SBT) model. We introduce a randomization auxiliary variable and asharding tree to decide partitioning of data, and fit each partition componentto a sub-model using Bayesian Additive Regression Tree (BART). By observingthat the optimal design of a sharding tree can determine optimal sharding forsub-models on a product space, we introduce an intersection tree structure tocompletely specify both the sharding and modeling using only tree structures.In addition to experiments, we also derive the theoretical optimal weights forminimizing posterior contractions and prove the worst-case complexity of SBT.", "output": "Sharded Bayesian Additive Regression Trees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The emergence of various notions of ``consistency'' in diffusion models hasgarnered considerable attention and helped achieve improved sample quality,likelihood estimation, and accelerated sampling. Although similar concepts havebeen proposed in the literature, the precise relationships among them remainunclear. In this study, we establish theoretical connections between threerecent ``consistency'' notions designed to enhance diffusion models fordistinct objectives. Our insights offer the potential for a more comprehensiveand encompassing framework for consistency-type models.", "output": "On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamical systems with complex behaviours, e.g. immune system cellsinteracting with a pathogen, are commonly modelled by splitting the behaviourinto different regimes, or modes, each with simpler dynamics, and then learningthe switching behaviour from one mode to another. Switching Dynamical Systems(SDS) are a powerful tool that automatically discovers these modes andmode-switching behaviour from time series data. While effective, these methodsfocus on independent objects, where the modes of one object are independent ofthe modes of the other objects. In this paper, we focus on the more generalinteracting object setting for switching dynamical systems, where theper-object dynamics also depends on an unknown and dynamically changing subsetof other objects and their modes. To this end, we propose a novel graph-basedapproach for switching dynamical systems, GRAph Switching dynamical Systems(GRASS), in which we use a dynamic graph to characterize interactions betweenobjects and learn both intra-object and inter-object mode-switching behaviour.We introduce two new datasets for this setting, a synthesized ODE-drivenparticles dataset and a real-world Salsa Couple Dancing dataset. Experimentsshow that GRASS can consistently outperforms previous state-of-the-art methods.", "output": "Graph Switching Dynamical Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "E-commerce websites (e.g. Amazon) have a plethora of structured andunstructured information (text and images) present on the product pages.Sellers often either don't label or mislabel values of the attributes (e.g.color, size etc.) for their products. Automatically identifying these attributevalues from an eCommerce product page that contains both text and images is achallenging task, especially when the attribute value is not explicitlymentioned in the catalog. In this paper, we present a scalable solution forthis problem where we pose attribute extraction problem as a question-answeringtask, which we solve using textbf{MXT}, consisting of three key components:(i) textbf{M}AG (Multimodal Adaptation Gate), (ii) textbf{X}ception network,and (iii) textbf{T}5 encoder-decoder. Our system consists of a generativemodel that emph{generates} attribute-values for a given product by using bothtextual and visual characteristics (e.g. images) of the product. We show thatour system is capable of handling zero-shot attribute prediction (whenattribute value is not seen in training data) and value-absent prediction (whenattribute value is not mentioned in the text) which are missing in traditionalclassification-based and NER-based models respectively. We have trained ourmodels using distant supervision, removing dependency on human labeling, thusmaking them practical for real-world applications. With this framework, we areable to train a single model for 1000s of (product-type, attribute) pairs, thusreducing the overhead of training and maintaining separate models. Extensiveexperiments on two real world datasets show that our framework improves theabsolute recall@90P by 10.16% and 6.9% from the existing state of the artmodels. In a popular e-commerce store, we have deployed our models for 1000s of(product-type, attribute) pairs.", "output": "Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pretrained code language models have enabled great progress towards programsynthesis. However, common approaches only consider in-file local context andthus miss information and constraints imposed by other parts of the codebaseand its external dependencies. Existing code completion benchmarks also lacksuch context. To resolve these restrictions we curate a new dataset ofpermissively licensed Python packages that includes full projects and theirdependencies and provide tools to extract non-local information with the helpof program analyzers. We then focus on the task of function call argumentcompletion which requires predicting the arguments to function calls. We showthat existing code completion models do not yield good results on ourcompletion task. To better solve this task, we query a program analyzer forinformation relevant to a given function call, and consider ways to provide theanalyzer results to different code completion models during inference andtraining. Our experiments show that providing access to the functionimplementation and function usages greatly improves the argument completionperformance. Our ablation study provides further insights on how differenttypes of information available from the program analyzer and different ways ofincorporating the information affect the model performance.", "output": "Better Context Makes Better Code Language Models: A Case Study on Function Call Argument Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Propensity scores are commonly used to balance observed covariates whileestimating treatment effects. Estimates obtained through propensity scoreweighing can be biased when the propensity score model cannot learn the truetreatment assignment mechanism. We argue that the probabilistic output of alearned propensity score model should be calibrated, i.e. a predictivetreatment probability of 90% should correspond to 90% of individuals beingassigned the treatment group. We propose simple recalibration techniques toensure this property. We investigate the theoretical properties of a calibratedpropensity score model and its role in unbiased treatment effect estimation. Wedemonstrate improved causal effect estimation with calibrated propensity scoresin several tasks including high-dimensional genome-wide association studies,where we also show reduced computational requirements when calibration isapplied to simpler propensity score models.", "output": "Calibrated Propensity Scores for Causal Effect Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tensor time series (TTS) data, a generalization of one-dimensional timeseries on a high-dimensional space, is ubiquitous in real-world scenarios,especially in monitoring systems involving multi-source spatio-temporal data(e.g., transportation demands and air pollutants). Compared to modeling timeseries or multivariate time series, which has received much attention andachieved tremendous progress in recent years, tensor time series has been paidless effort. Properly coping with the tensor time series is a much morechallenging task, due to its high-dimensional and complex inner structure. Inthis paper, we develop a novel TTS forecasting framework, which seeks toindividually model each heterogeneity component implied in the time, thelocation, and the source variables. We name this framework as GMRL, short forGaussian Mixture Representation Learning. Experiment results on two real-worldTTS datasets verify the superiority of our approach compared with thestate-of-the-art baselines.", "output": "Learning Gaussian Mixture Representations for Tensor Time Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attention networks such as transformers have achieved state-of-the-artperformance in many domains. These networks rely heavily on the dot productattention operator, which computes the similarity between two points by takingtheir inner product. However, the inner product does not explicitly model thecomplex structural properties of real world datasets, such as hierarchiesbetween data points. To remedy this, we introduce cone attention, a drop-inreplacement for dot product attention based on hyperbolic entailment cones.Cone attention associates two points by the depth of their lowest commonancestor in a hierarchy defined by hyperbolic cones, which intuitively measuresthe divergence of two points and gives a hierarchy aware similarity score. Wetest cone attention on a wide variety of models and tasks and show that itimproves task-level performance over dot product attention and other baselines,and is able to match dot-product attention with significantly fewer parameters.Our results suggest that cone attention is an effective way to capturehierarchical relationships when calculating attention.", "output": "Coneheads: Hierarchy Aware Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tensor decomposition is a fundamental method used in various areas to dealwith high-dimensional data. emph{Tensor power method} (TPM) is one of thewidely-used techniques in the decomposition of tensors. This paper presents anovel tensor power method for decomposing arbitrary order tensors, whichovercomes limitations of existing approaches that are often restricted tolower-order (less than $3$) tensors or require strong assumptions about theunderlying data structure. We apply sketching method, and we are able toachieve the running time of $widetilde{O}(n^{p-1})$, on the power $p$ anddimension $n$ tensor. We provide a detailed analysis for any $p$-th ordertensor, which is never given in previous works.", "output": "Faster Robust Tensor Power Method for Arbitrary Order."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical imaging informatics is a rapidly growing field that combines theprinciples of medical imaging and informatics to improve the acquisition,management, and interpretation of medical images. This chapter introduces thebasic concepts of medical imaging informatics, including image processing,feature engineering, and machine learning. It also discusses the recentadvancements in computer vision and deep learning technologies and how they areused to develop new quantitative image markers and prediction models fordisease detection, diagnosis, and prognosis prediction. By covering the basicknowledge of medical imaging informatics, this chapter provides a foundationfor understanding the role of informatics in medicine and its potential impacton patient care.", "output": "Introduction to Medical Imaging Informatics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning (CL) is an important technique to allow artificial neuralnetworks to work in open environments. CL enables a system to learn new taskswithout severe interference to its performance on old tasks, i.e., overcome theproblems of catastrophic forgetting. In joint learning, it is well known thatthe out-of-distribution (OOD) problem caused by intentional attacks orenvironmental perturbations will severely impair the ability of networks togeneralize. In this work, we reported a special form of catastrophic forgettingraised by the OOD problem in continual learning settings, and we named itout-of-distribution forgetting (OODF). In continual image classification tasks,we found that for a given category, introducing an intra-class distributionshift significantly impaired the recognition accuracy of CL methods for thatcategory during subsequent learning. Interestingly, this phenomenon is specialfor CL as the same level of distribution shift had only negligible effects inthe joint learning scenario. We verified that CL methods without dedicatingsubnetworks for individual tasks are all vulnerable to OODF. Moreover, OODFdoes not depend on any specific way of shifting the distribution, suggesting itis a risk for CL in a wide range of circumstances. Taken together, our workidentified an under-attended risk during CL, highlighting the importance ofdeveloping approaches that can overcome OODF.", "output": "Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has recently allowed leveraging large datasetsof unlabeled speech signals to reach impressive performance on speech tasksusing only small amounts of annotated data. The high number of proposedapproaches fostered the need and rise of extended benchmarks that evaluatetheir performance on a set of downstream tasks exploring various aspects of thespeech signal. However, and while the number of considered tasks has beengrowing, most rely upon a single decoding architecture that maps the frozen SSLrepresentations to the downstream labels. This work investigates the robustnessof such benchmarking results to changes in the decoder architecture.Interestingly, it appears that varying the architecture of the downstreamdecoder leads to significant variations in the leaderboards of most tasks.Concerningly, our study reveals that benchmarking using limited decoders maycause a counterproductive increase in the sizes of the developed SSL models.", "output": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "MindBigData 2023 MNIST-8B is the largest, to date (June 1st 2023), brainsignals open dataset created for Machine Learning, based on EEG signals from asingle subject captured using a custom 128 channels device, replicating thefull 70,000 digits from Yaan LeCun et all MNIST dataset. The brain signals werecaptured while the subject was watching the pixels of the original digits oneby one on a screen and listening at the same time to the spoken number 0 to 9from the real label. The data, collection procedures, hardware and softwarecreated are described in detail, background extra information and other relateddatasets can be found at our previous paper MindBigData 2022: A Large Datasetof Brain Signals.", "output": "MindBigData 2023 MNIST-8B The 8 billion datapoints Multimodal Dataset of Brain Signals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)has emerged as a highly successful approach, with training only a small numberof parameters without sacrificing performance and becoming the de-factolearning paradigm with the increasing size of PLMs. However, existing PEFTmethods are not memory-efficient, because they still require caching most ofthe intermediate activations for the gradient calculation, akin to fine-tuning.One effective way to reduce the activation memory is to apply a reversiblemodel, so the intermediate activations are not necessary to be cached and canbe recomputed. Nevertheless, modifying a PLM to its reversible variant withPEFT is not straightforward, since the reversible model has a distinctarchitecture from the currently released PLMs. In this paper, we firstinvestigate what is a key factor for the success of existing PEFT methods, andrealize that it's essential to preserve the PLM's starting point wheninitializing a PEFT method. With this finding, we propose memory-efficientfine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM'sstarting point and making it reversible without additional pre-training. Weevaluate MEFT on the GLUE benchmark and five question-answering tasks withvarious backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces theactivation memory up to 84% of full fine-tuning with a negligible amount oftrainable parameters. Moreover, MEFT achieves the same score on GLUE and acomparable score on the question-answering tasks as full fine-tuning.", "output": "Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-Supervised Learning (SSL) has allowed leveraging large amounts ofunlabeled speech data to improve the performance of speech recognition modelseven with small annotated datasets. Despite this, speech SSL representationsmay fail while facing an acoustic mismatch between the pretraining and targetdatasets. To address this issue, we propose a novel supervised domainadaptation method, designed for cases exhibiting such a mismatch in acousticdomains. It consists in applying properly calibrated data augmentations on alarge clean dataset, bringing it closer to the target domain, and using it aspart of an initial fine-tuning stage. Augmentations are automatically selectedthrough the minimization of a conditional-dependence estimator, based on thetarget dataset. The approach is validated during an oracle experiment withcontrolled distortions and on two amateur-collected low-resource domains,reaching better performances compared to the baselines in both cases.", "output": "Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on manygraph analysis tasks such as node classification and link prediction. However,important unsupervised problems on graphs, such as graph clustering, haveproved more resistant to advances in GNNs. Graph clustering has the sameoverall goal as node pooling in GNNs - does this mean that GNN pooling methodsdo a good job at clustering graphs?Surprisingly, the answer is no - current GNN pooling methods often fail torecover the cluster structure in cases where simple baselines, such as k-meansapplied on learned representations, work well. We investigate further bycarefully designing a set of experiments to study different signal-to-noisescenarios both in graph structure and attribute data. To address these methods'poor performance in clustering, we introduce Deep Modularity Networks (DMoN),an unsupervised pooling method inspired by the modularity measure of clusteringquality, and show how it tackles recovery of the challenging clusteringstructure of real-world graphs. Similarly, on real-world data, we show thatDMoN produces high quality clusters which correlate strongly with ground truthlabels, achieving state-of-the-art results with over 40% improvement over otherpooling methods across different metrics.", "output": "Graph Clustering with Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scaling methods have long been utilized to simplify and clusterhigh-dimensional data. However, the general latent spaces across all predefinedgroups derived from these methods sometimes do not fall into researchers'interest regarding specific patterns within groups. To tackle this issue, weadopt an emerging analysis approach called contrastive learning. We contributeto this growing field by extending its ideas to multiple correspondenceanalysis (MCA) in order to enable an analysis of data often encountered bysocial scientists -- containing binary, ordinal, and nominal variables. Wedemonstrate the utility of contrastive MCA (cMCA) by analyzing two differentsurveys of voters in the U.S. and U.K. Our results suggest that, first, cMCAcan identify substantively important dimensions and divisions among subgroupsthat are overlooked by traditional methods; second, for other cases, cMCA canderive latent traits that emphasize subgroups seen moderately in those derivedby traditional methods.", "output": "Contrastive Multiple Correspondence Analysis (cMCA): Using Contrastive Learning to Identify Latent Subgroups in Political Parties."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "I study a stochastic multi-arm bandit problem where rewards are subject toadversarial corruption. I propose a novel attack strategy that manipulates alearner employing the UCB algorithm into pulling some non-optimal target arm $T- o(T)$ times with a cumulative cost that scales as $widehat{O}(sqrt{logT})$, where $T$ is the number of rounds. I also prove the first lower bound onthe cumulative attack cost. The lower bound matches the upper bound up to$O(log log T)$ factors, showing the proposed attack strategy to be nearoptimal.", "output": "Near Optimal Adversarial Attack on UCB Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-time perception requires planned resource utilization. Computationalplanning in real-time perception is governed by two considerations -- accuracyand latency. There exist run-time decisions (e.g. choice of input resolution)that induce tradeoffs affecting performance on a given hardware, arising fromintrinsic (content, e.g. scene clutter) and extrinsic (system, e.g. resourcecontention) characteristics.Earlier runtime execution frameworks employed rule-based decision algorithmsand operated with a fixed algorithm latency budget to balance these concerns,which is sub-optimal and inflexible. We propose Chanakya, a learned approximateexecution framework that naturally derives from the streaming perceptionparadigm, to automatically learn decisions induced by these tradeoffs instead.Chanakya is trained via novel rewards balancing accuracy and latencyimplicitly, without approximating either objectives. Chanakya simultaneouslyconsiders intrinsic and extrinsic context, and predicts decisions in a flexiblemanner. Chanakya, designed with low overhead in mind, outperformsstate-of-the-art static and dynamic execution policies on public datasets onboth server GPUs and edge devices.", "output": "Learning Runtime Decisions for Adaptive Real-Time Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given an algorithmic predictor that is accurate on some source populationconsisting of strategic human decision subjects, will it remain accurate if thepopulation respond to it? In our setting, an agent or a user corresponds to asample $(X,Y)$ drawn from a distribution $cal{D}$ and will face a model $h$and its classification result $h(X)$. Agents can modify $X$ to adapt to $h$,which will incur a distribution shift on $(X,Y)$. Our formulation is motivatedby applications where the deployed machine learning models are subjected tohuman agents, and will ultimately face responsive and interactive datadistributions. We formalize the discussions of the transferability of a modelby studying how the performance of the model trained on the available sourcedistribution (data) would translate to the performance on its induced domain.We provide both upper bounds for the performance gap due to the induced domainshift, as well as lower bounds for the trade-offs that a classifier has tosuffer on either the source training distribution or the induced targetdistribution. We provide further instantiated analysis for two popular domainadaptation settings, including covariate shift and target shift.", "output": "Model Transferability With Responsive Decision Subjects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks have become increasingly of interest in dynamical systemprediction, but out-of-distribution generalization and long-term stabilitystill remains challenging. In this work, we treat the domain parameters ofdynamical systems as factors of variation of the data generating process. Byleveraging ideas from supervised disentanglement and causal factorization, weaim to separate the domain parameters from the dynamics in the latent space ofgenerative models. In our experiments we model dynamics both in phase space andin video sequences and conduct rigorous OOD evaluations. Results indicate thatdisentangled VAEs adapt better to domain parameters spaces that were notpresent in the training data. At the same time, disentanglement can improve thelong-term and out-of-distribution predictions of state-of-the-art models invideo sequences.", "output": "Disentangled Generative Models for Robust Prediction of System Dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exponential tilting is a technique commonly used in fields such asstatistics, probability, information theory, and optimization to createparametric distribution shifts. Despite its prevalence in related fields,tilting has not seen widespread use in machine learning. In this work, we aimto bridge this gap by exploring the use of tilting in risk minimization. Westudy a simple extension to ERM -- tilted empirical risk minimization (TERM) --which uses exponential tilting to flexibly tune the impact of individuallosses. The resulting framework has several useful properties: We show thatTERM can increase or decrease the influence of outliers, respectively, toenable fairness or robustness; has variance-reduction properties that canbenefit generalization; and can be viewed as a smooth approximation to the tailprobability of losses. Our work makes rigorous connections between TERM andrelated objectives, such as Value-at-Risk, Conditional Value-at-Risk, anddistributionally robust optimization (DRO). We develop batch and stochasticfirst-order optimization methods for solving TERM, provide convergenceguarantees for the solvers, and show that the framework can be efficientlysolved relative to common alternatives. Finally, we demonstrate that TERM canbe used for a multitude of applications in machine learning, such as enforcingfairness between subgroups, mitigating the effect of outliers, and handlingclass imbalance. Despite the straightforward modification TERM makes totraditional ERM objectives, we find that the framework can consistentlyoutperform ERM and deliver competitive performance with state-of-the-art,problem-specific approaches.", "output": "On Tilted Losses in Machine Learning: Theory and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Materialized model query aims to find the most appropriate materialized modelas the initial model for model reuse. It is the precondition of model reuse,and has recently attracted much attention. {Nonetheless, the existing methodssuffer from the need to provide source data, limited range of applications, andinefficiency since they do not construct a suitable metric to measure thetarget-related knowledge of materialized models. To address this, we presenttextsf{MMQ}, a source-data free, general, efficient, and effectivematerialized model query framework.} It uses a Gaussian mixture-based metriccalled separation degree to rank materialized models. For each materializedmodel, textsf{MMQ} first vectorizes the samples in the target dataset intoprobability vectors by directly applying this model, then utilizes Gaussiandistribution to fit for each class of probability vectors, and finally usesseparation degree on the Gaussian distributions to measure the target-relatedknowledge of the materialized model. Moreover, we propose an improvedtextsf{MMQ} (textsf{I-MMQ}), which significantly reduces the query time whileretaining the query performance of textsf{MMQ}. Extensive experiments on arange of practical model reuse workloads demonstrate the effectiveness andefficiency of textsf{MMQ}.", "output": "Finding Materialized Models for Model Reuse."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of determining the underlying dynamics of a system when onlygiven data of its state over time has challenged scientists for decades. Inthis paper, the approach of using machine learning to model the {em updates}of the phase space variables is introduced; this is done as a function of thephase space variables. (More generally, the modeling is done over the jet spaceof the variables.) This approach is shown to accurately replicate the dynamicsfor the examples of the harmonic oscillator, the pendulum, and the Duffingoscillator; the underlying differential equation is also accurately recoveredin each example. In addition, the results in no way depend on how the data issampled over time (i.e., regularly or irregularly). It is demonstrated thatthis approach (named \"FJet\") is similar to the model resulting from a Taylorseries expansion of the Runge-Kutta (RK) numerical integration scheme. Thisanalogy confers the advantage of explicitly revealing the appropriate functionsto use in the modeling, as well as revealing the error estimate of the updates.Thus, this new approach can be thought of as a way to determine thecoefficients of an RK scheme by machine learning. Finally, it is shown in theundamped harmonic oscillator example that the stability of the updates isstable for $10^9$ times longer than with $4$th-order RK.", "output": "Extracting Dynamical Models from Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent times, deep neural networks achieved outstanding predictiveperformance on various classification and pattern recognition tasks. However,many real-world prediction problems have ordinal response variables, and thisordering information is ignored by conventional classification losses such asthe multi-category cross-entropy. Ordinal regression methods for deep neuralnetworks address this. One such method is the CORAL method, which is based onan earlier binary label extension framework and achieves rank consistency amongits output layer tasks by imposing a weight-sharing constraint. However, whileearlier experiments showed that CORAL's rank consistency is beneficial forperformance, it is limited by a weight-sharing constraint in a neural network'sfully connected output layer, which may restrict the expressiveness andcapacity of a network trained using CORAL. We propose a new method forrank-consistent ordinal regression without this limitation. Our rank-consistentordinal regression framework (CORN) achieves rank consistency by a noveltraining scheme. This training scheme uses conditional training sets to obtainthe unconditional rank probabilities through applying the chain rule forconditional probability distributions. Experiments on various datasetsdemonstrate the efficacy of the proposed method to utilize the ordinal targetinformation, and the absence of the weight-sharing restriction improves theperformance substantially compared to the CORAL reference approach.Additionally, the suggested CORN method is not tied to any specificarchitecture and can be utilized with any deep neural network classifier totrain it for ordinal regression tasks.", "output": "Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, graph neural networks (GNNs), as the backbone of graph-basedmachine learning, demonstrate great success in various domains (e.g.,e-commerce). However, the performance of GNNs is usually unsatisfactory due tothe highly sparse and irregular graph-based operations. To this end, we proposeTC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units(TCUs). The core idea is to reconcile the \"Sparse\" GNN computation with thehigh-performance \"Dense\" TCUs. Specifically, we conduct an in-depth analysis ofthe sparse operations in mainstream GNN computing frameworks. We introduce anovel sparse graph translation technique to facilitate TCU processing of thesparse GNN workload. We implement an effective CUDA core and TCU collaborationdesign to fully utilize GPU resources. We integrate TC-GNN with the PyTorchframework for high programmability. Rigorous experiments show an average of1.70X speedup over the state-of-the-art DGL framework across various models anddatasets.", "output": "TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpretability of neural networks and their underlying theoretical behaviorremain an open field of study even after the great success of their practicalapplications, particularly with the emergence of deep learning. In this work,NN2Poly is proposed: a theoretical approach to obtain an explicit polynomialmodel that provides an accurate representation of an already trainedfully-connected feed-forward artificial neural network (a multilayer perceptronor MLP). This approach extends a previous idea proposed in the literature,which was limited to single hidden layer networks, to work with arbitrarilydeep MLPs in both regression and classification tasks. The objective of thispaper is to achieve this by using a Taylor expansion on the activationfunction, at each layer, and then using several combinatorial properties tocalculate the coefficients of the desired polynomials. Discussion is presentedon the main computational challenges of this method, and the way to overcomethem by imposing certain constraints during the training phase. Finally,simulation experiments as well as an application to a real data set arepresented to demonstrate the effectiveness of the proposed method.", "output": "NN2Poly: A polynomial representation for deep feed-forward artificial neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the high cost of communication, federated learning (FL) systems needto sample a subset of clients that are involved in each round of training. As aresult, client sampling plays an important role in FL systems as it affects theconvergence rate of optimization algorithms used to train machine learningmodels. Despite its importance, there is limited work on how to sample clientseffectively. In this paper, we cast client sampling as an online learning taskwith bandit feedback, which we solve with an online stochastic mirror descent(OSMD) algorithm designed to minimize the sampling variance. We thentheoretically show how our sampling method can improve the convergence speed ofoptimization algorithms. To handle the tuning parameters in OSMD that depend onthe unknown problem parameters, we use the online ensemble method and doublingtrick. We prove a dynamic regret bound relative to any sampling sequence. Theregret bound depends on the total variation of the comparator sequence, whichnaturally captures the intrinsic difficulty of the problem. To the best of ourknowledge, these theoretical contributions are new and the proof technique isof independent interest. Through both synthetic and real data experiments, weillustrate advantages of the proposed client sampling algorithm over the widelyused uniform sampling and existing online learning based sampling strategies.The proposed adaptive sampling procedure is applicable beyond the FL problemstudied here and can be used to improve the performance of stochasticoptimization procedures such as stochastic gradient descent and stochasticcoordinate descent.", "output": "Adaptive Client Sampling in Federated Learning via Online Learning with Bandit Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mobile and edge computing devices for always-on classification tasks requireenergy-efficient neural network architectures. In this paper we present severalchanges to neural architecture searches (NAS) that improve the chance ofsuccess in practical situations. Our search simultaneously optimizes fornetwork accuracy, energy efficiency and memory usage. We benchmark theperformance of our search on real hardware, but since running thousands oftests with real hardware is difficult we use a random forest model to roughlypredict the energy usage of a candidate network. We present a search strategythat uses both Bayesian and regularized evolutionary search with particleswarms, and employs early-stopping to reduce the computational burden. Oursearch, evaluated on a sound-event classification dataset based upon AudioSet,results in an order of magnitude less energy per inference and a much smallermemory footprint than our baseline MobileNetV1/V2 implementations whileslightly improving task accuracy. We also demonstrate how combining a 2Dspectrogram with a convolution with many filters causes a computationalbottleneck for audio classification and that alternative approaches reduce thecomputational burden but sacrifice task accuracy.", "output": "Neural Architecture Search for Energy Efficient Always-on Audio Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) haverecently emerged as a promising direction for tuning Deep Learning methods.However, existing methods suffer from a sub-optimal allocation of the HPObudget to the hyperparameter configurations. In this work, we introduce DyHPO,a Bayesian Optimization method that learns to decide which hyperparameterconfiguration to train further in a dynamic race among all feasibleconfigurations. We propose a new deep kernel for Gaussian Processes that embedsthe learning curve dynamics, and an acquisition function that incorporatesmulti-budget information. We demonstrate the significant superiority of DyHPOagainst state-of-the-art hyperparameter optimization methods throughlarge-scale experiments comprising 50 datasets (Tabular, Image, NLP) anddiverse architectures (MLP, CNN/NAS, RNN).", "output": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the robust interpolation problem of arbitrary data distributionssupported on a bounded space and propose a two-fold law of robustness. Robustinterpolation refers to the problem of interpolating $n$ noisy training datapoints in $mathbb{R}^d$ by a Lipschitz function. Although this problem hasbeen well understood when the samples are drawn from an isoperimetrydistribution, much remains unknown concerning its performance under generic oreven the worst-case distributions. We prove a Lipschitzness lower bound$Omega(sqrt{n/p})$ of the interpolating neural network with $p$ parameters onarbitrary data distributions. With this result, we validate the law ofrobustness conjecture in prior work by Bubeck, Li, and Nagaraj on two-layerneural networks with polynomial weights. We then extend our result to arbitraryinterpolating approximators and prove a Lipschitzness lower bound$Omega(n^{1/d})$ for robust interpolation. Our results demonstrate a two-foldlaw of robustness: i) we show the potential benefit of overparametrization forsmooth data interpolation when $n=mathrm{poly}(d)$, and ii) we disprove thepotential existence of an $O(1)$-Lipschitz robust interpolating function when$n=exp(omega(d))$.", "output": "A Law of Robustness beyond Isoperimetry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces CO3 -- an algorithm for communication-efficientfederated Deep Neural Network (DNN) training. CO3 takes its name from threeprocessing applied which reduce the communication load when transmitting thelocal DNN gradients from the remote users to the Parameter Server. Namely: (i)gradient quantization through floating-point conversion, (ii) losslesscompression of the quantized gradient, and (iii) quantization error correction.We carefully design each of the steps above to assure good training performanceunder a constraint on the communication rate. In particular, in steps (i) and(ii), we adopt the assumption that DNN gradients are distributed according to ageneralized normal distribution, which is validated numerically in the paper.For step (iii), we utilize an error feedback with memory decay mechanism tocorrect the quantization error introduced in step (i). We argue that the memorydecay coefficient, similarly to the learning rate, can be optimally tuned toimprove convergence. A rigorous convergence analysis of the proposed CO3 withSGD is provided. Moreover, with extensive simulations, we show that CO3 offersimproved performance when compared with existing gradient compression schemesin the literature which employ sketching and non-uniform quantization of thelocal gradients.", "output": "How to Attain Communication-Efficient DNN Training? Convert, Compress, Correct."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents mixed variational flows (MixFlows), a new variationalfamily that consists of a mixture of repeated applications of a map to aninitial reference distribution. First, we provide efficient algorithms fori.i.d. sampling, density evaluation, and unbiased ELBO estimation. We then showthat MixFlows have MCMC-like convergence guarantees when the flow map isergodic and measure-preserving, and provide bounds on the accumulation of errorfor practical implementations where the flow map is approximated. Finally, wedevelop an implementation of MixFlows based on uncorrected discretizedHamiltonian dynamics combined with deterministic momentum refreshment.Simulated and real data experiments show that MixFlows can provide morereliable posterior approximations than several black-box normalizing flows, aswell as samples of comparable quality to those obtained from state-of-the-artMCMC methods.", "output": "MixFlows: principled variational inference via mixed flows."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wider adoption of neural networks in many critical domains such as financeand healthcare is being hindered by the need to explain their predictions andto impose additional constraints on them. Monotonicity constraint is one of themost requested properties in real-world scenarios and is the focus of thispaper. One of the oldest ways to construct a monotonic fully connected neuralnetwork is to constrain signs on its weights. Unfortunately, this constructiondoes not work with popular non-saturated activation functions as it can onlyapproximate convex functions. We show this shortcoming can be fixed byconstructing two additional activation functions from a typical unsaturatedmonotonic activation function and employing each of them on the part ofneurons. Our experiments show this approach of building monotonic neuralnetworks has better accuracy when compared to other state-of-the-art methods,while being the simplest one in the sense of having the least number ofparameters, and not requiring any modifications to the learning procedure orpost-learning steps. Finally, we prove it can approximate any continuousmonotone function on a compact subset of $mathbb{R}^n$.", "output": "Constrained Monotonic Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study single-machine scheduling of jobs, each belonging to a job type thatdetermines its duration distribution. We start by analyzing the scenario wherethe type characteristics are known and then move to two learning scenarioswhere the types are unknown: non-preemptive problems, where each started jobmust be completed before moving to another job; and preemptive problems, wherejob execution can be paused in the favor of moving to a different job. In bothcases, we design algorithms that achieve sublinear excess cost, compared to theperformance with known types, and prove lower bounds for the non-preemptivecase. Notably, we demonstrate, both theoretically and through simulations, howpreemptive algorithms can greatly outperform non-preemptive ones when thedurations of different job types are far from one another, a phenomenon thatdoes not occur when the type durations are known.", "output": "On Preemption and Learning in Stochastic Scheduling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training, which is to enhance robustness against adversarialattacks, has received much attention because it is easy to generatehuman-imperceptible perturbations of data to deceive a given deep neuralnetwork. In this paper, we propose a new adversarial training algorithm that istheoretically well motivated and empirically superior to other existingalgorithms. A novel feature of the proposed algorithm is to apply moreregularization to data vulnerable to adversarial attacks than other existingregularization algorithms do. Theoretically, we show that our algorithm can beunderstood as an algorithm of minimizing the regularized empirical riskmotivated from a newly derived upper bound of the robust risk. Numericalexperiments illustrate that our proposed algorithm improves the generalization(accuracy on examples) and robustness (accuracy on adversarial attacks)simultaneously to achieve the state-of-the-art performance.", "output": "Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There has been a recent surge of interest in automating software engineeringtasks using deep learning. This paper addresses the problem of code generationwhere the goal is to generate target code given source code in a differentlanguage or a natural language description. Most of the state-of-the-art deeplearning models for code generation use training strategies primarily designedfor natural language. However, understanding and generating code requires amore rigorous comprehension of the code syntax and semantics. With thismotivation, we develop an encoder-decoder Transformer model where both theencoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively. We not only make the encoderstructure-aware by leveraging the source code's syntax tree and data flowgraph, but we also support the decoder in preserving the syntax and data flowof the target code by introducing two novel auxiliary tasks: AST (AbstractSyntax Tree) paths prediction and data flow prediction. To the best of ourknowledge, this is the first work to introduce a structure-aware Transformerdecoder that models both syntax and data flow to enhance the quality ofgenerated code. The proposed StructCoder model achieves state-of-the-artperformance on code translation and text-to-code generation tasks in theCodeXGLUE benchmark, and improves over baselines of similar size on the APPScode generation benchmark. Our code is publicly available at", "output": "StructCoder: Structure-Aware Transformer for Code Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nonlinear independent component analysis (ICA) aims to recover the underlyingindependent latent sources from their observable nonlinear mixtures. How tomake the nonlinear ICA model identifiable up to certain trivial indeterminaciesis a long-standing problem in unsupervised learning. Recent breakthroughsreformulate the standard independence assumption of sources as conditionalindependence given some auxiliary variables (e.g., class labels and/ordomain/time indexes) as weak supervision or inductive bias. However, nonlinearICA with unconditional priors cannot benefit from such developments. We explorean alternative path and consider only assumptions on the mixing process, suchas Structural Sparsity. We show that under specific instantiations of suchconstraints, the independent latent sources can be identified from theirnonlinear mixtures up to a permutation and a component-wise transformation,thus achieving nontrivial identifiability of nonlinear ICA without auxiliaryvariables. We provide estimation methods and validate the theoretical resultsexperimentally. The results on image data suggest that our conditions may holdin a number of practical data generating processes.", "output": "On the Identifiability of Nonlinear ICA: Sparsity and Beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Using the 20 questions estimation framework with query-dependent noise, westudy non-adaptive search strategies for a moving target over the unit cubewith unknown initial location and velocities under a piecewise constantvelocity model. In this search problem, there is an oracle who knows theinstantaneous location of the target at any time. Our task is to query theoracle as few times as possible to accurately estimate the location of thetarget at any specified time. We first study the case where the oracle's answerto each query is corrupted by discrete noise and then generalize our results tothe case of additive white Gaussian noise. In our formulation, the performancecriterion is the resolution, which is defined as the maximal $L_infty$distance between the true locations and estimated locations. We characterizethe minimal resolution of an optimal non-adaptive query procedure with a finitenumber of queries by deriving non-asymptotic and asymptotic bounds. Our boundsare tight in the first-order asymptotic sense when the number of queriessatisfies a certain condition and our bounds are tight in the strongersecond-order asymptotic sense when the target moves with a constant velocity.To prove our results, we relate the current problem to channel coding, borrowideas from finite blocklength information theory and construct bounds on thenumber of possible quantized target trajectories.", "output": "Resolution Limits of Non-Adaptive 20 Questions Search for a Moving Target."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a framework for the stability verification of Mixed-Integer LinearProgramming (MILP) representable control policies. This framework compares afixed candidate policy, which admits an efficient parameterization and can beevaluated at a low computational cost, against a fixed baseline policy, whichis known to be stable but expensive to evaluate. We provide sufficientconditions for the closed-loop stability of the candidate policy in terms ofthe worst-case approximation error with respect to the baseline policy, and weshow that these conditions can be checked by solving a Mixed-Integer QuadraticProgram (MIQP). Additionally, we demonstrate that an outer and innerapproximation of the stability region of the candidate policy can be computedby solving an MILP. The proposed framework is sufficiently general toaccommodate a broad range of candidate policies including ReLU Neural Networks(NNs), optimal solution maps of parametric quadratic programs, and ModelPredictive Control (MPC) policies. We also present an open-source toolbox inPython based on the proposed framework, which allows for the easy verificationof custom NN architectures and MPC formulations. We showcase the flexibilityand reliability of our framework in the context of a DC-DC power converter casestudy and investigate its computational complexity.", "output": "Stability Verification of Neural Network Controllers using Mixed-Integer Programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data imbalance is a common problem in machine learning that can have acritical effect on the performance of a model. Various solutions exist buttheir impact on the convergence of the learning dynamics is not understood.Here, we elucidate the significant negative impact of data imbalance onlearning, showing that the learning curves for minority and majority classesfollow sub-optimal trajectories when training with a gradient-based optimizer.This slowdown is related to the imbalance ratio and can be traced back to acompetition between the optimization of different classes. Our maincontribution is the analysis of the convergence of full-batch (GD) andstochastic gradient descent (SGD), and of variants that renormalize thecontribution of each per-class gradient. We find that GD is not guaranteed todecrease the loss for each class but that this problem can be addressed byperforming a per-class normalization of the gradient. With SGD, class imbalancehas an additional effect on the direction of the gradients: the minority classsuffers from a higher directional noise, which reduces the effectiveness of theper-class gradient normalization. Our findings not only allow us to understandthe potential and limitations of strategies involving the per-class gradients,but also the reason for the effectiveness of previously used solutions forclass imbalance such as oversampling.", "output": "A Theoretical Analysis of the Learning Dynamics under Class Imbalance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tiny machine learning (TinyML), executing AI workloads on resource and powerstrictly restricted systems, is an important and challenging topic. This brieffirstly presents an extremely tiny backbone to construct high efficiency CNNmodels for various visual tasks. Then, a specially designed neural co-processor(NCP) is interconnected with MCU to build an ultra-low power TinyML system,which stores all features and weights on chip and completely removes both oflatency and power consumption in off-chip memory access. Furthermore, anapplication specific instruction-set is further presented for realizing agiledevelopment and rapid deployment. Extensive experiments demonstrate that theproposed TinyML system based on our model, NCP and instruction set yieldsconsiderable accuracy and achieves a record ultra-low power of 160mW whileimplementing object detection and recognition at 30FPS. The demo video isavailable on url{", "output": "An Ultra-low Power TinyML System for Real-time Visual Processing at Edge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a Bayesian forecast aggregation model where $n$ experts, afterobserving private signals about an unknown binary event, report their posteriorbeliefs about the event to a principal, who then aggregates the reports into asingle prediction for the event. The signals of the experts and the outcome ofthe event follow a joint distribution that is unknown to the principal, but theprincipal has access to i.i.d. \"samples\" from the distribution, where eachsample is a tuple of the experts' reports (not signals) and the realization ofthe event. Using these samples, the principal aims to find an$varepsilon$-approximately optimal aggregator, where optimality is measured interms of the expected squared distance between the aggregated prediction andthe realization of the event. We show that the sample complexity of thisproblem is at least $tilde Omega(m^{n-2} / varepsilon)$ for arbitrarydiscrete distributions, where $m$ is the size of each expert's signal space.This sample complexity grows exponentially in the number of experts $n$. But,if the experts' signals are independent conditioned on the realization of theevent, then the sample complexity is significantly reduced, to $tilde O(1 /varepsilon^2)$, which does not depend on $n$. Our results can be generalizedto non-binary events. The proof of our results uses a reduction from thedistribution learning problem and reveals the fact that forecast aggregation isalmost as difficult as distribution learning.", "output": "Sample Complexity of Forecast Aggregation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An inherent problem of reinforcement learning is performing exploration of anenvironment through random actions, of which a large portion can beunproductive. Instead, exploration can be improved by initializing the learningpolicy with an existing (previously learned or hard-coded) oracle policy,offline data, or demonstrations. In the case of using an oracle policy, it canbe unclear how best to incorporate the oracle policy's experience into thelearning policy in a way that maximizes learning sample efficiency. In thispaper, we propose a method termed Critic Confidence Guided Exploration (CCGE)for incorporating such an oracle policy into standard actor-criticreinforcement learning algorithms. More specifically, CCGE takes in the oraclepolicy's actions as suggestions and incorporates this information into thelearning scheme when uncertainty is high, while ignoring it when theuncertainty is low. CCGE is agnostic to methods of estimating uncertainty, andwe show that it is equally effective with two different techniques.Empirically, we evaluate the effect of CCGE on various benchmark reinforcementlearning tasks, and show that this idea can lead to improved sample efficiencyand final performance. Furthermore, when evaluated on sparse rewardenvironments, CCGE is able to perform competitively against adjacent algorithmsthat also leverage an oracle policy. Our experiments show that it is possibleto utilize uncertainty as a heuristic to guide exploration using an oracle inreinforcement learning. We expect that this will inspire more research in thisdirection, where various heuristics are used to determine the direction ofguidance provided to learning.", "output": "Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpretable machine learning seeks to understand the reasoning process ofcomplex black-box systems that are long notorious for lack of explainability.One flourishing approach is through counterfactual explanations, which providesuggestions on what a user can do to alter an outcome. Not only must acounterfactual example counter the original prediction from the black-boxclassifier but it should also satisfy various constraints for practicalapplications. Diversity is one of the critical constraints that however remainsless discussed. While diverse counterfactuals are ideal, it is computationallychallenging to simultaneously address some other constraints. Furthermore,there is a growing privacy concern over the released counterfactual data. Tothis end, we propose a feature-based learning framework that effectivelyhandles the counterfactual constraints and contributes itself to the limitedpool of private explanation models. We demonstrate the flexibility andeffectiveness of our method in generating diverse counterfactuals ofactionability and plausibility. Our counterfactual engine is more efficientthan counterparts of the same capacity while yielding the lowestre-identification risks.", "output": "Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To generate accurate videos, algorithms have to understand the spatial andtemporal dependencies in the world. Current algorithms enable accuratepredictions over short horizons but tend to suffer from temporalinconsistencies. When generated content goes out of view and is laterrevisited, the model invents different content instead. Despite this severelimitation, no established benchmarks on complex data exist for rigorouslyevaluating video generation with long temporal dependencies. In this paper, wecurate 3 challenging video datasets with long-range dependencies by renderingwalks through 3D scenes of procedural mazes, Minecraft worlds, and indoorscans. We perform a comprehensive evaluation of current models and observetheir limitations in temporal consistency. Moreover, we introduce theTemporally Consistent Transformer (TECO), a generative model that substantiallyimproves long-term consistency while also reducing sampling time. Bycompressing its input sequence into fewer embeddings, applying a temporaltransformer, and expanding back using a spatial MaskGit, TECO outperformsexisting models across many metrics. Videos are available on the website:", "output": "Temporally Consistent Transformers for Video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In silico prediction of the ligand binding pose to a given protein target isa crucial but challenging task in drug discovery. This work focuses on blindflexible selfdocking, where we aim to predict the positions, orientations andconformations of docked molecules. Traditional physics-based methods usuallysuffer from inaccurate scoring functions and high inference cost. Recently,data-driven methods based on deep learning techniques are attracting growinginterest thanks to their efficiency during inference and promising performance.These methods usually either adopt a two-stage approach by first predicting thedistances between proteins and ligands and then generating the finalcoordinates based on the predicted distances, or directly predicting the globalroto-translation of ligands. In this paper, we take a different route. Inspiredby the resounding success of AlphaFold2 for protein structure prediction, wepropose E3Bind, an end-to-end equivariant network that iteratively updates theligand pose. E3Bind models the protein-ligand interaction through carefulconsideration of the geometric constraints in docking and the local context ofthe binding site. Experiments on standard benchmark datasets demonstrate thesuperior performance of our end-to-end trainable model compared to traditionaland recently-proposed deep learning methods.", "output": "E3Bind: An End-to-End Equivariant Network for Protein-Ligand Docking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Much of text-to-speech research relies on human evaluation, which incursheavy costs and slows down the development process. The problem is particularlyacute in heavily multilingual applications, where recruiting and polling judgescan take weeks. We introduce SQuId (Speech Quality Identification), amultilingual naturalness prediction model trained on over a million ratings andtested in 65 locales-the largest effort of this type to date. The main insightis that training one model on many locales consistently outperforms mono-localebaselines. We present our task, the model, and show that it outperforms acompetitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We thendemonstrate the effectiveness of cross-locale transfer during fine-tuning andhighlight its effect on zero-shot locales, i.e., locales for which there is nofine-tuning data. Through a series of analyses, we highlight the role ofnon-linguistic effects such as sound artifacts in cross-locale transfer.Finally, we present the effect of our design decision, e.g., model size,pre-training diversity, and language rebalancing with several ablationexperiments.", "output": "SQuId: Measuring Speech Naturalness in Many Languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the class of location-scale or heteroscedastic noise models (LSNMs),in which the effect $Y$ can be written as a function of the cause $X$ and anoise source $N$ independent of $X$, which may be scaled by a positive function$g$ over the cause, i.e., $Y = f(X) + g(X)N$. Despite the generality of themodel class, we show the causal direction is identifiable up to somepathological cases. To empirically validate these theoretical findings, wepropose two estimators for LSNMs: an estimator based on (non-linear) featuremaps, and one based on neural networks. Both model the conditional distributionof $Y$ given $X$ as a Gaussian parameterized by its natural parameters. Whenthe feature maps are correctly specified, we prove that our estimator isjointly concave, and a consistent estimator for the cause-effect identificationtask. Although the the neural network does not inherit those guarantees, it canfit functions of arbitrary complexity, and reaches state-of-the-art performanceacross benchmarks.", "output": "On the Identifiability and Estimation of Causal Location-Scale Noise Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The acquisition of explicit user feedback (e.g., ratings) in real-worldrecommender systems is often hindered by the need for active user involvement.To mitigate this issue, implicit feedback (e.g., clicks) generated during userbrowsing is exploited as a viable substitute. However, implicit feedbackpossesses a high degree of noise, which significantly undermines recommendationquality. While many methods have been proposed to address this issue byassigning varying weights to implicit feedback, two shortcomings persist: (1)the weight calculation in these methods is iteration-independent, withoutconsidering the influence of weights in previous iterations, and (2) the weightcalculation often relies on prior knowledge, which may not always be readilyavailable or universally applicable.To overcome these two limitations, we model recommendation denoising as abi-level optimization problem. The inner optimization aims to derive aneffective model for the recommendation, as well as guiding the weightdetermination, thereby eliminating the need for prior knowledge. The outeroptimization leverages gradients of the inner optimization and adjusts theweights in a manner considering the impact of previous weights. To efficientlysolve this bi-level optimization problem, we employ a weight generator to avoidthe storage of weights and a one-step gradient-matching-based loss tosignificantly reduce computational time. The experimental results on threebenchmark datasets demonstrate that our proposed approach outperforms bothstate-of-the-art general and denoising recommendation models. The code isavailable at ", "output": "Efficient Bi-Level Optimization for Recommendation Denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As industrial applications are increasingly automated by machine learningmodels, enforcing personal data ownership and intellectual property rightsrequires tracing training data back to their rightful owners. Membershipinference algorithms approach this problem by using statistical techniques todiscern whether a target sample was included in a model's training set.However, existing methods only utilize the unaltered target sample or simpleaugmentations of the target to compute statistics. Such a sparse sampling ofthe model's behavior carries little information, leading to poor inferencecapabilities. In this work, we use adversarial tools to directly optimize forqueries that are discriminative and diverse. Our improvements achievesignificantly more accurate membership inference than existing methods,especially in offline scenarios and in the low false-positive regime which iscritical in legal settings. Code is available at", "output": "Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bilevel optimization recently has received tremendous attention due to itsgreat success in solving important machine learning problems like metalearning, reinforcement learning, and hyperparameter optimization. Extendingsingle-agent training on bilevel problems to the decentralized setting is anatural generalization, and there has been a flurry of work studyingdecentralized bilevel optimization algorithms. However, it remains unknown howto design the distributed algorithm with sample complexity and convergence ratecomparable to SGD for stochastic optimization, and at the same time withoutdirectly computing the exact Hessian or Jacobian matrices. In this paper wepropose such an algorithm. More specifically, we propose a novel decentralizedstochastic bilevel optimization (DSBO) algorithm that only requires first orderstochastic oracle, Hessian-vector product and Jacobian-vector product oracle.The sample complexity of our algorithm matches the currently best known resultsfor DSBO, and the advantage of our algorithm is that it does not requireestimating the full Hessian and Jacobian matrices, thereby having improvedper-iteration complexity.", "output": "Decentralized Stochastic Bilevel Optimization with Improved per-Iteration Complexity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixup is a data augmentation technique that relies on training using randomconvex combinations of data points and their labels. In recent years, Mixup hasbecome a standard primitive used in the training of state-of-the-art imageclassification models due to its demonstrated benefits over empirical riskminimization with regards to generalization and robustness. In this work, wetry to explain some of this success from a feature learning perspective. Wefocus our attention on classification problems in which each class may havemultiple associated features (or views) that can be used to predict the classcorrectly. Our main theoretical results demonstrate that, for a non-trivialclass of data distributions with two features per class, training a 2-layerconvolutional network using empirical risk minimization can lead to learningonly one feature for almost all classes while training with a specificinstantiation of Mixup succeeds in learning both features for every class. Wealso show empirically that these theoretical insights extend to the practicalsettings of image benchmarks modified to have multiple features.", "output": "Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Decoding methods for large language models often trade-off between diversityof outputs and parallelism of computation. Methods such as beam search andGumbel top-k sampling can guarantee a different output for each element of thebeam, but are not easy to parallelize. Alternatively, methods such astemperature sampling and its modifications (top-k sampling, nucleus sampling,typical decoding, and others), are embarrassingly parallel, but have noguarantees about duplicate samples. We present a framework for samplingaccording to an arithmetic code book implicitly defined by a large languagemodel, compatible with common sampling variations, with provable beam diversityunder certain conditions, as well as being embarrassingly parallel andproviding unbiased and consistent expectations from the original model. Wedemonstrate the effectiveness of our approach on WMT machine translation, morethan halving the standard deviation when estimating expected BLEU score reward,and closing the BLEU score gap between independent sampling and beam search byup to 63%.", "output": "Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Carbon capture and storage (CCS) plays an essential role in globaldecarbonization. Scaling up CCS deployment requires accurate andhigh-resolution modeling of the storage reservoir pressure buildup and thegaseous plume migration. However, such modeling is very challenging at scaledue to the high computational costs of existing numerical methods. Thischallenge leads to significant uncertainties in evaluating storageopportunities, which can delay the pace of large-scale CCS deployment. Weintroduce Nested Fourier Neural Operator (FNO), a machine-learning frameworkfor high-resolution dynamic 3D CO2 storage modeling at a basin scale. NestedFNO produces forecasts at different refinement levels using a hierarchy of FNOsand speeds up flow prediction nearly 700,000 times compared to existingmethods. By learning the solution operator for the family of governing partialdifferential equations, Nested FNO creates a general-purpose numericalsimulator alternative for CO2 storage with diverse reservoir conditions,geological heterogeneity, and injection schemes. Our framework enablesunprecedented real-time modeling and probabilistic simulations that can supportthe scale-up of global CCS deployment.", "output": "Real-time high-resolution CO$_2$ geological storage prediction using nested Fourier neural operators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have been reported to have strong performance onnatural language processing tasks. However, performance metrics such asaccuracy do not measure the quality of the model in terms of its ability torobustly represent complex linguistic structures. In this paper, focusing onthe ability of language models to represent syntax, we propose a framework toassess the consistency and robustness of linguistic representations. To thisend, we introduce measures of robustness of neural network models that leveragerecent advances in extracting linguistic constructs from LLMs via probingtasks, i.e., simple tasks used to extract meaningful information about a singlefacet of a language model, such as syntax reconstruction and rootidentification. Empirically, we study the performance of four LLMs across sixdifferent corpora on the proposed robustness measures by analysing theirperformance and robustness with respect to syntax-preserving perturbations. Weprovide evidence that context-free representation (e.g., GloVe) are in somecases competitive with context-dependent representations from modern LLMs(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our keyobservation is that emergent syntactic representations in neural networks arebrittle. We make the code, trained models and logs available to the communityas a contribution to the debate about the capabilities of LLMs.", "output": "Emergent Linguistic Structures in Neural Networks are Fragile."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study neural network loss landscapes through the lens of modeconnectivity, the observation that minimizers of neural networks retrieved viatraining on a dataset are connected via simple paths of low loss. Specifically,we ask the following question: are minimizers that rely on different mechanismsfor making their predictions connected via simple paths of low loss? We providea definition of mechanistic similarity as shared invariances to inputtransformations and demonstrate that lack of linear connectivity between twomodels implies they use dissimilar mechanisms for making their predictions.Relevant to practice, this result helps us demonstrate that naive fine-tuningon a downstream dataset can fail to alter a model's mechanisms, e.g.,fine-tuning can fail to eliminate a model's reliance on spurious attributes.Our analysis also motivates a method for targeted alteration of a model'smechanisms, named connectivity-based fine-tuning (CBFT), which we analyze usingseveral synthetic datasets for the task of reducing a model's reliance onspurious attributes.", "output": "Mechanistic Mode Connectivity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To align conditional text generation model outputs with desired behaviors,there has been an increasing focus on training the model using reinforcementlearning (RL) with reward functions learned from human annotations. Under thisframework, we identify three common cases where high rewards are incorrectlyassigned to undesirable patterns: noise-induced spurious correlation, naturallyoccurring spurious correlation, and covariate shift. We show that even thoughlearned metrics achieve high performance on the distribution of the data usedto train the reward function, the undesirable patterns may be amplified duringRL training of the text generation model. While there has been discussion aboutreward gaming in the RL or safety community, in this discussion piece, we wouldlike to highlight reward gaming in the natural language generation (NLG)community using concrete conditional text generation examples and discusspotential fixes and areas for future work.", "output": "Reward Gaming in Conditional Text Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Feature selection is a technique in statistical prediction modeling thatidentifies features in a record with a strong statistical connection to thetarget variable. Excluding features with a weak statistical connection to thetarget variable in training not only drops the dimension of the data, whichdecreases the time complexity of the algorithm, it also decreases noise withinthe data which assists in avoiding overfitting. In all, feature selectionassists in training a robust statistical model that performs well and isstable. Given the lack of scalability in classical computation, currenttechniques only consider the predictive power of the feature and not redundancybetween the features themselves. Recent advancements in feature selection thatleverages quantum annealing (QA) gives a scalable technique that aims tomaximize the predictive power of the features while minimizing redundancy. As aconsequence, it is expected that this algorithm would assist in thebias/variance trade-off yielding better features for training a statisticalmodel. This paper tests this intuition against classical methods by utilizingopen-source data sets and evaluate the efficacy of each trained statisticalmodel well-known prediction algorithms. The numerical results display anadvantage utilizing the features selected from the algorithm that leveraged QA.", "output": "An Advantage Using Feature Selection with a Quantum Annealer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel method to optimize the structure of factor graphs forgraph-based inference. As an example inference task, we consider symboldetection on linear inter-symbol interference channels. The factor graphframework has the potential to yield low-complexity symbol detectors. However,the sum-product algorithm on cyclic factor graphs is suboptimal and itsperformance is highly sensitive to the underlying graph. Therefore, we optimizethe structure of the underlying factor graphs in an end-to-end manner usingmachine learning. For that purpose, we transform the structural optimizationinto a clustering problem of low-degree factor nodes that incorporates theknown channel model into the optimization. Furthermore, we study thecombination of this approach with neural belief propagation, yieldingnear-maximum a posteriori symbol detection performance for specific channels.", "output": "Structural Optimization of Factor Graphs for Symbol Detection via Continuous Clustering and Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spontaneous speech has many affective and pragmatic functions that areinteresting and challenging to model in TTS. However, the presence of reducedarticulation, fillers, repetitions, and other disfluencies in spontaneousspeech make the text and acoustics less aligned than in read speech, which isproblematic for attention-based TTS. We propose a TTS architecture that canrapidly learn to speak from small and irregular datasets, while alsoreproducing the diversity of expressive phenomena present in spontaneousspeech. Specifically, we add utterance-level prosody control to an existingneural HMM-based TTS system which is capable of stable, monotonic alignmentsfor spontaneous speech. We objectively evaluate control accuracy and performperceptual tests that demonstrate that prosody control does not degradesynthesis quality. To exemplify the power of combining prosody control andecologically valid data for reproducing intricate spontaneous speech phenomena,we evaluate the system's capability of synthesizing two types of creaky voice.Audio samples are available at", "output": "Prosody-controllable spontaneous TTS with neural HMMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Behavior constrained policy optimization has been demonstrated to be asuccessful paradigm for tackling Offline Reinforcement Learning. By exploitinghistorical transitions, a policy is trained to maximize a learned valuefunction while constrained by the behavior policy to avoid a significantdistributional shift. In this paper, we propose our closed-form policyimprovement operators. We make a novel observation that the behavior constraintnaturally motivates the use of first-order Taylor approximation, leading to alinear approximation of the policy objective. Additionally, as practicaldatasets are usually collected by heterogeneous policies, we model the behaviorpolicies as a Gaussian Mixture and overcome the induced optimizationdifficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality,giving rise to a closed-form policy improvement operator. We instantiateoffline RL algorithms with our novel policy improvement operators andempirically demonstrate their effectiveness over state-of-the-art algorithms onthe standard D4RL benchmark. Our code is available at", "output": "Offline Reinforcement Learning with Closed-Form Policy Improvement Operators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present in this paper a new architecture, the Pattern AttentionTransformer (PAT), that is composed of the new doughnut kernel. Compared withtokens in the NLP field, Transformer in computer vision has the problem ofhandling the high resolution of pixels in images. In ViT, an image is cut intosquare-shaped patches. As the follow-up of ViT, Swin Transformer proposes anadditional step of shifting to decrease the existence of fixed boundaries,which also incurs 'two connected Swin Transformer blocks' as the minimum unitof the model. Inheriting the patch/window idea, our doughnut kernel enhancesthe design of patches further. It replaces the line-cut boundaries with twotypes of areas: sensor and updating, which is based on the comprehension ofself-attention (named QKVA grid). The doughnut kernel also brings a new topicabout the shape of kernels beyond square. To verify its performance on imageclassification, PAT is designed with Transformer blocks of regular octagonshape doughnut kernels. Its architecture is lighter: the minimum patternattention layer is only one for each stage. Under similar complexity ofcomputation, its performances on ImageNet 1K reach higher throughput (+10%) andsurpass Swin Transformer (+0.8 acc1).", "output": "Pattern Attention Transformer with Doughnut Kernel."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We perform an empirical study of the behaviour of deep networks when fullylinearizing some of its feature channels through a sparsity prior on theoverall number of nonlinear units in the network. In experiments on imageclassification and machine translation tasks, we investigate how much we cansimplify the network function towards linearity before performance collapses.First, we observe a significant performance gap when reducing nonlinearity inthe network function early on as opposed to late in training, in-line withrecent observations on the time-evolution of the data-dependent NTK. Second, wefind that after training, we are able to linearize a significant number ofnonlinear units while maintaining a high performance, indicating that much of anetwork's expressivity remains unused but helps gradient descent in earlystages of training. To characterize the depth of the resulting partiallylinearized network, we introduce a measure called average path length,representing the average number of active nonlinearities encountered along apath in the network graph. Under sparsity pressure, we find that the remainingnonlinear units organize into distinct structures, forming core-networks ofnear constant effective depth and width, which in turn depend on taskdifficulty.", "output": "Nonlinear Advantage: Trained Networks Might Not Be As Complex as You Think."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Surrogate models are necessary to optimize meaningful quantities in physicaldynamics as their recursive numerical resolutions are often prohibitivelyexpensive. It is mainly the case for fluid dynamics and the resolution ofNavier-Stokes equations. However, despite the fast-growing field of data-drivenmodels for physical systems, reference datasets representing real-worldphenomena are lacking. In this work, we develop AirfRANS, a dataset forstudying the two-dimensional incompressible steady-state Reynolds-AveragedNavier-Stokes equations over airfoils at a subsonic regime and for differentangles of attacks. We also introduce metrics on the stress forces at thesurface of geometries and visualization of boundary layers to assess thecapabilities of models to accurately predict the meaningful information of theproblem. Finally, we propose deep learning baselines on four machine learningtasks to study AirfRANS under different constraints for generalizationconsiderations: big and scarce data regime, Reynolds number, and angle ofattack extrapolation.", "output": "AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier-Stokes Solutions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Chain of thought prompting successfully improves the reasoning capabilitiesof large language models, achieving state of the art results on a range ofdatasets. However, these reasoning capabilities only appear to emerge in modelswith a size of over 100 billion parameters. In this paper, we explore thetransfer of such reasoning capabilities to models with less than 100 billionparameters via knowledge distillation. Specifically, we finetune a studentmodel on the chain of thought outputs generated by a larger teacher model. Ourexperiments show that the proposed method improves task performance acrossarithmetic, commonsense and symbolic reasoning datasets. For example, theaccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned onPaLM-540B generated chains of thought.", "output": "Teaching Small Language Models to Reason."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning, as an essential ability for complex problem-solving, can provideback-end support for various real-world applications, such as medicaldiagnosis, negotiation, etc. This paper provides a comprehensive survey ofcutting-edge research on reasoning with language model prompting. We introduceresearch works with comparisons and summaries and provide systematic resourcesto help beginners. We also discuss the potential reasons for emerging suchreasoning abilities and highlight future research directions. Resources areavailable at  (updatedperiodically).", "output": "Reasoning with Language Model Prompting: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An increasing number of public datasets have shown a marked impact onautomated organ segmentation and tumor detection. However, due to the smallsize and partially labeled problem of each dataset, as well as a limitedinvestigation of diverse types of tumors, the resulting models are oftenlimited to segmenting specific organs/tumors and ignore the semantics ofanatomical structures, nor can they be extended to novel domains. To addressthese issues, we propose the CLIP-Driven Universal Model, which incorporatestext embedding learned from Contrastive Language-Image Pre-training (CLIP) tosegmentation models. This CLIP-based label encoding captures anatomicalrelationships, enabling the model to learn a structured feature embedding andsegment 25 organs and 6 types of tumors. The proposed model is developed froman assembly of 14 datasets, using a total of 3,410 CT scans for training andthen evaluated on 6,162 external CT scans from 3 additional datasets. We rankfirst on the Medical Segmentation Decathlon (MSD) public leaderboard andachieve state-of-the-art results on Beyond The Cranial Vault (BTCV).Additionally, the Universal Model is computationally more efficient (6x faster)compared with dataset-specific models, generalized better to CT scans fromvarying sites, and shows stronger transfer learning performance on novel tasks.", "output": "CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy storage resources must consider both price uncertainties and theirphysical operating characteristics when participating in wholesale electricitymarkets. This is a challenging problem as electricity prices are highlyvolatile, and energy storage has efficiency losses, power, and energyconstraints. This paper presents a novel, versatile, and transferable approachcombining model-based optimization with a convolutional long short-term memorynetwork for energy storage to respond to or bid into wholesale electricitymarkets. We test our proposed approach using historical prices from New YorkState, showing it achieves state-of-the-art results, achieving between 70% tonear 90% profit ratio compared to perfect foresight cases, in both priceresponse and wholesale market bidding setting with various energy storagedurations. We also test a transfer learning approach by pre-training thebidding model using New York data and applying it to arbitrage in Queensland,Australia. The result shows transfer learning achieves exceptional arbitrageprofitability with as little as three days of local training data,demonstrating its significant advantage over training from scratch in scenarioswith very limited data availability.", "output": "Transferable Energy Storage Bidder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Barlow Twins and VICReg are self-supervised representation learning modelsthat use regularizers to decorrelate features. Although these models are aseffective as conventional representation learning models, their training can becomputationally demanding if the dimension d of the projected embeddings ishigh. As the regularizers are defined in terms of individual elements of across-correlation or covariance matrix, computing the loss for n samples takesO(n d^2) time. In this paper, we propose a relaxed decorrelating regularizerthat can be computed in O(n d log d) time by Fast Fourier Transform. We alsopropose an inexpensive technique to mitigate undesirable local minima thatdevelop with the relaxation. The proposed regularizer exhibits accuracycomparable to that of existing regularizers in downstream tasks, whereas theirtraining requires less memory and is faster for large d. The source code isavailable.", "output": "Learning Decorrelated Representations Efficiently Using Fast Fourier Transform."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Climate change results in an increased probability of extreme weather eventsthat put societies and businesses at risk on a global scale. Therefore, nearreal-time mapping of natural hazards is an emerging priority for the support ofnatural disaster relief, risk management, and informing governmental policydecisions. Recent methods to achieve near real-time mapping increasinglyleverage deep learning (DL). However, DL-based approaches are designed for onespecific task in a single geographic region based on specific frequency bandsof satellite data. Therefore, DL models used to map specific natural hazardsstruggle with their generalization to other types of natural hazards in unseenregions. In this work, we propose a methodology to significantly improve thegeneralizability of DL natural hazards mappers based on pre-training on asuitable pre-task. Without access to any data from the target domain, wedemonstrate this improved generalizability across four U-Net architectures forthe segmentation of unseen natural hazards. Importantly, our method isinvariant to geographic differences and differences in the type of frequencybands of satellite data. By leveraging characteristics of unlabeled images fromthe target domain that are publicly available, our approach is able to furtherimprove the generalization behavior without fine-tuning. Thereby, our approachsupports the development of foundation models for earth monitoring with theobjective of directly segmenting unseen natural hazards across novel geographicregions given different sources of satellite imagery.", "output": "Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A Deep Neural Network (DNN) is a composite function of vector-valuedfunctions, and in order to train a DNN, it is necessary to calculate thegradient of the loss function with respect to all parameters. This calculationcan be a non-trivial task because the loss function of a DNN is a compositionof several nonlinear functions, each with numerous parameters. TheBackpropagation (BP) algorithm leverages the composite structure of the DNN toefficiently compute the gradient. As a result, the number of layers in thenetwork does not significantly impact the complexity of the calculation. Theobjective of this paper is to express the gradient of the loss function interms of a matrix multiplication using the Jacobian operator. This can beachieved by considering the total derivative of each layer with respect to itsparameters and expressing it as a Jacobian matrix. The gradient can then berepresented as the matrix product of these Jacobian matrices. This approach isvalid because the chain rule can be applied to a composition of vector-valuedfunctions, and the use of Jacobian matrices allows for the incorporation ofmultiple inputs and outputs. By providing concise mathematical justifications,the results can be made understandable and useful to a broad audience fromvarious disciplines.", "output": "The Backpropagation algorithm for a math student."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNNs) achieve remarkable performance in graph machinelearning tasks but can be hard to train on large-graph data, where theirlearning dynamics are not well understood. We investigate the training dynamicsof large-graph GNNs using graph neural tangent kernels (GNTKs) and graphons. Inthe limit of large width, optimization of an overparametrized NN is equivalentto kernel regression on the NTK. Here, we investigate how the GNTK evolves asanother independent dimension is varied: the graph size. We use graphons todefine limit objects -- graphon NNs for GNNs, and graphon NTKs for GNTKs -- ,and prove that, on a sequence of graphs, the GNTKs converge to the graphon NTK.We further prove that the spectrum of the GNTK, which is related to thedirections of fastest learning which becomes relevant during early stopping,converges to the spectrum of the graphon NTK. This implies that in thelarge-graph limit, the GNTK fitted on a graph of moderate size can be used tosolve the same task on the large graph, and to infer the learning dynamics ofthe large-graph GNN. These results are verified empirically on node regressionand classification tasks.", "output": "Graph Neural Tangent Kernel: Convergence on Large Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We provide a theoretical framework for Reinforcement Learning with HumanFeedback (RLHF). Our analysis shows that when the true reward function islinear, the widely used maximum likelihood estimator (MLE) converges under boththe Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However,we show that when training a policy based on the learned reward model, MLEfails while a pessimistic MLE provides policies with improved performance undercertain coverage assumptions. Additionally, we demonstrate that under the PLmodel, the true MLE and an alternative MLE that splits the $K$-wise comparisoninto pairwise comparisons both converge. Moreover, the true MLE isasymptotically more efficient. Our results validate the empirical success ofexisting RLHF algorithms in InstructGPT and provide new insights for algorithmdesign. Furthermore, our results unify the problem of RLHF and max-entropyInverse Reinforcement Learning (IRL), and provide the first sample complexitybound for max-entropy IRL.", "output": "Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, particle-based variational inference (ParVI) methods such asStein variational gradient descent (SVGD) have grown in popularity as scalablemethods for Bayesian inference. Unfortunately, the properties of such methodsinvariably depend on hyperparameters such as the learning rate, which must becarefully tuned by the practitioner in order to ensure convergence to thetarget measure at a suitable rate. In this paper, we introduce a suite of newparticle-based methods for scalable Bayesian inference based on coin betting,which are entirely learning-rate free. We illustrate the performance of ourapproach on a range of numerical examples, including several high-dimensionalmodels and datasets, demonstrating comparable performance to other ParVIalgorithms with no need to tune a learning rate.", "output": "Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Source separation involves the ill-posed problem of retrieving a set ofsource signals that have been observed through a mixing operator. Solving thisproblem requires prior knowledge, which is commonly incorporated by imposingregularity conditions on the source signals, or implicitly learned throughsupervised or unsupervised methods from existing data. While data-drivenmethods have shown great promise in source separation, they often require largeamounts of data, which rarely exists in planetary space missions. To addressthis challenge, we propose an unsupervised source separation scheme for domainswith limited data access that involves solving an optimization problem in thewavelet scattering covariance representation space$unicode{x2014}$aninterpretable, low-dimensional representation of stationary processes. Wepresent a real-data example in which we remove transient, thermally-inducedmicrotilts$unicode{x2014}$known as glitches$unicode{x2014}$from data recordedby a seismometer during NASA's InSight mission on Mars. Thanks to the waveletscattering covariances' ability to capture non-Gaussian properties ofstochastic processes, we are able to separate glitches using only a fewglitch-free data snippets.", "output": "Unearthing InSights into Mars: Unsupervised Source Separation with Limited Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot classification consists of a training phase where a model is learnedon a relatively large dataset and an adaptation phase where the learned modelis adapted to previously-unseen tasks with limited labeled samples. In thispaper, we empirically prove that the training algorithm and the adaptationalgorithm can be completely disentangled, which allows algorithm analysis anddesign to be done individually for each phase. Our meta-analysis for each phasereveals several interesting insights that may help better understand keyaspects of few-shot classification and connections with other fields such asvisual representation learning and transfer learning. We hope the insights andresearch challenges revealed in this paper can inspire future work in relateddirections. Code and pre-trained models (in PyTorch) are available at", "output": "A Closer Look at Few-shot Classification Again."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Designing machine learning architectures for processing neural networks intheir raw weight matrix form is a newly introduced research direction.Unfortunately, the unique symmetry structure of deep weight spaces makes thisdesign very challenging. If successful, such architectures would be capable ofperforming a wide range of intriguing tasks, from adapting a pre-trainednetwork to a new domain to editing objects represented as functions (INRs orNeRFs). As a first step towards this goal, we present here a novel networkarchitecture for learning in deep weight spaces. It takes as input aconcatenation of weights and biases of a pre-trained MLP and processes it usinga composition of layers that are equivariant to the natural permutationsymmetry of the MLP's weights: Changing the order of neurons in intermediatelayers of the MLP does not affect the function it represents. We provide a fullcharacterization of all affine equivariant and invariant layers for thesesymmetries and show how these layers can be implemented using three basicoperations: pooling, broadcasting, and fully connected layers applied to theinput in an appropriate manner. We demonstrate the effectiveness of ourarchitecture and its advantages over natural baselines in a variety of learningtasks.", "output": "Equivariant Architectures for Learning in Deep Weight Spaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Demographic parity is the most widely recognized measure of group fairness inmachine learning, which ensures equal treatment of different demographicgroups. Numerous works aim to achieve demographic parity by pursuing thecommonly used metric $Delta DP$. Unfortunately, in this paper, we reveal thatthe fairness metric $Delta DP$ can not precisely measure the violation ofdemographic parity, because it inherently has the following drawbacks: i)zero-value $Delta DP$ does not guarantee zero violation of demographic parity,ii) $Delta DP$ values can vary with different classification thresholds. Tothis end, we propose two new fairness metrics, Area Between Probability densityfunction Curves (ABPC) and Area Between Cumulative density function Curves(ABCC), to precisely measure the violation of demographic parity at thedistribution level. The new fairness metrics directly measure the differencebetween the distributions of the prediction probability for differentdemographic groups. Thus our proposed new metrics enjoy: i) zero-valueABCC/ABPC guarantees zero violation of demographic parity; ii) ABCC/ABPCguarantees demographic parity while the classification thresholds are adjusted.We further re-evaluate the existing fair models with our proposed fairnessmetrics and observe different fairness behaviors of those models under the newmetrics. The code is available at", "output": "Retiring $\\Delta$DP: New Distribution-Level Metrics for Demographic Parity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Retrosynthesis, which aims to find a route to synthesize a target moleculefrom commercially available starting materials, is a critical task in drugdiscovery and materials design. Recently, the combination of ML-basedsingle-step reaction predictors with multi-step planners has led to promisingresults. However, the single-step predictors are mostly trained offline tooptimize the single-step accuracy, without considering complete routes. Here,we leverage reinforcement learning (RL) to improve the single-step predictor,by using a tree-shaped MDP to optimize complete routes. Specifically, wepropose a novel online training algorithm, called Planning with Dual ValueNetworks (PDVN), which alternates between the planning phase and updatingphase. In PDVN, we construct two separate value networks to predict thesynthesizability and cost of molecules, respectively. To maintain thesingle-step accuracy, we design a two-branch network structure for thesingle-step predictor. On the widely-used USPTO dataset, our PDVN algorithmimproves the search success rate of existing multi-step planners (e.g.,increasing the success rate from 85.79% to 98.95% for Retro*, and reducing thenumber of model calls by half while solving 99.47% molecules for RetroGraph).Additionally, PDVN helps find shorter synthesis routes (e.g., reducing theaverage route length from 5.76 to 4.83 for Retro*, and from 5.63 to 4.78 forRetroGraph).", "output": "Retrosynthetic Planning with Dual Value Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the linear contextual multi-class multi-period packing problem(LMMP) where the goal is to pack items such that the total vector ofconsumption is below a given budget vector and the total value is as large aspossible. We consider the setting where the reward and the consumption vectorassociated with each action is a class-dependent linear function of thecontext, and the decision-maker receives bandit feedback. LMMP includes linearcontextual bandits with knapsacks and online revenue management as specialcases. We establish a new estimator which guarantees a faster convergence rate,and consequently, a lower regret in such problems. We propose a bandit policythat is a closed-form function of said estimated parameters. When the contextsare non-degenerate, the regret of the proposed policy is sublinear in thecontext dimension, the number of classes, and the time horizon $T$ when thebudget grows at least as $sqrt{T}$. We also resolve an open problem posed byAgrawal &amp; Devanur (2016) and extend the result to a multi-class setting. Ournumerical experiments clearly demonstrate that the performance of our policy issuperior to other benchmarks in the literature.", "output": "Improved Algorithms for Multi-period Multi-class Packing Problems with Bandit Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an efficient method to ground pretrained text-only language modelsto the visual domain, enabling them to process arbitrarily interleavedimage-and-text data, and generate text interleaved with retrieved images. Ourmethod leverages the abilities of language models learnt from large scaletext-only pretraining, such as in-context learning and free-form textgeneration. We keep the language model frozen, and finetune input and outputlinear layers to enable cross-modality interactions. This allows our model toprocess arbitrarily interleaved image-and-text inputs, and generate free-formtext interleaved with retrieved images. We achieve strong zero-shot performanceon grounded tasks such as contextual image retrieval and multimodal dialogue,and showcase compelling interactive abilities. Our approach works with anyoff-the-shelf language model and paves the way towards an effective, generalsolution for leveraging pretrained language models in visually groundedsettings.", "output": "Grounding Language Models to Images for Multimodal Inputs and Outputs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes Fed-GLOSS-DP, a novel privacy-preserving approach forfederated learning. Unlike previous linear point-wise gradient-sharing schemes,such as FedAvg, our formulation enables a type of global optimization byleveraging synthetic samples received from clients. These synthetic samples,serving as loss surrogates, approximate local loss landscapes by simulating theutility of real images within a local region. We additionally introduce anapproach to measure effective approximation regions reflecting the quality ofthe approximation. Therefore, the server can recover the global loss landscapeand comprehensively optimize the model. Moreover, motivated by the emergingprivacy concerns, we demonstrate that our approach seamlessly works withrecord-level differential privacy (DP), granting theoretical privacy guaranteesfor every data record on the clients. Extensive results validate the efficacyof our formulation on various datasets with highly skewed distributions. Ourmethod consistently improves over the baselines, especially considering highlyskewed distributions and noisy gradients due to DP. The source code will bereleased upon publication.", "output": "Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have demonstrated the benefits of capturing long-distancedependency in graphs by deeper graph neural networks (GNNs). But deeper GNNssuffer from the long-lasting scalability challenge due to the neighborhoodexplosion problem in large-scale graphs. In this work, we propose to capturelong-distance dependency in graphs by shallower models instead of deepermodels, which leads to a much more efficient model, LazyGNN, for graphrepresentation learning. Moreover, we demonstrate that LazyGNN is compatiblewith existing scalable approaches (such as sampling methods) for furtheraccelerations through the development of mini-batch LazyGNN. Comprehensiveexperiments demonstrate its superior prediction performance and scalability onlarge-scale benchmarks. The implementation of LazyGNN is available at", "output": "LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weight oscillation is an undesirable side effect of quantization-awaretraining, in which quantized weights frequently jump between two quantizedlevels, resulting in training instability and a sub-optimal final model. Wediscover that the learnable scaling factor, a widely-used $textit{de facto}$setting in quantization aggravates weight oscillation. In this study, weinvestigate the connection between the learnable scaling factor and quantizedweight oscillation and use ViT as a case driver to illustrate the findings andremedies. In addition, we also found that the interdependence between quantizedweights in $textit{query}$ and $textit{key}$ of a self-attention layer makesViT vulnerable to oscillation. We, therefore, propose three techniquesaccordingly: statistical weight quantization ($rm StatsQ$) to improvequantization robustness compared to the prevalent learnable-scale-based method;confidence-guided annealing ($rm CGA$) that freezes the weights with$textit{high confidence}$ and calms the oscillating weights; and$textit{query}$-$textit{key}$ reparameterization ($rm QKR$) to resolve thequery-key intertwined oscillation and mitigate the resulting gradientmisestimation. Extensive experiments demonstrate that these proposed techniquessuccessfully abate weight oscillation and consistently achieve substantialaccuracy improvement on ImageNet. Specifically, our 2-bit DeiT-T/DeiT-Salgorithms outperform the previous state-of-the-art by 9.8% and 7.7%,respectively. Code and models are available at:url{", "output": "Oscillation-free Quantization for Low-bit Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) has emerged as a powerful framework to learnrepresentations from raw data without supervision. Yet in practice, engineersface issues such as instability in tuning optimizers and collapse ofrepresentations during training. Such challenges motivate the need for a theoryto shed light on the complex interplay between the choice of data augmentation,network architecture, and training algorithm. We study such an interplay with aprecise analysis of generalization performance on both pretraining anddownstream tasks in a theory friendly setup, and highlight several insights forSSL practitioners that arise from our theory.", "output": "The SSL Interplay: Augmentations, Inductive Bias, and Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The strength of modern generative models lies in their ability to becontrolled through text-based prompts. Typical \"hard\" prompts are made frominterpretable words and tokens, and must be hand-crafted by humans. There arealso \"soft\" prompts, which consist of continuous feature vectors. These can bediscovered using powerful optimization methods, but they cannot be easilyinterpreted, re-used across models, or plugged into a text-based interface.We describe an approach to robustly optimize hard text prompts throughefficient gradient-based optimization. Our approach automatically generateshard text-based prompts for both text-to-image and text-to-text applications.In the text-to-image setting, the method creates hard prompts for diffusionmodels, allowing API users to easily generate, discover, and mix and matchimage concepts without prior knowledge on how to prompt the model. In thetext-to-text setting, we show that hard prompts can be automatically discoveredthat are effective in tuning LMs for classification.", "output": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the advance of language models, privacy protection is receiving moreattention. Training data extraction is therefore of great importance, as it canserve as a potential tool to assess privacy leakage. However, due to thedifficulty of this task, most of the existing methods are proof-of-concept andstill not effective enough. In this paper, we investigate and benchmark tricksfor improving training data extraction using a publicly available dataset.Because most existing extraction methods use a pipeline ofgenerating-then-ranking, i.e., generating text candidates as potential trainingdata and then ranking them based on specific criteria, our research focuses onthe tricks for both text generation (e.g., sampling strategy) and text ranking(e.g., token-level criteria). The experimental results show that severalpreviously overlooked tricks can be crucial to the success of training dataextraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricksoutperform the baseline by a large margin in most cases, providing a muchstronger baseline for future research. The code is available at", "output": "Bag of Tricks for Training Data Extraction from Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It has been recognized that the data generated by the denoising diffusionprobabilistic model (DDPM) improves adversarial training. After two years ofrapid development in diffusion models, a question naturally arises: can betterdiffusion models further improve adversarial training? This paper gives anaffirmative answer by employing the most recent diffusion model which hashigher efficiency ($sim 20$ sampling steps) and image quality (lower FIDscore) compared with DDPM. Our adversarially trained models achievestate-of-the-art performance on RobustBench using only generated data (noexternal datasets). Under the $ell_infty$-norm threat model with$epsilon=8/255$, our models achieve $70.69%$ and $42.67%$ robust accuracy onCIFAR-10 and CIFAR-100, respectively, i.e. improving upon previousstate-of-the-art models by $+4.58%$ and $+8.03%$. Under the $ell_2$-normthreat model with $epsilon=128/255$, our models achieve $84.86%$ on CIFAR-10($+4.44%$). These results also beat previous works that use external data. Wealso provide compelling results on the SVHN and TinyImageNet datasets. Our codeis available at ", "output": "Better Diffusion Models Further Improve Adversarial Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While Reinforcement Learning (RL) achieves tremendous success in sequentialdecision-making problems of many domains, it still faces key challenges of datainefficiency and the lack of interpretability. Interestingly, many researchershave leveraged insights from the causality literature recently, bringing forthflourishing works to unify the merits of causality and address well thechallenges from RL. As such, it is of great necessity and significance tocollate these Causal Reinforcement Learning (CRL) works, offer a review of CRLmethods, and investigate the potential functionality from causality toward RL.In particular, we divide existing CRL approaches into two categories accordingto whether their causality-based information is given in advance or not. Wefurther analyze each category in terms of the formalization of differentmodels, ranging from the Markov Decision Process (MDP), Partially ObservedMarkov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic TreatmentRegime (DTR). Moreover, we summarize the evaluation matrices and open sourceswhile we discuss emerging applications, along with promising prospects for thefuture development of CRL.", "output": "A Survey on Causal Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNN) have shown great capacity of modeling a dynamicalsystem; nevertheless, they usually do not obey physics constraints such asconservation laws. This paper proposes a new learning framework named ConCerNetto improve the trustworthiness of the DNN based dynamics modeling to endow theinvariant properties. ConCerNet consists of two steps: (i) a contrastivelearning method to automatically capture the system invariants (i.e.conservation properties) along the trajectory observations; (ii) a neuralprojection layer to guarantee that the learned dynamics models preserve thelearned invariants. We theoretically prove the functional relationship betweenthe learned latent representation and the unknown system invariant function.Experiments show that our method consistently outperforms the baseline neuralnetworks in both coordinate error and conservation metrics by a large margin.With neural network based parameterization and no dependence on priorknowledge, our method can be extended to complex and large-scale dynamics byleveraging an autoencoder.", "output": "ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We generalize the notion of average Lipschitz smoothness proposed by Ashlagiet al. (COLT 2021) by extending it to H\"older smoothness. This measure of the\"effective smoothness\" of a function is sensitive to the underlyingdistribution and can be dramatically smaller than its classic \"worst-caseH\"older constant. We consider both the realizable and the agnostic (noisy)regression settings, proving upper and lower risk bounds in terms of theaverage H\"older smoothness; these rates improve upon both previously knownrates even in the special case of average Lipschitz smoothness. Moreover, ourlower bound is tight in the realizable setting up to log factors, thus weestablish the minimax rate. From an algorithmic perspective, since our notionof average smoothness is defined with respect to the unknown underlyingdistribution, the learner does not have an explicit representation of thefunction class, hence is unable to execute ERM. Nevertheless, we providedistinct learning algorithms that achieve both (nearly) optimal learning rates.Our results hold in any totally bounded metric space, and are stated in termsof its intrinsic geometry. Overall, our results show that the classicworst-case notion of H\"older smoothness can be essentially replaced by itsaverage, yielding considerably sharper guarantees.", "output": "Near-optimal learning with average H\\\"older smoothness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stochastic Rising Bandits (SRBs) model sequential decision-making problems inwhich the expected rewards of the available options increase every time theyare selected. This setting captures a wide range of scenarios in which theavailable options are learning entities whose performance improves (inexpectation) over time. While previous works addressed the regret minimizationproblem, this paper, focuses on the fixed-budget Best Arm Identification (BAI)problem for SRBs. In this scenario, given a fixed budget of rounds, we areasked to provide a recommendation about the best option at the end of theidentification process. We propose two algorithms to tackle the above-mentionedsetting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, whichemploys a successive reject procedure. Then, we prove that, with a sufficientlylarge budget, they provide guarantees on the probability of properlyidentifying the optimal option at the end of the learning process. Furthermore,we derive a lower bound on the error probability, matched by our R-SR (up tologarithmic factors), and illustrate how the need for a sufficiently largebudget is unavoidable in the SRB setting. Finally, we numerically validate theproposed algorithms in both synthetic and real-world environments and comparethem with the currently available BAI strategies.", "output": "Best Arm Identification for Stochastic Rising Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The machine learning security community has developed myriad defenses forevasion attacks over the past decade. An understudied question in thatcommunity is: for whom do these defenses defend? In this work, we consider somecommon approaches to defending learned systems and whether those approaches mayoffer unexpected performance inequities when used by different sub-populations.We outline simple parity metrics and a framework for analysis that can begin toanswer this question through empirical results of the fairness implications ofmachine learning security methods. Many methods have been proposed that cancause direct harm, which we describe as biased vulnerability and biasedrejection. Our framework and metric can be applied to robustly trained models,preprocessing-based methods, and rejection methods to capture behavior oversecurity budgets. We identify a realistic dataset with a reasonablecomputational cost suitable for measuring the equality of defenses. Through acase study in speech command recognition, we show how such defenses do notoffer equal protection for social subgroups and how to perform such analysesfor robustness training, and we present a comparison of fairness between tworejection-based defenses: randomized smoothing and neural rejection. We offerfurther analysis of factors that correlate to equitable defenses to stimulatethe future investigation of how to assist in building such defenses. To thebest of our knowledge, this is the first work that examines the fairnessdisparity in the accuracy-robustness trade-off in speech data and addressesfairness evaluation for rejection-based defenses.", "output": "Measuring Equality in Machine Learning Security Defenses."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of estimating the distribution of the return of a policyusing an offline dataset that is not generated from the policy, i.e.,distributional offline policy evaluation (OPE). We propose an algorithm calledFitted Likelihood Estimation (FLE), which conducts a sequence of MaximumLikelihood Estimation (MLE) and has the flexibility of integrating anystate-of-the-art probabilistic generative models as long as it can be trainedvia MLE. FLE can be used for both finite-horizon and infinite-horizondiscounted settings where rewards can be multi-dimensional vectors. Ourtheoretical results show that for both finite-horizon and infinite-horizondiscounted settings, FLE can learn distributions that are close to the groundtruth under total variation distance and Wasserstein distance, respectively.Our theoretical results hold under the conditions that the offline data coversthe test policy's traces and that the supervised learning MLE proceduressucceed. Experimentally, we demonstrate the performance of FLE with twogenerative models, Gaussian mixture models and diffusion models. For themulti-dimensional reward setting, FLE with diffusion models is capable ofestimating the complicated distribution of the return of a test policy.", "output": "Distributional Offline Policy Evaluation with Predictive Error Guarantees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning agent behaviors from observational data has shown to improve ourunderstanding of their decision-making processes, advancing our ability toexplain their interactions with the environment and other agents. Whilemultiple learning techniques have been proposed in the literature, there is oneparticular setting that has not been explored yet: multi agent systems whereagent identities remain anonymous. For instance, in financial markets labeleddata that identifies market participant strategies is typically proprietary,and only the anonymous state-action pairs that result from the interaction ofmultiple market participants are publicly available. As a result, sequences ofagent actions are not observable, restricting the applicability of existingwork. In this paper, we propose a Policy Clustering algorithm, called K-SHAP,that learns to group anonymous state-action pairs according to the agentpolicies. We frame the problem as an Imitation Learning (IL) task, and we learna world-policy able to mimic all the agent behaviors upon differentenvironmental states. We leverage the world-policy to explain each anonymousobservation through an additive feature attribution method called SHAP (SHapleyAdditive exPlanations). Finally, by clustering the explanations we show that weare able to identify different agent policies and group observationsaccordingly. We evaluate our approach on simulated synthetic market data and areal-world financial dataset. We show that our proposal significantly andconsistently outperforms the existing methods, identifying different agentstrategies.", "output": "K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurately estimating uncertainty is an essential component ofdecision-making and forecasting in machine learning. However, existinguncertainty estimation methods may fail when data no longer follows thedistribution seen during training. Here, we introduce online uncertaintyestimation algorithms that are guaranteed to be reliable on arbitrary streamsof data points, including data chosen by an adversary. Specifically, ouralgorithms perform post-hoc recalibration of a black-box regression model andproduce outputs that are provably calibrated -- i.e., an 80% confidenceinterval will contain the true outcome 80% of the time -- and that have lowregret relative to the learning objective of the base model. We apply ouralgorithms in the context of Bayesian optimization, an online model-baseddecision-making task in which the data distribution shifts over time, andobserve accelerated convergence to improved optima. Our results suggest thatrobust uncertainty quantification has the potential to improve onlinedecision-making.", "output": "Adversarial Calibrated Regression for Online Decision Making."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sparse coding, which refers to modeling a signal as sparse linearcombinations of the elements of a learned dictionary, has proven to be asuccessful (and interpretable) approach in applications such as signalprocessing, computer vision, and medical imaging. While this success hasspurred much work on provable guarantees for dictionary recovery when thelearned dictionary is the same size as the ground-truth dictionary, work on thesetting where the learned dictionary is larger (or over-realized) with respectto the ground truth is comparatively nascent. Existing theoretical results inthis setting have been constrained to the case of noise-less data. We show inthis work that, in the presence of noise, minimizing the standard dictionarylearning objective can fail to recover the elements of the ground-truthdictionary in the over-realized regime, regardless of the magnitude of thesignal in the data-generating process. Furthermore, drawing from the growingbody of work on self-supervised learning, we propose a novel masking objectivefor which recovering the ground-truth dictionary is in fact optimal as thesignal increases for a large class of data-generating processes. We corroborateour theoretical results with experiments across several parameter regimesshowing that our proposed objective also enjoys better empirical performancethan the standard reconstruction objective.", "output": "Hiding Data Helps: On the Benefits of Masking for Sparse Coding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of relative representations for latent embeddings has shown potentialin enabling latent space communication and zero-shot model stitching across awide range of applications. Nevertheless, relative representations rely on acertain amount of parallel anchors to be given as input, which can beimpractical to obtain in certain scenarios. To overcome this limitation, wepropose an optimization-based method to discover new parallel anchors from alimited known set (seed). Our approach can be used to find semanticcorrespondence between different domains, align their relative spaces, andachieve competitive results in several tasks.", "output": "Bootstrapping Parallel Anchors for Relative Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Leveraging privileged information (PI), or features available during trainingbut not at test time, has recently been shown to be an effective method foraddressing label noise. However, the reasons for its effectiveness are not wellunderstood. In this study, we investigate the role played by differentproperties of the PI in explaining away label noise. Through experiments onmultiple datasets with real PI (CIFAR-N/H) and a new large-scale benchmarkImageNet-PI, we find that PI is most helpful when it allows networks to easilydistinguish clean from noisy data, while enabling a learning shortcut tomemorize the noisy examples. Interestingly, when PI becomes too predictive ofthe target label, PI methods often perform worse than their no-PI baselines.Based on these findings, we propose several enhancements to thestate-of-the-art PI methods and demonstrate the potential of PI as a means oftackling label noise. Finally, we show how we can easily combine the resultingPI approaches with existing no-PI techniques designed to deal with label noise.", "output": "When does Privileged Information Explain Away Label Noise?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks are an attractive alternative for simulating complexdynamical systems, as in comparison to traditional scientific computingmethods, they offer reduced computational costs during inference and can betrained directly from observational data. Existing methods, however, cannotextrapolate accurately and are prone to error accumulation in long-timeintegration. Herein, we address this issue by combining neural operators withrecurrent neural networks to construct a novel and effective architecture,resulting in superior accuracy compared to the state-of-the-art. The new hybridmodel is based on operator learning while offering a recurrent structure tocapture temporal dependencies. The integrated framework is shown to stabilizethe solution and reduce error accumulation for both interpolation andextrapolation of the Korteweg-de Vries equation.", "output": "Neural Operator Learning for Long-Time Integration in Dynamical Systems with Recurrent Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a deep matrix factorization model of covariance matrices trainedwith the Bures-Wasserstein distance. While recent works have made importantadvances in the study of the optimization problem for overparametrized low-rankmatrix approximation, much emphasis has been placed on discriminative settingsand the square loss. In contrast, our model considers another interesting typeof loss and connects with the generative setting. We characterize the criticalpoints and minimizers of the Bures-Wasserstein distance over the space ofrank-bounded matrices. For low-rank matrices the Hessian of this loss cantheoretically blow up, which creates challenges to analyze convergence ofoptimizaton methods. We establish convergence results for gradient flow using asmooth perturbative version of the loss and convergence results for finite stepsize gradient descent under certain assumptions on the initial weights.", "output": "Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many real-world applications based on online learning produce streaming datathat is haphazard in nature, i.e., contains missing features, features becomingobsolete in time, the appearance of new features at later points in time and alack of clarity on the total number of input features. These challenges make ithard to build a learnable system for such applications, and almost no workexists in deep learning that addresses this issue. In this paper, we presentAux-Drop, an auxiliary dropout regularization strategy for online learning thathandles the haphazard input features in an effective manner. Aux-Drop adaptsthe conventional dropout regularization scheme for the haphazard input featurespace ensuring that the final output is minimally impacted by the chaoticappearance of such features. It helps to prevent the co-adaptation ofespecially the auxiliary and base features, as well as reduces the strongdependence of the output on any of the auxiliary inputs of the model. Thishelps in better learning for scenarios where certain features disappear in timeor when new features are to be modelled. The efficacy of Aux-Drop has beendemonstrated through extensive numerical experiments on SOTA benchmarkingdatasets that include Italy Power Demand, HIGGS, SUSY and multiple UCIdatasets. The code is available at ", "output": "Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Currently, there is a significant amount of research being conducted in thefield of artificial intelligence to improve the explainability andinterpretability of deep learning models. It is found that if end-usersunderstand the reason for the production of some output, it is easier to trustthe system. Recommender systems are one example of systems that great effortshave been conducted to make their output more explainable. One method forproducing a more explainable output is using counterfactual reasoning, whichinvolves altering minimal features to generate a counterfactual item thatresults in changing the output of the system. This process allows theidentification of input features that have a significant impact on the desiredoutput, leading to effective explanations. In this paper, we present a methodfor generating counterfactual explanations for both tabular and textualfeatures. We evaluated the performance of our proposed method on threereal-world datasets and demonstrated a +5% improvement on finding effectivefeatures (based on model-based measures) compared to the baseline method.", "output": "Explaining Recommendation System Using Counterfactual Textual Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classifier guidance -- using the gradients of an image classifier to steerthe generations of a diffusion model -- has the potential to dramaticallyexpand the creative control over image generation and editing. However,currently classifier guidance requires either training new noise-aware modelsto obtain accurate gradients or using a one-step denoising approximation of thefinal generation, which leads to misaligned gradients and sub-optimal control.We highlight this approximation's shortcomings and propose a novel guidancemethod: Direct Optimization of Diffusion Latents (DOODL), which enablesplug-and-play guidance by optimizing diffusion latents w.r.t. the gradients ofa pre-trained classifier on the true generated pixels, using an invertiblediffusion process to achieve memory-efficient backpropagation. Showcasing thepotential of more precise guidance, DOODL outperforms one-step classifierguidance on computational and human evaluation metrics across different formsof guidance: using CLIP guidance to improve generations of complex prompts fromDrawBench, using fine-grained visual classifiers to expand the vocabulary ofStable Diffusion, enabling image-conditioned generation with a CLIP visualencoder, and improving image aesthetics using an aesthetic scoring network.Code at ", "output": "End-to-End Diffusion Latent Optimization Improves Classifier Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Laplacian based algorithms for data lying on a manifold have beenproven effective for tasks such as dimensionality reduction, clustering, anddenoising. In this work, we consider data sets whose data points not only lieon a manifold, but are also closed under the action of a continuous group. Anexample of such a data set is volumes that lie on a low dimensional manifold,where each volume may be rotated in three-dimensional space. We introduce theG-invariant graph Laplacian that generalizes the graph Laplacian by accountingfor the action of the group on the data set. We show that like the standardgraph Laplacian, the G-invariant graph Laplacian converges to theLaplace-Beltrami operator on the data manifold, but with a significantlyimproved convergence rate. Furthermore, we show that the eigenfunctions of theG-invariant graph Laplacian admit the form of tensor products between the groupelements and eigenvectors of certain matrices, which can be computedefficiently using FFT-type algorithms. We demonstrate our construction and itsadvantages on the problem of filtering data on a noisy manifold closed underthe action of the special unitary group SU(2).", "output": "The G-invariant graph Laplacian."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a new policy representation based on score-based diffusion models(SDMs). We apply our new policy representation in the domain ofGoal-Conditioned Imitation Learning (GCIL) to learn general-purposegoal-specified policies from large uncurated datasets without rewards. Our newgoal-conditioned policy architecture \"$textbf{BE}$havior generation with$textbf{S}$c$textbf{O}$re-based Diffusion Policies\" (BESO) leverages agenerative, score-based diffusion model as its policy. BESO decouples thelearning of the score model from the inference sampling process, and, henceallows for fast sampling strategies to generate goal-specified behavior in just3 denoising steps, compared to 30+ steps of other diffusion based policies.Furthermore, BESO is highly expressive and can effectively capturemulti-modality present in the solution space of the play data. Unlike previousmethods such as Latent Plans or C-Bet, BESO does not rely on complexhierarchical policies or additional clustering for effective goal-conditionedbehavior learning. Finally, we show how BESO can even be used to learn agoal-independent policy from play-data using classifier-free guidance. To thebest of our knowledge this is the first work that a) represents a behaviorpolicy based on such a decoupled SDM b) learns an SDM based policy in thedomain of GCIL and c) provides a way to simultaneously learn a goal-dependentand a goal-independent policy from play-data. We evaluate BESO through detailedsimulation and show that it consistently outperforms several state-of-the-artgoal-conditioned imitation learning methods on challenging benchmarks. Weadditionally provide extensive ablation studies and experiments to demonstratethe effectiveness of our method for goal-conditioned behavior generation.Demonstrations and Code are available at", "output": "Goal-Conditioned Imitation Learning using Score-based Diffusion Policies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We are developing a virtual coaching system that helps patients adhere tobehavior change interventions (BCI). Our proposed system predicts whether apatient will perform the targeted behavior and uses counterfactual exampleswith feature control to guide personalizsation of BCI. We use simulated patientdata with varying levels of receptivity to intervention to arrive at the studydesign which would enable evaluation of our system.", "output": "Personalising Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy and data-efficient online time series prediction for predictingevolving dynamical systems are critical in several fields, especially edge AIapplications that need to update continuously based on streaming data. However,current DNN-based supervised online learning models require a large amount oftraining data and cannot quickly adapt when the underlying system changes.Moreover, these models require continuous retraining with incoming data makingthem highly inefficient. To solve these issues, we present a novel ContinuousLearning-based Unsupervised Recurrent Spiking Neural Network Model (CLURSNN),trained with spike timing dependent plasticity (STDP). CLURSNN makes onlinepredictions by reconstructing the underlying dynamical system using RandomDelay Embedding by measuring the membrane potential of neurons in the recurrentlayer of the RSNN with the highest betweenness centrality. We also usetopological data analysis to propose a novel methodology using the WassersteinDistance between the persistence homologies of the predicted and observed timeseries as a loss function. We show that the proposed online time seriesprediction methodology outperforms state-of-the-art DNN models when predictingan evolving Lorenz63 dynamical system.", "output": "Brain-Inspired Spiking Neural Network for Online Unsupervised Time Series Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study dynamic discrete choice models, where a commonly studied probleminvolves estimating parameters of agent reward functions (also known as\"structural\" parameters), using agent behavioral data. Maximum likelihoodestimation for such models requires dynamic programming, which is limited bythe curse of dimensionality. In this work, we present a novel algorithm thatprovides a data-driven method for selecting and aggregating states, whichlowers the computational and sample complexity of estimation. Our method worksin two stages. In the first stage, we use a flexible inverse reinforcementlearning approach to estimate agent Q-functions. We use these estimatedQ-functions, along with a clustering algorithm, to select a subset of statesthat are the most pivotal for driving changes in Q-functions. In the secondstage, with these selected \"aggregated\" states, we conduct maximum likelihoodestimation using a commonly used nested fixed-point algorithm. The proposedtwo-stage approach mitigates the curse of dimensionality by reducing theproblem dimension. Theoretically, we derive finite-sample bounds on theassociated estimation error, which also characterize the trade-off ofcomputational complexity, estimation error, and sample complexity. Wedemonstrate the empirical performance of the algorithm in two classic dynamicdiscrete choice estimation applications.", "output": "A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data valuation is a powerful framework for providing statistical insightsinto which data are beneficial or detrimental to model training. ManyShapley-based data valuation methods have shown promising results in variousdownstream tasks, however, they are well known to be computationallychallenging as it requires training a large number of models. As a result, ithas been recognized as infeasible to apply to large datasets. To address thisissue, we propose Data-OOB, a new data valuation method for a bagging modelthat utilizes the out-of-bag estimate. The proposed method is computationallyefficient and can scale to millions of data by reusing trained weak learners.Specifically, Data-OOB takes less than 2.25 hours on a single CPU processorwhen there are $10^6$ samples to evaluate and the input dimension is 100.Furthermore, Data-OOB has solid theoretical interpretations in that itidentifies the same important data point as the infinitesimal jackknifeinfluence function when two different points are compared. We conductcomprehensive experiments using 12 classification datasets, each with thousandsof sample sizes. We demonstrate that the proposed method significantlyoutperforms existing state-of-the-art data valuation methods in identifyingmislabeled data and finding a set of helpful (or harmful) data points,highlighting the potential for applying data values in real-world applications.", "output": "Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate and efficient classification of different types of cancer iscritical for early detection and effective treatment. In this paper, we presentthe results of our experiments using the EfficientNet algorithm forclassification of brain tumor, breast cancer mammography, chest cancer, andskin cancer. We used publicly available datasets and preprocessed the images toensure consistency and comparability. Our experiments show that theEfficientNet algorithm achieved high accuracy, precision, recall, and F1 scoreson each of the cancer datasets, outperforming other state-of-the-art algorithmsin the literature. We also discuss the strengths and weaknesses of theEfficientNet algorithm and its potential applications in clinical practice. Ourresults suggest that the EfficientNet algorithm is well-suited forclassification of different types of cancer and can be used to improve theaccuracy and efficiency of cancer diagnosis.", "output": "EfficientNet Algorithm for Classification of Different Types of Cancer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Water is the lifeblood of river networks, and its quality plays a crucialrole in sustaining both aquatic ecosystems and human societies. Real-timemonitoring of water quality is increasingly reliant on in-situ sensortechnology. Anomaly detection is crucial for identifying erroneous patterns insensor data, but can be a challenging task due to the complexity andvariability of the data, even under normal conditions. This paper presents asolution to the challenging task of anomaly detection for river network sensordata, which is essential for accurate and continuous monitoring. We use a graphneural network model, the recently proposed Graph Deviation Network (GDN),which employs graph attention-based forecasting to capture the complexspatio-temporal relationships between sensors. We propose an alternate anomalyscoring method, GDN+, based on the learned graph. To evaluate the model'sefficacy, we introduce new benchmarking simulation experiments withhighly-sophisticated dependency structures and subsequence anomalies of varioustypes. We further examine the strengths and weaknesses of this baselineapproach, GDN, in comparison to other benchmarking methods on complexreal-world river network data. Findings suggest that GDN+ outperforms thebaseline approach in high-dimensional data, while also providing improvedinterpretability. We also introduce software called gnnad.", "output": "Graph Neural Network-Based Anomaly Detection for River Network Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have seen a rich literature of data-driven approaches designedfor power grid applications. However, insufficient consideration of domainknowledge can impose a high risk to the practicality of the methods.Specifically, ignoring the grid-specific spatiotemporal patterns (in load,generation, and topology, etc.) can lead to outputting infeasible,unrealizable, or completely meaningless predictions on new inputs. To addressthis concern, this paper investigates real-world operational data to provideinsights into power grid behavioral patterns, including the time-varyingtopology, load, and generation, as well as the spatial differences (in peakhours, diverse styles) between individual loads and generations. Then based onthese observations, we evaluate the generalization risks in some existing MLworks causedby ignoring these grid-specific patterns in model design andtraining.", "output": "Power Grid Behavioral Patterns and Risks of Generalization in Applied Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many recent advances in natural language generation have been fueled bytraining large language models on internet-scale data. However, this paradigmcan lead to models that generate toxic, inaccurate, and unhelpful content, andautomatic evaluation metrics often fail to identify these behaviors. As modelsbecome more capable, human feedback is an invaluable signal for evaluating andimproving models. This survey aims to provide an overview of the recentresearch that has leveraged human feedback to improve natural languagegeneration. First, we introduce an encompassing formalization of feedback, andidentify and organize existing research into a taxonomy following thisformalization. Next, we discuss how feedback can be described by its format andobjective, and cover the two approaches proposed to use feedback (either fortraining or decoding): directly using the feedback or training feedback models.We also discuss existing datasets for human-feedback data collection, andconcerns surrounding feedback collection. Finally, we provide an overview ofthe nascent field of AI feedback, which exploits large language models to makejudgments based on a set of principles and minimize the need for humanintervention.", "output": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A bound uniform over various loss-classes is given for data generated bystationary and phi-mixing processes, where the mixing time (the time needed toobtain approximate independence) enters the sample complexity only in anadditive way. For slowly mixing processes this can be a considerable advantageover results with multiplicative dependence on the mixing time. The admissibleloss-classes include functions with prescribed Lipschitz norms or smoothnessparameters. The bound can also be applied to be uniform over unconstrainedloss-classes, where it depends on local Lipschitz properties of the function onthe sample path.", "output": "Generalization for slowly mixing processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scaling language models have revolutionized widespread NLP tasks, yet littlecomprehensively explored few-shot relation extraction with large languagemodels. In this paper, we investigate principal methodologies, in-contextlearning and data generation, for few-shot relation extraction via GPT-3.5through exhaustive experiments. To enhance few-shot performance, we furtherpropose task-related instructions and schema-constrained data generation. Weobserve that in-context learning can achieve performance on par with previousprompt learning approaches, and data generation with the large language modelcan boost previous solutions to obtain new state-of-the-art few-shot results onfour widely-studied relation extraction datasets. We hope our work can inspirefuture research for the capabilities of large language models in few-shotrelation extraction. Code is available in", "output": "How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have exhibited excellent performance in various domains. Theprobability flow ordinary differential equation (ODE) of diffusion models(i.e., diffusion ODEs) is a particular case of continuous normalizing flows(CNFs), which enables deterministic inference and exact likelihood evaluation.However, the likelihood estimation results by diffusion ODEs are still far fromthose of the state-of-the-art likelihood-based generative models. In this work,we propose several improved techniques for maximum likelihood estimation fordiffusion ODEs, including both training and evaluation perspectives. Fortraining, we propose velocity parameterization and explore variance reductiontechniques for faster convergence. We also derive an error-bounded high-orderflow matching objective for finetuning, which improves the ODE likelihood andsmooths its trajectory. For evaluation, we propose a novel training-freetruncated-normal dequantization to fill the training-evaluation gap commonlyexisting in diffusion ODEs. Building upon these techniques, we achievestate-of-the-art likelihood estimation results on image datasets (2.56 onCIFAR-10, 3.43/3.69 on ImageNet-32) without variational dequantization or dataaugmentation.", "output": "Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To balance quality and cost, various domain areas of science and engineeringrun simulations at multiple levels of sophistication. Multi-fidelity activelearning aims to learn a direct mapping from input parameters to simulationoutputs at the highest fidelity by actively acquiring data from multiplefidelity levels. However, existing approaches based on Gaussian processes arehardly scalable to high-dimensional data. Deep learning-based methods oftenimpose a hierarchical structure in hidden representations, which only supportspassing information from low-fidelity to high-fidelity. These approaches canlead to the undesirable propagation of errors from low-fidelity representationsto high-fidelity ones. We propose a novel framework called DisentangledMulti-fidelity Deep Bayesian Active Learning (D-MFDAL), that learns thesurrogate models conditioned on the distribution of functions at multiplefidelities. On benchmark tasks of learning deep surrogates of partialdifferential equations including heat equation, Poisson's equation and fluidsimulations, our approach significantly outperforms state-of-the-art inprediction accuracy and sample efficiency. Our code is available at", "output": "Disentangled Multi-Fidelity Deep Bayesian Active Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Matrix manifolds, such as manifolds of Symmetric Positive Definite (SPD)matrices and Grassmann manifolds, appear in many applications. Recently, byapplying the theory of gyrogroups and gyrovector spaces that is a powerfulframework for studying hyperbolic geometry, some works have attempted to buildprincipled generalizations of Euclidean neural networks on matrix manifolds.However, due to the lack of many concepts in gyrovector spaces for theconsidered manifolds, e.g., the inner product and gyroangles, techniques andmathematical tools provided by these works are still limited compared to thosedeveloped for studying hyperbolic geometry. In this paper, we generalize somenotions in gyrovector spaces for SPD and Grassmann manifolds, and propose newmodels and layers for building neural networks on these manifolds. We show theeffectiveness of our approach in two applications, i.e., human actionrecognition and knowledge graph completion.", "output": "Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimal margin Distribution Machine (ODM) is a newly proposed statisticallearning framework rooting in the novel margin theory, which demonstratesbetter generalization performance than the traditional large margin basedcounterparts. Nonetheless, it suffers from the ubiquitous scalability problemregarding both computation time and memory as other kernel methods. This paperproposes a scalable ODM, which can achieve nearly ten times speedup compared tothe original ODM training method. For nonlinear kernels, we propose a noveldistribution-aware partition method to make the local ODM trained on eachpartition be close and converge fast to the global one. When linear kernel isapplied, we extend a communication efficient SVRG method to accelerate thetraining further. Extensive empirical studies validate that our proposed methodis highly computational efficient and almost never worsen the generalization.", "output": "Scalable Optimal Margin Distribution Machine."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continuous-time reinforcement learning tasks commonly use discrete steps offixed cycle times for actions. As practitioners need to choose the action-cycletime for a given task, a significant concern is whether the hyper-parameters ofthe learning algorithm need to be re-tuned for each choice of the cycle time,which is prohibitive for real-world robotics. In this work, we investigate thewidely-used baseline hyper-parameter values of two policy gradient algorithms-- PPO and SAC -- across different cycle times. Using a benchmark task wherethe baseline hyper-parameters of both algorithms were shown to work well, wereveal that when a cycle time different than the task default is chosen, PPOwith baseline hyper-parameters fails to learn. Moreover, both PPO and SAC withtheir baseline hyper-parameters perform substantially worse than their tunedvalues for each cycle time. We propose novel approaches for setting thesehyper-parameters based on the cycle time. In our experiments on simulated andreal-world robotic tasks, the proposed approaches performed at least as well asthe baseline hyper-parameters, with significantly better performance for mostchoices of the cycle time, and did not result in learning failure for any cycletime. Hyper-parameter tuning still remains a significant barrier for real-worldrobotics, as our approaches require some initial tuning on a new task, eventhough it is negligible compared to an extensive tuning for each cycle time.Our approach requires no additional tuning after the cycle time is changed fora given task and is a step toward avoiding extensive and costly hyper-parametertuning for real-world policy optimization.", "output": "Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale multi-modal contrastive learning frameworks like CLIP typicallyrequire a large amount of image-text samples for training. However, thesesamples are always collected continuously in real scenarios. This paperdiscusses the feasibility of continual CLIP training using streaming data.Unlike continual learning based on self-supervised learning methods for pureimages, which is empirically robust against catastrophic forgetting, CLIP'sperformance degeneration in the continual setting is significant andnon-neglectable. By analyzing the changes in the model's representation spaceduring continual CLIP training from a spatial geometry perspective, we exploreand summarize these spatial variations as Spatial Disorder (SD), which can bedivided into Intra-modal Rotation and Inter-modal Deviation. Moreover, weempirically and theoretically demonstrate how SD leads to a performance declinefor CLIP on cross-modal retrieval tasks. To alleviate SD, we propose a newcontinual vision-language representation learning framework Mod-X: Maintainoff-diagonal information-matriX. By selectively aligning the off-diagonalinformation distribution of contrastive matrices, the Mod-X improves thecapability of the multi-modal model by maintaining the multi-modalrepresentation space alignment on the old data domain during continuouslyfitting the new training data domain. Experiments on commonly used datasetswith different scales and scopes have demonstrated the effectiveness of ourmethod.", "output": "Continual Vision-Language Representation Learning with Off-Diagonal Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We apply convolutional neural networks (CNN) to the problem of imageorientation detection in the context of determining the correct orientation(from 0, 90, 180, and 270 degrees) of a consumer photo. The problem isespecially important for digitazing analog photographs. We substantiallyimprove on the published state of the art in terms of the performance on one ofthe standard datasets, and test our system on a more difficult large dataset ofconsumer photos. We use Guided Backpropagation to obtain insights into how ourCNN detects photo orientation, and to explain its mistakes.", "output": "Automatic Photo Orientation Detection with Convolutional Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a lot of research effort into developing different techniques forneural networks compression. However, the community lacks standardisedevaluation metrics, which are key to identifying the most suitable compressiontechnique for different applications. This paper reviews existing neuralnetwork compression evaluation metrics and implements them into astandardisation framework called NetZIP. We introduce two novel metrics tocover existing gaps of evaluation in the literature: 1) Compression andHardware Agnostic Theoretical Speed (CHATS) and 2) Overall Compression Success(OCS). We demonstrate the use of NetZIP using two case studies focusing onobject classification and object detection.", "output": "Evaluation Metrics for DNNs Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of root cause analysis (RCA) is to identify the root causes ofsystem faults/failures by analyzing system monitoring data. Efficient RCA cangreatly accelerate system failure recovery and mitigate system damages orfinancial losses. However, previous research has mostly focused on developingoffline RCA algorithms, which often require manually initiating the RCAprocess, a significant amount of time and data to train a robust model, andthen being retrained from scratch for a new system fault.In this paper, we propose CORAL, a novel online RCA framework that canautomatically trigger the RCA process and incrementally update the RCA model.CORAL consists of Trigger Point Detection, Incremental Disentangled CausalGraph Learning, and Network Propagation-based Root Cause Localization. TheTrigger Point Detection component aims to detect system state transitionsautomatically and in near-real-time. To achieve this, we develop an onlinetrigger point detection approach based on multivariate singular spectrumanalysis and cumulative sum statistics. To efficiently update the RCA model, wepropose an incremental disentangled causal graph learning approach to decouplethe state-invariant and state-dependent information. After that, CORAL appliesa random walk with restarts to the updated causal graph to accurately identifyroot causes. The online RCA process terminates when the causal graph and thegenerated root cause list converge. Extensive experiments on three real-worlddatasets with case studies demonstrate the effectiveness and superiority of theproposed framework.", "output": "Disentangled Causal Graph Learning forOnline Unsupervised Root Cause Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Speech processing techniques are useful for analyzing speech and languagedevelopment in children with Autism Spectrum Disorder (ASD), who are oftenvaried and delayed in acquiring these skills. Early identification andintervention are crucial, but traditional assessment methodologies such ascaregiver reports are not adequate for the requisite behavioral phenotyping.Natural Language Sample (NLS) analysis has gained attention as a promisingcomplement. Researchers have developed benchmarks for spoken languagecapabilities in children with ASD, obtainable through the analysis of NLS. Thispaper proposes applications of speech processing technologies in support ofautomated assessment of children's spoken language development byclassification between child and adult speech and between speech and nonverbalvocalization in NLS, with respective F1 macro scores of 82.6% and 67.8%,underscoring the potential for accurate and scalable tools for ASD research andclinical use.", "output": "Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present SPECTRON, a novel approach to adapting pre-trained language models(LMs) to perform speech continuation. By leveraging pre-trained speechencoders, our model generates both text and speech outputs with the entiresystem being trained end-to-end operating directly on spectrograms. Trainingthe entire model in the spectrogram domain simplifies our speech continuationsystem versus existing cascade methods which use discrete speechrepresentations. We further show our method surpasses existing spoken languagemodels both in semantic content and speaker preservation while also benefitingfrom the knowledge transferred from pre-existing models. Audio samples can befound in our website ", "output": "LMs with a Voice: Spoken Language Modeling beyond Speech Tokens."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have discovered that Chain-of-Thought prompting (CoT) candramatically improve the performance of Large Language Models (LLMs),particularly when dealing with complex tasks involving mathematics orreasoning. Despite the enormous empirical success, the underlying mechanismsbehind CoT and how it unlocks the potential of LLMs remain elusive. In thispaper, we take a first step towards theoretically answering these questions.Specifically, we examine the expressivity of LLMs with CoT in solvingfundamental mathematical and decision-making problems. We start by giving animpossibility result showing that bounded-depth Transformers are unable todirectly produce correct answers for basic arithmetic/equation tasks unless themodel size grows super-polynomially with respect to the input length. Incontrast, we then prove by construction that autoregressive Transformers ofconstant size suffice to solve both tasks by generating CoT derivations using acommonly-used math language format. Moreover, we show LLMs with CoT are capableof solving a general class of decision-making problems known as DynamicProgramming, thus justifying its power in tackling complex real-world tasks.Finally, extensive experiments on four tasks show that, while Transformersalways fail to predict the answers directly, they can consistently learn togenerate correct solutions step-by-step given sufficient CoT demonstrations.", "output": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Past research has identified a rich set of handcrafted linguistic featuresthat can potentially assist various tasks. However, their extensive numbermakes it difficult to effectively select and utilize existing handcraftedfeatures. Coupled with the problem of inconsistent implementation acrossresearch works, there has been no categorization scheme or generally-acceptedfeature names. This creates unwanted confusion. Also, most existing handcraftedfeature extraction libraries are not open-source or not actively maintained. Asa result, a researcher often has to build such an extraction system from theground up.We collect and categorize more than 220 popular handcrafted features groundedon past literature. Then, we conduct a correlation analysis study on severaltask-specific datasets and report the potential use cases of each feature.Lastly, we devise a multilingual handcrafted linguistic feature extractionsystem in a systematically expandable manner. We open-source our system forpublic access to a rich set of pre-implemented handcrafted features. Our systemis coined LFTK and is the largest of its kind. Find it atgithub.com/brucewlee/lftk.", "output": "LFTK: Handcrafted Features in Computational Linguistics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given samples from two joint distributions, we consider the problem ofOptimal Transportation (OT) between the corresponding distributions conditionedon a common variable. The objective of this work is to estimate the associatedtransport cost (Wasserstein distance) as well as the transport plan between theconditionals as a function of the conditioned value. Since matching conditionaldistributions is at the core of supervised training of discriminative modelsand (implicit) conditional-generative models, OT between conditionals has thepotential to be employed in diverse machine learning applications. However,since the conditionals involved in OT are implicitly specified via the jointsamples, it is challenging to formulate this problem, especially when (i) thevariable conditioned on is continuous and (ii) the marginal of this variable inthe two distributions is different. We overcome these challenges by employing aspecific kernel MMD (Maximum Mean Discrepancy) based regularizer that ensuresthe marginals of our conditional transport plan are close to the conditionalsspecified via the given joint samples. Under mild conditions, we prove that ourestimator for this regularized transport cost is statistically consistent andderive finite-sample bounds on the estimation error. Application-specificdetails for parameterizing our conditional transport plan are also presented.Furthermore, we empirically evaluate our methodology on benchmark datasets inapplications like classification, prompt learning for few-shot classification,and conditional-generation in the context of predicting cell responses tocancer treatment.", "output": "Empirical Optimal Transport between Conditional Distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Reinforcement Learning (DRL) has demonstrated promising capability insolving complex control problems. However, DRL applications in safety-criticalsystems are hindered by the inherent lack of robust verification techniques toassure their performance in such applications. One of the key requirements ofthe verification process is the development of effective techniques to explainthe system functionality, i.e., why the system produces specific results ingiven circumstances. Recently, interpretation methods based on theCounterfactual (CF) explanation approach have been proposed to address theproblem of explanation in DRLs. This paper proposes a novel CF explanationframework to explain the decisions made by a black-box DRL. To evaluate theefficacy of the proposed explanation framework, we carried out severalexperiments in the domains of automated driving systems and Atari Pong game.Our analysis demonstrates that the proposed framework generates plausible andmeaningful explanations for various decisions made by deep underlying DRLs.Source codes are available at:url{", "output": "Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using Policy Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine Learning (ML) is increasingly used to drive the operation of complexdistributed systems deployed on the cloud-edge continuum enabled by 5G.Correspondingly, distributed systems' behavior is becoming morenon-deterministic in nature. This evolution of distributed systems requires thedefinition of new assurance approaches for the verification of non-functionalproperties. Certification, the most popular assurance technique for system andsoftware verification, is not immediately applicable to systems whose behavioris determined by Machine Learning-based inference. However, there is anincreasing push from policy makers, regulators, and industrial stakeholderstowards the definition of techniques for the certification of non-functionalproperties (e.g., fairness, robustness, privacy) of ML. This article analyzesthe challenges and deficiencies of current certification schemes, discussesopen research issues and proposes a first certification scheme for ML-baseddistributed systems.", "output": "Towards Certification of Machine Learning-Based Distributed Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The captivating realm of Minecraft has attracted substantial researchinterest in recent years, serving as a rich platform for developing intelligentagents capable of functioning in open-world environments. However, the currentresearch landscape predominantly focuses on specific objectives, such as thepopular \"ObtainDiamond\" task, and has not yet shown effective generalization toa broader spectrum of tasks. Furthermore, the current leading success rate forthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations ofReinforcement Learning (RL) based controllers used in existing methods. Totackle these challenges, we introduce Ghost in the Minecraft (GITM), a novelframework integrates Large Language Models (LLMs) with text-based knowledge andmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. Theseagents, equipped with the logic and common sense capabilities of LLMs, canskillfully navigate complex, sparse-reward environments with text-basedinteractions. We develop a set of structured actions and leverage LLMs togenerate action plans for the agents to execute. The resulting LLM-based agentmarkedly surpasses previous methods, achieving a remarkable improvement of+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superiorrobustness compared to traditional RL-based controllers. Notably, our agent isthe first to procure all items in the Minecraft Overworld technology tree,demonstrating its extensive capabilities. GITM does not need any GPU fortraining, but a single CPU node with 32 CPU cores is enough. This researchshows the potential of LLMs in developing capable agents for handlinglong-horizon, complex tasks and adapting to uncertainties in open-worldenvironments. See the project website at ", "output": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The emergent capabilities of Large Language Models (LLMs) have made itcrucial to align their values with those of humans. Current methodologiestypically attempt alignment with a homogeneous human value and requires humanverification, yet lack consensus on the desired aspect and depth of alignmentand resulting human biases. In this paper, we propose A2EHV, an AutomatedAlignment Evaluation with a Heterogeneous Value system that (1) is automated tominimize individual human biases, and (2) allows assessments against varioustarget values to foster heterogeneous agents. Our approach pivots on theconcept of value rationality, which represents the ability for agents toexecute behaviors that satisfy a target value the most. The quantification ofvalue rationality is facilitated by the Social Value Orientation framework fromsocial psychology, which partitions the value space into four categories toassess social preferences from agents' behaviors. We evaluate the valuerationality of eight mainstream LLMs and observe that large models are moreinclined to align neutral values compared to those with strong personal values.By examining the behavior of these LLMs, we contribute to a deeperunderstanding of value alignment within a heterogeneous value system.", "output": "Heterogeneous Value Evaluation for Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building generalizable AI models is one of the primary challenges in thehealthcare domain. While radiologists rely on generalizable descriptive rulesof abnormality, Neural Network (NN) models suffer even with a slight shift ininput distribution (e.g., scanner type). Fine-tuning a model to transferknowledge from one domain to another requires a significant amount of labeleddata in the target domain. In this paper, we develop an interpretable modelthat can be efficiently fine-tuned to an unseen target domain with minimalcomputational cost. We assume the interpretable component of NN to beapproximately domain-invariant. However, interpretable models typicallyunderperform compared to their Blackbox (BB) variants. We start with a BB inthe source domain and distill it into a emph{mixture} of shallow interpretablemodels using human-understandable concepts. As each interpretable model coversa subset of data, a mixture of interpretable models achieves comparableperformance as BB. Further, we use the pseudo-labeling technique fromsemi-supervised learning (SSL) to learn the concept classifier in the targetdomain, followed by fine-tuning the interpretable models in the target domain.We evaluate our model using a real-life large-scale chest-X-ray (CXR)classification dataset. The code is available at:url{", "output": "Distilling BlackBox to Interpretable models for Efficient Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attention has become a common ingredient in deep learning architectures. Itadds a dynamical selection of information on top of the static selection ofinformation supported by weights. In the same way, we can imagine ahigher-order informational filter built on top of attention: an AttentionSchema (AS), namely, a descriptive and predictive model of attention. Incognitive neuroscience, Attention Schema Theory (AST) supports this idea ofdistinguishing attention from AS. A strong prediction of this theory is that anagent can use its own AS to also infer the states of other agents' attentionand consequently enhance coordination with other agents. As such, multi-agentreinforcement learning would be an ideal setting to experimentally test thevalidity of AST. We explore different ways in which attention and AS interactwith each other. Our preliminary results indicate that agents that implementthe AS as a recurrent internal control achieve the best performance. Ingeneral, these exploratory experiments suggest that equipping artificial agentswith a model of attention can enhance their social intelligence.", "output": "Attention Schema in Neural Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning (DL) has emerged as a powerful subset of machine learning (ML)and artificial intelligence (AI), outperforming traditional ML methods,especially in handling unstructured and large datasets. Its impact spans acrossvarious domains, including speech recognition, healthcare, autonomous vehicles,cybersecurity, predictive analytics, and more. However, the complexity anddynamic nature of real-world problems present challenges in designing effectivedeep learning models. Consequently, several deep learning models have beendeveloped to address different problems and applications. In this article, weconduct a comprehensive survey of various deep learning models, includingConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),Generative Models, Deep Reinforcement Learning (DRL), and Deep TransferLearning. We examine the structure, applications, benefits, and limitations ofeach model. Furthermore, we perform an analysis using three publicly availabledatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowneddeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),Bidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.", "output": "A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study the implicit regularization of stochastic gradientdescent (SGD) through the lens of {em dynamical stability} (Wu et al., 2018).We start by revising existing stability analyses of SGD, showing how theFrobenius norm and trace of Hessian relate to different notions of stability.Notably, if a global minimum is linearly stable for SGD, then the trace ofHessian must be less than or equal to $2/eta$, where $eta$ denotes thelearning rate. By contrast, for gradient descent (GD), the stability imposes asimilar constraint but only on the largest eigenvalue of Hessian. We then turnto analyze the generalization properties of these stable minima, focusingspecifically on two-layer ReLU networks and diagonal linear networks. Notably,we establish the {em equivalence} between these metrics of sharpness andcertain parameter norms for the two models, which allows us to show that thestable minima of SGD provably generalize well. By contrast, thestability-induced regularization of GD is provably too weak to ensuresatisfactory generalization. This discrepancy provides an explanation of whySGD often generalizes better than GD. Note that the learning rate (LR) plays apivotal role in the strength of stability-induced regularization. As the LRincreases, the regularization effect becomes more pronounced, elucidating whySGD with a larger LR consistently demonstrates superior generalizationcapabilities. Additionally, numerical experiments are provided to support ourtheoretical findings.", "output": "The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents Translatotron 3, a novel approach to train a directspeech-to-speech translation model from monolingual speech-text datasets onlyin a fully unsupervised manner. Translatotron 3 combines masked autoencoder,unsupervised embedding mapping, and back-translation to achieve this goal.Experimental results in speech-to-speech translation tasks between Spanish andEnglish show that Translatotron 3 outperforms a baseline cascade system,reporting 18.14 BLEU points improvement on the synthesizedUnpaired-Conversational dataset. In contrast to supervised approaches thatnecessitate real paired data, which is unavailable, or specialized modeling toreplicate para-/non-linguistic information, Translatotron 3 showcases itscapability to retain para-/non-linguistic such as pauses, speaking rates, andspeaker identity. Audio samples can be found in our website<a href=\" http URL</a>", "output": "Translatotron 3: Speech to Speech Translation with Monolingual Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conformal prediction is emerging as a popular paradigm for providing rigorousuncertainty quantification in machine learning since it can be easily appliedas a post-processing step to already trained models. In this paper, we extendconformal prediction to the federated learning setting. The main challenge weface is data heterogeneity across the clients - this violates the fundamentaltenet of exchangeability required for conformal prediction. We propose a weakernotion of partial exchangeability, better suited to the FL setting, and use itto develop the Federated Conformal Prediction (FCP) framework. We show FCPenjoys rigorous theoretical guarantees and excellent empirical performance onseveral computer vision and medical imaging datasets. Our results demonstrate apractical approach to incorporating meaningful uncertainty quantification indistributed and heterogeneous environments. We provide code used in ourexperiments ", "output": "Federated Conformal Predictors for Distributed Uncertainty Quantification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Root causes of disease intuitively correspond to root vertices that increasethe likelihood of a diagnosis. This description of a root cause neverthelesslacks the rigorous mathematical formulation needed for the development ofcomputer algorithms designed to automatically detect root causes from data.Prior work defined patient-specific root causes of disease using aninterventionalist account that only climbs to the second rung of Pearl's Ladderof Causation. In this theoretical piece, we climb to the third rung byproposing a counterfactual definition matching clinical intuition based onfixed factual data alone. We then show how to assign a root causal contributionscore to each variable using Shapley values from explainable artificialintelligence. The proposed counterfactual formulation of patient-specific rootcauses of disease accounts for noisy labels, adapts to disease prevalence andadmits fast computation without the need for counterfactual simulation.", "output": "Counterfactual Formulation of Patient-Specific Root Causes of Disease."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A hyper-relational knowledge graph has been recently studied where a tripletis associated with a set of qualifiers; a qualifier is composed of a relationand an entity, providing auxiliary information for a triplet. While existinghyper-relational knowledge graph embedding methods assume that the entities arediscrete objects, some information should be represented using numeric values,e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, OxfordUniv.) can be associated with a qualifier such as (start time, 1911). In thispaper, we propose a unified framework named HyNT that learns representations ofa hyper-relational knowledge graph containing numeric literals in eithertriplets or qualifiers. We define a context transformer and a predictiontransformer to learn the representations based not only on the correlationsbetween a triplet and its qualifiers but also on the numeric information. Bylearning compact representations of triplets and qualifiers and feeding theminto the transformers, we reduce the computation cost of using transformers.Using HyNT, we can predict missing numeric values in addition to missingentities or relations in a hyper-relational knowledge graph. Experimentalresults show that HyNT significantly outperforms state-of-the-art methods onreal-world datasets.", "output": "Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative neural models hold great promise in enhancing programmingeducation by synthesizing new content for students. We seek to design neuralmodels that can automatically generate programming tasks for a givenspecification in the context of visual programming domains. Despite the recentsuccesses of large generative models like GPT-4, our initial results show thatthese models are ineffective in synthesizing visual programming tasks andstruggle with logical and spatial reasoning. We propose a novel neuro-symbolictechnique, NeurTaskSyn, that can synthesize programming tasks for aspecification given in the form of desired programming concepts exercised byits solution code and constraints on the visual task. NeurTaskSyn has twocomponents: the first component is trained via imitation learning procedure togenerate possible solution codes, and the second component is trained viareinforcement learning procedure to guide an underlying symbolic executionengine that generates visual tasks for these codes. We demonstrate theeffectiveness of NeurTaskSyn through an extensive empirical evaluation and aqualitative study on reference tasks taken from the Hour of Code: Classic Mazechallenge by Code-dot-org and the Intro to Programming with Karel course byCodeHS-dot-com.", "output": "Neural Task Synthesis for Visual Programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trustworthy Federated Learning (TFL) typically leverages protectionmechanisms to guarantee privacy. However, protection mechanisms inevitablyintroduce utility loss or efficiency reduction while protecting data privacy.Therefore, protection mechanisms and their parameters should be carefullychosen to strike an optimal tradeoff between textit{privacy leakage},textit{utility loss}, and textit{efficiency reduction}. To this end,federated learning practitioners need tools to measure the three factors andoptimize the tradeoff between them to choose the protection mechanism that ismost appropriate to the application at hand. Motivated by this requirement, wepropose a framework that (1) formulates TFL as a problem of finding aprotection mechanism to optimize the tradeoff between privacy leakage, utilityloss, and efficiency reduction and (2) formally defines bounded measurements ofthe three factors. We then propose a meta-learning algorithm to approximatethis optimization problem and find optimal protection parameters forrepresentative protection mechanisms, including Randomization, HomomorphicEncryption, Secret Sharing, and Compression. We further design estimationalgorithms to quantify these found optimal protection parameters in a practicalhorizontal federated learning setting and provide a theoretical analysis of theestimation error.", "output": "A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shownexceptional performance across various tasks. Although parameter-efficientfine-tuning (PEFT) has emerged to cheaply fine-tune these large models ondownstream tasks, their deployment is still hindered by the vast model scaleand computational costs. Neural network pruning offers a solution for modelcompression by removing redundant parameters, but most existing methods rely oncomputing parameter gradients. However, obtaining the gradients iscomputationally prohibitive for LPMs, which necessitates the exploration ofalternative approaches. To this end, we propose a unified framework forefficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first designa PEFT-aware pruning criterion, which utilizes the values and gradients ofLow-Rank Adaption (LoRA), rather than the gradients of pre-trained parametersfor importance estimation. We then propose an iterative pruning procedure toremove redundant parameters while maximizing the advantages of PEFT. Thus, ourLoRAPrune delivers an accurate, compact model for efficient inference in ahighly cost-effective manner. Experimental results on various tasks demonstratethat our method achieves state-of-the-art results. For instance, in the VTAB-1kbenchmark, LoRAPrune utilizes only 0.76% of the trainable parameters andoutperforms magnitude and movement pruning methods by a significant margin,achieving a mean Top-1 accuracy that is 5.7% and 4.3% higher, respectively.Moreover, our approach achieves comparable performance to PEFT methods,highlighting its efficacy in delivering high-quality results while benefitingfrom the advantages of pruning.", "output": "Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As large language models continue to be widely developed, robust uncertaintyquantification techniques will become crucial for their safe deployment inhigh-stakes scenarios. In this work, we explore how conformal prediction can beused to provide uncertainty quantification in language models for the specifictask of multiple-choice question-answering. We find that the uncertaintyestimates from conformal prediction are tightly correlated with predictionaccuracy. This observation can be useful for downstream applications such asselective classification and filtering out low-quality predictions. We alsoinvestigate the exchangeability assumption required by conformal prediction toout-of-subject questions, which may be a more realistic scenario for manypractical applications. Our work contributes towards more trustworthy andreliable usage of large language models in safety-critical situations, whererobust guarantees of error rate are required.", "output": "Conformal Prediction with Large Language Models for Multi-Choice Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a strongly polynomial-time algorithm to generate bandwidth optimalallgather/reduce-scatter on any network topology, with or without switches. Ouralgorithm constructs pipeline schedules achieving provably the best possiblebandwidth performance on a given topology. To provide a universal solution, wemodel the network topology as a directed graph with heterogeneous linkcapacities and switches directly as vertices in the graph representation. Thealgorithm is strongly polynomial-time with respect to the topology size. Thiswork heavily relies on previous graph theory work on edge-disjoint spanningtrees and edge splitting. While we focus on allgather, the methods in thispaper can be easily extended to generate schedules for reduce, broadcast,reduce-scatter, and allreduce.", "output": "Bandwidth Optimal Pipeline Schedule for Collective Communication."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Path of Destruction (PoD) is a self-supervised method for learning iterativegenerators. The core idea is to produce a training set by destroying a set ofartifacts, and for each destructive step create a training instance based onthe corresponding repair action. A generator trained on this dataset can thengenerate new artifacts by repairing from arbitrary states. The PoD method isvery data-efficient in terms of original training examples and well-suited tofunctional artifacts composed of categorical data, such as game levels anddiscrete 3D structures. In this paper, we extend the Path of Destruction methodto allow designer control over aspects of the generated artifacts.Controllability is introduced by adding conditional inputs to the state-actionpairs that make up the repair trajectories. We test the controllable PoD methodin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.", "output": "Controllable Path of Destruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a recent surge in the development of spatio-temporal forecastingmodels in the transportation domain. Long-range traffic forecasting, however,remains a challenging task due to the intricate and extensive spatio-temporalcorrelations observed in traffic networks. Current works primarily rely on roadnetworks with graph structures and learn representations using graph neuralnetworks (GNNs), but this approach suffers from over-smoothing problem in deeparchitectures. To tackle this problem, recent methods introduced thecombination of GNNs with residual connections or neural ordinary differentialequations (ODE). However, current graph ODE models face two key limitations infeature extraction: (1) they lean towards global temporal patterns, overlookinglocal patterns that are important for unexpected events; and (2) they lackdynamic semantic edges in their architectural design. In this paper, we proposea novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-ODE)which is designed with multiple connective ODE-GNN modules to learn betterrepresentations by capturing different views of complex local and globaldynamic spatio-temporal dependencies. We also add some techniques like sharedweights and divergence constraints into the intermediate layers of distinctODE-GNN modules to further improve their communication towards the forecastingtask. Our extensive set of experiments conducted on six real-world datasetsdemonstrate the superior performance of GRAM-ODE compared with state-of-the-artbaselines as well as the contribution of different components to the overallperformance. The code is available at ", "output": "Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although Graph Neural Networks (GNNs) have been successful in nodeclassification tasks, their performance heavily relies on the availability of asufficient number of labeled nodes per class. In real-world situations, not allclasses have many labeled nodes and there may be instances where the modelneeds to classify new classes, making manual labeling difficult. To solve thisproblem, it is important for GNNs to be able to classify nodes with a limitednumber of labeled nodes, known as few-shot node classification. Previousepisodic meta-learning based methods have demonstrated success in few-shot nodeclassification, but our findings suggest that optimal performance can only beachieved with a substantial amount of diverse training meta-tasks. To addressthis challenge of meta-learning based few-shot learning (FSL), we propose a newapproach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEGframework enables the model to learn transferable task-adaptation strategiesusing a limited number of training meta-tasks, allowing it to acquiremeta-knowledge for a wide range of meta-tasks. By incorporating equivariantneural networks, TEG can utilize their strong generalization abilities to learnhighly adaptable task-specific strategies. As a result, TEG achievesstate-of-the-art performance with limited training meta-tasks. Our experimentson various benchmark datasets demonstrate TEG's superiority in terms ofaccuracy and generalization ability, even when using minimal meta-trainingdata, highlighting the effectiveness of our proposed approach in addressing thechallenges of meta-learning based few-shot node classification. Our code isavailable at the following link: ", "output": "Task-Equivariant Graph Few-shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The multi-criteria (MC) recommender system, which leverages MC ratinginformation in a wide range of e-commerce areas, is ubiquitous nowadays.Surprisingly, although graph neural networks (GNNs) have been widely applied todevelop various recommender systems due to GNN's high expressive capability inlearning graph representations, it has been still unexplored how to design MCrecommender systems with GNNs. In light of this, we make the first attempttowards designing a GNN-aided MC recommender system. Specifically, rather thanstraightforwardly adopting existing GNN-based recommendation methods, we devisea novel criteria preference-aware light graph convolution CPA-LGC method, whichis capable of precisely capturing the criteria preference of users as well asthe collaborative signal in complex high-order connectivities. To this end, wefirst construct an MC expansion graph that transforms user--item MC ratingsinto an expanded bipartite graph to potentially learn from the collaborativesignal in MC ratings. Next, to strengthen the capability of criteria preferenceawareness, CPA-LGC incorporates newly characterized embeddings, includinguser-specific criteria-preference embeddings and item-specific criterionembeddings, into our graph convolution model. Through comprehensive evaluationsusing four real-world datasets, we demonstrate (a) the superiority overbenchmark MC recommendation methods and benchmark recommendation methods usingGNNs with tremendous gains, (b) the effectiveness of core components inCPA-LGC, and (c) the computational efficiency.", "output": "Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have shown great promise in unsupervised representationlearning (URL) for multivariate time series, because URL has the capability inlearning generalizable representation for many downstream tasks without usinginaccessible labels. However, existing approaches usually adopt the modelsoriginally designed for other domains (e.g., computer vision) to encode thetime series data and rely on strong assumptions to design learning objectives,which limits their ability to perform well. To deal with these problems, wepropose a novel URL framework for multivariate time series by learningtime-series-specific shapelet-based representation through a popularcontrasting learning paradigm. To the best of our knowledge, this is the firstwork that explores the shapelet-based embedding in the unsupervisedgeneral-purpose representation learning. A unified shapelet-based encoder and anovel learning objective with multi-grained contrasting and multi-scalealignment are particularly designed to achieve our goal, and a dataaugmentation library is employed to improve the generalization. We conductextensive experiments using tens of real-world datasets to assess therepresentation quality on many downstream tasks, including classification,clustering, and anomaly detection. The results demonstrate the superiority ofour method against not only URL competitors, but also techniques speciallydesigned for downstream tasks. Our code has been made publicly available at", "output": "Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study reinforcement learning (RL) in the setting of continuous time andspace, for an infinite horizon with a discounted objective and the underlyingdynamics driven by a stochastic differential equation. Built upon recentadvances in the continuous approach to RL, we develop a notion of occupationtime (specifically for a discounted objective), and show how it can beeffectively used to derive performance-difference and local-approximationformulas. We further extend these results to illustrate their applications inthe PG (policy gradient) and TRPO/PPO (trust region policy optimization/proximal policy optimization) methods, which have been familiar and powerfultools in the discrete RL setting but under-developed in continuous RL. Throughnumerical experiments, we demonstrate the effectiveness and advantages of ourapproach.", "output": "Policy Optimization for Continuous Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a policy gradient algorithm for robust infinite-horizon MarkovDecision Processes (MDPs) with non-rectangular uncertainty sets, therebyaddressing an open challenge in the robust MDP literature. Indeed, uncertaintysets that display statistical optimality properties and make optimal use oflimited data often fail to be rectangular. Unfortunately, the correspondingrobust MDPs cannot be solved with dynamic programming techniques and are infact provably intractable. This prompts us to develop a projected Langevindynamics algorithm tailored to the robust policy evaluation problem, whichoffers global optimality guarantees. We also propose a deterministic policygradient method that solves the robust policy evaluation problem approximately,and we prove that the approximation error scales with a new measure ofnon-rectangularity of the uncertainty set. Numerical experiments showcase thatour projected Langevin dynamics algorithm can escape local optima, whilealgorithms tailored to rectangular uncertainty fail to do so.", "output": "Policy Gradient Algorithms for Robust MDPs with Non-Rectangular Uncertainty Sets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating graphs from a target distribution is a significant challengeacross many domains, including drug discovery and social network analysis. Inthis work, we introduce a novel graph generation method leveraging $K^2$-treerepresentation which was originally designed for lossless graph compression.Our motivation stems from the ability of the $K^2$-trees to enable compactgeneration while concurrently capturing the inherent hierarchical structure ofa graph. In addition, we make further contributions by (1) presenting asequential $K^2$-tree representation that incorporates pruning, flattening, andtokenization processes and (2) introducing a Transformer-based architecturedesigned to generate the sequence by incorporating a specialized treepositional encoding scheme. Finally, we extensively evaluate our algorithm onfour general and two molecular graph datasets to confirm its superiority forgraph generation.", "output": "Hierarchical Graph Generation with $K^2$-trees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a simple and scalable framework for extreme multiclassclassification based on Johnson-Lindenstrauss matrices (JLMs). Using thecolumns of a JLM to embed the labels, a $C$-class classification problem istransformed into a regression problem with $cO(log C)$ output dimension. Wederive an excess risk bound, revealing a tradeoff between computationalefficiency and prediction accuracy, and further show that under the Massartnoise condition, the penalty for dimension reduction vanishes. Our approach iseasily parallelizable, and experimental results demonstrate its effectivenessand scalability in large-scale applications.", "output": "Label Embedding by Johnson-Lindenstrauss Matrices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent offline meta-reinforcement learning (meta-RL) methods typicallyutilize task-dependent behavior policies (e.g., training RL agents on eachindividual task) to collect a multi-task dataset. However, these methods alwaysrequire extra information for fast adaptation, such as offline context fortesting tasks. To address this problem, we first formally characterize a uniquechallenge in offline meta-RL: transition-reward distribution shift betweenoffline datasets and online adaptation. Our theory finds thatout-of-distribution adaptation episodes may lead to unreliable policyevaluation and that online adaptation with in-distribution episodes can ensureadaptation performance guarantee. Based on these theoretical insights, wepropose a novel adaptation framework, called In-Distribution online Adaptationwith uncertainty Quantification (IDAQ), which generates in-distribution contextusing a given uncertainty quantification and performs effective task beliefinference to address new tasks. We find a return-based uncertaintyquantification for IDAQ that performs effectively. Experiments show that IDAQachieves state-of-the-art performance on the Meta-World ML1 benchmark comparedto baselines with/without offline adaptation.", "output": "Offline Meta Reinforcement Learning with In-Distribution Online Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extensive research has been conducted on fault diagnosis of planetarygearboxes using vibration signals and deep learning (DL) approaches. However,DL-based methods are susceptible to the domain shift problem caused by varyingoperating conditions of the gearbox. Although domain adaptation and datasynthesis methods have been proposed to overcome such domain shifts, they areoften not directly applicable in real-world situations where only healthy datais available in the target domain. To tackle the challenge of extreme domainshift scenarios where only healthy data is available in the target domain, thispaper proposes two novel domain knowledge-informed data synthesis methodsutilizing the health data map (HDMap). The two proposed approaches are referredto as scaled CutPaste and FaultPaste. The HDMap is used to physically representthe vibration signal of the planetary gearbox as an image-like matrix, allowingfor visualization of fault-related features. CutPaste and FaultPaste are thenapplied to generate faulty samples based on the healthy data in the targetdomain, using domain knowledge and fault signatures extracted from the sourcedomain, respectively. In addition to generating realistic faults, the proposedmethods introduce scaling of fault signatures for controlled synthesis offaults with various severity levels. A case study is conducted on a planetarygearbox testbed to evaluate the proposed approaches. The results show that theproposed methods are capable of accurately diagnosing faults, even in cases ofextreme domain shift, and can estimate the severity of faults that have notbeen previously observed in the target domain.", "output": "Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Geometry and color information provided by the point clouds are both crucialfor 3D scene understanding. Two pieces of information characterize thedifferent aspects of point clouds, but existing methods lack an elaboratedesign for the discrimination and relevance. Hence we explore a 3Dself-supervised paradigm that can better utilize the relations of point cloudinformation. Specifically, we propose a universal 3D scene pre-trainingframework via Geometry-Color Contrast (Point-GCC), which aligns geometry andcolor information using a Siamese network. To take care of actual applicationtasks, we design (i) hierarchical supervision with point-level contrast andreconstruct and object-level contrast based on the novel deep clustering moduleto close the gap between pre-training and downstream tasks; (ii)architecture-agnostic backbone to adapt for various downstream models.Benefiting from the object-level representation associated with downstreamtasks, Point-GCC can directly evaluate model performance and the resultdemonstrates the effectiveness of our methods. Transfer learning results on awide range of tasks also show consistent improvements across all datasets.e.g., new state-of-the-art object detection results on SUN RGB-D and S3DISdatasets. Codes will be released at ", "output": "Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative diffusion models have recently emerged as a leading approach forgenerating high-dimensional data. In this paper, we show that the dynamics ofthese models exhibit a spontaneous symmetry breaking that divides thegenerative dynamics into two distinct phases: 1) A linear steady-state dynamicsaround a central fixed-point and 2) an attractor dynamics directed towards thedata manifold. These two \"phases\" are separated by the change in stability ofthe central fixed-point, with the resulting window of instability beingresponsible for the diversity of the generated samples. Using both theoreticaland empirical evidence, we show that an accurate simulation of the earlydynamics does not significantly contribute to the final generation, since earlyfluctuations are reverted to the central fixed point. To leverage this insight,we propose a Gaussian late initialization scheme, which significantly improvesmodel performance, achieving up to 3x FID improvements on fast samplers, whilealso increasing sample diversity (e.g., racial composition of generated CelebAimages). Our work offers a new way to understand the generative dynamics ofdiffusion models that has the potential to bring about higher performance andless biased fast-samplers.", "output": "Spontaneous symmetry breaking in generative diffusion models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Audio anti-spoofing for automatic speaker verification aims to safeguardusers' identities from spoofing attacks. Although state-of-the-art spoofingcountermeasure(CM) models perform well on specific datasets, they lackgeneralization when evaluated with different datasets. To address thislimitation, previous studies have explored large pre-trained models, whichrequire significant resources and time. We aim to develop a compact butwell-generalizing CM model that can compete with large pre-trained models. Ourapproach involves multi-dataset co-training and sharpness-aware minimization,which has not been investigated in this domain. Extensive experiments revealthat proposed method yield competitive results across various datasets whileutilizing 4,000 times less parameters than the large pre-trained models.", "output": "Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inductive knowledge graph completion has been considered as the task ofpredicting missing triplets between new entities that are not observed duringtraining. While most inductive knowledge graph completion methods assume thatall entities can be new, they do not allow new relations to appear at inferencetime. This restriction prohibits the existing methods from appropriatelyhandling real-world knowledge graphs where new entities accompany newrelations. In this paper, we propose an INductive knowledge GRAph eMbeddingmethod, InGram, that can generate embeddings of new relations as well as newentities at inference time. Given a knowledge graph, we define a relation graphas a weighted graph consisting of relations and the affinity weights betweenthem. Based on the relation graph and the original knowledge graph, InGramlearns how to aggregate neighboring embeddings to generate relation and entityembeddings using an attention mechanism. Experimental results show that InGramoutperforms 14 different state-of-the-art methods on varied inductive learningscenarios.", "output": "InGram: Inductive Knowledge Graph Embedding via Relation Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendlyframework to extend Recursive Neural Networks (RvNNs) with beam search forlatent structure induction. We further extend this framework by proposing arelaxation of the hard top-k operators in beam search for better propagation ofgradient signals. We evaluate our proposed models in differentout-of-distribution splits in both synthetic and realistic data. Ourexperiments show that BTCell achieves near-perfect performance on severalchallenging structure-sensitive synthetic tasks like ListOps and logicalinference while maintaining comparable performance in realistic data againstother RvNN-based models. Additionally, we identify a previously unknown failurecase for neural models in generalization to unseen number of arguments inListOps. The code is available at:", "output": "Beam Tree Recursive Cells."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Watermarking the outputs of generative models is a crucial technique fortracing copyright and preventing potential harm from AI-generated content. Inthis paper, we introduce a novel technique called Tree-Ring Watermarking thatrobustly fingerprints diffusion model outputs. Unlike existing methods thatperform post-hoc modifications to images after sampling, Tree-Ring Watermarkingsubtly influences the entire sampling process, resulting in a model fingerprintthat is invisible to humans. The watermark embeds a pattern into the initialnoise vector used for sampling. These patterns are structured in Fourier spaceso that they are invariant to convolutions, crops, dilations, flips, androtations. After image generation, the watermark signal is detected byinverting the diffusion process to retrieve the noise vector, which is thenchecked for the embedded signal. We demonstrate that this technique can beeasily applied to arbitrary diffusion models, including text-conditioned StableDiffusion, as a plug-in with negligible loss in FID. Our watermark issemantically hidden in the image space and is far more robust than watermarkingalternatives that are currently deployed. Code is available at", "output": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We explore the use of graph networks to deal with irregular-geometrydetectors in the context of particle reconstruction. Thanks to theirrepresentation-learning capabilities, graph networks can exploit the fulldetector granularity, while natively managing the event sparsity andarbitrarily complex detector geometries. We introduce two distance-weightedgraph network architectures, dubbed GarNet and GravNet layers, and apply themto a typical particle reconstruction task. The performance of the newarchitectures is evaluated on a data set of simulated particle interactions ona toy model of a highly granular calorimeter, loosely inspired by the endcapcalorimeter to be installed in the CMS detector for the High-Luminosity LHCphase. We study the clustering of energy depositions, which is the basis forcalorimetric particle reconstruction, and provide a quantitative comparison toalternative approaches. The proposed algorithms provide an interestingalternative to existing methods, offering equally performing or lessresource-demanding solutions with less underlying assumptions on the detectorgeometry and, consequently, the possibility to generalize to other detectors.", "output": "Learning representations of irregular particle-detector geometry with distance-weighted graph networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We advance the state-of-the-art in the accuracy of code prediction (nexttoken prediction) used in autocomplete systems. First, we report that using therecently proposed Transformer architecture even out-of-the-box outperformsprevious neural and non-neural systems for code prediction. We then show thatby making the Transformer architecture aware of the syntactic structure ofcode, we further increase the margin by which a Transformer-based systemoutperforms previous systems. With this, it outperforms the accuracy of anRNN-based system (similar to Hellendoorn et al. 2018) by 18.3%, the Deep3system (Raychev et al 2016) by 14.1%, and an adaptation of Code2Seq (Alon etal., 2018) for code prediction by 14.4%.We present in the paper several ways of communicating the code structure tothe Transformer, which is fundamentally built for processing sequence data. Weprovide a comprehensive experimental evaluation of our proposal, along withalternative design choices, on a standard Python dataset, as well as on aFacebook internal Python corpus. Our code and data preparation pipeline will beavailable in open source.", "output": "Code Prediction by Feeding Trees to Transformers."}]