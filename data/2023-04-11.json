[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human movement analysis is a key area of research in robotics, biomechanics,and data science. It encompasses tracking, posture estimation, and movementsynthesis. While numerous methodologies have evolved over time, a systematicand quantitative evaluation of these approaches using verifiable ground truthdata of three-dimensional human movement is still required to define thecurrent state of the art. This paper presents seven datasets recorded usinginertial-based motion capture. The datasets contain professional gesturescarried out by industrial operators and skilled craftsmen performed in realconditions in-situ. The datasets were created with the intention of being usedfor research in human motion modeling, analysis, and generation. The protocolsfor data collection are described in detail, and a preliminary analysis of thecollected data is provided as a benchmark. The Gesture Operational Model, ahybrid stochastic-biomechanical approach based on kinematic descriptors, isutilized to model the dynamics of the experts' movements and createmathematical representations of their motion trajectories for analysis andquantifying their body dexterity. The models allowed accurate the generation ofhuman professional poses and an intuitive description of how body jointscooperate and change over time through the performance of the task.", "output": "Motion Capture Benchmark of Real Industrial Tasks and Traditional Crafts for Human Movement Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human expectations stem from their knowledge of the others and the world.Where human-robot interaction is concerned, such knowledge about the robot maybe inconsistent with the ground truth, resulting in the robot not meeting itsexpectations. Explicable planning was previously introduced as a novel planningapproach to reconciling human expectations and the optimal robot behavior formore interpretable robot decision-making. One critical issue that remainsunaddressed is safety during explicable decision-making which can lead toexplicable behaviors that are unsafe. We propose Safe Explicable Planning(SEP), which extends explicable planning to support the specification of asafety bound. The objective of SEP is to find a policy that generates abehavior close to human expectations while satisfying the safety constraintsintroduced by the bound, which is a special case of multi-objectiveoptimization where the solution to SEP lies on the Pareto frontier. Under sucha formulation, we propose a novel and efficient method that returns the safeexplicable policy and an approximate solution. In addition, we providetheoretical proof for the optimality of the exact solution under thedesigner-specified bound. Our evaluation results confirm the applicability andefficacy of our method for safe explicable planning.", "output": "Safe Explicable Robot Planning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A prediction model is most useful if it generalizes beyond the developmentdata with external validations, but to what extent should it generalize remainsunclear. In practice, prediction models are externally validated using datafrom very different settings, including populations from other health systemsor countries, with predictably poor results. This may not be a fair reflectionof the performance of the model which was designed for a specific targetpopulation or setting, and may be stretching the expected modelgeneralizability. To address this, we suggest to externally validate a modelusing new data from the target population to ensure clear implications ofvalidation performance on model reliability, whereas model generalizability tobroader settings should be carefully investigated during model developmentinstead of explored post-hoc. Based on this perspective, we propose a roadmapthat facilitates the development and application of reliable, fair, andtrustworthy artificial intelligence prediction models.", "output": "A roadmap to fair and trustworthy prediction model validation in healthcare."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exploring the expected quantizing scheme with suitable mixed-precision policyis the key point to compress deep neural networks (DNNs) in high efficiency andaccuracy. This exploration implies heavy workloads for domain experts, and anautomatic compression method is needed. However, the huge search space of theautomatic method introduces plenty of computing budgets that make the automaticprocess challenging to be applied in real scenarios. In this paper, we proposean end-to-end framework named AutoQNN, for automatically quantizing differentlayers utilizing different schemes and bitwidths without any human labor.AutoQNN can seek desirable quantizing schemes and mixed-precision policies formainstream DNN models efficiently by involving three techniques: quantizingscheme search (QSS), quantizing precision learning (QPL), and quantizedarchitecture generation (QAG). QSS introduces five quantizing schemes anddefines three new schemes as a candidate set for scheme search, and then usesthe differentiable neural architecture search (DNAS) algorithm to seek thelayer- or model-desired scheme from the set. QPL is the first method to learnmixed-precision policies by reparameterizing the bitwidths of quantizingschemes, to the best of our knowledge. QPL optimizes both classification lossand precision loss of DNNs efficiently and obtains the relatively optimalmixed-precision model within limited model size and memory footprint. QAG isdesigned to convert arbitrary architectures into corresponding quantized oneswithout manual intervention, to facilitate end-to-end neural networkquantization. We have implemented AutoQNN and integrated it into Keras.Extensive experiments demonstrate that AutoQNN can consistently outperformstate-of-the-art quantization.", "output": "AutoQNN: An End-to-End Framework for Automatically Quantizing Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in generative artificial intelligence (AI) have capturedworldwide attention. Tools such as Dalle-2 and ChatGPT suggest that taskspreviously thought to be beyond the capabilities of AI may now augment theproductivity of creative media in various new ways, including through thegeneration of synthetic video. This research paper explores the utility ofusing AI-generated synthetic video to create viable educational content foronline educational settings. To date, there is limited research investigatingthe real-world educational value of AI-generated synthetic media. To addressthis gap, we examined the impact of using AI-generated synthetic video in anonline learning platform on both learners content acquisition and learningexperience. We took a mixed-method approach, randomly assigning adult learners(n=83) into one of two micro-learning conditions, collecting pre- andpost-learning assessments, and surveying participants on their learningexperience. The control condition included a traditionally produced instructorvideo, while the experimental condition included a synthetic video with arealistic AI-generated character. The results show that learners in bothconditions demonstrated significant improvement from pre- to post-learning(p&lt;.001), with no significant differences in gains between the two conditions(p=.80). In addition, no differences were observed in how learners perceivedthe traditional and synthetic videos. These findings suggest that AI-generatedsynthetic learning videos have the potential to be a viable substitute forvideos produced via traditional methods in online educational settings, makinghigh quality educational content more accessible across the globe.", "output": "Generative AI for learning: Investigating the potential of synthetic learning videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative modelling over continuous-time geometric constructs, a.k.a such ashandwriting, sketches, drawings etc., have been accomplished throughautoregressive distributions. Such strictly-ordered discrete factorizationhowever falls short of capturing key properties of chirographic data -- itfails to build holistic understanding of the temporal concept due to one-wayvisibility (causality). Consequently, temporal data has been modelled asdiscrete token sequences of fixed sampling rate instead of capturing the trueunderlying concept. In this paper, we introduce a powerful model-class namely\"Denoising Diffusion Probabilistic Models\" or DDPMs for chirographic data thatspecifically addresses these flaws. Our model named \"ChiroDiff\", beingnon-autoregressive, learns to capture holistic concepts and therefore remainsresilient to higher temporal sampling rate up to a good extent. Moreover, weshow that many important downstream utilities (e.g. conditional sampling,creative mixing) can be flexibly implemented using ChiroDiff. We further showsome unique use-cases like stochastic vectorization, de-noising/healing,abstraction are also possible with this model-class. We perform quantitativeand qualitative evaluation of our framework on relevant datasets and found itto be better or on par with competing approaches.", "output": "ChiroDiff: Modelling chirographic data with Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the success of deep-learning models in many tasks, there have beenconcerns about such models learning shortcuts, and their lack of robustness toirrelevant confounders. When it comes to models directly trained on humanfaces, a sensitive confounder is that of human identities. Many face-relatedtasks should ideally be identity-independent, and perform uniformly acrossdifferent individuals (i.e. be fair). One way to measure and enforce suchrobustness and performance uniformity is through enforcing it during training,assuming identity-related information is available at scale. However, due toprivacy concerns and also the cost of collecting such information, this isoften not the case, and most face datasets simply contain input images andtheir corresponding task-related labels. Thus, improving identity-relatedrobustness without the need for such annotations is of great importance. Here,we explore using face-recognition embedding vectors, as proxies for identities,to enforce such robustness. We propose to use the structure in theface-recognition embedding space, to implicitly emphasize rare samples withineach class. We do so by weighting samples according to their conditionalinverse density (CID) in the proxy embedding space. Our experiments suggestthat such a simple sample weighting scheme, not only improves the trainingrobustness, it often improves the overall performance as a result of suchrobustness. We also show that employing such constraints during trainingresults in models that are significantly less sensitive to different levels ofbias in the dataset.", "output": "Improving Identity-Robustness for Face Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans have a powerful and mysterious capacity to reason. By working througha series of purely mental steps, we can make inferences we would not be capableof making directly -- despite that fact that we get no additional data from theworld. Similarly, large language models can perform better at complex tasksthrough chain-of-thought reasoning, where they generate intermediate stepsbefore answering a question. We use language models to investigate thequestions of when and why reasoning is helpful, testing the hypothesis thatreasoning is effective when training data consisting of local clusters ofvariables that influence each other strongly. These training conditions enablethe chaining of accurate local inferences in order to estimate relationshipsbetween variables that were not seen together in training. We train anautoregressive transformer on samples from joint distributions defined by Bayesnets, but only include a subset of all the variables in each sample. We comparelanguage models' ability to match conditional probabilities both with andwithout intermediate reasoning steps, finding that intermediate steps help onlywhen the training data is locally structured with respect to dependenciesbetween variables. Furthermore, intermediate variables need to be relevant tothe relationship between observed information and target inferences. Ourresults illustrate how the statistical structure of training data drives theeffectiveness of reasoning step by step.", "output": "Why think step-by-step? Reasoning emerges from the locality of experience."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work we theoretically show that conservative objective models (COMs)for offline model-based optimisation (MBO) are a special kind of contrastivedivergence-based energy model, one where the energy function represents boththe unconditional probability of the input and the conditional probability ofthe reward variable. While the initial formulation only samples modes from itslearned distribution, we propose a simple fix that replaces its gradient ascentsampler with a Langevin MCMC sampler. This gives rise to a specialprobabilistic model where the probability of sampling an input is proportionalto its predicted reward. Lastly, we show that better samples can be obtained ifthe model is decoupled so that the unconditional and conditional probabilitiesare modelled separately.", "output": "Conservative objective models are a special kind of contrastive divergence-based energy model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The two fields of urban planning and artificial intelligence (AI) arose anddeveloped separately. However, there is now cross-pollination and increasinginterest in both fields to benefit from the advances of the other. In thepresent paper, we introduce the importance of urban planning from thesustainability, living, economic, disaster, and environmental perspectives. Wereview the fundamental concepts of urban planning and relate these concepts tocrucial open problems of machine learning, including adversarial learning,generative neural networks, deep encoder-decoder networks, conversational AI,and geospatial and temporal machine learning, thereby assaying how AI cancontribute to modern urban planning. Thus, a central problem is automatedland-use configuration, which is formulated as the generation of land uses andbuilding configuration for a target area from surrounding geospatial, humanmobility, social media, environment, and economic activities. Finally, wedelineate some implications of AI for urban planning and propose key researchareas at the intersection of both topics.", "output": "Towards Automated Urban Planning: When Generative and ChatGPT-like AI Meets Urban Planning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, short Text Matching tasks have been widely applied in thefields ofadvertising search and recommendation. The difficulty lies in the lackof semantic information and word ambiguity caused by the short length of thetext. Previous works have introduced complement sentences or knowledge bases toprovide additional feature information. However, these methods have not fullyinteracted between the original sentence and the complement sentence, and havenot considered the noise issue that may arise from the introduction of externalknowledge bases. Therefore, this paper proposes a short Text Matching modelthat combines contrastive learning and external knowledge. The model uses agenerative model to generate corresponding complement sentences and uses thecontrastive learning method to guide the model to obtain more semanticallymeaningful encoding of the original sentence. In addition, to avoid noise, weuse keywords as the main semantics of the original sentence to retrievecorresponding knowledge words in the knowledge base, and construct a knowledgegraph. The graph encoding model is used to integrate the knowledge baseinformation into the model. Our designed model achieves state-of-the-artperformance on two publicly available Chinese Text Matching datasets,demonstrating the effectiveness of our model.", "output": "The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a framework for efficient 3D clothed avatarreconstruction. By combining the advantages of the high accuracy ofoptimization-based methods and the efficiency of learning-based methods, wepropose a coarse-to-fine way to realize a high-fidelity clothed avatarreconstruction (CAR) from a single image. At the first stage, we use animplicit model to learn the general shape in the canonical space of a person ina learning-based way, and at the second stage, we refine the surface detail byestimating the non-rigid deformation in the posed space in an optimization way.A hyper-network is utilized to generate a good initialization so that theconvergence o f the optimization process is greatly accelerated. Extensiveexperiments on various datasets show that the proposed CAR successfullyproduces high-fidelity avatars for arbitrarily clothed humans in real scenes.", "output": "High-Fidelity Clothed Avatar Reconstruction from a Single Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of artificial intelligence for science, it is consistently anessential challenge to face a limited amount of labeled data for real-worldproblems. The prevailing approach is to pretrain a powerful task-agnostic modelon a large unlabeled corpus but may struggle to transfer knowledge todownstream tasks. In this study, we propose InstructMol, a semi-supervisedlearning algorithm, to take better advantage of unlabeled examples. Itintroduces an instructor model to provide the confidence ratios as themeasurement of pseudo-labels' reliability. These confidence scores then guidethe target model to pay distinct attention to different data points, avoidingthe over-reliance on labeled data and the negative influence of incorrectpseudo-annotations. Comprehensive experiments show that InstructBiosubstantially improves the generalization ability of molecular models, in notonly molecular property predictions but also activity cliff estimations,demonstrating the superiority of the proposed method. Furthermore, our evidenceindicates that InstructBio can be equipped with cutting-edge pretrainingmethods and used to establish large-scale and task-specific pseudo-labeledmolecular datasets, which reduces the predictive errors and shortens thetraining process. Our work provides strong evidence that semi-supervisedlearning can be a promising tool to overcome the data scarcity limitation andadvance molecular representation learning.", "output": "InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spurious correlations that degrade model generalization or lead the model tobe right for the wrong reasons are one of the main robustness concerns forreal-world deployments. However, mitigating these correlations duringpre-training for large-scale models can be costly and impractical, particularlyfor those without access to high-performance computing resources. This paperproposes a novel approach to address spurious correlations during fine-tuningfor a given domain of interest. With a focus on multi-modal models (e.g.,CLIP), the proposed method leverages different modalities in these models todetect and explicitly set apart spurious attributes from the affected class,achieved through a multi-modal contrastive loss function that expressesspurious relationships through language. Our experimental results and in-depthvisualizations on CLIP show that such an intervention can effectively i)improve the model's accuracy when spurious attributes are not present, and ii)directs the model's activation maps towards the actual class rather than thespurious attribute when present. In particular, on the Waterbirds dataset, ouralgorithm achieved a worst-group accuracy 23% higher than ERM on CLIP with aResNet-50 backbone, and 32% higher on CLIP with a ViT backbone, whilemaintaining the same average accuracy as ERM.", "output": "Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Adversarial Networks (GANs) have emerged as a significant playerin generative modeling by mapping lower-dimensional random noise tohigher-dimensional spaces. These networks have been used to generatehigh-resolution images and 3D objects. The efficient modeling of 3D objects andhuman faces is crucial in the development process of 3D graphical environmentssuch as games or simulations. 3D GANs are a new type of generative model usedfor 3D reconstruction, point cloud reconstruction, and 3D semantic scenecompletion. The choice of distribution for noise is critical as it representsthe latent space. Understanding a GAN's latent space is essential forfine-tuning the generated samples, as demonstrated by the morphing ofsemantically meaningful parts of images. In this work, we explore the latentspace and 3D GANs, examine several GAN variants and training methods to gaininsights into improving 3D GAN training, and suggest potential futuredirections for further research.", "output": "3D GANs and Latent Space: A comprehensive survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning about code and explaining its purpose are fundamental skills forcomputer scientists. There has been extensive research in the field ofcomputing education on the relationship between a student's ability to explaincode and other skills such as writing and tracing code. In particular, theability to describe at a high-level of abstraction how code will behave overall possible inputs correlates strongly with code writing skills. However,developing the expertise to comprehend and explain code accurately andsuccinctly is a challenge for many students. Existing pedagogical approachesthat scaffold the ability to explain code, such as producing exemplar codeexplanations on demand, do not currently scale well to large classrooms. Therecent emergence of powerful large language models (LLMs) may offer a solution.In this paper, we explore the potential of LLMs in generating explanations thatcan serve as examples to scaffold students' ability to understand and explaincode. To evaluate LLM-created explanations, we compare them with explanationscreated by students in a large course ($n approx 1000$) with respect toaccuracy, understandability and length. We find that LLM-created explanations,which can be produced automatically on demand, are rated as being significantlyeasier to understand and more accurate summaries of code than student-createdexplanations. We discuss the significance of this finding, and suggest how suchmodels can be incorporated into introductory programming education.", "output": "Comparing Code Explanations Created by Students and Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the advent of general-purpose speech representations from large-scaleself-supervised models, applying a single model to multiple downstream tasks isbecoming a de-facto approach. However, the pooling problem remains; the lengthof speech representations is inherently variable. The naive average pooling isoften used, even though it ignores the characteristics of speech, such asdifferently lengthed phonemes. Hence, we design a novel pooling method tosquash acoustically similar representations via vector quantization, which doesnot require additional training, unlike attention-based pooling. Further, weevaluate various unsupervised pooling methods on various self-supervisedmodels. We gather diverse methods scattered around speech and text to evaluateon various tasks: keyword spotting, speaker identification, intentclassification, and emotion recognition. Finally, we quantitatively andqualitatively analyze our method, comparing it with supervised pooling methods.", "output": "Unsupervised Speech Representation Pooling Using Vector Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The observation and description of collective excitations in solids is afundamental issue when seeking to understand the physics of a many-body system.Analysis of these excitations is usually carried out by measuring the dynamicalstructure factor, S(Q, $omega$), with inelastic neutron or x-ray scatteringtechniques and comparing this against a calculated dynamical model. Here, wedevelop an artificial intelligence framework which combines a neural networktrained to mimic simulated data from a model Hamiltonian with automaticdifferentiation to recover unknown parameters from experimental data. Webenchmark this approach on a Linear Spin Wave Theory (LSWT) simulator andadvanced inelastic neutron scattering data from the square-lattice spin-1antiferromagnet La$_2$NiO$_4$. We find that the model predicts the unknownparameters with excellent agreement relative to analytical fitting. In doingso, we illustrate the ability to build and train a differentiable model onlyonce, which then can be applied in real-time to multi-dimensional scatteringdata, without the need for human-guided peak finding and fitting algorithms.This prototypical approach promises a new technology for this field toautomatically detect and refine more advanced models for ordered quantumsystems.", "output": "Capturing dynamical correlations using implicit neural representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Named Entity Recognition (NER) plays a vital role in various Natural LanguageProcessing tasks such as information retrieval, text classification, andquestion answering. However, NER can be challenging, especially in low-resourcelanguages with limited annotated datasets and tools. This paper adds to theeffort of addressing these challenges by introducing MphayaNER, the firstTshivenda NER corpus in the news domain. We establish NER baselines bytextit{fine-tuning} state-of-the-art models on MphayaNER. The study alsoexplores zero-shot transfer between Tshivenda and other related Bantulanguages, with chiShona and Kiswahili showing the best results. AugmentingMphayaNER with chiShona data was also found to improve model performancesignificantly. Both MphayaNER and the baseline models are made publiclyavailable.", "output": "MphayaNER: Named Entity Recognition for Tshivenda."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantization has emerged as an essential technique for deploying deep neuralnetworks (DNNs) on devices with limited resources. However, quantized modelsexhibit vulnerabilities when exposed to various noises in real-worldapplications. Despite the importance of evaluating the impact of quantizationon robustness, existing research on this topic is limited and often disregardsestablished principles of robustness evaluation, resulting in incomplete andinconclusive findings. To address this gap, we thoroughly evaluated therobustness of quantized models against various noises (adversarial attacks,natural corruptions, and systematic noises) on ImageNet. Extensive experimentsdemonstrate that lower-bit quantization is more resilient to adversarialattacks but is more susceptible to natural corruptions and systematic noises.Notably, our investigation reveals that impulse noise (in natural corruptions)and the nearest neighbor interpolation (in systematic noises) have the mostsignificant impact on quantized models. Our research contributes to advancingthe robust quantization of models and their deployment in real-world scenarios.", "output": "Benchmarking the Robustness of Quantized Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, self-supervised learning (SSL) has achieved tremendous success inlearning image representation. Despite the empirical success, mostself-supervised learning methods are rather \"inefficient\" learners, typicallytaking hundreds of training epochs to fully converge. In this work, we showthat the key towards efficient self-supervised learning is to increase thenumber of crops from each image instance. Leveraging one of thestate-of-the-art SSL method, we introduce a simplistic form of self-supervisedlearning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL)that does not rely on many heuristic techniques for SSL such as weight sharingbetween the branches, feature-wise normalization, output quantization, and stopgradient, etc, and reduces the training epochs by two orders of magnitude. Weshow that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5%on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just oneepoch. Furthermore, the proposed method achieves 91.5% on CIFAR-10, 70.1% onCIFAR-100, 51.5% on Tiny ImageNet and 78.9% on ImageNet-100 with linear probingin less than ten training epochs. In addition, we show that EMP-SSL showssignificantly better transferability to out-of-domain datasets compared tobaseline SSL methods. We will release the code in", "output": "EMP-SSL: Towards Self-Supervised Learning in One Training Epoch."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal knowledge graphs (TKGs) model the temporal evolution of events andhave recently attracted increasing attention. Since TKGs are intrinsicallyincomplete, it is necessary to reason out missing elements. Although existingTKG reasoning methods have the ability to predict missing future events, theyfail to generate explicit reasoning paths and lack explainability. Asreinforcement learning (RL) for multi-hop reasoning on traditional knowledgegraphs starts showing superior explainability and performance in recentadvances, it has opened up opportunities for exploring RL techniques on TKGreasoning. However, the performance of RL-based TKG reasoning methods islimited due to: (1) lack of ability to capture temporal evolution and semanticdependence jointly; (2) excessive reliance on manually designed rewards. Toovercome these challenges, we propose an adaptive reinforcement learning modelbased on attention mechanism (DREAM) to predict missing elements in the future.Specifically, the model contains two components: (1) a multi-faceted attentionrepresentation learning method that captures semantic dependence and temporalevolution jointly; (2) an adaptive RL framework that conducts multi-hopreasoning by adaptively learning the reward functions. Experimental resultsdemonstrate DREAM outperforms state-of-the-art models on public dataset", "output": "DREAM: Adaptive Reinforcement Learning based on Attention Mechanism for Temporal Knowledge Graph Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The integration of renewable energy sources into the power grid is becomingincreasingly important as the world moves towards a more sustainable energyfuture. However, the intermittent nature of renewable energy sources can makeit challenging to manage the power grid and ensure a stable supply ofelectricity. In this paper, we propose a deep learning-based approach forpredicting energy demand in a smart power grid, which can improve theintegration of renewable energy sources by providing accurate predictions ofenergy demand. We use long short-term memory networks, which are well-suitedfor time series data, to capture complex patterns and dependencies in energydemand data. The proposed approach is evaluated using four datasets ofhistorical energy demand data from different energy distribution companiesincluding American Electric Power, Commonwealth Edison, Dayton Power and Light,and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model isalso compared with two other state of the art forecasting algorithms namely,Facebook Prophet and Support Vector Regressor. The experimental results showthat the proposed REDf model can accurately predict energy demand with a meanabsolute error of 1.4%. This approach has the potential to improve theefficiency and stability of the power grid by allowing for better management ofthe integration of renewable energy sources.", "output": "REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An efficient team is essential for the company to successfully complete newprojects. To solve the team formation problem considering person-job matching(TFP-PJM), a 0-1 integer programming model is constructed, which considers bothperson-job matching and team members' willingness to communicate on teamefficiency, with the person-job matching score calculated using intuitionisticfuzzy numbers. Then, a reinforcement learning-assisted genetic programmingalgorithm (RL-GP) is proposed to enhance the quality of solutions. The RL-GPadopts the ensemble population strategies. Before the population evolution ateach generation, the agent selects one from four population search modesaccording to the information obtained, thus realizing a sound balance ofexploration and exploitation. In addition, surrogate models are used in thealgorithm to evaluate the formation plans generated by individuals, whichspeeds up the algorithm learning process. Afterward, a series of comparisonexperiments are conducted to verify the overall performance of RL-GP and theeffectiveness of the improved strategies within the algorithm. Thehyper-heuristic rules obtained through efficient learning can be utilized asdecision-making aids when forming project teams. This study reveals theadvantages of reinforcement learning methods, ensemble strategies, and thesurrogate model applied to the GP framework. The diversity and intelligentselection of search patterns along with fast adaptation evaluation, aredistinct features that enable RL-GP to be deployed in real-world enterpriseenvironments.", "output": "A Reinforcement Learning-assisted Genetic Programming Algorithm for Team Formation Problem Considering Person-Job Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Named Entity Recognition (NER) is a fundamental NLP tasks with a wide rangeof practical applications. The performance of state-of-the-art NER methodsdepends on high quality manually anotated datasets which still do not exist forsome languages. In this work we aim to remedy this situation in Slovak byintroducing WikiGoldSK, the first sizable human labelled Slovak NER dataset. Webenchmark it by evaluating state-of-the-art multilingual Pretrained LanguageModels and comparing it to the existing silver-standard Slovak NER dataset. Wealso conduct few-shot experiments and show that training on a sliver-standarddataset yields better results. To enable future work that can be based onSlovak NER, we release the dataset, code, as well as the trained modelspublicly under permissible licensing terms at", "output": "WikiGoldSK: Annotated Dataset, Baselines and Few-Shot Learning Experiments for Slovak Named Entity Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spiking Neural Networks (SNNs) have shown capabilities of achieving highaccuracy under unsupervised settings and low operational power/energy due totheir bio-plausible computations. Previous studies identified that DRAM-basedoff-chip memory accesses dominate the energy consumption of SNN processing.However, state-of-the-art works do not optimize the DRAM energy-per-access,thereby hindering the SNN-based systems from achieving further energyefficiency gains. To substantially reduce the DRAM energy-per-access, aneffective solution is to decrease the DRAM supply voltage, but it may lead toerrors in DRAM cells (i.e., so-called approximate DRAM). Towards this, wepropose textit{EnforceSNN}, a novel design framework that provides a solutionfor resilient and energy-efficient SNN inference using reduced-voltage DRAM forembedded systems. The key mechanisms of our EnforceSNN are: (1) employingquantized weights to reduce the DRAM access energy; (2) devising an efficientDRAM mapping policy to minimize the DRAM energy-per-access; (3) analyzing theSNN error tolerance to understand its accuracy profile considering differentbit error rate (BER) values; (4) leveraging the information for developing anefficient fault-aware training (FAT) that considers different BER values andbit error locations in DRAM to improve the SNN error tolerance; and (5)developing an algorithm to select the SNN model that offers good trade-offsamong accuracy, memory, and energy consumption. The experimental results showthat our EnforceSNN maintains the accuracy (i.e., no accuracy loss for BERless-or-equal 10^-3) as compared to the baseline SNN with accurate DRAM, whileachieving up to 84.9% of DRAM energy saving and up to 4.1x speed-up of DRAMdata throughput across different network sizes.", "output": "EnforceSNN: Enabling Resilient and Energy-Efficient Spiking Neural Network Inference considering Approximate DRAMs for Embedded Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To maximize the performance and energy efficiency of Spiking Neural Network(SNN) processing on resource-constrained embedded systems, specialized hardwareaccelerators/chips are employed. However, these SNN chips may suffer frompermanent faults which can affect the functionality of weight memory and neuronbehavior, thereby causing potentially significant accuracy degradation andsystem malfunctioning. Such permanent faults may come from manufacturingdefects during the fabrication process, and/or from device/transistor damages(e.g., due to wear out) during the run-time operation. However, the impact ofpermanent faults in SNN chips and the respective mitigation techniques have notbeen thoroughly investigated yet. Toward this, we propose RescueSNN, a novelmethodology to mitigate permanent faults in the compute engine of SNN chipswithout requiring additional retraining, thereby significantly cutting down thedesign time and retraining costs, while maintaining the throughput and quality.The key ideas of our RescueSNN methodology are (1) analyzing thecharacteristics of SNN under permanent faults; (2) leveraging this analysis toimprove the SNN fault-tolerance through effective fault-aware mapping (FAM);and (3) devising lightweight hardware enhancements to support FAM. Our FAMtechnique leverages the fault map of SNN compute engine for (i) minimizingweight corruption when mapping weight bits on the faulty memory cells, and (ii)selectively employing faulty neurons that do not cause significant accuracydegradation to maintain accuracy and throughput, while considering the SNNoperations and processing dataflow. The experimental results show that ourRescueSNN improves accuracy by up to 80% while maintaining the throughputreduction below 25% in high fault rate (e.g., 0.5 of the potential faultlocations), as compared to running SNNs on the faulty chip without mitigation.", "output": "RescueSNN: Enabling Reliable Executions on Spiking Neural Network Accelerators under Permanent Faults."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The sequence-to-sequence (seq2seq) task aims at generating the targetsequence based on the given input source sequence. Traditionally, most of theseq2seq task is resolved by the Encoder-Decoder framework which requires anencoder to encode the source sequence and a decoder to generate the targettext. Recently, a bunch of new approaches have emerged that apply decoder-onlylanguage models directly to the seq2seq task. Despite the significantadvancements in applying language models to the seq2seq task, there is still alack of thorough analysis on the effectiveness of the decoder-only languagemodel architecture. This paper aims to address this gap by conducting adetailed comparison between the encoder-decoder architecture and thedecoder-only language model framework through the analysis of a regularizedencoder-decoder structure. This structure is designed to replicate allbehaviors in the classical decoder-only language model but has an encoder and adecoder making it easier to be compared with the classical encoder-decoderstructure. Based on the analysis, we unveil the attention degeneration problemin the language model, namely, as the generation step number grows, less andless attention is focused on the source sequence. To give a quantitativeunderstanding of this problem, we conduct a theoretical sensitivity analysis ofthe attention output with respect to the source input. Grounded on ouranalysis, we propose a novel partial attention language model to solve theattention degeneration problem. Experimental results on machine translation,summarization, and data-to-text generation tasks support our analysis anddemonstrate the effectiveness of our proposed model.", "output": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The paper describes a transformer-based system designed for SemEval-2023 Task9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predictthe intimacy of tweets in a range from 1 (not intimate at all) to 5 (veryintimate). The official training set for the competition consisted of tweets insix languages (English, Spanish, Italian, Portuguese, French, and Chinese). Thetest set included the given six languages as well as external data with fourlanguages not presented in the training set (Hindi, Arabic, Dutch, and Korean).We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTamodel adapted to the Twitter domain. To improve the performance of unseenlanguages, each tweet was supplemented by its English translation. We exploredthe effectiveness of translated data for the languages seen in fine-tuningcompared to unseen languages and estimated strategies for using translated datain transformer-based models. Our solution ranked 4th on the leaderboard whileachieving an overall Pearson's r of 0.599 over the test set. The proposedsystem improves up to 0.088 Pearson's r over a score averaged across all 45submissions.", "output": "tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multiple Sclerosis (MS) is a chronic disease developed in human brain andspinal cord, which can cause permanent damage or deterioration of the nerves.The severity of MS disease is monitored by the Expanded Disability Status Scale(EDSS), composed of several functional sub-scores. Early and accurateclassification of MS disease severity is critical for slowing down orpreventing disease progression via applying early therapeutic interventionstrategies. Recent advances in deep learning and the wide use of ElectronicHealth Records (EHR) creates opportunities to apply data-driven and predictivemodeling tools for this goal. Previous studies focusing on using single-modalmachine learning and deep learning algorithms were limited in terms ofprediction accuracy due to the data insufficiency or model simplicity. In thispaper, we proposed an idea of using patients' multimodal longitudinal andlongitudinal EHR data to predict multiple sclerosis disease severity at thehospital visit. This work has two important contributions. First, we describe apilot effort to leverage structured EHR data, neuroimaging data and clinicalnotes to build a multi-modal deep learning framework to predict patient's MSdisease severity. The proposed pipeline demonstrates up to 25% increase interms of the area under the Area Under the Receiver Operating Characteristiccurve (AUROC) compared to models using single-modal data. Second, the studyalso provides insights regarding the amount useful signal embedded in each datamodality with respect to MS disease prediction, which may improve datacollection processes.", "output": "Predicting multiple sclerosis disease severity with multimodal deep neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We define very large multi-objective optimization problems to bemultiobjective optimization problems in which the number of decision variablesis greater than 100,000 dimensions. This is an important class of problems asmany real-world problems require optimizing hundreds of thousands of variables.Existing evolutionary optimization methods fall short of such requirements whendealing with problems at this very large scale. Inspired by the success ofexisting recommender systems to handle very large-scale items with limitedhistorical interactions, in this paper we propose a method termed Verylarge-scale Multiobjective Optimization through Recommender Systems (VMORS).The idea of the proposed method is to transform the defined such verylarge-scale problems into a problem that can be tackled by a recommendersystem. In the framework, the solutions are regarded as users, and thedifferent evolution directions are items waiting for the recommendation. We useThompson sampling to recommend the most suitable items (evolutionarydirections) for different users (solutions), in order to locate the optimalsolution to a multiobjective optimization problem in a very large search spacewithin acceptable time. We test our proposed method on different problems from100,000 to 500,000 dimensions, and experimental results show that our methodnot only shows good performance but also significant improvement over existingmethods.", "output": "A Recommender System Approach for Very Large-scale Multiobjective Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The large-scale multiobjective optimization problem (LSMOP) is characterizedby simultaneously optimizing multiple conflicting objectives and involvinghundreds of decision variables. {Many real-world applications in engineeringfields can be modeled as LSMOPs; simultaneously, engineering applicationsrequire insensitivity in performance.} This requirement usually means that theresults from the algorithm runs should not only be good for every run in termsof performance but also that the performance of multiple runs should notfluctuate too much, i.e., the algorithm shows good insensitivity. Consideringthat substantial computational resources are requested for each run, it isessential to improve upon the performance of the large-scale multiobjectiveoptimization algorithm, as well as the insensitivity of the algorithm. However,existing large-scale multiobjective optimization algorithms solely focus onimproving the performance of the algorithms, leaving the insensitivitycharacteristics unattended. {In this work, we propose an evolutionary algorithmfor solving LSMOPs based on Monte Carlo tree search, the so-called LMMOCTS,which aims to improve the performance and insensitivity for large-scalemultiobjective optimization problems.} The proposed method samples the decisionvariables to construct new nodes on the Monte Carlo tree for optimization andevaluation. {It selects nodes with good evaluation for further search to reducethe performance sensitivity caused by large-scale decision variables.} Wecompare the proposed algorithm with several state-of-the-art designs ondifferent benchmark functions. We also propose two metrics to measure thesensitivity of the algorithm. The experimental results confirm theeffectiveness and performance insensitivity of the proposed design for solvinglarge-scale multiobjective optimization problems.", "output": "Improving Performance Insensitivity of Large-scale Multiobjective Optimization via Monte Carlo Tree Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a deep learning-based pipeline for categorizing Bengalitoxic comments, in which at first a binary classification model is used todetermine whether a comment is toxic or not, and then a multi-label classifieris employed to determine which toxicity type the comment belongs to. For thispurpose, we have prepared a manually labeled dataset consisting of 16,073instances among which 8,488 are Toxic and any toxic comment may correspond toone or more of the six toxic categories - vulgar, hate, religious, threat,troll, and insult simultaneously. Long Short Term Memory (LSTM) with BERTEmbedding achieved 89.42% accuracy for the binary classification task while asa multi-label classifier, a combination of Convolutional Neural Network andBi-directional Long Short Term Memory (CNN-BiLSTM) with attention mechanismachieved 78.92% accuracy and 0.86 as weighted F1-score. To explain thepredictions and interpret the word feature importance during classification bythe proposed models, we utilized Local Interpretable Model-AgnosticExplanations (LIME) framework. We have made our dataset public and can beaccessed at -", "output": "Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncovering data generative factors is the ultimate goal of disentanglementlearning. Although many works proposed disentangling generative models able touncover the underlying generative factors of a dataset, so far no one was ableto uncover OOD generative factors (i.e., factors of variations that are notexplicitly shown on the dataset). Moreover, the datasets used to validate thesemodels are synthetically generated using a balanced mixture of some predefinedgenerative factors, implicitly assuming that generative factors are uniformlydistributed across the datasets. However, real datasets do not present thisproperty. In this work we analyse the effect of using datasets with unbalancedgenerative factors, providing qualitative and quantitative results for widelyused generative models. Moreover, we propose TC-VAE, a generative modeloptimized using a lower bound of the joint total correlation between thelearned latent representations and the input data. We show that the proposedmodel is able to uncover OOD generative factors on different datasets andoutperforms on average the related baselines in terms of downstreamdisentanglement metrics.", "output": "TC-VAE: Uncovering Out-of-Distribution Data Generative Factors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Network pruning is a widely used technique to reduce computation cost andmodel size for deep neural networks. However, the typical three-stage pipelinesignificantly increases the overall training time. In this paper, we develop asystematic weight-pruning optimization approach based on Surrogate Lagrangianrelaxation, which is tailored to overcome difficulties caused by the discretenature of the weight-pruning problem. We prove that our method ensures fastconvergence of the model compression problem, and the convergence of the SLR isaccelerated by using quadratic penalties. Model parameters obtained by SLRduring the training phase are much closer to their optimal values as comparedto those obtained by other state-of-the-art methods. We evaluate our method onimage classification tasks using CIFAR-10 and ImageNet with state-of-the-artMLP-Mixer, Swin Transformer, and VGG-16, ResNet-18, ResNet-50 and ResNet-110,MobileNetV2. We also evaluate object detection and segmentation tasks on COCO,KITTI benchmark, and TuSimple lane detection dataset using a variety of models.Experimental results demonstrate that our SLR-based weight-pruning optimizationapproach achieves a higher compression rate than state-of-the-art methods underthe same accuracy requirement and also can achieve higher accuracy under thesame compression rate requirement. Under classification tasks, our SLR approachconverges to the desired accuracy $3times$ faster on both of the datasets.Under object detection and segmentation tasks, SLR also converges $2times$faster to the desired accuracy. Further, our SLR achieves high model accuracyeven at the hard-pruning stage without retraining, which reduces thetraditional three-stage pruning into a two-stage process. Given a limitedbudget of retraining epochs, our approach quickly recovers the model'saccuracy.", "output": "Surrogate Lagrangian Relaxation: A Path To Retrain-free Deep Neural Network Pruning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Sat-NeRF, a modified implementation of the recently introducedShadow Neural Radiance Field (S-NeRF) model. This method is able to synthesizenovel views from a sparse set of satellite images of a scene, while accountingfor the variation in lighting present in the pictures. The trained model canalso be used to accurately estimate the surface elevation of the scene, whichis often a desirable quantity for satellite observation applications. S-NeRFimproves on the standard Neural Radiance Field (NeRF) method by considering theradiance as a function of the albedo and the irradiance. Both these quantitiesare output by fully connected neural network branches of the model, and thelatter is considered as a function of the direct light from the sun and thediffuse color from the sky. The implementations were run on a dataset ofsatellite images, augmented using a zoom-and-crop technique. A hyperparameterstudy for NeRF was carried out, leading to intriguing observations on themodel's convergence. Finally, both NeRF and S-NeRF were run until 100k epochsin order to fully fit the data and produce their best possible predictions. Thecode related to this article can be found at", "output": "NeRF applied to satellite imagery for surface reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Protecting data privacy is paramount in the fields such as finance, banking,and healthcare. Federated Learning (FL) has attracted widespread attention dueto its decentralized, distributed training and the ability to protect theprivacy while obtaining a global shared model. However, FL presents challengessuch as communication overhead, and limited resource capability. This motivatedus to propose a two-stage federated learning approach toward the objective ofprivacy protection, which is a first-of-its-kind study as follows: (i) Duringthe first stage, the synthetic dataset is generated by employing two differentdistributions as noise to the vanilla conditional tabular generativeadversarial neural network (CTGAN) resulting in modified CTGAN, and (ii) In thesecond stage, the Federated Probabilistic Neural Network (FedPNN) is developedand employed for building globally shared classification model. We alsoemployed synthetic dataset metrics to check the quality of the generatedsynthetic dataset. Further, we proposed a meta-clustering algorithm whereby thecluster centers obtained from the clients are clustered at the server fortraining the global model. Despite PNN being a one-pass learning classifier,its complexity depends on the training data size. Therefore, we employed amodified evolving clustering method (ECM), another one-pass algorithm tocluster the training data thereby increasing the speed further. Moreover, weconducted sensitivity analysis by varying Dthr, a hyperparameter of ECM at theserver and client, one at a time. The effectiveness of our approach isvalidated on four finance and medical datasets.", "output": "FedPNN: One-shot Federated Classification via Evolving Clustering Method and Probabilistic Neural Network hybrid."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a new benchmarking suite for high-dimensional control, targetedat testing high spatial and temporal precision, coordination, and planning, allwith an underactuated system frequently making-and-breaking contacts. Theproposed challenge is mastering the piano through bi-manual dexterity, using apair of simulated anthropomorphic robot hands. We call it RoboPianist, and theinitial version covers a broad set of 150 variable-difficulty songs. Weinvestigate both model-free and model-based methods on the benchmark,characterizing their performance envelopes. We observe that while certainexisting methods, when well-tuned, can achieve impressive levels of performancein certain aspects, there is significant room for improvement. RoboPianistprovides a rich quantitative benchmarking environment, with human-interpretableresults, high ease of expansion by simply augmenting the repertoire with newsongs, and opportunities for further research, including in multi-tasklearning, zero-shot generalization, multimodal (sound, vision, touch) learning,and imitation. Supplementary information, including videos of our controlpolicies, can be found at ", "output": "RoboPianist: A Benchmark for High-Dimensional Robot Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) enables distributed clients to collaboratively traina machine learning model without sharing raw data with each other. However, itsuffers the leakage of private information from uploading models. In addition,as the model size grows, the training latency increases due to limitedtransmission bandwidth and the model performance degrades while usingdifferential privacy (DP) protection. In this paper, we propose a gradientsparsification empowered FL framework over wireless channels, in order toimprove training efficiency without sacrificing convergence performance.Specifically, we first design a random sparsification algorithm to retain afraction of the gradient elements in each client's local training, therebymitigating the performance degradation induced by DP and and reducing thenumber of transmission parameters over wireless channels. Then, we analyze theconvergence bound of the proposed algorithm, by modeling a non-convex FLproblem. Next, we formulate a time-sequential stochastic optimization problemfor minimizing the developed convergence bound, under the constraints oftransmit power, the average transmitting delay, as well as the client's DPrequirement. Utilizing the Lyapunov drift-plus-penalty framework, we develop ananalytical solution to the optimization problem. Extensive experiments havebeen implemented on three real life datasets to demonstrate the effectivenessof our proposed algorithm. We show that our proposed algorithms can fullyexploit the interworking between communication and computation to outperformthe baselines, i.e., random scheduling, round robin and delay-minimizationalgorithms.", "output": "Gradient Sparsification for Efficient Wireless Federated Learning with Differential Privacy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper describes the participation of team QUST in the SemEval2023 task3. The monolingual models are first evaluated with the under-sampling of themajority classes in the early stage of the task. Then, the pre-trainedmultilingual model is fine-tuned with a combination of the class weights andthe sample weights. Two different fine-tuning strategies, the task-agnostic andthe task-dependent, are further investigated. All experiments are conductedunder the 10-fold cross-validation, the multilingual approaches are superior tothe monolingual ones. The submitted system achieves the second best in Italianand Spanish (zero-shot) in subtask-1.", "output": "Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern society, road safety relies heavily on the psychological andphysiological state of drivers. Negative factors such as fatigue, drowsiness,and stress can impair drivers' reaction time and decision making abilities,leading to an increased incidence of traffic accidents. Among the numerousstudies for impaired driving detection, wearable physiological measurement is areal-time approach to monitoring a driver's state. However, currently, thereare few driver physiological datasets in open road scenarios and the existingdatasets suffer from issues such as poor signal quality, small sample sizes,and short data collection periods. Therefore, in this paper, a large-scalemultimodal driving dataset for driver impairment detection and biometric datarecognition is designed and described. The dataset contains two modalities ofdriving signals: six-axis inertial signals and electrocardiogram (ECG) signals,which were recorded while over one hundred drivers were following the sameroute through open roads during several months. Both the ECG signal sensor andthe six-axis inertial signal sensor are installed on a specially designedsteering wheel cover, allowing for data collection without disturbing thedriver. Additionally, electrodermal activity (EDA) signals were also recordedduring the driving process and will be integrated into the presented datasetsoon. Future work can build upon this dataset to advance the field of driverimpairment detection. New methods can be explored for integrating other typesof biometric signals, such as eye tracking, to further enhance theunderstanding of driver states. The insights gained from this dataset can alsoinform the development of new driver assistance systems, promoting saferdriving practices and reducing the risk of traffic accidents. The OpenDriverdataset will be publicly available soon.", "output": "OpenDriver: an open-road driver state detection dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern fulfillment warehouses, agents traverse the map to complete endlesstasks that arrive on the fly, which is formulated as a lifelong Multi-AgentPath Finding (lifelong MAPF) problem. The goal of tackling this challengingproblem is to find the path for each agent in a finite runtime while maximizingthe throughput. However, existing methods encounter exponential growth ofruntime and undesirable phenomena of deadlocks and rerouting as the map size oragent density grows. To address these challenges in lifelong MAPF, we explorethe idea of highways mainly studied for one-shot MAPF (i.e., finding paths atonce beforehand), which reduces the complexity of the problem by encouragingagents to move in the same direction. We utilize two methods to incorporate thehighway idea into the lifelong MAPF framework and discuss the properties thatminimize the existing problems of deadlocks and rerouting. The experimentalresults demonstrate that the runtime is considerably reduced and the decay ofthroughput is gradually insignificant as the map size enlarges under thesettings of the highway. Furthermore, when the density of agents increases, thephenomena of deadlocks and rerouting are significantly reduced by leveragingthe highway.", "output": "The Study of Highway for Lifelong Multi-Agent Path Finding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the model aging problem, Deep Neural Networks (DNNs) need updates toadjust them to new data distributions. The common practice leveragesincremental learning (IL), e.g., Class-based Incremental Learning (CIL) thatupdates output labels, to update the model with new data and a limited numberof old data. This avoids heavyweight training (from scratch) using conventionalmethods and saves storage space by reducing the number of old data to store.But it also leads to poor performance in fairness. In this paper, we show thatCIL suffers both dataset and algorithm bias problems, and existing solutionscan only partially solve the problem. We propose a novel framework, CILIATE,that fixes both dataset and algorithm bias in CIL. It features a noveldifferential analysis guided dataset and training refinement process thatidentifies unique and important samples overlooked by existing CIL and enforcesthe model to learn from them. Through this process, CILIATE improves thefairness of CIL by 17.03%, 22.46%, and 31.79% compared to state-of-the-artmethods, iCaRL, BiC, and WA, respectively, based on our evaluation on threepopular datasets and widely used ResNet models.", "output": "CILIATE: Towards Fairer Class-based Incremental Learning by Dataset and Training Refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Owing to success in the data-rich domain of natural images, Transformers haverecently become popular in medical image segmentation. However, the pairing ofTransformers with convolutional blocks in varying architectural permutationsleaves their relative effectiveness to open interpretation. We introduceTransformer Ablations that replace the Transformer blocks with plain linearoperators to quantify this effectiveness. With experiments on 8 models on 2medical image segmentation tasks, we explore -- 1) the replaceable nature ofTransformer-learnt representations, 2) Transformer capacity alone cannotprevent representational replaceability and works in tandem with effectivedesign, 3) The mere existence of explicit feature hierarchies in transformerblocks is more beneficial than accompanying self-attention modules, 4) Majorspatial downsampling before Transformer modules should be used with caution.", "output": "Transformer Utilization in Medical Image Segmentation Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video captioning aims to convey dynamic scenes from videos using naturallanguage, facilitating the understanding of spatiotemporal information withinour environment. Although there have been recent advances, generating detailedand enriched video descriptions continues to be a substantial challenge. Inthis work, we introduce Video ChatCaptioner, an innovative approach forcreating more comprehensive spatiotemporal video descriptions. Our methodemploys a ChatGPT model as a controller, specifically designed to select framesfor posing video content-driven questions. Subsequently, a robust algorithm isutilized to answer these visual queries. This question-answer frameworkeffectively uncovers intricate video details and shows promise as a method forenhancing video content. Following multiple conversational rounds, ChatGPT cansummarize enriched video content based on previous conversations. Wequalitatively demonstrate that our Video ChatCaptioner can generate captionscontaining more visual details about the videos. The code is publicly availableat ", "output": "Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "MANET is a collection of mobile nodes that communicate through wirelessnetworks as they move from one point to another. MANET is aninfrastructure-less network with a changeable topology; as a result, it is verysusceptible to attacks. MANET attack prevention represents a seriousdifficulty. Malicious network nodes are the source of network-based attacks. Ina MANET, attacks can take various forms, and each one alters the network'soperation in its unique way. In general, attacks can be separated into twocategories: those that target the data traffic on a network and those thattarget the control traffic. This article explains the many sorts of assaults,their impact on MANET, and the MANET-based defence measures that are currentlyin place. The suggested SRA that employs blockchain technology (SRABC) protectsMANET from attacks and authenticates nodes. The secure routing algorithm (SRA)proposed by blockchain technology safeguards control and data flow againstthreats. This is achieved by generating a Hash Function for every transaction.We will begin by discussing the security of the MANET. This article's secondsection explores the role of blockchain in MANET security. In the thirdsection, the SRA is described in connection with blockchain. In the fourthphase, PDR and Throughput are utilised to conduct an SRA review usingBlockchain employing PDR and Throughput. The results suggest that the proposedtechnique enhances MANET security while concurrently decreasing delay. Theperformance of the proposed technique is analysed and compared to the routingprotocols Q-AODV and DSR.", "output": "Secure Routing Protocol To Mitigate Attacks By Using Blockchain Technology In Manet."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Labeling time series data is an expensive task because of domain expertiseand dynamic nature of the data. Hence, we often have to deal with limitedlabeled data settings. Data augmentation techniques have been successfullydeployed in domains like computer vision to exploit the use of existing labeleddata. We adapt one of the most commonly used technique called MixUp, in thetime series domain. Our proposed, MixUp++ and LatentMixUp++, use simplemodifications to perform interpolation in raw time series and classificationmodel's latent space, respectively. We also extend these methods withsemi-supervised learning to exploit unlabeled data. We observe significantimprovements of 1% - 15% on time series classification on two publicdatasets, for both low labeled data as well as high labeled data regimes, withLatentMixUp++.", "output": "Embarrassingly Simple MixUp for Time-series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces FrenchMedMCQA, the first publicly availableMultiple-Choice Question Answering (MCQA) dataset in French for medical domain.It is composed of 3,105 questions taken from real exams of the French medicalspecialization diploma in pharmacy, mixing single and multiple answers. Eachinstance of the dataset contains an identifier, a question, five possibleanswers and their manual correction(s). We also propose first baseline modelsto automatically process this MCQA task in order to report on the currentperformances and to highlight the difficulty of the task. A detailed analysisof the results showed that it is necessary to have representations adapted tothe medical domain or to the MCQA task: in our case, English specialized modelsyielded better results than generic French ones, even though FrenchMedMCQA isin French. Corpus, models and tools are available online.", "output": "FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a distributed Generative Adversarial Networks(discGANs) to generate synthetic tabular data specific to the healthcaredomain. While using GANs to generate images has been well studied, little to noattention has been given to generation of tabular data. Modeling distributionsof discrete and continuous tabular data is a non-trivial task with highutility. We applied discGAN to model non-Gaussian multi-modal healthcare data.We generated 249,000 synthetic records from original 2,027 eICU dataset. Weevaluated the performance of the model using machine learning efficacy, theKolmogorov-Smirnov (KS) test for continuous variables and chi-squared test fordiscrete variables. Our results show that discGAN was able to generate datawith distributions similar to the real data.", "output": "Distributed Conditional GAN (discGAN) For Synthetic Healthcare Data Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapid advancement in data-driven research has increased the demand foreffective graph data analysis. However, real-world data often exhibits classimbalance, leading to poor performance of machine learning models. To overcomethis challenge, class-imbalanced learning on graphs (CILG) has emerged as apromising solution that combines the strengths of graph representation learningand class-imbalanced learning. In recent years, significant progress has beenmade in CILG. Anticipating that such a trend will continue, this survey aims tooffer a comprehensive understanding of the current state-of-the-art in CILG andprovide insights for future research directions. Concerning the former, weintroduce the first taxonomy of existing work and its connection to existingimbalanced learning literature. Concerning the latter, we critically analyzerecent work in CILG and discuss urgent lines of inquiry within the topic.Moreover, we provide a continuously maintained reading list of papers and codeat ", "output": "Class-Imbalanced Learning on Graphs: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate time series forecasting is critical for a wide range of problemswith temporal data. Ensemble modeling is a well-established technique forleveraging multiple predictive models to increase accuracy and robustness, asthe performance of a single predictor can be highly variable due to shifts inthe underlying data distribution. This paper proposes a new methodology forbuilding robust ensembles of time series forecasting models. Our approachutilizes Adaptive Robust Optimization (ARO) to construct a linear regressionensemble in which the models' weights can adapt over time. We demonstrate theeffectiveness of our method through a series of synthetic experiments andreal-world applications, including air pollution management, energy consumptionforecasting, and tropical cyclone intensity forecasting. Our results show thatour adaptive ensembles outperform the best ensemble member in hindsight by16-26% in root mean square error and 14-28% in conditional value at risk andimprove over competitive ensemble techniques.", "output": "Ensemble Modeling for Time Series Forecasting: an Adaptive Robust Optimization Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding the continuous states of objects is essential for task learningand planning in the real world. However, most existing task learning benchmarksassume discrete(e.g., binary) object goal states, which poses challenges forthe learning of complex tasks and transferring learned policy from simulatedenvironments to the real world. Furthermore, state discretization limits arobot's ability to follow human instructions based on the grounding of actionsand states. To tackle these challenges, we present ARNOLD, a benchmark thatevaluates language-grounded task learning with continuous states in realistic3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involveunderstanding object states and learning policies for continuous goals. Topromote language-instructed learning, we provide expert demonstrations withtemplate-generated language descriptions. We assess task performance byutilizing the latest language-conditioned policy learning models. Our resultsindicate that current models for language-conditioned manipulations continue toexperience significant challenges in novel goal-state generalizations, scenegeneralizations, and object generalizations. These findings highlight the needto develop new algorithms that address this gap and underscore the potentialfor further research in this area. See our project page at:", "output": "ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, ChatGPT has drawn great attention from both the research communityand the public. We are particularly curious about whether it can serve as auniversal sentiment analyzer. To this end, in this work, we provide apreliminary evaluation of ChatGPT on the understanding of opinions, sentiments,and emotions contained in the text. Specifically, we evaluate it in foursettings, including standard evaluation, polarity shift evaluation, open-domainevaluation, and sentiment inference evaluation. The above evaluation involves18 benchmark datasets and 5 representative sentiment analysis tasks, and wecompare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA)models on end-task. Moreover, we also conduct human evaluation and present somequalitative case studies to gain a deep comprehension of its sentiment analysiscapabilities.", "output": "Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Common Information (CI) approach provides a systematic way to transform amulti-agent stochastic control problem to a single-agent partially observedMarkov decision problem (POMDP) called the coordinator's POMDP. However, such aPOMDP can be hard to solve due to its extraordinarily large action space. Wepropose a new algorithm for multi-agent stochastic control problems, calledcoordinator's heuristic search value iteration (CHSVI), that combines the CIapproach and point-based POMDP algorithms for large action spaces. Wedemonstrate the algorithm through optimally solving several benchmark problems.", "output": "A Novel Point-based Algorithm for Multi-agent Control Using the Common Information Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce a new NLP task -- generating short factualarticles with references for queries by mining supporting evidence from theWeb. In this task, called WebBrain, the ultimate goal is to generate a fluent,informative, and factually-correct short article (e.g., a Wikipedia article)for a factual query unseen in Wikipedia. To enable experiments on WebBrain, weconstruct a large-scale dataset WebBrain-Raw by extracting English Wikipediaarticles and their crawlable Wikipedia references. WebBrain-Raw is ten timeslarger than the previous biggest peer dataset, which can greatly benefit theresearch community. From WebBrain-Raw, we construct two task-specific datasets:WebBrain-R and WebBrain-G, which are used to train in-domain retriever andgenerator, respectively. Besides, we empirically analyze the performances ofthe current state-of-the-art NLP techniques on WebBrain and introduce a newframework ReGen, which enhances the generation factualness by improved evidenceretrieval and task-specific pre-training for generation. Experiment resultsshow that ReGen outperforms all baselines in both automatic and humanevaluations.", "output": "WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human intelligence has the remarkable ability to assemble basic skills intocomplex ones so as to solve complex tasks. This ability is equally importantfor Artificial Intelligence (AI), and thus, we assert that in addition to thedevelopment of large, comprehensive intelligent models, it is equally crucialto equip such models with the capability to harness various domain-specificexpert models for complex task-solving in the pursuit of Artificial GeneralIntelligence (AGI). Recent developments in Large Language Models (LLMs) havedemonstrated remarkable learning and reasoning abilities, making them promisingas a controller to select, synthesize, and execute external models to solvecomplex tasks. In this project, we develop OpenAGI, an open-source AGI researchplatform, specifically designed to offer complex, multi-step tasks andaccompanied by task-specific datasets, evaluation metrics, and a diverse rangeof extensible models. OpenAGI formulates complex tasks as natural languagequeries, serving as input to the LLM. The LLM subsequently selects,synthesizes, and executes models provided by OpenAGI to address the task.Furthermore, we propose a Reinforcement Learning from Task Feedback (RLTF)mechanism, which uses the task-solving result as feedback to improve the LLM'stask-solving ability. Thus, the LLM is responsible for synthesizing variousexternal models for solving complex tasks, while RLTF provides feedback toimprove its task-solving ability, enabling a feedback loop for self-improvingAI. We believe that the paradigm of LLMs operating various expert models forcomplex task-solving is a promising approach towards AGI. To facilitate thecommunity's long-term improvement and evaluation of AGI's ability, weopen-source the code, benchmark, and evaluation methods of the OpenAGI projectat ", "output": "OpenAGI: When LLM Meets Domain Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graphs (KGs) store rich facts about the real world. In this paper,we study KG alignment, which aims to find alignment between not only entitiesbut also relations and classes in different KGs. Alignment at the entity levelcan cross-fertilize alignment at the schema level. We propose a new KGalignment approach, called DAAKG, based on deep learning and active learning.With deep learning, it learns the embeddings of entities, relations andclasses, and jointly aligns them in a semi-supervised manner. With activelearning, it estimates how likely an entity, relation or class pair can beinferred, and selects the best batch for human labeling. We design twoapproximation algorithms for efficient solution to batch selection. Ourexperiments on benchmark datasets show the superior accuracy and generalizationof DAAKG and validate the effectiveness of all its modules.", "output": "Deep Active Alignment of Knowledge Graph Entities and Schemata."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised representation learning on (large) graphs has receivedsignificant attention in the research community due to the compactness andrichness of the learned embeddings and the abundance of unlabelled graph data.When deployed, these node representations must be generated with appropriatefairness constraints to minimize bias induced by them on downstream tasks.Consequently, group and individual fairness notions for graph learningalgorithms have been investigated for specific downstream tasks. One majorlimitation of these fairness notions is that they do not consider theconnectivity patterns in the graph leading to varied node influence (orcentrality power). In this paper, we design a centrality-aware fairnessframework for inductive graph representation learning algorithms. We proposeCAFIN (Centrality Aware Fairness inducing IN-processing), an in-processingtechnique that leverages graph structure to improve GraphSAGE's representations- a popular framework in the unsupervised inductive setting. We demonstrate theefficacy of CAFIN in the inductive setting on two popular downstream tasks -Link prediction and Node Classification. Empirically, they consistentlyminimize the disparity in fairness between groups across datasets (varying from18 to 80% reduction in imparity, a measure of group fairness) from differentdomains while incurring only a minimal performance cost.", "output": "CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual and linguistic pre-training aims to learn vision and languagerepresentations together, which can be transferred to visual-linguisticdownstream tasks. However, there exists semantic confusion between language andvision during the pre-training stage. Moreover, current pre-trained models tendto take lots of computation resources for fine-tuning when transferred todownstream tasks. In this work, we present a simple but effective approach forlearning Contrastive and Adaptive representations of Vision and Language,namely CAVL. Specifically, we introduce a pair-wise contrastive loss to learnalignments between the whole sentence and each image in the same batch duringthe pre-training process. At the fine-tuning stage, we introduce twolightweight adaptation networks to reduce model parameters and increasetraining speed for saving computation resources. We evaluate our CAVL on sixmain downstream tasks, including Visual Question Answering (VQA), VisualCommonsense Reasoning (VCR), Natural Language for Visual Reasoning (NLVR),Region-to-Phrase Grounding (RPG), Text-to-Image Retrieval (TIR), and Zero-shotText-to-Image Retrieval (ZS-TIR). Compared to baselines, we achieve superiorperformance and reduce the fine-tuning time by a large margin (in particular,76.17%). Extensive experiments and ablation studies demonstrate the efficiencyof contrastive pre-training and adaptive fine-tuning proposed in our CAVL.", "output": "CAVL: Learning Contrastive and Adaptive Representations of Vision and Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the increasing demand for oriented object detection e.g. in autonomousdriving and remote sensing, the oriented annotation has become alabor-intensive work. To make full use of existing horizontally annotateddatasets and reduce the annotation cost, a weakly-supervised detector H2RBoxfor learning the rotated box (RBox) from the horizontal box (HBox) has beenproposed and received great attention. This paper presents a new version,H2RBox-v2, to further bridge the gap between HBox-supervised andRBox-supervised oriented object detection. While exploiting axisymmetry viaflipping and rotating consistencies is available through our theoreticalanalysis, H2RBox-v2, using a weakly-supervised branch similar to H2RBox, isembedded with a novel self-supervised branch that learns orientations from thesymmetry inherent in the image of objects. Complemented by modules to cope withperipheral issues, e.g. angular periodicity, a stable and effective solution isachieved. To our knowledge, H2RBox-v2 is the first symmetry-supervised paradigmfor oriented object detection. Compared to H2RBox, our method is lesssusceptible to low annotation quality and insufficient training data, which insuch cases is expected to give a competitive performance much closer tofully-supervised oriented object detectors. Specifically, the performancecomparison between H2RBox-v2 and Rotated FCOS on DOTA-v1.0/1.5/2.0 is72.31%/64.76%/50.33% vs. 72.44%/64.53%/51.77%, 89.66% vs. 88.99% on HRSC, and42.27% vs. 41.25% on FAIR1M.", "output": "H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online class-incremental continual learning is a specific task of continuallearning. It aims to continuously learn new classes from data stream and thesamples of data stream are seen only once, which suffers from the catastrophicforgetting issue, i.e., forgetting historical knowledge of old classes.Existing replay-based methods effectively alleviate this issue by saving andreplaying part of old data in a proxy-based or contrastive-based replay manner.Although these two replay manners are effective, the former would incline tonew classes due to class imbalance issues, and the latter is unstable and hardto converge because of the limited number of samples. In this paper, we conducta comprehensive analysis of these two replay manners and find that they can becomplementary. Inspired by this finding, we propose a novel replay-based methodcalled proxy-based contrastive replay (PCR). The key operation is to replacethe contrastive samples of anchors with corresponding proxies in thecontrastive-based way. It alleviates the phenomenon of catastrophic forgettingby effectively addressing the imbalance issue, as well as keeps a fasterconvergence of the model. We conduct extensive experiments on three real-worldbenchmark datasets, and empirical results consistently demonstrate thesuperiority of PCR over various state-of-the-art methods.", "output": "PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image segmentation is a challenging task with inherent ambiguity andhigh uncertainty, attributed to factors such as unclear tumor boundaries andmultiple plausible annotations. The accuracy and diversity of segmentationmasks are both crucial for providing valuable references to radiologists inclinical practice. While existing diffusion models have shown strong capacitiesin various visual generation tasks, it is still challenging to deal withdiscrete masks in segmentation. To achieve accurate and diverse medical imagesegmentation masks, we propose a novel conditional Bernoulli Diffusion modelfor medical image segmentation (BerDiff). Instead of using the Gaussian noise,we first propose to use the Bernoulli noise as the diffusion kernel to enhancethe capacity of the diffusion model for binary segmentation tasks, resulting inmore accurate segmentation masks. Second, by leveraging the stochastic natureof the diffusion model, our BerDiff randomly samples the initial Bernoullinoise and intermediate latent variables multiple times to produce a range ofdiverse segmentation masks, which can highlight salient regions of interestthat can serve as valuable references for radiologists. In addition, ourBerDiff can efficiently sample sub-sequences from the overall trajectory of thereverse diffusion, thereby speeding up the segmentation process. Extensiveexperimental results on two medical image segmentation datasets with differentmodalities demonstrate that our BerDiff outperforms other recently publishedstate-of-the-art methods. Our results suggest diffusion models could serve as astrong backbone for medical image segmentation.", "output": "BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Feature extraction is an essential task in graph analytics. These featurevectors, called graph descriptors, are used in downstream vector-space-basedgraph analysis models. This idea has proved fruitful in the past, withspectral-based graph descriptors providing state-of-the-art classificationaccuracy. However, known algorithms to compute meaningful descriptors do notscale to large graphs since: (1) they require storing the entire graph inmemory, and (2) the end-user has no control over the algorithm's runtime. Inthis paper, we present streaming algorithms to approximately compute threedifferent graph descriptors capturing the essential structure of graphs.Operating on edge streams allows us to avoid storing the entire graph inmemory, and controlling the sample size enables us to keep the runtime of ouralgorithms within desired bounds. We demonstrate the efficacy of the proposeddescriptors by analyzing the approximation error and classification accuracy.Our scalable algorithms compute descriptors of graphs with millions of edgeswithin minutes. Moreover, these descriptors yield predictive accuracycomparable to the state-of-the-art methods but can be computed using only 25%as much memory.", "output": "Computing Graph Descriptors on Edge Streams."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "By driving models to converge to flat minima, sharpness-aware learningalgorithms (such as SAM) have shown the power to achieve state-of-the-artperformances. However, these algorithms will generally incur one extraforward-backward propagation at each training iteration, which largely burdensthe computation especially for scalable models. To this end, we propose asimple yet efficient training scheme, called Randomized Sharpness-AwareTraining (RST). Optimizers in RST would perform a Bernoulli trial at eachiteration to choose randomly from base algorithms (SGD) and sharpness-awarealgorithms (SAM) with a probability arranged by a predefined schedulingfunction. Due to the mixture of base algorithms, the overall count ofpropagation pairs could be largely reduced. Also, we give theoretical analysison the convergence of RST. Then, we empirically study the computation cost andeffect of various types of scheduling functions, and give directions on settingappropriate scheduling functions. Further, we extend the RST to a generalframework (G-RST), where we can adjust regularization degree on sharpnessfreely for any scheduling function. We show that G-RST can outperform SAM inmost cases while saving 50% extra computation cost.", "output": "Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Protein representation learning methods have shown great potential to yielduseful representation for many downstream tasks, especially on proteinclassification. Moreover, a few recent studies have shown great promise inaddressing insufficient labels of proteins with self-supervised learningmethods. However, existing protein language models are usually pretrained onprotein sequences without considering the important protein structuralinformation. To this end, we propose a novel structure-aware proteinself-supervised learning method to effectively capture structural informationof proteins. In particular, a well-designed graph neural network (GNN) model ispretrained to preserve the protein structural information with self-supervisedtasks from a pairwise residue distance perspective and a dihedral angleperspective, respectively. Furthermore, we propose to leverage the availableprotein language model pretrained on protein sequences to enhance theself-supervised learning. Specifically, we identify the relation between thesequential information in the protein language model and the structuralinformation in the specially designed GNN model via a novel pseudo bi-leveloptimization scheme. Experiments on several supervised downstream tasks verifythe effectiveness of our proposed method.The code of the proposed method isavailable in url{", "output": "Structure-aware Protein Self-supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article presents a synthetic distracted driving (SynDD2 - a continuum ofSynDD1) dataset for machine learning models to detect and analyze drivers'various distracted behavior and different gaze zones. We collected the data ina stationary vehicle using three in-vehicle cameras positioned at locations: onthe dashboard, near the rearview mirror, and on the top right-side windowcorner. The dataset contains two activity types: distracted activities and gazezones for each participant, and each activity type has two sets: withoutappearance blocks and with appearance blocks such as wearing a hat orsunglasses. The order and duration of each activity for each participant arerandom. In addition, the dataset contains manual annotations for each activity,having its start and end time annotated. Researchers could use this dataset toevaluate the performance of machine learning algorithms to classify variousdistracting activities and gaze zones of drivers.", "output": "Synthetic Distracted Driving (SynDD2) dataset for analyzing distracted behaviors and various gaze zones of a driver."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Audio-based classification techniques on body sounds have long been studiedto support diagnostic decisions, particularly in pulmonary diseases. Inresponse to the urgency of the COVID-19 pandemic, a growing number of modelsare developed to identify COVID-19 patients based on acoustic input. Mostmodels focus on cough because the dry cough is the best-known symptom ofCOVID-19. However, other body sounds, such as breath and speech, have also beenrevealed to correlate with COVID-19 as well. In this work, rather than relyingon a specific body sound, we propose Fused Audio Instance and Representationfor COVID-19 Detection (FAIR4Cov). It relies on constructing a joint featurevector obtained from a plurality of body sounds in waveform and spectrogramrepresentation. The core component of FAIR4Cov is a self-attention fusion unitthat is trained to establish the relation of multiple body sounds and audiorepresentations and integrate it into a compact feature vector. We set up ourexperiments on different combinations of body sounds using only waveform,spectrogram, and a joint representation of waveform and spectrogram. Ourfindings show that the use of self-attention to combine extracted features fromcough, breath, and speech sounds leads to the best performance with an AreaUnder the Receiver Operating Characteristic Curve (AUC) score of 0.8658, asensitivity of 0.8057, and a specificity of 0.7958. This AUC is 0.0227 higherthan the one of the models trained on spectrograms only and 0.0847 higher thanthe one of the models trained on waveforms only. The results demonstrate thatthe combination of spectrogram with waveform representation helps to enrich theextracted features and outperforms the models with single representation.", "output": "FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, the interpretation of why a machine learning (ML) model makescertain inferences is as crucial as the accuracy of such inferences. Some MLmodels like the decision tree possess inherent interpretability that can bedirectly comprehended by humans. Others like artificial neural networks (ANN),however, rely on external methods to uncover the deduction mechanism. SHapleyAdditive exPlanations (SHAP) is one of such external methods, which requires abackground dataset when interpreting ANNs. Generally, a background datasetconsists of instances randomly sampled from the training dataset. However, thesampling size and its effect on SHAP remain to be unexplored. In our empiricalstudy on the MIMIC-III dataset, we show that the two core explanations - SHAPvalues and variable rankings fluctuate when using different background datasetsacquired from random sampling, indicating that users cannot unquestioninglytrust the one-shot interpretation from SHAP. Luckily, such fluctuationdecreases with the increase of the background dataset size. Also, we notice anU-shape in the stability assessment of SHAP variable rankings, demonstratingthat SHAP is more reliable in ranking the most and least important variablescompared to moderately important ones. Overall, our results suggest that usersshould take into account how background data affects SHAP results, withimproved SHAP stability as the background sample size increases.", "output": "An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in large language models (LLMs) have transformed the field ofnatural language processing (NLP). From GPT-3 to PaLM, the state-of-the-artperformance on natural language tasks is being pushed forward with every newlarge language model. Along with natural language abilities, there has been asignificant interest in understanding whether such models exhibit reasoningcapabilities with the use of reasoning benchmarks. However, even though resultsare seemingly positive, these benchmarks prove to be simplistic in nature andthe performance of LLMs on these benchmarks cannot be used as evidence tosupport, many a times outlandish, claims being made about LLMs' reasoningcapabilities. Further, these only represent a very limited set of simplereasoning tasks and we need to look at more sophisticated reasoning problems ifwe are to measure the true limits of such LLM-based systems. Motivated by this,we propose an extensible assessment framework to test the capabilities of LLMson reasoning about actions and change, a central aspect of human intelligence.We provide multiple test cases that are more involved than any of thepreviously established benchmarks and each test case evaluates a differentaspect of reasoning about actions and change. Results on GPT-3 (davinci),Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performanceon such reasoning tasks.", "output": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present ShapeCrafter, a neural network for recursive text-conditioned 3Dshape generation. Existing methods to generate text-conditioned 3D shapesconsume an entire text prompt to generate a 3D shape in a single step. However,humans tend to describe shapes recursively-we may start with an initialdescription and progressively add details based on intermediate results. Tocapture this recursive process, we introduce a method to generate a 3D shapedistribution, conditioned on an initial phrase, that gradually evolves as morephrases are added. Since existing datasets are insufficient for training thisapproach, we present Text2Shape++, a large dataset of 369K shape-text pairsthat supports recursive shape generation. To capture local details that areoften used to refine shape descriptions, we build on top of vector-quantizeddeep implicit functions that generate a distribution of high-quality shapes.Results show that our method can generate shapes consistent with textdescriptions, and shapes evolve gradually as more phrases are added. Our methodsupports shape editing, extrapolation, and can enable new applications inhuman-machine collaboration for creative design.", "output": "ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a method to map 2D image observations of a scene to a persistent3D scene representation, enabling novel view synthesis and disentangledrepresentation of the movable and immovable components of the scene. Motivatedby the bird's-eye-view (BEV) representation commonly used in vision androbotics, we propose conditional neural groundplans, ground-aligned 2D featuregrids, as persistent and memory-efficient scene representations. Our method istrained self-supervised from unlabeled multi-view observations usingdifferentiable rendering, and learns to complete geometry and appearance ofoccluded regions. In addition, we show that we can leverage multi-view videosat training time to learn to separately reconstruct static and movablecomponents of the scene from a single image at test time. The ability toseparately reconstruct movable objects enables a variety of downstream tasksusing simple heuristics, such as extraction of object-centric 3Drepresentations, novel view synthesis, instance-level segmentation, 3D boundingbox prediction, and scene editing. This highlights the value of neuralgroundplans as a backbone for efficient 3D scene understanding models.", "output": "Neural Groundplans: Persistent Neural Scene Representations from a Single Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Adversarial Networks (GANs) have shown compelling results invarious tasks and applications in recent years. However, mode collapse remainsa critical problem in GANs. In this paper, we propose a novel training pipelineto address the mode collapse issue of GANs. Different from existing methods, wepropose to generalize the discriminator as feature embedding and maximize theentropy of distributions in the embedding space learned by the discriminator.Specifically, two regularization terms, i.e., Deep Local Linear Embedding(DLLE) and Deep Isometric feature Mapping (DIsoMap), are designed to encouragethe discriminator to learn the structural information embedded in the data,such that the embedding space learned by the discriminator can be well-formed.Based on the well-learned embedding space supported by the discriminator, anon-parametric entropy estimator is designed to efficiently maximize theentropy of embedding vectors, playing as an approximation of maximizing theentropy of the generated distribution. By improving the discriminator andmaximizing the distance of the most similar samples in the embedding space, ourpipeline effectively reduces the mode collapse without sacrificing the qualityof generated samples. Extensive experimental results show the effectiveness ofour method, which outperforms the GAN baseline, MaF-GAN on CelebA (9.13 vs.12.43 in FID) and surpasses the recent state-of-the-art energy-based model onthe ANIME-FACE dataset (2.80 vs. 2.26 in Inception score). The code isavailable at ", "output": "Combating Mode Collapse in GANs via Manifold Entropy Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conformal prediction is a distribution-free technique for establishing validprediction intervals. Although conventionally people conduct conformalprediction in the output space, this is not the only possibility. In thispaper, we propose feature conformal prediction, which extends the scope ofconformal prediction to semantic feature spaces by leveraging the inductivebias of deep representation learning. From a theoretical perspective, wedemonstrate that feature conformal prediction provably outperforms regularconformal prediction under mild assumptions. Our approach could be combinedwith not only vanilla conformal prediction, but also other adaptive conformalprediction methods. Apart from experiments on existing predictive inferencebenchmarks, we also demonstrate the state-of-the-art performance of theproposed methods on large-scale tasks such as ImageNet classification andCityscapes image segmentation.The code is available aturl{", "output": "Predictive Inference with Feature Conformal Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a smoothly broken power law functional form (referred to by us asa Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates thescaling behaviors of deep neural networks (i.e. how the evaluation metric ofinterest varies as the amount of compute used for training, number of modelparameters, training dataset size, model input size, number of training steps,or upstream performance varies) for various architectures and for each ofvarious tasks within a large and diverse set of upstream and downstream tasks,in zero-shot, prompted, and fine-tuned settings. This set includes large-scalevision, language, audio, video, diffusion, generative modeling, multimodallearning, contrastive learning, AI alignment, robotics, out-of-distribution(OOD) generalization, continual learning, transfer learning, uncertaintyestimation / calibration, out-of-distribution detection, adversarialrobustness, distillation, sparsity, retrieval, quantization, pruning, fairness,molecules, computer programming/coding, math word problems, \"emergent\" \"phasetransitions / changes\", arithmetic, unsupervised/self-supervised learning, &amp;reinforcement learning (single agent &amp; multi-agent). When compared to otherfunctional forms for neural scaling behavior, this functional form yieldsextrapolations of scaling behavior that are considerably more accurate on thisset. Moreover, this functional form accurately models &amp; extrapolates scalingbehavior that other functional forms are incapable of expressing such as thenon-monotonic transitions present in the scaling behavior of phenomena such asdouble descent &amp; the delayed, sharp inflection points present in the scalingbehavior of tasks such as arithmetic. Lastly, we use this functional form toglean insights about the limit of the predictability of scaling behavior. Codeis available at ", "output": "Broken Neural Scaling Laws."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The substitute-based recommendation is widely used in E-commerce to providebetter alternatives to customers. However, existing research typically uses thecustomer behavior signals like co-view and view-but-purchase-another to capturethe substitute relationship. Despite its intuitive soundness, we find that suchan approach might ignore the functionality and characteristics of products. Inthis paper, we adapt substitute recommendation into language matching problemby taking product title description as model input to consider productfunctionality. We design a new transformation method to de-noise the signalsderived from production data. In addition, we consider multilingual supportfrom the engineering point of view. Our proposed end-to-end transformer-basedmodel achieves both successes from offline and online experiments. The proposedmodel has been deployed in a large-scale E-commerce website for 11 marketplacesin 6 languages. Our proposed model is demonstrated to increase revenue by 19%based on an online A/B experiment.", "output": "A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, the field of quantum science has attracted significantinterest across various disciplines, including quantum machine learning,quantum communication, and quantum computing. Among these emerging areas,quantum federated learning (QFL) has gained particular attention due to theintegration of quantum neural networks (QNNs) with traditional federatedlearning (FL) techniques. In this study, a novel approach entitled quantumsplit learning (QSL) is presented, which represents an advanced extension ofclassical split learning. Previous research in classical computing hasdemonstrated numerous advantages of split learning, such as acceleratedconvergence, reduced communication costs, and enhanced privacy protection. Tomaximize the potential of QSL, cross-channel pooling is introduced, a techniquethat capitalizes on the distinctive properties of quantum state tomographyfacilitated by QNNs. Through rigorous numerical analysis, evidence is providedthat QSL not only achieves a 1.64% higher top-1 accuracy compared to QFL butalso demonstrates robust privacy preservation in the context of the MNISTclassification task.", "output": "Quantum Split Neural Network Learning using Cross-Channel Pooling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ensembling has proven to be a powerful technique for boosting modelperformance, uncertainty estimation, and robustness in supervised learning.Advances in self-supervised learning (SSL) enable leveraging large unlabeledcorpora for state-of-the-art few-shot and supervised learning performance. Inthis paper, we explore how ensemble methods can improve recent SSL techniquesby developing a framework that permits data-dependent weighted cross-entropylosses. We refrain from ensembling the representation backbone; this choiceyields an efficient ensemble method that incurs a small training cost andrequires no architectural changes or computational overhead to downstreamevaluation. The effectiveness of our method is demonstrated with twostate-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al.,2022). Our method outperforms both in multiple evaluation metrics onImageNet-1K, particularly in the few-shot setting. We explore several weightingschemes and find that those which increase the diversity of ensemble heads leadto better downstream evaluation results. Thorough experiments yield improvedprior art baselines which our method still surpasses; e.g., our overallimprovement with MSN ViT-B/16 is 3.9 p.p. for 1-shot learning.", "output": "Weighted Ensemble Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently the Transformer structure has shown good performances in graphlearning tasks. However, these Transformer models directly work on graph nodesand may have difficulties learning high-level information. Inspired by thevision transformer, which applies to image patches, we propose a newTransformer-based graph neural network: Patch Graph Transformer (PatchGT).Unlike previous transformer-based models for learning graph representations,PatchGT learns from non-trainable graph patches, not from nodes directly. Itcan help save computation and improve the model performance. The key idea is tosegment a graph into patches based on spectral clustering without any trainableparameters, with which the model can first use GNN layers to learn patch-levelrepresentations and then use Transformer to obtain graph-level representations.The architecture leverages the spectral information of graphs and combines thestrengths of GNNs and Transformers. Further, we show the limitations ofprevious hierarchical trainable clusters theoretically and empirically. We alsoprove the proposed non-trainable spectral clustering method is permutationinvariant and can help address the information bottlenecks in the graph.PatchGT achieves higher expressiveness than 1-WL-type GNNs, and the empiricalstudy shows that PatchGT achieves competitive performances on benchmarkdatasets and provides interpretability to its predictions. The implementationof our algorithm is released at our Github repo:", "output": "PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing knowledge graph (KG) embedding models have primarily focused onstatic KGs. However, real-world KGs do not remain static, but rather evolve andgrow in tandem with the development of KG applications. Consequently, new factsand previously unseen entities and relations continually emerge, necessitatingan embedding model that can quickly learn and transfer new knowledge throughgrowth. Motivated by this, we delve into an expanding field of KG embedding inthis paper, i.e., lifelong KG embedding. We consider knowledge transfer andretention of the learning on growing snapshots of a KG without having to learnembeddings from scratch. The proposed model includes a masked KG autoencoderfor embedding learning and update, with an embedding transfer strategy toinject the learned knowledge into the new entity and relation embeddings, andan embedding regularization method to avoid catastrophic forgetting. Toinvestigate the impacts of different aspects of KG growth, we construct fourdatasets to evaluate the performance of lifelong KG embedding. Experimentalresults show that the proposed model outperforms the state-of-the-art inductiveand lifelong embedding baselines.", "output": "Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) has emerged as a promising distributed learningparadigm with an added advantage of data privacy. With the growing interest inhaving collaboration among data owners, FL has gained significant attention oforganizations. The idea of FL is to enable collaborating participants trainmachine learning (ML) models on decentralized data without breaching privacy.In simpler words, federated learning is the approach of ``bringing the model tothe data, instead of bringing the data to the mode''. Federated learning, whenapplied to data which is partitioned vertically across participants, is able tobuild a complete ML model by combining local models trained only using the datawith distinct features at the local sites. This architecture of FL is referredto as vertical federated learning (VFL), which differs from the conventional FLon horizontally partitioned data. As VFL is different from conventional FL, itcomes with its own issues and challenges. In this paper, we present astructured literature review discussing the state-of-the-art approaches in VFL.Additionally, the literature review highlights the existing solutions tochallenges in VFL and provides potential research directions in this domain.", "output": "Vertical Federated Learning: A Structured Literature Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transfer learning on edge is challenging due to on-device limited resources.Existing work addresses this issue by training a subset of parameters or addingmodel patches. Developed with inference in mind, Inverted Residual Blocks(IRBs) split a convolutional layer into depthwise and pointwise convolutions,leading to more stacking layers, e.g., convolution, normalization, andactivation layers. Though they are efficient for inference, IRBs require thatadditional activation maps are stored in memory for training weights forconvolution layers and scales for normalization layers. As a result, their highmemory cost prohibits training IRBs on resource-limited edge devices, andmaking them unsuitable in the context of transfer learning. To address thisissue, we present MobileTL, a memory and computationally efficient on-devicetransfer learning method for models built with IRBs. MobileTL trains the shiftsfor internal normalization layers to avoid storing activation maps for thebackward pass. Also, MobileTL approximates the backward computation of theactivation layer (e.g., Hard-Swish and ReLU6) as a signed function whichenables storing a binary mask instead of activation maps for the backward pass.MobileTL fine-tunes a few top blocks (close to output) rather than propagatingthe gradient through the whole network to reduce the computation cost. Ourmethod reduces memory usage by 46% and 53% for MobileNetV2 and V3 IRBs,respectively. For MobileNetV3, we observe a 36% reduction in floating-pointoperations (FLOPs) when fine-tuning 5 blocks, while only incurring a 0.6%accuracy reduction on CIFAR10. Extensive experiments on multiple datasetsdemonstrate that our method is Pareto-optimal (best accuracy under givenhardware constraints) compared to prior work in transfer learning for edgedevices.", "output": "MobileTL: On-device Transfer Learning with Inverted Residual Blocks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervision for metric learning has long been given in the form ofequivalence between human-labeled classes. Although this type of supervisionhas been a basis of metric learning for decades, we argue that it hindersfurther advances in the field. In this regard, we propose a new regularizationmethod, dubbed HIER, to discover the latent semantic hierarchy of trainingdata, and to deploy the hierarchy to provide richer and more fine-grainedsupervision than inter-class separability induced by common metric learninglosses.HIER achieves this goal with no annotation for the semantic hierarchybut by learning hierarchical proxies in hyperbolic spaces. The hierarchicalproxies are learnable parameters, and each of them is trained to serve as anancestor of a group of data or other proxies to approximate the semantichierarchy among them. HIER deals with the proxies along with data in hyperbolicspace since the geometric properties of the space are well-suited to representtheir hierarchical structure. The efficacy of HIER is evaluated on fourstandard benchmarks, where it consistently improved the performance ofconventional methods when integrated with them, and consequently achieved thebest records, surpassing even the existing hyperbolic metric learningtechnique, in almost all settings.", "output": "HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model bias triggered by long-tailed data has been widely studied. However,measure based on the number of samples cannot explicate three phenomenasimultaneously: (1) Given enough data, the classification performance gain ismarginal with additional samples. (2) Classification performance decaysprecipitously as the number of training samples decreases when there isinsufficient data. (3) Model trained on sample-balanced datasets still hasdifferent biases for different classes. In this work, we define and quantifythe semantic scale of classes, which is used to measure the feature diversityof classes. It is exciting to find experimentally that there is a marginaleffect of semantic scale, which perfectly describes the first two phenomena.Further, the quantitative measurement of semantic scale imbalance is proposed,which can accurately reflect model bias on multiple datasets, even onsample-balanced data, revealing a novel perspective for the study of classimbalance. Due to the prevalence of semantic scale imbalance, we proposesemantic-scale-balanced learning, including a general loss improvement schemeand a dynamic re-weighting training framework that overcomes the challenge ofcalculating semantic scales in real-time during iterations. Comprehensiveexperiments show that dynamic semantic-scale-balanced learning consistentlyenables the model to perform superiorly on large-scale long-tailed andnon-long-tailed natural and medical datasets, which is a good starting pointfor mitigating the prevalent but unnoticed model bias.", "output": "Delving into Semantic Scale Imbalance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In psychiatric diagnosis, a contemporary data-driven, manual-based method formental disorders classification is the most popular technique; however, it hasseveral inevitable flaws. Using the three-way decision as a framework, wepropose a unified model that stands for clinicians' subjective approach (CSA)analysis consisting of three parts: quantitative analysis, quantitativeanalysis, and evaluation-based analysis. A ranking list and a set of numericalweights based on illness magnitude levels according to the clinician's greatestdegree of assumptions are the findings of the qualitative and quantitativeinvestigation. We further create a comparative classification of illnesses intothree groups with varying important levels; a three-way evaluation-based modelis utilized in this study for the aim of understanding and portraying theseresults in a more clear way. This proposed method might be integrated with themanual-based process as a complementary tool to improve precision whilediagnosing mental disorders", "output": "Classifying Mental-Disorders through Clinicians Subjective Approach based on Three-way Decision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatial and temporal modeling is one of the most core aspects of few-shotaction recognition. Most previous works mainly focus on long-term temporalrelation modeling based on high-level spatial representations, withoutconsidering the crucial low-level spatial features and short-term temporalrelations. Actually, the former feature could bring rich local semanticinformation, and the latter feature could represent motion characteristics ofadjacent frames, respectively. In this paper, we propose SloshNet, a newframework that revisits the spatial and temporal modeling for few-shot actionrecognition in a finer manner. First, to exploit the low-level spatialfeatures, we design a feature fusion architecture search module toautomatically search for the best combination of the low-level and high-levelspatial features. Next, inspired by the recent transformer, we introduce along-term temporal modeling module to model the global temporal relations basedon the extracted spatial appearance features. Meanwhile, we design anothershort-term temporal modeling module to encode the motion characteristicsbetween adjacent frame representations. After that, the final predictions canbe obtained by feeding the embedded rich spatial-temporal features to a commonframe-level class prototype matcher. We extensively validate the proposedSloshNet on four few-shot action recognition datasets, includingSomething-Something V2, Kinetics, UCF101, and HMDB51. It achieves favorableresults against state-of-the-art methods in all datasets.", "output": "Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work provides a formalization of Knowledge Graphs (KGs) as a new classof graphs that we denote doubly exchangeable attributed graphs, where node andpairwise (joint 2-node) representations must be equivariant to permutations ofboth node ids and edge (&amp; node) attributes (relations &amp; node features).Double-permutation equivariant KG representations open a new research directionin KGs. We show that this equivariance imposes a structural representation ofrelations that allows neural networks to perform complex logical reasoningtasks in KGs. Finally, we introduce a general blueprint for such equivariantrepresentations and test a simple GNN-based double-permutation equivariantneural architecture that achieve state-of-the-art Hits@10 test accuracy in theWN18RR, FB237 and NELL995 inductive KG completion tasks, and can accuratelyperform logical reasoning tasks that no existing methods can perform, to thebest of our knowledge.", "output": "Double Permutation Equivariance for Knowledge Graph Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning techniques are effective for building predictive modelsbecause they are good at identifying patterns in large datasets. Development ofa model for complex real life problems often stops at the point of publication,proof of concept or when made accessible through some mode of deployment.However, a model in the medical domain risks becoming obsolete as soon aspatient demographic changes. The maintenance and monitoring of predictivemodels post-publication is crucial to guarantee their safe and effective longterm use. As machine learning techniques are effectively trained to look forpatterns in available datasets, the performance of a model for complex reallife problems will not peak and remain fixed at the point of publication oreven point of deployment. Rather, data changes over time, and they also changedwhen models are transported to new places to be used by new demography.", "output": "Learning machines for health and beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the outstanding capability for data generation, Generative AdversarialNetworks (GANs) have attracted considerable attention in unsupervised learning.However, training GANs is difficult, since the training distribution is dynamicfor the discriminator, leading to unstable image representation. In this paper,we address the problem of training GANs from a novel perspective, emph{i.e.,}robust image classification. Motivated by studies on robust imagerepresentation, we propose a simple yet effective module, namely AdaptiveMix,for GANs, which shrinks the regions of training data in the imagerepresentation space of the discriminator. Considering it is intractable todirectly bound feature space, we propose to construct hard samples and narrowdown the feature distance between hard and easy samples. The hard samples areconstructed by mixing a pair of training images. We evaluate the effectivenessof our AdaptiveMix with widely-used and state-of-the-art GAN architectures. Theevaluation results demonstrate that our AdaptiveMix can facilitate the trainingof GANs and effectively improve the image quality of generated samples. We alsoshow that our AdaptiveMix can be further applied to image classification andOut-Of-Distribution (OOD) detection tasks, by equipping it withstate-of-the-art methods. Extensive experiments on seven publicly availabledatasets show that our method effectively boosts the performance of baselines.The code is publicly available at", "output": "Improving GAN Training via Feature Space Shrinkage."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to its complexity, graph learning-based multi-modal integration andclassification is one of the most challenging obstacles for disease prediction.To effectively offset the negative impact between modalities in the process ofmulti-modal integration and extract heterogeneous information from graphs, wepropose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning).For the problem of negative impact between modalities, we propose a multi-modalgraph embedding module to construct a multi-modal graph. Different fromconventional methods that manually construct static graphs for all modalities,each modality generates a separate graph by adaptive learning, where a functiongraph and a supervision graph are introduced for optimization during themulti-graph fusion embedding process. We then propose a multi-kernel graphlearning module to extract heterogeneous information from the multi-modalgraph. The information in the multi-modal graph at different levels isaggregated by convolutional kernels with different receptive field sizes,followed by generating a cross-kernel discovery tensor for disease prediction.Our method is evaluated on the benchmark Autism Brain Imaging Data Exchange(ABIDE) dataset and outperforms the state-of-the-art methods. In addition,discriminative brain regions associated with autism are identified by ourmodel, providing guidance for the study of autism pathology.", "output": "Multi-modal Multi-kernel Graph Learning for Autism Prediction and Biomarker Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human behavior is conditioned by codes and norms that constrain action.Rules, ``manners,'' laws, and moral imperatives are examples of classes ofconstraints that govern human behavior. These systems of constraints are``messy:'' individual constraints are often poorly defined, what constraintsare relevant in a particular situation may be unknown or ambiguous, constraintsinteract and conflict with one another, and determining how to act within thebounds of the relevant constraints may be a significant challenge, especiallywhen rapid decisions are needed. Despite such messiness, humans incorporateconstraints in their decisions robustly and rapidly. General,artificially-intelligent agents must also be able to navigate the messiness ofsystems of real-world constraints in order to behave predictability andreliably. In this paper, we characterize sources of complexity in constraintprocessing for general agents and describe a computational-level analysis forsuch textit{constraint compliance}. We identify key algorithmic requirementsbased on the computational-level analysis and outline an initial, exploratoryimplementation of a general approach to constraint compliance.", "output": "Computational-level Analysis of Constraint Compliance for General Intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The huge supporting training data on the Internet has been a key factor inthe success of deep learning models. However, this abundance ofpublic-available data also raises concerns about the unauthorized exploitationof datasets for commercial purposes, which is forbidden by dataset licenses. Inthis paper, we propose a backdoor-based watermarking approach that serves as ageneral framework for safeguarding public-available data. By inserting a smallnumber of watermarking samples into the dataset, our approach enables thelearning model to implicitly learn a secret function set by defenders. Thishidden function can then be used as a watermark to track down third-partymodels that use the dataset illegally. Unfortunately, existing backdoorinsertion methods often entail adding arbitrary and mislabeled data to thetraining set, leading to a significant drop in performance and easy detectionby anomaly detection algorithms. To overcome this challenge, we introduce aclean-label backdoor watermarking framework that uses imperceptibleperturbations to replace mislabeled samples. As a result, the watermarkingsamples remain consistent with the original labels, making them difficult todetect. Our experiments on text, image, and audio datasets demonstrate that theproposed framework effectively safeguards datasets with minimal impact onoriginal task performance. We also show that adding just 1% of watermarkingsamples can inject a traceable watermarking function and that our watermarkingsamples are stealthy and look benign upon visual inspection.", "output": "Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning (DL) can aid doctors in detecting worsening patient statesearly, affording them time to react and prevent bad outcomes. While DL-basedearly warning models usually work well in the hospitals they were trained for,they tend to be less reliable when applied at new hospitals. This makes itdifficult to deploy them at scale. Using carefully harmonised intensive caredata from four data sources across Europe and the US (totalling 334,812 stays),we systematically assessed the reliability of DL models for three commonadverse events: death, acute kidney injury (AKI), and sepsis. We tested whetherusing more than one data source and/or explicitly optimising forgeneralisability during training improves model performance at new hospitals.We found that models achieved high AUROC for mortality (0.838-0.869), AKI(0.823-0.866), and sepsis (0.749-0.824) at the training hospital. As expected,performance dropped at new hospitals, sometimes by as much as -0.200. Usingmore than one data source for training mitigated the performance drop, withmulti-source models performing roughly on par with the best single-sourcemodel. This suggests that as data from more hospitals become available fortraining, model robustness is likely to increase, lower-bounding robustnesswith the performance of the most applicable data source in the training data.Dedicated methods promoting generalisability did not noticeably improveperformance in our experiments.", "output": "From Single-Hospital to Multi-Centre Applications: Enhancing the Generalisability of Deep Learning Models for Adverse Event Prediction in the ICU."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present emph{TabRet}, a pre-trainable Transformer-based model for tabulardata. TabRet is designed to work on a downstream task that contains columns notseen in pre-training. Unlike other methods, TabRet has an extra learning stepbefore fine-tuning called emph{retokenizing}, which calibrates featureembeddings based on the masked autoencoding loss. In experiments, wepre-trained TabRet with a large collection of public health surveys andfine-tuned it on classification tasks in healthcare, and TabRet achieved thebest AUC performance on four datasets. In addition, an ablation study showsretokenizing and random shuffle augmentation of columns during pre-trainingcontributed to performance gains. The code is available at .", "output": "TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pretrained language models often generate outputs that are not in line withhuman preferences, such as harmful text or factually incorrect summaries.Recent work approaches the above issues by learning from a simple form of humanfeedback: comparisons between pairs of model-generated outputs. However,comparison feedback only conveys limited information about human preferences.In this paper, we introduce Imitation learning from Language Feedback (ILF), anew approach that utilizes more informative language feedback. ILF consists ofthree steps that are applied iteratively: first, conditioning the languagemodel on the input, an initial LM output, and feedback to generate refinements.Second, selecting the refinement incorporating the most feedback. Third,finetuning the language model to maximize the likelihood of the chosenrefinement given the input. We show theoretically that ILF can be viewed asBayesian Inference, similar to Reinforcement Learning from human feedback. Weevaluate ILF's effectiveness on a carefully-controlled toy task and a realisticsummarization task. Our experiments demonstrate that large language modelsaccurately incorporate feedback and that finetuning with ILF scales well withthe dataset size, even outperforming finetuning on human summaries. Learningfrom both language and comparison feedback outperforms learning from eachalone, achieving human-level summarization performance.", "output": "Training Language Models with Language Feedback at Scale."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language is essentially a complex, intricate system of human expressionsgoverned by grammatical rules. It poses a significant challenge to developcapable AI algorithms for comprehending and grasping a language. As a majorapproach, language modeling has been widely studied for language understandingand generation in the past two decades, evolving from statistical languagemodels to neural language models. Recently, pre-trained language models (PLMs)have been proposed by pre-training Transformer models over large-scale corpora,showing strong capabilities in solving various NLP tasks. Since researchershave found that model scaling can lead to performance improvement, they furtherstudy the scaling effect by increasing the model size to an even larger size.Interestingly, when the parameter scale exceeds a certain level, these enlargedlanguage models not only achieve a significant performance improvement but alsoshow some special abilities that are not present in small-scale languagemodels. To discriminate the difference in parameter scale, the researchcommunity has coined the term large language models (LLM) for the PLMs ofsignificant size. Recently, the research on LLMs has been largely advanced byboth academia and industry, and a remarkable progress is the launch of ChatGPT,which has attracted widespread attention from society. The technical evolutionof LLMs has been making an important impact on the entire AI community, whichwould revolutionize the way how we develop and use AI algorithms. In thissurvey, we review the recent advances of LLMs by introducing the background,key findings, and mainstream techniques. In particular, we focus on four majoraspects of LLMs, namely pre-training, adaptation tuning, utilization, andcapacity evaluation. Besides, we also summarize the available resources fordeveloping LLMs and discuss the remaining issues for future directions.", "output": "A Survey of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) are revolutionizing several areas of ArtificialIntelligence. One of the most remarkable applications is creative writing,e.g., poetry or storytelling: the generated outputs are often of astonishingquality. However, a natural question arises: can LLMs be really consideredcreative? In this article we firstly analyze the development of LLMs under thelens of creativity theories, investigating the key open questions andchallenges. Then, we discuss a set of \"easy\" and \"hard\" problems in machinecreativity, presenting them in relation to LLMs. Finally, we examine thesocietal impact of these technologies with a particular focus on the creativeindustries.", "output": "On the Creativity of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A backdoor attack allows a malicious user to manipulate the environment orcorrupt the training data, thus inserting a backdoor into the trained agent.Such attacks compromise the RL system's reliability, leading to potentiallycatastrophic results in various key fields. In contrast, relatively limitedresearch has investigated effective defenses against backdoor attacks in RL.This paper proposes the Recovery Triggered States (RTS) method, a novelapproach that effectively protects the victim agents from backdoor attacks. RTSinvolves building a surrogate network to approximate the dynamics model.Developers can then recover the environment from the triggered state to a cleanstate, thereby preventing attackers from activating backdoors hidden in theagent by presenting the trigger. When training the surrogate to predict states,we incorporate agent action information to reduce the discrepancy between theactions taken by the agent on predicted states and the actions taken on realstates. RTS is the first approach to defend against backdoor attacks in asingle-agent setting. Our results show that using RTS, the cumulative rewardonly decreased by 1.41% under the backdoor attack.", "output": "Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Scene Representation Transformer (SRT) is a recent method to render novelviews at interactive rates. Since SRT uses camera poses with respect to anarbitrarily chosen reference camera, it is not invariant to the order of theinput views. As a result, SRT is not directly applicable to large-scale sceneswhere the reference frame would need to be changed regularly. In this work, wepropose Relative Pose Attention SRT (RePAST): Instead of fixing a referenceframe at the input, we inject pairwise relative camera pose informationdirectly into the attention mechanism of the Transformers. This leads to amodel that is by definition invariant to the choice of any global referenceframe, while still retaining the full capabilities of the original method.Empirical results show that adding this invariance to the model does not leadto a loss in quality. We believe that this is a step towards applying fullylatent transformer-based rendering methods to large-scale scenes.", "output": "RePAST: Relative Pose Attention Scene Representation Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In response to innovations in machine learning (ML) models, productionworkloads changed radically and rapidly. TPU v4 is the fifth Google domainspecific architecture (DSA) and its third supercomputer for such ML models.Optical circuit switches (OCSes) dynamically reconfigure its interconnecttopology to improve scale, availability, utilization, modularity, deployment,security, power, and performance; users can pick a twisted 3D torus topology ifdesired. Much cheaper, lower power, and faster than Infiniband, OCSes andunderlying optical components are &lt;5% of system cost and &lt;3% of system power.Each TPU v4 includes SparseCores, dataflow processors that accelerate modelsthat rely on embeddings by 5x-7x yet use only 5% of die area and power.Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improvesperformance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chipsand thus ~10x faster overall, which along with OCS flexibility helps largelanguage models. For similar sized systems, it is ~4.3x-4.5x faster than theGraphcore IPU Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power thanthe Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computersof Google Cloud use ~3x less energy and produce ~20x less CO2e thancontemporary DSAs in a typical on-premise data center.", "output": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Skin lesion recognition using deep learning has made remarkable progress, andthere is an increasing need for deploying these systems in real-worldscenarios. However, recent research has revealed that deep neural networks forskin lesion recognition may overly depend on disease-irrelevant image artifacts(i.e. dark corners, dense hairs), leading to poor generalization in unseenenvironments. To address this issue, we propose a novel domain generalizationmethod called EPVT, which involves embedding prompts into the visiontransformer to collaboratively learn knowledge from diverse domains.Concretely, EPVT leverages a set of domain prompts, each of which plays as adomain expert, to capture domain-specific knowledge; and a shared prompt forgeneral knowledge over the entire dataset. To facilitate knowledge sharing andthe interaction of different prompts, we introduce a domain prompt generatorthat enables low-rank multiplicative updates between domain prompts and theshared prompt. A domain mixup strategy is additionally devised to reduce theco-occurring artifacts in each domain, which allows for more flexible decisionmargins and mitigates the issue of incorrectly assigned domain labels.Experiments on four out-of-distribution datasets and six different biased ISICdatasets demonstrate the superior generalization ability of EPVT in skin lesionrecognition across various environments. Our code and dataset will be releasedat ", "output": "EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data has growing significance in exploring cutting-edge materials, and thenumber of datasets has been generated either by hand or automated approaches.However, the materials science field struggles to effectively utilize theabundance of generated data, especially in applied disciplines where materialsare evaluated based on device performance rather than their properties. Thisarticle presents a new NLP task called structured information inference (SII)to address the complexities of information extraction at the device level inmaterials science. We accomplished this task by tuning GPT-3 on an existingperovskite solar cell FAIR (Findable, Accessible, Interoperable, Reusable)dataset with 91.8% F1-score and we updated the dataset with all relatedscientific papers up to now. The produced data is formatted and normalized,enabling its direct utilization as input in subsequent data analysis. Thisfeature will enable materials scientists to develop their own models byselecting high-quality review papers within their domain. Furthermore, wedesigned experiments to predict solar cells' electrical performance and designmaterials or devices with target parameters through LLM. We obtained comparableperformance with traditional machine learning methods without featureselection, demonstrating the potential of LLMs to learn scientific knowledgeand design new materials like a materials scientist.", "output": "Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Architectures Search (NAS) becomes more and more popular over theseyears. However, NAS-generated models tends to suffer greater vulnerability tovarious malicious attacks. Lots of robust NAS methods leverage adversarialtraining to enhance the robustness of NAS-generated models, however, theyneglected the nature accuracy of NAS-generated models. In our paper, we proposea novel NAS method, Robust Neural Architecture Search (RNAS). To design aregularization term to balance accuracy and robustness, RNAS generatesarchitectures with both high accuracy and good robustness. To reduce searchcost, we further propose to use noise examples instead adversarial examples asinput to search architectures. Extensive experiments show that RNAS achievesstate-of-the-art (SOTA) performance on both image classification andadversarial attacks, which illustrates the proposed RNAS achieves a goodtradeoff between robustness and accuracy.", "output": "Robust Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The retriever-reader framework is popular for open-domain question answering(ODQA), where a retriever samples for the reader a set of relevant candidatepassages from a large corpus. A key assumption behind this method is that highrelevance scores from the retriever likely indicate high answerability from thereader, which implies a high probability that the retrieved passages containanswers to a given question. In this work, we empirically dispel this beliefand observe that recent dense retrieval models based on DPR often rankunanswerable counterfactual passages higher than their answerable originalpassages. To address such answer-unawareness in dense retrievers, we seek touse counterfactual samples as additional training resources to bettersynchronize the relevance measurement of DPR with the answerability ofquestion-passage pairs. Specifically, we present counterfactually-PivotingContrastive Learning (PiCL), a novel representation learning approach forpassage retrieval that leverages counterfactual samples as pivots betweenpositive and negative samples in their learned embedding space. We incorporatePiCL into the retriever training to show the effectiveness of PiCL on ODQAbenchmarks and the robustness of the learned models.", "output": "Revisiting Dense Retrieval with Unanswerable Counterfactuals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, reinforcement learning (RL) has emerged as a popularapproach for solving sequence-based tasks in machine learning. However, findingsuitable alternatives to RL remains an exciting and innovative research area.One such alternative that has garnered attention is the Non-Axiomatic ReasoningSystem (NARS), which is a general-purpose cognitive reasoning framework. Inthis paper, we delve into the potential of NARS as a substitute for RL insolving sequence-based tasks. To investigate this, we conduct a comparativeanalysis of the performance of ONA as an implementation of NARS and$Q$-Learning in various environments that were created using the Open AI gym.The environments have different difficulty levels, ranging from simple tocomplex. Our results demonstrate that NARS is a promising alternative to RL,with competitive performance in diverse environments, particularly innon-deterministic ones.", "output": "Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The success of contextual word representations and advances in neuralinformation retrieval have made dense vector-based retrieval a standardapproach for passage and document ranking. While effective and efficient,dual-encoders are brittle to variations in query distributions and noisyqueries. Data augmentation can make models more robust but introduces overheadto training set generation and requires retraining and index regeneration. Wepresent Contrastive Alignment POst Training (CAPOT), a highly efficientfinetuning method that improves model robustness without requiring indexregeneration, the training set optimization, or alteration. CAPOT enablesrobust retrieval by freezing the document encoder while the query encoderlearns to align noisy queries with their unaltered root. We evaluate CAPOTnoisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,finding CAPOT has a similar impact as data augmentation with none of itsoverhead.", "output": "Noise-Robust Dense Retrieval via Contrastive Alignment Post Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of knowledge graph (KG) applications has led to a rising needfor entity alignment (EA) between heterogeneous KGs that are extracted fromvarious sources. Recently, graph neural networks (GNNs) have been widelyadopted in EA tasks due to GNNs' impressive ability to capture structureinformation. However, we have observed that the oversimplified settings of theexisting common EA datasets are distant from real-world scenarios, whichobstructs a full understanding of the advancements achieved by recent methods.This phenomenon makes us ponder: Do existing GNN-based EA methods really makegreat progress?In this paper, to study the performance of EA methods in realistic settings,we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGsand general KGs) which are different with regard to the scale and structure,and share fewer overlapping entities. First, we sweep the unreasonablesettings, and propose two new HHKG datasets that closely mimic real-world EAscenarios. Then, based on the proposed datasets, we conduct extensiveexperiments to evaluate previous representative EA methods, and revealinteresting findings about the progress of GNN-based EA methods. We find thatthe structural information becomes difficult to exploit but still valuable inaligning HHKGs. This phenomenon leads to inferior performance of existing EAmethods, especially GNN-based methods. Our findings shed light on the potentialproblems resulting from an impulsive application of GNN-based methods as apanacea for all EA datasets. Finally, we introduce a simple but effectivemethod: Simple-HHEA, which comprehensively utilizes entity name, structure, andtemporal information. Experiment results show Simple-HHEA outperforms previousmodels on HHKG datasets.", "output": "Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human movement analysis is a key area of research in robotics, biomechanics,and data science. It encompasses tracking, posture estimation, and movementsynthesis. While numerous methodologies have evolved over time, a systematicand quantitative evaluation of these approaches using verifiable ground truthdata of three-dimensional human movement is still required to define thecurrent state of the art. This paper presents seven datasets recorded usinginertial-based motion capture. The datasets contain professional gesturescarried out by industrial operators and skilled craftsmen performed in realconditions in-situ. The datasets were created with the intention of being usedfor research in human motion modeling, analysis, and generation. The protocolsfor data collection are described in detail, and a preliminary analysis of thecollected data is provided as a benchmark. The Gesture Operational Model, ahybrid stochastic-biomechanical approach based on kinematic descriptors, isutilized to model the dynamics of the experts' movements and createmathematical representations of their motion trajectories for analysis andquantifying their body dexterity. The models allowed accurate the generation ofhuman professional poses and an intuitive description of how body jointscooperate and change over time through the performance of the task.", "output": "Motion Capture Benchmark of Real Industrial Tasks and Traditional Crafts for Human Movement Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exploring the expected quantizing scheme with suitable mixed-precision policyis the key point to compress deep neural networks (DNNs) in high efficiency andaccuracy. This exploration implies heavy workloads for domain experts, and anautomatic compression method is needed. However, the huge search space of theautomatic method introduces plenty of computing budgets that make the automaticprocess challenging to be applied in real scenarios. In this paper, we proposean end-to-end framework named AutoQNN, for automatically quantizing differentlayers utilizing different schemes and bitwidths without any human labor.AutoQNN can seek desirable quantizing schemes and mixed-precision policies formainstream DNN models efficiently by involving three techniques: quantizingscheme search (QSS), quantizing precision learning (QPL), and quantizedarchitecture generation (QAG). QSS introduces five quantizing schemes anddefines three new schemes as a candidate set for scheme search, and then usesthe differentiable neural architecture search (DNAS) algorithm to seek thelayer- or model-desired scheme from the set. QPL is the first method to learnmixed-precision policies by reparameterizing the bitwidths of quantizingschemes, to the best of our knowledge. QPL optimizes both classification lossand precision loss of DNNs efficiently and obtains the relatively optimalmixed-precision model within limited model size and memory footprint. QAG isdesigned to convert arbitrary architectures into corresponding quantized oneswithout manual intervention, to facilitate end-to-end neural networkquantization. We have implemented AutoQNN and integrated it into Keras.Extensive experiments demonstrate that AutoQNN can consistently outperformstate-of-the-art quantization.", "output": "AutoQNN: An End-to-End Framework for Automatically Quantizing Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in generative artificial intelligence (AI) have capturedworldwide attention. Tools such as Dalle-2 and ChatGPT suggest that taskspreviously thought to be beyond the capabilities of AI may now augment theproductivity of creative media in various new ways, including through thegeneration of synthetic video. This research paper explores the utility ofusing AI-generated synthetic video to create viable educational content foronline educational settings. To date, there is limited research investigatingthe real-world educational value of AI-generated synthetic media. To addressthis gap, we examined the impact of using AI-generated synthetic video in anonline learning platform on both learners content acquisition and learningexperience. We took a mixed-method approach, randomly assigning adult learners(n=83) into one of two micro-learning conditions, collecting pre- andpost-learning assessments, and surveying participants on their learningexperience. The control condition included a traditionally produced instructorvideo, while the experimental condition included a synthetic video with arealistic AI-generated character. The results show that learners in bothconditions demonstrated significant improvement from pre- to post-learning(p&lt;.001), with no significant differences in gains between the two conditions(p=.80). In addition, no differences were observed in how learners perceivedthe traditional and synthetic videos. These findings suggest that AI-generatedsynthetic learning videos have the potential to be a viable substitute forvideos produced via traditional methods in online educational settings, makinghigh quality educational content more accessible across the globe.", "output": "Generative AI for learning: Investigating the potential of synthetic learning videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative modelling over continuous-time geometric constructs, a.k.a such ashandwriting, sketches, drawings etc., have been accomplished throughautoregressive distributions. Such strictly-ordered discrete factorizationhowever falls short of capturing key properties of chirographic data -- itfails to build holistic understanding of the temporal concept due to one-wayvisibility (causality). Consequently, temporal data has been modelled asdiscrete token sequences of fixed sampling rate instead of capturing the trueunderlying concept. In this paper, we introduce a powerful model-class namely\"Denoising Diffusion Probabilistic Models\" or DDPMs for chirographic data thatspecifically addresses these flaws. Our model named \"ChiroDiff\", beingnon-autoregressive, learns to capture holistic concepts and therefore remainsresilient to higher temporal sampling rate up to a good extent. Moreover, weshow that many important downstream utilities (e.g. conditional sampling,creative mixing) can be flexibly implemented using ChiroDiff. We further showsome unique use-cases like stochastic vectorization, de-noising/healing,abstraction are also possible with this model-class. We perform quantitativeand qualitative evaluation of our framework on relevant datasets and found itto be better or on par with competing approaches.", "output": "ChiroDiff: Modelling chirographic data with Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Privacy-preserving nerual network inference has been well studied whilehomomorphic CNN training still remains an open challenging task. In this paper,we present a practical solution to implement privacy-preserving CNN trainingbased on mere Homomorphic Encryption (HE) technique. To our best knowledge,this is the first attempt successfully to crack this nut and no work everbefore has achieved this goal. Several techniques combine to make it done: (1)with transfer learning, privacy-preserving CNN training can be reduced tohomomorphic neural network training, or even multiclass logistic regression(MLR) training; (2) via a faster gradient variant called $texttt{QuadraticGradient}$, an enhanced gradient method for MLR with a state-of-the-artperformance in converge speed is applied in this work to achieve highperformance; (3) we employ the thought of transformation in mathematics totransform approximating Softmax function in encryption domain to thewell-studied approximation of Sigmoid function. A new type of loss function isalongside been developed to complement this change; and (4) we use a simple butflexible matrix-encoding method named $texttt{Volley Revolver}$ to manage thedata flow in the ciphertexts, which is the key factor to complete the wholehomomorphic CNN training. The complete, runnable C++ code to implement our workcan be found at:  select $texttt{REGNET_X_400MF}$ as our pre-train model for usingtransfer learning. We use the first 128 MNIST training images as training dataand the whole MNIST testing dataset as the testing data. The client only needsto upload 6 ciphertexts to the cloud and it takes $sim 21$ mins to perform 2iterations on a cloud with 64 vCPUs, resulting in a precision of $21.49%$.", "output": "Privacy-Preserving CNN Training with Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate and reliable optical remote sensing image-based small-ship detectionis crucial for maritime surveillance systems, but existing methods oftenstruggle with balancing detection performance and computational complexity. Inthis paper, we propose a novel lightweight framework calledtextit{HSI-ShipDetectionNet} that is based on high-order spatial interactionsand is suitable for deployment on resource-limited platforms, such assatellites and unmanned aerial vehicles. HSI-ShipDetectionNet includes aprediction branch specifically for tiny ships and a lightweight hybridattention block for reduced complexity. Additionally, the use of a high-orderspatial interactions module improves advanced feature understanding andmodeling ability. Our model is evaluated using the public Kaggle marine shipdetection dataset and compared with multiple state-of-the-art models includingsmall object detection models, lightweight detection models, and ship detectionmodels. The results show that HSI-ShipDetectionNet outperforms the other modelsin terms of recall, and mean average precision (mAP) while being lightweightand suitable for deployment on resource-limited platforms.", "output": "High-order Spatial Interactions Enhanced Lightweight Model for Optical Remote Sensing Image-based Small Ship Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Widely adopted motion forecasting datasets substitute the observed sensoryinputs with higher-level abstractions such as 3D boxes and polylines. Thesesparse shapes are inferred through annotating the original scenes withperception systems' predictions. Such intermediate representations tie thequality of the motion forecasting models to the performance of computer visionmodels. Moreover, the human-designed explicit interfaces between perception andmotion forecasting typically pass only a subset of the semantic informationpresent in the original sensory input. To study the effect of these modularapproaches, design new paradigms that mitigate these limitations, andaccelerate the development of end-to-end motion forecasting models, we augmentthe Waymo Open Motion Dataset (WOMD) with large-scale, high-quality, diverseLiDAR data for the motion forecasting task.The new augmented dataset WOMD-LiDAR consists of over 100,000 scenes thateach spans 20 seconds, consisting of well-synchronized and calibrated highquality LiDAR point clouds captured across a range of urban and suburbangeographies ( Compared to Waymo OpenDataset (WOD), WOMD-LiDAR dataset contains 100x more scenes. Furthermore, weintegrate the LiDAR data into the motion forecasting model training and providea strong baseline. Experiments show that the LiDAR data brings improvement inthe motion forecasting task. We hope that WOMD-LiDAR will provide newopportunities for boosting end-to-end motion forecasting models.", "output": "WOMD-LiDAR: Raw Sensor Dataset Benchmark for Motion Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the success of deep-learning models in many tasks, there have beenconcerns about such models learning shortcuts, and their lack of robustness toirrelevant confounders. When it comes to models directly trained on humanfaces, a sensitive confounder is that of human identities. Many face-relatedtasks should ideally be identity-independent, and perform uniformly acrossdifferent individuals (i.e. be fair). One way to measure and enforce suchrobustness and performance uniformity is through enforcing it during training,assuming identity-related information is available at scale. However, due toprivacy concerns and also the cost of collecting such information, this isoften not the case, and most face datasets simply contain input images andtheir corresponding task-related labels. Thus, improving identity-relatedrobustness without the need for such annotations is of great importance. Here,we explore using face-recognition embedding vectors, as proxies for identities,to enforce such robustness. We propose to use the structure in theface-recognition embedding space, to implicitly emphasize rare samples withineach class. We do so by weighting samples according to their conditionalinverse density (CID) in the proxy embedding space. Our experiments suggestthat such a simple sample weighting scheme, not only improves the trainingrobustness, it often improves the overall performance as a result of suchrobustness. We also show that employing such constraints during trainingresults in models that are significantly less sensitive to different levels ofbias in the dataset.", "output": "Improving Identity-Robustness for Face Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Aiming at answering questions based on the content of remotely sensed images,visual question answering for remote sensing data (RSVQA) has attracted muchattention nowadays. However, previous works in RSVQA have focused little on therobustness of RSVQA. As we aim to enhance the reliability of RSVQA models, howto learn robust representations against new words and different questiontemplates with the same meaning is the key challenge. With the proposedaugmented dataset, we are able to obtain more questions in addition to theoriginal ones with the same meaning. To make better use of this information, inthis study, we propose a contrastive learning strategy for training robustRSVQA models against diverse question templates and words. Experimental resultsdemonstrate that the proposed augmented dataset is effective in improving therobustness of the RSVQA model. In addition, the contrastive learning strategyperforms well on the low resolution (LR) dataset.", "output": "Multilingual Augmentation for Robust Visual Question Answering in Remote Sensing Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Facial expression recognition (FER) algorithms work well in constrainedenvironments with little or no occlusion of the face. However, real-world faceocclusion is prevalent, most notably with the need to use a face mask in thecurrent Covid-19 scenario. While there are works on the problem of occlusion inFER, little has been done before on the particular face mask scenario.Moreover, the few works in this area largely use synthetically created maskedFER datasets. Motivated by these challenges posed by the pandemic to FER, wepresent a novel dataset, the Masked Student Dataset of Expressions or MSD-E,consisting of 1,960 real-world non-masked and masked facial expression imagescollected from 142 individuals. Along with the issue of obfuscated facialfeatures, we illustrate how other subtler issues in masked FER are representedin our dataset. We then provide baseline results using ResNet-18, finding thatits performance dips in the non-masked case when trained for FER in thepresence of masks. To tackle this, we test two training paradigms: contrastivelearning and knowledge distillation, and find that they increase the model'sperformance in the masked scenario while maintaining its non-maskedperformance. We further visualise our results using t-SNE plots and Grad-CAM,demonstrating that these paradigms capitalise on the limited features availablein the masked scenario. Finally, we benchmark SOTA methods on MSD-E.", "output": "Masked Student Dataset of Expressions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion-based models have achieved state-of-the-art performance ontext-to-image synthesis tasks. However, one critical limitation of these modelsis the low fidelity of generated images with respect to the text description,such as missing objects, mismatched attributes, and mislocated objects. One keyreason for such inconsistencies is the inaccurate cross-attention to text inboth the spatial dimension, which controls at what pixel region an objectshould appear, and the temporal dimension, which controls how different levelsof details are added through the denoising steps. In this paper, we propose anew text-to-image algorithm that adds explicit control over spatial-temporalcross-attention in diffusion models. We first utilize a layout predictor topredict the pixel regions for objects mentioned in the text. We then imposespatial attention control by combining the attention over the entire textdescription and that over the local description of the particular object in thecorresponding pixel region of that object. The temporal attention control isfurther added by allowing the combination weights to change at each denoisingstep, and the combination weights are optimized to ensure high fidelity betweenthe image and the text. Experiments show that our method generates images withhigher fidelity compared to diffusion-model-based baselines without fine-tuningthe diffusion model. Our code is publicly available at", "output": "Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Loop Closure Detection (LCD) is an essential component of visual simultaneouslocalization and mapping (SLAM) systems. It enables the recognition ofpreviously visited scenes to eliminate pose and map estimate drifts arisingfrom long-term exploration. However, current appearance-based LCD methods facesignificant challenges, including high computational costs, viewpoint variance,and dynamic objects in scenes. This paper introduces an online based onSuperpixel Grids (SGs) LCD approach, SGIDN-LCD, to find similarities betweenscenes via hand-crafted features extracted from SGs. Unlike traditionalBag-of-Words (BoW) models requiring pre-training, we propose an adaptivemechanism to group similar images called $textbf{textit{dynamic}}$$textbf{textit{node}}$, which incremental adjusts the database in an onlinemanner, allowing for efficient retrieval of previously viewed images.Experimental results demonstrate the SGIDN-LCD significantly improving LCDprecision-recall and efficiency. Moreover, our proposed overall LCD methodoutperforms state-of-the-art approaches on multiple typical datasets.", "output": "SGIDN-LCD: An Appearance-based Loop Closure Detection Algorithm using Superpixel Grids and Incremental Dynamic Nodes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of the convolutional neural network based prior in imaging inverseproblems has become increasingly popular. Current state-of-the-art methods,however, can easily result in severe overfitting, which makes a number of earlystopping techniques necessary to eliminate the overfitting problem. To motivateour work, we review some existing approaches to image priors. We find that thedeep image prior in combined with the handcrafted prior has an outstandingperformance in terms of interpretability and representability. We propose amulti-code deep image prior, a multiple latent codes variant of the deep imageprior, which can be utilized to eliminate overfitting and is also robust to thedifferent numbers of the latent codes. Due to the non-differentiability of thehandcrafted prior, we use the alternative direction method of multipliers(ADMM) algorithm. We compare the performance of the proposed method on an imagedenoising problem and a highly ill-posed CT reconstruction problem against theexisting state-of-the-art methods, including PnP-DIP, DIP-VBTV and ADMM DIP-WTVmethods. For the CelebA dataset denoising, we obtain 1.46 dB peak signal tonoise ratio improvement against all compared methods. For the CTreconstruction, the corresponding average improvement of three test images is4.3 dB over DIP, and 1.7 dB over ADMM DIP-WTV, and 1.2 dB over PnP-DIP alongwith a significant improvement in the structural similarity index.", "output": "Multi-code deep image prior based plug-and-play ADMM for image denoising and CT reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The internet gives the world an open platform to express their views andshare their stories. While this is very valuable, it makes fake news one of oursociety's most pressing problems. Manual fact checking process is timeconsuming, which makes it challenging to disprove misleading assertions beforethey cause significant harm. This is he driving interest in automatic fact orclaim verification. Some of the existing datasets aim to support development ofautomating fact-checking techniques, however, most of them are text based.Multi-modal fact verification has received relatively scant attention. In thispaper, we provide a multi-modal fact-checking dataset called FACTIFY 2,improving Factify 1 by using new data sources and adding satire articles.Factify 2 has 50,000 new data instances. Similar to FACTIFY 1.0, we have threebroad categories - support, no-evidence, and refute, with sub-categories basedon the entailment of visual and textual data. We also provide a BERT and VisonTransformer based baseline, which acheives 65% F1 score in the test set. Thebaseline codes and the dataset will be made available at", "output": "Factify 2: A Multimodal Fake News and Satire News Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a framework for efficient 3D clothed avatarreconstruction. By combining the advantages of the high accuracy ofoptimization-based methods and the efficiency of learning-based methods, wepropose a coarse-to-fine way to realize a high-fidelity clothed avatarreconstruction (CAR) from a single image. At the first stage, we use animplicit model to learn the general shape in the canonical space of a person ina learning-based way, and at the second stage, we refine the surface detail byestimating the non-rigid deformation in the posed space in an optimization way.A hyper-network is utilized to generate a good initialization so that theconvergence o f the optimization process is greatly accelerated. Extensiveexperiments on various datasets show that the proposed CAR successfullyproduces high-fidelity avatars for arbitrarily clothed humans in real scenes.", "output": "High-Fidelity Clothed Avatar Reconstruction from a Single Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Zero-shot video object segmentation (ZS-VOS) aims to segment foregroundobjects in a video sequence without prior knowledge of these objects. However,existing ZS-VOS methods often struggle to distinguish between foreground andbackground or to keep track of the foreground in complex scenarios. The commonpractice of introducing motion information, such as optical flow, can lead tooverreliance on optical flow estimation. To address these challenges, wepropose an encoder-decoder-based hierarchical co-attention propagation network(HCPN) capable of tracking and segmenting objects. Specifically, our model isbuilt upon multiple collaborative evolutions of the parallel co-attentionmodule (PCM) and the cross co-attention module (CCM). PCM captures commonforeground regions among adjacent appearance and motion features, while CCMfurther exploits and fuses cross-modal motion features returned by PCM. Ourmethod is progressively trained to achieve hierarchical spatio-temporal featurepropagation across the entire video. Experimental results demonstrate that ourHCPN outperforms all previous methods on public benchmarks, showcasing itseffectiveness for ZS-VOS.", "output": "Co-attention Propagation Network for Zero-Shot Video Object Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In deep learning, Multi-Layer Perceptrons (MLPs) have once again garneredattention from researchers. This paper introduces MC-MLP, a general MLP-likebackbone for computer vision that is composed of a series of fully-connected(FC) layers. In MC-MLP, we propose that the same semantic information hasvarying levels of difficulty in learning, depending on the coordinate frame offeatures. To address this, we perform an orthogonal transform on the featureinformation, equivalent to changing the coordinate frame of features. Throughthis design, MC-MLP is equipped with multi-coordinate frame receptive fieldsand the ability to learn information across different coordinate frames.Experiments demonstrate that MC-MLP outperforms most MLPs in imageclassification tasks, achieving better performance at the same parameter level.The code will be available at: ", "output": "MC-MLP:Multiple Coordinate Frames in all-MLP Architecture for Vision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Infrared thermography has been widely used in several domains to capture andmeasure temperature distributions across surfaces and objects. This methodologycan be further expanded to 3D applications if the spatial distribution of thetemperature distribution is available. Structure from Motion (SfM) is aphotometric range imaging technique that makes it possible to obtain 3Drenderings from a cloud of 2D images. To explore the possibility of 3Dreconstruction via SfM from infrared images, this article proposes aphotometric correction model for infrared sensors based on temperatureconstancy. Photometric correction is accomplished by estimating the sceneirradiance as the values from the solution to a differential equation formicrobolometer pixel excitation with unknown coefficients and initialconditions. The model was integrated into an SfM framework and experimentalevaluations demonstrate the contribution of the photometric correction forimproving the estimates of both the camera motion and the scene structure.Further, experiments show that the reconstruction quality from the correctedinfrared imagery achieves performance on par with state-of-the-artreconstruction using RGB sensors.", "output": "Photometric Correction for Infrared Sensors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning aims to efficiently learn from a non-stationary stream ofdata while avoiding forgetting the knowledge of old data. In many practicalapplications, data complies with non-Euclidean geometry. As such, the commonlyused Euclidean space cannot gracefully capture non-Euclidean geometricstructures of data, leading to inferior results. In this paper, we studycontinual learning from a novel perspective by exploring data geometry for thenon-stationary stream of data. Our method dynamically expands the geometry ofthe underlying space to match growing geometric structures induced by new data,and prevents forgetting by keeping geometric structures of old data intoaccount. In doing so, making use of the mixed curvature space, we propose anincremental search scheme, through which the growing geometric structures areencoded. Then, we introduce an angular-regularization loss and aneighbor-robustness loss to train the model, capable of penalizing the changeof global geometric structures and local geometric structures. Experiments showthat our method achieves better performance than baseline methods designed inEuclidean space.", "output": "Exploring Data Geometry for Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Adversarial Networks (GANs) have emerged as a significant playerin generative modeling by mapping lower-dimensional random noise tohigher-dimensional spaces. These networks have been used to generatehigh-resolution images and 3D objects. The efficient modeling of 3D objects andhuman faces is crucial in the development process of 3D graphical environmentssuch as games or simulations. 3D GANs are a new type of generative model usedfor 3D reconstruction, point cloud reconstruction, and 3D semantic scenecompletion. The choice of distribution for noise is critical as it representsthe latent space. Understanding a GAN's latent space is essential forfine-tuning the generated samples, as demonstrated by the morphing ofsemantically meaningful parts of images. In this work, we explore the latentspace and 3D GANs, examine several GAN variants and training methods to gaininsights into improving 3D GAN training, and suggest potential futuredirections for further research.", "output": "3D GANs and Latent Space: A comprehensive survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Normalizing flows (NFs) provide a powerful tool to construct an expressivedistribution by a sequence of trackable transformations of a base distributionand form a probabilistic model of underlying data. Rotation, as an importantquantity in computer vision, graphics, and robotics, can exhibit manyambiguities when occlusion and symmetry occur and thus demands suchprobabilistic models. Though much progress has been made for NFs in Euclideanspace, there are no effective normalizing flows without discontinuity ormany-to-one mapping tailored for SO(3) manifold. Given the unique non-Euclideanproperties of the rotation manifold, adapting the existing NFs to SO(3)manifold is non-trivial. In this paper, we propose a novel normalizing flow onSO(3) by combining a Mobius transformation-based coupling layer and aquaternion affine transformation. With our proposed rotation normalizing flows,one can not only effectively express arbitrary distributions on SO(3), but alsoconditionally build the target distribution given input observations. Extensiveexperiments show that our rotation normalizing flows significantly outperformthe baselines on both unconditional and conditional tasks.", "output": "Delving into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prenatal ultrasound imaging is the first-choice modality to assess fetalhealth. Medical image datasets for AI and ML methods must be diverse (i.e.diagnoses, diseases, pathologies, scanners, demographics, etc), however thereare few public ultrasound fetal imaging datasets due to insufficient amounts ofclinical data, patient privacy, rare occurrence of abnormalities in generalpractice, and limited experts for data collection and validation. To addresssuch data scarcity, we proposed generative adversarial networks (GAN)-basedmodels, diffusion-super-resolution-GAN and transformer-based-GAN, to synthesiseimages of fetal ultrasound brain planes from one public dataset. We reportedthat GAN-based methods can generate 256x256 pixel size of fetal ultrasoundtrans-cerebellum brain image plane with stable training losses, resulting inlower FID values for diffusion-super-resolution-GAN (average 7.04 and lower FID5.09 at epoch 10) than the FID values of transformer-based-GAN (average 36.02and lower 28.93 at epoch 60). The results of this work illustrate the potentialof GAN-based methods to synthesise realistic high-resolution ultrasound images,leading to future work with other fetal brain planes, anatomies, devices andthe need of a pool of experts to evaluate synthesised images. Code, data andother resources to reproduce this work are available aturl{", "output": "Towards Realistic Ultrasound Fetal Brain Imaging Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The observation and description of collective excitations in solids is afundamental issue when seeking to understand the physics of a many-body system.Analysis of these excitations is usually carried out by measuring the dynamicalstructure factor, S(Q, $omega$), with inelastic neutron or x-ray scatteringtechniques and comparing this against a calculated dynamical model. Here, wedevelop an artificial intelligence framework which combines a neural networktrained to mimic simulated data from a model Hamiltonian with automaticdifferentiation to recover unknown parameters from experimental data. Webenchmark this approach on a Linear Spin Wave Theory (LSWT) simulator andadvanced inelastic neutron scattering data from the square-lattice spin-1antiferromagnet La$_2$NiO$_4$. We find that the model predicts the unknownparameters with excellent agreement relative to analytical fitting. In doingso, we illustrate the ability to build and train a differentiable model onlyonce, which then can be applied in real-time to multi-dimensional scatteringdata, without the need for human-guided peak finding and fitting algorithms.This prototypical approach promises a new technology for this field toautomatically detect and refine more advanced models for ordered quantumsystems.", "output": "Capturing dynamical correlations using implicit neural representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To bring digital avatars into people's lives, it is highly demanded toefficiently generate complete, realistic, and animatable head avatars. Thistask is challenging, and it is difficult for existing methods to satisfy allthe requirements at once. To achieve these goals, we propose GANHead(Generative Animatable Neural Head Avatar), a novel generative head model thattakes advantages of both the fine-grained control over the explicit expressionparameters and the realistic rendering results of implicit representations.Specifically, GANHead represents coarse geometry, fine-gained details andtexture via three networks in canonical space to obtain the ability to generatecomplete and realistic head avatars. To achieve flexible animation, we definethe deformation filed by standard linear blend skinning (LBS), with the learnedcontinuous pose and expression bases and LBS weights. This allows the avatarsto be directly animated by FLAME parameters and generalize well to unseen posesand expressions. Compared to state-of-the-art (SOTA) methods, GANHead achievessuperior performance on head avatar generation and raw scan fitting.", "output": "GANHead: Towards Generative Animatable Neural Head Avatars."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cyber attacks has always been of a great concern. Websites and services withpoor security layers are the most vulnerable to such cyber attacks. Theattackers can easily access sensitive data like credit card details and socialsecurity number from such vulnerable services. Currently to stop cyber attacks,various different methods are opted from using two-step verification methodslike One-Time Password and push notification services to using high-endbio-metric devices like finger print reader and iris scanner are used assecurity layers. These current security measures carry a lot of cons and theworst is that user always need to carry the authentication device on them toaccess their data. To overcome this, we are proposing a technique of usingkeystroke dynamics (typing pattern) of a user to authenticate the genuine user.In the method, we are taking a data set of 51 users typing a password in 8sessions done on alternate days to record mood fluctuations of the user.Developed and implemented anomaly-detection algorithm based on distance metricsand machine learning algorithms like Artificial Neural networks (ANN) andconvolutional neural network (CNN) to classify the users. In ANN, weimplemented multi-class classification using 1-D convolution as the data wascorrelated and multi-class classification with negative class which was used toclassify anomaly based on all users put together. We were able to achieve anaccuracy of 95.05% using ANN with Negative Class. From the results achieved, wecan say that the model works perfectly and can be bought into the market as asecurity layer and a good alternative to two-step verification using externaldevices. This technique will enable users to have two-step security layerwithout worrying about carry an authentication device.", "output": "KeyDetect --Detection of anomalies and user based on Keystroke Dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anticipation problem has been studied considering different aspects such aspredicting humans' locations, predicting hands and objects trajectories, andforecasting actions and human-object interactions. In this paper, we studiedthe short-term object interaction anticipation problem from the egocentricpoint of view, proposing a new end-to-end architecture named StillFast. Ourapproach simultaneously processes a still image and a video detecting andlocalizing next-active objects, predicting the verb which describes the futureinteraction and determining when the interaction will start. Experiments on thelarge-scale egocentric dataset EGO4D show that our method outperformedstate-of-the-art approaches on the considered task. Our method is ranked firstin the public leaderboard of the EGO4D short term object interactionanticipation challenge 2022. Please see the project web page for code andadditional details: ", "output": "StillFast: An End-to-End Approach for Short-Term Object Interaction Anticipation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, self-supervised learning (SSL) has achieved tremendous success inlearning image representation. Despite the empirical success, mostself-supervised learning methods are rather \"inefficient\" learners, typicallytaking hundreds of training epochs to fully converge. In this work, we showthat the key towards efficient self-supervised learning is to increase thenumber of crops from each image instance. Leveraging one of thestate-of-the-art SSL method, we introduce a simplistic form of self-supervisedlearning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL)that does not rely on many heuristic techniques for SSL such as weight sharingbetween the branches, feature-wise normalization, output quantization, and stopgradient, etc, and reduces the training epochs by two orders of magnitude. Weshow that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5%on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just oneepoch. Furthermore, the proposed method achieves 91.5% on CIFAR-10, 70.1% onCIFAR-100, 51.5% on Tiny ImageNet and 78.9% on ImageNet-100 with linear probingin less than ten training epochs. In addition, we show that EMP-SSL showssignificantly better transferability to out-of-domain datasets compared tobaseline SSL methods. We will release the code in", "output": "EMP-SSL: Towards Self-Supervised Learning in One Training Epoch."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "During the last few years, continual learning (CL) strategies for imageclassification and segmentation have been widely investigated designinginnovative solutions to tackle catastrophic forgetting, like knowledgedistillation and self-inpainting. However, the application of continuallearning paradigms to point clouds is still unexplored and investigation isrequired, especially using architectures that capture the sparsity and unevendistribution of LiDAR data. The current paper analyzes the problem of classincremental learning applied to point cloud semantic segmentation, comparingapproaches and state-of-the-art architectures. To the best of our knowledge,this is the first example of class-incremental continual learning for LiDARpoint cloud semantic segmentation. Different CL strategies were adapted toLiDAR point clouds and tested, tackling both classic fine-tuning scenarios andthe Coarse-to-Fine learning paradigm. The framework has been evaluated throughtwo different architectures on SemanticKITTI, obtaining results in line withstate-of-the-art CL strategies and standard offline learning.", "output": "Continual Learning for LiDAR Semantic Segmentation: Class-Incremental and Coarse-to-Fine strategies on Sparse Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Failure to recognize samples from the classes unseen during training is amajor limit of artificial intelligence (AI) in real-world implementation ofretinal anomaly classification. To resolve this obstacle, we propose anuncertainty-inspired open-set (UIOS) model which was trained with fundus imagesof 9 common retinal conditions. Besides the probability of each category, UIOSalso calculates an uncertainty score to express its confidence. Our UIOS modelwith thresholding strategy achieved an F1 score of 99.55%, 97.01% and 91.91%for the internal testing set, external testing set and non-typical testing set,respectively, compared to the F1 score of 92.20%, 80.69% and 64.74% by thestandard AI model. Furthermore, UIOS correctly predicted high uncertaintyscores, which prompted the need for a manual check, in the datasets of rareretinal diseases, low-quality fundus images, and non-fundus images. This workprovides a robust method for real-world screening of retinal anomalies.", "output": "Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing dehazing approaches struggle to process real-world hazy images owingto the lack of paired real data and robust priors. In this work, we present anew paradigm for real image dehazing from the perspectives of synthesizing morerealistic hazy data and introducing more robust priors into the network.Specifically, (1) instead of adopting the de facto physical scattering model,we rethink the degradation of real hazy images and propose a phenomenologicalpipeline considering diverse degradation types. (2) We propose a Real ImageDehazing network via high-quality Codebook Priors (RIDCP). Firstly, a VQGAN ispre-trained on a large-scale high-quality dataset to obtain the discretecodebook, encapsulating high-quality priors (HQPs). After replacing thenegative effects brought by haze with HQPs, the decoder equipped with a novelnormalized feature alignment module can effectively utilize high-qualityfeatures and produce clean results. However, although our degradation pipelinedrastically mitigates the domain gap between synthetic and real data, it isstill intractable to avoid it, which challenges HQPs matching in the wild.Thus, we re-calculate the distance when matching the features to the HQPs by acontrollable matching operation, which facilitates finding better counterparts.We provide a recommendation to control the matching based on an explainablesolution. Users can also flexibly adjust the enhancement degree as per theirpreference. Extensive experiments verify the effectiveness of our datasynthesis pipeline and the superior performance of RIDCP in real imagedehazing.", "output": "RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the training process of the implicit 3D reconstruction network, the choiceof spatial query points' sampling strategy affects the final performance of themodel. Different works have differences in the selection of samplingstrategies, not only in the spatial distribution of query points but also inthe order of magnitude difference in the density of query points. For how toselect the sampling strategy of query points, current works are more akin to anenumerating operation to find the optimal solution, which seriously affectswork efficiency. In this work, we explored the relationship between samplingstrategy and network final performance through classification analysis andexperimental comparison from three aspects: the relationship between networktype and sampling strategy, the relationship between implicit function andsampling strategy, and the impact of sampling density on model performance. Inaddition, we also proposed two methods, linear sampling and distance masking,to improve the sampling strategy of query points, making it more robust.", "output": "Analysis of Sampling Strategies for Implicit 3D Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Fields (NeRF) have been widely adopted as practical andversatile representations for 3D scenes, facilitating various downstream tasks.However, different architectures, including plain Multi-Layer Perceptron (MLP),Tensors, low-rank Tensors, Hashtables, and their compositions, have theirtrade-offs. For instance, Hashtables-based representations allow for fasterrendering but lack clear geometric meaning, making spatial-relation-awareediting challenging. To address this limitation and maximize the potential ofeach architecture, we propose Progressive Volume Distillation with ActiveLearning (PVD-AL), a systematic distillation method that enables any-to-anyconversions between different architectures. PVD-AL decomposes each structureinto two parts and progressively performs distillation from shallower to deepervolume representation, leveraging effective information retrieved from therendering process. Additionally, a Three-Levels of active learning techniqueprovides continuous feedback during the distillation process, resulting inhigh-performance results. Empirical evidence is presented to validate ourmethod on multiple benchmark datasets. For example, PVD-AL can distill anMLP-based model from a Hashtables-based model at a 10~20X faster speed and0.8dB~2dB higher PSNR than training the NeRF model from scratch. Moreover,PVD-AL permits the fusion of diverse features among distinct structures,enabling models with multiple editing properties and providing a more efficientmodel to meet real-time requirements. Project website:<a href=\" http URL</a>", "output": "PVD-AL: Progressive Volume Distillation with Active Learning for Efficient Conversion Between Different NeRF Architectures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As deep learning advances, edge devices and lightweight neural networks arebecoming more important. To reduce latency in the AI accelerator, it'sessential to not only reduce FLOPs but also enhance hardware performance. Weproposed an arithmetic intensity balancing convolution (ABConv) to address theissue of the overall intensity being limited by the small weight arithmeticintensity for convolution with a small spatial size. ABConv increased themaximum bound of overall arithmetic intensity and significantly reducedlatency, without sacrificing accuracy. We tested the latency and hardwareperformance of ABConv on the Arm Ethos-U65 NPU in various configurations andused it to replace some of MobileNetV1 and ResNet50 in image classification forCIFAR100.", "output": "Arithmetic Intensity Balancing Convolution for Hardware-aware Efficient Block Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Portrait retouching aims to improve the aesthetic quality of input portraitphotos and especially requires human-region priority. pink{The deeplearning-based methods largely elevate the retouching efficiency and providepromising retouched results. However, existing portrait retouching methodsfocus on automatic retouching, which treats all human-regions equally andignores users' preferences for specific individuals,} thus suffering fromlimited flexibility in interactive scenarios. In this work, we emphasize theimportance of users' intents and explore the interactive portrait retouchingtask. Specifically, we propose a region-aware retouching framework with twobranches: an automatic branch and an interactive branch. pink{The automaticbranch involves an encoding-decoding process, which searches region candidatesand performs automatic region-aware retouching without user guidance. Theinteractive branch encodes sparse user guidance into a priority conditionvector and modulates latent features with a region selection module to furtheremphasize the user-specified regions. Experimental results show that ourinteractive branch effectively captures users' intents and generalizes well tounseen scenes with sparse user guidance, while our automatic branch alsooutperforms the state-of-the-art retouching methods due to improvedregion-awareness.}", "output": "Region-Aware Portrait Retouching with Sparse Interactive Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive learning, relying on effective positive and negative samplepairs, is beneficial to learn informative skeleton representations inunsupervised skeleton-based action recognition. To achieve these positive andnegative pairs, existing weak/strong data augmentation methods have to randomlychange the appearance of skeletons for indirectly pursuing semanticperturbations. However, such approaches have two limitations: 1) solelyperturbing appearance cannot well capture the intrinsic semantic information ofskeletons, and 2) randomly perturbation may change the originalpositive/negative pairs to soft positive/negative ones. To address the abovedilemma, we start the first attempt to explore an attack-based augmentationscheme that additionally brings in direct semantic perturbation, forconstructing hard positive pairs and further assisting in constructing hardnegative pairs. In particular, we propose a novel Attack-AugmentationMixing-Contrastive learning (A$^2$MC) to contrast hard positive features andhard negative features for learning more robust skeleton representations. InA$^2$MC, Attack-Augmentation (Att-Aug) is designed to collaboratively performtargeted and untargeted perturbations of skeletons via attack and augmentationrespectively, for generating high-quality hard positive features. Meanwhile,Positive-Negative Mixer (PNM) is presented to mix hard positive features andnegative features for generating hard negative features, which are adopted forupdating the mixed memory banks. Extensive experiments on three public datasetsdemonstrate that A$^2$MC is competitive with the state-of-the-art methods.", "output": "Attack is Good Augmentation: Towards Skeleton-Contrastive Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modalityfor dental examination. However, its applicability is limited as compared to 3DCone-beam computed tomography (CBCT), because PX only provides 2D flattenedimages of the oral structure. In this paper, we propose a new framework whichestimates 3D oral structure from real-world PX images. Since there are not manymatching PX and CBCT data, we used simulated PX from CBCT for training,however, we used real-world panoramic radiographs at the inference time. Wepropose a new ray-sampling method to make simulated panoramic radiographsinspired by the principle of panoramic radiography along with the renderingfunction derived from the Beer-Lambert law. Our model consists of three parts:translation module, generation module, and refinement module. The translationmodule changes the real-world panoramic radiograph to the simulated trainingimage style. The generation module makes the 3D structure from the input imagewithout any prior information such as a dental arch. Our ray-based generationapproach makes it possible to reverse the process of generating PX from oralstructure in order to reconstruct CBCT data. Lastly, the refinement moduleenhances the quality of the 3D output. Results show that our approach worksbetter for simulated and real-world images compared to other state-of-the-artmethods.", "output": "NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We offer a study that connects robust discriminative classifiers trained withadversarial training (AT) with generative modeling in the form of Energy-basedModels (EBM). We do so by decomposing the loss of a discriminative classifierand showing that the discriminative model is also aware of the input datadensity. Though a common assumption is that adversarial points leave themanifold of the input data, our study finds out that, surprisingly, untargetedadversarial points in the input space are very likely under the generativemodel hidden inside the discriminative classifier -- have low energy in theEBM. We present two evidence: untargeted attacks are even more likely than thenatural data and their likelihood increases as the attack strength increases.This allows us to easily detect them and craft a novel attack calledHigh-Energy PGD that fools the classifier yet has energy similar to the dataset.", "output": "Exploring the Connection between Robust and Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Enable neural networks to capture 3D geometrical-aware features is essentialin multi-view based vision tasks. Previous methods usually encode the 3Dinformation of multi-view stereo into the 2D features. In contrast, we presenta novel method, named POEM, that directly operates on the 3D POints Embedded inthe Multi-view stereo for reconstructing hand mesh in it. Point is a naturalform of 3D information and an ideal medium for fusing features across views, asit has different projections on different views. Our method is thus in light ofa simple yet effective idea, that a complex 3D hand mesh can be represented bya set of 3D points that 1) are embedded in the multi-view stereo, 2) carryfeatures from the multi-view images, and 3) encircle the hand. To leverage thepower of points, we design two operations: point-based feature fusion andcross-set point attention mechanism. Evaluation on three challenging multi-viewdatasets shows that POEM outperforms the state-of-the-art in hand meshreconstruction. Code and models are available for research at", "output": "POEM: Reconstructing Hand in a Point Embedded Multi-view Stereo."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In geospatial planning, it is often essential to represent objects in avectorized format, as this format easily translates to downstream tasks such asweb development, graphics, or design. While these problems are frequentlyaddressed using semantic segmentation, which requires additionalpost-processing to vectorize objects in a non-trivial way, we present anImage-to-Sequence model that allows for direct shape inference and is ready forvector-based workflows out of the box. We demonstrate the model's performancein various ways, including perturbations to the image input that correspond tovariations or artifacts commonly encountered in remote sensing applications.Our model outperforms prior works when using ground truth bounding boxes (oneobject per image), achieving the lowest maximum tangent angle error.", "output": "Polygonizer: An auto-regressive building delineator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-supervised learning (SSL) has attracted much attention since it reducesthe expensive costs of collecting adequate well-labeled training data,especially for deep learning methods. However, traditional SSL is built upon anassumption that labeled and unlabeled data should be from the same distributione.g., classes and domains. However, in practical scenarios, unlabeled datawould be from unseen classes or unseen domains, and it is still challenging toexploit them by existing SSL methods. Therefore, in this paper, we proposed aunified framework to leverage these unseen unlabeled data for open-scenariosemi-supervised medical image classification. We first design a novel scoringmechanism, called dual-path outliers estimation, to identify samples fromunseen classes. Meanwhile, to extract unseen-domain samples, we then apply aneffective variational autoencoder (VAE) pre-training. After that, we conductdomain adaptation to fully exploit the value of the detected unseen-domainsamples to boost semi-supervised training. We evaluated our proposed frameworkon dermatology and ophthalmology tasks. Extensive experiments demonstrate ourmodel can achieve superior classification performance in various medical SSLscenarios.", "output": "Towards Open-Scenario Semi-supervised Medical Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we emphasize the integration of a pre-trained MICA model withan imperfect face dataset, employing a self-supervised learning approach. Wepresent an innovative method for regenerating flawed facial structures,yielding 3D printable outputs that effectively support physicians in theirpatient treatment process. Our results highlight the model's capacity forconcealing scars and achieving comprehensive facial reconstructions withoutdiscernible scarring. By capitalizing on pre-trained models and necessitatingonly a few hours of supplementary training, our methodology adeptly devises anoptimal model for reconstructing damaged and imperfect facial features.Harnessing contemporary 3D printing technology, we institute a standardizedprotocol for fabricating realistic, camouflaging mask models for patients in alaboratory environment.", "output": "Application of Self-Supervised Learning to MICA Model for Reconstructing Imperfect 3D Facial Structures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lip-reading has made impressive progress in recent years, driven by advancesin deep learning. Nonetheless, the prerequisite such advances is a suitabledataset. This paper provides a new in-the-wild dataset for Persian word-levellipreading containing 244,000 videos from approximately 1,800 speakers. Weevaluated the state-of-the-art method in this field and used a novel approachfor word-level lip-reading. In this method, we used the AV-HuBERT model forfeature extraction and obtained significantly better performance on ourdataset.", "output": "Word-level Persian Lipreading Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Identifying the type of kidney stones can allow urologists to determine theircause of formation, improving the prescription of appropriate treatments todiminish future relapses. Currently, the associated ex-vivo diagnosis (known asMorpho-constitutional Analysis, MCA) is time-consuming, expensive and requiresa great deal of experience, as it requires a visual analysis component that ishighly operator dependant. Recently, machine learning methods have beendeveloped for in-vivo endoscopic stone recognition. Deep Learning (DL) basedmethods outperform non-DL methods in terms of accuracy but lack explainability.Despite this trade-off, when it comes to making high-stakes decisions, it'simportant to prioritize understandable Computer-Aided Diagnosis (CADx) thatsuggests a course of action based on reasonable evidence, rather than a modelprescribing a course of action. In this proposal, we learn Prototypical Parts(PPs) per kidney stone subtype, which are used by the DL model to generate anoutput classification. Using PPs in the classification task enables case-basedreasoning explanations for such output, thus making the model interpretable. Inaddition, we modify global visual characteristics to describe their relevanceto the PPs and the sensitivity of our model's performance. With this, weprovide explanations with additional information at the sample, class and modellevels in contrast to previous works. Although our implementation's averageaccuracy is lower than state-of-the-art (SOTA) non-interpretable DL models by1.5 %, our models perform 2.8% better on perturbed images with a lower standarddeviation, without adversarial training. Thus, Learning PPs has the potentialto create more robust DL models.", "output": "Deep Prototypical-Parts Ease Morphological Kidney Stone Identification and are Competitively Robust to Photometric Perturbations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Acquiring and annotating sufficient labeled data is crucial in developingaccurate and robust learning-based models, but obtaining such data can bechallenging in many medical image segmentation tasks. One promising solution isto synthesize realistic data with ground-truth mask annotations. However, noprior studies have explored generating complete 3D volumetric images withmasks. In this paper, we present MedGen3D, a deep generative framework that cangenerate paired 3D medical images and masks. First, we represent the 3D medicaldata as 2D sequences and propose the Multi-Condition Diffusion ProbabilisticModel (MC-DPM) to generate multi-label mask sequences adhering to anatomicalgeometry. Then, we use an image sequence generator and semantic diffusionrefiner conditioned on the generated mask sequences to produce realistic 3Dmedical images that align with the generated masks. Our proposed frameworkguarantees accurate alignment between synthetic images and segmentation maps.Experiments on 3D thoracic CT and brain MRI datasets show that our syntheticdata is both diverse and faithful to the original data, and demonstrate thebenefits for downstream segmentation tasks. We anticipate that MedGen3D'sability to synthesize paired 3D medical images and masks will prove valuable intraining deep learning models for medical imaging tasks.", "output": "MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents a study on label noise in medical image segmentation byconsidering a noise model based on Gaussian field deformations. Such noise isof interest because it yields realistic looking segmentations and because it isunbiased in the sense that the expected deformation is the identity mapping.Efficient methods for sampling and closed form solutions for the marginalprobabilities are provided. Moreover, theoretically optimal solutions to theloss functions cross-entropy and soft-Dice are studied and it is shown how theydiverge as the level of noise increases. Based on recent work on loss functioncharacterization, it is shown that optimal solutions to soft-Dice can berecovered by thresholding solutions to cross-entropy with a particular a prioriunknown threshold that efficiently can be computed. This raises the questionwhether the decrease in performance seen when using cross-entropy as comparedto soft-Dice is caused by using the wrong threshold. The hypothesis isvalidated in 5-fold studies on three organ segmentation problems from theTotalSegmentor data set, using 4 different strengths of noise. The results showthat changing the threshold leads the performance of cross-entropy to go fromsystematically worse than soft-Dice to similar or better results thansoft-Dice.", "output": "Marginal Thresholding in Noisy Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Sat-NeRF, a modified implementation of the recently introducedShadow Neural Radiance Field (S-NeRF) model. This method is able to synthesizenovel views from a sparse set of satellite images of a scene, while accountingfor the variation in lighting present in the pictures. The trained model canalso be used to accurately estimate the surface elevation of the scene, whichis often a desirable quantity for satellite observation applications. S-NeRFimproves on the standard Neural Radiance Field (NeRF) method by considering theradiance as a function of the albedo and the irradiance. Both these quantitiesare output by fully connected neural network branches of the model, and thelatter is considered as a function of the direct light from the sun and thediffuse color from the sky. The implementations were run on a dataset ofsatellite images, augmented using a zoom-and-crop technique. A hyperparameterstudy for NeRF was carried out, leading to intriguing observations on themodel's convergence. Finally, both NeRF and S-NeRF were run until 100k epochsin order to fully fit the data and produce their best possible predictions. Thecode related to this article can be found at", "output": "NeRF applied to satellite imagery for surface reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of deep long-tailed learning, a prevalent challenge in the realmof generic visual recognition, persists in a multitude of real-worldapplications. To tackle the heavily-skewed dataset issue in long-tailedclassification, prior efforts have sought to augment existing deep models withthe elaborate class-balancing strategies, such as class rebalancing, dataaugmentation, and module improvement. Despite the encouraging performance, thelimited class knowledge of the tailed classes in the training dataset stillbottlenecks the performance of the existing deep models. In this paper, wepropose an innovative long-tailed learning paradigm that breaks the bottleneckby guiding the learning of deep networks with external prior knowledge. This isspecifically achieved by devising an elaborated ``prophetic'' teacher, termedas ``Propheter'', that aims to learn the potential class distributions. Thetarget long-tailed prediction model is then optimized under the instruction ofthe well-trained ``Propheter'', such that the distributions of differentclasses are as distinguishable as possible from each other. Experiments oneight long-tailed benchmarks across three architectures demonstrate that theproposed prophetic paradigm acts as a promising solution to the challenge oflimited class knowledge in long-tailed datasets. Our code and model can befound in the supplementary material.", "output": "Propheter: Prophetic Teacher Guided Long-Tailed Distribution Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents Scalable Semantic Transfer (SST), a novel trainingparadigm, to explore how to leverage the mutual benefits of the data fromdifferent label domains (i.e. various levels of label granularity) to train apowerful human parsing network. In practice, two common application scenariosare addressed, termed universal parsing and dedicated parsing, where the formeraims to learn homogeneous human representations from multiple label domains andswitch predictions by only using different segmentation heads, and the latteraims to learn a specific domain prediction while distilling the semanticknowledge from other domains. The proposed SST has the following appealingbenefits: (1) it can capably serve as an effective training scheme to embedsemantic associations of human body parts from multiple label domains into thehuman representation learning process; (2) it is an extensible semantictransfer framework without predetermining the overall relations of multiplelabel domains, which allows continuously adding human parsing datasets topromote the training. (3) the relevant modules are only used for auxiliarytraining and can be removed during inference, eliminating the extra reasoningcost. Experimental results demonstrate SST can effectively achieve promisinguniversal human parsing performance as well as impressive improvements comparedto its counterparts on three human parsing benchmarks (i.e.,PASCAL-Person-Part, ATR, and CIHP). Code is available at", "output": "Semantic Human Parsing via Scalable Semantic Transfer over Multiple Label Domains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning methods have emerged as powerful tools for analyzinghistopathological images, but current methods are often specialized forspecific domains and software environments, and few open-source options existfor deploying models in an interactive interface. Experimenting with differentdeep learning approaches typically requires switching software libraries andreprocessing data, reducing the feasibility and practicality of experimentingwith new architectures. We developed a flexible deep learning library forhistopathology called Slideflow, a package which supports a broad array of deeplearning methods for digital pathology and includes a fast whole-slideinterface for deploying trained models. Slideflow includes unique tools forwhole-slide image data processing, efficient stain normalization andaugmentation, weakly-supervised whole-slide classification, uncertaintyquantification, feature generation, feature space analysis, and explainability.Whole-slide image processing is highly optimized, enabling whole-slide tileextraction at 40X magnification in 2.5 seconds per slide. Theframework-agnostic data processing pipeline enables rapid experimentation withnew methods built with either Tensorflow or PyTorch, and the graphical userinterface supports real-time visualization of slides, predictions, heatmaps,and feature space characteristics on a variety of hardware devices, includingARM-based devices such as the Raspberry Pi.", "output": "Slideflow: Deep Learning for Digital Histopathology with Real-Time Whole-Slide Visualization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The segment anything model (SAM) was released as a foundation model for imagesegmentation. The promptable segmentation model was trained by over 1 billionmasks on 11M licensed and privacy-respecting images. The model supportszero-shot image segmentation with various segmentation prompts (e.g., points,boxes, masks). It makes the SAM attractive for medical image analysis,especially for digital pathology where the training data are rare. In thisstudy, we evaluate the zero-shot segmentation performance of SAM model onrepresentative segmentation tasks on whole slide imaging (WSI), including (1)tumor segmentation, (2) non-tumor tissue segmentation, (3) cell nucleisegmentation. Core Results: The results suggest that the zero-shot SAM modelachieves remarkable segmentation performance for large connected objects.However, it does not consistently achieve satisfying performance for denseinstance object segmentation, even with 20 prompts (clicks/boxes) on eachimage. We also summarized the identified limitations for digital pathology: (1)image resolution, (2) multiple scales, (3) prompt selection, and (4) modelfine-tuning. In the future, the few-shot fine-tuning with images fromdownstream pathological segmentation tasks might help the model to achievebetter performance in dense object segmentation.", "output": "Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the development of deep generative models, recent years have seen greatsuccess of Chinese landscape painting generation. However, few works focus oncontrollable Chinese landscape painting generation due to the lack of data andlimited modeling capabilities. In this work, we propose a controllable Chineselandscape painting generation method named CCLAP, which can generate paintingwith specific content and style based on Latent Diffusion Model. Specifically,it consists of two cascaded modules, i.e., content generator and styleaggregator. The content generator module guarantees the content of generatedpaintings specific to the input text. While the style aggregator module is togenerate paintings of a style corresponding to a reference image. Moreover, anew dataset of Chinese landscape paintings named CLAP is collected forcomprehensive evaluation. Both the qualitative and quantitative resultsdemonstrate that our method achieves state-of-the-art performance, especiallyin artfully-composed and artistic conception. Codes are available at", "output": "CCLAP: Controllable Chinese Landscape Painting Generation via Latent Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "COVID19 is a highly contagious disease infected millions of people worldwide.With limited testing components, screening tools such as chest radiography canassist the clinicians in the diagnosis and assessing the progress of disease.The performance of deep learning-based systems for diagnosis of COVID-19disease in radiograph images has been encouraging. This paper investigates theconcept of transfer learning using two of the most well-known VGGNetarchitectures, namely VGG-16 and VGG-19. The classifier block andhyperparameters are fine-tuned to adopt the models for automatic detection ofCovid-19 in chest x-ray images. We generated two different datasets to evaluatethe performance of the proposed system for the identification of positiveCovid-19 instances in a multiclass and binary classification problems. Theexperimental outcome demonstrates the usefulness of transfer learning forsmall-sized datasets particularly in the field of medical imaging, not only toprevent over-fitting and convergence problems but also to attain optimalclassification performance as well.", "output": "Detection of COVID19 in Chest X-Ray Images Using Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning with large-scale unlabeled data has become a powerful tool forpre-training Visual Transformers (VTs). However, prior works tend to overlookthat, in real-world scenarios, the input data may be corrupted and unreliable.Pre-training VTs on such corrupted data can be challenging, especially when wepre-train via the masked autoencoding approach, where both the inputs andmasked ``ground truth\" targets can potentially be unreliable in this case. Toaddress this limitation, we introduce the Token Boosting Module (TBM) as aplug-and-play component for VTs that effectively allows the VT to learn toextract clean and robust features during masked autoencoding pre-training. Weprovide theoretical analysis to show how TBM improves model pre-training withmore robust and generalizable representations, thus benefiting downstreamtasks. We conduct extensive experiments to analyze TBM's effectiveness, andresults on four corrupted datasets demonstrate that TBM consistently improvesperformance on downstream tasks.", "output": "Token Boosting for Robust Self-Supervised Visual Transformer Pre-training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the prevalence of multimodal learning, camera-LiDAR fusion has gainedpopularity in 3D object detection. Although multiple fusion approaches havebeen proposed, they can be classified into either sparse-only or dense-onlyfashion based on the feature representation in the fusion module. In thispaper, we analyze them in a common taxonomy and thereafter observe twochallenges: 1) sparse-only solutions preserve 3D geometric prior and yet loserich semantic information from the camera, and 2) dense-only alternativesretain the semantic continuity but miss the accurate geometric information fromLiDAR. By analyzing these two formulations, we conclude that the informationloss is inevitable due to their design scheme. To compensate for theinformation loss in either manner, we propose Sparse Dense Fusion (SDF), acomplementary framework that incorporates both sparse-fusion and dense-fusionmodules via the Transformer architecture. Such a simple yet effectivesparse-dense fusion structure enriches semantic texture and exploits spatialstructure information simultaneously. Through our SDF strategy, we assemble twopopular methods with moderate performance and outperform baseline by 4.3% inmAP and 2.5% in NDS, ranking first on the nuScenes benchmark. Extensiveablations demonstrate the effectiveness of our method and empirically align ouranalysis.", "output": "Sparse Dense Fusion for 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bounded by the inherent ambiguity of depth perception, contemporarymulti-view 3D object detection methods fall into the performance bottleneck.Intuitively, leveraging temporal multi-view stereo (MVS) technology is thenatural knowledge for tackling this ambiguity. However, traditional attempts ofMVS has two limitations when applying to 3D object detection scenes: 1) Theaffinity measurement among all views suffers expensive computational cost; 2)It is difficult to deal with outdoor scenarios where objects are often mobile.To this end, we propose BEVStereo++: by introducing a dynamic temporal stereostrategy, BEVStereo++ is able to cut down the harm that is brought byintroducing temporal stereo when dealing with those two scenarios. Going onestep further, we apply Motion Compensation Module and long sequence FrameFusion to BEVStereo++, which shows further performance boosting and errorreduction. Without bells and whistles, BEVStereo++ achievesstate-of-the-art(SOTA) on both Waymo and nuScenes dataset.", "output": "BEVStereo++: Accurate Depth Estimation in Multi-view 3D Object Detection via Dynamic Temporal Stereo."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing point cloud modeling datasets primarily express the modelingprecision by pose or trajectory precision rather than the point cloud modelingeffect itself. Under this demand, we first independently construct a set ofLiDAR system with an optical stage, and then we build a HPMB dataset based onthe constructed LiDAR system, a High-Precision, Multi-Beam, real-world dataset.Second, we propose an modeling evaluation method based on HPMB for object-levelmodeling to overcome this limitation. In addition, the existing point cloudmodeling methods tend to generate continuous skeletons of the globalenvironment, hence lacking attention to the shape of complex objects. To tacklethis challenge, we propose a novel learning-based joint framework, DSMNet, forhigh-precision 3D surface modeling from sparse point cloud frames. DSMNetcomprises density-aware Point Cloud Registration (PCR) and geometry-aware PointCloud Sampling (PCS) to effectively learn the implicit structure feature ofsparse point clouds. Extensive experiments demonstrate that DSMNet outperformsthe state-of-the-art methods in PCS and PCR on Multi-View Partial Point Cloud(MVP) database. Furthermore, the experiments on the open source KITTI and ourproposed HPMB datasets show that DSMNet can be generalized as a post-processingof Simultaneous Localization And Mapping (SLAM), thereby improving modelingprecision in environments with sparse point clouds.", "output": "DSMNet: Deep High-precision 3D Surface Modeling from Sparse Point Cloud Frames."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern society, road safety relies heavily on the psychological andphysiological state of drivers. Negative factors such as fatigue, drowsiness,and stress can impair drivers' reaction time and decision making abilities,leading to an increased incidence of traffic accidents. Among the numerousstudies for impaired driving detection, wearable physiological measurement is areal-time approach to monitoring a driver's state. However, currently, thereare few driver physiological datasets in open road scenarios and the existingdatasets suffer from issues such as poor signal quality, small sample sizes,and short data collection periods. Therefore, in this paper, a large-scalemultimodal driving dataset for driver impairment detection and biometric datarecognition is designed and described. The dataset contains two modalities ofdriving signals: six-axis inertial signals and electrocardiogram (ECG) signals,which were recorded while over one hundred drivers were following the sameroute through open roads during several months. Both the ECG signal sensor andthe six-axis inertial signal sensor are installed on a specially designedsteering wheel cover, allowing for data collection without disturbing thedriver. Additionally, electrodermal activity (EDA) signals were also recordedduring the driving process and will be integrated into the presented datasetsoon. Future work can build upon this dataset to advance the field of driverimpairment detection. New methods can be explored for integrating other typesof biometric signals, such as eye tracking, to further enhance theunderstanding of driver states. The insights gained from this dataset can alsoinform the development of new driver assistance systems, promoting saferdriving practices and reducing the risk of traffic accidents. The OpenDriverdataset will be publicly available soon.", "output": "OpenDriver: an open-road driver state detection dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the modality gap between visible and infrared images with high visualambiguity, learning textbf{diverse} modality-shared semantic concepts forvisible-infrared person re-identification (VI-ReID) remains a challengingproblem. Body shape is one of the significant modality-shared cues for VI-ReID.To dig more diverse modality-shared cues, we expect that erasingbody-shape-related semantic concepts in the learned features can force the ReIDmodel to extract more and other modality-shared features for identification. Tothis end, we propose shape-erased feature learning paradigm that decorrelatesmodality-shared features in two orthogonal subspaces. Jointly learningshape-related feature in one subspace and shape-erased features in theorthogonal complement achieves a conditional mutual information maximizationbetween shape-erased feature and identity discarding body shape information,thus enhancing the diversity of the learned representation explicitly.Extensive experiments on SYSU-MM01, RegDB, and HITSZ-VCM datasets demonstratethe effectiveness of our method.", "output": "Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomaly detection suffered from the lack of anomalies due to the diversity ofabnormalities and the difficulties of obtaining large-scale anomaly data.Semi-supervised anomaly detection methods are often used to solely leveragenormal data to detect abnormalities that deviated from the learnt normalitydistributions. Meanwhile, given the fact that limited anomaly data can beobtained with a minor cost in practice, some researches also investigatedanomaly detection methods under supervised scenarios with limited anomaly data.In order to address the lack of abnormal data for robust anomaly detection, wepropose Adversarial Generative Anomaly Detection (AGAD), a self-contrast-basedanomaly detection paradigm that learns to detect anomalies by generatingtextit{contextual adversarial information} from the massive normal examples.Essentially, our method generates pseudo-anomaly data for both supervised andsemi-supervised anomaly detection scenarios. Extensive experiments are carriedout on multiple benchmark datasets and real-world datasets, the results showsignificant improvement in both supervised and semi-supervised scenarios.Importantly, our approach is data-efficient that can boost up the detectionaccuracy with no more than 5% anomalous training data.", "output": "AGAD: Adversarial Generative Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Association football is a complex and dynamic sport, with numerous actionsoccurring simultaneously in each game. Analyzing football videos is challengingand requires identifying subtle and diverse spatio-temporal patterns. Despiterecent advances in computer vision, current algorithms still face significantchallenges when learning from limited annotated data, lowering theirperformance in detecting these patterns. In this paper, we propose an activelearning framework that selects the most informative video samples to beannotated next, thus drastically reducing the annotation effort andaccelerating the training of action spotting models to reach the highestaccuracy at a faster pace. Our approach leverages the notion of uncertaintysampling to select the most challenging video clips to train on next, hasteningthe learning process of the algorithm. We demonstrate that our proposed activelearning framework effectively reduces the required training data for accurateaction spotting in football videos. We achieve similar performances for actionspotting with NetVLAD++ on SoccerNet-v2, using only one-third of the dataset,indicating significant capabilities for reducing annotation time and improvingdata efficiency. We further validate our approach on two new datasets thatfocus on temporally localizing actions of headers and passes, proving itseffectiveness across different action semantics in football. We believe ouractive learning framework for action spotting would support furtherapplications of action spotting algorithms and accelerate annotation campaignsin the sports domain.", "output": "Towards Active Learning for Action Spotting in Association Football Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Owing to success in the data-rich domain of natural images, Transformers haverecently become popular in medical image segmentation. However, the pairing ofTransformers with convolutional blocks in varying architectural permutationsleaves their relative effectiveness to open interpretation. We introduceTransformer Ablations that replace the Transformer blocks with plain linearoperators to quantify this effectiveness. With experiments on 8 models on 2medical image segmentation tasks, we explore -- 1) the replaceable nature ofTransformer-learnt representations, 2) Transformer capacity alone cannotprevent representational replaceability and works in tandem with effectivedesign, 3) The mere existence of explicit feature hierarchies in transformerblocks is more beneficial than accompanying self-attention modules, 4) Majorspatial downsampling before Transformer modules should be used with caution.", "output": "Transformer Utilization in Medical Image Segmentation Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video captioning aims to convey dynamic scenes from videos using naturallanguage, facilitating the understanding of spatiotemporal information withinour environment. Although there have been recent advances, generating detailedand enriched video descriptions continues to be a substantial challenge. Inthis work, we introduce Video ChatCaptioner, an innovative approach forcreating more comprehensive spatiotemporal video descriptions. Our methodemploys a ChatGPT model as a controller, specifically designed to select framesfor posing video content-driven questions. Subsequently, a robust algorithm isutilized to answer these visual queries. This question-answer frameworkeffectively uncovers intricate video details and shows promise as a method forenhancing video content. Following multiple conversational rounds, ChatGPT cansummarize enriched video content based on previous conversations. Wequalitatively demonstrate that our Video ChatCaptioner can generate captionscontaining more visual details about the videos. The code is publicly availableat ", "output": "Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The vulnerability in the algorithm supply chain of deep learning has imposednew challenges to image retrieval systems in the downstream. Among a variety oftechniques, deep hashing is gaining popularity. As it inherits the algorithmicbackend from deep learning, a handful of attacks are recently proposed todisrupt normal image retrieval. Unfortunately, the defense strategies insoftmax classification are not readily available to be applied in the imageretrieval domain. In this paper, we propose an efficient and unsupervisedscheme to identify unique adversarial behaviors in the hamming space. Inparticular, we design three criteria from the perspectives of hamming distance,quantization loss and denoising to defend against both untargeted and targetedattacks, which collectively limit the adversarial space. The extensiveexperiments on four datasets demonstrate 2-23% improvements of detection rateswith minimum computational overhead for real-time image queries.", "output": "Unsupervised Multi-Criteria Adversarial Detection in Deep Image Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised crowd counting relies heavily on costly manual labeling, which isdifficult and expensive, especially in dense scenes. To alleviate the problem,we propose a novel unsupervised framework for crowd counting, named CrowdCLIP.The core idea is built on two observations: 1) the recent contrastivepre-trained vision-language model (CLIP) has presented impressive performanceon various downstream tasks; 2) there is a natural mapping between crowdpatches and count text. To the best of our knowledge, CrowdCLIP is the first toinvestigate the vision language knowledge to solve the counting problem.Specifically, in the training stage, we exploit the multi-modal ranking loss byconstructing ranking text prompts to match the size-sorted crowd patches toguide the image encoder learning. In the testing stage, to deal with thediversity of image patches, we propose a simple yet effective progressivefiltering strategy to first select the highly potential crowd patches and thenmap them into the language space with various counting intervals. Extensiveexperiments on five challenging datasets demonstrate that the proposedCrowdCLIP achieves superior performance compared to previous unsupervisedstate-of-the-art counting methods. Notably, CrowdCLIP even surpasses somepopular fully-supervised methods under the cross-dataset setting. The sourcecode will be available at ", "output": "CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-attention mechanism has been a key factor in the recent progress ofVision Transformer (ViT), which enables adaptive feature extraction from globalcontexts. However, existing self-attention methods either adopt sparse globalattention or window attention to reduce the computation complexity, which maycompromise the local feature learning or subject to some handcrafted designs.In contrast, local attention, which restricts the receptive field of each queryto its own neighboring pixels, enjoys the benefits of both convolution andself-attention, namely local inductive bias and dynamic feature selection.Nevertheless, current local attention modules either use inefficient Im2Colfunction or rely on specific CUDA kernels that are hard to generalize todevices without CUDA support. In this paper, we propose a novel local attentionmodule, Slide Attention, which leverages common convolution operations toachieve high efficiency, flexibility and generalizability. Specifically, wefirst re-interpret the column-based Im2Col function from a new row-basedperspective and use Depthwise Convolution as an efficient substitution. On thisbasis, we propose a deformed shifting module based on the re-parameterizationtechnique, which further relaxes the fixed key/value positions to deformedfeatures in the local region. In this way, our module realizes the localattention paradigm in both efficient and flexible manner. Extensive experimentsshow that our slide attention module is applicable to a variety of advancedVision Transformer models and compatible with various hardware devices, andachieves consistently improved performances on comprehensive benchmarks. Codeis available at ", "output": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing super-resolution models for pathology images can only work in fixedinteger magnifications and have limited performance. Though implicit neuralnetwork-based methods have shown promising results in arbitrary-scalesuper-resolution of natural images, it is not effective to directly apply themin pathology images, because pathology images have special fine-grained imagetextures different from natural images. To address this challenge, we propose adual-branch framework with an efficient self-texture enhancement mechanism forarbitrary-scale super-resolution of pathology images. Extensive experiments ontwo public datasets show that our method outperforms both existing fixed-scaleand arbitrary-scale algorithms. To the best of our knowledge, this is the firstwork to achieve arbitrary-scale super-resolution in the field of pathologyimages. Codes will be available.", "output": "Towards Arbitrary-scale Histopathology Image Super-resolution: An Efficient Dual-branch Framework based on Implicit Self-texture Enhancement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores the potential of curriculum learning in LiDAR-based 3Dobject detection by proposing a curricular object manipulation (COM) framework.The framework embeds the curricular training strategy into both the loss designand the augmentation process. For the loss design, we propose the COMLoss todynamically predict object-level difficulties and emphasize objects ofdifferent difficulties based on training stages. On top of the widely-usedaugmentation technique called GT-Aug in LiDAR detection tasks, we propose anovel COMAug strategy which first clusters objects in ground-truth databasebased on well-designed heuristics. Group-level difficulties rather thanindividual ones are then predicted and updated during training for stableresults. Model performance and generalization capabilities can be improved bysampling and augmenting progressively more difficult objects into the trainingsamples. Extensive experiments and ablation studies reveal the superior andgenerality of the proposed framework. The code is available at", "output": "Curricular Object Manipulation in LiDAR-based Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning in real-world scenarios is a major challenge. A generalcontinual learning model should have a constant memory size and no predefinedtask boundaries, as is the case in semi-supervised Video Object Segmentation(VOS), where continual learning challenges particularly present themselves inworking on long video sequences. In this article, we first formulate theproblem of semi-supervised VOS, specifically online VOS, as a continuallearning problem, and then secondly provide a public VOS dataset, CLVOS23,focusing on continual learning. Finally, we propose and implement aregularization-based continual learning approach on LWL, an existing online VOSbaseline, to demonstrate the efficacy of continual learning when applied toonline VOS and to establish a CLVOS23 baseline. We apply the proposed baselineto the Long Videos dataset as well as to two short video VOS datasets, DAVIS16and DAVIS17. To the best of our knowledge, this is the first time that VOS hasbeen defined and addressed as a continual learning problem.", "output": "CLVOS23: A Long Video Object Segmentation Dataset for Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "RGB-T tracking involves the use of images from both visible and thermalmodalities. The primary objective is to adaptively lever-age the relativelydominant modality in varying conditions to achieve more robust trackingcompared to single-modality track-ing. An RGB-T tracker based on mixedattention mechanism to achieve complementary fusion of modalities (referred toas MACFT) is proposed in this paper. In the feature extraction stage, weutilize different transformer backbone branches to extract specific and sharedinformation from different modali-ties. By performing mixed attentionoperations in the backbone to enable information interaction andself-enhancement between the template and search images, it constructs a robustfeature representation that better understands the high-level semantic featuresof the target. Then, in the feature fusion stage, a modal-ity-adaptive fusionis achieved through a mixed attention-based modality fusion network, whichsuppresses the low-quality mo-dality noise while enhancing the information ofthe dominant modality. Evaluation on multiple RGB-T public datasetsdemon-strates that our proposed tracker outperforms other RGB-T trackers ongeneral evaluation metrics while also being able to adapt to long-term trackingscenarios.", "output": "RGB-T Tracking Based on Mixed Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Controllable human image generation (HIG) has numerous real-lifeapplications. State-of-the-art solutions, such as ControlNet and T2I-Adapter,introduce an additional learnable branch on top of the frozen pre-trainedstable diffusion (SD) model, which can enforce various conditions, includingskeleton guidance of HIG. While such a plug-and-play approach is appealing, theinevitable and uncertain conflicts between the original images produced fromthe frozen SD branch and the given condition incur significant challenges forthe learnable branch, which essentially conducts image feature editing forcondition enforcement. In this work, we propose a native skeleton-guideddiffusion model for controllable HIG called HumanSD. Instead of performingimage editing with dual-branch diffusion, we fine-tune the original SD modelusing a novel heatmap-guided denoising loss. This strategy effectively andefficiently strengthens the given skeleton condition during model trainingwhile mitigating the catastrophic forgetting effects. HumanSD is fine-tuned onthe assembly of three large-scale human-centric datasets with text-image-poseinformation, two of which are established in this work. As shown in Figure 1,HumanSD outperforms ControlNet in terms of accurate pose control and imagequality, particularly when the given skeleton guidance is sophisticated.", "output": "HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a dense neural simultaneous localization and mapping (SLAM)approach for monocular RGBD input which anchors the features of a neural scenerepresentation in a point cloud that is iteratively generated in aninput-dependent data-driven manner. We demonstrate that both tracking andmapping can be performed with the same point-based neural scene representationby minimizing an RGBD-based re-rendering loss. In contrast to recent denseneural SLAM methods which anchor the scene features in a sparse grid, ourpoint-based approach allows dynamically adapting the anchor point density tothe information density of the input. This strategy reduces runtime and memoryusage in regions with fewer details and dedicates higher point density toresolve fine details. Our approach performs either better or competitive toexisting dense neural RGBD SLAM methods in tracking, mapping and renderingaccuracy on the Replica, TUM-RGBD and ScanNet datasets. The source code isavailable at ", "output": "Point-SLAM: Dense Neural Point Cloud-based SLAM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Micropaleontology in geosciences focuses on studying the evolution ofmicrofossils (e.g., foraminifera) through geological records to reconstructpast environmental and climatic conditions. This field heavily relies on visualrecognition of microfossil features, making it suitable for computer visiontechnology, specifically deep convolutional neural networks (CNNs), to automateand optimize microfossil identification and classification. However, theapplication of deep learning in micropaleontology is hindered by limitedavailability of high-quality, high-resolution labeled fossil images and thesignificant manual labeling effort required by experts. To address thesechallenges, we propose a novel deep learning workflow combining hierarchicalvision transformers with style-based generative adversarial network algorithmsto efficiently acquire and synthetically generate realistic high-resolutionlabeled datasets of micropaleontology in large volumes. Our study shows thatthis workflow can generate high-resolution images with a high signal-to-noiseratio (39.1 dB) and realistic synthetic images with a Frechet inceptiondistance similarity score of 14.88. Additionally, our workflow provides a largevolume of self-labeled datasets for model benchmarking and various downstreamvisual tasks, including fossil classification and segmentation. For the firsttime, we performed few-shot semantic segmentation of different foraminiferachambers on both generated and synthetic images with high accuracy. This novelmeta-learning approach is only possible with the availability ofhigh-resolution, high-volume labeled datasets. Our deep learning-based workflowshows promise in advancing and optimizing micropaleontological research andother visual-dependent geological analyses.", "output": "ForamViT-GAN: Exploring New Paradigms in Deep Learning for Micropaleontological Image Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an end-to-end automated workflow that uses large-scale remotecompute resources and an embedded GPU platform at the edge to enableAI/ML-accelerated real-time analysis of data collected for x-ray ptychography.Ptychography is a lensless method that is being used to image samples through asimultaneous numerical inversion of a large number of diffraction patterns fromadjacent overlapping scan positions. This acquisition method can enablenanoscale imaging with x-rays and electrons, but this often requires very largeexperimental datasets and commensurately high turnaround times, which can limitexperimental capabilities such as real-time experimental steering andlow-latency monitoring. In this work, we introduce a software system that canautomate ptychography data analysis tasks. We accelerate the data analysispipeline by using a modified version of PtychoNN -- an ML-based approach tosolve phase retrieval problem that shows two orders of magnitude speedupcompared to traditional iterative methods. Further, our system coordinates andoverlaps different data analysis tasks to minimize synchronization overheadbetween different stages of the workflow. We evaluate our workflow system withreal-world experimental workloads from the 26ID beamline at Advanced PhotonSource and ThetaGPU cluster at Argonne Leadership Computing Resources.", "output": "AI-assisted Automated Workflow for Real-time X-ray Ptychography Data Analysis via Federated Resources."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The indeterminate nature of human motion requires trajectory predictionsystems to use a probabilistic model to formulate the multi-modality phenomenonand infer a finite set of future trajectories. However, the inference processesof most existing methods rely on Monte Carlo random sampling, which isinsufficient to cover the realistic paths with finite samples, due to the longtail effect of the predicted distribution. To promote the sampling process ofstochastic prediction, we propose a novel method, called BOsampler, toadaptively mine potential paths with Bayesian optimization in an unsupervisedmanner, as a sequential design strategy in which new prediction is dependent onthe previously drawn samples. Specifically, we model the trajectory sampling asa Gaussian process and construct an acquisition function to measure thepotential sampling value. This acquisition function applies the originaldistribution as prior and encourages exploring paths in the long-tail region.This sampling method can be integrated with existing stochastic predictivemodels without retraining. Experimental results on various baseline methodsdemonstrate the effectiveness of our method.", "output": "Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the past few years, in the context of fully-supervised semanticsegmentation, several losses -- such as cross-entropy and dice -- have emergedas de facto standards to supervise neural networks. The Dice loss is aninteresting case, as it comes from the relaxation of the popular Dicecoefficient; one of the main evaluation metric in medical imaging applications.In this paper, we first study theoretically the gradient of the dice loss,showing that concretely it is a weighted negative of the ground truth, with avery small dynamic range. This enables us, in the second part of this paper, tomimic the supervision of the dice loss, through a simple element-wisemultiplication of the network output with a negative of the ground truth. Thisrather surprising result sheds light on the practical supervision performed bythe dice loss during gradient descent. This can help the practitioner tounderstand and interpret results while guiding researchers when designing newlosses.", "output": "On the dice loss gradient and the ways to mimic it."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding the continuous states of objects is essential for task learningand planning in the real world. However, most existing task learning benchmarksassume discrete(e.g., binary) object goal states, which poses challenges forthe learning of complex tasks and transferring learned policy from simulatedenvironments to the real world. Furthermore, state discretization limits arobot's ability to follow human instructions based on the grounding of actionsand states. To tackle these challenges, we present ARNOLD, a benchmark thatevaluates language-grounded task learning with continuous states in realistic3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involveunderstanding object states and learning policies for continuous goals. Topromote language-instructed learning, we provide expert demonstrations withtemplate-generated language descriptions. We assess task performance byutilizing the latest language-conditioned policy learning models. Our resultsindicate that current models for language-conditioned manipulations continue toexperience significant challenges in novel goal-state generalizations, scenegeneralizations, and object generalizations. These findings highlight the needto develop new algorithms that address this gap and underscore the potentialfor further research in this area. See our project page at:", "output": "ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work proposes a self-supervised learning system for segmenting rigidobjects in RGB images. The proposed pipeline is trained on unlabeled RGB-Dvideos of static objects, which can be captured with a camera carried by amobile robot. A key feature of the self-supervised training process is agraph-matching algorithm that operates on the over-segmentation output of thepoint cloud that is reconstructed from each video. The graph matching, alongwith point cloud registration, is able to find reoccurring object patternsacross videos and combine them into 3D object pseudo labels, even underocclusions or different viewing angles. Projected 2D object masks from 3Dpseudo labels are used to train a pixel-wise feature extractor throughcontrastive learning. During online inference, a clustering method uses thelearned features to cluster foreground pixels into object segments. Experimentshighlight the method's effectiveness on both real and synthetic video datasets,which include cluttered scenes of tabletop objects. The proposed methodoutperforms existing unsupervised methods for object segmentation by a largemargin.", "output": "Self-Supervised Learning of Object Segmentation from Unlabeled RGB-D Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The successful implementation of vision-based navigation in agriculturalfields hinges upon two critical components: 1) the accurate identification ofkey components within the scene, and 2) the identification of lanes through thedetection of boundary lines that separate the crops from the traversableground. We propose Agronav, an end-to-end vision-based autonomous navigationframework, which outputs the centerline from the input image by sequentiallyprocessing it through semantic segmentation and semantic line detection models.We also present Agroscapes, a pixel-level annotated dataset collected acrosssix different crops, captured from varying heights and angles. This ensuresthat the framework trained on Agroscapes is generalizable across both groundand aerial robotic platforms. Codes, models and dataset will be released athref{", "output": "Agronav: Autonomous Navigation Framework for Agricultural Robots and Vehicles using Semantic Segmentation and Semantic Line Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel framework for finding a set of tight bounding boxes of a3D shape via neural-network-based over-segmentation and iterative merging andrefinement. Achieving tight bounding boxes of a shape while guaranteeing thecomplete boundness is an essential task for efficient geometric operations andunsupervised semantic part detection, but previous methods fail to achieve bothfull coverage and tightness. Neural-network-based methods are not suitable forthese goals due to the non-differentiability of the objective, and also classiciterative search methods suffer from their sensitivity to the initialization.We demonstrate that the best integration of the learning-based and iterativesearch methods can achieve the bounding boxes with both properties. We employan existing unsupervised segmentation network to textbf{split} the shape andobtain over-segmentation. Then, we apply hierarchical textbf{merging} with ournovel tightness-aware merging and stopping criteria. To overcome thesensitivity to the initialization, we also textbf{refine} the bounding boxparameters in a game setup with a soft reward function promoting a widerexploration. Lastly, we further improve the bounding boxes with a MCTS-basedmulti-action space exploration. Our experimental results demonstrate the fullcoverage, tightness, and the adequate number of bounding boxes of our method.", "output": "Split, Merge, and Refine: Fitting Tight Bounding Boxes via Learned Over-Segmentation and Iterative Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in diffusion models enable many powerful instruments forimage editing. One of these instruments is text-driven image manipulations:editing semantic attributes of an image according to the provided textdescription. % Popular text-conditional diffusion models offer varioushigh-quality image manipulation methods for a broad range of text prompts.Existing diffusion-based methods already achieve high-quality imagemanipulations for a broad range of text prompts. However, in practice, thesemethods require high computation costs even with a high-end GPU. This greatlylimits potential real-world applications of diffusion-based image editing,especially when running on user devices.In this paper, we address efficiency of the recent text-driven editingmethods based on unconditional diffusion models and develop a novel algorithmthat learns image manipulations 4.5-10 times faster and applies them 8 timesfaster. We carefully evaluate the visual quality and expressiveness of ourapproach on multiple datasets using human annotators. Our experimentsdemonstrate that our algorithm achieves the quality of much more expensivemethods. Finally, we show that our approach can adapt the pretrained model tothe user-specified image and text description on the fly just for 4 seconds. Inthis setting, we notice that more compact unconditional diffusion models can beconsidered as a rational alternative to the popular text-conditionalcounterparts.", "output": "Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Radiance field is an effective representation of 3D scenes, which has beenwidely adopted in novel-view synthesis and 3D reconstruction. It is still anopen and challenging problem to evaluate the geometry, i.e., the density field,as the ground-truth is almost impossible to be obtained. One alternativeindirect solution is to transform the density field into a point-cloud andcompute its Chamfer Distance with the scanned ground-truth. However, manywidely-used datasets have no point-cloud ground-truth since the scanningprocess along with the equipment is expensive and complicated. To this end, wepropose a novel metric, named Inverse Mean Residual Color (IMRC), which canevaluate the geometry only with the observation images. Our key insight is thatthe better the geometry is, the lower-frequency the computed color field is.From this insight, given reconstructed density field and the observationimages, we design a closed-form method to approximate the color field withlow-frequency spherical harmonics and compute the inverse mean residual color.Then the higher the IMRC, the better the geometry. Qualitative and quantitativeexperimental results verify the effectiveness of our proposed IMRC metric. Wealso benchmark several state-of-the-art methods using IMRC to promote futurerelated research.", "output": "Evaluate Geometry of Radiance Field with Low-frequency Color Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A surge of interest has emerged in utilizing Transformers in diverse visiontasks owing to its formidable performance. However, existing approachesprimarily focus on optimizing internal model architecture designs that oftenentail significant trial and error with high burdens. In this work, we proposea new paradigm dubbed Decision Stream Calibration that boosts the performanceof general Vision Transformers. To achieve this, we shed light on theinformation propagation mechanism in the learning procedure by exploring thecorrelation between different tokens and the relevance coefficient of multipledimensions. Upon further analysis, it was discovered that 1) the final decisionis associated with tokens of foreground targets, while token features offoreground target will be transmitted into the next layer as much as possible,and the useless token features of background area will be eliminated graduallyin the forward propagation. 2) Each category is solely associated with specificsparse dimensions in the tokens. Based on the discoveries mentioned above, wedesigned a two-stage calibration scheme, namely ViT-Calibrator, including tokenpropagation calibration stage and dimension propagation calibration stage.Extensive experiments on commonly used datasets show that the proposed approachcan achieve promising results. The source codes are given in the supplements.", "output": "ViT-Calibrator: Decision Stream Calibration for Vision Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing approaches for autonomous control of pan-tilt-zoom (PTZ) cameras usemultiple stages where object detection and localization are performedseparately from the control of the PTZ mechanisms. These approaches requiremanual labels and suffer from performance bottlenecks due to error propagationacross the multi-stage flow of information. The large size of object detectionneural networks also makes prior solutions infeasible for real-time deploymentin resource-constrained devices. We present an end-to-end deep reinforcementlearning (RL) solution called Eagle to train a neural network policy thatdirectly takes images as input to control the PTZ camera. Trainingreinforcement learning is cumbersome in the real world due to labeling effort,runtime environment stochasticity, and fragile experimental setups. Weintroduce a photo-realistic simulation framework for training and evaluation ofPTZ camera control policies. Eagle achieves superior camera control performanceby maintaining the object of interest close to the center of captured images athigh resolution and has up to 17% more tracking duration than thestate-of-the-art. Eagle policies are lightweight (90x fewer parameters thanYolo5s) and can run on embedded camera platforms such as Raspberry PI (33 FPS)and Jetson Nano (38 FPS), facilitating real-time PTZ tracking forresource-constrained environments. With domain randomization, Eagle policiestrained in our simulator can be transferred directly to real-world scenarios.", "output": "Eagle: End-to-end Deep Reinforcement Learning based Autonomous Control of PTZ Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hashing is very popular for remote sensing image search. This articleproposes a multiview hashing with learnable parameters to retrieve the queriedimages for a large-scale remote sensing dataset. Existing methods alwaysneglect that real-world remote sensing data lies on a low-dimensional manifoldembedded in high-dimensional ambient space. Unlike previous methods, thisarticle proposes to learn the consensus compact codes in a view-specificlow-dimensional subspace. Furthermore, we have added a hyperparameter learnablemodule to avoid complex parameter tuning. In order to prove the effectivenessof our method, we carried out experiments on three widely used remote sensingdata sets and compared them with seven state-of-the-art methods. Extensiveexperiments show that the proposed method can achieve competitive resultscompared to the other method.", "output": "Locality Preserving Multiview Graph Hashing for Large Scale Remote Sensing Image Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, video text detection, tracking, and recognition in natural scenesare becoming very popular in the computer vision community. However, mostexisting algorithms and benchmarks focus on common text cases (e.g., normalsize, density) and single scenarios, while ignoring extreme video textchallenges, i.e., dense and small text in various scenarios. In thiscompetition report, we establish a video text reading benchmark, DSText, whichfocuses on dense and small text reading challenges in the video with variousscenarios. Compared with the previous datasets, the proposed dataset mainlyinclude three new challenges: 1) Dense video texts, a new challenge for videotext spotter. 2) High-proportioned small texts. 3) Various new scenarios, e.g.,Game, sports, etc. The proposed DSText includes 100 video clips from 12 openscenarios, supporting two tasks (i.e., video text tracking (Task 1) andend-to-end video text spotting (Task 2)). During the competition period (openedon 15th February 2023 and closed on 20th March 2023), a total of 24 teamsparticipated in the three proposed tasks with around 30 valid submissions,respectively. In this article, we describe detailed statistical information ofthe dataset, tasks, evaluation protocols and the results summaries of the ICDAR2023 on DSText competition. Moreover, we hope the benchmark will promise videotext research in the community.", "output": "ICDAR 2023 Video Text Reading Competition for Dense and Small Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial attacks in the input (pixel) space typically incorporate noisemargins such as $L_1$ or $L_{infty}$-norm to produce imperceptibly perturbeddata that confound deep learning networks. Such noise margins confine themagnitude of permissible noise. In this work, we propose injecting adversarialperturbations in the latent (feature) space using a generative adversarialnetwork, removing the need for margin-based priors. Experiments on MNIST,CIFAR10, Fashion-MNIST, CIFAR100 and Stanford Dogs datasets support theeffectiveness of the proposed method in generating adversarial attacks in thelatent space while ensuring a high degree of visual realism with respect topixel-based adversarial attack methods.", "output": "Generating Adversarial Attacks in the Latent Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents one of the first learning-based NeRF 3D instancesegmentation pipelines, dubbed as Instance Neural Radiance Field, or InstanceNeRF. Taking a NeRF pretrained from multi-view RGB images as input, InstanceNeRF can learn 3D instance segmentation of a given scene, represented as aninstance field component of the NeRF model. To this end, we adopt a 3Dproposal-based mask prediction network on the sampled volumetric features fromNeRF, which generates discrete 3D instance masks. The coarse 3D mask predictionis then projected to image space to match 2D segmentation masks from differentviews generated by existing panoptic segmentation models, which are used tosupervise the training of the instance field. Notably, beyond generatingconsistent 2D segmentation maps from novel views, Instance NeRF can queryinstance information at any 3D point, which greatly enhances NeRF objectsegmentation and manipulation. Our method is also one of the first to achievesuch results without ground-truth instance information during inference.Experimented on synthetic and real-world NeRF datasets with complex indoorscenes, Instance NeRF surpasses previous NeRF segmentation works andcompetitive 2D segmentation methods in segmentation performance on unseenviews. See the demo video at ", "output": "Instance Neural Radiance Field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual and linguistic pre-training aims to learn vision and languagerepresentations together, which can be transferred to visual-linguisticdownstream tasks. However, there exists semantic confusion between language andvision during the pre-training stage. Moreover, current pre-trained models tendto take lots of computation resources for fine-tuning when transferred todownstream tasks. In this work, we present a simple but effective approach forlearning Contrastive and Adaptive representations of Vision and Language,namely CAVL. Specifically, we introduce a pair-wise contrastive loss to learnalignments between the whole sentence and each image in the same batch duringthe pre-training process. At the fine-tuning stage, we introduce twolightweight adaptation networks to reduce model parameters and increasetraining speed for saving computation resources. We evaluate our CAVL on sixmain downstream tasks, including Visual Question Answering (VQA), VisualCommonsense Reasoning (VCR), Natural Language for Visual Reasoning (NLVR),Region-to-Phrase Grounding (RPG), Text-to-Image Retrieval (TIR), and Zero-shotText-to-Image Retrieval (ZS-TIR). Compared to baselines, we achieve superiorperformance and reduce the fine-tuning time by a large margin (in particular,76.17%). Extensive experiments and ablation studies demonstrate the efficiencyof contrastive pre-training and adaptive fine-tuning proposed in our CAVL.", "output": "CAVL: Learning Contrastive and Adaptive Representations of Vision and Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cloth-changing person reidentification (ReID) is a newly emerging researchtopic that is aimed at addressing the issues of large feature variations due tocloth-changing and pedestrian view/pose changes. Although significant progresshas been achieved by introducing extra information (e.g., human contoursketching information, human body keypoints, and 3D human information),cloth-changing person ReID is still challenging due to impressionablepedestrian representations. Moreover, human semantic information and pedestrianidentity information are not fully explored. To solve these issues, we proposea novel identity-guided collaborative learning scheme (IGCL) for cloth-changingperson ReID, where the human semantic is fully utilized and the identity isunchangeable to guide collaborative learning. First, we design a novel clothingattention degradation stream to reasonably reduce the interference caused byclothing information where clothing attention and mid-level collaborativelearning are employed. Second, we propose a human semantic attention and bodyjigsaw stream to highlight the human semantic information and simulatedifferent poses of the same identity. In this way, the extraction features notonly focus on human semantic information that is unrelated to the backgroundbut also are suitable for pedestrian pose variations. Moreover, a pedestrianidentity enhancement stream is further proposed to enhance the identityimportance and extract more favorable identity robust features. Mostimportantly, all these streams are jointly explored in an end-to-end unifiedframework, and the identity is utilized to guide the optimization. Extensiveexperiments on five public clothing person ReID datasets demonstrate that theproposed IGCL significantly outperforms SOTA methods and that the extractedfeature is more robust, discriminative, and clothing-irrelevant.", "output": "Identity-Guided Collaborative Learning for Cloth-Changing Person Reidentification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the increasing demand for oriented object detection e.g. in autonomousdriving and remote sensing, the oriented annotation has become alabor-intensive work. To make full use of existing horizontally annotateddatasets and reduce the annotation cost, a weakly-supervised detector H2RBoxfor learning the rotated box (RBox) from the horizontal box (HBox) has beenproposed and received great attention. This paper presents a new version,H2RBox-v2, to further bridge the gap between HBox-supervised andRBox-supervised oriented object detection. While exploiting axisymmetry viaflipping and rotating consistencies is available through our theoreticalanalysis, H2RBox-v2, using a weakly-supervised branch similar to H2RBox, isembedded with a novel self-supervised branch that learns orientations from thesymmetry inherent in the image of objects. Complemented by modules to cope withperipheral issues, e.g. angular periodicity, a stable and effective solution isachieved. To our knowledge, H2RBox-v2 is the first symmetry-supervised paradigmfor oriented object detection. Compared to H2RBox, our method is lesssusceptible to low annotation quality and insufficient training data, which insuch cases is expected to give a competitive performance much closer tofully-supervised oriented object detectors. Specifically, the performancecomparison between H2RBox-v2 and Rotated FCOS on DOTA-v1.0/1.5/2.0 is72.31%/64.76%/50.33% vs. 72.44%/64.53%/51.77%, 89.66% vs. 88.99% on HRSC, and42.27% vs. 41.25% on FAIR1M.", "output": "H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online class-incremental continual learning is a specific task of continuallearning. It aims to continuously learn new classes from data stream and thesamples of data stream are seen only once, which suffers from the catastrophicforgetting issue, i.e., forgetting historical knowledge of old classes.Existing replay-based methods effectively alleviate this issue by saving andreplaying part of old data in a proxy-based or contrastive-based replay manner.Although these two replay manners are effective, the former would incline tonew classes due to class imbalance issues, and the latter is unstable and hardto converge because of the limited number of samples. In this paper, we conducta comprehensive analysis of these two replay manners and find that they can becomplementary. Inspired by this finding, we propose a novel replay-based methodcalled proxy-based contrastive replay (PCR). The key operation is to replacethe contrastive samples of anchors with corresponding proxies in thecontrastive-based way. It alleviates the phenomenon of catastrophic forgettingby effectively addressing the imbalance issue, as well as keeps a fasterconvergence of the model. We conduct extensive experiments on three real-worldbenchmark datasets, and empirical results consistently demonstrate thesuperiority of PCR over various state-of-the-art methods.", "output": "PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Referring expression segmentation aims to segment an object described by alanguage expression from an image. Despite the recent progress on this task,existing models tackling this task may not be able to fully capture semanticsand visual representations of individual concepts, which limits theirgeneralization capability, especially when handling novel compositions oflearned concepts. In this work, through the lens of meta learning, we propose aMeta Compositional Referring Expression Segmentation (MCRES) framework toenhance model compositional generalization performance. Specifically, to handlevarious levels of novel compositions, our framework first uses training data toconstruct a virtual training set and multiple virtual testing sets, where datasamples in each virtual testing set contain a level of novel compositionsw.r.t. the virtual training set. Then, following a novel meta optimizationscheme to optimize the model to obtain good testing performance on the virtualtesting sets after training on the virtual training set, our framework caneffectively drive the model to better capture semantics and visualrepresentations of individual concepts, and thus obtain robust generalizationperformance even when handling novel compositions. Extensive experiments onthree benchmark datasets demonstrate the effectiveness of our framework.", "output": "Meta Compositional Referring Expression Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Avoiding the introduction of ghosts when synthesising LDR images as highdynamic range (HDR) images is a challenging task. Convolutional neural networks(CNNs) are effective for HDR ghost removal in general, but are challenging todeal with the LDR images if there are large movements oroversaturation/undersaturation. Existing dual-branch methods combining CNN andTransformer omit part of the information from non-reference images, while thefeatures extracted by the CNN-based branch are bound to the kernel size withsmall receptive field, which are detrimental to the deblurring and the recoveryof oversaturated/undersaturated regions. In this paper, we propose a novelhierarchical dual Transformer method for ghost-free HDR (HDT-HDR) imagesgeneration, which extracts global features and local features simultaneously.First, we use a CNN-based head with spatial attention mechanisms to extractfeatures from all the LDR images. Second, the LDR features are delivered to theHierarchical Dual Transformer (HDT). In each Dual Transformer (DT), the globalfeatures are extracted by the window-based Transformer, while the local detailsare extracted using the channel attention mechanism with deformable CNNs.Finally, the ghost free HDR image is obtained by dimensional mapping on the HDToutput. Abundant experiments demonstrate that our HDT-HDR achieves thestate-of-the-art performance among existing HDR ghost removal methods.", "output": "High Dynamic Range Imaging with Context-aware Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Micro-expressions are spontaneous, rapid and subtle facial movements that canneither be forged nor suppressed. They are very important nonverbalcommunication clues, but are transient and of low intensity thus difficult torecognize. Recently deep learning based methods have been developed formicro-expression (ME) recognition using feature extraction and fusiontechniques, however, targeted feature learning and efficient feature fusionstill lack further study according to the ME characteristics. To address theseissues, we propose a novel framework Feature Representation Learning withadaptive Displacement Generation and Transformer fusion (FRL-DGT), in which aconvolutional Displacement Generation Module (DGM) with self-supervisedlearning is used to extract dynamic features from onset/apex frames targeted tothe subsequent ME recognition task, and a well-designed Transformer Fusionmechanism composed of three Transformer-based fusion modules (local, globalfusions based on AU regions and full-face fusion) is applied to extract themulti-level informative features after DGM for the final ME prediction. Theextensive experiments with solid leave-one-subject-out (LOSO) evaluationresults have demonstrated the superiority of our proposed FRL-DGT tostate-of-the-art methods.", "output": "Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optical-flow-based and kernel-based approaches have been widely explored fortemporal compensation in satellite video super-resolution (VSR). However, thesetechniques involve high computational consumption and are prone to fail undercomplex motions. In this paper, we proposed to exploit the well-definedtemporal difference for efficient and robust temporal compensation. To fullyutilize the temporal information within frames, we separately modeled theshort-term and long-term temporal discrepancy since they provide distinctivecomplementary properties. Specifically, a short-term temporal difference moduleis designed to extract local motion representations from residual maps betweenadjacent frames, which provides more clues for accurate texture representation.Meanwhile, the global dependency in the entire frame sequence is explored vialong-term difference learning. The differences between forward and backwardsegments are incorporated and activated to modulate the temporal feature,resulting in holistic global compensation. Besides, we further proposed adifference compensation unit to enrich the interaction between the spatialdistribution of the target frame and compensated results, which helps maintainspatial consistency while refining the features to avoid misalignment.Extensive objective and subjective evaluation of five mainstream satellitevideos demonstrates that the proposed method performs favorably for satelliteVSR. Code will be available at url{", "output": "Local-Global Temporal Difference Learning for Satellite Video Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image segmentation is a challenging task with inherent ambiguity andhigh uncertainty, attributed to factors such as unclear tumor boundaries andmultiple plausible annotations. The accuracy and diversity of segmentationmasks are both crucial for providing valuable references to radiologists inclinical practice. While existing diffusion models have shown strong capacitiesin various visual generation tasks, it is still challenging to deal withdiscrete masks in segmentation. To achieve accurate and diverse medical imagesegmentation masks, we propose a novel conditional Bernoulli Diffusion modelfor medical image segmentation (BerDiff). Instead of using the Gaussian noise,we first propose to use the Bernoulli noise as the diffusion kernel to enhancethe capacity of the diffusion model for binary segmentation tasks, resulting inmore accurate segmentation masks. Second, by leveraging the stochastic natureof the diffusion model, our BerDiff randomly samples the initial Bernoullinoise and intermediate latent variables multiple times to produce a range ofdiverse segmentation masks, which can highlight salient regions of interestthat can serve as valuable references for radiologists. In addition, ourBerDiff can efficiently sample sub-sequences from the overall trajectory of thereverse diffusion, thereby speeding up the segmentation process. Extensiveexperimental results on two medical image segmentation datasets with differentmodalities demonstrate that our BerDiff outperforms other recently publishedstate-of-the-art methods. Our results suggest diffusion models could serve as astrong backbone for medical image segmentation.", "output": "BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The filming of sporting events projects and flattens the movement of athletesin the world onto a 2D broadcast image. The pixel locations of joints in theseimages can be detected with high validity. Recovering the actual 3D movement ofthe limbs (kinematics) of the athletes requires lifting these 2D pixellocations back into a third dimension, implying a certain scene geometry. Thewell-known line markings of sports fields allow for the calibration of thecamera and for determining the actual geometry of the scene. Close-up shots ofathletes are required to extract detailed kinematics, which in turn obfuscatesthe pertinent field markers for camera calibration. We suggest partial sportsfield registration, which determines a set of scene-consistent cameracalibrations up to a single degree of freedom. Through joint optimization of 3Dpose estimation and camera calibration, we demonstrate the successfulextraction of 3D running kinematics on a 400m track. In this work, we combineadvances in 2D human pose estimation and camera calibration via partial sportsfield registration to demonstrate an avenue for collecting valid large-scalekinematic datasets. We generate a synthetic dataset of more than 10k images inUnreal Engine 5 with different viewpoints, running styles, and body types, toshow the limitations of existing monocular 3D HPE methods. Synthetic data andcode are available at ", "output": "Monocular 3D Human Pose Estimation for Sports Broadcasts using Partial Sports Field Registration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of semi-supervised medical image segmentation, the shortage oflabeled data is the fundamental problem. How to effectively learn imagefeatures from unlabeled images to improve segmentation accuracy is the mainresearch direction in this field. Traditional self-training methods canpartially solve the problem of insufficient labeled data by generating pseudolabels for iterative training. However, noise generated due to the model'suncertainty during training directly affects the segmentation results.Therefore, we added sample-level and pixel-level uncertainty to stabilize thetraining process based on the self-training framework. Specifically, we savedseveral moments of the model during pre-training, and used the differencebetween their predictions on unlabeled samples as the sample-level uncertaintyestimate for that sample. Then, we gradually add unlabeled samples from easy tohard during training. At the same time, we added a decoder with differentupsampling methods to the segmentation network and used the difference betweenthe outputs of the two decoders as pixel-level uncertainty. In short, weselectively retrained unlabeled samples and assigned pixel-level uncertainty topseudo labels to optimize the self-training process. We compared thesegmentation results of our model with five semi-supervised approaches on thepublic 2017 ACDC dataset and 2018 Prostate dataset. Our proposed methodachieves better segmentation performance on both datasets under the samesettings, demonstrating its effectiveness, robustness, and potentialtransferability to other medical image segmentation tasks. Keywords: Medicalimage segmentation, semi-supervised learning, self-training, uncertaintyestimation", "output": "Self-training with dual uncertainty for semi-supervised medical image segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Single-frame infrared small target (SIRST) detection aims at separating smalltargets from clutter backgrounds on infrared images. Recently, deep learningbased methods have achieved promising performance on SIRST detection, but atthe cost of a large amount of training data with expensive pixel-levelannotations. To reduce the annotation burden, we propose the first method toachieve SIRST detection with single-point supervision. The core idea of thiswork is to recover the per-pixel mask of each target from the given singlepoint label by using clustering approaches, which looks simple but is indeedchallenging since targets are always insalient and accompanied with backgroundclutters. To handle this issue, we introduce randomness to the clusteringprocess by adding noise to the input images, and then obtain much more reliablepseudo masks by averaging the clustered results. Thanks to this \"Monte Carlo\"clustering approach, our method can accurately recover pseudo masks and thusturn arbitrary fully supervised SIRST detection networks into weakly supervisedones with only single point annotation. Experiments on four datasetsdemonstrate that our method can be applied to existing SIRST detection networksto achieve comparable performance with their fully supervised counterparts,which reveals that single-point supervision is strong enough for SIRSTdetection. Our code will be available at:", "output": "Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Considering a 2D matrix of positive and negative numbers, how might one drawa rectangle within it whose contents sum higher than all other rectangles'?This fundamental problem, commonly known the maximum rectangle problem orsubwindow search, spans many computational domains. Yet, the problem has notbeen solved without demanding computational resources at least linearlyproportional to the size of the matrix. In this work, we present a new approachto the problem which achieves sublinear time and memory complexities byinterpolating between a small amount of equidistant sections of the matrix.Applied to natural images, our solution outperforms the state-of-the-art byachieving an 11x increase in speed and memory efficiency at 99% comparativeaccuracy. In general, our solution outperforms existing solutions when matricesare sufficiently large and a marginal decrease in accuracy is acceptable, suchas in many problems involving natural images. As such, it is well-suited forreal-time application and in a variety of computationally hard instances of themaximum rectangle problem.", "output": "\"Sliced\" Subwindow Search: a Sublinear-complexity Solution to the Maximum Rectangle Problem."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Zero-Shot Learning (ZSL) learns models for recognizing new classes. One ofthe main challenges in ZSL is the domain discrepancy caused by the categoryinconsistency between training and testing data. Domain adaptation is the mostintuitive way to address this challenge. However, existing domain adaptationtechniques cannot be directly applied into ZSL due to the disjoint label spacebetween source and target domains. This work proposes the TransferrableSemantic-Visual Relation (TSVR) approach towards transductive ZSL. TSVRredefines image recognition as predicting the similarity/dissimilarity labelsfor semantic-visual fusions consisting of class attributes and visual features.After the above transformation, the source and target domains can have the samelabel space, which hence enables to quantify domain discrepancy. For theredefined problem, the number of similar semantic-visual pairs is significantlysmaller than that of dissimilar ones. To this end, we further propose to useDomain-Specific Batch Normalization to align the domain discrepancy.", "output": "Learning Cross-domain Semantic-Visual Relationships for Transductive Zero-Shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural network pruning is a fruitful area of research with surging interestin high sparsity regimes. Benchmarking in this domain heavily relies onfaithful representation of the sparsity of subnetworks, which has beentraditionally computed as the fraction of removed connections (directsparsity). This definition, however, fails to recognize unpruned parametersthat detached from input or output layers of underlying subnetworks,potentially underestimating actual effective sparsity: the fraction ofinactivated connections. While this effect might be negligible for moderatelypruned networks (up to 10-100 compression rates), we find that it plays anincreasing role for thinner subnetworks, greatly distorting comparison betweendifferent pruning algorithms. For example, we show that effective compressionof a randomly pruned LeNet-300-100 can be orders of magnitude larger than itsdirect counterpart, while no discrepancy is ever observed when using SynFlowfor pruning [Tanaka et al., 2020]. In this work, we adopt the lens of effectivesparsity to reevaluate several recent pruning algorithms on common benchmarkarchitectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that theirabsolute and relative performance changes dramatically in this new and moreappropriate framework. To aim for effective, rather than direct, sparsity, wedevelop a low-cost extension to most pruning algorithms. Further, equipped witheffective sparsity as a reference frame, we partially reconfirm that randompruning with appropriate sparsity allocation across layers performs as well orbetter than more sophisticated algorithms for pruning at initialization [Su etal., 2020]. In response to this observation, using a simple analogy of pressuredistribution in coupled cylinders from physics, we design novel layerwisesparsity quotas that outperform all existing baselines in the context of randompruning.", "output": "Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We demonstrate the benefit of using an ultimate skip (US) connection forfacial expression synthesis using generative adversarial networks (GAN). Adirect connection transfers identity, facial, and color details from input tooutput while suppressing artifacts. The intermediate layers can therefore focuson expression generation only. This leads to a light-weight US-GAN modelcomprised of encoding layers, a single residual block, decoding layers, and anultimate skip connection from input to output. US-GAN has $3times$ fewerparameters than state-of-the-art models and is trained on $2$ orders ofmagnitude smaller dataset. It yields $7%$ increase in face verification score(FVS) and $27%$ decrease in average content distance (ACD). Based on arandomized user-study, US-GAN outperforms the state of the art by $25%$ inface realism, $43%$ in expression quality, and $58%$ in identitypreservation.", "output": "US-GAN: On the importance of Ultimate Skip Connection for Facial Expression Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Super-resolution as an ill-posed problem has many high-resolution candidatesfor a low-resolution input. However, the popular $ell_1$ loss used to best fitthe given HR image fails to consider this fundamental property ofnon-uniqueness in image restoration. In this work, we fix the missing piece in$ell_1$ loss by formulating super-resolution with neural networks as aprobabilistic model. It shows that $ell_1$ loss is equivalent to a degradedlikelihood function that removes the randomness from the learning process. Byintroducing a data-adaptive random variable, we present a new objectivefunction that aims at minimizing the expectation of the reconstruction errorover all plausible solutions. The experimental results show consistentimprovements on mainstream architectures, with no extra parameter or computingcost at inference time.", "output": "Revisiting L1 Loss in Super-Resolution: A Probabilistic View and Beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient detection and description of geometric regions in images is aprerequisite in visual systems for localization and mapping. Such systems stillrely on traditional hand-crafted methods for efficient generation oflightweight descriptors, a common limitation of the more powerful neuralnetwork models that come with high compute and specific hardware requirements.In this paper, we focus on the adaptations required by detection anddescription neural networks to enable their use in computationally limitedplatforms such as robots, mobile, and augmented reality devices. To that end,we investigate and adapt network quantization techniques to accelerateinference and enable its use on compute limited platforms. In addition, werevisit common practices in descriptor quantization and propose the use of abinary descriptor normalization layer, enabling the generation of distinctivebinary descriptors with a constant number of ones. ZippyPoint, our efficientquantized network with binary descriptors, improves the network runtime speed,the descriptor matching speed, and the 3D model size, by at least an order ofmagnitude when compared to full-precision counterparts. These improvements comeat a minor performance degradation as evaluated on the tasks of homographyestimation, visual localization, and map-free visual relocalization. Code andmodels are available at ", "output": "ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article presents a synthetic distracted driving (SynDD2 - a continuum ofSynDD1) dataset for machine learning models to detect and analyze drivers'various distracted behavior and different gaze zones. We collected the data ina stationary vehicle using three in-vehicle cameras positioned at locations: onthe dashboard, near the rearview mirror, and on the top right-side windowcorner. The dataset contains two activity types: distracted activities and gazezones for each participant, and each activity type has two sets: withoutappearance blocks and with appearance blocks such as wearing a hat orsunglasses. The order and duration of each activity for each participant arerandom. In addition, the dataset contains manual annotations for each activity,having its start and end time annotated. Researchers could use this dataset toevaluate the performance of machine learning algorithms to classify variousdistracting activities and gaze zones of drivers.", "output": "Synthetic Distracted Driving (SynDD2) dataset for analyzing distracted behaviors and various gaze zones of a driver."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deformable linear objects (e.g., cables, ropes, and threads) commonly appearin our everyday lives. However, perception of these objects and the study ofphysical interaction with them is still a growing area. There have already beensuccessful methods to model and track deformable linear objects. However, thenumber of methods that can automatically extract the initial conditions innon-trivial situations for these methods has been limited, and they have beenintroduced to the community only recently. On the other hand, while physicalinteraction with these objects has been done with ground manipulators, therehave not been any studies on physical interaction and manipulation of thedeformable linear object with aerial robots.This workshop describes our recent work on detecting deformable linearobjects, which uses the segmentation output of the existing methods to providethe initialization required by the tracking methods automatically. It workswith crossings and can fill the gaps and occlusions in the segmentation andoutput the model desirable for physical interaction and simulation. Then wepresent our work on using the method for tasks such as routing and manipulationwith the ground and aerial robots. We discuss our feasibility analysis onextending the physical interaction with these objects to aerial manipulationapplications.", "output": "Detection and Physical Interaction with Deformable Linear Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a multi-sensor fusion method for capturing challenging 3D humanmotions with accurate consecutive local poses and global trajectories inlarge-scale scenarios, only using single LiDAR and 4 IMUs, which are set upconveniently and worn lightly. Specifically, to fully utilize the globalgeometry information captured by LiDAR and local dynamic motions captured byIMUs, we design a two-stage pose estimator in a coarse-to-fine manner, wherepoint clouds provide the coarse body shape and IMU measurements optimize thelocal actions. Furthermore, considering the translation deviation caused by theview-dependent partial point cloud, we propose a pose-guided translationcorrector. It predicts the offset between captured points and the real rootlocations, which makes the consecutive movements and trajectories more preciseand natural. Moreover, we collect a LiDAR-IMU multi-modal mocap dataset, LIPD,with diverse human actions in long-range scenarios. Extensive quantitative andqualitative experiments on LIPD and other open datasets all demonstrate thecapability of our approach for compelling motion capture in large-scalescenarios, which outperforms other methods by an obvious margin. We willrelease our code and captured dataset to stimulate future research.", "output": "LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse Inertial and LiDAR Sensors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present a novel end-to-end group collaborative learningnetwork, termed GCoNet+, which can effectively and efficiently (250 fps)identify co-salient objects in natural scenes. The proposed GCoNet+ achievesthe new state-of-the-art performance for co-salient object detection (CoSOD)through mining consensus representations based on the following two essentialcriteria: 1) intra-group compactness to better formulate the consistency amongco-salient objects by capturing their inherent shared attributes using ournovel group affinity module (GAM); 2) inter-group separability to effectivelysuppress the influence of noisy objects on the output by introducing our newgroup collaborating module (GCM) conditioning on the inconsistent consensus. Tofurther improve the accuracy, we design a series of simple yet effectivecomponents as follows: i) a recurrent auxiliary classification module (RACM)promoting model learning at the semantic level; ii) a confidence enhancementmodule (CEM) assisting the model in improving the quality of the finalpredictions; and iii) a group-based symmetric triplet (GST) loss guiding themodel to learn more discriminative features. Extensive experiments on threechallenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate thatour GCoNet+ outperforms the existing 12 cutting-edge models. Code has beenreleased at ", "output": "GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deepfakes pose a serious threat to digital well-being by fuelingmisinformation. As deepfakes get harder to recognize with the naked eye, humanusers become increasingly reliant on deepfake detection models to decide if avideo is real or fake. Currently, models yield a prediction for a video'sauthenticity, but do not integrate a method for alerting a human user. Weintroduce a framework for amplifying artifacts in deepfake videos to make themmore detectable by people. We propose a novel, semi-supervised ArtifactAttention module, which is trained on human responses to create attention mapsthat highlight video artifacts. These maps make two contributions. First, theyimprove the performance of our deepfake detection classifier. Second, theyallow us to generate novel \"Deepfake Caricatures\": transformations of thedeepfake that exacerbate artifacts to improve human detection. In a user study,we demonstrate that Caricatures greatly increase human detection, across videopresentation times and user engagement levels. Overall, we demonstrate thesuccess of a human-centered approach to designing deepfake mitigation methods.", "output": "Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Drawing images of characters with desired poses is an essential but laborioustask in anime production. Assisting artists to create is a research hotspot inrecent years. In this paper, we present the Collaborative Neural Rendering(CoNR) method, which creates new images for specified poses from a fewreference images (AKA Character Sheets). In general, the diverse hairstyles andgarments of anime characters defies the employment of universal body modelslike SMPL, which fits in most nude human shapes. To overcome this, CoNR uses acompact and easy-to-obtain landmark encoding to avoid creating a unified UVmapping in the pipeline. In addition, the performance of CoNR can besignificantly improved when referring to multiple reference images, thanks tofeature space cross-view warping in a carefully designed neural network.Moreover, we have collected a character sheet dataset containing over 700,000hand-drawn and synthesized images of diverse poses to facilitate research inthis area. Our code and demo are available at", "output": "Collaborative Neural Rendering using Anime Character Sheets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present ShapeCrafter, a neural network for recursive text-conditioned 3Dshape generation. Existing methods to generate text-conditioned 3D shapesconsume an entire text prompt to generate a 3D shape in a single step. However,humans tend to describe shapes recursively-we may start with an initialdescription and progressively add details based on intermediate results. Tocapture this recursive process, we introduce a method to generate a 3D shapedistribution, conditioned on an initial phrase, that gradually evolves as morephrases are added. Since existing datasets are insufficient for training thisapproach, we present Text2Shape++, a large dataset of 369K shape-text pairsthat supports recursive shape generation. To capture local details that areoften used to refine shape descriptions, we build on top of vector-quantizeddeep implicit functions that generate a distribution of high-quality shapes.Results show that our method can generate shapes consistent with textdescriptions, and shapes evolve gradually as more phrases are added. Our methodsupports shape editing, extrapolation, and can enable new applications inhuman-machine collaboration for creative design.", "output": "ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a method to map 2D image observations of a scene to a persistent3D scene representation, enabling novel view synthesis and disentangledrepresentation of the movable and immovable components of the scene. Motivatedby the bird's-eye-view (BEV) representation commonly used in vision androbotics, we propose conditional neural groundplans, ground-aligned 2D featuregrids, as persistent and memory-efficient scene representations. Our method istrained self-supervised from unlabeled multi-view observations usingdifferentiable rendering, and learns to complete geometry and appearance ofoccluded regions. In addition, we show that we can leverage multi-view videosat training time to learn to separately reconstruct static and movablecomponents of the scene from a single image at test time. The ability toseparately reconstruct movable objects enables a variety of downstream tasksusing simple heuristics, such as extraction of object-centric 3Drepresentations, novel view synthesis, instance-level segmentation, 3D boundingbox prediction, and scene editing. This highlights the value of neuralgroundplans as a backbone for efficient 3D scene understanding models.", "output": "Neural Groundplans: Persistent Neural Scene Representations from a Single Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pathologists need to combine information from differently stained pathologyslices for accurate diagnosis. Deformable image registration is a necessarytechnique for fusing multi-modal pathology slices. This paper proposes a hybriddeep feature-based deformable image registration framework for stainedpathology samples. We first extract dense feature points via the detector-basedand detector-free deep learning feature networks and perform points matching.Then, to further reduce false matches, an outlier detection method combiningthe isolation forest statistical model and the local affine correction model isproposed. Finally, the interpolation method generates the deformable vectorfield for pathology image registration based on the above matching points. Weevaluate our method on the dataset of the Non-rigid Histology ImageRegistration (ANHIR) challenge, which is co-organized with the IEEE ISBI 2019conference. Our technique outperforms the traditional approaches by 17% withthe Average-Average registration target error (rTRE) reaching 0.0034. Theproposed method achieved state-of-the-art performance and ranked 1st inevaluating the test dataset. The proposed hybrid deep feature-basedregistration method can potentially become a reliable method for pathologyimage registration.", "output": "A Hybrid Deep Feature-Based Deformable Image Registration Method for Pathology Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image clustering is an important and open-challenging task in computervision. Although many methods have been proposed to solve the image clusteringtask, they only explore images and uncover clusters according to the imagefeatures, thus being unable to distinguish visually similar but semanticallydifferent images. In this paper, we propose to investigate the task of imageclustering with the help of a visual-language pre-training model. Differentfrom the zero-shot setting, in which the class names are known, we only knowthe number of clusters in this setting. Therefore, how to map images to aproper semantic space and how to cluster images from both image and semanticspaces are two key problems. To solve the above problems, we propose a novelimage clustering method guided by the visual-language pre-training model CLIP,named textbf{Semantic-Enhanced Image Clustering (SIC)}. In this new method, wepropose a method to map the given images to a proper semantic space first andefficient methods to generate pseudo-labels according to the relationshipsbetween images and semantics. Finally, we propose performing clustering withconsistency learning in both image space and semantic space, in aself-supervised learning fashion. The theoretical result of convergenceanalysis shows that our proposed method can converge at a sublinear speed.Theoretical analysis of expectation risk also shows that we can reduce theexpected risk by improving neighborhood consistency, increasing predictionconfidence, or reducing neighborhood imbalance. Experimental results on fivebenchmark datasets clearly show the superiority of our new method.", "output": "Semantic-Enhanced Image Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Adversarial Networks (GANs) have shown compelling results invarious tasks and applications in recent years. However, mode collapse remainsa critical problem in GANs. In this paper, we propose a novel training pipelineto address the mode collapse issue of GANs. Different from existing methods, wepropose to generalize the discriminator as feature embedding and maximize theentropy of distributions in the embedding space learned by the discriminator.Specifically, two regularization terms, i.e., Deep Local Linear Embedding(DLLE) and Deep Isometric feature Mapping (DIsoMap), are designed to encouragethe discriminator to learn the structural information embedded in the data,such that the embedding space learned by the discriminator can be well-formed.Based on the well-learned embedding space supported by the discriminator, anon-parametric entropy estimator is designed to efficiently maximize theentropy of embedding vectors, playing as an approximation of maximizing theentropy of the generated distribution. By improving the discriminator andmaximizing the distance of the most similar samples in the embedding space, ourpipeline effectively reduces the mode collapse without sacrificing the qualityof generated samples. Extensive experimental results show the effectiveness ofour method, which outperforms the GAN baseline, MaF-GAN on CelebA (9.13 vs.12.43 in FID) and surpasses the recent state-of-the-art energy-based model onthe ANIME-FACE dataset (2.80 vs. 2.26 in Inception score). The code isavailable at ", "output": "Combating Mode Collapse in GANs via Manifold Entropy Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a simple yet effective framework MaskCLIP, whichincorporates a newly proposed masked self-distillation into contrastivelanguage-image pretraining. The core idea of masked self-distillation is todistill representation from a full image to the representation predicted from amasked image. Such incorporation enjoys two vital benefits. First, maskedself-distillation targets local patch representation learning, which iscomplementary to vision-language contrastive focusing on text-relatedrepresentation. Second, masked self-distillation is also consistent withvision-language contrastive from the perspective of training objective as bothutilize the visual encoder for feature aligning, and thus is able to learnlocal semantics getting indirect supervision from the language. We providespecially designed experiments with a comprehensive analysis to validate thetwo benefits. Symmetrically, we also introduce the local semantic supervisioninto the text branch, which further improves the pretraining performance. Withextensive experiments, we show that MaskCLIP, when applied to variouschallenging downstream tasks, achieves superior results in linear probing,finetuning, and zero-shot performance with the guidance of the languageencoder. Code will be release at url{", "output": "MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep generative models (DGMs) are data-eager because learning a complex modelon limited data suffers from a large variance and easily overfits. Inspired bythe classical perspective of the bias-variance tradeoff, we propose regularizeddeep generative model (Reg-DGM), which leverages a nontransferable pre-trainedmodel to reduce the variance of generative modeling with limited data.Formally, Reg-DGM optimizes a weighted sum of a certain divergence and theexpectation of an energy function, where the divergence is between the data andthe model distributions, and the energy function is defined by the pre-trainedmodel w.r.t. the model distribution. We analyze a simple yet representativeGaussian-fitting case to demonstrate how the weighting hyperparameter tradesoff the bias and the variance. Theoretically, we characterize the existence andthe uniqueness of the global minimum of Reg-DGM in a non-parametric setting andprove its convergence with neural networks trained by gradient-based methods.Empirically, with various pre-trained feature extractors and a data-dependentenergy function, Reg-DGM consistently improves the generation performance ofstrong DGMs with limited data and achieves competitive results to thestate-of-the-art methods. Our implementation is available at", "output": "Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurately and timely detecting multiscale small objects that contain tens ofpixels from remote sensing images (RSI) remains challenging. Most of theexisting solutions primarily design complex deep neural networks to learnstrong feature representations for objects separated from the background, whichoften results in a heavy computation burden. In this article, we propose anaccurate yet fast object detection method for RSI, named SuperYOLO, which fusesmultimodal data and performs high-resolution (HR) object detection onmultiscale objects by utilizing the assisted super resolution (SR) learning andconsidering both the detection accuracy and computation cost. First, we utilizea symmetric compact multimodal fusion (MF) to extract supplementary informationfrom various data for improving small object detection in RSI. Furthermore, wedesign a simple and flexible SR branch to learn HR feature representations thatcan discriminate small objects from vast backgrounds with low-resolution (LR)input, thus further improving the detection accuracy. Moreover, to avoidintroducing additional computation, the SR branch is discarded in the inferencestage, and the computation of the network model is reduced due to the LR input.Experimental results show that, on the widely used VEDAI RS dataset, SuperYOLOachieves an accuracy of 75.09% (in terms of mAP50 ), which is more than 10%higher than the SOTA large models, such as YOLOv5l, YOLOv5x, and RS designedYOLOrs. Meanwhile, the parameter size and GFLOPs of SuperYOLO are about 18times and 3.8 times less than YOLOv5x. Our proposed model shows a favorableaccuracy and speed tradeoff compared to the state-of-the-art models. The codewill be open-sourced at ", "output": "SuperYOLO: Super Resolution Assisted Object Detection in Multimodal Remote Sensing Imagery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Photo retouching is a difficult task for novice users as it requires expertknowledge and advanced tools. Photographers often spend a great deal of timegenerating high-quality retouched photos with intricate details. In this paper,we introduce a one-shot learning based technique to automatically retouchdetails of an input image based on just a single pair of before and afterexample images. Our approach provides accurate and generalizable detail edittransfer to new images. We achieve these by proposing a new representation forimage to image maps. Specifically, we propose neural field based transformationblending in the patch space for defining patch to patch transformations foreach frequency band. This parametrization of the map with anchortransformations and associated weights, and spatio-spectral localized patches,allows us to capture details well while staying generalizable. We evaluate ourtechnique both on known ground truth filters and artist retouching edits. Ourmethod accurately transfers complex detail retouching edits.", "output": "One-shot Detail Retouching with Patch Space Neural Transformation Blending."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most computer vision research focuses on datasets containing thousands ofimages of commonplace objects. However, many high-impact datasets, such asthose in medicine and the geosciences, contain fine-grain objects that requiredomain-expert knowledge to recognize and are time-consuming to collect andannotate. As a result, these datasets contain few labeled images, and currentmachine vision models cannot train intensively on them. Originally introducedto correct large-language models, model-editing techniques in machine learninghave been shown to improve model performance using only small amounts of dataand additional training. Using a Mask R-CNN to segment ancient reef fossils inrock sample images, we present a two-part paradigm to improve fossilsegmentation with few labeled images: we first identify model weaknesses usingimage perturbations and then mitigate those weaknesses using model editing.Specifically, we apply domain-informed image perturbations to expose the MaskR-CNN's inability to distinguish between different classes of fossils and itsinconsistency in segmenting fossils with different textures. To address theseshortcomings, we extend an existing model-editing method for correctingsystematic mistakes in image classification to image segmentation with noadditional labeled data needed and show its effectiveness in decreasingconfusion between different kinds of fossils. We also highlight the bestsettings for model editing in our situation: making a single edit using allrelevant pixels in one image (vs. using multiple images, multiple edits, orfewer pixels). Though we focus on fossil segmentation, our approach may beuseful in other similar fine-grain segmentation problems where data is limited.", "output": "Improving Data-Efficient Fossil Segmentation via Model Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scarcity of data and incremental learning of new tasks pose two majorbottlenecks for many modern computer vision algorithms. The phenomenon ofcatastrophic forgetting, i.e., the model's inability to classify previouslylearned data after training with new batches of data, is a major challenge.Conventional methods address catastrophic forgetting while compromising thecurrent session's training. Generative replay-based approaches, such asgenerative adversarial networks (GANs), have been proposed to mitigatecatastrophic forgetting, but training GANs with few samples may lead toinstability. To address these challenges, we propose a novel method thatimproves classification robustness by identifying a better embedding spaceusing an improved contrasting loss. Our approach retains previously acquiredknowledge in the embedding space, even when trained with new classes, byupdating previous session class prototypes to represent the true class mean,which is crucial for our nearest class mean classification strategy. Wedemonstrate the effectiveness of our method by showing that the embedding spaceremains intact after training the model with new classes and outperformsexisting state-of-the-art algorithms in terms of accuracy across differentsessions.", "output": "Prototypical quadruplet for few-shot class incremental learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain shift is a well-known problem in the medical imaging community. Inparticular, for endoscopic image analysis where the data can have differentmodalities the performance of deep learning (DL) methods gets adverselyaffected. In other words, methods developed on one modality cannot be used fora different modality. However, in real clinical settings, endoscopists switchbetween modalities for better mucosal visualisation. In this paper, we explorethe domain generalisation technique to enable DL methods to be used in suchscenarios. To this extend, we propose to use super pixels generated with SimpleLinear Iterative Clustering (SLIC) which we refer to as \"SUPRA\" for SUPeRpixelAugmented method. SUPRA first generates a preliminary segmentation mask makinguse of our new loss \"SLICLoss\" that encourages both an accurate andcolor-consistent segmentation. We demonstrate that SLICLoss when combined withBinary Cross Entropy loss (BCE) can improve the model's generalisability withdata that presents significant domain shift. We validate this novel compoundloss on a vanilla U-Net using the EndoUDA dataset, which contains images forBarret's Esophagus and polyps from two modalities. We show that our methodyields an improvement of nearly 20% in the target domain set compared to thebaseline.", "output": "SUPRA: Superpixel Guided Loss for Improved Multi-modal Segmentation in Endoscopy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-modality (MM) image fusion aims to render fused images that maintainthe merits of different modalities, e.g., functional highlight and detailedtextures. To tackle the challenge in modeling cross-modality features anddecomposing desirable modality-specific and modality-shared features, wepropose a novel Correlation-Driven feature Decomposition Fusion (CDDFuse)network. Firstly, CDDFuse uses Restormer blocks to extract cross-modalityshallow features. We then introduce a dual-branch Transformer-CNN featureextractor with Lite Transformer (LT) blocks leveraging long-range attention tohandle low-frequency global features and Invertible Neural Networks (INN)blocks focusing on extracting high-frequency local information. Acorrelation-driven loss is further proposed to make the low-frequency featurescorrelated while the high-frequency features uncorrelated based on the embeddedinformation. Then, the LT-based global fusion and INN-based local fusion layersoutput the fused image. Extensive experiments demonstrate that our CDDFuseachieves promising results in multiple fusion tasks, including infrared-visibleimage fusion and medical image fusion. We also show that CDDFuse can boost theperformance in downstream infrared-visible semantic segmentation and objectdetection in a unified benchmark. The code is available at", "output": "CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monocular 3D human pose estimation is quite challenging due to the inherentambiguity and occlusion, which often lead to high uncertainty andindeterminacy. On the other hand, diffusion models have recently emerged as aneffective tool for generating high-quality images from noise. Inspired by theircapability, we explore a novel pose estimation framework (DiffPose) thatformulates 3D pose estimation as a reverse diffusion process. We incorporatenovel designs into our DiffPose to facilitate the diffusion process for 3D poseestimation: a pose-specific initialization of pose uncertainty distributions, aGaussian Mixture Model-based forward diffusion process, and acontext-conditioned reverse diffusion process. Our proposed DiffPosesignificantly outperforms existing methods on the widely used pose estimationbenchmarks Human3.6M and MPI-INF-3DHP. Project page:", "output": "DiffPose: Toward More Reliable 3D Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Climate change is expected to intensify and increase extreme events in theweather cycle. Since this has a significant impact on various sectors of ourlife, recent works are concerned with identifying and predicting such extremeevents from Earth observations. With respect to wildfire danger forecasting,previous deep learning approaches duplicate static variables along the timedimension and neglect the intrinsic differences between static and dynamicvariables. Furthermore, most existing multi-branch architectures lose theinterconnections between the branches during the feature learning stage. Toaddress these issues, this paper proposes a 2D/3D two-branch convolutionalneural network (CNN) with a Location-aware Adaptive Normalization layer (LOAN).Using LOAN as a building block, we can modulate the dynamic featuresconditional on their geographical locations. Thus, our approach considersfeature properties as a unified yet compound 2D/3D model. Besides, we proposeusing the sinusoidal-based encoding of the day of the year to provide the modelwith explicit temporal information about the target day within the year. Ourexperimental results show a better performance of our approach than otherbaselines on the challenging FireCube dataset. The results show thatlocation-aware adaptive feature normalization is a promising technique to learnthe relation between dynamic variables and their geographic locations, which ishighly relevant for areas where remote sensing data builds the basis foranalysis. The source code is available at ", "output": "Location-aware Adaptive Normalization: A Deep Learning Approach For Wildfire Danger Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we investigate improving the generalizability of GAN-generatedimage detectors by performing data augmentation in the fingerprint domain.Specifically, we first separate the fingerprints and contents of theGAN-generated images using an autoencoder based GAN fingerprint extractor,followed by random perturbations of the fingerprints. Then the originalfingerprints are substituted with the perturbed fingerprints and added to theoriginal contents, to produce images that are visually invariant but withdistinct fingerprints. The perturbed images can successfully imitate imagesgenerated by different GANs to improve the generalization of the detectors,which is demonstrated by the spectra visualization. To our knowledge, we arethe first to conduct data augmentation in the fingerprint domain. Our workexplores a novel prospect that is distinct from previous works on spatial andfrequency domain augmentation. Extensive cross-GAN experiments demonstrate theeffectiveness of our method compared to the state-of-the-art methods indetecting fake images generated by unknown GANs.", "output": "General GAN-generated image detection by data augmentation in fingerprint domain."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervision for metric learning has long been given in the form ofequivalence between human-labeled classes. Although this type of supervisionhas been a basis of metric learning for decades, we argue that it hindersfurther advances in the field. In this regard, we propose a new regularizationmethod, dubbed HIER, to discover the latent semantic hierarchy of trainingdata, and to deploy the hierarchy to provide richer and more fine-grainedsupervision than inter-class separability induced by common metric learninglosses.HIER achieves this goal with no annotation for the semantic hierarchybut by learning hierarchical proxies in hyperbolic spaces. The hierarchicalproxies are learnable parameters, and each of them is trained to serve as anancestor of a group of data or other proxies to approximate the semantichierarchy among them. HIER deals with the proxies along with data in hyperbolicspace since the geometric properties of the space are well-suited to representtheir hierarchical structure. The efficacy of HIER is evaluated on fourstandard benchmarks, where it consistently improved the performance ofconventional methods when integrated with them, and consequently achieved thebest records, surpassing even the existing hyperbolic metric learningtechnique, in almost all settings.", "output": "HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model bias triggered by long-tailed data has been widely studied. However,measure based on the number of samples cannot explicate three phenomenasimultaneously: (1) Given enough data, the classification performance gain ismarginal with additional samples. (2) Classification performance decaysprecipitously as the number of training samples decreases when there isinsufficient data. (3) Model trained on sample-balanced datasets still hasdifferent biases for different classes. In this work, we define and quantifythe semantic scale of classes, which is used to measure the feature diversityof classes. It is exciting to find experimentally that there is a marginaleffect of semantic scale, which perfectly describes the first two phenomena.Further, the quantitative measurement of semantic scale imbalance is proposed,which can accurately reflect model bias on multiple datasets, even onsample-balanced data, revealing a novel perspective for the study of classimbalance. Due to the prevalence of semantic scale imbalance, we proposesemantic-scale-balanced learning, including a general loss improvement schemeand a dynamic re-weighting training framework that overcomes the challenge ofcalculating semantic scales in real-time during iterations. Comprehensiveexperiments show that dynamic semantic-scale-balanced learning consistentlyenables the model to perform superiorly on large-scale long-tailed andnon-long-tailed natural and medical datasets, which is a good starting pointfor mitigating the prevalent but unnoticed model bias.", "output": "Delving into Semantic Scale Imbalance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current dataset collection methods typically scrape large amounts of datafrom the web. While this technique is extremely scalable, data collected inthis way tends to reinforce stereotypical biases, can contain personallyidentifiable information, and typically originates from Europe and NorthAmerica. In this work, we rethink the dataset collection paradigm and introduceGeoDE, a geographically diverse dataset with 61,940 images from 40 classes and6 world regions, and no personally identifiable information, collected throughcrowd-sourcing. We analyse GeoDE to understand differences in images collectedin this manner compared to web-scraping. Despite the smaller size of thisdataset, we demonstrate its use as both an evaluation and training dataset,highlight shortcomings in current models, as well as show improved performanceswhen even small amounts of GeoDE (1000 - 2000 images per region) are added to atraining dataset. We release the full dataset and code at", "output": "GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-driving vehicles rely on urban street maps for autonomous navigation. Inthis paper, we introduce Pix2Map, a method for inferring urban street maptopology directly from ego-view images, as needed to continually update andexpand existing maps. This is a challenging task, as we need to infer a complexurban road topology directly from raw image data. The main insight of thispaper is that this problem can be posed as cross-modal retrieval by learning ajoint, cross-modal embedding space for images and existing maps, represented asdiscrete graphs that encode the topological layout of the visual surroundings.We conduct our experimental evaluation using the Argoverse dataset and showthat it is indeed possible to accurately retrieve street maps corresponding toboth seen and unseen roads solely from image data. Moreover, we show that ourretrieved maps can be used to update or expand existing maps and even showproof-of-concept results for visual localization and image retrieval fromspatial graphs.", "output": "Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Infrared small target detection (ISTD) under complex backgrounds is adifficult problem, for the differences between targets and backgrounds are noteasy to distinguish. Background reconstruction is one of the methods to dealwith this problem. This paper proposes an ISTD method based on backgroundreconstruction called Dynamic Background Reconstruction (DBR). DBR consists ofthree modules: a dynamic shift window module (DSW), a background reconstructionmodule (BR), and a detection head (DH). BR takes advantage of VisionTransformers in reconstructing missing patches and adopts a grid maskingstrategy with a masking ratio of 50% to reconstruct clean backgrounds withouttargets. To avoid dividing one target into two neighboring patches, resultingin reconstructing failure, DSW is performed before input embedding. DSWcalculates offsets, according to which infrared images dynamically shift. Toreduce False Positive (FP) cases caused by regarding reconstruction errors astargets, DH utilizes a structure of densely connected Transformer to furtherimprove the detection performance. Experimental results show that DBR achievesthe best F1-score on the two ISTD datasets, MFIRST (64.10%) and SIRST(75.01%).", "output": "Dynamic Background Reconstruction via MAE for Infrared Small Target Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gestures that accompany speech are an essential part of natural and efficientembodied human communication. The automatic generation of such co-speechgestures is a long-standing problem in computer animation and is considered anenabling technology in film, games, virtual social spaces, and for interactionwith social robots. The problem is made challenging by the idiosyncratic andnon-periodic nature of human co-speech gesture motion, and by the greatdiversity of communicative functions that gestures encompass. Gesturegeneration has seen surging interest recently, owing to the emergence of moreand larger datasets of human gesture motion, combined with strides indeep-learning-based generative models, that benefit from the growingavailability of data. This review article summarizes co-speech gesturegeneration research, with a particular focus on deep generative models. First,we articulate the theory describing human gesticulation and how it complementsspeech. Next, we briefly discuss rule-based and classical statistical gesturesynthesis, before delving into deep learning approaches. We employ the choiceof input modalities as an organizing principle, examining systems that generategestures from audio, text, and non-linguistic input. We also chronicle theevolution of the related training data sets in terms of size, diversity, motionquality, and collection method. Finally, we identify key research challenges ingesture generation, including data availability and quality; producinghuman-like motion; grounding the gesture in the co-occurring speech ininteraction with other speakers, and in the environment; performing gestureevaluation; and integration of gesture synthesis into applications. Wehighlight recent approaches to tackling the various key challenges, as well asthe limitations of these approaches, and point toward areas of futuredevelopment.", "output": "A Comprehensive Review of Data-Driven Co-Speech Gesture Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We explore three applications of Min-Max-Jump distance (MMJ distance).MMJ-based K-means revises K-means with MMJ distance. MMJ-based Silhouettecoefficient revises Silhouette coefficient with MMJ distance. We also testedthe Clustering with Neural Network and Index (CNNI) model with MMJ-basedSilhouette coefficient. In the last application, we tested using Min-Max-Jumpdistance for predicting labels of new points, after a clustering analysis ofdata. Result shows Min-Max-Jump distance achieves good performances in all thethree proposed applications.", "output": "Min-Max-Jump distance and its applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classical adversarial attacks for Face Recognition (FR) models typicallygenerate discrete examples for target identity with a single state image.However, such paradigm of point-wise attack exhibits poor generalizationagainst numerous unknown states of identity and can be easily defended. In thispaper, by rethinking the inherent relationship between the face of targetidentity and its variants, we introduce a new pipeline of Generalized ManifoldAdversarial Attack (GMAA) to achieve a better attack performance by expandingthe attack range. Specifically, this expansion lies on two aspects - GMAA notonly expands the target to be attacked from one to many to encourage a goodgeneralization ability for the generated adversarial examples, but it alsoexpands the latter from discrete points to manifold by leveraging the domainknowledge that face expression change can be continuous, which enhances theattack effect as a data augmentation mechanism did. Moreover, we further designa dual supervision with local and global constraints as a minor contribution toimprove the visual quality of the generated adversarial examples. Wedemonstrate the effectiveness of our method based on extensive experiments, andreveal that GMAA promises a semantic continuous adversarial space with a highergeneralization ability and visual quality", "output": "Discrete Point-wise Attack Is Not Enough: Generalized Manifold Adversarial Attack for Face Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatial and temporal modeling is one of the most core aspects of few-shotaction recognition. Most previous works mainly focus on long-term temporalrelation modeling based on high-level spatial representations, withoutconsidering the crucial low-level spatial features and short-term temporalrelations. Actually, the former feature could bring rich local semanticinformation, and the latter feature could represent motion characteristics ofadjacent frames, respectively. In this paper, we propose SloshNet, a newframework that revisits the spatial and temporal modeling for few-shot actionrecognition in a finer manner. First, to exploit the low-level spatialfeatures, we design a feature fusion architecture search module toautomatically search for the best combination of the low-level and high-levelspatial features. Next, inspired by the recent transformer, we introduce along-term temporal modeling module to model the global temporal relations basedon the extracted spatial appearance features. Meanwhile, we design anothershort-term temporal modeling module to encode the motion characteristicsbetween adjacent frame representations. After that, the final predictions canbe obtained by feeding the embedded rich spatial-temporal features to a commonframe-level class prototype matcher. We extensively validate the proposedSloshNet on four few-shot action recognition datasets, includingSomething-Something V2, Kinetics, UCF101, and HMDB51. It achieves favorableresults against state-of-the-art methods in all datasets.", "output": "Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The loss function for bounding box regression (BBR) is essential to objectdetection. Its good definition will bring significant performance improvementto the model. Most existing works assume that the examples in the training dataare high-quality and focus on strengthening the fitting ability of BBR loss. Ifwe blindly strengthen BBR on low-quality examples, it will jeopardizelocalization performance. Focal-EIoU v1 was proposed to solve this problem, butdue to its static focusing mechanism (FM), the potential of non-monotonic FMwas not fully exploited. Based on this idea, we propose an IoU-based loss witha dynamic non-monotonic FM named Wise-IoU (WIoU). The dynamic non-monotonic FMuses the outlier degree instead of IoU to evaluate the quality of anchor boxesand provides a wise gradient gain allocation strategy. This strategy reducesthe competitiveness of high-quality anchor boxes while also reducing theharmful gradient generated by low-quality examples. This allows WIoU to focuson ordinary-quality anchor boxes and improve the detector's overallperformance. When WIoU is applied to the state-of-the-art real-time detectorYOLOv7, the AP-75 on the MS-COCO dataset is improved from 53.03% to 54.50%.Code is available at ", "output": "Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently 3D-aware GAN methods with neural radiance field have developedrapidly. However, current methods model the whole image as an overall neuralradiance field, which limits the partial semantic editability of syntheticresults. Since NeRF renders an image pixel by pixel, it is possible to splitNeRF in the spatial dimension. We propose a Compositional Neural Radiance Field(CNeRF) for semantic 3D-aware portrait synthesis and manipulation. CNeRFdivides the image by semantic regions and learns an independent neural radiancefield for each region, and finally fuses them and renders the complete image.Thus we can manipulate the synthesized semantic regions independently, whilefixing the other parts unchanged. Furthermore, CNeRF is also designed todecouple shape and texture within each semantic region. Compared tostate-of-the-art 3D-aware GAN methods, our approach enables fine-grainedsemantic region manipulation, while maintaining high-quality 3D-consistentsynthesis. The ablation studies show the effectiveness of the structure andloss function used by our method. In addition real image inversion and cartoonportrait 3D editing experiments demonstrate the application potential of ourmethod.", "output": "Semantic 3D-aware Portrait Synthesis and Manipulation Based on Compositional Neural Radiance Field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-mydriatic retinal color fundus photography (CFP) is widely available dueto the advantage of not requiring pupillary dilation, however, is prone to poorquality due to operators, systemic imperfections, or patient-related causes.Optimal retinal image quality is mandated for accurate medical diagnoses andautomated analyses. Herein, we leveraged the Optimal Transport (OT) theory topropose an unpaired image-to-image translation scheme for mapping low-qualityretinal CFPs to high-quality counterparts. Furthermore, to improve theflexibility, robustness, and applicability of our image enhancement pipeline inthe clinical practice, we generalized a state-of-the-art model-based imagereconstruction method, regularization by denoising, by plugging in priorslearned by our OT-guided image-to-image translation network. We named it asregularization by enhancing (RE). We validated the integrated framework, OTRE,on three publicly available retinal image datasets by assessing the qualityafter enhancement and their performance on various downstream tasks, includingdiabetic retinopathy grading, vessel segmentation, and diabetic lesionsegmentation. The experimental results demonstrated the superiority of ourproposed framework over some state-of-the-art unsupervised competitors and astate-of-the-art supervised method.", "output": "OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To date, query performance prediction (QPP) in the context of content-basedimage retrieval remains a largely unexplored task, especially in thequery-by-example scenario, where the query is an image. To boost theexploration of the QPP task in image retrieval, we propose the first benchmarkfor image query performance prediction (iQPP). First, we establish a set offour data sets (PASCAL VOC 2012, Caltech-101, ROxford5k and RParis6k) andestimate the ground-truth difficulty of each query as the average precision orthe precision@k, using two state-of-the-art image retrieval models. Next, wepropose and evaluate novel pre-retrieval and post-retrieval query performancepredictors, comparing them with existing or adapted (from text to image)predictors. The empirical results show that most predictors do not generalizeacross evaluation scenarios. Our comprehensive experiments indicate that iQPPis a challenging benchmark, revealing an important research gap that needs tobe addressed in future work. We release our code and data as open source at to foster future research.", "output": "iQPP: A Benchmark for Image Query Performance Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An robust sign language recognition system can greatly alleviatecommunication barriers, particularly for people who struggle with verbalcommunication. This is crucial for human growth and progress as it enables theexpression of thoughts, feelings, and ideas. However, sign recognition is acomplex task that faces numerous challenges such as same gesture patterns formultiple signs, lighting, clothing, carrying conditions, and the presence oflarge poses, as well as illumination discrepancies across different views.Additionally, the absence of an extensive Bangla sign language video datasetmakes it even more challenging to operate recognition systems, particularlywhen utilizing deep learning techniques. In order to address this issue,firstly, we created a large-scale dataset called the MVBSL-W50, which comprises50 isolated words across 13 categories. Secondly, we developed anattention-based Bi-GRU model that captures the temporal dynamics of poseinformation for individuals communicating through sign language. The proposedmodel utilizes human pose information, which has shown to be successful inanalyzing sign language patterns. By focusing solely on movement informationand disregarding body appearance and environmental factors, the model issimplified and can achieve a speedier performance. The accuracy of the model isreported to be 85.64%.", "output": "Word level Bangla Sign Language Dataset for Continuous BSL Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic communication is considered the future of mobile communication,which aims to transmit data beyond Shannon's theorem of communications bytransmitting the semantic meaning of the data rather than the bit-by-bitreconstruction of the data at the receiver's end. The semantic communicationparadigm aims to bridge the gap of limited bandwidth problems in modernhigh-volume multimedia application content transmission. Integrating AItechnologies with the 6G communications networks paved the way to developsemantic communication-based end-to-end communication systems. In this study,we have implemented a semantic communication-based end-to-end imagetransmission system, and we discuss potential design considerations indeveloping semantic communication systems in conjunction with physical channelcharacteristics. A Pre-trained GAN network is used at the receiver as thetransmission task to reconstruct the realistic image based on the Semanticsegmented image at the receiver input. The semantic segmentation task at thetransmitter (encoder) and the GAN network at the receiver (decoder) is trainedon a common knowledge base, the COCO-Stuff dataset. The research shows that theresource gain in the form of bandwidth saving is immense when transmitting thesemantic segmentation map through the physical channel instead of the groundtruth image in contrast to conventional communication systems. Furthermore, theresearch studies the effect of physical channel distortions and quantizationnoise on semantic communication-based multimedia content transmission.", "output": "Wireless End-to-End Image Transmission System using Semantic Communications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While multi-modal foundation models pre-trained on large-scale data have beensuccessful in natural language understanding and vision recognition, their usein medical domains is still limited due to the fine-grained nature of medicaltasks and the high demand for domain knowledge. To address this challenge, wepropose a novel approach called Knowledge-enhanced Auto Diagnosis (KAD) whichleverages existing medical domain knowledge to guide vision-languagepre-training using paired chest X-rays and radiology reports. We evaluate KADon {four} external X-ray datasets and demonstrate that its zero-shotperformance is not only comparable to that of fully-supervised models, but alsosuperior to the average of three expert radiologists for three (out of five)pathologies with statistical significance. Moreover, when few-shot annotationis available, KAD outperforms all existing approaches in fine-tuning settings,demonstrating its potential for application in different clinical scenarios.", "output": "Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the outstanding capability for data generation, Generative AdversarialNetworks (GANs) have attracted considerable attention in unsupervised learning.However, training GANs is difficult, since the training distribution is dynamicfor the discriminator, leading to unstable image representation. In this paper,we address the problem of training GANs from a novel perspective, emph{i.e.,}robust image classification. Motivated by studies on robust imagerepresentation, we propose a simple yet effective module, namely AdaptiveMix,for GANs, which shrinks the regions of training data in the imagerepresentation space of the discriminator. Considering it is intractable todirectly bound feature space, we propose to construct hard samples and narrowdown the feature distance between hard and easy samples. The hard samples areconstructed by mixing a pair of training images. We evaluate the effectivenessof our AdaptiveMix with widely-used and state-of-the-art GAN architectures. Theevaluation results demonstrate that our AdaptiveMix can facilitate the trainingof GANs and effectively improve the image quality of generated samples. We alsoshow that our AdaptiveMix can be further applied to image classification andOut-Of-Distribution (OOD) detection tasks, by equipping it withstate-of-the-art methods. Extensive experiments on seven publicly availabledatasets show that our method effectively boosts the performance of baselines.The code is publicly available at", "output": "Improving GAN Training via Feature Space Shrinkage."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to discover abstract physical concepts and understand how theywork in the world through observing lies at the core of human intelligence. Theacquisition of this ability is based on compositionally perceiving theenvironment in terms of objects and relations in an unsupervised manner. Recentapproaches learn object-centric representations and capture visually observableconcepts of objects, e.g., shape, size, and location. In this paper, we take astep forward and try to discover and represent intrinsic physical concepts suchas mass and charge. We introduce the PHYsical Concepts Inference NEtwork(PHYCINE), a system that infers physical concepts in different abstract levelswithout supervision. The key insights underlining PHYCINE are two-fold,commonsense knowledge emerges with prediction, and physical concepts ofdifferent abstract levels should be reasoned in a bottom-up fashion. Empiricalevaluation demonstrates that variables inferred by our system work inaccordance with the properties of the corresponding physical concepts. We alsoshow that object representations containing the discovered physical conceptsvariables could help achieve better performance in causal reasoning tasks,i.e., ComPhy.", "output": "Intrinsic Physical Concepts Discovery with Object-Centric Predictive Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For a considerable time, researchers have focused on developing a method thatestablishes a deep connection between the generative diffusion model andmathematical physics. Despite previous efforts, progress has been limited tothe pursuit of a single specialized method. In order to advance theinterpretability of diffusion models and explore new research directions, it isessential to establish a unified ODE-style generative diffusion model. Such amodel should draw inspiration from physical models and possess a cleargeometric meaning. This paper aims to identify various physical models that aresuitable for constructing ODE-style generative diffusion models accurately froma mathematical perspective. We then summarize these models into a unifiedmethod. Additionally, we perform a case study where we use the theoreticalmodel identified by our method to develop a range of new diffusion modelmethods, and conduct experiments. Our experiments on CIFAR-10 demonstrate theeffectiveness of our approach. We have constructed a computational frameworkthat attains highly proficient results with regards to image generation speed,alongside an additional model that demonstrates exceptional performance in bothInception score and FID score. These results underscore the significance of ourmethod in advancing the field of diffusion models.", "output": "Interpretable ODE-style Generative Diffusion Model via Force Field Construction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lithography is fundamental to integrated circuit fabrication, necessitatinglarge computation overhead. The advancement of machine learning (ML)-basedlithography models alleviates the trade-offs between manufacturing processexpense and capability. However, all previous methods regard the lithographysystem as an image-to-image black box mapping, utilizing network parameters tolearn by rote mappings from massive mask-to-aerial or mask-to-resist imagepairs, resulting in poor generalization capability. In this paper, we propose anew ML-based paradigm disassembling the rigorous lithographic model intonon-parametric mask operations and learned optical kernels containingdeterminant source, pupil, and lithography information. By optimizingcomplex-valued neural fields to perform optical kernel regression fromcoordinates, our method can accurately restore lithography system using asmall-scale training dataset with fewer parameters, demonstrating superiorgeneralization capability as well. Experiments show that our framework can use31% of parameters while achieving 69$times$ smaller mean squared error with1.3$times$ higher throughput than the state-of-the-art.", "output": "Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have achieved remarkable success in text-to-imagegeneration, enabling the creation of high-quality images from text prompts orother modalities. However, existing methods for customizing these models arelimited by handling multiple personalized subjects and the risk of overfitting.Moreover, their large number of parameters is inefficient for model storage. Inthis paper, we propose a novel approach to address these limitations inexisting text-to-image diffusion models for personalization. Our methodinvolves fine-tuning the singular values of the weight matrices, leading to acompact and efficient parameter space that reduces the risk of overfitting andlanguage-drifting. We also propose a Cut-Mix-Unmix data-augmentation techniqueto enhance the quality of multi-subject image generation and a simpletext-based image editing framework. Our proposed SVDiff method has asignificantly smaller model size (1.7MB for StableDiffusion) compared toexisting methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making itmore practical for real-world applications.", "output": "SVDiff: Compact Parameter Space for Diffusion Fine-Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exemplar-based class-incremental learning (CIL) finetunes the model with allsamples of new classes but few-shot exemplars of old classes in eachincremental phase, where the \"few-shot\" abides by the limited memory budget. Inthis paper, we break this \"few-shot\" limit based on a simple yet surprisinglyeffective idea: compressing exemplars by downsampling non-discriminative pixelsand saving \"many-shot\" compressed exemplars in the memory. Without needing anymanual annotation, we achieve this compression by generating 0-1 masks ondiscriminative pixels from class activation maps (CAM). We propose an adaptivemask generation model called class-incremental masking (CIM) to explicitlyresolve two difficulties of using CAM: 1) transforming the heatmaps of CAM to0-1 masks with an arbitrary threshold leads to a trade-off between the coverageon discriminative pixels and the quantity of exemplars, as the total memory isfixed; and 2) optimal thresholds vary for different object classes, which isparticularly obvious in the dynamic environment of CIL. We optimize the CIMmodel alternatively with the conventional CIL model through a bileveloptimization problem. We conduct extensive experiments on high-resolution CILbenchmarks including Food-101, ImageNet-100, and ImageNet-1000, and show thatusing the compressed exemplars by CIM can achieve a new state-of-the-art CILaccuracy, e.g., 4.8 percentage points higher than FOSTER on 10-PhaseImageNet-1000. Our code is available at ", "output": "Class-Incremental Exemplar Compression for Class-Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent success of the CLIP model has shown its potential to be applied toa wide range of vision and language tasks. However this only establishesembedding space relationship of language to images, not to the video domain. Inthis paper, we propose a novel approach to map video embedding space to naturallangugage. We propose a two-stage approach that first extracts visual featuresfrom each frame of a video using a pre-trained CNN, and then uses the CLIPmodel to encode the visual features for the video domain, along with thecorresponding text descriptions. We evaluate our method on two benchmarkdatasets, UCF101 and HMDB51, and achieve state-of-the-art performance on bothtasks.", "output": "Learning video embedding space with Natural Language Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have witnessed great progress in deep neural networks forreal-time applications. However, most existing works do not explicitly considerthe general case where the device's state and the available resources fluctuateover time, and none of them investigate or address the impact of varyingcomputational resources for online video understanding tasks. This paperproposes a System-status-aware Adaptive Network (SAN) that considers thedevice's real-time state to provide high-quality predictions with low delay.Usage of our agent's policy improves efficiency and robustness to fluctuationsof the system status. On two widely used video understanding tasks, SAN obtainsstate-of-the-art performance while constantly keeping processing delays low.Moreover, training such an agent on various types of hardware configurations isnot easy as the labeled training data might not be available, or can becomputationally prohibitive. To address this challenging problem, we propose aMeta Self-supervised Adaptation (MSA) method that adapts the agent's policy tonew hardware configurations at test-time, allowing for easy deployment of themodel onto other unseen hardware platforms.", "output": "System-status-aware Adaptive Network for Online Streaming Video Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image arbitrary-scale super-resolution (MIASSR) has recently gainedwidespread attention, aiming to super sample medical volumes at arbitraryscales via a single model. However, existing MIASSR methods face two majorlimitations: (i) reliance on high-resolution (HR) volumes and (ii) limitedgeneralization ability, which restricts their application in various scenarios.To overcome these limitations, we propose Cube-based Neural Radiance Field(CuNeRF), a zero-shot MIASSR framework that can yield medical images atarbitrary scales and viewpoints in a continuous domain. Unlike existing MIASSRmethods that fit the mapping between low-resolution (LR) and HR volumes, CuNeRFfocuses on building a coordinate-intensity continuous representation from LRvolumes without the need for HR references. This is achieved by the proposeddifferentiable modules: including cube-based sampling, isotropic volumerendering, and cube-based hierarchical rendering. Through extensive experimentson magnetic resource imaging (MRI) and computed tomography (CT) modalities, wedemonstrate that CuNeRF outperforms state-of-the-art MIASSR methods. CuNeRFyields better visual verisimilitude and reduces aliasing artifacts at variousupsampling factors. Moreover, our CuNeRF does not need any LR-HR trainingpairs, which is more flexible and easier to be used than others. Our code willbe publicly available soon.", "output": "CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image Arbitrary-Scale Super Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Compared with previous two-stream trackers, the recent one-stream trackingpipeline, which allows earlier interaction between the template and searchregion, has achieved a remarkable performance gain. However, existingone-stream trackers always let the template interact with all parts inside thesearch region throughout all the encoder layers. This could potentially lead totarget-background confusion when the extracted feature representations are notsufficiently discriminative. To alleviate this issue, we propose a generalizedrelation modeling method based on adaptive token division. The proposed methodis a generalized formulation of attention-based relation modeling forTransformer tracking, which inherits the merits of both previous two-stream andone-stream pipelines whilst enabling more flexible relation modeling byselecting appropriate search tokens to interact with template tokens. Anattention masking strategy and the Gumbel-Softmax technique are introduced tofacilitate the parallel computation and end-to-end learning of the tokendivision module. Extensive experiments show that our method is superior to thetwo-stream and one-stream pipelines and achieves state-of-the-art performanceon six challenging benchmarks with a real-time running speed.", "output": "Generalized Relation Modeling for Transformer Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networkshave proven to be highly effective, achieving state-of-the-art results. Thisstudy introduces a novel streaming architecture based toolflow for mapping suchmodels onto FPGAs considering the model's inherent characteristics and thefeatures of the targeted FPGA device. The HARFLOW3D toolflow takes as input a3D CNN in ONNX format and a description of the FPGA characteristics, generatinga design that minimizes the latency of the computation. The toolflow iscomprised of a number of parts, including i) a 3D CNN parser, ii) a performanceand resource model, iii) a scheduling algorithm for executing 3D models on thegenerated hardware, iv) a resource-aware optimization engine tailored for 3Dmodels, v) an automated mapping to synthesizable code for FPGAs. The ability ofthe toolflow to support a broad range of models and devices is shown through anumber of experiments on various 3D CNN and FPGA system pairs. Furthermore, thetoolflow has produced high-performing results for 3D CNN models that have notbeen mapped to FPGAs before, demonstrating the potential of FPGA-based systemsin this space. Overall, HARFLOW3D has demonstrated its ability to delivercompetitive latency compared to a range of state-of-the-art hand-tunedapproaches being able to achieve up to 5$times$ better performance compared tosome of the existing works.", "output": "HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformations based on domain expertise (expert transformations), such asrandom-resized-crop and color-jitter, have proven critical to the success ofcontrastive learning techniques such as SimCLR. Recently, several attempts havebeen made to replace such domain-specific, human-designed transformations withgenerated views that are learned. However for imagery data, so far none ofthese view-generation methods has been able to outperform experttransformations. In this work, we tackle a different question: instead ofreplacing expert transformations with generated views, can we constructivelyassimilate generated views with expert transformations? We answer this questionin the affirmative and propose a view generation method and a simple, effectiveassimilation method that together improve the state-of-the-art by up to ~3.6%on three different datasets. Importantly, we conduct a detailed empirical studythat systematically analyzes a range of view generation and assimilationmethods and provides a holistic picture of the efficacy of learned views incontrastive representation learning.", "output": "Constructive Assimilation: Boosting Contrastive Learning Performance through View Generation Strategies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, deep learning has been widely used in SAR ATR and achievedexcellent performance on the MSTAR dataset. However, due to constrained imagingconditions, MSTAR has data biases such as background correlation, i.e.,background clutter properties have a spurious correlation with target classes.Deep learning can overfit clutter to reduce training errors. Therefore, thedegree of overfitting for clutter reflects the non-causality of deep learningin SAR ATR. Existing methods only qualitatively analyze this phenomenon. Inthis paper, we quantify the contributions of different regions to targetrecognition based on the Shapley value. The Shapley value of clutter measuresthe degree of overfitting. Moreover, we explain how data bias and model biascontribute to non-causality. Concisely, data bias leads to comparablesignal-to-clutter ratios and clutter textures in training and test sets. Andvarious model structures have different degrees of overfitting for thesebiases. The experimental results of various models under standard operatingconditions on the MSTAR dataset support our conclusions. Our code is availableat ", "output": "Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Scene Representation Transformer (SRT) is a recent method to render novelviews at interactive rates. Since SRT uses camera poses with respect to anarbitrarily chosen reference camera, it is not invariant to the order of theinput views. As a result, SRT is not directly applicable to large-scale sceneswhere the reference frame would need to be changed regularly. In this work, wepropose Relative Pose Attention SRT (RePAST): Instead of fixing a referenceframe at the input, we inject pairwise relative camera pose informationdirectly into the attention mechanism of the Transformers. This leads to amodel that is by definition invariant to the choice of any global referenceframe, while still retaining the full capabilities of the original method.Empirical results show that adding this invariance to the model does not leadto a loss in quality. We believe that this is a step towards applying fullylatent transformer-based rendering methods to large-scale scenes.", "output": "RePAST: Relative Pose Attention Scene Representation Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we address the task of finding representative subsets of pointsin a 3D point cloud by means of a point-wise ordering. Only a few works havetried to address this challenging vision problem, all with the help of hard toobtain point and cloud labels. Different from these works, we introduce thetask of point-wise ordering in 3D point clouds through self-supervision, whichwe call self-ordering. We further contribute the first end-to-end trainablenetwork that learns a point-wise ordering in a self-supervised fashion. Itutilizes a novel differentiable point scoring-sorting strategy and itconstructs an hierarchical contrastive scheme to obtain self-supervisionsignals. We extensively ablate the method and show its scalability and superiorperformance even compared to supervised ordering methods on multiple datasetsand tasks including zero-shot ordering of point clouds from unseen categories.", "output": "Self-Ordering Point Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative A.I. models have emerged as versatile tools across diverseindustries, with applications in privacy-preserving data sharing, computationalart, personalization of products and services, and immersive entertainment.Here, we introduce a new privacy concern in the adoption and use of generativeA.I. models: that of coincidental generation, where a generative model's outputis similar enough to an existing entity, beyond those represented in thedataset used to train the model, to be mistaken for it. Consider, for example,synthetic portrait generators, which are today deployed in commercialapplications such as virtual modeling agencies and synthetic stock photography.Due to the low intrinsic dimensionality of human face perception, everysynthetically generated face will coincidentally resemble an actual person.Such examples of coincidental generation all but guarantee the misappropriationof likeness and expose organizations that use generative A.I. to legal andregulatory risk.", "output": "Coincidental Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Safety is the primary priority of autonomous driving. Nevertheless, nopublished dataset currently supports the direct and explainable safetyevaluation for autonomous driving. In this work, we propose DeepAccident, alarge-scale dataset generated via a realistic simulator containing diverseaccident scenarios that frequently occur in real-world driving. The proposedDeepAccident dataset contains 57K annotated frames and 285K annotated samples,approximately 7 times more than the large-scale nuScenes dataset with 40kannotated samples. In addition, we propose a new task, end-to-end motion andaccident prediction, based on the proposed dataset, which can be used todirectly evaluate the accident prediction ability for different autonomousdriving algorithms. Furthermore, for each scenario, we set four vehicles alongwith one infrastructure to record data, thus providing diverse viewpoints foraccident scenarios and enabling V2X (vehicle-to-everything) research onperception and prediction tasks. Finally, we present a baseline V2X model namedV2XFormer that demonstrates superior performance for motion and accidentprediction and 3D object detection compared to the single-vehicle model.", "output": "DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Skin lesion recognition using deep learning has made remarkable progress, andthere is an increasing need for deploying these systems in real-worldscenarios. However, recent research has revealed that deep neural networks forskin lesion recognition may overly depend on disease-irrelevant image artifacts(i.e. dark corners, dense hairs), leading to poor generalization in unseenenvironments. To address this issue, we propose a novel domain generalizationmethod called EPVT, which involves embedding prompts into the visiontransformer to collaboratively learn knowledge from diverse domains.Concretely, EPVT leverages a set of domain prompts, each of which plays as adomain expert, to capture domain-specific knowledge; and a shared prompt forgeneral knowledge over the entire dataset. To facilitate knowledge sharing andthe interaction of different prompts, we introduce a domain prompt generatorthat enables low-rank multiplicative updates between domain prompts and theshared prompt. A domain mixup strategy is additionally devised to reduce theco-occurring artifacts in each domain, which allows for more flexible decisionmargins and mitigates the issue of incorrectly assigned domain labels.Experiments on four out-of-distribution datasets and six different biased ISICdatasets demonstrate the superior generalization ability of EPVT in skin lesionrecognition across various environments. Our code and dataset will be releasedat ", "output": "EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a complete software pipeline for revealing the hidden texts of theHerculaneum papyri using X-ray CT images. This enhanced virtual unwrappingpipeline combines machine learning with a novel geometric framework linking 3Dand 2D images. We also present EduceLab-Scrolls, a comprehensive open datasetrepresenting two decades of research effort on this problem. EduceLab-Scrollscontains a set of volumetric X-ray CT images of both small fragments andintact, rolled scrolls. The dataset also contains 2D image labels that are usedin the supervised training of an ink detection model. Labeling is enabled byaligning spectral photography of scroll fragments with X-ray CT images of thesame fragments, thus creating a machine-learnable mapping between image spacesand modalities. This alignment permits supervised learning for the detection of\"invisible\" carbon ink in X-ray CT, a task that is \"impossible\" even for humanexpert labelers. To our knowledge, this is the first aligned dataset of itskind and is the largest dataset ever released in the heritage domain. Ourmethod is capable of revealing accurate lines of text on scroll fragments withknown ground truth. Revealed text is verified using visual confirmation,quantitative image metrics, and scholarly review. EduceLab-Scrolls has alsoenabled the discovery, for the first time, of hidden texts from the Herculaneumpapyri, which we present here. We anticipate that the EduceLab-Scrolls datasetwill generate more textual discovery as research continues.", "output": "EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri using X-ray CT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently developed text-to-image diffusion models make it easy to edit orcreate high-quality images. Their ease of use has raised concerns about thepotential for malicious editing or deepfake creation. Imperceptibleperturbations have been proposed as a means of protecting images from maliciousediting by preventing diffusion models from generating realistic images.However, we find that the aforementioned perturbations are not robust to JPEGcompression, which poses a major weakness because of the common usage andavailability of JPEG. We discuss the importance of robustness for additiveimperceptible perturbations and encourage alternative approaches to protectimages against editing.", "output": "JPEG Compressed Images Can Bypass Protections Against AI Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Multi-Source 3D (MS3D), a new self-training pipeline forunsupervised domain adaptation in 3D object detection. Despite the remarkableaccuracy of 3D detectors, they often overfit to specific domain biases, leadingto suboptimal performance in various sensor setups and environments. Existingmethods typically focus on adapting a single detector to the target domain,overlooking the fact that different detectors possess distinct expertise ondifferent unseen domains. MS3D leverages this by combining differentpre-trained detectors from multiple source domains and incorporating temporalinformation to produce high-quality pseudo-labels for fine-tuning. Our proposedKernel-Density Estimation (KDE) Box Fusion method fuses box proposals frommultiple domains to obtain pseudo-labels that surpass the performance of thebest source domain detectors. MS3D exhibits greater robustness to domain shiftsand produces accurate pseudo-labels over greater distances, making itwell-suited for high-to-low beam domain adaptation and vice versa. Our methodachieved state-of-the-art performance on all evaluated datasets, and wedemonstrate that the choice of pre-trained source detectors has minimal impacton the self-training result, making MS3D suitable for real-world applications.", "output": "MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a new synthesis-based approach for batch imageprocessing. Unlike existing tools that can only apply global edits to theentire image, our method can apply fine-grained edits to individual objectswithin the image. For example, our method can selectively blur or crop specificobjects that have a certain property. To facilitate such fine-grained imageediting tasks, we propose a neuro-symbolic domain-specific language (DSL) thatcombines pre-trained neural networks for image classification with otherlanguage constructs that enable symbolic reasoning. Our method canautomatically learn programs in this DSL from user demonstrations by utilizinga novel synthesis algorithm. We have implemented the proposed technique in atool called ImageEye and evaluated it on 50 image editing tasks. Our evaluationshows that ImageEye is able to automate 96% of these tasks.", "output": "ImageEye: Batch Image Processing Using Program Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human movement analysis is a key area of research in robotics, biomechanics,and data science. It encompasses tracking, posture estimation, and movementsynthesis. While numerous methodologies have evolved over time, a systematicand quantitative evaluation of these approaches using verifiable ground truthdata of three-dimensional human movement is still required to define thecurrent state of the art. This paper presents seven datasets recorded usinginertial-based motion capture. The datasets contain professional gesturescarried out by industrial operators and skilled craftsmen performed in realconditions in-situ. The datasets were created with the intention of being usedfor research in human motion modeling, analysis, and generation. The protocolsfor data collection are described in detail, and a preliminary analysis of thecollected data is provided as a benchmark. The Gesture Operational Model, ahybrid stochastic-biomechanical approach based on kinematic descriptors, isutilized to model the dynamics of the experts' movements and createmathematical representations of their motion trajectories for analysis andquantifying their body dexterity. The models allowed accurate the generation ofhuman professional poses and an intuitive description of how body jointscooperate and change over time through the performance of the task.", "output": "Motion Capture Benchmark of Real Industrial Tasks and Traditional Crafts for Human Movement Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Applying machine learning to biological sequences - DNA, RNA and protein -has enormous potential to advance human health, environmental sustainability,and fundamental biological understanding. However, many existing machinelearning methods are ineffective or unreliable in this problem domain. We studythese challenges theoretically, through the lens of kernels. Methods based onkernels are ubiquitous: they are used to predict molecular phenotypes, designnovel proteins, compare sequence distributions, and more. Many methods that donot use kernels explicitly still rely on them implicitly, including a widevariety of both deep learning and physics-based techniques. While kernels forother types of data are well-studied theoretically, the structure of biologicalsequence space (discrete, variable length sequences), as well as biologicalnotions of sequence similarity, present unique mathematical challenges. Weformally analyze how well kernels for biological sequences can approximatearbitrary functions on sequence space and how well they can distinguishdifferent sequence distributions. In particular, we establish conditions underwhich biological sequence kernels are universal, characteristic and metrize thespace of distributions. We show that a large number of existing kernel-basedmachine learning methods for biological sequences fail to meet our conditionsand can as a consequence fail severely. We develop straightforward andcomputationally tractable ways of modifying existing kernels to satisfy ourconditions, imbuing them with strong guarantees on accuracy and reliability.Our proof techniques build on and extend the theory of kernels with discretemasses. We illustrate our theoretical results in simulation and on realbiological data sets.", "output": "Biological Sequence Kernels with Guaranteed Flexibility."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "UCI WorldTour races, the premier men's elite road cycling tour, are gruelingevents that put riders' physical fitness and endurance to the test. The coachesof Team Jumbo-Visma have long been responsible for predicting the energy needsof each rider of the Dutch team for every race on the calendar. Those must beestimated to ensure riders have the energy and resources necessary to maintaina high level of performance throughout a race. This task, however, is bothtime-consuming and challenging, as it requires precise estimates of race speedand power output. Traditionally, the approach to predicting energy needs hasrelied on coaches' judgement and experience, but this method has itslimitations and often leads to inaccurate predictions. In this paper, wepropose a new, more effective approach to predicting energy needs for cyclingraces. By predicting the speed and power with regression models, we provide thecoaches with calorie needs estimate for each individual rider per stageinstantly. In addition, we compare methods to quantify uncertainty inestimating the speed and power of Team Jumbo-Visma riders for cycling races.The empirical analysis of the jackknife+, jackknife-minmax,jackknife-minmax-after-bootstrap, CV+, CV-minmax, conformalized quantileregression (CQR) and inductive conformal prediction (ICP) methods in conformalprediction reveals all methods except minmax based methods achieve validprediction intervals while producing prediction intervals tight enough to beused for decision making. Furthermore, methods computing prediction intervalsof fixed size produce significantly tighter intervals for low significancevalue. Among the methods computing intervals of varying length across the inputspace, namely the CQR and ICP methods, ICP computes tighter predictionintervals at larger significance level.", "output": "Conformal Regression in Calorie Prediction for Team Jumbo-Visma."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A prediction model is most useful if it generalizes beyond the developmentdata with external validations, but to what extent should it generalize remainsunclear. In practice, prediction models are externally validated using datafrom very different settings, including populations from other health systemsor countries, with predictably poor results. This may not be a fair reflectionof the performance of the model which was designed for a specific targetpopulation or setting, and may be stretching the expected modelgeneralizability. To address this, we suggest to externally validate a modelusing new data from the target population to ensure clear implications ofvalidation performance on model reliability, whereas model generalizability tobroader settings should be carefully investigated during model developmentinstead of explored post-hoc. Based on this perspective, we propose a roadmapthat facilitates the development and application of reliable, fair, andtrustworthy artificial intelligence prediction models.", "output": "A roadmap to fair and trustworthy prediction model validation in healthcare."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exploring the expected quantizing scheme with suitable mixed-precision policyis the key point to compress deep neural networks (DNNs) in high efficiency andaccuracy. This exploration implies heavy workloads for domain experts, and anautomatic compression method is needed. However, the huge search space of theautomatic method introduces plenty of computing budgets that make the automaticprocess challenging to be applied in real scenarios. In this paper, we proposean end-to-end framework named AutoQNN, for automatically quantizing differentlayers utilizing different schemes and bitwidths without any human labor.AutoQNN can seek desirable quantizing schemes and mixed-precision policies formainstream DNN models efficiently by involving three techniques: quantizingscheme search (QSS), quantizing precision learning (QPL), and quantizedarchitecture generation (QAG). QSS introduces five quantizing schemes anddefines three new schemes as a candidate set for scheme search, and then usesthe differentiable neural architecture search (DNAS) algorithm to seek thelayer- or model-desired scheme from the set. QPL is the first method to learnmixed-precision policies by reparameterizing the bitwidths of quantizingschemes, to the best of our knowledge. QPL optimizes both classification lossand precision loss of DNNs efficiently and obtains the relatively optimalmixed-precision model within limited model size and memory footprint. QAG isdesigned to convert arbitrary architectures into corresponding quantized oneswithout manual intervention, to facilitate end-to-end neural networkquantization. We have implemented AutoQNN and integrated it into Keras.Extensive experiments demonstrate that AutoQNN can consistently outperformstate-of-the-art quantization.", "output": "AutoQNN: An End-to-End Framework for Automatically Quantizing Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in generative artificial intelligence (AI) have capturedworldwide attention. Tools such as Dalle-2 and ChatGPT suggest that taskspreviously thought to be beyond the capabilities of AI may now augment theproductivity of creative media in various new ways, including through thegeneration of synthetic video. This research paper explores the utility ofusing AI-generated synthetic video to create viable educational content foronline educational settings. To date, there is limited research investigatingthe real-world educational value of AI-generated synthetic media. To addressthis gap, we examined the impact of using AI-generated synthetic video in anonline learning platform on both learners content acquisition and learningexperience. We took a mixed-method approach, randomly assigning adult learners(n=83) into one of two micro-learning conditions, collecting pre- andpost-learning assessments, and surveying participants on their learningexperience. The control condition included a traditionally produced instructorvideo, while the experimental condition included a synthetic video with arealistic AI-generated character. The results show that learners in bothconditions demonstrated significant improvement from pre- to post-learning(p&lt;.001), with no significant differences in gains between the two conditions(p=.80). In addition, no differences were observed in how learners perceivedthe traditional and synthetic videos. These findings suggest that AI-generatedsynthetic learning videos have the potential to be a viable substitute forvideos produced via traditional methods in online educational settings, makinghigh quality educational content more accessible across the globe.", "output": "Generative AI for learning: Investigating the potential of synthetic learning videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative modelling over continuous-time geometric constructs, a.k.a such ashandwriting, sketches, drawings etc., have been accomplished throughautoregressive distributions. Such strictly-ordered discrete factorizationhowever falls short of capturing key properties of chirographic data -- itfails to build holistic understanding of the temporal concept due to one-wayvisibility (causality). Consequently, temporal data has been modelled asdiscrete token sequences of fixed sampling rate instead of capturing the trueunderlying concept. In this paper, we introduce a powerful model-class namely\"Denoising Diffusion Probabilistic Models\" or DDPMs for chirographic data thatspecifically addresses these flaws. Our model named \"ChiroDiff\", beingnon-autoregressive, learns to capture holistic concepts and therefore remainsresilient to higher temporal sampling rate up to a good extent. Moreover, weshow that many important downstream utilities (e.g. conditional sampling,creative mixing) can be flexibly implemented using ChiroDiff. We further showsome unique use-cases like stochastic vectorization, de-noising/healing,abstraction are also possible with this model-class. We perform quantitativeand qualitative evaluation of our framework on relevant datasets and found itto be better or on par with competing approaches.", "output": "ChiroDiff: Modelling chirographic data with Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning models are often misspecified in the likelihood, which leadsto a lack of robustness in the predictions. In this paper, we introduce aframework for correcting likelihood misspecifications in several paradigmagnostic noisy prior models and test the model's ability to remove themisspecification. The \"ABC-GAN\" framework introduced is a novel generativemodeling paradigm, which combines Generative Adversarial Networks (GANs) andApproximate Bayesian Computation (ABC). This new paradigm assists the existingGANs by incorporating any subjective knowledge available about the modelingprocess via ABC, as a regularizer, resulting in a partially interpretable modelthat operates well under low data regimes. At the same time, unlike anyBayesian analysis, the explicit knowledge need not be perfect, since thegenerator in the GAN can be made arbitrarily complex. ABC-GAN eliminates theneed for summary statistics and distance metrics as the discriminatorimplicitly learns them and enables simultaneous specification of multiplegenerative models. The model misspecification is simulated in our experimentsby introducing noise of various biases and variances. The correction term islearnt via the ABC-GAN, with skip connections, referred to as skipGAN. Thestrength of the skip connection indicates the amount of correction needed orhow misspecified the prior model is. Based on a simple experimental setup, weshow that the ABC-GAN models not only correct the misspecification of theprior, but also perform as well as or better than the respective priors undernoisier conditions. In this proposal, we show that ABC-GANs get the best ofboth worlds.", "output": "Correcting Model Misspecification via Generative Adversarial Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Privacy-preserving nerual network inference has been well studied whilehomomorphic CNN training still remains an open challenging task. In this paper,we present a practical solution to implement privacy-preserving CNN trainingbased on mere Homomorphic Encryption (HE) technique. To our best knowledge,this is the first attempt successfully to crack this nut and no work everbefore has achieved this goal. Several techniques combine to make it done: (1)with transfer learning, privacy-preserving CNN training can be reduced tohomomorphic neural network training, or even multiclass logistic regression(MLR) training; (2) via a faster gradient variant called $texttt{QuadraticGradient}$, an enhanced gradient method for MLR with a state-of-the-artperformance in converge speed is applied in this work to achieve highperformance; (3) we employ the thought of transformation in mathematics totransform approximating Softmax function in encryption domain to thewell-studied approximation of Sigmoid function. A new type of loss function isalongside been developed to complement this change; and (4) we use a simple butflexible matrix-encoding method named $texttt{Volley Revolver}$ to manage thedata flow in the ciphertexts, which is the key factor to complete the wholehomomorphic CNN training. The complete, runnable C++ code to implement our workcan be found at:  select $texttt{REGNET_X_400MF}$ as our pre-train model for usingtransfer learning. We use the first 128 MNIST training images as training dataand the whole MNIST testing dataset as the testing data. The client only needsto upload 6 ciphertexts to the cloud and it takes $sim 21$ mins to perform 2iterations on a cloud with 64 vCPUs, resulting in a precision of $21.49%$.", "output": "Privacy-Preserving CNN Training with Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs), such as OpenAI's Codex, have demonstrated theirpotential to generate code from natural language descriptions across a widerange of programming tasks. Several benchmarks have recently emerged toevaluate the ability of LLMs to generate functionally correct code from naturallanguage intent with respect to a set of hidden test cases. This has enabledthe research community to identify significant and reproducible advancements inLLM capabilities. However, there is currently a lack of benchmark datasets forassessing the ability of LLMs to generate functionally correct code edits basedon natural language descriptions of intended changes. This paper aims toaddress this gap by motivating the problem NL2Fix of translating naturallanguage descriptions of code changes (namely bug fixes described in Issuereports in repositories) into correct code fixes. To this end, we introduceDefects4J-NL2Fix, a dataset of 283 Java programs from the popular Defects4Jdataset augmented with high-level descriptions of bug fixes, and empiricallyevaluate the performance of several state-of-the-art LLMs for the this task.Results show that these LLMS together are capable of generating plausible fixesfor 64.6% of the bugs, and the best LLM-based technique can achieve up to21.20% top-1 and 35.68% top-5 accuracy on this benchmark.", "output": "Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning from demonstrations (LfD) methods guide learning agents to a desiredsolution using demonstrations from a teacher. While some LfD methods can handlesmall mismatches in the action spaces of the teacher and student, here weaddress the case where the teacher demonstrates the task in an action spacethat can be substantially different from that of the student -- therebyinducing a large action space mismatch. We bridge this gap with a framework,Morphological Adaptation in Imitation Learning (MAIL), that allows training anagent from demonstrations by other agents with significantly differentmorphologies (from the student or each other). MAIL is able to learn fromsuboptimal demonstrations, so long as they provide some guidance towards adesired solution. We demonstrate MAIL on challenging household clothmanipulation tasks and introduce a new DRY CLOTH task -- cloth manipulation in3D task with obstacles. In these tasks, we train a visual control policy for arobot with one end-effector using demonstrations from a simulated agent withtwo end-effectors. MAIL shows up to 27% improvement over LfD and non-LfDbaselines. It is deployed to a real Franka Panda robot, and can handle multiplevariations in cloth properties (color, thickness, size, material) and pose(rotation and translation). We further show generalizability to transfers fromn-to-m end-effectors, in the context of a simple rearrangement task.", "output": "Bridging Action Space Mismatch in Learning from Demonstrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the success of deep-learning models in many tasks, there have beenconcerns about such models learning shortcuts, and their lack of robustness toirrelevant confounders. When it comes to models directly trained on humanfaces, a sensitive confounder is that of human identities. Many face-relatedtasks should ideally be identity-independent, and perform uniformly acrossdifferent individuals (i.e. be fair). One way to measure and enforce suchrobustness and performance uniformity is through enforcing it during training,assuming identity-related information is available at scale. However, due toprivacy concerns and also the cost of collecting such information, this isoften not the case, and most face datasets simply contain input images andtheir corresponding task-related labels. Thus, improving identity-relatedrobustness without the need for such annotations is of great importance. Here,we explore using face-recognition embedding vectors, as proxies for identities,to enforce such robustness. We propose to use the structure in theface-recognition embedding space, to implicitly emphasize rare samples withineach class. We do so by weighting samples according to their conditionalinverse density (CID) in the proxy embedding space. Our experiments suggestthat such a simple sample weighting scheme, not only improves the trainingrobustness, it often improves the overall performance as a result of suchrobustness. We also show that employing such constraints during trainingresults in models that are significantly less sensitive to different levels ofbias in the dataset.", "output": "Improving Identity-Robustness for Face Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans have a powerful and mysterious capacity to reason. By working througha series of purely mental steps, we can make inferences we would not be capableof making directly -- despite that fact that we get no additional data from theworld. Similarly, large language models can perform better at complex tasksthrough chain-of-thought reasoning, where they generate intermediate stepsbefore answering a question. We use language models to investigate thequestions of when and why reasoning is helpful, testing the hypothesis thatreasoning is effective when training data consisting of local clusters ofvariables that influence each other strongly. These training conditions enablethe chaining of accurate local inferences in order to estimate relationshipsbetween variables that were not seen together in training. We train anautoregressive transformer on samples from joint distributions defined by Bayesnets, but only include a subset of all the variables in each sample. We comparelanguage models' ability to match conditional probabilities both with andwithout intermediate reasoning steps, finding that intermediate steps help onlywhen the training data is locally structured with respect to dependenciesbetween variables. Furthermore, intermediate variables need to be relevant tothe relationship between observed information and target inferences. Ourresults illustrate how the statistical structure of training data drives theeffectiveness of reasoning step by step.", "output": "Why think step-by-step? Reasoning emerges from the locality of experience."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "StepMix is an open-source software package for the pseudo-likelihoodestimation (one-, two- and three-step approaches) of generalized finite mixturemodels (latent profile and latent class analysis) with external variables(covariates and distal outcomes). In many applications in social sciences, themain objective is not only to cluster individuals into latent classes, but alsoto use these classes to develop more complex statistical models. These modelsgenerally divide into a measurement model that relates the latent classes toobserved indicators, and a structural model that relates covariates and outcomevariables to the latent classes. The measurement and structural models can beestimated jointly using the so-called one-step approach or sequentially usingstepwise methods, which present significant advantages for practitionersregarding the interpretability of the estimated latent classes. In addition tothe one-step approach, StepMix implements the most important stepwiseestimation methods from the literature, including the bias-adjusted three-stepmethods with BCH and ML corrections and the more recent two-step approach.These pseudo-likelihood estimators are presented in this paper under a unifiedframework as specific expectation-maximization subroutines. To facilitate andpromote their adoption among the data science community, StepMix follows theobject-oriented design of the scikit-learn library and provides interfaces inboth Python and R.", "output": "StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Compiled binary executables are often the only available artifact in reverseengineering, malware analysis, and software systems maintenance. Unfortunately,the lack of semantic information like variable types makes comprehendingbinaries difficult. In efforts to improve the comprehensibility of binaries,researchers have recently used machine learning techniques to predict semanticinformation contained in the original source code. Chen et al. implementedDIRTY, a Transformer-based Encoder-Decoder architecture capable of augmentingdecompiled code with variable names and types by leveraging decompiler outputtokens and variable size information. Chen et al. were able to demonstrate asubstantial increase in name and type extraction accuracy on Hex-Raysdecompiler outputs compared to existing static analysis and AI-basedtechniques. We extend the original DIRTY results by re-training the DIRTY modelon a dataset produced by the open-source Ghidra decompiler. Although Chen etal. concluded that Ghidra was not a suitable decompiler candidate due to itsdifficulty in parsing and incorporating DWARF symbols during analysis, wedemonstrate that straightforward parsing of variable data generated by Ghidraresults in similar retyping performance. We hope this work inspires furtherinterest and adoption of the Ghidra decompiler for use in research projects.", "output": "Revisiting Deep Learning for Variable Type Recovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data prefetching is important for storage system optimization and accessperformance improvement. Traditional prefetchers work well for mining accesspatterns of sequential logical block address (LBA) but cannot handle complexnon-sequential patterns that commonly exist in real-world applications. Thestate-of-the-art (SOTA) learning-based prefetchers cover more LBA accesses.However, they do not adequately consider the spatial interdependencies betweenLBA deltas, which leads to limited performance and robustness. This paperproposes a novel Stream-Graph neural network-based Data Prefetcher (SGDP).Specifically, SGDP models LBA delta streams using a weighted directed graphstructure to represent interactive relations among LBA deltas and furtherextracts hybrid features by graph neural networks for data prefetching. Weconduct extensive experiments on eight real-world datasets. Empirical resultsverify that SGDP outperforms the SOTA methods in terms of the hit ratio by6.21%, the effective prefetching ratio by 7.00%, and speeds up inference timeby 3.13X on average. Besides, we generalize SGDP to different variants bydifferent stream constructions, further expanding its application scenarios anddemonstrating its robustness. SGDP offers a novel data prefetching solution andhas been verified in commercial hybrid storage systems in the experimentalphase. Our codes and appendix are available at", "output": "SGDP: A Stream-Graph Neural Network Based Data Prefetcher."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work we theoretically show that conservative objective models (COMs)for offline model-based optimisation (MBO) are a special kind of contrastivedivergence-based energy model, one where the energy function represents boththe unconditional probability of the input and the conditional probability ofthe reward variable. While the initial formulation only samples modes from itslearned distribution, we propose a simple fix that replaces its gradient ascentsampler with a Langevin MCMC sampler. This gives rise to a specialprobabilistic model where the probability of sampling an input is proportionalto its predicted reward. Lastly, we show that better samples can be obtained ifthe model is decoupled so that the unconditional and conditional probabilitiesare modelled separately.", "output": "Conservative objective models are a special kind of contrastive divergence-based energy model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Selective prediction aims to learn a reliable model that abstains from makingpredictions when the model uncertainty is high. These predictions can then bedeferred to a human expert for further evaluation. In many real-worldscenarios, however, the distribution of test data is different from thetraining data. This results in more inaccurate predictions, necessitatingincreased human labeling, which is difficult and expensive in many scenarios.Active learning circumvents this difficulty by only querying the mostinformative examples and, in several cases, has been shown to lower the overalllabeling effort. In this work, we bridge the gap between selective predictionand active learning, proposing a new learning paradigm called active selectiveprediction which learns to query more informative samples from the shiftedtarget domain while increasing accuracy and coverage. For this new problem, wepropose a simple but effective solution, ASPEST, that trains ensembles of modelsnapshots using self-training with their aggregated outputs as pseudo labels.Extensive experiments on several image, text and structured datasets withdomain shifts demonstrate that active selective prediction can significantlyoutperform prior work on selective prediction and active learning (e.g. on theMNIST$to$SVHN benchmark with the labeling budget of 100, ASPEST improves theAUC metric from 79.36% to 88.84%) and achieves more optimal utilization ofhumans in the loop.", "output": "ASPEST: Bridging the Gap Between Active Learning and Selective Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce OFTER, a time series forecasting pipeline tailored for mid-sizedmultivariate time series. OFTER utilizes the non-parametric models of k-nearestneighbors and Generalized Regression Neural Networks, integrated with adimensionality reduction component. To circumvent the curse of dimensionality,we employ a weighted norm based on a modified version of the maximalcorrelation coefficient. The pipeline we introduce is specifically designed foronline tasks, has an interpretable output, and is able to outperform severalstate-of-the art baselines. The computational efficacy of the algorithm, itsonline nature, and its ability to operate in low signal-to-noise regimes,render OFTER an ideal approach for financial multivariate time series problems,such as daily equity forecasting. Our work demonstrates that while deeplearning models hold significant promise for time series forecasting,traditional methods carefully integrating mainstream tools remain verycompetitive alternatives with the added benefits of scalability andinterpretability.", "output": "OFTER: An Online Pipeline for Time Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in Natural Language Processing (NLP) have led to thedevelopment of NLP-based recommender systems that have shown superiorperformance. However, current models commonly treat items as mere IDs and adoptdiscriminative modeling, resulting in limitations of (1) fully leveraging thecontent information of items and the language modeling capabilities of NLPmodels; (2) interpreting user interests to improve relevance and diversity; and(3) adapting practical circumstances such as growing item inventories. Toaddress these limitations, we present GPT4Rec, a novel and flexible generativeframework inspired by search engines. It first generates hypothetical \"searchqueries\" given item titles in a user's history, and then retrieves items forrecommendation by searching these queries. The framework overcomes previouslimitations by learning both user and item embeddings in the language space. Towell-capture user interests with different aspects and granularity forimproving relevance and diversity, we propose a multi-query generationtechnique with beam search. The generated queries naturally serve asinterpretable representations of user interests and can be searched torecommend cold-start items. With GPT-2 language model and BM25 search engine,our framework outperforms state-of-the-art methods by $75.7%$ and $22.2%$ inRecall@K on two public datasets. Experiments further revealed that multi-querygeneration with beam search improves both the diversity of retrieved items andthe coverage of a user's multi-interests. The adaptiveness and interpretabilityof generated queries are discussed with qualitative case studies.", "output": "GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding how proteins structurally interact is crucial to modernbiology, with applications in drug discovery and protein design. Recent machinelearning methods have formulated protein-small molecule docking as a generativeproblem with significant performance boosts over both traditional and deeplearning baselines. In this work, we propose a similar approach for rigidprotein-protein docking: DiffDock-PP is a diffusion generative model thatlearns to translate and rotate unbound protein structures into their boundconformations. We achieve state-of-the-art performance on DIPS with a medianC-RMSD of 4.85, outperforming all considered baselines. Additionally,DiffDock-PP is faster than all search-based methods and generates reliableconfidence estimates for its predictions. Our code is publicly available at$texttt{", "output": "DiffDock-PP: Rigid Protein-Protein Docking with Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The two fields of urban planning and artificial intelligence (AI) arose anddeveloped separately. However, there is now cross-pollination and increasinginterest in both fields to benefit from the advances of the other. In thepresent paper, we introduce the importance of urban planning from thesustainability, living, economic, disaster, and environmental perspectives. Wereview the fundamental concepts of urban planning and relate these concepts tocrucial open problems of machine learning, including adversarial learning,generative neural networks, deep encoder-decoder networks, conversational AI,and geospatial and temporal machine learning, thereby assaying how AI cancontribute to modern urban planning. Thus, a central problem is automatedland-use configuration, which is formulated as the generation of land uses andbuilding configuration for a target area from surrounding geospatial, humanmobility, social media, environment, and economic activities. Finally, wedelineate some implications of AI for urban planning and propose key researchareas at the intersection of both topics.", "output": "Towards Automated Urban Planning: When Generative and ChatGPT-like AI Meets Urban Planning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a novel continual learning method based on multifidelity deepneural networks. This method learns the correlation between the output ofpreviously trained models and the desired output of the model on the currenttraining dataset, limiting catastrophic forgetting. On its own themultifidelity continual learning method shows robust results that limitforgetting across several datasets. Additionally, we show that themultifidelity method can be combined with existing continual learning methods,including replay and memory aware synapses, to further limit catastrophicforgetting. The proposed continual learning method is especially suited forphysical problems where the data satisfy the same physical laws on each domain,or for physics-informed neural networks, because in these cases we expect thereto be a strong correlation between the output of the previous model and themodel on the current training domain.", "output": "A multifidelity approach to continual learning for physical systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of artificial intelligence for science, it is consistently anessential challenge to face a limited amount of labeled data for real-worldproblems. The prevailing approach is to pretrain a powerful task-agnostic modelon a large unlabeled corpus but may struggle to transfer knowledge todownstream tasks. In this study, we propose InstructMol, a semi-supervisedlearning algorithm, to take better advantage of unlabeled examples. Itintroduces an instructor model to provide the confidence ratios as themeasurement of pseudo-labels' reliability. These confidence scores then guidethe target model to pay distinct attention to different data points, avoidingthe over-reliance on labeled data and the negative influence of incorrectpseudo-annotations. Comprehensive experiments show that InstructBiosubstantially improves the generalization ability of molecular models, in notonly molecular property predictions but also activity cliff estimations,demonstrating the superiority of the proposed method. Furthermore, our evidenceindicates that InstructBio can be equipped with cutting-edge pretrainingmethods and used to establish large-scale and task-specific pseudo-labeledmolecular datasets, which reduces the predictive errors and shortens thetraining process. Our work provides strong evidence that semi-supervisedlearning can be a promising tool to overcome the data scarcity limitation andadvance molecular representation learning.", "output": "InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimal control is notoriously difficult for stochastic nonlinear systems.Ren et al. introduced Spectral Dynamics Embedding for developing reinforcementlearning methods for controlling an unknown system. It uses aninfinite-dimensional feature to linearly represent the state-value function andexploits finite-dimensional truncation approximation for practicalimplementation. However, the finite-dimensional approximation properties incontrol have not been investigated even when the model is known. In this paper,we provide a tractable stochastic nonlinear control algorithm that exploits thenonlinear dynamics upon the finite-dimensional feature approximation, SpectralDynamics Embedding Control (SDEC), with an in-depth theoretical analysis tocharacterize the approximation error induced by the finite-dimension truncationand statistical error induced by finite-sample approximation in both policyevaluation and policy optimization. We also empirically test the algorithm andcompare the performance with Koopman-based methods and iLQR methods on thependulum swingup problem.", "output": "Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spurious correlations that degrade model generalization or lead the model tobe right for the wrong reasons are one of the main robustness concerns forreal-world deployments. However, mitigating these correlations duringpre-training for large-scale models can be costly and impractical, particularlyfor those without access to high-performance computing resources. This paperproposes a novel approach to address spurious correlations during fine-tuningfor a given domain of interest. With a focus on multi-modal models (e.g.,CLIP), the proposed method leverages different modalities in these models todetect and explicitly set apart spurious attributes from the affected class,achieved through a multi-modal contrastive loss function that expressesspurious relationships through language. Our experimental results and in-depthvisualizations on CLIP show that such an intervention can effectively i)improve the model's accuracy when spurious attributes are not present, and ii)directs the model's activation maps towards the actual class rather than thespurious attribute when present. In particular, on the Waterbirds dataset, ouralgorithm achieved a worst-group accuracy 23% higher than ERM on CLIP with aResNet-50 backbone, and 32% higher on CLIP with a ViT backbone, whilemaintaining the same average accuracy as ERM.", "output": "Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Crops are constantly challenged by different environmental conditions. Seedtreatment by nanomaterials is a cost-effective and environmentally-friendlysolution for environmental stress mitigation in crop plants. Here, 56 seednanopriming treatments are used to alleviate environmental stresses in maize.Seven selected nanopriming treatments significantly increase the stressresistance index (SRI) by 13.9% and 12.6% under salinity stress and combinedheat-drought stress, respectively. Metabolomics data reveals that ZnOnanopriming treatment, with the highest SRI value, mainly regulates thepathways of amino acid metabolism, secondary metabolite synthesis, carbohydratemetabolism, and translation. Understanding the mechanism of seed nanopriming isstill difficult due to the variety of nanomaterials and the complexity ofinteractions between nanomaterials and plants. Using the nanopriming data, wepresent an interpretable structure-activity relationship (ISAR) approach basedon interpretable machine learning for predicting and understanding its stressmitigation effects. The post hoc and model-based interpretation approaches ofmachine learning are combined to provide complementary benefits and giveresearchers or policymakers more illuminating or trustworthy results. Theconcentration, size, and zeta potential of nanoparticles are identified asdominant factors for correlating root dry weight under salinity stress, andtheir effects and interactions are explained. Additionally, a web-basedinteractive tool is developed for offering prediction-level interpretation andgathering more details about specific nanopriming treatments. This work offersa promising framework for accelerating the agricultural applications ofnanomaterials and may profoundly contribute to nanosafety assessment.", "output": "Interpretable machine learning-accelerated seed treatment by nanomaterials for environmental stress alleviation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Adversarial Networks (GANs) have emerged as a significant playerin generative modeling by mapping lower-dimensional random noise tohigher-dimensional spaces. These networks have been used to generatehigh-resolution images and 3D objects. The efficient modeling of 3D objects andhuman faces is crucial in the development process of 3D graphical environmentssuch as games or simulations. 3D GANs are a new type of generative model usedfor 3D reconstruction, point cloud reconstruction, and 3D semantic scenecompletion. The choice of distribution for noise is critical as it representsthe latent space. Understanding a GAN's latent space is essential forfine-tuning the generated samples, as demonstrated by the morphing ofsemantically meaningful parts of images. In this work, we explore the latentspace and 3D GANs, examine several GAN variants and training methods to gaininsights into improving 3D GAN training, and suggest potential futuredirections for further research.", "output": "3D GANs and Latent Space: A comprehensive survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sampling from high-dimensional distributions is a fundamental problem instatistical research and practice. However, great challenges emerge when thetarget density function is unnormalized and contains isolated modes. We tacklethis difficulty by fitting an invertible transformation mapping, called atransport map, between a reference probability measure and the targetdistribution, so that sampling from the target distribution can be achieved bypushing forward a reference sample through the transport map. We theoreticallyanalyze the limitations of existing transport-based sampling methods using theWasserstein gradient flow theory, and propose a new method called TemperFlowthat addresses the multimodality issue. TemperFlow adaptively learns a sequenceof tempered distributions to progressively approach the target distribution,and we prove that it overcomes the limitations of existing methods. Variousexperiments demonstrate the superior performance of this novel sampler comparedto traditional methods, and we show its applications in modern deep learningtasks such as image generation. The programming code for the numericalexperiments is available at ", "output": "Efficient Multimodal Sampling via Tempered Distribution Flow."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As machine learning has been deployed ubiquitously across applications inmodern data science, algorithmic fairness has become a great concern andvarieties of fairness criteria have been proposed. Among them, imposingfairness constraints during learning, i.e. in-processing fair training, hasbeen a popular type of training method because they don't require accessingsensitive attributes during test time in contrast to post-processing methods.Although imposing fairness constraints have been studied extensively forclassical machine learning models, the effect these techniques have on deepneural networks is still unclear. Recent research has shown that addingfairness constraints to the objective function leads to severe over-fitting tofairness criteria in large models, and how to solve this challenge is animportant open question. To address this challenge, we leverage the wisdom andpower of pre-training and fine-tuning and develop a simple but novel frameworkto train fair neural networks in an efficient and inexpensive way. We conductcomprehensive experiments on two popular image datasets with state-of-artarchitectures under different fairness notions to show that last-layerfine-tuning is sufficient for promoting fairness of the deep neural network.Our framework brings new insights into representation learning in training fairneural networks.", "output": "Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the advent of general-purpose speech representations from large-scaleself-supervised models, applying a single model to multiple downstream tasks isbecoming a de-facto approach. However, the pooling problem remains; the lengthof speech representations is inherently variable. The naive average pooling isoften used, even though it ignores the characteristics of speech, such asdifferently lengthed phonemes. Hence, we design a novel pooling method tosquash acoustically similar representations via vector quantization, which doesnot require additional training, unlike attention-based pooling. Further, weevaluate various unsupervised pooling methods on various self-supervisedmodels. We gather diverse methods scattered around speech and text to evaluateon various tasks: keyword spotting, speaker identification, intentclassification, and emotion recognition. Finally, we quantitatively andqualitatively analyze our method, comparing it with supervised pooling methods.", "output": "Unsupervised Speech Representation Pooling Using Vector Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prenatal ultrasound imaging is the first-choice modality to assess fetalhealth. Medical image datasets for AI and ML methods must be diverse (i.e.diagnoses, diseases, pathologies, scanners, demographics, etc), however thereare few public ultrasound fetal imaging datasets due to insufficient amounts ofclinical data, patient privacy, rare occurrence of abnormalities in generalpractice, and limited experts for data collection and validation. To addresssuch data scarcity, we proposed generative adversarial networks (GAN)-basedmodels, diffusion-super-resolution-GAN and transformer-based-GAN, to synthesiseimages of fetal ultrasound brain planes from one public dataset. We reportedthat GAN-based methods can generate 256x256 pixel size of fetal ultrasoundtrans-cerebellum brain image plane with stable training losses, resulting inlower FID values for diffusion-super-resolution-GAN (average 7.04 and lower FID5.09 at epoch 10) than the FID values of transformer-based-GAN (average 36.02and lower 28.93 at epoch 60). The results of this work illustrate the potentialof GAN-based methods to synthesise realistic high-resolution ultrasound images,leading to future work with other fetal brain planes, anatomies, devices andthe need of a pool of experts to evaluate synthesised images. Code, data andother resources to reproduce this work are available aturl{", "output": "Towards Realistic Ultrasound Fetal Brain Imaging Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, knowledge tracing models have been applied in educational datamining such as the Self-attention knowledge tracing model(SAKT), which modelsthe relationship between exercises and Knowledge concepts(Kcs). However,relation modeling in traditional Knowledge tracing models only considers thestatic question-knowledge relationship and knowledge-knowledge relationship andtreats these relationships with equal importance. This kind of relationmodeling is difficult to avoid the influence of subjective labeling andconsiders the relationship between exercises and KCs, or KCs and KCsseparately. In this work, a novel knowledge tracing model, named KnowledgeRelation Rank Enhanced Heterogeneous Learning Interaction Modeling for NeuralGraph Forgetting Knowledge Tracing(NGFKT), is proposed to reduce the impact ofthe subjective labeling by calibrating the skill relation matrix and theQ-matrix and apply the Graph Convolutional Network(GCN) to model theheterogeneous interactions between students, exercises, and skills.Specifically, the skill relation matrix and Q-matrix are generated by theKnowledge Relation Importance Rank Calibration method(KRIRC). Then thecalibrated skill relation matrix, Q-matrix, and the heterogeneous interactionsare treated as the input of the GCN to generate the exercise embedding andskill embedding. Next, the exercise embedding, skill embedding, itemdifficulty, and contingency table are incorporated to generate an exerciserelation matrix as the inputs of the Position-Relation-Forgetting attentionmechanism. Finally, the Position-Relation-Forgetting attention mechanism isapplied to make the predictions. Experiments are conducted on the two publiceducational datasets and results indicate that the NGFKT model outperforms allbaseline models in terms of AUC, ACC, and Performance Stability(PS).", "output": "Knowledge Relation Rank Enhanced Heterogeneous Learning Interaction Modeling for Neural Graph Forgetting Knowledge Tracing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the increasing data volume, there is a trend of using large-scalepre-trained models to store the knowledge into an enormous number of modelparameters. The training of these models is composed of lots of dense algebras,requiring a huge amount of hardware resources. Recently, sparsely-gatedMixture-of-Experts (MoEs) are becoming more popular and have demonstratedimpressive pretraining scalability in various downstream tasks. However, such asparse conditional computation may not be effective as expected in practicalsystems due to the routing imbalance and fluctuation problems. Generally, MoEsare becoming a new data analytics paradigm in the data life cycle and sufferingfrom unique challenges at scales, complexities, and granularities never beforepossible.In this paper, we propose a novel DNN training framework, FlexMoE, whichsystematically and transparently address the inefficiency caused by dynamicdataflow. We first present an empirical analysis on the problems andopportunities of training MoE models, which motivates us to overcome therouting imbalance and fluctuation problems by a dynamic expert management anddevice placement mechanism. Then we introduce a novel scheduling module overthe existing DNN runtime to monitor the data flow, make the scheduling plans,and dynamically adjust the model-to-hardware mapping guided by the real-timedata traffic. A simple but efficient heuristic algorithm is exploited todynamically optimize the device placement during training. We have conductedexperiments on both NLP models (e.g., BERT and GPT) and vision models (e.g.,Swin). And results show FlexMoE can achieve superior performance compared withexisting systems on real-world workloads -- FlexMoE outperforms DeepSpeed by1.70x on average and up to 2.10x, and outperforms FasterMoE by 1.30x on averageand up to 1.45x.", "output": "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models can be fooled by small $l_p$-norm adversarialperturbations and natural perturbations in terms of attributes. Although therobustness against each perturbation has been explored, it remains a challengeto address the robustness against joint perturbations effectively. In thispaper, we study the robustness of deep learning models against jointperturbations by proposing a novel attack mechanism named Semantic-PreservingAdversarial (SPA) attack, which can then be used to enhance adversarialtraining. Specifically, we introduce an attribute manipulator to generatenatural and human-comprehensible perturbations and a noise generator togenerate diverse adversarial noises. Based on such combined noises, we optimizeboth the attribute value and the diversity variable to generatejointly-perturbed samples. For robust training, we adversarially train the deeplearning model against the generated joint perturbations. Empirical results onfour benchmarks show that the SPA attack causes a larger performance declinewith small $l_{infty}$ norm-ball constraints compared to existing approaches.Furthermore, our SPA-enhanced training outperforms existing defense methodsagainst such joint perturbations.", "output": "Robust Deep Learning Models Against Semantic-Preserving Adversarial Attack."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantization has emerged as an essential technique for deploying deep neuralnetworks (DNNs) on devices with limited resources. However, quantized modelsexhibit vulnerabilities when exposed to various noises in real-worldapplications. Despite the importance of evaluating the impact of quantizationon robustness, existing research on this topic is limited and often disregardsestablished principles of robustness evaluation, resulting in incomplete andinconclusive findings. To address this gap, we thoroughly evaluated therobustness of quantized models against various noises (adversarial attacks,natural corruptions, and systematic noises) on ImageNet. Extensive experimentsdemonstrate that lower-bit quantization is more resilient to adversarialattacks but is more susceptible to natural corruptions and systematic noises.Notably, our investigation reveals that impulse noise (in natural corruptions)and the nearest neighbor interpolation (in systematic noises) have the mostsignificant impact on quantized models. Our research contributes to advancingthe robust quantization of models and their deployment in real-world scenarios.", "output": "Benchmarking the Robustness of Quantized Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Water crisis is a crucial concern around the globe. Appropriate and timelymaintenance of water pumps in drought-hit countries is vital for communitiesrelying on the well. In this paper, we analyze and apply a sequential attentivedeep neural architecture, TabNet, for predicting water pump repair status inTanzania. The model combines the valuable benefits of tree-based algorithms andneural networks, enabling end-to-end training, model interpretability, sparsefeature selection, and efficient learning on tabular data. Finally, we comparethe performance of TabNet with popular gradient tree-boosting algorithms likeXGBoost, LightGBM,CatBoost, and demonstrate how we can further uplift theperformance by choosing focal loss as the objective function while training onimbalanced data.", "output": "Pump It Up: Predict Water Pump Status using Attentive Tabular Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Capsule Networks (CapsNets) are able to hierarchically preserve the poserelationships between multiple objects for image classification tasks. Otherthan achieving high accuracy, another relevant factor in deploying CapsNets insafety-critical applications is the robustness against input transformationsand malicious adversarial attacks.In this paper, we systematically analyze and evaluate different factorsaffecting the robustness of CapsNets, compared to traditional ConvolutionalNeural Networks (CNNs). Towards a comprehensive comparison, we test two CapsNetmodels and two CNN models on the MNIST, GTSRB, and CIFAR10 datasets, as well ason the affine-transformed versions of such datasets. With a thorough analysis,we show which properties of these architectures better contribute to increasingthe robustness and their limitations. Overall, CapsNets achieve betterrobustness against adversarial examples and affine transformations, compared toa traditional CNN with a similar number of parameters. Similar conclusions havebeen derived for deeper versions of CapsNets and CNNs. Moreover, our resultsunleash a key finding that the dynamic routing does not contribute much toimproving the CapsNets' robustness. Indeed, the main generalizationcontribution is due to the hierarchical feature learning through capsules.", "output": "RobCaps: Evaluating the Robustness of Capsule Networks against Affine Transformations and Adversarial Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Failure to recognize samples from the classes unseen during training is amajor limit of artificial intelligence (AI) in real-world implementation ofretinal anomaly classification. To resolve this obstacle, we propose anuncertainty-inspired open-set (UIOS) model which was trained with fundus imagesof 9 common retinal conditions. Besides the probability of each category, UIOSalso calculates an uncertainty score to express its confidence. Our UIOS modelwith thresholding strategy achieved an F1 score of 99.55%, 97.01% and 91.91%for the internal testing set, external testing set and non-typical testing set,respectively, compared to the F1 score of 92.20%, 80.69% and 64.74% by thestandard AI model. Furthermore, UIOS correctly predicted high uncertaintyscores, which prompted the need for a manual check, in the datasets of rareretinal diseases, low-quality fundus images, and non-fundus images. This workprovides a robust method for real-world screening of retinal anomalies.", "output": "Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new data analysis perspective to determine variable importanceregardless of the underlying learning task. Traditionally, variable selectionis considered an important step in supervised learning for both classificationand regression problems. The variable selection also becomes critical whencosts associated with the data collection and storage are considerably high forcases like remote sensing. Therefore, we propose a new methodology to selectimportant variables from the data by first creating dependency networks amongall variables and then ranking them (i.e. nodes) by graph centrality measures.Selecting Top-$n$ variables according to preferred centrality measure willyield a strong candidate subset of variables for further learning tasks e.g.clustering. We present our tool as a Shiny app which is a user-friendlyinterface development environment. We also extend the user interface for twowell-known unsupervised variable selection methods from literature forcomparison reasons.", "output": "DiscoVars: A New Data Analysis Perspective -- Application in Variable Selection for Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformers' compute-intensive operations pose enormous challenges for theirdeployment in resource-constrained EdgeAI / tinyML devices. As an establishedneural network compression technique, quantization reduces the hardwarecomputational and memory resources. In particular, fixed-point quantization isdesirable to ease the computations using lightweight blocks, like adders andmultipliers, of the underlying hardware. However, deploying fully-quantizedTransformers on existing general-purpose hardware, generic AI accelerators, orspecialized architectures for Transformers with floating-point units might beinfeasible and/or inefficient.Towards this, we propose SwiftTron, an efficient specialized hardwareaccelerator designed for Quantized Transformers. SwiftTron supports theexecution of different types of Transformers' operations (like Attention,Softmax, GELU, and Layer Normalization) and accounts for diverse scalingfactors to perform correct computations. We synthesize the complete SwiftTronarchitecture in a $65$ nm CMOS technology with the ASIC design flow. OurAccelerator executes the RoBERTa-base model in 1.83 ns, while consuming 33.64mW power, and occupying an area of 273 mm^2. To ease the reproducibility, theRTL of our SwiftTron architecture is released at", "output": "SwiftTron: An Efficient Hardware Accelerator for Quantized Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the task of comparing two classification algorithms, the widely-usedMcNemar's test aims to infer the presence of a significant difference betweenthe error rates of the two classification algorithms. However, the power of theconventional McNemar's test is usually unpromising because the hold-out (HO)method in the test merely uses a single train-validation split that usuallyproduces a highly varied estimation of the error rates. In contrast, across-validation (CV) method repeats the HO method in multiple times andproduces a stable estimation. Therefore, a CV method has a great advantage toimprove the power of McNemar's test. Among all types of CV methods, ablock-regularized 5$times$2 CV (BCV) has been shown in many previous studiesto be superior to the other CV methods in the comparison task of algorithmsbecause the 5$times$2 BCV can produce a high-quality estimator of the errorrate by regularizing the numbers of overlapping records between all trainingsets. In this study, we compress the 10 correlated contingency tables in the5$times$2 BCV to form an effective contingency table. Then, we define a5$times$2 BCV McNemar's test on the basis of the effective contingency table.We demonstrate the reasonable type I error and the promising power of theproposed 5$times$2 BCV McNemar's test on multiple simulated and real-worlddata sets.", "output": "Block-regularized 5$\\times$2 Cross-validated McNemar's Test for Comparing Two Classification Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Genetic algorithms constitute a family of black-box optimization algorithms,which take inspiration from the principles of biological evolution. While theyprovide a general-purpose tool for optimization, their particularinstantiations can be heuristic and motivated by loose biological intuition. Inthis work we explore a fundamentally different approach: Given a sufficientlyflexible parametrization of the genetic operators, we discover entirely newgenetic algorithms in a data-driven fashion. More specifically, we parametrizeselection and mutation rate adaptation as cross- and self-attention modules anduse Meta-Black-Box-Optimization to evolve their parameters on a set of diverseoptimization tasks. The resulting Learned Genetic Algorithm outperformsstate-of-the-art adaptive baseline genetic algorithms and generalizes farbeyond its meta-training settings. The learned algorithm can be applied topreviously unseen optimization problems, search dimensions &amp; evaluationbudgets. We conduct extensive analysis of the discovered operators and provideablation experiments, which highlight the benefits of flexible moduleparametrization and the ability to transfer (`plug-in') the learned operatorsto conventional genetic algorithms.", "output": "Discovering Attention-Based Genetic Algorithms via Meta-Black-Box Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We provide a unified framework for characterizing pure and approximatedifferentially private (DP) learnabiliity. The framework uses the language ofgraph theory: for a concept class $mathcal{H}$, we define the contradictiongraph $G$ of $mathcal{H}$. It vertices are realizable datasets, and twodatasets $S,S'$ are connected by an edge if they contradict each other (i.e.,there is a point $x$ that is labeled differently in $S$ and $S'$). Our mainfinding is that the combinatorial structure of $G$ is deeply related tolearning $mathcal{H}$ under DP. Learning $mathcal{H}$ under pure DP iscaptured by the fractional clique number of $G$. Learning $mathcal{H}$ underapproximate DP is captured by the clique number of $G$. Consequently, weidentify graph-theoretic dimensions that characterize DP learnability: theclique dimension and fractional clique dimension. Along the way, we revealproperties of the contradiction graph which may be of independent interest. Wealso suggest several open questions and directions for future research.", "output": "A Unified Characterization of Private Learnability via Graph Theory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The integration of renewable energy sources into the power grid is becomingincreasingly important as the world moves towards a more sustainable energyfuture. However, the intermittent nature of renewable energy sources can makeit challenging to manage the power grid and ensure a stable supply ofelectricity. In this paper, we propose a deep learning-based approach forpredicting energy demand in a smart power grid, which can improve theintegration of renewable energy sources by providing accurate predictions ofenergy demand. We use long short-term memory networks, which are well-suitedfor time series data, to capture complex patterns and dependencies in energydemand data. The proposed approach is evaluated using four datasets ofhistorical energy demand data from different energy distribution companiesincluding American Electric Power, Commonwealth Edison, Dayton Power and Light,and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model isalso compared with two other state of the art forecasting algorithms namely,Facebook Prophet and Support Vector Regressor. The experimental results showthat the proposed REDf model can accurately predict energy demand with a meanabsolute error of 1.4%. This approach has the potential to improve theefficiency and stability of the power grid by allowing for better management ofthe integration of renewable energy sources.", "output": "REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training sophisticated machine learning (ML) models requires large datasetsthat are difficult or expensive to collect for many applications. If priorknowledge about system dynamics is available, mechanistic representations canbe used to supplement real-world data. We present SimbaML (Simulation-BasedML), an open-source tool that unifies realistic synthetic dataset generationfrom ordinary differential equation-based models and the direct analysis andinclusion in ML pipelines. SimbaML conveniently enables investigating transferlearning from synthetic to real-world data, data augmentation, identifyingneeds for data collection, and benchmarking physics-informed ML approaches.SimbaML is available from ", "output": "SimbaML: Connecting Mechanistic Models and Machine Learning with Augmented Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Overloading in DC servo motors is a major concern in industries, as manycompanies face the problem of finding expert operators, and also humanmonitoring may not be an effective solution. Therefore, this paper proposed anembedded Artificial intelligence (AI) approach using a Convolutional NeuralNetwork (CNN) using a new transformation to extract faults from real-time inputsignals without human interference. Our main purpose is to extract as many aspossible features from the input signal to achieve a relaxed dataset thatresults in an effective but compact network to provide real-time faultdetection even in a low-memory microcontroller. Besides, fault detection methoda synchronous dual-motor system is also proposed to take action in faultyevents. To fulfill this intention, a one-dimensional input signal from theoutput current of each DC servo motor is monitored and transformed into a 3dstack of data and then the CNN is implemented into the processor to detect anyfault corresponding to overloading, finally experimental setup results in99.9997% accuracy during testing for a model with nearly 8000 parameters. Inaddition, the proposed dual-motor system could achieve overload reduction andprovide a fault-tolerant system and it is shown that this system also takesadvantage of less energy consumption.", "output": "A new transformation for embedded convolutional neural network approach toward real-time servo motor overload fault-detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a growing literature on the study of large-width properties of deepGaussian neural networks (NNs), i.e. deep NNs with Gaussian-distributedparameters or weights, and Gaussian stochastic processes. Motivated by someempirical and theoretical studies showing the potential of replacing Gaussiandistributions with Stable distributions, namely distributions with heavy tails,in this paper we investigate large-width properties of deep Stable NNs, i.e.deep NNs with Stable-distributed parameters. For sub-linear activationfunctions, a recent work has characterized the infinitely wide limit of asuitable rescaled deep Stable NN in terms of a Stable stochastic process, bothunder the assumption of a ``joint growth\" and under the assumption of a``sequential growth\" of the width over the NN's layers. Here, assuming a``sequential growth\" of the width, we extend such a characterization to ageneral class of activation functions, which includes sub-linear,asymptotically linear and super-linear functions. As a novelty with respect toprevious works, our results rely on the use of a generalized central limittheorem for heavy tails distributions, which allows for an interesting unifiedtreatment of infinitely wide limits for deep Stable NNs. Our study shows thatthe scaling of Stable NNs and the stability of their infinitely wide limits maydepend on the choice of the activation function, bringing out a criticaldifference with respect to the Gaussian setting.", "output": "Infinitely wide limits for deep Stable neural networks: sub-linear, linear and super-linear activation functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a growing interest on large-width asymptotic properties of Gaussianneural networks (NNs), namely NNs whose weights are initialized according toGaussian distributions. A well-established result is that, as the width goes toinfinity, a Gaussian NN converges in distribution to a Gaussian stochasticprocess, which provides an asymptotic or qualitative Gaussian approximation ofthe NN. In this paper, we introduce some non-asymptotic or quantitativeGaussian approximations of Gaussian NNs, quantifying the approximation errorwith respect to some popular distances for (probability) distributions, e.g.the $1$-Wasserstein distance, the total variation distance and theKolmogorov-Smirnov distance. Our results rely on the use of second-orderGaussian Poincar'e inequalities, which provide tight estimates of theapproximation error, with optimal rates. This is a novel application ofsecond-order Gaussian Poincar'e inequalities, which are well-known in theprobabilistic literature for being a powerful tool to obtain Gaussianapproximations of general functionals of Gaussian stochastic processes. Ageneralization of our results to deep Gaussian NNs is discussed.", "output": "Non-asymptotic approximations of Gaussian neural networks via second-order Poincar\\'e inequalities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As deep learning advances, edge devices and lightweight neural networks arebecoming more important. To reduce latency in the AI accelerator, it'sessential to not only reduce FLOPs but also enhance hardware performance. Weproposed an arithmetic intensity balancing convolution (ABConv) to address theissue of the overall intensity being limited by the small weight arithmeticintensity for convolution with a small spatial size. ABConv increased themaximum bound of overall arithmetic intensity and significantly reducedlatency, without sacrificing accuracy. We tested the latency and hardwareperformance of ABConv on the Arm Ethos-U65 NPU in various configurations andused it to replace some of MobileNetV1 and ResNet50 in image classification forCIFAR100.", "output": "Arithmetic Intensity Balancing Convolution for Hardware-aware Efficient Block Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An efficient team is essential for the company to successfully complete newprojects. To solve the team formation problem considering person-job matching(TFP-PJM), a 0-1 integer programming model is constructed, which considers bothperson-job matching and team members' willingness to communicate on teamefficiency, with the person-job matching score calculated using intuitionisticfuzzy numbers. Then, a reinforcement learning-assisted genetic programmingalgorithm (RL-GP) is proposed to enhance the quality of solutions. The RL-GPadopts the ensemble population strategies. Before the population evolution ateach generation, the agent selects one from four population search modesaccording to the information obtained, thus realizing a sound balance ofexploration and exploitation. In addition, surrogate models are used in thealgorithm to evaluate the formation plans generated by individuals, whichspeeds up the algorithm learning process. Afterward, a series of comparisonexperiments are conducted to verify the overall performance of RL-GP and theeffectiveness of the improved strategies within the algorithm. Thehyper-heuristic rules obtained through efficient learning can be utilized asdecision-making aids when forming project teams. This study reveals theadvantages of reinforcement learning methods, ensemble strategies, and thesurrogate model applied to the GP framework. The diversity and intelligentselection of search patterns along with fast adaptation evaluation, aredistinct features that enable RL-GP to be deployed in real-world enterpriseenvironments.", "output": "A Reinforcement Learning-assisted Genetic Programming Algorithm for Team Formation Problem Considering Person-Job Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modalityfor dental examination. However, its applicability is limited as compared to 3DCone-beam computed tomography (CBCT), because PX only provides 2D flattenedimages of the oral structure. In this paper, we propose a new framework whichestimates 3D oral structure from real-world PX images. Since there are not manymatching PX and CBCT data, we used simulated PX from CBCT for training,however, we used real-world panoramic radiographs at the inference time. Wepropose a new ray-sampling method to make simulated panoramic radiographsinspired by the principle of panoramic radiography along with the renderingfunction derived from the Beer-Lambert law. Our model consists of three parts:translation module, generation module, and refinement module. The translationmodule changes the real-world panoramic radiograph to the simulated trainingimage style. The generation module makes the 3D structure from the input imagewithout any prior information such as a dental arch. Our ray-based generationapproach makes it possible to reverse the process of generating PX from oralstructure in order to reconstruct CBCT data. Lastly, the refinement moduleenhances the quality of the 3D output. Results show that our approach worksbetter for simulated and real-world images compared to other state-of-the-artmethods.", "output": "NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We offer a study that connects robust discriminative classifiers trained withadversarial training (AT) with generative modeling in the form of Energy-basedModels (EBM). We do so by decomposing the loss of a discriminative classifierand showing that the discriminative model is also aware of the input datadensity. Though a common assumption is that adversarial points leave themanifold of the input data, our study finds out that, surprisingly, untargetedadversarial points in the input space are very likely under the generativemodel hidden inside the discriminative classifier -- have low energy in theEBM. We present two evidence: untargeted attacks are even more likely than thenatural data and their likelihood increases as the attack strength increases.This allows us to easily detect them and craft a novel attack calledHigh-Energy PGD that fools the classifier yet has energy similar to the dataset.", "output": "Exploring the Connection between Robust and Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spiking Neural Networks (SNNs) have shown capabilities of achieving highaccuracy under unsupervised settings and low operational power/energy due totheir bio-plausible computations. Previous studies identified that DRAM-basedoff-chip memory accesses dominate the energy consumption of SNN processing.However, state-of-the-art works do not optimize the DRAM energy-per-access,thereby hindering the SNN-based systems from achieving further energyefficiency gains. To substantially reduce the DRAM energy-per-access, aneffective solution is to decrease the DRAM supply voltage, but it may lead toerrors in DRAM cells (i.e., so-called approximate DRAM). Towards this, wepropose textit{EnforceSNN}, a novel design framework that provides a solutionfor resilient and energy-efficient SNN inference using reduced-voltage DRAM forembedded systems. The key mechanisms of our EnforceSNN are: (1) employingquantized weights to reduce the DRAM access energy; (2) devising an efficientDRAM mapping policy to minimize the DRAM energy-per-access; (3) analyzing theSNN error tolerance to understand its accuracy profile considering differentbit error rate (BER) values; (4) leveraging the information for developing anefficient fault-aware training (FAT) that considers different BER values andbit error locations in DRAM to improve the SNN error tolerance; and (5)developing an algorithm to select the SNN model that offers good trade-offsamong accuracy, memory, and energy consumption. The experimental results showthat our EnforceSNN maintains the accuracy (i.e., no accuracy loss for BERless-or-equal 10^-3) as compared to the baseline SNN with accurate DRAM, whileachieving up to 84.9% of DRAM energy saving and up to 4.1x speed-up of DRAMdata throughput across different network sizes.", "output": "EnforceSNN: Enabling Resilient and Energy-Efficient Spiking Neural Network Inference considering Approximate DRAMs for Embedded Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To maximize the performance and energy efficiency of Spiking Neural Network(SNN) processing on resource-constrained embedded systems, specialized hardwareaccelerators/chips are employed. However, these SNN chips may suffer frompermanent faults which can affect the functionality of weight memory and neuronbehavior, thereby causing potentially significant accuracy degradation andsystem malfunctioning. Such permanent faults may come from manufacturingdefects during the fabrication process, and/or from device/transistor damages(e.g., due to wear out) during the run-time operation. However, the impact ofpermanent faults in SNN chips and the respective mitigation techniques have notbeen thoroughly investigated yet. Toward this, we propose RescueSNN, a novelmethodology to mitigate permanent faults in the compute engine of SNN chipswithout requiring additional retraining, thereby significantly cutting down thedesign time and retraining costs, while maintaining the throughput and quality.The key ideas of our RescueSNN methodology are (1) analyzing thecharacteristics of SNN under permanent faults; (2) leveraging this analysis toimprove the SNN fault-tolerance through effective fault-aware mapping (FAM);and (3) devising lightweight hardware enhancements to support FAM. Our FAMtechnique leverages the fault map of SNN compute engine for (i) minimizingweight corruption when mapping weight bits on the faulty memory cells, and (ii)selectively employing faulty neurons that do not cause significant accuracydegradation to maintain accuracy and throughput, while considering the SNNoperations and processing dataflow. The experimental results show that ourRescueSNN improves accuracy by up to 80% while maintaining the throughputreduction below 25% in high fault rate (e.g., 0.5 of the potential faultlocations), as compared to running SNNs on the faulty chip without mitigation.", "output": "RescueSNN: Enabling Reliable Executions on Spiking Neural Network Accelerators under Permanent Faults."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of uncertainty quantification in high dimensionalregression and classification for which deep ensemble have proven to bepromising methods. Recent observations have shown that deep ensemble oftenreturn overconfident estimates outside the training domain, which is a majorlimitation because shifted distributions are often encountered in real-lifescenarios. The principal challenge for this problem is to solve the trade-offbetween increasing the diversity of the ensemble outputs and making accuratein-distribution predictions. In this work, we show that an ensemble of networkswith large weights fitting the training data are likely to meet these twoobjectives. We derive a simple and practical approach to produce suchensembles, based on an original anti-regularization term penalizing smallweights and a control process of the weight increase which maintains thein-distribution loss under an acceptable threshold. The developed approach doesnot require any out-of-distribution training data neither any trade-offhyper-parameter calibration. We derive a theoretical framework for thisapproach and show that the proposed optimization can be seen as a\"water-filling\" problem. Several experiments in both regression andclassification settings highlight that Deep Anti-Regularized Ensembles (DARE)significantly improve uncertainty quantification outside the training domain incomparison to recent deep ensembles and out-of-distribution detection methods.All the conducted experiments are reproducible and the source code is availableat url{", "output": "Deep Anti-Regularized Ensembles provide reliable out-of-distribution uncertainty quantification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In geospatial planning, it is often essential to represent objects in avectorized format, as this format easily translates to downstream tasks such asweb development, graphics, or design. While these problems are frequentlyaddressed using semantic segmentation, which requires additionalpost-processing to vectorize objects in a non-trivial way, we present anImage-to-Sequence model that allows for direct shape inference and is ready forvector-based workflows out of the box. We demonstrate the model's performancein various ways, including perturbations to the image input that correspond tovariations or artifacts commonly encountered in remote sensing applications.Our model outperforms prior works when using ground truth bounding boxes (oneobject per image), achieving the lowest maximum tangent angle error.", "output": "Polygonizer: An auto-regressive building delineator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a novel deep generative model, called BSDE-Gen, whichcombines the flexibility of backward stochastic differential equations (BSDEs)with the power of deep neural networks for generating high-dimensional complextarget data, particularly in the field of image generation. The incorporationof stochasticity and uncertainty in the generative modeling process makesBSDE-Gen an effective and natural approach for generating high-dimensionaldata. The paper provides a theoretical framework for BSDE-Gen, describes itsmodel architecture, presents the maximum mean discrepancy (MMD) loss functionused for training, and reports experimental results.", "output": "Deep Generative Modeling with Backward Stochastic Differential Equations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The graph colouring problem consists of assigning labels, or colours, to thevertices of a graph such that no two adjacent vertices share the same colour.In this work we investigate whether deep reinforcement learning can be used todiscover a competitive construction heuristic for graph colouring. Our proposedapproach, ReLCol, uses deep Q-learning together with a graph neural network forfeature extraction, and employs a novel way of parameterising the graph thatresults in improved performance. Using standard benchmark graphs with variedtopologies, we empirically evaluate the benefits and limitations of theheuristic learned by ReLCol relative to existing construction algorithms, anddemonstrate that reinforcement learning is a promising direction for furtherresearch on the graph colouring problem.", "output": "Generating a Graph Colouring Heuristic with Deep Q-Learning and Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The sequence-to-sequence (seq2seq) task aims at generating the targetsequence based on the given input source sequence. Traditionally, most of theseq2seq task is resolved by the Encoder-Decoder framework which requires anencoder to encode the source sequence and a decoder to generate the targettext. Recently, a bunch of new approaches have emerged that apply decoder-onlylanguage models directly to the seq2seq task. Despite the significantadvancements in applying language models to the seq2seq task, there is still alack of thorough analysis on the effectiveness of the decoder-only languagemodel architecture. This paper aims to address this gap by conducting adetailed comparison between the encoder-decoder architecture and thedecoder-only language model framework through the analysis of a regularizedencoder-decoder structure. This structure is designed to replicate allbehaviors in the classical decoder-only language model but has an encoder and adecoder making it easier to be compared with the classical encoder-decoderstructure. Based on the analysis, we unveil the attention degeneration problemin the language model, namely, as the generation step number grows, less andless attention is focused on the source sequence. To give a quantitativeunderstanding of this problem, we conduct a theoretical sensitivity analysis ofthe attention output with respect to the source input. Grounded on ouranalysis, we propose a novel partial attention language model to solve theattention degeneration problem. Experimental results on machine translation,summarization, and data-to-text generation tasks support our analysis anddemonstrate the effectiveness of our proposed model.", "output": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The paper describes a transformer-based system designed for SemEval-2023 Task9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predictthe intimacy of tweets in a range from 1 (not intimate at all) to 5 (veryintimate). The official training set for the competition consisted of tweets insix languages (English, Spanish, Italian, Portuguese, French, and Chinese). Thetest set included the given six languages as well as external data with fourlanguages not presented in the training set (Hindi, Arabic, Dutch, and Korean).We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTamodel adapted to the Twitter domain. To improve the performance of unseenlanguages, each tweet was supplemented by its English translation. We exploredthe effectiveness of translated data for the languages seen in fine-tuningcompared to unseen languages and estimated strategies for using translated datain transformer-based models. Our solution ranked 4th on the leaderboard whileachieving an overall Pearson's r of 0.599 over the test set. The proposedsystem improves up to 0.088 Pearson's r over a score averaged across all 45submissions.", "output": "tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient representation of quantum many-body states on classical computersis a problem of enormous practical interest. An ideal representation of aquantum state combines a succinct characterization informed by the system'sstructure and symmetries, along with the ability to predict the physicalobservables of interest. A number of machine learning approaches have beenrecently used to construct such classical representations [1-6] which enablepredictions of observables [7] and account for physical symmetries [8].However, the structure of a quantum state gets typically lost unless aspecialized ansatz is employed based on prior knowledge of the system [9-12].Moreover, most such approaches give no information about what states are easierto learn in comparison to others. Here, we propose a new generativeenergy-based representation of quantum many-body states derived from Gibbsdistributions used for modeling the thermal states of classical spin systems.Based on the prior information on a family of quantum states, the energyfunction can be specified by a small number of parameters using an explicitlow-degree polynomial or a generic parametric family such as neural nets, andcan naturally include the known symmetries of the system. Our results show thatsuch a representation can be efficiently learned from data using exactalgorithms in a form that enables the prediction of expectation values ofphysical observables. Importantly, the structure of the learned energy functionprovides a natural explanation for the hardness of learning for a given classof quantum states.", "output": "Learning Energy-Based Representations of Quantum Many-Body States."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multiple Sclerosis (MS) is a chronic disease developed in human brain andspinal cord, which can cause permanent damage or deterioration of the nerves.The severity of MS disease is monitored by the Expanded Disability Status Scale(EDSS), composed of several functional sub-scores. Early and accurateclassification of MS disease severity is critical for slowing down orpreventing disease progression via applying early therapeutic interventionstrategies. Recent advances in deep learning and the wide use of ElectronicHealth Records (EHR) creates opportunities to apply data-driven and predictivemodeling tools for this goal. Previous studies focusing on using single-modalmachine learning and deep learning algorithms were limited in terms ofprediction accuracy due to the data insufficiency or model simplicity. In thispaper, we proposed an idea of using patients' multimodal longitudinal andlongitudinal EHR data to predict multiple sclerosis disease severity at thehospital visit. This work has two important contributions. First, we describe apilot effort to leverage structured EHR data, neuroimaging data and clinicalnotes to build a multi-modal deep learning framework to predict patient's MSdisease severity. The proposed pipeline demonstrates up to 25% increase interms of the area under the Area Under the Receiver Operating Characteristiccurve (AUROC) compared to models using single-modal data. Second, the studyalso provides insights regarding the amount useful signal embedded in each datamodality with respect to MS disease prediction, which may improve datacollection processes.", "output": "Predicting multiple sclerosis disease severity with multimodal deep neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Response curves exhibit the magnitude of the response of a sensitive systemto a varying stimulus. However, response of such systems may be sensitive tomultiple stimuli (i.e., input features) that are not necessarily independent.As a consequence, the shape of response curves generated for a selected inputfeature (referred to as \"active feature\") might depend on the values of theother input features (referred to as \"passive features\"). In this work weconsider the case of systems whose response is approximated using regressionneural networks. We propose to use counterfactual explanations (CFEs) for theidentification of the features with the highest relevance on the shape ofresponse curves generated by neural network black boxes. CFEs are generated bya genetic algorithm-based approach that solves a multi-objective optimizationproblem. In particular, given a response curve generated for an active feature,a CFE finds the minimum combination of passive features that need to bemodified to alter the shape of the response curve. We tested our method on asynthetic dataset with 1-D inputs and two crop yield prediction datasets with2-D inputs. The relevance ranking of features and feature combinations obtainedon the synthetic dataset coincided with the analysis of the equation that wasused to generate the problem. Results obtained on the yield prediction datasetsrevealed that the impact on fertilizer responsivity of passive features dependson the terrain characteristics of each field.", "output": "Counterfactual Explanations of Neural Network-Generated Response Curves."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) has demonstrated impressive performance invarious areas such as video games and robotics. However, ensuring safety andstability, which are two critical properties from a control perspective,remains a significant challenge when using RL to control real-world systems. Inthis paper, we first provide definitions of safety and stability for the RLsystem, and then combine the control barrier function (CBF) and controlLyapunov function (CLF) methods with the actor-critic method in RL to propose aBarrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain theaforementioned safety and stability for the system. In this framework, CBFconstraints for safety and CLF constraint for stability are constructed basedon the data sampled from the replay buffer, and the augmented Lagrangian methodis used to update the parameters of the RL-based controller. Furthermore, anadditional backup controller is introduced in case the RL-based controllercannot provide valid control signals when safety and stability constraintscannot be satisfied simultaneously. Simulation results show that this frameworkyields a controller that can help the system approach the desired state andcause fewer violations of safety constraints compared to baseline algorithms.", "output": "A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a deep learning-based pipeline for categorizing Bengalitoxic comments, in which at first a binary classification model is used todetermine whether a comment is toxic or not, and then a multi-label classifieris employed to determine which toxicity type the comment belongs to. For thispurpose, we have prepared a manually labeled dataset consisting of 16,073instances among which 8,488 are Toxic and any toxic comment may correspond toone or more of the six toxic categories - vulgar, hate, religious, threat,troll, and insult simultaneously. Long Short Term Memory (LSTM) with BERTEmbedding achieved 89.42% accuracy for the binary classification task while asa multi-label classifier, a combination of Convolutional Neural Network andBi-directional Long Short Term Memory (CNN-BiLSTM) with attention mechanismachieved 78.92% accuracy and 0.86 as weighted F1-score. To explain thepredictions and interpret the word feature importance during classification bythe proposed models, we utilized Local Interpretable Model-AgnosticExplanations (LIME) framework. We have made our dataset public and can beaccessed at -", "output": "Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We formulate, analyze and solve the problem of best arm identification withfairness constraints on subpopulations (BAICS). Standard best armidentification problems aim at selecting an arm that has the largest expectedreward where the expectation is taken over the entire population. The BAICSproblem requires that an selected arm must be fair to all subpopulations (e.g.,different ethnic groups, age groups, or customer types) by satisfyingconstraints that the expected reward conditional on every subpopulation needsto be larger than some thresholds. The BAICS problem aims at correctlyidentify, with high confidence, the arm with the largest expected reward fromall arms that satisfy subpopulation constraints. We analyze the complexity ofthe BAICS problem by proving a best achievable lower bound on the samplecomplexity with closed-form representation. We then design an algorithm andprove that the algorithm's sample complexity matches with the lower bound interms of order. A brief account of numerical experiments are conducted toillustrate the theoretical findings.", "output": "Best Arm Identification with Fairness Constraints on Subpopulations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the mixing time of Metropolis-Adjusted Langevin algorithm (MALA) forsampling a target density on $mathbb{R}^d$. We assume that the target densitysatisfies $psi_mu$-isoperimetry and that the operator norm and trace of itsHessian are bounded by $L$ and $Upsilon$ respectively. Our main resultestablishes that, from a warm start, to achieve $epsilon$-total variationdistance to the target density, MALA mixes in$Oleft(frac{(LUpsilon)^{frac12}}{psi_mu^2}logleft(frac{1}{epsilon}right)right)$ iterations. Notably, this resultholds beyond the log-concave sampling setting and the mixing time depends ononly $Upsilon$ rather than its upper bound $L d$. In the $m$-stronglylogconcave and $L$-log-smooth sampling setting, our bound recovers the previousminimax mixing bound of MALA~cite{wu2021minimax}.", "output": "A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm under Smoothness and Isoperimetry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised discovery of stories with correlated news articles in real-timehelps people digest massive news streams without expensive human annotations. Acommon approach of the existing studies for unsupervised online story discoveryis to represent news articles with symbolic- or graph-based embedding andincrementally cluster them into stories. Recent large language models areexpected to improve the embedding further, but a straightforward adoption ofthe models by indiscriminately encoding all information in articles isineffective to deal with text-rich and evolving news streams. In this work, wepropose a novel thematic embedding with an off-the-shelf pretrained sentenceencoder to dynamically represent articles and stories by considering theirshared temporal themes. To realize the idea for unsupervised online storydiscovery, a scalable framework USTORY is introduced with two main techniques,theme- and time-aware dynamic embedding and novelty-aware adaptive clustering,fueled by lightweight story summaries. A thorough evaluation with real newsdata sets demonstrates that USTORY achieves higher story discovery performancesthan baselines while being robust and scalable to various streaming settings.", "output": "Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncovering data generative factors is the ultimate goal of disentanglementlearning. Although many works proposed disentangling generative models able touncover the underlying generative factors of a dataset, so far no one was ableto uncover OOD generative factors (i.e., factors of variations that are notexplicitly shown on the dataset). Moreover, the datasets used to validate thesemodels are synthetically generated using a balanced mixture of some predefinedgenerative factors, implicitly assuming that generative factors are uniformlydistributed across the datasets. However, real datasets do not present thisproperty. In this work we analyse the effect of using datasets with unbalancedgenerative factors, providing qualitative and quantitative results for widelyused generative models. Moreover, we propose TC-VAE, a generative modeloptimized using a lower bound of the joint total correlation between thelearned latent representations and the input data. We show that the proposedmodel is able to uncover OOD generative factors on different datasets andoutperforms on average the related baselines in terms of downstreamdisentanglement metrics.", "output": "TC-VAE: Uncovering Out-of-Distribution Data Generative Factors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Approximate computing methods have shown great potential for deep learning.Due to the reduced hardware costs, these methods are especially suitable forinference tasks on battery-operated devices that are constrained by their powerbudget. However, approximate computing hasn't reached its full potential due tothe lack of work on training methods. In this work, we discuss training methodsfor approximate hardware. We demonstrate how training needs to be specializedfor approximate hardware, and propose methods to speed up the training processby up to 18X.", "output": "Training Neural Networks for Execution on Approximate Hardware."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Sat-NeRF, a modified implementation of the recently introducedShadow Neural Radiance Field (S-NeRF) model. This method is able to synthesizenovel views from a sparse set of satellite images of a scene, while accountingfor the variation in lighting present in the pictures. The trained model canalso be used to accurately estimate the surface elevation of the scene, whichis often a desirable quantity for satellite observation applications. S-NeRFimproves on the standard Neural Radiance Field (NeRF) method by considering theradiance as a function of the albedo and the irradiance. Both these quantitiesare output by fully connected neural network branches of the model, and thelatter is considered as a function of the direct light from the sun and thediffuse color from the sky. The implementations were run on a dataset ofsatellite images, augmented using a zoom-and-crop technique. A hyperparameterstudy for NeRF was carried out, leading to intriguing observations on themodel's convergence. Finally, both NeRF and S-NeRF were run until 100k epochsin order to fully fit the data and produce their best possible predictions. Thecode related to this article can be found at", "output": "NeRF applied to satellite imagery for surface reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In some practical learning tasks, such as traffic video analysis, the numberof available training samples is restricted by different factors, such aslimited communication bandwidth and computation power; therefore, it isimperative to select diverse data samples that contribute the most to thequality of the learning system. One popular approach to selecting diversesamples is Determinantal Point Process (DPP). However, it suffers from a fewknown drawbacks, such as restriction of the number of samples to the rank ofthe similarity matrix, and not being customizable for specific learning tasks(e.g., multi-level classification tasks). In this paper, we propose a new wayof measuring task-oriented diversity based on the Rate-Distortion (RD) theory,appropriate for multi-level classification. To this end, we establish afundamental relationship between DPP and RD theory, which led to designingRD-DPP, an RD-based value function to evaluate the diversity gain of datasamples. We also observe that the upper bound of the diversity of data selectedby DPP has a universal trend of phase transition that quickly approaches itsmaximum point, then slowly converges to its final limits, meaning that DPP isbeneficial only at the beginning of sample accumulation. We use this fact todesign a bi-modal approach for sequential data selection.", "output": "RD-DPP: Rate-Distortion Theory Meets Determinantal Point Process to Diversify Learning Data Samples."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Protecting data privacy is paramount in the fields such as finance, banking,and healthcare. Federated Learning (FL) has attracted widespread attention dueto its decentralized, distributed training and the ability to protect theprivacy while obtaining a global shared model. However, FL presents challengessuch as communication overhead, and limited resource capability. This motivatedus to propose a two-stage federated learning approach toward the objective ofprivacy protection, which is a first-of-its-kind study as follows: (i) Duringthe first stage, the synthetic dataset is generated by employing two differentdistributions as noise to the vanilla conditional tabular generativeadversarial neural network (CTGAN) resulting in modified CTGAN, and (ii) In thesecond stage, the Federated Probabilistic Neural Network (FedPNN) is developedand employed for building globally shared classification model. We alsoemployed synthetic dataset metrics to check the quality of the generatedsynthetic dataset. Further, we proposed a meta-clustering algorithm whereby thecluster centers obtained from the clients are clustered at the server fortraining the global model. Despite PNN being a one-pass learning classifier,its complexity depends on the training data size. Therefore, we employed amodified evolving clustering method (ECM), another one-pass algorithm tocluster the training data thereby increasing the speed further. Moreover, weconducted sensitivity analysis by varying Dthr, a hyperparameter of ECM at theserver and client, one at a time. The effectiveness of our approach isvalidated on four finance and medical datasets.", "output": "FedPNN: One-shot Federated Classification via Evolving Clustering Method and Probabilistic Neural Network hybrid."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Subpopulation shift exists widely in many real-world applications, whichrefers to the training and test distributions that contain the samesubpopulation groups but with different subpopulation proportions. Ignoringsubpopulation shifts may lead to significant performance degradation andfairness concerns. Importance reweighting is a classical and effective way tohandle the subpopulation shift. However, recent studies have recognized thatmost of these approaches fail to improve the performance especially whenapplied to over-parameterized neural networks which are capable of fitting anytraining samples. In this work, we propose a simple yet practical framework,called reweighted mixup (RMIX), to mitigate the overfitting issue inover-parameterized models by conducting importance weighting on the ''mixed''samples. Benefiting from leveraging reweighting in mixup, RMIX allows the modelto explore the vicinal space of minority samples more, thereby obtaining morerobust model against subpopulation shift. When the subpopulation membershipsare unknown, the training-trajectories-based uncertainty estimation is equippedin the proposed RMIX to flexibly characterize the subpopulation distribution.We also provide insightful theoretical analysis to verify that RMIX achievesbetter generalization bounds over prior works. Further, we conduct extensiveempirical studies across a wide range of tasks to validate the effectiveness ofthe proposed method.", "output": "Reweighted Mixup for Subpopulation Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Distribution shift (e.g., task or domain shift) in continual learning (CL)usually results in catastrophic forgetting of neural networks. Although it canbe alleviated by repeatedly replaying buffered data, the every-step replay istime-consuming. In this paper, we study which modules in neural networks aremore prone to forgetting by investigating their training dynamics during CL.Our proposed metrics show that only a few modules are more task-specific andsensitively alter between tasks, while others can be shared across tasks ascommon knowledge. Hence, we attribute forgetting mainly to the former and findthat finetuning them only on a small buffer at the end of any CL method canbring non-trivial improvement. Due to the small number of finetuned parameters,such ``Forgetting Prioritized Finetuning (FPF)'' is efficient in computation.We further propose a more efficient and simpler method that entirely removesthe every-step replay and replaces them by only $k$-times of FPF periodicallytriggered during CL. Surprisingly, this ``$k$-FPF'' performs comparably to FPFand outperforms the SOTA CL methods but significantly reduces theircomputational overhead and cost. In experiments on several benchmarks of class-and domain-incremental CL, FPF consistently improves existing CL methods by alarge margin, and $k$-FPF further excels in efficiency without degrading theaccuracy. We also empirically studied the impact of buffer size, epochs pertask, and finetuning modules on the cost and accuracy of our methods.", "output": "Does Continual Learning Equally Forget All Parameters?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hierarchical Federated Learning (HFL) is a distributed machine learningparadigm tailored for multi-tiered computation architectures, which supportsmassive access of devices' models simultaneously. To enable efficient HFL, itis crucial to design suitable incentive mechanisms to ensure that devicesactively participate in local training. However, there are few studies onincentive mechanism design for HFL. In this paper, we design two-levelincentive mechanisms for the HFL with a two-tiered computing structure toencourage the participation of entities in each tier in the HFL training. Inthe lower-level game, we propose a coalition formation game to joint optimizethe edge association and bandwidth allocation problem, and obtain efficientcoalition partitions by the proposed preference rule, which can be proven to bestable by exact potential game. In the upper-level game, we design theStackelberg game algorithm, which not only determines the optimal number ofedge aggregations for edge servers to maximize their utility, but also optimizethe unit reward provided for the edge aggregation performance to ensure theinterests of cloud servers. Furthermore, numerical results indicate that theproposed algorithms can achieve better performance than the benchmark schemes.", "output": "Design of Two-Level Incentive Mechanisms for Hierarchical Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimization algorithms are very different from human optimizers. A humanbeing would gain more experiences through problem-solving, which helps her/himin solving a new unseen problem. Yet an optimization algorithm never gains anyexperiences by solving more problems. In recent years, efforts have been madetowards endowing optimization algorithms with some abilities of experiencelearning, which is regarded as experience-based optimization. In this paper, weargue that hard optimization problems could be tackled efficiently by makingbetter use of experiences gained in related problems. We demonstrate our ideasin the context of expensive optimization, where we aim to find a near-optimalsolution to an expensive optimization problem with as few fitness evaluationsas possible. To achieve this, we propose an experience-based surrogate-assistedevolutionary algorithm (SAEA) framework to enhance the optimization efficiencyof expensive problems, where experiences are gained across related expensivetasks via a novel meta-learning method. These experiences serve as thetask-independent parameters of a deep kernel learning surrogate, then thesolutions sampled from the target task are used to adapt task-specificparameters for the surrogate. With the help of experience learning, competitiveregression-based surrogates can be initialized using only 1$d$ solutions fromthe target task ($d$ is the dimension of the decision space). Our experimentalresults on expensive multi-objective and constrained optimization problemsdemonstrate that experiences gained from related tasks are beneficial for thesaving of evaluation budgets on the target problem.", "output": "Experience-Based Evolutionary Algorithms for Expensive Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) obtain tremendous success in modeling relationaldata. Still, they are prone to adversarial attacks, which are massive threatsto applying GNNs to risk-sensitive domains. Existing defensive methods neitherguarantee performance facing new data/tasks or adversarial attacks nor provideinsights to understand GNN robustness from an architectural perspective. NeuralArchitecture Search (NAS) has the potential to solve this problem by automatingGNN architecture designs. Nevertheless, current graph NAS approaches lackrobust design and are vulnerable to adversarial attacks. To tackle thesechallenges, we propose a novel Robust Neural Architecture search framework forGNNs (G-RNA). Specifically, we design a robust search space for themessage-passing mechanism by adding graph structure mask operations into thesearch space, which comprises various defensive operation candidates and allowsus to search for defensive GNNs. Furthermore, we define a robustness metric toguide the search procedure, which helps to filter robust architectures. In thisway, G-RNA helps understand GNN robustness from an architectural perspectiveand effectively searches for optimal adversarial robust GNNs. Extensiveexperimental results on benchmark datasets show that G-RNA significantlyoutperforms manually designed robust GNNs and vanilla graph NAS baselines by12.1% to 23.4% under adversarial attacks.", "output": "Adversarially Robust Neural Architecture Search for Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider distributed learning scenarios where M machines interact with aparameter server along several communication rounds in order to minimize ajoint objective function. Focusing on the heterogeneous case, where differentmachines may draw samples from different data-distributions, we design thefirst local update method that provably benefits over the two most prominentdistributed baselines: namely Minibatch-SGD and Local-SGD. Key to our approachis a slow querying technique that we customize to the distributed setting,which in turn enables a better mitigation of the bias caused by local updates.", "output": "SLowcal-SGD: Slow Query Points Improve Local-SGD for Stochastic Convex Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider stochastic convex optimization problems where the objective is anexpectation over smooth functions. For this setting we suggest a novel gradientestimate that combines two recent mechanism that are related to notion ofmomentum. Then, we design an SGD-style algorithm as well as an acceleratedversion that make use of this new estimator, and demonstrate the robustness ofthese new approaches to the choice of the learning rate. Concretely, we showthat these approaches obtain the optimal convergence rates for both noiselessand noisy case with the same choice of fixed learning rate. Moreover, for thenoisy case we show that these approaches achieve the same optimal bound for avery wide range of learning rates.", "output": "$\\mu^2$-SGD: Stable Stochastic Optimization via a Double Momentum Mechanism."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The conditional randomization test (CRT) was recently proposed to testwhether two random variables X and Y are conditionally independent given randomvariables Z. The CRT assumes that the conditional distribution of X given Z isknown under the null hypothesis and then it is compared to the distribution ofthe observed samples of the original data. The aim of this paper is to developa novel alternative of CRT by using nearest-neighbor sampling without assumingthe exact form of the distribution of X given Z. Specifically, we utilize thecomputationally efficient 1-nearest-neighbor to approximate the conditionaldistribution that encodes the null hypothesis. Then, theoretically, we showthat the distribution of the generated samples is very close to the trueconditional distribution in terms of total variation distance. Furthermore, wetake the classifier-based conditional mutual information estimator as our teststatistic. The test statistic as an empirical fundamental information theoreticquantity is able to well capture the conditional-dependence feature. We showthat our proposed test is computationally very fast, while controlling type Iand II errors quite well. Finally, we demonstrate the efficiency of ourproposed test in both synthetic and real data analyses.", "output": "Nearest-Neighbor Sampling Based Conditional Independence Testing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Implicit Neural Representations (INRs) have recently exhibited immensepotential in the field of scientific visualization for both data generation andvisualization tasks. However, these representations often consist of largemulti-layer perceptrons (MLPs), necessitating millions of operations for asingle forward pass, consequently hindering interactive visual exploration.While reducing the size of the MLPs and employing efficient parametric encodingschemes can alleviate this issue, it compromises generalizability for unseenparameters, rendering it unsuitable for tasks such as temporalsuper-resolution. In this paper, we introduce HyperINR, a novel hypernetworkarchitecture capable of directly predicting the weights for a compact INR. Byharnessing an ensemble of multiresolution hash encoding units in unison, theresulting INR attains state-of-the-art inference performance (up to 100x higherinference bandwidth) and can support interactive photo-realistic volumevisualization. Additionally, by incorporating knowledge distillation,exceptional data and visualization generation quality is achieved, making ourmethod valuable for real-time parameter exploration. We validate theeffectiveness of the HyperINR architecture through a comprehensive ablationstudy. We showcase the versatility of HyperINR across three distinct scientificdomains: novel view synthesis, temporal super-resolution of volume data, andvolume rendering with dynamic global shadows. By simultaneously achievingefficiency and generalizability, HyperINR paves the way for applying INR in awider array of scientific visualization applications.", "output": "HyperINR: A Fast and Predictive Hypernetwork for Implicit Neural Representations via Knowledge Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The deep feedforward neural networks (DNNs) are increasingly deployed insocioeconomic critical decision support software systems. DNNs areexceptionally good at finding minimal, sufficient statistical patterns withintheir training data. Consequently, DNNs may learn to encode decisions --amplifying existing biases or introducing new ones -- that may disadvantageprotected individuals/groups and may stand to violate legal protections. Whilethe existing search based software testing approaches have been effective indiscovering fairness defects, they do not supplement these defects withdebugging aids -- such as severity and causal explanations -- crucial to helpdevelopers triage and decide on the next course of action. Can we measure theseverity of fairness defects in DNNs? Are these defects symptomatic of impropertraining or they merely reflect biases present in the training data? To answersuch questions, we present DICE: an information-theoretic testing and debuggingframework to discover and localize fairness defects in DNNs.The key goal of DICE is to assist software developers in triaging fairnessdefects by ordering them by their severity. Towards this goal, we quantifyfairness in terms of protected information (in bits) used in decision making. Aquantitative view of fairness defects not only helps in ordering these defects,our empirical evaluation shows that it improves the search efficiency due toresulting smoothness of the search space. Guided by the quantitative fairness,we present a causal debugging framework to localize inadequately trained layersand neurons responsible for fairness defects. Our experiments over ten DNNs,developed for socially critical tasks, show that DICE efficiently characterizesthe amounts of discrimination, effectively generates discriminatory instances,and localizes layers/neurons with significant biases.", "output": "Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern society, road safety relies heavily on the psychological andphysiological state of drivers. Negative factors such as fatigue, drowsiness,and stress can impair drivers' reaction time and decision making abilities,leading to an increased incidence of traffic accidents. Among the numerousstudies for impaired driving detection, wearable physiological measurement is areal-time approach to monitoring a driver's state. However, currently, thereare few driver physiological datasets in open road scenarios and the existingdatasets suffer from issues such as poor signal quality, small sample sizes,and short data collection periods. Therefore, in this paper, a large-scalemultimodal driving dataset for driver impairment detection and biometric datarecognition is designed and described. The dataset contains two modalities ofdriving signals: six-axis inertial signals and electrocardiogram (ECG) signals,which were recorded while over one hundred drivers were following the sameroute through open roads during several months. Both the ECG signal sensor andthe six-axis inertial signal sensor are installed on a specially designedsteering wheel cover, allowing for data collection without disturbing thedriver. Additionally, electrodermal activity (EDA) signals were also recordedduring the driving process and will be integrated into the presented datasetsoon. Future work can build upon this dataset to advance the field of driverimpairment detection. New methods can be explored for integrating other typesof biometric signals, such as eye tracking, to further enhance theunderstanding of driver states. The insights gained from this dataset can alsoinform the development of new driver assistance systems, promoting saferdriving practices and reducing the risk of traffic accidents. The OpenDriverdataset will be publicly available soon.", "output": "OpenDriver: an open-road driver state detection dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomaly detection suffered from the lack of anomalies due to the diversity ofabnormalities and the difficulties of obtaining large-scale anomaly data.Semi-supervised anomaly detection methods are often used to solely leveragenormal data to detect abnormalities that deviated from the learnt normalitydistributions. Meanwhile, given the fact that limited anomaly data can beobtained with a minor cost in practice, some researches also investigatedanomaly detection methods under supervised scenarios with limited anomaly data.In order to address the lack of abnormal data for robust anomaly detection, wepropose Adversarial Generative Anomaly Detection (AGAD), a self-contrast-basedanomaly detection paradigm that learns to detect anomalies by generatingtextit{contextual adversarial information} from the massive normal examples.Essentially, our method generates pseudo-anomaly data for both supervised andsemi-supervised anomaly detection scenarios. Extensive experiments are carriedout on multiple benchmark datasets and real-world datasets, the results showsignificant improvement in both supervised and semi-supervised scenarios.Importantly, our approach is data-efficient that can boost up the detectionaccuracy with no more than 5% anomalous training data.", "output": "AGAD: Adversarial Generative Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents RISC, an open-source Python package data generator( RISC generates look-alike automobileinsurance contracts based on the Quebec regulatory insurance form in French andEnglish. Insurance contracts are 90 to 100 pages long and use complex legal andinsurance-specific vocabulary for a layperson. Hence, they are a much morecomplex class of documents than those in traditional NLP corpora. Therefore, weintroduce RISCBAC, a Realistic Insurance Synthetic Bilingual AutomobileContract dataset based on the mandatory Quebec car insurance contract. Thedataset comprises 10,000 French and English unannotated insurance contracts.RISCBAC enables NLP research for unsupervised automatic summarisation, questionanswering, text simplification, machine translation and more. Moreover, it canbe further automatically annotated as a dataset for supervised tasks such asNER", "output": "RISC: Generating Realistic Synthetic Bilingual Insurance Contract."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the model aging problem, Deep Neural Networks (DNNs) need updates toadjust them to new data distributions. The common practice leveragesincremental learning (IL), e.g., Class-based Incremental Learning (CIL) thatupdates output labels, to update the model with new data and a limited numberof old data. This avoids heavyweight training (from scratch) using conventionalmethods and saves storage space by reducing the number of old data to store.But it also leads to poor performance in fairness. In this paper, we show thatCIL suffers both dataset and algorithm bias problems, and existing solutionscan only partially solve the problem. We propose a novel framework, CILIATE,that fixes both dataset and algorithm bias in CIL. It features a noveldifferential analysis guided dataset and training refinement process thatidentifies unique and important samples overlooked by existing CIL and enforcesthe model to learn from them. Through this process, CILIATE improves thefairness of CIL by 17.03%, 22.46%, and 31.79% compared to state-of-the-artmethods, iCaRL, BiC, and WA, respectively, based on our evaluation on threepopular datasets and widely used ResNet models.", "output": "CILIATE: Towards Fairer Class-based Incremental Learning by Dataset and Training Refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Based on the variational method, we propose a novel paradigm that provides aunified framework of training neural operators and solving partial differentialequations (PDEs) with the variational form, which we refer to as thevariational operator learning (VOL). We first derive the functionalapproximation of the system from the node solution prediction given by neuraloperators, and then conduct the variational operation by automaticdifferentiation, constructing a forward-backward propagation loop to derive theresidual of the linear system. One or several update steps of the steepestdecent method (SD) and the conjugate gradient method (CG) are provided in everyiteration as a cheap yet effective update for training the neural operators.Experimental results show the proposed VOL can learn a variety of solutionoperators in PDEs of the steady heat transfer and the variable stiffnesselasticity with satisfactory results and small error. The proposed VOL achievesnearly label-free training. Only five to ten labels are used for the outputdistribution-shift session in all experiments. Generalization benefits of theVOL are investigated and discussed.", "output": "Variational operator learning: A unified paradigm for training neural operators and solving partial differential equations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this article, we strengthen the proof methods of some previously weaklyconsistent variants of random forests into strongly consistent proof methods,and improve the data utilization of these variants, in order to obtain bettertheoretical properties and experimental performance. In addition, based on themultinomial random forest (MRF) and Bernoulli random forest (BRF), we propose adata-driven multinomial random forest (DMRF) algorithm, which has lowercomplexity than MRF and higher complexity than BRF while satisfying strongconsistency. It has better performance in classification and regressionproblems than previous RF variants that only satisfy weak consistency, and inmost cases even surpasses standard random forest. To the best of our knowledge,DMRF is currently the most excellent strongly consistent RF variant with lowalgorithm complexity", "output": "Data-driven multinomial random forest."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Methods for making high-quality recommendations often rely on learning latentrepresentations from interaction data. These methods, while performant, do notprovide ready mechanisms for users to control the recommendation they receive.Our work tackles this problem by proposing LACE, a novel concept valuebottleneck model for controllable text recommendations. LACE represents eachuser with a succinct set of human-readable concepts through retrieval givenuser-interacted documents and learns personalized representations of theconcepts based on user documents. This concept based user profile is thenleveraged to make recommendations. The design of our model affords control overthe recommendations through a number of intuitive interactions with atransparent user profile. We first establish the quality of recommendationsobtained from LACE in an offline evaluation on three recommendation tasksspanning six datasets in warm-start, cold-start, and zero-shot setups. Next, wevalidate the controllability of LACE under simulated user interactions.Finally, we implement LACE in an interactive controllable recommender systemand conduct a user study to demonstrate that users are able to improve thequality of recommendations they receive through interactions with an editableuser profile.", "output": "Editable User Profiles for Controllable Text Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "MANET is a collection of mobile nodes that communicate through wirelessnetworks as they move from one point to another. MANET is aninfrastructure-less network with a changeable topology; as a result, it is verysusceptible to attacks. MANET attack prevention represents a seriousdifficulty. Malicious network nodes are the source of network-based attacks. Ina MANET, attacks can take various forms, and each one alters the network'soperation in its unique way. In general, attacks can be separated into twocategories: those that target the data traffic on a network and those thattarget the control traffic. This article explains the many sorts of assaults,their impact on MANET, and the MANET-based defence measures that are currentlyin place. The suggested SRA that employs blockchain technology (SRABC) protectsMANET from attacks and authenticates nodes. The secure routing algorithm (SRA)proposed by blockchain technology safeguards control and data flow againstthreats. This is achieved by generating a Hash Function for every transaction.We will begin by discussing the security of the MANET. This article's secondsection explores the role of blockchain in MANET security. In the thirdsection, the SRA is described in connection with blockchain. In the fourthphase, PDR and Throughput are utilised to conduct an SRA review usingBlockchain employing PDR and Throughput. The results suggest that the proposedtechnique enhances MANET security while concurrently decreasing delay. Theperformance of the proposed technique is analysed and compared to the routingprotocols Q-AODV and DSR.", "output": "Secure Routing Protocol To Mitigate Attacks By Using Blockchain Technology In Manet."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data valuation is a growing research field that studies the influence ofindividual data points for machine learning (ML) models. Data Shapley, inspiredby cooperative game theory and economics, is an effective method for datavaluation. However, it is well-known that the Shapley value (SV) can becomputationally expensive. Fortunately, Jia et al. (2019) showed that forK-Nearest Neighbors (KNN) models, the computation of Data Shapley issurprisingly simple and efficient.In this note, we revisit the work of Jia et al. (2019) and propose a morenatural and interpretable utility function that better reflects the performanceof KNN models. We derive the corresponding calculation procedure for the DataShapley of KNN classifiers/regressors with the new utility functions. Our newapproach, dubbed soft-label KNN-SV, achieves the same time complexity as theoriginal method. We further provide an efficient approximation algorithm forsoft-label KNN-SV based on locality sensitive hashing (LSH). Our experimentalresults demonstrate that Soft-label KNN-SV outperforms the original method onmost datasets in the task of mislabeled data detection, making it a betterbaseline for future work on data valuation.", "output": "A Note on \"Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms\"."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning in real-world scenarios is a major challenge. A generalcontinual learning model should have a constant memory size and no predefinedtask boundaries, as is the case in semi-supervised Video Object Segmentation(VOS), where continual learning challenges particularly present themselves inworking on long video sequences. In this article, we first formulate theproblem of semi-supervised VOS, specifically online VOS, as a continuallearning problem, and then secondly provide a public VOS dataset, CLVOS23,focusing on continual learning. Finally, we propose and implement aregularization-based continual learning approach on LWL, an existing online VOSbaseline, to demonstrate the efficacy of continual learning when applied toonline VOS and to establish a CLVOS23 baseline. We apply the proposed baselineto the Long Videos dataset as well as to two short video VOS datasets, DAVIS16and DAVIS17. To the best of our knowledge, this is the first time that VOS hasbeen defined and addressed as a continual learning problem.", "output": "CLVOS23: A Long Video Object Segmentation Dataset for Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion Models (DMs), also referred to as score-based diffusion models,utilize neural networks to specify score functions. Unlike most otherprobabilistic models, DMs directly model the score functions, which makes themmore flexible to parametrize and potentially highly expressive forprobabilistic modeling. DMs can learn fine-grained knowledge, i.e., marginalscore functions, of the underlying distribution. Therefore, a crucial researchdirection is to explore how to distill the knowledge of DMs and fully utilizetheir potential. Our objective is to provide a comprehensible overview of themodern approaches for distilling DMs, starting with an introduction to DMs anda discussion of the challenges involved in distilling them into neural vectorfields. We also provide an overview of the existing works on distilling DMsinto both stochastic and deterministic implicit generators. Finally, we reviewthe accelerated diffusion sampling algorithms as a training-free method fordistillation. Our tutorial is intended for individuals with a basicunderstanding of generative models who wish to apply DM's distillation orembark on a research project in this field.", "output": "A Comprehensive Survey on Knowledge Distillation of Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Labeling time series data is an expensive task because of domain expertiseand dynamic nature of the data. Hence, we often have to deal with limitedlabeled data settings. Data augmentation techniques have been successfullydeployed in domains like computer vision to exploit the use of existing labeleddata. We adapt one of the most commonly used technique called MixUp, in thetime series domain. Our proposed, MixUp++ and LatentMixUp++, use simplemodifications to perform interpolation in raw time series and classificationmodel's latent space, respectively. We also extend these methods withsemi-supervised learning to exploit unlabeled data. We observe significantimprovements of 1% - 15% on time series classification on two publicdatasets, for both low labeled data as well as high labeled data regimes, withLatentMixUp++.", "output": "Embarrassingly Simple MixUp for Time-series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Through this paper, we introduce a novel driver cognitive load assessmentdataset, CL-Drive, which contains Electroencephalogram (EEG) signals along withother physiological signals such as Electrocardiography (ECG) and ElectrodermalActivity (EDA) as well as eye tracking data. The data was collected from 21subjects while driving in an immersive vehicle simulator, in various drivingconditions, to induce different levels of cognitive load in the subjects. Thetasks consisted of 9 complexity levels for 3 minutes each. Each driver reportedtheir subjective cognitive load every 10 seconds throughout the experiment. Thedataset contains the subjective cognitive load recorded as ground truth. Inthis paper, we also provide benchmark classification results for differentmachine learning and deep learning models for both binary and ternary labeldistributions. We followed 2 evaluation criteria namely 10-fold andleave-one-subject-out (LOSO). We have trained our models on both hand-craftedfeatures as well as on raw data.", "output": "Multimodal Brain-Computer Interface for In-Vehicle Driver Cognitive Load Measurement: Dataset and Baselines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Missing data in time series is a challenging issue affecting time seriesanalysis. Missing data occurs due to problems like data drops or sensormalfunctioning. Imputation methods are used to fill in these values, withquality of imputation having a significant impact on downstream tasks likeclassification. In this work, we propose a semi-supervised imputation method,ST-Impute, that uses both unlabeled data along with downstream task's labeleddata. ST-Impute is based on sparse self-attention and trains on tasks thatmimic the imputation process. Our results indicate that the proposed methodoutperforms the existing supervised and unsupervised time series imputationmethods measured on the imputation quality as well as on the downstream tasksingesting imputed time series.", "output": "Filling out the missing gaps: Time Series Imputation with Semi-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a distributed Generative Adversarial Networks(discGANs) to generate synthetic tabular data specific to the healthcaredomain. While using GANs to generate images has been well studied, little to noattention has been given to generation of tabular data. Modeling distributionsof discrete and continuous tabular data is a non-trivial task with highutility. We applied discGAN to model non-Gaussian multi-modal healthcare data.We generated 249,000 synthetic records from original 2,027 eICU dataset. Weevaluated the performance of the model using machine learning efficacy, theKolmogorov-Smirnov (KS) test for continuous variables and chi-squared test fordiscrete variables. Our results show that discGAN was able to generate datawith distributions similar to the real data.", "output": "Distributed Conditional GAN (discGAN) For Synthetic Healthcare Data Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapid advancement in data-driven research has increased the demand foreffective graph data analysis. However, real-world data often exhibits classimbalance, leading to poor performance of machine learning models. To overcomethis challenge, class-imbalanced learning on graphs (CILG) has emerged as apromising solution that combines the strengths of graph representation learningand class-imbalanced learning. In recent years, significant progress has beenmade in CILG. Anticipating that such a trend will continue, this survey aims tooffer a comprehensive understanding of the current state-of-the-art in CILG andprovide insights for future research directions. Concerning the former, weintroduce the first taxonomy of existing work and its connection to existingimbalanced learning literature. Concerning the latter, we critically analyzerecent work in CILG and discuss urgent lines of inquiry within the topic.Moreover, we provide a continuously maintained reading list of papers and codeat ", "output": "Class-Imbalanced Learning on Graphs: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In applied fields where the speed of inference and model flexibility arecrucial, the use of Bayesian inference for models with a stochastic process astheir prior, e.g. Gaussian processes (GPs) is ubiquitous. Recent literature hasdemonstrated that the computational bottleneck caused by GP priors or theirfinite realizations can be encoded using deep generative models such asvariational autoencoders (VAEs), and the learned generators can then be usedinstead of the original priors during Markov chain Monte Carlo (MCMC) inferencein a drop-in manner. While this approach enables fast and highly efficientinference, it loses information about the stochastic process hyperparameters,and, as a consequence, makes inference over hyperparameters impossible and thelearned priors indistinct. We propose to resolve the aforementioned issue anddisentangle the learned priors by conditioning the VAE on stochastic processhyperparameters. This way, the hyperparameters are encoded alongside GPrealisations and can be explicitly estimated at the inference stage. We believethat the new method, termed PriorCVAE, will be a useful tool among approximateinference approaches and has the potential to have a large impact on spatialand spatiotemporal inference in crucial real-life applications. Code showcasingthe PriorCVAE technique can be accessed via the following link:", "output": "PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate time series forecasting is critical for a wide range of problemswith temporal data. Ensemble modeling is a well-established technique forleveraging multiple predictive models to increase accuracy and robustness, asthe performance of a single predictor can be highly variable due to shifts inthe underlying data distribution. This paper proposes a new methodology forbuilding robust ensembles of time series forecasting models. Our approachutilizes Adaptive Robust Optimization (ARO) to construct a linear regressionensemble in which the models' weights can adapt over time. We demonstrate theeffectiveness of our method through a series of synthetic experiments andreal-world applications, including air pollution management, energy consumptionforecasting, and tropical cyclone intensity forecasting. Our results show thatour adaptive ensembles outperform the best ensemble member in hindsight by16-26% in root mean square error and 14-28% in conditional value at risk andimprove over competitive ensemble techniques.", "output": "Ensemble Modeling for Time Series Forecasting: an Adaptive Robust Optimization Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Meta-learning has arisen as a successful method for improving trainingperformance by training over many similar tasks, especially with deep neuralnetworks (DNNs). However, the theoretical understanding of when and whyoverparameterized models such as DNNs can generalize well in meta-learning isstill limited. As an initial step towards addressing this challenge, this paperstudies the generalization performance of overfitted meta-learning under alinear regression model with Gaussian features. In contrast to a few recentstudies along the same line, our framework allows the number of modelparameters to be arbitrarily larger than the number of features in the groundtruth signal, and hence naturally captures the overparameterized regime inpractical deep meta-learning. We show that the overfitted min $ell_2$-normsolution of model-agnostic meta-learning (MAML) can be beneficial, which issimilar to the recent remarkable findings on ``benign overfitting'' and``double descent'' phenomenon in the classical (single-task) linear regression.However, due to the uniqueness of meta-learning such as task-specific gradientdescent inner training and the diversity/fluctuation of the ground-truthsignals among training tasks, we find new and interesting properties that donot exist in single-task linear regression. We first provide a high-probabilityupper bound (under reasonable tightness) on the generalization error, wherecertain terms decrease when the number of features increases. Our analysissuggests that benign overfitting is more significant and easier to observe whenthe noise and the diversity/fluctuation of the ground truth of each trainingtask are large. Under this circumstance, we show that the overfitted min$ell_2$-norm solution can achieve an even lower generalization error than theunderparameterized solution.", "output": "Theoretical Characterization of the Generalization Performance of Overfitted Meta-Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Microseismic source imaging plays a significant role in passive seismicmonitoring. However, such a process is prone to failure due to the aliasingproblem when dealing with sparse measured data. Thus, we propose a directmicroseismic imaging framework based on physics-informed neural networks(PINNs), which can generate focused source images, even with very sparserecordings. We use the PINNs to represent a multi-frequency wavefield and thenapply the inverse Fourier transform to extract the source image. Specially, wemodify the representation of the frequency-domain wavefield to inherentlysatisfy the boundary conditions (the measured data on the surface) by means ofthe hard constraint, which helps to avoid the difficulty in balancing the dataand PDE losses in PINNs. Furthermore, we propose the causality lossimplementation with respect to depth to enhance the convergence of PINNs. Thenumerical experiments on the Overthrust model show that the method can admitreliable and accurate source imaging for single- or multiple- sources and evenin passive monitoring settings. Then, we further apply our method on thehydraulic fracturing field data, and demonstrate that our method can correctlyimage the source.", "output": "Microseismic source imaging using physics-informed neural networks with hard constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Decentralized learning enables serverless training of deep neural networks(DNNs) in a distributed manner on multiple nodes. This allows for the use oflarge datasets, as well as the ability to train with a wide variety of datasources. However, one of the key challenges with decentralized learning isheterogeneity in the data distribution across the nodes. In this paper, wepropose In-Distribution Knowledge Distillation (IDKD) to address the challengeof heterogeneous data distribution. The goal of IDKD is to homogenize the datadistribution across the nodes. While such data homogenization can be achievedby exchanging data among the nodes sacrificing privacy, IDKD achieves the sameobjective using a common public dataset across nodes without breaking theprivacy constraint. This public dataset is different from the training datasetand is used to distill the knowledge from each node and communicate it to itsneighbors through the generated labels. With traditional knowledgedistillation, the generalization of the distilled model is reduced because allthe public dataset samples are used irrespective of their similarity to thelocal dataset. Thus, we introduce an Out-of-Distribution (OoD) detector at eachnode to label a subset of the public dataset that maps close to the localtraining data distribution. Finally, only labels corresponding to these subsetsare exchanged among the nodes and with appropriate label averaging each node isfinetuned on these data subsets along with its local data. Our experiments onmultiple image classification datasets and graph topologies show that theproposed IDKD scheme is more effective than traditional knowledge distillationand achieves state-of-the-art generalization performance on heterogeneouslydistributed data with minimal communication overhead.", "output": "Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of pretrained embeddings has become widespread in modern e-commercemachine learning (ML) systems. In practice, however, we have encounteredseveral key issues when using pretrained embedding in a real-world productionsystem, many of which cannot be fully explained by current knowledge.Unfortunately, we find that there is a lack of a thorough understanding of howpre-trained embeddings work, especially their intrinsic properties andinteractions with downstream tasks. Consequently, it becomes challenging tomake interactive and scalable decisions regarding the use of pre-trainedembeddings in practice.Our investigation leads to two significant discoveries about using pretrainedembeddings in e-commerce applications. Firstly, we find that the design of thepretraining and downstream models, particularly how they encode and decodeinformation via embedding vectors, can have a profound impact. Secondly, weestablish a principled perspective of pre-trained embeddings via the lens ofkernel analysis, which can be used to evaluate their predictability,interactively and scalably. These findings help to address the practicalchallenges we faced and offer valuable guidance for successful adoption ofpretrained embeddings in real-world production. Our conclusions are backed bysolid theoretical reasoning, benchmark experiments, as well as online testings.", "output": "Pretrained Embeddings for E-commerce Machine Learning: When it Fails and Why?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the trade-off between expectation and tail risk for regretdistribution in the stochastic multi-armed bandit problem. We fullycharacterize the interplay among three desired properties for policy design:worst-case optimality, instance-dependent consistency, and light-tailed risk.We show how the order of expected regret exactly affects the decaying rate ofthe regret tail probability for both the worst-case and instance-dependentscenario. A novel policy is proposed to characterize the optimal regret tailprobability for any regret threshold. Concretely, for any given $alphain[1/2,1)$ and $betain[0, alpha]$, our policy achieves a worst-case expected regretof $tilde O(T^alpha)$ (we call it $alpha$-optimal) and an instance-dependentexpected regret of $tilde O(T^beta)$ (we call it $beta$-consistent), whileenjoys a probability of incurring an $tilde O(T^delta)$ regret($deltageqalpha$ in the worst-case scenario and $deltageqbeta$ in theinstance-dependent scenario) that decays exponentially with a polynomial $T$term. Such decaying rate is proved to be best achievable. Moreover, we discoveran intrinsic gap of the optimal tail rate under the instance-dependent scenariobetween whether the time horizon $T$ is known a priori or not. Interestingly,when it comes to the worst-case scenario, this gap disappears. Finally, weextend our proposed policy design to (1) a stochastic multi-armed banditsetting with non-stationary baseline rewards, and (2) a stochastic linearbandit setting. Our results reveal insights on the trade-off between regretexpectation and regret tail risk for both worst-case and instance-dependentscenarios, indicating that more sub-optimality and inconsistency leave spacefor more light-tailed risk of incurring a large regret, and that knowing theplanning horizon in advance can make a difference on alleviating tail risks.", "output": "Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation and Tail Risk."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Black-box adversarial attacks have shown strong potential to subvert machinelearning models. Existing black-box adversarial attacks craft the adversarialexamples by iteratively querying the target model and/or leveraging thetransferability of a local surrogate model. Whether such attack can succeedremains unknown to the adversary when empirically designing the attack. In thispaper, to our best knowledge, we take the first step to study a new paradigm ofadversarial attacks -- certifiable black-box attack that can guarantee theattack success rate of the crafted adversarial examples. Specifically, werevise the randomized smoothing to establish novel theories for ensuring theattack success rate of the adversarial examples. To craft the adversarialexamples with the certifiable attack success rate (CASR) guarantee, we designseveral novel techniques, including a randomized query method to query thetarget model, an initialization method with smoothed self-supervisedperturbation to derive certifiable adversarial examples, and a geometricshifting method to reduce the perturbation size of the certifiable adversarialexamples for better imperceptibility. We have comprehensively evaluated theperformance of the certifiable black-box attack on CIFAR10 and ImageNetdatasets against different levels of defenses. Both theoretical andexperimental results have validated the effectiveness of the proposedcertifiable attack.", "output": "Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in diffusion models enable many powerful instruments forimage editing. One of these instruments is text-driven image manipulations:editing semantic attributes of an image according to the provided textdescription. % Popular text-conditional diffusion models offer varioushigh-quality image manipulation methods for a broad range of text prompts.Existing diffusion-based methods already achieve high-quality imagemanipulations for a broad range of text prompts. However, in practice, thesemethods require high computation costs even with a high-end GPU. This greatlylimits potential real-world applications of diffusion-based image editing,especially when running on user devices.In this paper, we address efficiency of the recent text-driven editingmethods based on unconditional diffusion models and develop a novel algorithmthat learns image manipulations 4.5-10 times faster and applies them 8 timesfaster. We carefully evaluate the visual quality and expressiveness of ourapproach on multiple datasets using human annotators. Our experimentsdemonstrate that our algorithm achieves the quality of much more expensivemethods. Finally, we show that our approach can adapt the pretrained model tothe user-specified image and text description on the fly just for 4 seconds. Inthis setting, we notice that more compact unconditional diffusion models can beconsidered as a rational alternative to the popular text-conditionalcounterparts.", "output": "Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solving the ground state and the ground-state properties of quantum many-bodysystems is generically a hard task for classical algorithms. For a family ofHamiltonians defined on an $m$-dimensional space of physical parameters, theground state and its properties at an arbitrary parameter configuration can bepredicted via a machine learning protocol up to a prescribed prediction error$varepsilon$, provided that a sample set (of size $N$) of the states can beefficiently prepared and measured. In a recent work [Huang et al., Science 377,eabk3333 (2022)], a rigorous guarantee for such an generalization was proved.Unfortunately, an exponential scaling, $N = m^{ {cal{O}}left(frac{1}{varepsilon} right) }$, was found to be universal for genericgapped Hamiltonians. This result applies to the situation where the dimensionof the parameter space is large while the scaling with the accuracy is not anurgent factor, not entering the realm of more precise learning and prediction.In this work, we consider an alternative scenario, where $m$ is a finite, notnecessarily large constant while the scaling with the prediction error becomesthe central concern. By exploiting physical constraints and positive goodkernels for predicting the density matrix, we rigorously obtain anexponentially improved sample complexity, $N = mathrm{poly}left(varepsilon^{-1}, n, log frac{1}{delta}right)$, where $mathrm{poly}$denotes a polynomial function; $n$ is the number of qubits in the system, and($1-delta$) is the probability of success. Moreover, if restricted to learningground-state properties with strong locality assumptions, the number of samplescan be further reduced to $N = mathrm{poly} left(varepsilon^{-1}, logfrac{n}{delta}right)$. This provably rigorous result represents asignificant improvement and an indispensable extension of the existing work.", "output": "Exponentially Improved Efficient Machine Learning for Quantum Many-body States with Provable Guarantees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing approaches for autonomous control of pan-tilt-zoom (PTZ) cameras usemultiple stages where object detection and localization are performedseparately from the control of the PTZ mechanisms. These approaches requiremanual labels and suffer from performance bottlenecks due to error propagationacross the multi-stage flow of information. The large size of object detectionneural networks also makes prior solutions infeasible for real-time deploymentin resource-constrained devices. We present an end-to-end deep reinforcementlearning (RL) solution called Eagle to train a neural network policy thatdirectly takes images as input to control the PTZ camera. Trainingreinforcement learning is cumbersome in the real world due to labeling effort,runtime environment stochasticity, and fragile experimental setups. Weintroduce a photo-realistic simulation framework for training and evaluation ofPTZ camera control policies. Eagle achieves superior camera control performanceby maintaining the object of interest close to the center of captured images athigh resolution and has up to 17% more tracking duration than thestate-of-the-art. Eagle policies are lightweight (90x fewer parameters thanYolo5s) and can run on embedded camera platforms such as Raspberry PI (33 FPS)and Jetson Nano (38 FPS), facilitating real-time PTZ tracking forresource-constrained environments. With domain randomization, Eagle policiestrained in our simulator can be transferred directly to real-world scenarios.", "output": "Eagle: End-to-end Deep Reinforcement Learning based Autonomous Control of PTZ Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One major issue in learning-based model predictive control (MPC) forautonomous driving is the contradiction between the system model's predictionaccuracy and computation efficiency. The more situations a system model covers,the more complex it is, along with highly nonlinear and nonconvex properties.These issues make the optimization too complicated to solve and renderreal-time control impractical.To address these issues, we propose ahierarchical learning residual model which leverages random forests and linearregression.The learned model consists of two levels. The low level uses linearregression to fit the residues, and the high level uses random forests toswitch different linear models. Meanwhile, we adopt the linear dynamic bicyclemodel with error states as the nominal model.The switched linear regressionmodel is added to the nominal model to form the system model. It reformulatesthe learning-based MPC as a quadratic program (QP) problem and optimizationsolvers can effectively solve it. Experimental path tracking results show thatthe driving vehicle's prediction accuracy and tracking accuracy aresignificantly improved compared with the nominal MPC.Compared with thestate-of-the-art Gaussian process-based nonlinear model predictive control(GP-NMPC), our method gets better performance on tracking accuracy whilemaintaining a lower computation consumption.", "output": "Learning Residual Model of Model Predictive Control via Random Forests for Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human intelligence has the remarkable ability to assemble basic skills intocomplex ones so as to solve complex tasks. This ability is equally importantfor Artificial Intelligence (AI), and thus, we assert that in addition to thedevelopment of large, comprehensive intelligent models, it is equally crucialto equip such models with the capability to harness various domain-specificexpert models for complex task-solving in the pursuit of Artificial GeneralIntelligence (AGI). Recent developments in Large Language Models (LLMs) havedemonstrated remarkable learning and reasoning abilities, making them promisingas a controller to select, synthesize, and execute external models to solvecomplex tasks. In this project, we develop OpenAGI, an open-source AGI researchplatform, specifically designed to offer complex, multi-step tasks andaccompanied by task-specific datasets, evaluation metrics, and a diverse rangeof extensible models. OpenAGI formulates complex tasks as natural languagequeries, serving as input to the LLM. The LLM subsequently selects,synthesizes, and executes models provided by OpenAGI to address the task.Furthermore, we propose a Reinforcement Learning from Task Feedback (RLTF)mechanism, which uses the task-solving result as feedback to improve the LLM'stask-solving ability. Thus, the LLM is responsible for synthesizing variousexternal models for solving complex tasks, while RLTF provides feedback toimprove its task-solving ability, enabling a feedback loop for self-improvingAI. We believe that the paradigm of LLMs operating various expert models forcomplex task-solving is a promising approach towards AGI. To facilitate thecommunity's long-term improvement and evaluation of AGI's ability, weopen-source the code, benchmark, and evaluation methods of the OpenAGI projectat ", "output": "OpenAGI: When LLM Meets Domain Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal learning is defined as learning over multiple heterogeneous inputmodalities such as video, audio, and text. In this work, we are concerned withunderstanding how models behave as the type of modalities differ betweentraining and deployment, a situation that naturally arises in many applicationsof multimodal learning to hardware platforms. We present a multimodalrobustness framework to provide a systematic analysis of common multimodalrepresentation learning methods. Further, we identify robustness short-comingsof these approaches and propose two intervention techniques leading to$1.5times$-$4times$ robustness improvements on three datasets, AudioSet,Kinetics-400 and ImageNet-Captions. Finally, we demonstrate that theseinterventions better utilize additional modalities, if present, to achievecompetitive results of $44.2$ mAP on AudioSet 20K.", "output": "On Robustness in Multimodal Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial attacks in the input (pixel) space typically incorporate noisemargins such as $L_1$ or $L_{infty}$-norm to produce imperceptibly perturbeddata that confound deep learning networks. Such noise margins confine themagnitude of permissible noise. In this work, we propose injecting adversarialperturbations in the latent (feature) space using a generative adversarialnetwork, removing the need for margin-based priors. Experiments on MNIST,CIFAR10, Fashion-MNIST, CIFAR100 and Stanford Dogs datasets support theeffectiveness of the proposed method in generating adversarial attacks in thelatent space while ensuring a high degree of visual realism with respect topixel-based adversarial attack methods.", "output": "Generating Adversarial Attacks in the Latent Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised representation learning on (large) graphs has receivedsignificant attention in the research community due to the compactness andrichness of the learned embeddings and the abundance of unlabelled graph data.When deployed, these node representations must be generated with appropriatefairness constraints to minimize bias induced by them on downstream tasks.Consequently, group and individual fairness notions for graph learningalgorithms have been investigated for specific downstream tasks. One majorlimitation of these fairness notions is that they do not consider theconnectivity patterns in the graph leading to varied node influence (orcentrality power). In this paper, we design a centrality-aware fairnessframework for inductive graph representation learning algorithms. We proposeCAFIN (Centrality Aware Fairness inducing IN-processing), an in-processingtechnique that leverages graph structure to improve GraphSAGE's representations- a popular framework in the unsupervised inductive setting. We demonstrate theefficacy of CAFIN in the inductive setting on two popular downstream tasks -Link prediction and Node Classification. Empirically, they consistentlyminimize the disparity in fairness between groups across datasets (varying from18 to 80% reduction in imparity, a measure of group fairness) from differentdomains while incurring only a minimal performance cost.", "output": "CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have shown their power in different areas.Attention computation, as an important subroutine of LLMs, has also attractedinterests in theory. Recently the static computation and dynamic maintenance ofattention matrix has been studied by [Alman and Song 2023] and [Brand, Song andZhou 2023] from both algorithmic perspective and hardness perspective. In thiswork, we consider the sparsification of the attention problem. We make onesimplification which is the logit matrix is symmetric. Let $n$ denote thelength of sentence, let $d$ denote the embedding dimension. Given a matrix $Xin mathbb{R}^{n times d}$, suppose $d gg n$ and $| X X^top |_{infty} &lt;r$ with $r in (0,0.1)$, then we aim for finding $Y in mathbb{R}^{n timesm}$ (where $mll d$) such that begin{align*} | D(Y)^{-1} exp( Y Y^top ) -D(X)^{-1} exp( X X^top) |_{infty} leq O(r) end{align*} We provide tworesults for this problem.$bullet$ Our first result is a randomized algorithm. It runs in$widetilde{O}(mathrm{nnz}(X) + n^{omega} ) $ time, has $1-delta$ succeedprobability, and chooses $m = O(n log(n/delta))$. Here $mathrm{nnz}(X)$denotes the number of non-zero entries in $X$. We use $omega$ to denote theexponent of matrix multiplication. Currently $omega approx 2.373$.$bullet$ Our second result is a deterministic algorithm. It runs in$widetilde{O}(min{sum_{iin[d]}mathrm{nnz}(X_i)^2, dn^{omega-1}} +n^{omega+1})$ time and chooses $m = O(n)$. Here $X_i$ denote the $i$-th columnof matrix $X$.Our main findings have the following implication for applied LLMs task: forany super large feature dimension, we can reduce it down to the size nearlylinear in length of sentence.", "output": "Randomized and Deterministic Attention Sparsification Algorithms for Over-parameterized Feature Dimension."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual and linguistic pre-training aims to learn vision and languagerepresentations together, which can be transferred to visual-linguisticdownstream tasks. However, there exists semantic confusion between language andvision during the pre-training stage. Moreover, current pre-trained models tendto take lots of computation resources for fine-tuning when transferred todownstream tasks. In this work, we present a simple but effective approach forlearning Contrastive and Adaptive representations of Vision and Language,namely CAVL. Specifically, we introduce a pair-wise contrastive loss to learnalignments between the whole sentence and each image in the same batch duringthe pre-training process. At the fine-tuning stage, we introduce twolightweight adaptation networks to reduce model parameters and increasetraining speed for saving computation resources. We evaluate our CAVL on sixmain downstream tasks, including Visual Question Answering (VQA), VisualCommonsense Reasoning (VCR), Natural Language for Visual Reasoning (NLVR),Region-to-Phrase Grounding (RPG), Text-to-Image Retrieval (TIR), and Zero-shotText-to-Image Retrieval (ZS-TIR). Compared to baselines, we achieve superiorperformance and reduce the fine-tuning time by a large margin (in particular,76.17%). Extensive experiments and ablation studies demonstrate the efficiencyof contrastive pre-training and adaptive fine-tuning proposed in our CAVL.", "output": "CAVL: Learning Contrastive and Adaptive Representations of Vision and Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online class-incremental continual learning is a specific task of continuallearning. It aims to continuously learn new classes from data stream and thesamples of data stream are seen only once, which suffers from the catastrophicforgetting issue, i.e., forgetting historical knowledge of old classes.Existing replay-based methods effectively alleviate this issue by saving andreplaying part of old data in a proxy-based or contrastive-based replay manner.Although these two replay manners are effective, the former would incline tonew classes due to class imbalance issues, and the latter is unstable and hardto converge because of the limited number of samples. In this paper, we conducta comprehensive analysis of these two replay manners and find that they can becomplementary. Inspired by this finding, we propose a novel replay-based methodcalled proxy-based contrastive replay (PCR). The key operation is to replacethe contrastive samples of anchors with corresponding proxies in thecontrastive-based way. It alleviates the phenomenon of catastrophic forgettingby effectively addressing the imbalance issue, as well as keeps a fasterconvergence of the model. We conduct extensive experiments on three real-worldbenchmark datasets, and empirical results consistently demonstrate thesuperiority of PCR over various state-of-the-art methods.", "output": "PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of semi-supervised medical image segmentation, the shortage oflabeled data is the fundamental problem. How to effectively learn imagefeatures from unlabeled images to improve segmentation accuracy is the mainresearch direction in this field. Traditional self-training methods canpartially solve the problem of insufficient labeled data by generating pseudolabels for iterative training. However, noise generated due to the model'suncertainty during training directly affects the segmentation results.Therefore, we added sample-level and pixel-level uncertainty to stabilize thetraining process based on the self-training framework. Specifically, we savedseveral moments of the model during pre-training, and used the differencebetween their predictions on unlabeled samples as the sample-level uncertaintyestimate for that sample. Then, we gradually add unlabeled samples from easy tohard during training. At the same time, we added a decoder with differentupsampling methods to the segmentation network and used the difference betweenthe outputs of the two decoders as pixel-level uncertainty. In short, weselectively retrained unlabeled samples and assigned pixel-level uncertainty topseudo labels to optimize the self-training process. We compared thesegmentation results of our model with five semi-supervised approaches on thepublic 2017 ACDC dataset and 2018 Prostate dataset. Our proposed methodachieves better segmentation performance on both datasets under the samesettings, demonstrating its effectiveness, robustness, and potentialtransferability to other medical image segmentation tasks. Keywords: Medicalimage segmentation, semi-supervised learning, self-training, uncertaintyestimation", "output": "Self-training with dual uncertainty for semi-supervised medical image segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Symmetries such as gauge invariance and anyonic symmetry play a crucial rolein quantum many-body physics. We develop a general approach to constructinggauge invariant or anyonic symmetric autoregressive neural networks, includinga wide range of architectures such as Transformer and recurrent neural network,for quantum lattice models. These networks can be efficiently sampled andexplicitly obey gauge symmetries or anyonic constraint. We prove that ourmethods can provide exact representation for the ground and excited states ofthe 2D and 3D toric codes, and the X-cube fracton model. We variationallyoptimize our symmetry incorporated autoregressive neural networks for groundstates as well as real-time dynamics for a variety of models. We simulate thedynamics and the ground states of the quantum link model of $text{U(1)}$lattice gauge theory, obtain the phase diagram for the 2D $mathbb{Z}_2$ gaugetheory, determine the phase transition and the central charge of the$text{SU(2)}_3$ anyonic chain, and also compute the ground state energy of the$text{SU(2)}$ invariant Heisenberg spin chain. Our approach provides powerfultools for exploring condensed matter physics, high energy physics and quantuminformation science.", "output": "Gauge Invariant and Anyonic Symmetric Autoregressive Neural Networks for Quantum Lattice Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We establish exact asymptotic expressions for the normalized mutualinformation and minimum mean-square-error (MMSE) of sparse linear regression inthe sub-linear sparsity regime. Our result is achieved by a generalization ofthe adaptive interpolation method in Bayesian inference for linear regimes tosub-linear ones. A modification of the well-known approximate message passingalgorithm to approach the MMSE fundamental limit is also proposed, and itsstate evolution is rigorously analyzed. Our results show that the traditionallinear assumption between the signal dimension and number of observations inthe replica and adaptive interpolation methods is not necessary for sparsesignals. They also show how to modify the existing well-known AMP algorithmsfor linear regimes to sub-linear ones.", "output": "Fundamental limits and algorithms for sparse linear regression with sublinear sparsity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Global Navigation Satellite Systems typically perform poorly in urbanenvironments, where the likelihood of line-of-sight conditions between devicesand satellites is low. Therefore, alternative location methods are required toachieve good accuracy. We present LocUNet: A convolutional, end-to-end trainedneural network (NN) for the localization task, which is able to estimate theposition of a user from the received signal strength (RSS) of a small number ofBase Stations (BS). Using estimations of pathloss radio maps of the BSs and theRSS measurements of the users to be localized, LocUNet can localize users withstate-of-the-art accuracy and enjoys high robustness to inaccuracies in theestimations of radio maps. The proposed method does not require generating RSSfingerprints of each specific area where the localization task is performed andis suitable for real-time applications. Moreover, two novel datasets that allowfor numerical evaluations of RSS and ToA methods in realistic urbanenvironments are presented and made publicly available for the researchcommunity. By using these datasets, we also provide a fair comparison ofstate-of-the-art RSS and ToA-based methods in the dense urban scenario and shownumerically that LocUNet outperforms all the compared methods.", "output": "Real-time Outdoor Localization Using Radio Maps: A Deep Learning Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural network pruning is a fruitful area of research with surging interestin high sparsity regimes. Benchmarking in this domain heavily relies onfaithful representation of the sparsity of subnetworks, which has beentraditionally computed as the fraction of removed connections (directsparsity). This definition, however, fails to recognize unpruned parametersthat detached from input or output layers of underlying subnetworks,potentially underestimating actual effective sparsity: the fraction ofinactivated connections. While this effect might be negligible for moderatelypruned networks (up to 10-100 compression rates), we find that it plays anincreasing role for thinner subnetworks, greatly distorting comparison betweendifferent pruning algorithms. For example, we show that effective compressionof a randomly pruned LeNet-300-100 can be orders of magnitude larger than itsdirect counterpart, while no discrepancy is ever observed when using SynFlowfor pruning [Tanaka et al., 2020]. In this work, we adopt the lens of effectivesparsity to reevaluate several recent pruning algorithms on common benchmarkarchitectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that theirabsolute and relative performance changes dramatically in this new and moreappropriate framework. To aim for effective, rather than direct, sparsity, wedevelop a low-cost extension to most pruning algorithms. Further, equipped witheffective sparsity as a reference frame, we partially reconfirm that randompruning with appropriate sparsity allocation across layers performs as well orbetter than more sophisticated algorithms for pruning at initialization [Su etal., 2020]. In response to this observation, using a simple analogy of pressuredistribution in coupled cylinders from physics, we design novel layerwisesparsity quotas that outperform all existing baselines in the context of randompruning.", "output": "Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Feature extraction is an essential task in graph analytics. These featurevectors, called graph descriptors, are used in downstream vector-space-basedgraph analysis models. This idea has proved fruitful in the past, withspectral-based graph descriptors providing state-of-the-art classificationaccuracy. However, known algorithms to compute meaningful descriptors do notscale to large graphs since: (1) they require storing the entire graph inmemory, and (2) the end-user has no control over the algorithm's runtime. Inthis paper, we present streaming algorithms to approximately compute threedifferent graph descriptors capturing the essential structure of graphs.Operating on edge streams allows us to avoid storing the entire graph inmemory, and controlling the sample size enables us to keep the runtime of ouralgorithms within desired bounds. We demonstrate the efficacy of the proposeddescriptors by analyzing the approximation error and classification accuracy.Our scalable algorithms compute descriptors of graphs with millions of edgeswithin minutes. Moreover, these descriptors yield predictive accuracycomparable to the state-of-the-art methods but can be computed using only 25%as much memory.", "output": "Computing Graph Descriptors on Edge Streams."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "HyperParameter Optimization (HPO) aims at finding the best HyperParameters(HPs) of learning models, such as neural networks, in the fastest and mostefficient way possible. Most recent HPO algorithms try to optimize HPsregardless of the model that obtained them, assuming that for different models,same HPs will produce very similar results. We break free from this paradigmand propose a new take on preexisting methods that we called GenealogicalPopulation Based Training (GPBT). GPBT, via the shared histories of\"genealogically\"-related models, exploit the coupling of HPs and models in anefficient way. We experimentally demonstrate that our method cuts down by 2 to3 times the computational cost required, generally allows a 1% accuracyimprovement on computer vision tasks, and reduces the variance of the resultsby an order of magnitude, compared to the current algorithms. Our method issearch-algorithm agnostic so that the inner search routine can be any searchalgorithm like TPE, GP, CMA or random search.", "output": "Genealogical Population-Based Training for Hyperparameter Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The planted densest subgraph detection problem refers to the task of testingwhether in a given (random) graph there is a subgraph that is unusually dense.Specifically, we observe an undirected and unweighted graph on $n$ nodes. Underthe null hypothesis, the graph is a realization of an ErdH{o}s-R'{e}nyi graphwith edge probability (or, density) $q$. Under the alternative, there is asubgraph on $k$ vertices with edge probability $p&gt;q$. The statistical as wellas the computational barriers of this problem are well-understood for a widerange of the edge parameters $p$ and $q$. In this paper, we consider a naturalvariant of the above problem, where one can only observe a small part of thegraph using adaptive edge queries.For this model, we determine the number of queries necessary and sufficientfor detecting the presence of the planted subgraph. Specifically, we show thatany (possibly randomized) algorithm must make $mathsf{Q} =Omega(frac{n^2}{k^2chi^4(p||q)}log^2n)$ adaptive queries (on expectation)to the adjacency matrix of the graph to detect the planted subgraph withprobability more than $1/2$, where $chi^2(p||q)$ is the Chi-Square distance.On the other hand, we devise a quasi-polynomial-time algorithm that detects theplanted subgraph with high probability by making $mathsf{Q} =O(frac{n^2}{k^2chi^4(p||q)}log^2n)$ non-adaptive queries. We then propose apolynomial-time algorithm which is able to detect the planted subgraph using$mathsf{Q} = O(frac{n^3}{k^3chi^2(p||q)}log^3 n)$ queries. We conjecturethat in the leftover regime, where $frac{n^2}{k^2}llmathsf{Q}llfrac{n^3}{k^3}$, no polynomial-time algorithms exist. Our results resolve twoquestions posed in cite{racz2020finding}, where the special case of adaptivedetection and recovery of a planted clique was considered.", "output": "Random Subgraph Detection Using Queries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How can one analyze detailed 3D biological objects, such as neurons andbotanical trees, that exhibit complex geometrical and topological variation? Inthis paper, we develop a novel mathematical framework for representing,comparing, and computing geodesic deformations between the shapes of suchtree-like 3D objects. A hierarchical organization of subtrees characterizesthese objects -- each subtree has the main branch with some side branchesattached -- and one needs to match these structures across objects formeaningful comparisons. We propose a novel representation that extends theSquare-Root Velocity Function (SRVF), initially developed for Euclidean curves,to tree-shaped 3D objects. We then define a new metric that quantifies thebending, stretching, and branch sliding needed to deform one tree-shaped objectinto the other. Compared to the current metrics, such as the Quotient EuclideanDistance (QED) and the Tree Edit Distance (TED), the proposed representationand metric capture the full elasticity of the branches (i.e., bending andstretching) as well as the topological variations (i.e., branch death/birth andsliding). It completely avoids the shrinkage that results from the edgecollapse and node split operations of the QED and TED metrics. We demonstratethe utility of this framework in comparing, matching, and computing geodesicsbetween biological objects such as neurons and botanical trees. The frameworkis also applied to various shape analysis tasks: (i) symmetry analysis andsymmetrization of tree-shaped 3D objects, (ii) computing summary statistics(means and modes of variations) of populations of tree-shaped 3D objects, (iii)fitting parametric probability distributions to such populations, and (iv)finally synthesizing novel tree-shaped 3D objects through random sampling fromestimated probability distributions.", "output": "Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adhesive joints are increasingly used in industry for a wide variety ofapplications because of their favorable characteristics such as highstrength-to-weight ratio, design flexibility, limited stress concentrations,planar force transfer, good damage tolerance, and fatigue resistance. Findingthe optimal process parameters for an adhesive bonding process is challenging:the optimization is inherently multi-objective (aiming to maximize breakstrength while minimizing cost), constrained (the process should not result inany visual damage to the materials, and stress tests should not result infailures that are adhesion-related), and uncertain (testing the same processparameters several times may lead to different break strengths). Real-lifephysical experiments in the lab are expensive to perform. Traditionalevolutionary approaches (such as genetic algorithms) are then ill-suited tosolve the problem, due to the prohibitive amount of experiments required forevaluation. Although Bayesian optimization-based algorithms are preferred tosolve such expensive problems, few methods consider the optimization of morethan one (noisy) objective and several constraints at the same time. In thisresearch, we successfully applied specific machine learning techniques(Gaussian Process Regression) to emulate the objective and constraint functionsbased on a limited amount of experimental data. The techniques are embedded ina Bayesian optimization algorithm, which succeeds in detecting Pareto-optimalprocess settings in a highly efficient way (i.e., requiring a limited number ofphysical experiments).", "output": "Constrained multi-objective optimization of process design parameters in settings with scarce data: an application to adhesive bonding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multivariate time series (MTS) forecasting plays an important role in theautomation and optimization of intelligent applications. It is a challengingtask, as we need to consider both complex intra-variable dependencies andinter-variable dependencies. Existing works only learn temporal patterns withthe help of single inter-variable dependencies. However, there are multi-scaletemporal patterns in many real-world MTS. Single inter-variable dependenciesmake the model prefer to learn one type of prominent and shared temporalpatterns. In this paper, we propose a multi-scale adaptive graph neural network(MAGNN) to address the above issue. MAGNN exploits a multi-scale pyramidnetwork to preserve the underlying temporal dependencies at different timescales. Since the inter-variable dependencies may be different under distincttime scales, an adaptive graph learning module is designed to infer thescale-specific inter-variable dependencies without pre-defined priors. Giventhe multi-scale feature representations and scale-specific inter-variabledependencies, a multi-scale temporal graph neural network is introduced tojointly model intra-variable dependencies and inter-variable dependencies.After that, we develop a scale-wise fusion module to effectively promote thecollaboration across different time scales, and automatically capture theimportance of contributed temporal patterns. Experiments on four real-worlddatasets demonstrate that MAGNN outperforms the state-of-the-art methods acrossvarious settings.", "output": "Multi-Scale Adaptive Graph Neural Network for Multivariate Time Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The author's recent research papers, \"Cumulative deviation of a subpopulationfrom the full population\" and \"A graphical method of cumulative differencesbetween two subpopulations\" (both published in volume 8 of Springer'sopen-access \"Journal of Big Data\" during 2021), propose graphical methods andsummary statistics, without extensively calibrating formal significance tests.The summary metrics and methods can measure the calibration of probabilisticpredictions and can assess differences in responses between a subpopulation andthe full population while controlling for a covariate or score via conditioningon it. These recently published papers construct significance tests based onthe scalar summary statistics, but only sketch how to calibrate the attainedsignificance levels (also known as \"P-values\") for the tests. The presentarticle reviews and synthesizes work spanning many decades in order to detailhow to calibrate the P-values. The present paper presents computationallyefficient, easily implemented numerical methods for evaluating properlycalibrated P-values, together with rigorous mathematical proofs guaranteeingtheir accuracy, and illustrates and validates the methods with open-sourcesoftware and numerical examples.", "output": "Calibration of P-values for calibration and for deviation of a subpopulation from the full population."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient detection and description of geometric regions in images is aprerequisite in visual systems for localization and mapping. Such systems stillrely on traditional hand-crafted methods for efficient generation oflightweight descriptors, a common limitation of the more powerful neuralnetwork models that come with high compute and specific hardware requirements.In this paper, we focus on the adaptations required by detection anddescription neural networks to enable their use in computationally limitedplatforms such as robots, mobile, and augmented reality devices. To that end,we investigate and adapt network quantization techniques to accelerateinference and enable its use on compute limited platforms. In addition, werevisit common practices in descriptor quantization and propose the use of abinary descriptor normalization layer, enabling the generation of distinctivebinary descriptors with a constant number of ones. ZippyPoint, our efficientquantized network with binary descriptors, improves the network runtime speed,the descriptor matching speed, and the 3D model size, by at least an order ofmagnitude when compared to full-precision counterparts. These improvements comeat a minor performance degradation as evaluated on the tasks of homographyestimation, visual localization, and map-free visual relocalization. Code andmodels are available at ", "output": "ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "By driving models to converge to flat minima, sharpness-aware learningalgorithms (such as SAM) have shown the power to achieve state-of-the-artperformances. However, these algorithms will generally incur one extraforward-backward propagation at each training iteration, which largely burdensthe computation especially for scalable models. To this end, we propose asimple yet efficient training scheme, called Randomized Sharpness-AwareTraining (RST). Optimizers in RST would perform a Bernoulli trial at eachiteration to choose randomly from base algorithms (SGD) and sharpness-awarealgorithms (SAM) with a probability arranged by a predefined schedulingfunction. Due to the mixture of base algorithms, the overall count ofpropagation pairs could be largely reduced. Also, we give theoretical analysison the convergence of RST. Then, we empirically study the computation cost andeffect of various types of scheduling functions, and give directions on settingappropriate scheduling functions. Further, we extend the RST to a generalframework (G-RST), where we can adjust regularization degree on sharpnessfreely for any scheduling function. We show that G-RST can outperform SAM inmost cases while saving 50% extra computation cost.", "output": "Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Protein representation learning methods have shown great potential to yielduseful representation for many downstream tasks, especially on proteinclassification. Moreover, a few recent studies have shown great promise inaddressing insufficient labels of proteins with self-supervised learningmethods. However, existing protein language models are usually pretrained onprotein sequences without considering the important protein structuralinformation. To this end, we propose a novel structure-aware proteinself-supervised learning method to effectively capture structural informationof proteins. In particular, a well-designed graph neural network (GNN) model ispretrained to preserve the protein structural information with self-supervisedtasks from a pairwise residue distance perspective and a dihedral angleperspective, respectively. Furthermore, we propose to leverage the availableprotein language model pretrained on protein sequences to enhance theself-supervised learning. Specifically, we identify the relation between thesequential information in the protein language model and the structuralinformation in the specially designed GNN model via a novel pseudo bi-leveloptimization scheme. Experiments on several supervised downstream tasks verifythe effectiveness of our proposed method.The code of the proposed method isavailable in url{", "output": "Structure-aware Protein Self-supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Code is seldom written in a single left-to-right pass and is insteadrepeatedly edited and refined. We introduce InCoder, a unified generative modelthat can perform program synthesis (via left-to-right generation) as well asediting (via infilling). InCoder is trained to generate code files from a largecorpus of permissively licensed code, where regions of code have been randomlymasked and moved to the end of each file, allowing code infilling withbidirectional context. Our model is the first generative model that is able todirectly perform zero-shot code infilling, which we evaluate on challengingtasks such as type inference, comment generation, and variable re-naming. Wefind that the ability to condition on bidirectional context substantiallyimproves performance on these tasks, while still performing comparably onstandard program synthesis benchmarks in comparison to left-to-right onlymodels pretrained at similar scale. The InCoder models and code are publiclyreleased. ", "output": "InCoder: A Generative Model for Code Infilling and Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Audio-based classification techniques on body sounds have long been studiedto support diagnostic decisions, particularly in pulmonary diseases. Inresponse to the urgency of the COVID-19 pandemic, a growing number of modelsare developed to identify COVID-19 patients based on acoustic input. Mostmodels focus on cough because the dry cough is the best-known symptom ofCOVID-19. However, other body sounds, such as breath and speech, have also beenrevealed to correlate with COVID-19 as well. In this work, rather than relyingon a specific body sound, we propose Fused Audio Instance and Representationfor COVID-19 Detection (FAIR4Cov). It relies on constructing a joint featurevector obtained from a plurality of body sounds in waveform and spectrogramrepresentation. The core component of FAIR4Cov is a self-attention fusion unitthat is trained to establish the relation of multiple body sounds and audiorepresentations and integrate it into a compact feature vector. We set up ourexperiments on different combinations of body sounds using only waveform,spectrogram, and a joint representation of waveform and spectrogram. Ourfindings show that the use of self-attention to combine extracted features fromcough, breath, and speech sounds leads to the best performance with an AreaUnder the Receiver Operating Characteristic Curve (AUC) score of 0.8658, asensitivity of 0.8057, and a specificity of 0.7958. This AUC is 0.0227 higherthan the one of the models trained on spectrograms only and 0.0847 higher thanthe one of the models trained on waveforms only. The results demonstrate thatthe combination of spectrogram with waveform representation helps to enrich theextracted features and outperforms the models with single representation.", "output": "FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, the interpretation of why a machine learning (ML) model makescertain inferences is as crucial as the accuracy of such inferences. Some MLmodels like the decision tree possess inherent interpretability that can bedirectly comprehended by humans. Others like artificial neural networks (ANN),however, rely on external methods to uncover the deduction mechanism. SHapleyAdditive exPlanations (SHAP) is one of such external methods, which requires abackground dataset when interpreting ANNs. Generally, a background datasetconsists of instances randomly sampled from the training dataset. However, thesampling size and its effect on SHAP remain to be unexplored. In our empiricalstudy on the MIMIC-III dataset, we show that the two core explanations - SHAPvalues and variable rankings fluctuate when using different background datasetsacquired from random sampling, indicating that users cannot unquestioninglytrust the one-shot interpretation from SHAP. Luckily, such fluctuationdecreases with the increase of the background dataset size. Also, we notice anU-shape in the stability assessment of SHAP variable rankings, demonstratingthat SHAP is more reliable in ranking the most and least important variablescompared to moderately important ones. Overall, our results suggest that usersshould take into account how background data affects SHAP results, withimproved SHAP stability as the background sample size increases.", "output": "An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-dimensional multimodal data arises in many scientific fields. Theintegration of multimodal data becomes challenging when there is no knowncorrespondence between the samples and the features of different datasets. Totackle this challenge, we introduce AVIDA, a framework for simultaneouslyperforming data alignment and dimension reduction. In the numericalexperiments, Gromov-Wasserstein optimal transport and t-distributed stochasticneighbor embedding are used as the alignment and dimension reduction modulesrespectively. We show that AVIDA correctly aligns high-dimensional datasetswithout common features with four synthesized datasets and two real multimodalsingle-cell datasets. Compared to several existing methods, we demonstrate thatAVIDA better preserves structures of individual datasets, especially distinctlocal structures in the joint low-dimensional visualization, while achievingcomparable alignment performance. Such a property is important in multimodalsingle-cell data analysis as some biological processes are uniquely captured byone of the datasets. In general applications, other methods can be used for thealignment and dimension reduction modules.", "output": "AVIDA: Alternating method for Visualizing and Integrating Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although many fairness criteria have been proposed to ensure that machinelearning algorithms do not exhibit or amplify our existing social biases, thesealgorithms are trained on datasets that can themselves be statistically biased.In this paper, we investigate the robustness of a number of existing(demographic) fairness criteria when the algorithm is trained on biased data.We consider two forms of dataset bias: errors by prior decision makers in thelabeling process, and errors in measurement of the features of disadvantagedindividuals. We analytically show that some constraints (such as DemographicParity) can remain robust when facing certain statistical biases, while others(such as Equalized Odds) are significantly violated if trained on biased data.We also analyze the sensitivity of these criteria and the decision maker'sutility to biases. We provide numerical experiments based on three real-worlddatasets (the FICO, Adult, and German credit score datasets) supporting ouranalytical findings. Our findings present an additional guideline for choosingamong existing fairness criteria, or for proposing new criteria, when availabledatasets may be biased.", "output": "Social Bias Meets Data Bias: The Impacts of Labeling and Measurement Errors on Fairness Criteria."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While a growing body of literature has been studying new Graph NeuralNetworks (GNNs) that work on both homophilic and heterophilic graphs, littlehas been done on adapting classical GNNs to less-homophilic graphs. Althoughthe ability to handle less-homophilic graphs is restricted, classical GNNsstill stand out in several nice properties such as efficiency, simplicity, andexplainability. In this work, we propose a novel graph restructuring methodthat can be integrated into any type of GNNs, including classical GNNs, toleverage the benefits of existing GNNs while alleviating their limitations. Ourcontribution is threefold: a) learning the weight of pseudo-eigenvectors for anadaptive spectral clustering that aligns well with known node labels, b)proposing a new density-aware homophilic metric that is robust to labelimbalance, and c) reconstructing the adjacency matrix based on the result ofadaptive spectral clustering to maximize the homophilic scores. Theexperimental results show that our graph restructuring method can significantlyboost the performance of six classical GNNs by an average of 25% onless-homophilic graphs. The boosted performance is comparable tostate-of-the-art methods.", "output": "Restructuring Graph for Higher Homophily via Adaptive Spectral Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study differentially private empirical risk minimization(DP-ERM). It has been shown that the worst-case utility of DP-ERM reducespolynomially as the dimension increases. This is a major obstacle to privatelylearning large machine learning models. In high dimension, it is common forsome model's parameters to carry more information than others. To exploit this,we propose a differentially private greedy coordinate descent (DP-GCD)algorithm. At each iteration, DP-GCD privately performs a coordinate-wisegradient step along the gradients' (approximately) greatest entry. We showtheoretically that DP-GCD can achieve a logarithmic dependence on the dimensionfor a wide range of problems by naturally exploiting their structuralproperties (such as quasi-sparse solutions). We illustrate this behaviornumerically, both on synthetic and real datasets.", "output": "High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning models are vulnerable to Out-Of-Distribution (OOD) examples,and such a problem has drawn much attention. However, current methods lack afull understanding of different types of OOD data: there are benign OOD datathat can be properly adapted to enhance the learning performance, while othermalign OOD data would severely degenerate the classification result. To HarnessOOD data, this paper proposes a HOOD method that can leverage the content andstyle from each image instance to identify benign and malign OOD data.Particularly, we design a variational inference framework to causallydisentangle content and style features by constructing a structural causalmodel. Subsequently, we augment the content and style through an interventionprocess to produce malign and benign OOD data, respectively. The benign OODdata contain novel styles but hold our interested contents, and they can beleveraged to help train a style-invariant model. In contrast, the malign OODdata inherit unknown contents but carry familiar styles, by detecting them canimprove model robustness against deceiving anomalies. Thanks to the proposednovel disentanglement and data augmentation techniques, HOOD can effectivelydeal with OOD examples in unknown and open environments, whose effectiveness isempirically validated in three typical OOD applications including OODdetection, open-set semi-supervised learning, and open-set domain adaptation.", "output": "Harnessing Out-Of-Distribution Examples via Augmenting Content and Style."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is important to understand how dropout, a popular regularization method,aids in achieving a good generalization solution during neural networktraining. In this work, we present a theoretical derivation of an implicitregularization of dropout, which is validated by a series of experiments.Additionally, we numerically study two implications of the implicitregularization, which intuitively rationalizes why dropout helpsgeneralization. Firstly, we find that input weights of hidden neurons tend tocondense on isolated orientations trained with dropout. Condensation is afeature in the non-linear learning process, which makes the network lesscomplex. Secondly, we experimentally find that the training with dropout leadsto the neural network with a flatter minimum compared with standard gradientdescent training, and the implicit regularization is the key to finding flatsolutions. Although our theory mainly focuses on dropout used in the lasthidden layer, our experiments apply to general dropout in training neuralnetworks. This work points out a distinct characteristic of dropout comparedwith stochastic gradient descent and serves as an important basis for fullyunderstanding dropout.", "output": "Implicit regularization of dropout."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of ensuring confidentiality of dataset propertiesaggregated over many records of a dataset. Such properties can encode sensitiveinformation, such as trade secrets or demographic data, while involving anotion of data protection different to the privacy of individual recordstypically discussed in the literature. In this work, we demonstrate how adistribution privacy framework can be applied to formalize such dataconfidentiality. We extend the Wasserstein Mechanism from Pufferfish privacyand the Gaussian Mechanism from attribute privacy to this framework, thenanalyze their underlying data assumptions and how they can be relaxed. We thenempirically evaluate the privacy-utility tradeoffs of these mechanisms andapply them against a practical property inference attack which targets globalproperties of datasets. The results show that our mechanisms can indeed reducethe effectiveness of the attack while providing utility substantially greaterthan a crude group differential privacy baseline. Our work thus providesgroundwork for theoretical mechanisms for protecting global properties ofdatasets along with their evaluation in practice.", "output": "Protecting Global Properties of Datasets with Distribution Privacy Mechanisms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a method to map 2D image observations of a scene to a persistent3D scene representation, enabling novel view synthesis and disentangledrepresentation of the movable and immovable components of the scene. Motivatedby the bird's-eye-view (BEV) representation commonly used in vision androbotics, we propose conditional neural groundplans, ground-aligned 2D featuregrids, as persistent and memory-efficient scene representations. Our method istrained self-supervised from unlabeled multi-view observations usingdifferentiable rendering, and learns to complete geometry and appearance ofoccluded regions. In addition, we show that we can leverage multi-view videosat training time to learn to separately reconstruct static and movablecomponents of the scene from a single image at test time. The ability toseparately reconstruct movable objects enables a variety of downstream tasksusing simple heuristics, such as extraction of object-centric 3Drepresentations, novel view synthesis, instance-level segmentation, 3D boundingbox prediction, and scene editing. This highlights the value of neuralgroundplans as a backbone for efficient 3D scene understanding models.", "output": "Neural Groundplans: Persistent Neural Scene Representations from a Single Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep reinforcement learning with domain randomization learns a control policyin various simulations with randomized physical and sensor model parameters tobecome transferable to the real world in a zero-shot setting. However, a hugenumber of samples are often required to learn an effective policy when therange of randomized parameters is extensive due to the instability of policyupdates. To alleviate this problem, we propose a sample-efficient method namedcyclic policy distillation (CPD). CPD divides the range of randomizedparameters into several small sub-domains and assigns a local policy to eachone. Then local policies are learned while cyclically transitioning tosub-domains. CPD accelerates learning through knowledge transfer based onexpected performance improvements. Finally, all of the learned local policiesare distilled into a global policy for sim-to-real transfers. CPD'seffectiveness and sample efficiency are demonstrated through simulations withfour tasks (Pendulum from OpenAIGym and Pusher, Swimmer, and HalfCheetah fromMujoco), and a real-robot, ball-dispersal task. We published code and videosfrom our experiments at", "output": "Cyclic Policy Distillation: Sample-Efficient Sim-to-Real Reinforcement Learning with Domain Randomization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a differentiable WORLD synthesizer and demonstrateits use in end-to-end audio style transfer tasks such as (singing) voiceconversion and the DDSP timbre transfer task. Accordingly, our baselinedifferentiable synthesizer has no model parameters, yet it yields adequatesynthesis quality. We can extend the baseline synthesizer by appendinglightweight black-box postnets which apply further processing to the baselineoutput in order to improve fidelity. An alternative differentiable approachconsiders extraction of the source excitation spectrum directly, which canimprove naturalness albeit for a narrower class of style transfer applications.The acoustic feature parameterization used by our approaches has the addedbenefit that it naturally disentangles pitch and timbral information so thatthey can be modeled separately. Moreover, as there exists a robust means ofestimating these acoustic features from monophonic audio sources, it allows forparameter loss terms to be added to an end-to-end objective function, which canhelp convergence and/or further stabilize (adversarial) training.", "output": "Differentiable WORLD Synthesizer-based Neural Vocoder With Application To End-To-End Audio Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pathologists need to combine information from differently stained pathologyslices for accurate diagnosis. Deformable image registration is a necessarytechnique for fusing multi-modal pathology slices. This paper proposes a hybriddeep feature-based deformable image registration framework for stainedpathology samples. We first extract dense feature points via the detector-basedand detector-free deep learning feature networks and perform points matching.Then, to further reduce false matches, an outlier detection method combiningthe isolation forest statistical model and the local affine correction model isproposed. Finally, the interpolation method generates the deformable vectorfield for pathology image registration based on the above matching points. Weevaluate our method on the dataset of the Non-rigid Histology ImageRegistration (ANHIR) challenge, which is co-organized with the IEEE ISBI 2019conference. Our technique outperforms the traditional approaches by 17% withthe Average-Average registration target error (rTRE) reaching 0.0034. Theproposed method achieved state-of-the-art performance and ranked 1st inevaluating the test dataset. The proposed hybrid deep feature-basedregistration method can potentially become a reliable method for pathologyimage registration.", "output": "A Hybrid Deep Feature-Based Deformable Image Registration Method for Pathology Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image clustering is an important and open-challenging task in computervision. Although many methods have been proposed to solve the image clusteringtask, they only explore images and uncover clusters according to the imagefeatures, thus being unable to distinguish visually similar but semanticallydifferent images. In this paper, we propose to investigate the task of imageclustering with the help of a visual-language pre-training model. Differentfrom the zero-shot setting, in which the class names are known, we only knowthe number of clusters in this setting. Therefore, how to map images to aproper semantic space and how to cluster images from both image and semanticspaces are two key problems. To solve the above problems, we propose a novelimage clustering method guided by the visual-language pre-training model CLIP,named textbf{Semantic-Enhanced Image Clustering (SIC)}. In this new method, wepropose a method to map the given images to a proper semantic space first andefficient methods to generate pseudo-labels according to the relationshipsbetween images and semantics. Finally, we propose performing clustering withconsistency learning in both image space and semantic space, in aself-supervised learning fashion. The theoretical result of convergenceanalysis shows that our proposed method can converge at a sublinear speed.Theoretical analysis of expectation risk also shows that we can reduce theexpected risk by improving neighborhood consistency, increasing predictionconfidence, or reducing neighborhood imbalance. Experimental results on fivebenchmark datasets clearly show the superiority of our new method.", "output": "Semantic-Enhanced Image Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep generative models (DGMs) are data-eager because learning a complex modelon limited data suffers from a large variance and easily overfits. Inspired bythe classical perspective of the bias-variance tradeoff, we propose regularizeddeep generative model (Reg-DGM), which leverages a nontransferable pre-trainedmodel to reduce the variance of generative modeling with limited data.Formally, Reg-DGM optimizes a weighted sum of a certain divergence and theexpectation of an energy function, where the divergence is between the data andthe model distributions, and the energy function is defined by the pre-trainedmodel w.r.t. the model distribution. We analyze a simple yet representativeGaussian-fitting case to demonstrate how the weighting hyperparameter tradesoff the bias and the variance. Theoretically, we characterize the existence andthe uniqueness of the global minimum of Reg-DGM in a non-parametric setting andprove its convergence with neural networks trained by gradient-based methods.Empirically, with various pre-trained feature extractors and a data-dependentenergy function, Reg-DGM consistently improves the generation performance ofstrong DGMs with limited data and achieves competitive results to thestate-of-the-art methods. Our implementation is available at", "output": "Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural network (NN) classifiers are vulnerable to adversarial attacks.Although the existing gradient-based attacks achieve state-of-the-artperformance in feed-forward NNs and image recognition tasks, they do notperform as well on time series classification with recurrent neural network(RNN) models. This is because the cyclical structure of RNN prevents directmodel differentiation and the visual sensitivity of time series data toperturbations challenges the traditional local optimization objective of theadversarial attack. In this paper, a black-box method called TSFool is proposedto efficiently craft highly-imperceptible adversarial time series for RNNclassifiers. We propose a novel global optimization objective named CamouflageCoefficient to consider the imperceptibility of adversarial samples from theperspective of class distribution, and accordingly refine the adversarialattack as a multi-objective optimization problem to enhance the perturbationquality. To get rid of the dependence on gradient information, we also proposea new idea that introduces a representation model for RNN to capture deeplyembedded vulnerable samples having otherness between their features and latentmanifold, based on which the optimization solution can be heuristicallyapproximated. Experiments on 10 UCR datasets are conducted to confirm thatTSFool averagely outperforms existing methods with a 46.3% higher attacksuccess rate, 87.4% smaller perturbation and 25.6% better CamouflageCoefficient at a similar time cost.", "output": "TSFool: Crafting Highly-imperceptible Adversarial Time Series through Multi-objective Black-box Attack to Fool RNN Classifiers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability of overparameterized deep networks to interpolate noisy data,while at the same time showing good generalization performance, has beenrecently characterized in terms of the double descent curve for the test error.Common intuition from polynomial regression suggests that overparameterizednetworks are able to sharply interpolate noisy data, without considerablydeviating from the ground-truth signal, thus preserving generalization ability.At present, a precise characterization of the relationship betweeninterpolation and generalization for deep networks is missing. In this work, wequantify sharpness of fit of the training data interpolated by neural networkfunctions, by studying the loss landscape w.r.t. to the input variable locallyto each training point, over volumes around cleanly- and noisily-labelledtraining samples, as we systematically increase the number of model parametersand training epochs. Our findings show that loss sharpness in the input spacefollows both model- and epoch-wise double descent, with worse peaks observedaround noisy labels. While small interpolating models sharply fit both cleanand noisy data, large interpolating models express a smooth loss landscape,where noisy targets are predicted over large volumes around training datapoints, in contrast to existing intuition.", "output": "Deep Double Descent via Smooth Interpolation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training graph neural networks (GNNs) on large graphs is complex andextremely time consuming. This is attributed to overheads caused by sparsematrix multiplication, which are sidestepped when training multi-layerperceptrons (MLPs) with only node features. MLPs, by ignoring graph context,are simple and faster for graph data, however they usually sacrifice predictionaccuracy, limiting their applications for graph data. We observe that for mostmessage passing-based GNNs, we can trivially derive an analog MLP (we call thisa PeerMLP) with an equivalent weight space, by setting the trainable parameterswith the same shapes, making us curious about textbf{emph{how do GNNs usingweights from a fully trained PeerMLP perform?}} Surprisingly, we find that GNNsinitialized with such weights significantly outperform their PeerMLPs,motivating us to use PeerMLP training as a precursor, initialization step toGNN training. To this end, we propose an embarrassingly simple, yet hugelyeffective initialization method for GNN training acceleration, called MLPInit.Our extensive experiments on multiple large-scale graph datasets with diverseGNN architectures validate that MLPInit can accelerate the training of GNNs (upto 33X speedup on OGB-Products) and often improve prediction performance (e.g.,up to $7.97%$ improvement for GraphSAGE across $7$ datasets for nodeclassification, and up to $17.81%$ improvement across $4$ datasets for linkprediction on metric Hits@10). The code is available athref{", "output": "MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conformal prediction is a distribution-free technique for establishing validprediction intervals. Although conventionally people conduct conformalprediction in the output space, this is not the only possibility. In thispaper, we propose feature conformal prediction, which extends the scope ofconformal prediction to semantic feature spaces by leveraging the inductivebias of deep representation learning. From a theoretical perspective, wedemonstrate that feature conformal prediction provably outperforms regularconformal prediction under mild assumptions. Our approach could be combinedwith not only vanilla conformal prediction, but also other adaptive conformalprediction methods. Apart from experiments on existing predictive inferencebenchmarks, we also demonstrate the state-of-the-art performance of theproposed methods on large-scale tasks such as ImageNet classification andCityscapes image segmentation.The code is available aturl{", "output": "Predictive Inference with Feature Conformal Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-supervised learning and weakly supervised learning are importantparadigms that aim to reduce the growing demand for labeled data in currentmachine learning applications. In this paper, we introduce a novel analysis ofthe classical label propagation algorithm (LPA) (Zhu &amp; Ghahramani, 2002) thatmoreover takes advantage of useful prior information, specificallyprobabilistic hypothesized labels on the unlabeled data. We provide an errorbound that exploits both the local geometric properties of the underlying graphand the quality of the prior information. We also propose a framework toincorporate multiple sources of noisy information. In particular, we considerthe setting of weak supervision, where our sources of information are weaklabelers. We demonstrate the ability of our approach on multiple benchmarkweakly supervised classification tasks, showing improvements upon existingsemi-supervised and weakly supervised methods.", "output": "Label Propagation with Weak Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The brain effortlessly extracts latent causes of stimuli, but how it doesthis at the network level remains unknown. Most prior attempts at this problemproposed neural networks that implement independent component analysis whichworks under the limitation that latent causes are mutually independent. Here,we relax this limitation and propose a biologically plausible neural networkthat extracts correlated latent sources by exploiting information about theirdomains. To derive this network, we choose maximum correlative informationtransfer from inputs to outputs as the separation objective under theconstraint that the outputs are restricted to their presumed sets. The onlineformulation of this optimization problem naturally leads to neural networkswith local learning rules. Our framework incorporates infinitely many sourcedomain choices and flexibly models complex latent structures. Choices ofsimplex or polytopic source domains result in networks with piecewise-linearactivation functions. We provide numerical examples to demonstrate the superiorcorrelated source separation capability for both synthetic and natural sources.", "output": "Correlative Information Maximization Based Biologically Plausible Neural Networks for Correlated Source Separation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Devising a fair classifier that does not discriminate against differentgroups is an important problem in machine learning. Although researchers haveproposed various ways of defining group fairness, most of them only focused onthe immediate fairness, ignoring the long-term impact of a fair classifierunder the dynamic scenario where each individual can improve its feature overtime. Such dynamic scenarios happen in real world, e.g., college admission andcredit loaning, where each rejected sample makes effort to change its featuresto get accepted afterwards. In this dynamic setting, the long-term fairnessshould equalize the samples' feature distribution across different groups afterthe rejected samples make some effort to improve. In order to promote long-termfairness, we propose a new fairness notion called Equal Improvability (EI),which equalizes the potential acceptance rate of the rejected samples acrossdifferent groups assuming a bounded level of effort will be spent by eachrejected sample. We analyze the properties of EI and its connections withexisting fairness notions. To find a classifier that satisfies the EIrequirement, we propose and study three different approaches that solveEI-regularized optimization problems. Through experiments on both synthetic andreal datasets, we demonstrate that the proposed EI-regularized algorithmsencourage us to find a fair classifier in terms of EI. Finally, we provideexperimental results on dynamic scenarios which highlight the advantages of ourEI metric in achieving the long-term fairness. Codes are available in a GitHubrepository, see ", "output": "Equal Improvability: A New Fairness Notion Considering the Long-term Impact."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-training (ST), or pseudo-labeling has sparked significant interest inthe automatic speech recognition (ASR) community recently because of itssuccess in harnessing unlabeled data. Unlike prior semi-supervised learningapproaches that relied on iteratively regenerating pseudo-labels (PLs) from atrained model and using them to train a new model, recent state-of-the-artmethods perform `continuous training' where PLs are generated using a veryrecent version of the model being trained. Nevertheless, these approaches stillrely on bootstrapping the ST using an initial supervised learning phase wherethe model is trained on labeled data alone. We believe this has the potentialfor over-fitting to the labeled dataset in low resource settings and that STfrom the start of training should reduce over-fitting. In this paper we showhow we can do this by dynamically controlling the evolution of PLs during thetraining process in ASR. To the best of our knowledge, this is the first studythat shows the feasibility of generating PLs from the very start of thetraining. We are able to achieve this using two techniques that avoidinstabilities which lead to degenerate models that do not generalize. Firstly,we control the evolution of PLs through a curriculum that uses the onlinechanges in PLs to control the membership of the cache of PLs and improvegeneralization. Secondly, we find that by sampling transcriptions from thepredictive distribution, rather than only using the best transcription, we canstabilize training further. With these techniques, our ST models match priorworks without an external language model.", "output": "Continuous Pseudo-Labeling from the Start."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this note we demonstrate provable convergence of SGD to the global minimaof appropriately regularized $ell_2-$empirical risk of depth $2$ nets -- forarbitrary data and with any number of gates, if they are using adequatelysmooth and bounded activations like sigmoid and tanh. We build on the resultsin [1] and leverage a constant amount of Frobenius norm regularization on theweights, along with sampling of the initial weights from an appropriatedistribution. We also give a continuous time SGD convergence result that alsoapplies to smooth unbounded activations like SoftPlus. Our key idea is to showthe existence loss functions on constant sized neural nets which are \"VillaniFunctions\". [1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning ratesand schr\"odinger operators, 2020. <a href=\"/abs/2004.06977\">arXiv:2004.06977</a>", "output": "Global Convergence of SGD On Two Layer Neural Nets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a smoothly broken power law functional form (referred to by us asa Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates thescaling behaviors of deep neural networks (i.e. how the evaluation metric ofinterest varies as the amount of compute used for training, number of modelparameters, training dataset size, model input size, number of training steps,or upstream performance varies) for various architectures and for each ofvarious tasks within a large and diverse set of upstream and downstream tasks,in zero-shot, prompted, and fine-tuned settings. This set includes large-scalevision, language, audio, video, diffusion, generative modeling, multimodallearning, contrastive learning, AI alignment, robotics, out-of-distribution(OOD) generalization, continual learning, transfer learning, uncertaintyestimation / calibration, out-of-distribution detection, adversarialrobustness, distillation, sparsity, retrieval, quantization, pruning, fairness,molecules, computer programming/coding, math word problems, \"emergent\" \"phasetransitions / changes\", arithmetic, unsupervised/self-supervised learning, &amp;reinforcement learning (single agent &amp; multi-agent). When compared to otherfunctional forms for neural scaling behavior, this functional form yieldsextrapolations of scaling behavior that are considerably more accurate on thisset. Moreover, this functional form accurately models &amp; extrapolates scalingbehavior that other functional forms are incapable of expressing such as thenon-monotonic transitions present in the scaling behavior of phenomena such asdouble descent &amp; the delayed, sharp inflection points present in the scalingbehavior of tasks such as arithmetic. Lastly, we use this functional form toglean insights about the limit of the predictability of scaling behavior. Codeis available at ", "output": "Broken Neural Scaling Laws."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The substitute-based recommendation is widely used in E-commerce to providebetter alternatives to customers. However, existing research typically uses thecustomer behavior signals like co-view and view-but-purchase-another to capturethe substitute relationship. Despite its intuitive soundness, we find that suchan approach might ignore the functionality and characteristics of products. Inthis paper, we adapt substitute recommendation into language matching problemby taking product title description as model input to consider productfunctionality. We design a new transformation method to de-noise the signalsderived from production data. In addition, we consider multilingual supportfrom the engineering point of view. Our proposed end-to-end transformer-basedmodel achieves both successes from offline and online experiments. The proposedmodel has been deployed in a large-scale E-commerce website for 11 marketplacesin 6 languages. Our proposed model is demonstrated to increase revenue by 19%based on an online A/B experiment.", "output": "A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bayesian causal structure learning aims to learn a posterior distributionover directed acyclic graphs (DAGs), and the mechanisms that define therelationship between parent and child variables. By taking a Bayesian approach,it is possible to reason about the uncertainty of the causal model. The notionof modelling the uncertainty over models is particularly crucial for causalstructure learning since the model could be unidentifiable when given only afinite amount of observational data. In this paper, we introduce a novel methodto jointly learn the structure and mechanisms of the causal model usingVariational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). Weextend the method of Bayesian causal structure learning using GFlowNets tolearn not only the posterior distribution over the structure, but also theparameters of a linear-Gaussian model. Our results on simulated data suggestthat VBG is competitive against several baselines in modelling the posteriorover DAGs and mechanisms, while offering several advantages over existingmethods, including the guarantee to sample acyclic graphs, and the flexibilityto generalize to non-linear causal mechanisms.", "output": "Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scarcity of data and incremental learning of new tasks pose two majorbottlenecks for many modern computer vision algorithms. The phenomenon ofcatastrophic forgetting, i.e., the model's inability to classify previouslylearned data after training with new batches of data, is a major challenge.Conventional methods address catastrophic forgetting while compromising thecurrent session's training. Generative replay-based approaches, such asgenerative adversarial networks (GANs), have been proposed to mitigatecatastrophic forgetting, but training GANs with few samples may lead toinstability. To address these challenges, we propose a novel method thatimproves classification robustness by identifying a better embedding spaceusing an improved contrasting loss. Our approach retains previously acquiredknowledge in the embedding space, even when trained with new classes, byupdating previous session class prototypes to represent the true class mean,which is crucial for our nearest class mean classification strategy. Wedemonstrate the effectiveness of our method by showing that the embedding spaceremains intact after training the model with new classes and outperformsexisting state-of-the-art algorithms in terms of accuracy across differentsessions.", "output": "Prototypical quadruplet for few-shot class incremental learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain shift is a well-known problem in the medical imaging community. Inparticular, for endoscopic image analysis where the data can have differentmodalities the performance of deep learning (DL) methods gets adverselyaffected. In other words, methods developed on one modality cannot be used fora different modality. However, in real clinical settings, endoscopists switchbetween modalities for better mucosal visualisation. In this paper, we explorethe domain generalisation technique to enable DL methods to be used in suchscenarios. To this extend, we propose to use super pixels generated with SimpleLinear Iterative Clustering (SLIC) which we refer to as \"SUPRA\" for SUPeRpixelAugmented method. SUPRA first generates a preliminary segmentation mask makinguse of our new loss \"SLICLoss\" that encourages both an accurate andcolor-consistent segmentation. We demonstrate that SLICLoss when combined withBinary Cross Entropy loss (BCE) can improve the model's generalisability withdata that presents significant domain shift. We validate this novel compoundloss on a vanilla U-Net using the EndoUDA dataset, which contains images forBarret's Esophagus and polyps from two modalities. We show that our methodyields an improvement of nearly 20% in the target domain set compared to thebaseline.", "output": "SUPRA: Superpixel Guided Loss for Improved Multi-modal Segmentation in Endoscopy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, the field of quantum science has attracted significantinterest across various disciplines, including quantum machine learning,quantum communication, and quantum computing. Among these emerging areas,quantum federated learning (QFL) has gained particular attention due to theintegration of quantum neural networks (QNNs) with traditional federatedlearning (FL) techniques. In this study, a novel approach entitled quantumsplit learning (QSL) is presented, which represents an advanced extension ofclassical split learning. Previous research in classical computing hasdemonstrated numerous advantages of split learning, such as acceleratedconvergence, reduced communication costs, and enhanced privacy protection. Tomaximize the potential of QSL, cross-channel pooling is introduced, a techniquethat capitalizes on the distinctive properties of quantum state tomographyfacilitated by QNNs. Through rigorous numerical analysis, evidence is providedthat QSL not only achieves a 1.64% higher top-1 accuracy compared to QFL butalso demonstrates robust privacy preservation in the context of the MNISTclassification task.", "output": "Quantum Split Neural Network Learning using Cross-Channel Pooling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ensembling has proven to be a powerful technique for boosting modelperformance, uncertainty estimation, and robustness in supervised learning.Advances in self-supervised learning (SSL) enable leveraging large unlabeledcorpora for state-of-the-art few-shot and supervised learning performance. Inthis paper, we explore how ensemble methods can improve recent SSL techniquesby developing a framework that permits data-dependent weighted cross-entropylosses. We refrain from ensembling the representation backbone; this choiceyields an efficient ensemble method that incurs a small training cost andrequires no architectural changes or computational overhead to downstreamevaluation. The effectiveness of our method is demonstrated with twostate-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al.,2022). Our method outperforms both in multiple evaluation metrics onImageNet-1K, particularly in the few-shot setting. We explore several weightingschemes and find that those which increase the diversity of ensemble heads leadto better downstream evaluation results. Thorough experiments yield improvedprior art baselines which our method still surpasses; e.g., our overallimprovement with MSN ViT-B/16 is 3.9 p.p. for 1-shot learning.", "output": "Weighted Ensemble Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently the Transformer structure has shown good performances in graphlearning tasks. However, these Transformer models directly work on graph nodesand may have difficulties learning high-level information. Inspired by thevision transformer, which applies to image patches, we propose a newTransformer-based graph neural network: Patch Graph Transformer (PatchGT).Unlike previous transformer-based models for learning graph representations,PatchGT learns from non-trainable graph patches, not from nodes directly. Itcan help save computation and improve the model performance. The key idea is tosegment a graph into patches based on spectral clustering without any trainableparameters, with which the model can first use GNN layers to learn patch-levelrepresentations and then use Transformer to obtain graph-level representations.The architecture leverages the spectral information of graphs and combines thestrengths of GNNs and Transformers. Further, we show the limitations ofprevious hierarchical trainable clusters theoretically and empirically. We alsoprove the proposed non-trainable spectral clustering method is permutationinvariant and can help address the information bottlenecks in the graph.PatchGT achieves higher expressiveness than 1-WL-type GNNs, and the empiricalstudy shows that PatchGT achieves competitive performances on benchmarkdatasets and provides interpretability to its predictions. The implementationof our algorithm is released at our Github repo:", "output": "PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding self-supervised learning is important but challenging. Previoustheoretical works study the role of pretraining losses, and view neuralnetworks as general black boxes. However, the recent work of Saunshi et al.argues that the model architecture -- a component largely ignored by previousworks -- also has significant influences on the downstream performance ofself-supervised learning. In this work, we provide the first theoreticalanalysis of self-supervised learning that incorporates the effect of inductivebiases originating from the model class. In particular, we focus on contrastivelearning -- a popular self-supervised learning method that is widely used inthe vision domain. We show that when the model has limited capacity,contrastive representations would recover certain special clustering structuresthat are compatible with the model architecture, but ignore many otherclustering structures in the data distribution. As a result, our theory cancapture the more realistic setting where contrastive representations have muchlower dimensionality than the number of clusters in the data distribution. Weinstantiate our theory on several synthetic data distributions, and provideempirical evidence to support the theory.", "output": "A Theoretical Study of Inductive Biases in Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Regression trees are one of the oldest forms of AI models, and theirpredictions can be made without a calculator, which makes them broadly useful,particularly for high-stakes applications. Within the large literature onregression trees, there has been little effort towards full provableoptimization, mainly due to the computational hardness of the problem. Thiswork proposes a dynamic-programming-with-bounds approach to the construction ofprovably-optimal sparse regression trees. We leverage a novel lower bound basedon an optimal solution to the k-Means clustering algorithm in 1-dimension overthe set of labels. We are often able to find optimal sparse trees in seconds,even for challenging datasets that involve large numbers of samples andhighly-correlated features.", "output": "Optimal Sparse Regression Trees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) has emerged as a promising distributed learningparadigm with an added advantage of data privacy. With the growing interest inhaving collaboration among data owners, FL has gained significant attention oforganizations. The idea of FL is to enable collaborating participants trainmachine learning (ML) models on decentralized data without breaching privacy.In simpler words, federated learning is the approach of ``bringing the model tothe data, instead of bringing the data to the mode''. Federated learning, whenapplied to data which is partitioned vertically across participants, is able tobuild a complete ML model by combining local models trained only using the datawith distinct features at the local sites. This architecture of FL is referredto as vertical federated learning (VFL), which differs from the conventional FLon horizontally partitioned data. As VFL is different from conventional FL, itcomes with its own issues and challenges. In this paper, we present astructured literature review discussing the state-of-the-art approaches in VFL.Additionally, the literature review highlights the existing solutions tochallenges in VFL and provides potential research directions in this domain.", "output": "Vertical Federated Learning: A Structured Literature Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transfer learning on edge is challenging due to on-device limited resources.Existing work addresses this issue by training a subset of parameters or addingmodel patches. Developed with inference in mind, Inverted Residual Blocks(IRBs) split a convolutional layer into depthwise and pointwise convolutions,leading to more stacking layers, e.g., convolution, normalization, andactivation layers. Though they are efficient for inference, IRBs require thatadditional activation maps are stored in memory for training weights forconvolution layers and scales for normalization layers. As a result, their highmemory cost prohibits training IRBs on resource-limited edge devices, andmaking them unsuitable in the context of transfer learning. To address thisissue, we present MobileTL, a memory and computationally efficient on-devicetransfer learning method for models built with IRBs. MobileTL trains the shiftsfor internal normalization layers to avoid storing activation maps for thebackward pass. Also, MobileTL approximates the backward computation of theactivation layer (e.g., Hard-Swish and ReLU6) as a signed function whichenables storing a binary mask instead of activation maps for the backward pass.MobileTL fine-tunes a few top blocks (close to output) rather than propagatingthe gradient through the whole network to reduce the computation cost. Ourmethod reduces memory usage by 46% and 53% for MobileNetV2 and V3 IRBs,respectively. For MobileNetV3, we observe a 36% reduction in floating-pointoperations (FLOPs) when fine-tuning 5 blocks, while only incurring a 0.6%accuracy reduction on CIFAR10. Extensive experiments on multiple datasetsdemonstrate that our method is Pareto-optimal (best accuracy under givenhardware constraints) compared to prior work in transfer learning for edgedevices.", "output": "MobileTL: On-device Transfer Learning with Inverted Residual Blocks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A current goal in the graph neural network literature is to enabletransformers to operate on graph-structured data, given their success onlanguage and vision tasks. Since the transformer's original sinusoidalpositional encodings (PEs) are not applicable to graphs, recent work hasfocused on developing graph PEs, rooted in spectral graph theory or variousspatial features of a graph. In this work, we introduce a new graph PE, GraphAutomaton PE (GAPE), based on weighted graph-walking automata (a novelextension of graph-walking automata). We compare the performance of GAPE withother PE schemes on both machine translation and graph-structured tasks, and weshow that it generalizes several other PEs. An additional contribution of thisstudy is a theoretical and controlled experimental comparison of many recentPEs in graph transformers, independent of the use of edge features.", "output": "Bridging Graph Position Encodings for Transformers with Weighted Graph-Walking Automata."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven soft sensors are extensively used in industrial and chemicalprocesses to predict hard-to-measure process variables whose real value isdifficult to track during routine operations. The regression models used bythese sensors often require a large number of labeled examples, yet obtainingthe label information can be very expensive given the high time and costrequired by quality inspections. In this context, active learning methods canbe highly beneficial as they can suggest the most informative labels to query.However, most of the active learning strategies proposed for regression focuson the offline setting. In this work, we adapt some of these approaches to thestream-based scenario and show how they can be used to select the mostinformative data points. We also demonstrate how to use a semi-supervisedarchitecture based on orthogonal autoencoders to learn salient features in alower dimensional space. The Tennessee Eastman Process is used to compare thepredictive performance of the proposed approaches.", "output": "Online Active Learning for Soft Sensor Development using Semi-Supervised Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model bias triggered by long-tailed data has been widely studied. However,measure based on the number of samples cannot explicate three phenomenasimultaneously: (1) Given enough data, the classification performance gain ismarginal with additional samples. (2) Classification performance decaysprecipitously as the number of training samples decreases when there isinsufficient data. (3) Model trained on sample-balanced datasets still hasdifferent biases for different classes. In this work, we define and quantifythe semantic scale of classes, which is used to measure the feature diversityof classes. It is exciting to find experimentally that there is a marginaleffect of semantic scale, which perfectly describes the first two phenomena.Further, the quantitative measurement of semantic scale imbalance is proposed,which can accurately reflect model bias on multiple datasets, even onsample-balanced data, revealing a novel perspective for the study of classimbalance. Due to the prevalence of semantic scale imbalance, we proposesemantic-scale-balanced learning, including a general loss improvement schemeand a dynamic re-weighting training framework that overcomes the challenge ofcalculating semantic scales in real-time during iterations. Comprehensiveexperiments show that dynamic semantic-scale-balanced learning consistentlyenables the model to perform superiorly on large-scale long-tailed andnon-long-tailed natural and medical datasets, which is a good starting pointfor mitigating the prevalent but unnoticed model bias.", "output": "Delving into Semantic Scale Imbalance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attention mechanisms form a core component of several successful deeplearning architectures, and are based on one key idea: ''The output dependsonly on a small (but unknown) segment of the input.'' In several practicalapplications like image captioning and language translation, this is mostlytrue. In trained models with an attention mechanism, the outputs of anintermediate module that encodes the segment of input responsible for theoutput is often used as a way to peek into the `reasoning` of the network. Wemake such a notion more precise for a variant of the classification problemthat we term selective dependence classification (SDC) when used with attentionmodel architectures. Under such a setting, we demonstrate various error modeswhere an attention model can be accurate but fail to be interpretable, and showthat such models do occur as a result of training. We illustrate varioussituations that can accentuate and mitigate this behaviour. Finally, we use ourobjective definition of interpretability for SDC tasks to evaluate a fewattention model learning algorithms designed to encourage sparsity anddemonstrate that these algorithms help improve interpretability.", "output": "On the Interpretability of Attention Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A further understanding of cause and effect within observational data iscritical across many domains, such as economics, health care, public policy,web mining, online advertising, and marketing campaigns. Although significantadvances have been made to overcome the challenges in causal effect estimationwith observational data, such as missing counterfactual outcomes and selectionbias between treatment and control groups, the existing methods mainly focus onsource-specific and stationary observational data. Such learning strategiesassume that all observational data are already available during the trainingphase and from only one source. This practical concern of accessibility isubiquitous in various academic and industrial applications. That's what itboiled down to: in the era of big data, we face new challenges in causalinference with observational data, i.e., the extensibility for incrementallyavailable observational data, the adaptability for extra domain adaptationproblem except for the imbalance between treatment and control groups, and theaccessibility for an enormous amount of data. In this position paper, weformally define the problem of continual treatment effect estimation, describeits research challenges, and then present possible solutions to this problem.Moreover, we will discuss future research directions on this topic.", "output": "Continual Causal Effect Estimation: Challenges and Opportunities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-driving vehicles rely on urban street maps for autonomous navigation. Inthis paper, we introduce Pix2Map, a method for inferring urban street maptopology directly from ego-view images, as needed to continually update andexpand existing maps. This is a challenging task, as we need to infer a complexurban road topology directly from raw image data. The main insight of thispaper is that this problem can be posed as cross-modal retrieval by learning ajoint, cross-modal embedding space for images and existing maps, represented asdiscrete graphs that encode the topological layout of the visual surroundings.We conduct our experimental evaluation using the Argoverse dataset and showthat it is indeed possible to accurately retrieve street maps corresponding toboth seen and unseen roads solely from image data. Moreover, we show that ourretrieved maps can be used to update or expand existing maps and even showproof-of-concept results for visual localization and image retrieval fromspatial graphs.", "output": "Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gestures that accompany speech are an essential part of natural and efficientembodied human communication. The automatic generation of such co-speechgestures is a long-standing problem in computer animation and is considered anenabling technology in film, games, virtual social spaces, and for interactionwith social robots. The problem is made challenging by the idiosyncratic andnon-periodic nature of human co-speech gesture motion, and by the greatdiversity of communicative functions that gestures encompass. Gesturegeneration has seen surging interest recently, owing to the emergence of moreand larger datasets of human gesture motion, combined with strides indeep-learning-based generative models, that benefit from the growingavailability of data. This review article summarizes co-speech gesturegeneration research, with a particular focus on deep generative models. First,we articulate the theory describing human gesticulation and how it complementsspeech. Next, we briefly discuss rule-based and classical statistical gesturesynthesis, before delving into deep learning approaches. We employ the choiceof input modalities as an organizing principle, examining systems that generategestures from audio, text, and non-linguistic input. We also chronicle theevolution of the related training data sets in terms of size, diversity, motionquality, and collection method. Finally, we identify key research challenges ingesture generation, including data availability and quality; producinghuman-like motion; grounding the gesture in the co-occurring speech ininteraction with other speakers, and in the environment; performing gestureevaluation; and integration of gesture synthesis into applications. Wehighlight recent approaches to tackling the various key challenges, as well asthe limitations of these approaches, and point toward areas of futuredevelopment.", "output": "A Comprehensive Review of Data-Driven Co-Speech Gesture Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The advances in deep learning have enabled machine learning methods tooutperform human beings in various areas, but it remains a great challenge fora well-trained model to quickly adapt to a new task. One promising solution torealize this goal is through meta-learning, also known as learning to learn,which has achieved promising results in few-shot learning. However, currentapproaches are still enormously different from human beings' learning process,especially in the ability to extract structural and transferable knowledge.This drawback makes current meta-learning frameworks non-interpretable and hardto extend to more complex tasks. We tackle this problem by introducing conceptdiscovery to the few-shot learning problem, where we achieve more effectiveadaptation by meta-learning the structure among the data features, leading to acomposite representation of the data. Our proposed method Concept-BasedModel-Agnostic Meta-Learning (COMAML) has been shown to achieve consistentimprovements in the structured data for both synthesized datasets andreal-world datasets.", "output": "Concept Discovery for Fast Adapatation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Heterogeneity of data distributed across clients limits the performance ofglobal models trained through federated learning, especially in the settingswith highly imbalanced class distributions of local datasets. In recent years,personalized federated learning (pFL) has emerged as a potential solution tothe challenges presented by heterogeneous data. However, existing pFL methodstypically enhance performance of local models at the expense of the globalmodel's accuracy. We propose FedHKD (Federated Hyper-Knowledge Distillation), anovel FL algorithm in which clients rely on knowledge distillation (KD) totrain local models. In particular, each client extracts and sends to the serverthe means of local data representations and the corresponding soft predictions-- information that we refer to as ``hyper-knowledge\". The server aggregatesthis information and broadcasts it to the clients in support of local training.Notably, unlike other KD-based pFL methods, FedHKD does not rely on a publicdataset nor it deploys a generative model at the server. We analyze convergenceof FedHKD and conduct extensive experiments on visual datasets in a variety ofscenarios, demonstrating that FedHKD provides significant improvement in bothpersonalized as well as global model performance compared to state-of-the-artFL methods designed for heterogeneous data settings.", "output": "The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep models have achieved impressive progress in solving partial differentialequations (PDEs). A burgeoning paradigm is learning neural operators toapproximate the input-output mappings of PDEs. While previous deep models haveexplored the multiscale architectures and various operator designs, they arelimited to learning the operators as a whole in the coordinate space. In realphysical science problems, PDEs are complex coupled equations with numericalsolvers relying on discretization into high-dimensional coordinate space, whichcannot be precisely approximated by a single operator nor efficiently learneddue to the curse of dimensionality. We present Latent Spectral Models (LSM)toward an efficient and precise solver for high-dimensional PDEs. Going beyondthe coordinate space, LSM enables an attention-based hierarchical projectionnetwork to reduce the high-dimensional data into a compact latent space inlinear time. Inspired by classical spectral methods in numerical analysis, wedesign a neural spectral block to solve PDEs in the latent space thatapproximates complex input-output mappings via learning multiple basisoperators, enjoying nice theoretical guarantees for convergence andapproximation. Experimentally, LSM achieves consistent state-of-the-art andyields a relative error reduction of 11.5% averaged on seven benchmarkscovering both solid and fluid physics.", "output": "Solving High-Dimensional PDEs with Latent Spectral Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graphs consisting of vocal nodes (\"the vocal minority\") and silent nodes(\"the silent majority\"), namely VS-Graph, are ubiquitous in the real world. Thevocal nodes tend to have abundant features and labels. In contrast, silentnodes only have incomplete features and rare labels, e.g., the description andpolitical tendency of politicians (vocal) are abundant while not for ordinarypeople (silent) on the twitter's social network. Predicting the silent majorityremains a crucial yet challenging problem. However, most existingmessage-passing based GNNs assume that all nodes belong to the same domain,without considering the missing features and distribution-shift betweendomains, leading to poor ability to deal with VS-Graph. To combat the abovechallenges, we propose Knowledge Transferable Graph Neural Network (KT-GNN),which models distribution shifts during message passing and representationlearning by transferring knowledge from vocal nodes to silent nodes.Specifically, we design the domain-adapted \"feature completion and messagepassing mechanism\" for node representation learning while preserving domaindifference. And a knowledge transferable classifier based on KL-divergence isfollowed. Comprehensive experiments on real-world scenarios (i.e., companyfinancial risk assessment and political elections) demonstrate the superiorperformance of our method. Our source code has been open sourced.", "output": "Predicting the Silent Majority on Graphs: Knowledge Transferable Graph Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work provides a formalization of Knowledge Graphs (KGs) as a new classof graphs that we denote doubly exchangeable attributed graphs, where node andpairwise (joint 2-node) representations must be equivariant to permutations ofboth node ids and edge (&amp; node) attributes (relations &amp; node features).Double-permutation equivariant KG representations open a new research directionin KGs. We show that this equivariance imposes a structural representation ofrelations that allows neural networks to perform complex logical reasoningtasks in KGs. Finally, we introduce a general blueprint for such equivariantrepresentations and test a simple GNN-based double-permutation equivariantneural architecture that achieve state-of-the-art Hits@10 test accuracy in theWN18RR, FB237 and NELL995 inductive KG completion tasks, and can accuratelyperform logical reasoning tasks that no existing methods can perform, to thebest of our knowledge.", "output": "Double Permutation Equivariance for Knowledge Graph Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "RNA, whose functionality is largely determined by its structure, plays animportant role in many biological activities. The prediction of pairwisestructural proximity between each nucleotide of an RNA sequence cancharacterize the structural information of the RNA. Historically, this problemhas been tackled by machine learning models using expert-engineered featuresand trained on scarce labeled datasets. Here, we find that the knowledgelearned by a protein-coevolution Transformer-based deep neural network can betransferred to the RNA contact prediction task. As protein datasets are ordersof magnitude larger than those for RNA contact prediction, our findings and thesubsequent framework greatly reduce the data scarcity bottleneck. Experimentsconfirm that RNA contact prediction through transfer learning using a publiclyavailable protein model is greatly improved. Our findings indicate that thelearned structural patterns of proteins can be transferred to RNAs, opening uppotential new avenues for research.", "output": "Knowledge from Large-Scale Protein Contact Prediction Models Can Be Transferred to the Data-Scarce RNA Contact Prediction Task."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Aggregating pharmaceutical data in the drug-target interaction (DTI) domainhas the potential to deliver life-saving breakthroughs. It is, however,notoriously difficult due to regulatory constraints and commercial interests.This work proposes the application of federated learning, which we argue to bereconcilable with the industry's constraints, as it does not require sharing ofany information that would reveal the entities' data or any other high-levelsummary of it. When used on a representative GraphDTA model and the KIBAdataset it achieves up to 15% improved performance relative to the bestavailable non-privacy preserving alternative. Our extensive battery ofexperiments shows that, unlike in other domains, the non-IID data distributionin the DTI datasets does not deteriorate FL performance. Additionally, weidentify a material trade-off between the benefits of adding new data, and thecost of adding more clients.", "output": "A Federated Learning Benchmark for Drug-Target Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Decentralised Machine Learning (DML) enables collaborative machine learningwithout centralised input data. Federated Learning (FL) and Edge Inference areexamples of DML. While tools for DML (especially FL) are starting to flourish,many are not flexible and portable enough to experiment with novel processors(e.g., RISC-V), non-fully connected network topologies, and asynchronouscollaboration schemes. We overcome these limitations via a domain-specificlanguage allowing us to map DML schemes to an underlying middleware, i.e. theFastFlow parallel programming library. We experiment with it by generatingdifferent working DML schemes on x86-64 and ARM platforms and an emergingRISC-V one. We characterise the performance and energy efficiency of thepresented schemes and systems. As a byproduct, we introduce a RISC-V porting ofthe PyTorch framework, the first publicly available to our knowledge.", "output": "Experimenting with Emerging RISC-V Systems for Decentralised Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce the Weak-form Estimation of Nonlinear Dynamics (WENDy) methodfor estimating model parameters for non-linear systems of ODEs. Without relyingon any numerical differential equation solvers, WENDy computes accurateestimates and is robust to large (biologically relevant) levels of measurementnoise. For low dimensional systems with modest amounts of data, WENDy iscompetitive with conventional forward solver-based nonlinear least squaresmethods in terms of speed and accuracy. For both higher dimensional systems andstiff systems, WENDy is typically both faster (often by orders of magnitude)and more accurate than forward solver-based approaches.The core mathematical idea involves an efficient conversion of the strongform representation of a model to its weak form, and then solving a regressionproblem to perform parameter inference. The core statistical idea rests on theErrors-In-Variables framework, which necessitates the use of the iterativelyreweighted least squares algorithm. Further improvements are obtained by usingorthonormal test functions, created from a set of C-infinity bump functions ofvarying support sizes.We demonstrate the high robustness and computational efficiency by applyingWENDy to estimate parameters in some common models from population biology,neuroscience, and biochemistry, including logistic growth, Lotka-Volterra,FitzHugh-Nagumo, Hindmarsh-Rose, and a Protein Transduction Benchmark model.Software and code for reproducing the examples is available at(", "output": "Direct Estimation of Parameters in ODE Models Using WENDy: Weak-form Estimation of Nonlinear Dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning techniques are effective for building predictive modelsbecause they are good at identifying patterns in large datasets. Development ofa model for complex real life problems often stops at the point of publication,proof of concept or when made accessible through some mode of deployment.However, a model in the medical domain risks becoming obsolete as soon aspatient demographic changes. The maintenance and monitoring of predictivemodels post-publication is crucial to guarantee their safe and effective longterm use. As machine learning techniques are effectively trained to look forpatterns in available datasets, the performance of a model for complex reallife problems will not peak and remain fixed at the point of publication oreven point of deployment. Rather, data changes over time, and they also changedwhen models are transported to new places to be used by new demography.", "output": "Learning machines for health and beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to its complexity, graph learning-based multi-modal integration andclassification is one of the most challenging obstacles for disease prediction.To effectively offset the negative impact between modalities in the process ofmulti-modal integration and extract heterogeneous information from graphs, wepropose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning).For the problem of negative impact between modalities, we propose a multi-modalgraph embedding module to construct a multi-modal graph. Different fromconventional methods that manually construct static graphs for all modalities,each modality generates a separate graph by adaptive learning, where a functiongraph and a supervision graph are introduced for optimization during themulti-graph fusion embedding process. We then propose a multi-kernel graphlearning module to extract heterogeneous information from the multi-modalgraph. The information in the multi-modal graph at different levels isaggregated by convolutional kernels with different receptive field sizes,followed by generating a cross-kernel discovery tensor for disease prediction.Our method is evaluated on the benchmark Autism Brain Imaging Data Exchange(ABIDE) dataset and outperforms the state-of-the-art methods. In addition,discriminative brain regions associated with autism are identified by ourmodel, providing guidance for the study of autism pathology.", "output": "Multi-modal Multi-kernel Graph Learning for Autism Prediction and Biomarker Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating optimal dynamic policies from offline data is a fundamentalproblem in dynamic decision making. In the context of causal inference, theproblem is known as estimating the optimal dynamic treatment regime. Eventhough there exists a plethora of methods for estimation, constructingconfidence intervals for the value of the optimal regime and structuralparameters associated with it is inherently harder, as it involves non-linearand non-differentiable functionals of un-known quantities that need to beestimated. Prior work resorted to sub-sample approaches that can deterioratethe quality of the estimate. We show that a simple soft-max approximation tothe optimal treatment regime, for an appropriately fast growing temperatureparameter, can achieve valid inference on the truly optimal regime. Weillustrate our result for a two-period optimal dynamic regime, though ourapproach should directly extend to the finite horizon case. Our work combinestechniques from semi-parametric inference and $g$-estimation, together with anappropriate triangular array central limit theorem, as well as a novel analysisof the asymptotic influence and asymptotic bias of softmax approximations.", "output": "Inference on Optimal Dynamic Policies via Softmax Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rendering and inverse-rendering algorithms that drive conventional computergraphics have recently been superseded by neural representations (NR). NRs haverecently been used to learn the geometric and the material properties of thescenes and use the information to synthesize photorealistic imagery, therebypromising a replacement for traditional rendering algorithms with scalablequality and predictable performance. In this work we ask the question: Doesneural graphics (NG) need hardware support? We studied representative NGapplications showing that, if we want to render 4k res. at 60FPS there is a gapof 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,there is an even larger gap of 2-4 OOM between the desired performance and therequired system power. We identify that the input encoding and the MLP kernelsare the performance bottlenecks, consuming 72%,60% and 59% of application timefor multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,respectively. We propose a NG processing cluster, a scalable and flexiblehardware architecture that directly accelerates the input encoding and MLPkernels through dedicated engines and supports a wide range of NG applications.We also accelerate the rest of the kernels by fusing them together in Vulkan,which leads to 9.94X kernel-level performance improvement compared to un-fusedimplementation of the pre-processing and the post-processing kernels. Ourresults show that, NGPC gives up to 58X end-to-end application-levelperformance improvement, for multi res. hashgrid encoding on average across thefour NG applications, the performance benefits are 12X,20X,33X and 39X for thescaling factor of 8,16,32 and 64, respectively. Our results show that withmulti res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPSfor NeRF and 8k res. at 120FPS for all our other NG applications.", "output": "Hardware Acceleration of Neural Graphics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The phenomenon of distinct behaviors exhibited by neural networks undervarying scales of initialization remains an enigma in deep learning research.In this paper, based on the earlier work by Luo et al.~cite{luo2021phase}, wepresent a phase diagram of initial condensation for two-layer neural networks.Condensation is a phenomenon wherein the weight vectors of neural networksconcentrate on isolated orientations during the training process, and it is afeature in non-linear learning process that enables neural networks to possessbetter generalization abilities. Our phase diagram serves to provide acomprehensive understanding of the dynamical regimes of neural networks andtheir dependence on the choice of hyperparameters related to initialization.Furthermore, we demonstrate in detail the underlying mechanisms by which smallinitialization leads to condensation at the initial training stage.", "output": "Phase Diagram of Initial Condensation for Two-layer Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The density of states (DOS) is a spectral property of materials, whichprovides fundamental insights on various characteristics of materials. In thispaper, we propose a model to predict the DOS by reflecting the nature of DOS:DOS determines the general distribution of states as a function of energy.Specifically, we integrate the heterogeneous information obtained from thecrystal structure and the energies via multi-modal transformer, therebymodeling the complex relationships between the atoms in the crystal structure,and various energy levels. Extensive experiments on two types of DOS, i.e.,Phonon DOS and Electron DOS, with various real-world scenarios demonstrate thesuperiority of DOSTransformer. The source code for DOSTransformer is availableat ", "output": "Predicting Density of States via Multi-modal Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rising complexity of numerous novel applications that serve ourmodern society comes the strong need to design efficient computing platforms.Designing efficient hardware is, however, a complex multi-objective problemthat deals with multiple parameters and their interactions. Given that thereare a large number of parameters and objectives involved in hardware design,synthesizing all possible combinations is not a feasible method to find theoptimal solution. One promising approach to tackle this problem is statisticalmodeling of a desired hardware performance. Here, we propose a model-basedactive learning approach to solve this problem. Our proposed method usesBayesian models to characterize various aspects of hardware performance. Wealso use transfer learning and Gaussian regression bootstrapping techniques inconjunction with active learning to create more accurate models. Our proposedstatistical modeling method provides hardware models that are sufficientlyaccurate to perform design space exploration as well as performance predictionsimultaneously. We use our proposed method to perform design space explorationand performance prediction for various hardware setups, such asmicro-architecture design and OpenCL kernels for FPGA targets. Our experimentsshow that the number of samples required to create performance modelssignificantly reduces while maintaining the predictive power of our proposedstatistical models. For instance, in our performance prediction setting, theproposed method needs 65% fewer samples to create the model, and in the designspace exploration setting, our proposed method can find the best parametersettings by exploring less than 50 samples.", "output": "Statistical Hardware Design With Multi-model Active Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lithography is fundamental to integrated circuit fabrication, necessitatinglarge computation overhead. The advancement of machine learning (ML)-basedlithography models alleviates the trade-offs between manufacturing processexpense and capability. However, all previous methods regard the lithographysystem as an image-to-image black box mapping, utilizing network parameters tolearn by rote mappings from massive mask-to-aerial or mask-to-resist imagepairs, resulting in poor generalization capability. In this paper, we propose anew ML-based paradigm disassembling the rigorous lithographic model intonon-parametric mask operations and learned optical kernels containingdeterminant source, pupil, and lithography information. By optimizingcomplex-valued neural fields to perform optical kernel regression fromcoordinates, our method can accurately restore lithography system using asmall-scale training dataset with fewer parameters, demonstrating superiorgeneralization capability as well. Experiments show that our framework can use31% of parameters while achieving 69$times$ smaller mean squared error with1.3$times$ higher throughput than the state-of-the-art.", "output": "Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The huge supporting training data on the Internet has been a key factor inthe success of deep learning models. However, this abundance ofpublic-available data also raises concerns about the unauthorized exploitationof datasets for commercial purposes, which is forbidden by dataset licenses. Inthis paper, we propose a backdoor-based watermarking approach that serves as ageneral framework for safeguarding public-available data. By inserting a smallnumber of watermarking samples into the dataset, our approach enables thelearning model to implicitly learn a secret function set by defenders. Thishidden function can then be used as a watermark to track down third-partymodels that use the dataset illegally. Unfortunately, existing backdoorinsertion methods often entail adding arbitrary and mislabeled data to thetraining set, leading to a significant drop in performance and easy detectionby anomaly detection algorithms. To overcome this challenge, we introduce aclean-label backdoor watermarking framework that uses imperceptibleperturbations to replace mislabeled samples. As a result, the watermarkingsamples remain consistent with the original labels, making them difficult todetect. Our experiments on text, image, and audio datasets demonstrate that theproposed framework effectively safeguards datasets with minimal impact onoriginal task performance. We also show that adding just 1% of watermarkingsamples can inject a traceable watermarking function and that our watermarkingsamples are stealthy and look benign upon visual inspection.", "output": "Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning (DL) can aid doctors in detecting worsening patient statesearly, affording them time to react and prevent bad outcomes. While DL-basedearly warning models usually work well in the hospitals they were trained for,they tend to be less reliable when applied at new hospitals. This makes itdifficult to deploy them at scale. Using carefully harmonised intensive caredata from four data sources across Europe and the US (totalling 334,812 stays),we systematically assessed the reliability of DL models for three commonadverse events: death, acute kidney injury (AKI), and sepsis. We tested whetherusing more than one data source and/or explicitly optimising forgeneralisability during training improves model performance at new hospitals.We found that models achieved high AUROC for mortality (0.838-0.869), AKI(0.823-0.866), and sepsis (0.749-0.824) at the training hospital. As expected,performance dropped at new hospitals, sometimes by as much as -0.200. Usingmore than one data source for training mitigated the performance drop, withmulti-source models performing roughly on par with the best single-sourcemodel. This suggests that as data from more hospitals become available fortraining, model robustness is likely to increase, lower-bounding robustnesswith the performance of the most applicable data source in the training data.Dedicated methods promoting generalisability did not noticeably improveperformance in our experiments.", "output": "From Single-Hospital to Multi-Centre Applications: Enhancing the Generalisability of Deep Learning Models for Adverse Event Prediction in the ICU."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present emph{TabRet}, a pre-trainable Transformer-based model for tabulardata. TabRet is designed to work on a downstream task that contains columns notseen in pre-training. Unlike other methods, TabRet has an extra learning stepbefore fine-tuning called emph{retokenizing}, which calibrates featureembeddings based on the masked autoencoding loss. In experiments, wepre-trained TabRet with a large collection of public health surveys andfine-tuned it on classification tasks in healthcare, and TabRet achieved thebest AUC performance on four datasets. In addition, an ablation study showsretokenizing and random shuffle augmentation of columns during pre-trainingcontributed to performance gains. The code is available at .", "output": "TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Low-Earth orbit (LEO) satellites have been prosperously deployed for variousEarth observation missions due to its capability of collecting a large amountof image or sensor data. However, traditionally, the data training process isperformed in the terrestrial cloud server, which leads to a high transmissionoverhead. With the recent development of LEO, it is more imperative to provideultra-dense LEO constellation with enhanced on-board computation capability.Benefited from it, we have proposed a collaborative federated learning for lowEarth orbit (FELLO). We allocate the entire process on LEOs with low payloadinter-satellite transmissions, whilst the low-delay terrestrial gateway server(GS) only takes care for initial signal controlling. The GS initially selectsan LEO server, whereas its LEO clients are all determined by clusteringmechanism and communication capability through the optical inter-satellitelinks (ISLs). The re-clustering of changing LEO server will be executed oncewith low communication quality of FELLO. In the simulations, we havenumerically analyzed the proposed FELLO under practical Walker-based LEOconstellation configurations along with MNIST training dataset forclassification mission. The proposed FELLO outperforms the conventionalcentralized and distributed architectures with higher classification accuracyas well as comparably lower latency of joint communication and computing.", "output": "Edge Selection and Clustering for Federated Learning in Optical Inter-LEO Satellite Constellation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pretrained language models often generate outputs that are not in line withhuman preferences, such as harmful text or factually incorrect summaries.Recent work approaches the above issues by learning from a simple form of humanfeedback: comparisons between pairs of model-generated outputs. However,comparison feedback only conveys limited information about human preferences.In this paper, we introduce Imitation learning from Language Feedback (ILF), anew approach that utilizes more informative language feedback. ILF consists ofthree steps that are applied iteratively: first, conditioning the languagemodel on the input, an initial LM output, and feedback to generate refinements.Second, selecting the refinement incorporating the most feedback. Third,finetuning the language model to maximize the likelihood of the chosenrefinement given the input. We show theoretically that ILF can be viewed asBayesian Inference, similar to Reinforcement Learning from human feedback. Weevaluate ILF's effectiveness on a carefully-controlled toy task and a realisticsummarization task. Our experiments demonstrate that large language modelsaccurately incorporate feedback and that finetuning with ILF scales well withthe dataset size, even outperforming finetuning on human summaries. Learningfrom both language and comparison feedback outperforms learning from eachalone, achieving human-level summarization performance.", "output": "Training Language Models with Language Feedback at Scale."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networkshave proven to be highly effective, achieving state-of-the-art results. Thisstudy introduces a novel streaming architecture based toolflow for mapping suchmodels onto FPGAs considering the model's inherent characteristics and thefeatures of the targeted FPGA device. The HARFLOW3D toolflow takes as input a3D CNN in ONNX format and a description of the FPGA characteristics, generatinga design that minimizes the latency of the computation. The toolflow iscomprised of a number of parts, including i) a 3D CNN parser, ii) a performanceand resource model, iii) a scheduling algorithm for executing 3D models on thegenerated hardware, iv) a resource-aware optimization engine tailored for 3Dmodels, v) an automated mapping to synthesizable code for FPGAs. The ability ofthe toolflow to support a broad range of models and devices is shown through anumber of experiments on various 3D CNN and FPGA system pairs. Furthermore, thetoolflow has produced high-performing results for 3D CNN models that have notbeen mapped to FPGAs before, demonstrating the potential of FPGA-based systemsin this space. Overall, HARFLOW3D has demonstrated its ability to delivercompetitive latency compared to a range of state-of-the-art hand-tunedapproaches being able to achieve up to 5$times$ better performance compared tosome of the existing works.", "output": "HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We focus on learning unknown dynamics from data using ODE-nets templated onimplicit numerical initial value problem solvers. First, we perform InverseModified error analysis of the ODE-nets using unrolled implicit schemes forease of interpretation. It is shown that training an ODE-net using an unrolledimplicit scheme returns a close approximation of an Inverse ModifiedDifferential Equation (IMDE). In addition, we establish a theoretical basis forhyper-parameter selection when training such ODE-nets, whereas currentstrategies usually treat numerical integration of ODE-nets as a black box. Wethus formulate an adaptive algorithm which monitors the level of error andadapts the number of (unrolled) implicit solution iterations during thetraining process, so that the error of the unrolled approximation is less thanthe current learning loss. This helps accelerate training, while maintainingaccuracy. Several numerical experiments are performed to demonstrate theadvantages of the proposed algorithm compared to nonadaptive unrollings, andvalidate the theoretical analysis. We also note that this approach naturallyallows for incorporating partially known physical terms in the equations,giving rise to what is termed ``gray box\" identification.", "output": "Implementation and (Inverse Modified) Error Analysis for implicitly-templated ODE-nets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A backdoor attack allows a malicious user to manipulate the environment orcorrupt the training data, thus inserting a backdoor into the trained agent.Such attacks compromise the RL system's reliability, leading to potentiallycatastrophic results in various key fields. In contrast, relatively limitedresearch has investigated effective defenses against backdoor attacks in RL.This paper proposes the Recovery Triggered States (RTS) method, a novelapproach that effectively protects the victim agents from backdoor attacks. RTSinvolves building a surrogate network to approximate the dynamics model.Developers can then recover the environment from the triggered state to a cleanstate, thereby preventing attackers from activating backdoors hidden in theagent by presenting the trigger. When training the surrogate to predict states,we incorporate agent action information to reduce the discrepancy between theactions taken by the agent on predicted states and the actions taken on realstates. RTS is the first approach to defend against backdoor attacks in asingle-agent setting. Our results show that using RTS, the cumulative rewardonly decreased by 1.41% under the backdoor attack.", "output": "Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformations based on domain expertise (expert transformations), such asrandom-resized-crop and color-jitter, have proven critical to the success ofcontrastive learning techniques such as SimCLR. Recently, several attempts havebeen made to replace such domain-specific, human-designed transformations withgenerated views that are learned. However for imagery data, so far none ofthese view-generation methods has been able to outperform experttransformations. In this work, we tackle a different question: instead ofreplacing expert transformations with generated views, can we constructivelyassimilate generated views with expert transformations? We answer this questionin the affirmative and propose a view generation method and a simple, effectiveassimilation method that together improve the state-of-the-art by up to ~3.6%on three different datasets. Importantly, we conduct a detailed empirical studythat systematically analyzes a range of view generation and assimilationmethods and provides a holistic picture of the efficacy of learned views incontrastive representation learning.", "output": "Constructive Assimilation: Boosting Contrastive Learning Performance through View Generation Strategies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, deep learning has been widely used in SAR ATR and achievedexcellent performance on the MSTAR dataset. However, due to constrained imagingconditions, MSTAR has data biases such as background correlation, i.e.,background clutter properties have a spurious correlation with target classes.Deep learning can overfit clutter to reduce training errors. Therefore, thedegree of overfitting for clutter reflects the non-causality of deep learningin SAR ATR. Existing methods only qualitatively analyze this phenomenon. Inthis paper, we quantify the contributions of different regions to targetrecognition based on the Shapley value. The Shapley value of clutter measuresthe degree of overfitting. Moreover, we explain how data bias and model biascontribute to non-causality. Concisely, data bias leads to comparablesignal-to-clutter ratios and clutter textures in training and test sets. Andvarious model structures have different degrees of overfitting for thesebiases. The experimental results of various models under standard operatingconditions on the MSTAR dataset support our conclusions. Our code is availableat ", "output": "Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Scene Representation Transformer (SRT) is a recent method to render novelviews at interactive rates. Since SRT uses camera poses with respect to anarbitrarily chosen reference camera, it is not invariant to the order of theinput views. As a result, SRT is not directly applicable to large-scale sceneswhere the reference frame would need to be changed regularly. In this work, wepropose Relative Pose Attention SRT (RePAST): Instead of fixing a referenceframe at the input, we inject pairwise relative camera pose informationdirectly into the attention mechanism of the Transformers. This leads to amodel that is by definition invariant to the choice of any global referenceframe, while still retaining the full capabilities of the original method.Empirical results show that adding this invariance to the model does not leadto a loss in quality. We believe that this is a step towards applying fullylatent transformer-based rendering methods to large-scale scenes.", "output": "RePAST: Relative Pose Attention Scene Representation Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conformal prediction is a statistical tool for producing prediction regionsof machine learning models that are valid with high probability. However,applying conformal prediction to time series data leads to conservativeprediction regions. In fact, to obtain prediction regions over $T$ time stepswith confidence $1-delta$, {previous works require that each individualprediction region is valid} with confidence $1-delta/T$. We propose anoptimization-based method for reducing this conservatism to enable long horizonplanning and verification when using learning-enabled time series predictors.Instead of considering prediction errors individually at each time step, weconsider a parameterized prediction error over multiple time steps. Byoptimizing the parameters over an additional dataset, we find predictionregions that are not conservative. We show that this problem can be cast as amixed integer linear complementarity program (MILCP), which we then relax intoa linear complementarity program (LCP). Additionally, we prove that the relaxedLP has the same optimal cost as the original MILCP. Finally, we demonstrate theefficacy of our method on a case study using pedestrian trajectory predictors.", "output": "Conformal Prediction Regions for Time Series using Linear Complementarity Programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative A.I. models have emerged as versatile tools across diverseindustries, with applications in privacy-preserving data sharing, computationalart, personalization of products and services, and immersive entertainment.Here, we introduce a new privacy concern in the adoption and use of generativeA.I. models: that of coincidental generation, where a generative model's outputis similar enough to an existing entity, beyond those represented in thedataset used to train the model, to be mistaken for it. Consider, for example,synthetic portrait generators, which are today deployed in commercialapplications such as virtual modeling agencies and synthetic stock photography.Due to the low intrinsic dimensionality of human face perception, everysynthetically generated face will coincidentally resemble an actual person.Such examples of coincidental generation all but guarantee the misappropriationof likeness and expose organizations that use generative A.I. to legal andregulatory risk.", "output": "Coincidental Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Safety is the primary priority of autonomous driving. Nevertheless, nopublished dataset currently supports the direct and explainable safetyevaluation for autonomous driving. In this work, we propose DeepAccident, alarge-scale dataset generated via a realistic simulator containing diverseaccident scenarios that frequently occur in real-world driving. The proposedDeepAccident dataset contains 57K annotated frames and 285K annotated samples,approximately 7 times more than the large-scale nuScenes dataset with 40kannotated samples. In addition, we propose a new task, end-to-end motion andaccident prediction, based on the proposed dataset, which can be used todirectly evaluate the accident prediction ability for different autonomousdriving algorithms. Furthermore, for each scenario, we set four vehicles alongwith one infrastructure to record data, thus providing diverse viewpoints foraccident scenarios and enabling V2X (vehicle-to-everything) research onperception and prediction tasks. Finally, we present a baseline V2X model namedV2XFormer that demonstrates superior performance for motion and accidentprediction and 3D object detection compared to the single-vehicle model.", "output": "DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In response to innovations in machine learning (ML) models, productionworkloads changed radically and rapidly. TPU v4 is the fifth Google domainspecific architecture (DSA) and its third supercomputer for such ML models.Optical circuit switches (OCSes) dynamically reconfigure its interconnecttopology to improve scale, availability, utilization, modularity, deployment,security, power, and performance; users can pick a twisted 3D torus topology ifdesired. Much cheaper, lower power, and faster than Infiniband, OCSes andunderlying optical components are &lt;5% of system cost and &lt;3% of system power.Each TPU v4 includes SparseCores, dataflow processors that accelerate modelsthat rely on embeddings by 5x-7x yet use only 5% of die area and power.Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improvesperformance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chipsand thus ~10x faster overall, which along with OCS flexibility helps largelanguage models. For similar sized systems, it is ~4.3x-4.5x faster than theGraphcore IPU Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power thanthe Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computersof Google Cloud use ~3x less energy and produce ~20x less CO2e thancontemporary DSAs in a typical on-premise data center.", "output": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Skin lesion recognition using deep learning has made remarkable progress, andthere is an increasing need for deploying these systems in real-worldscenarios. However, recent research has revealed that deep neural networks forskin lesion recognition may overly depend on disease-irrelevant image artifacts(i.e. dark corners, dense hairs), leading to poor generalization in unseenenvironments. To address this issue, we propose a novel domain generalizationmethod called EPVT, which involves embedding prompts into the visiontransformer to collaboratively learn knowledge from diverse domains.Concretely, EPVT leverages a set of domain prompts, each of which plays as adomain expert, to capture domain-specific knowledge; and a shared prompt forgeneral knowledge over the entire dataset. To facilitate knowledge sharing andthe interaction of different prompts, we introduce a domain prompt generatorthat enables low-rank multiplicative updates between domain prompts and theshared prompt. A domain mixup strategy is additionally devised to reduce theco-occurring artifacts in each domain, which allows for more flexible decisionmargins and mitigates the issue of incorrectly assigned domain labels.Experiments on four out-of-distribution datasets and six different biased ISICdatasets demonstrate the superior generalization ability of EPVT in skin lesionrecognition across various environments. Our code and dataset will be releasedat ", "output": "EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While federated learning is promising for privacy-preserving collaborativelearning without revealing local data, it remains vulnerable to white-boxattacks and struggles to adapt to heterogeneous clients. Federated distillation(FD), built upon knowledge distillation--an effective technique fortransferring knowledge from a teacher model to student models--emerges as analternative paradigm, which provides enhanced privacy guarantees and addressesmodel heterogeneity. Nevertheless, challenges arise due to variations in localdata distributions and the absence of a well-trained teacher model, which leadsto misleading and ambiguous knowledge sharing that significantly degrades modelperformance. To address these issues, this paper proposes a selective knowledgesharing mechanism for FD, termed Selective-FD. It includes client-sideselectors and a server-side selector to accurately and precisely identifyknowledge from local and ensemble predictions, respectively. Empirical studies,backed by theoretical insights, demonstrate that our approach enhances thegeneralization capabilities of the FD framework and consistently outperformsbaseline methods. This study presents a promising direction for effectiveknowledge transfer in privacy-preserving collaborative learning.", "output": "Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a complete software pipeline for revealing the hidden texts of theHerculaneum papyri using X-ray CT images. This enhanced virtual unwrappingpipeline combines machine learning with a novel geometric framework linking 3Dand 2D images. We also present EduceLab-Scrolls, a comprehensive open datasetrepresenting two decades of research effort on this problem. EduceLab-Scrollscontains a set of volumetric X-ray CT images of both small fragments andintact, rolled scrolls. The dataset also contains 2D image labels that are usedin the supervised training of an ink detection model. Labeling is enabled byaligning spectral photography of scroll fragments with X-ray CT images of thesame fragments, thus creating a machine-learnable mapping between image spacesand modalities. This alignment permits supervised learning for the detection of\"invisible\" carbon ink in X-ray CT, a task that is \"impossible\" even for humanexpert labelers. To our knowledge, this is the first aligned dataset of itskind and is the largest dataset ever released in the heritage domain. Ourmethod is capable of revealing accurate lines of text on scroll fragments withknown ground truth. Revealed text is verified using visual confirmation,quantitative image metrics, and scholarly review. EduceLab-Scrolls has alsoenabled the discovery, for the first time, of hidden texts from the Herculaneumpapyri, which we present here. We anticipate that the EduceLab-Scrolls datasetwill generate more textual discovery as research continues.", "output": "EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri using X-ray CT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we consider a new class of RBF (Radial Basis Function) neuralnetworks, in which smoothing factors are replaced with shifts. We prove undercertain conditions on the activation function that these networks are capableof approximating any continuous multivariate function on any compact subset ofthe $d$-dimensional Euclidean space. For RBF networks with finitely many fixedcentroids we describe conditions guaranteeing approximation with arbitraryprecision.", "output": "On the universal approximation property of radial basis function neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently developed text-to-image diffusion models make it easy to edit orcreate high-quality images. Their ease of use has raised concerns about thepotential for malicious editing or deepfake creation. Imperceptibleperturbations have been proposed as a means of protecting images from maliciousediting by preventing diffusion models from generating realistic images.However, we find that the aforementioned perturbations are not robust to JPEGcompression, which poses a major weakness because of the common usage andavailability of JPEG. We discuss the importance of robustness for additiveimperceptible perturbations and encourage alternative approaches to protectimages against editing.", "output": "JPEG Compressed Images Can Bypass Protections Against AI Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Architectures Search (NAS) becomes more and more popular over theseyears. However, NAS-generated models tends to suffer greater vulnerability tovarious malicious attacks. Lots of robust NAS methods leverage adversarialtraining to enhance the robustness of NAS-generated models, however, theyneglected the nature accuracy of NAS-generated models. In our paper, we proposea novel NAS method, Robust Neural Architecture Search (RNAS). To design aregularization term to balance accuracy and robustness, RNAS generatesarchitectures with both high accuracy and good robustness. To reduce searchcost, we further propose to use noise examples instead adversarial examples asinput to search architectures. Extensive experiments show that RNAS achievesstate-of-the-art (SOTA) performance on both image classification andadversarial attacks, which illustrates the proposed RNAS achieves a goodtradeoff between robustness and accuracy.", "output": "Robust Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, reinforcement learning (RL) has emerged as a popularapproach for solving sequence-based tasks in machine learning. However, findingsuitable alternatives to RL remains an exciting and innovative research area.One such alternative that has garnered attention is the Non-Axiomatic ReasoningSystem (NARS), which is a general-purpose cognitive reasoning framework. Inthis paper, we delve into the potential of NARS as a substitute for RL insolving sequence-based tasks. To investigate this, we conduct a comparativeanalysis of the performance of ONA as an implementation of NARS and$Q$-Learning in various environments that were created using the Open AI gym.The environments have different difficulty levels, ranging from simple tocomplex. Our results demonstrate that NARS is a promising alternative to RL,with competitive performance in diverse environments, particularly innon-deterministic ones.", "output": "Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of knowledge graph (KG) applications has led to a rising needfor entity alignment (EA) between heterogeneous KGs that are extracted fromvarious sources. Recently, graph neural networks (GNNs) have been widelyadopted in EA tasks due to GNNs' impressive ability to capture structureinformation. However, we have observed that the oversimplified settings of theexisting common EA datasets are distant from real-world scenarios, whichobstructs a full understanding of the advancements achieved by recent methods.This phenomenon makes us ponder: Do existing GNN-based EA methods really makegreat progress?In this paper, to study the performance of EA methods in realistic settings,we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGsand general KGs) which are different with regard to the scale and structure,and share fewer overlapping entities. First, we sweep the unreasonablesettings, and propose two new HHKG datasets that closely mimic real-world EAscenarios. Then, based on the proposed datasets, we conduct extensiveexperiments to evaluate previous representative EA methods, and revealinteresting findings about the progress of GNN-based EA methods. We find thatthe structural information becomes difficult to exploit but still valuable inaligning HHKGs. This phenomenon leads to inferior performance of existing EAmethods, especially GNN-based methods. Our findings shed light on the potentialproblems resulting from an impulsive application of GNN-based methods as apanacea for all EA datasets. Finally, we introduce a simple but effectivemethod: Simple-HHEA, which comprehensively utilizes entity name, structure, andtemporal information. Experiment results show Simple-HHEA outperforms previousmodels on HHKG datasets.", "output": "Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing contrastive learning methods for anomalous sound detection refinethe audio representation of each audio sample by using the contrast between thesamples' augmentations (e.g., with time or frequency masking). However, theymight be biased by the augmented data, due to the lack of physical propertiesof machine sound, thereby limiting the detection performance. This paper usescontrastive learning to refine audio representations for each machine ID,rather than for each audio sample. The proposed two-stage method usescontrastive learning to pretrain the audio representation model byincorporating machine ID and a self-supervised ID classifier to fine-tune thelearnt model, while enhancing the relation between audio features from the sameID. Experiments show that our method outperforms the state-of-the-art methodsusing contrastive learning or self-supervised classification in overall anomalydetection performance and stability on DCASE 2020 Challenge Task2 dataset.", "output": "Anomalous Sound Detection using Audio Representation with Machine ID based Contrastive Learning Pretraining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the combination of the alternating direction method of multipliers(ADMM) with physics-informed neural networks (PINNs) for a general class ofnonsmooth partial differential equation (PDE)-constrained optimizationproblems, where additional regularization can be employed for constraints onthe control or design variables. The resulting ADMM-PINNs algorithmic frameworksubstantially enlarges the applicable range of PINNs to nonsmooth cases ofPDE-constrained optimization problems. The application of the ADMM makes itpossible to untie the PDE constraints and the nonsmooth regularization termsfor iterations. Accordingly, at each iteration, one of the resultingsubproblems is a smooth PDE-constrained optimization which can be efficientlysolved by PINNs, and the other is a simple nonsmooth optimization problem whichusually has a closed-form solution or can be efficiently solved by variousstandard optimization algorithms or pre-trained neural networks. The ADMM-PINNsalgorithmic framework does not require to solve PDEs repeatedly, and it ismesh-free, easy to implement, and scalable to different PDE settings. Wevalidate the efficiency of the ADMM-PINNs algorithmic framework by differentprototype applications, including inverse potential problems, sourceidentification in elliptic equations, control constrained optimal control ofthe Burgers equation, and sparse optimal control of parabolic equations.", "output": "The ADMM-PINNs Algorithmic Framework for Nonsmooth PDE-Constrained Optimization: A Deep Learning Approach."}]