[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A study was conducted to prove AI software could be used to translate andgenerate illustrations without any human intervention. This was done with thepurpose of showing and distributing it to the external customer, Pratham Books.The project aligns with the company's vision by leveraging the generalisationand scalability of Machine Learning algorithms, offering significant costefficiency increases to a wide range of literary audiences in variedgeographical locations. A comparative study methodology was utilised todetermine the best performant method out of the 3 devised, Prompt Augmentationusing Keywords, CLIP Embedding Mask, and Cross Attention Control with EditorialPrompts. A thorough evaluation process was completed using both quantitativeand qualitative measures. Each method had its own strengths and weaknesses, butthrough the evaluation, method 1 was found to have the best yielding results.Promising future advancements may be made to further increase image quality byincorporating Large Language Models and personalised stylistic models. Thepresented approach can also be adapted to Video and 3D sculpture generation fornovel illustrations in digital webbooks.", "output": "Automatic Geo-alignment of Artwork in Children's Story Books."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, evolutionary computation (EC) has been promoted by machinelearning, distributed computing, and big data technologies, resulting in newresearch directions of EC like distributed EC and surrogate-assisted EC. Theseadvances have significantly improved the performance and the application scopeof EC, but also trigger privacy leakages, such as the leakage of optimalresults and surrogate model. Accordingly, evolutionary computation combinedwith privacy protection is becoming an emerging topic. However, privacyconcerns in evolutionary computation lack a systematic exploration, especiallyfor the object, motivation, position, and method of privacy protection. To thisend, in this paper, we discuss three typical optimization paradigms (i.e.,textit{centralized optimization, distributed optimization, and data-drivenoptimization}) to characterize optimization modes of evolutionary computationand propose BOOM to sort out privacy concerns in evolutionary computation.Specifically, the centralized optimization paradigm allows clients to outsourceoptimization problems to the centralized server and obtain optimizationsolutions from the server. While the distributed optimization paradigm exploitsthe storage and computational power of distributed devices to solveoptimization problems. Also, the data-driven optimization paradigm utilizesdata collected in history to tackle optimization problems lacking explicitobjective functions. Particularly, this paper adopts BOOM to characterize theobject and motivation of privacy protection in three typical optimizationparadigms and discusses potential privacy-preserving technologies balancingoptimization performance and privacy guarantees in three typical optimizationparadigms. Furthermore, this paper attempts to foresee some new researchdirections of privacy-preserving evolutionary computation.", "output": "When Evolutionary Computation Meets Privacy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised Relation Extraction (RE) aims to identify relations betweenentities in text, without having access to labeled data during training. Thissetting is particularly relevant for domain specific RE where no annotateddataset is available and for open-domain RE where the types of relations are apriori unknown. Although recent approaches achieve promising results, theyheavily depend on hyperparameters whose tuning would most often require labeleddata. To mitigate the reliance on hyperparameters, we propose PromptORE, a''Prompt-based Open Relation Extraction'' model. We adapt the novelprompt-tuning paradigm to work in an unsupervised setting, and use it to embedsentences expressing a relation. We then cluster these embeddings to discovercandidate relations, and we experiment different strategies to automaticallyestimate an adequate number of clusters. To the best of our knowledge,PromptORE is the first unsupervised RE model that does not need hyperparametertuning. Results on three general and specific domain datasets show thatPromptORE consistently outperforms state-of-the-art models with a relative gainof more than 40% in B 3 , V-measure and ARI. Qualitative analysis alsoindicates PromptORE's ability to identify semantically coherent clusters thatare very close to true relations.", "output": "PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The innovation of Wi-Fi 6, IEEE 802.11ax, was be approved as the nextsixth-generation (6G) technology of wireless local area networks (WLANs) byimproving the fundamental performance of latency, throughput, and so on. Themain technical feature of orthogonal frequency division multiple access (OFDMA)supports multi-users to transmit respective data concurrently via thecorresponding access points (APs). However, the conventional IEEE 802.11protocol for Wi-Fi roaming selects the target AP only depending on receivedsignal strength indication (RSSI) which is obtained by the received Responseframe from the APs. In the long term, it may lead to congestion in a singlechannel under the scenarios of dense users further increasing the associationdelay and packet drop rate, even reducing the quality of service (QoS) of theoverall system. In this paper, we propose a multi-agent deep Q-learning forfast roaming (MADAR) algorithm to effectively minimize the latency during thestation roaming for Smart Warehouse in Wi-Fi 6 system. The MADAR algorithmconsiders not only RSSI but also channel state information (CSI), and throughonline neural network learning and weighting adjustments to maximize the rewardof the action selected from Epsilon-Greedy. Compared to existing benchmarkmethods, the MADAR algorithm has been demonstrated for improved roaming latencyby analyzing the simulation result and realistic dataset.", "output": "Distributed Multi-Agent Deep Q-Learning for Fast Roaming in IEEE 802.11ax Wi-Fi Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks (NNs) playing the role of controllers have demonstratedimpressive empirical performances on challenging control problems. However, thepotential adoption of NN controllers in real-life applications also gives riseto a growing concern over the safety of these neural-network controlled systems(NNCSs), especially when used in safety-critical applications. In this work, wepresent POLAR-Express, an efficient and precise formal reachability analysistool for verifying the safety of NNCSs. POLAR-Express uses Taylor modelarithmetic to propagate Taylor models (TMs) across a neural networklayer-by-layer to compute an overapproximation of the neural-network function.It can be applied to analyze any feed-forward neural network with continuousactivation functions. We also present a novel approach to propagate TMs moreefficiently and precisely across ReLU activation functions. In addition,POLAR-Express provides parallel computation support for the layer-by-layerpropagation of TMs, thus significantly improving the efficiency and scalabilityover its earlier prototype POLAR. Across the comparison with six otherstate-of-the-art tools on a diverse set of benchmarks, POLAR-Express achievesthe best verification efficiency and tightness in the reachable set analysis.", "output": "POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose DoE2Vec, a variational autoencoder (VAE)-based methodology tolearn optimization landscape characteristics for downstream meta-learningtasks, e.g., automated selection of optimization algorithms. Principally, usinglarge training data sets generated with a random function generator, DoE2Vecself-learns an informative latent representation for any design of experiments(DoE). Unlike the classical exploratory landscape analysis (ELA) method, ourapproach does not require any feature engineering and is easily applicable forhigh dimensional search spaces. For validation, we inspect the quality oflatent reconstructions and analyze the latent representations using differentexperiments. The latent representations not only show promising potentials inidentifying similar (cheap-to-evaluate) surrogate functions, but also cansignificantly boost performances when being used complementary to the classicalELA features in classification tasks.", "output": "DoE2Vec: Deep-learning Based Features for Exploratory Landscape Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of dimensionality reduction and visualization of high-dimensionaldatasets remains a challenging problem since long. Modern high-throughputtechnologies produce newer high-dimensional datasets having multiple views withrelatively new data types. Visualization of these datasets require propermethodology that can uncover hidden patterns in the data without affecting thelocal and global structures within the data. To this end, however, very fewsuch methodology exist, which can realise this task. In this work, we haveintroduced a novel unsupervised deep neural network model, called NeuroDAVIS,for data visualization. NeuroDAVIS is capable of extracting important featuresfrom the data, without assuming any data distribution, and visualizeeffectively in lower dimension. It has been shown theoritically thatneighbourhood relationship of the data in high dimension remains preserved inlower dimension. The performance of NeuroDAVIS has been evaluated on a widevariety of synthetic and real high-dimensional datasets including numeric,textual, image and biological data. NeuroDAVIS has been highly competitiveagainst both t-Distributed Stochastic Neighbor Embedding (t-SNE) and UniformManifold Approximation and Projection (UMAP) with respect to visualizationquality, and preservation of data size, shape, and both local and globalstructure. It has outperformed Fast interpolation-based t-SNE (Fit-SNE), avariant of t-SNE, for most of the high-dimensional datasets as well. For thebiological datasets, besides t-SNE, UMAP and Fit-SNE, NeuroDAVIS has alsoperformed well compared to other state-of-the-art algorithms, like Potential ofHeat-diffusion for Affinity-based Trajectory Embedding (PHATE) and the siameseneural network-based method, called IVIS. Downstream classification andclustering analyses have also revealed favourable results forNeuroDAVIS-generated embeddings.", "output": "NeuroDAVIS: A neural network model for data visualization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rapid growth of data availability and usage, quantifying the addedvalue of each training data point has become a crucial process in the field ofartificial intelligence. The Shapley values have been recognized as aneffective method for data valuation, enabling efficient training setsummarization, acquisition, and outlier removal. In this paper, we introduce\"STI-KNN\", an innovative algorithm that calculates the exact pair-interactionShapley values for KNN models in O(t n^2) time, which is a significantimprovement over the O(2^n)$ time complexity of baseline methods. By usingSTI-KNN, we can efficiently and accurately evaluate the value of individualdata points, leading to improved training outcomes and ultimately enhancing theeffectiveness of artificial intelligence applications.", "output": "Optimizing Data Shapley Interaction Calculation from O(2^n) to O(t n^2) for KNN models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Abnormal event detection, which refers to mining unusual interactions amonginvolved entities, plays an important role in many real applications. Previousworks mostly over-simplify this task as detecting abnormal pair-wiseinteractions. However, real-world events may contain multi-typed attributedentities and complex interactions among them, which forms an AttributedHeterogeneous Information Network (AHIN). With the boom of social networks,abnormal event detection in AHIN has become an important, but seldom exploredtask. In this paper, we firstly study the unsupervised abnormal event detectionproblem in AHIN. The events are considered as star-schema instances of AHIN andare further modeled by hypergraphs. A novel hypergraph contrastive learningmethod, named AEHCL, is proposed to fully capture abnormal event patterns.AEHCL designs the intra-event and inter-event contrastive modules to exploitself-supervised AHIN information. The intra-event contrastive module capturesthe pair-wise and multivariate interaction anomalies within an event, and theinter-event module captures the contextual anomalies among events. These twomodules collaboratively boost the performance of each other and improve thedetection results. During the testing phase, a contrastive learning-basedabnormal event score function is further proposed to measure the abnormalitydegree of events. Extensive experiments on three datasets in differentscenarios demonstrate the effectiveness of AEHCL, and the results improvestate-of-the-art baselines up to 12.0% in Average Precision (AP) and 4.6% inArea Under Curve (AUC) respectively.", "output": "Abnormal Event Detection via Hypergraph Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pre-trained language models for code (PLMCs) have gained attention in recentresearch. These models are pre-trained on large-scale datasets usingmulti-modal objectives. However, fine-tuning them requires extensivesupervision and is limited by the size of the dataset provided. We aim toimprove this issue by proposing a simple data augmentation framework. Ourframework utilizes knowledge gained during the pre-training and fine-tuningstage to generate pseudo data, which is then used as training data for the nextstep. We incorporate this framework into the state-of-the-art language models,such as CodeT5, CodeBERT, and UnixCoder. The results show that our frameworksignificantly improves PLMCs' performance in code-related sequence generationtasks, such as code summarization and code generation in the CodeXGLUEbenchmark.", "output": "Better Language Models of Code through Self-Improvement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper investigates the effectiveness of large language models (LLMs) inemail spam detection by comparing prominent models from three distinctfamilies: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, weexamine well-established machine learning techniques for spam detection, suchas Na\"ive Bayes and LightGBM, as baseline methods. We assess the performanceof these models across four public datasets, utilizing different numbers oftraining samples (full training set and few-shot settings). Our findings revealthat, in the majority of cases, LLMs surpass the performance of the popularbaseline techniques, particularly in few-shot scenarios. This adaptabilityrenders LLMs uniquely suited to spam detection tasks, where labeled samples arelimited in number and models require frequent updates. Additionally, weintroduce Spam-T5, a Flan-T5 model that has been specifically adapted andfine-tuned for the purpose of detecting email spam. Our results demonstratethat Spam-T5 surpasses baseline models and other LLMs in the majority ofscenarios, particularly when there are a limited number of training samplesavailable. Our code is publicly available at", "output": "Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clinical evidence encompasses the associations and impacts between patients,interventions (such as drugs or physiotherapy), problems, and outcomes. Thegoal of recommending clinical evidence is to provide medical practitioners withrelevant information to support their decision-making processes and to generatenew evidence. Our specific task focuses on recommending evidence based onclinical problems. However, the direct connections between certain clinicalproblems and related evidence are often sparse, creating a challenge of linksparsity. Additionally, to recommend appropriate evidence, it is essential tojointly exploit both topological relationships among evidence and textualinformation describing them. To address these challenges, we define twoknowledge graphs: an Evidence Co-reference Graph and an Evidence Text Graph, torepresent the topological and linguistic relations among evidential elements,respectively. We also introduce a multi-channel heterogeneous learning modeland a fusional attention mechanism to handle the co-reference-textheterogeneity in evidence recommendation. Our experiments demonstrate that ourmodel outperforms state-of-the-art methods on open data.", "output": "Enhancing Clinical Evidence Recommendation with Multi-Channel Heterogeneous Learning on Evidence Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous cyber agents may be developed by applying reinforcement and deepreinforcement learning (RL/DRL), where agents are trained in a representativeenvironment. The training environment must simulate with high-fidelity thenetwork Cyber Operations (CyOp) that the agent aims to explore. Given thecomplexity of net-work CyOps, a good simulator is difficult to achieve. Thiswork presents a systematic solution to automatically generate a high-fidelitysimulator in the Cyber Gym for Intelligent Learning (CyGIL). Throughrepresentation learning and continuous learning, CyGIL provides a unified CyOptraining environment where an emulated CyGIL-E automatically generates asimulated CyGIL-S. The simulator generation is integrated with the agenttraining process to further reduce the required agent training time. The agenttrained in CyGIL-S is transferrable directly to CyGIL-E showing fulltransferability to the emulated \"real\" network. Experimental results arepresented to demonstrate the CyGIL training performance. Enabling offline RL,the CyGIL solution presents a promising direction towards sim-to-real forleveraging RL agents in real-world cyber networks.", "output": "Unified Emulation-Simulation Training Environment for Autonomous Cyber Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AIheatwave due to its human-like conversations with detailed and articulateanswers across many domains of knowledge. While LLMs are being quickly appliedto many AI application domains, we are interested in the following question:Can safety analysis for safety-critical systems make use of LLMs? To answer, weconduct a case study of Systems Theoretic Process Analysis (STPA) on AutomaticEmergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalenttechniques for hazard analysis, is known to have limitations such as highcomplexity and subjectivity, which this paper aims to explore the use ofChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPAare investigated by considering its interaction with human experts: one-offsimplex interaction, recurring simplex interaction, and recurring duplexinteraction. Comparative results reveal that: (i) using ChatGPT without humanexperts' intervention can be inadequate due to reliability and accuracy issuesof LLMs; (ii) more interactions between ChatGPT and human experts may yieldbetter results; and (iii) using ChatGPT in STPA with extra care can outperformhuman safety experts alone, as demonstrated by reusing an existing comparisonmethod with baselines. In addition to making the first attempt to apply LLMs insafety analysis, this paper also identifies key challenges (e.g.,trustworthiness concern of LLMs, the need of standardisation) for futureresearch in this direction.", "output": "Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a logical system that combines the well-known classical epistemicconcepts of belief and knowledge with a concept of evidence such that theintuitive principle textit{`evidence yields belief and knowledge'} issatisfied. Our approach relies on previous works of the first authorcite{lewjlc2, lewigpl, lewapal} who introduced a modal system containing$S5$-style principles for the reasoning about intutionistic truth (i.e.textit{proof}) and, inspired by cite{artpro}, combined that system withconcepts of textit{intuitionistic} belief and knowledge. We consider thatcombined system and replace the constructive concept of textit{proof} with aclassical notion of textit{evidence}. This results in a logic that combinesmodal system $S5$ with classical epistemic principles where $squarevarphi$reads as `$varphi$ is evident' in an epistemic sense. Inspired bycite{lewapal}, and in contrast to the usual possible worlds semantics found inthe literature, we propose here a relational, frame-based semantics wherebelief and knowledge are not modeled via accessibility relations but directlyas sets of propositions (sets of sets of worlds).", "output": "Belief, knowledge and evidence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-lingual transfer of language models trained on high-resource languageslike English has been widely studied for many NLP tasks, but focus onconversational tasks has been rather limited. This is partly due to the highcost of obtaining non-English conversational data, which results in limitedcoverage. In this work, we introduce XSGD, a parallel and large-scalemultilingual conversation dataset that we created by translating theEnglish-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into105 other languages. XSGD contains approximately 330k utterances per language.To facilitate aligned cross-lingual representations, we develop an efficientprompt-tuning-based method for learning alignment prompts. We also investigatetwo different classifiers: NLI-based and vanilla classifiers, and testcross-lingual capability enabled by the aligned prompts. We evaluate ourmodel's cross-lingual generalization capabilities on two conversation tasks:slot-filling and intent classification. Our results demonstrate the strong andefficient modeling ability of NLI-based classifiers and the large cross-lingualtransfer improvements achieved by our aligned prompts, particularly in few-shotsettings.", "output": "Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores the use of affine hulls of points as a means ofrepresenting data via learning in Reproducing Kernel Hilbert Spaces (RKHS),with the goal of partitioning the data space into geometric bodies that concealprivacy-sensitive information about individual data points, while preservingthe structure of the original learning problem. To this end, we introduce theKernel Affine Hull Machine (KAHM), which provides an effective way of computinga distance measure from the resulting bounded geometric body. KAHM is acritical building block in wide and deep autoencoders, which enable datarepresentation learning for classification applications. To ensureprivacy-preserving learning, we propose a novel method for generatingfabricated data, which involves smoothing differentially private data samplesthrough a transformation process. The resulting fabricated data guarantees notonly differential privacy but also ensures that the KAHM modeling error is notlarger than that of the original training data samples. We also address theaccuracy-loss issue that arises with differentially private classifiers byusing fabricated data. This approach results in a significant reduction in therisk of membership inference attacks while incurring only a marginal loss ofaccuracy. As an application, a KAHM based differentially private federatedlearning scheme is introduced featuring that the evaluation of globalclassifier requires only locally computed distance measures. Overall, ourfindings demonstrate the potential of KAHM as effective tool forprivacy-preserving learning and classification.", "output": "Kernel Affine Hull Machines for Differentially Private Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Empirical design in reinforcement learning is no small task. Running goodexperiments requires attention to detail and at times significant computationalresources. While compute resources available per dollar have continued to growrapidly, so have the scale of typical experiments in reinforcement learning. Itis now common to benchmark agents with millions of parameters against dozens oftasks, each using the equivalent of 30 days of experience. The scale of theseexperiments often conflict with the need for proper statistical evidence,especially when comparing algorithms. Recent studies have highlighted howpopular algorithms are sensitive to hyper-parameter settings and implementationdetails, and that common empirical practice leads to weak statistical evidence(Machado et al., 2018; Henderson et al., 2018). Here we take this one stepfurther.This manuscript represents both a call to action, and a comprehensiveresource for how to do good experiments in reinforcement learning. Inparticular, we cover: the statistical assumptions underlying common performancemeasures, how to properly characterize performance variation and stability,hypothesis testing, special considerations for comparing multiple agents,baseline and illustrative example construction, and how to deal withhyper-parameters and experimenter bias. Throughout we highlight common mistakesfound in the literature and the statistical consequences of those in exampleexperiments. The objective of this document is to provide answers on how we canuse our unprecedented compute to do good science in reinforcement learning, aswell as stay alert to potential pitfalls in our empirical design.", "output": "Empirical Design in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Document similarity is an important part of Natural Language Processing andis most commonly used for plagiarism-detection and text summarization. Thus,finding the overall most effective document similarity algorithm could have amajor positive impact on the field of Natural Language Processing. This reportsets out to examine the numerous document similarity algorithms, and determinewhich ones are the most useful. It addresses the most effective documentsimilarity algorithm by categorizing them into 3 types of document similarityalgorithms: statistical algorithms, neural networks, and corpus/knowledge-basedalgorithms. The most effective algorithms in each category are also compared inour work using a series of benchmark datasets and evaluations that test everypossible area that each algorithm could be used in.", "output": "A Comparison of Document Similarity Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electroencephalogram (EEG) is a very promising and widely implementedprocedure to study brain signals and activities by amplifying and measuring thepost-synaptical potential arising from electrical impulses produced by neuronsand detected by specialized electrodes attached to specific points in thescalp. It can be studied for detecting brain abnormalities, headaches, andother conditions. However, there are limited studies performed to establish asmart decision-making model to identify EEG's relation with the mood of thesubject. In this experiment, EEG signals of 28 healthy human subjects have beenobserved with consent and attempts have been made to study and recognise moods.Savitzky-Golay band-pass filtering and Independent Component Analysis have beenused for data filtration.Different neural network algorithms have beenimplemented to analyze and classify the EEG data based on the mood of thesubject. The model is further optimised by the usage of Blackman window-basedFourier Transformation and extracting the most significant frequencies for eachelectrode. Using these techniques, up to 96.01% detection accuracy has beenobtained.", "output": "Optimized EEG based mood detection with signal processing and deep neural networks for brain-computer interface."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Objective. Research on brain-computer interfaces (BCIs) is advancing towardsrehabilitating severely disabled patients in the real world. Two key factorsfor successful decoding of user intentions are the size of implantedmicroelectrode arrays and a good online spike sorting algorithm. A small butdense microelectrode array with 3072 channels was recently developed fordecoding user intentions. The process of spike sorting determines the spikeactivity (SA) of different sources (neurons) from recorded neural data.Unfortunately, current spike sorting algorithms are unable to handle themassively increasing amount of data from dense microelectrode arrays, makingspike sorting a fragile component of the online BCI decoding framework.Approach. We proposed an adaptive and self-organized algorithm for online spikesorting, named Adaptive SpikeDeep-Classifier (Ada-SpikeDeepClassifier), whichuses SpikeDeeptector for channel selection, an adaptive background activityrejector (Ada-BAR) for discarding background events, and an adaptive spikeclassifier (Ada-Spike classifier) for classifying the SA of different neuralunits. Results. Our algorithm outperformed our previously publishedSpikeDeep-Classifier and eight other spike sorting algorithms, as evaluated ona human dataset and a publicly available simulated dataset. Significance. Theproposed algorithm is the first spike sorting algorithm that automaticallylearns the abrupt changes in the distribution of noise and SA. It is anartificial neural network-based algorithm that is well-suited for hardwareimplementation on neuromorphic chips that can be used for wearable invasiveBCIs.", "output": "Adaptive SpikeDeep-Classifier: Self-organizing and self-supervised machine learning algorithm for online spike sorting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work aims to enable autonomous agents for network cyber operations(CyOps) by applying reinforcement and deep reinforcement learning (RL/DRL). Therequired RL training environment is particularly challenging, as it mustbalance the need for high-fidelity, best achieved through real networkemulation, with the need for running large numbers of training episodes, bestachieved using simulation. A unified training environment, namely the Cyber Gymfor Intelligent Learning (CyGIL) is developed where an emulated CyGIL-Eautomatically generates a simulated CyGIL-S. From preliminary experimentalresults, CyGIL-S is capable to train agents in minutes compared with the daysrequired in CyGIL-E. The agents trained in CyGIL-S are transferrable directlyto CyGIL-E showing full decision proficiency in the emulated \"real\" network.Enabling offline RL, the CyGIL solution presents a promising direction towardssim-to-real for leveraging RL agents in real-world cyber networks.", "output": "Enabling A Network AI Gym for Autonomous Cyber Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative AI models, including large language models and multimodal modelsthat include text and other media, are on the cusp of transforming many aspectsof modern life, including entertainment, education, civic life, the arts, and arange of professions. There is potential for Generative AI to have asubstantive impact on the methods and pace of discovery for a range ofscientific disciplines. We interviewed twenty scientists from a range of fields(including the physical, life, and social sciences) to gain insight intowhether or how Generative AI technologies might add value to the practice oftheir respective disciplines, including not only ways in which AI mightaccelerate scientific discovery (i.e., research), but also other aspects oftheir profession, including the education of future scholars and thecommunication of scientific findings. In addition to identifying opportunitiesfor Generative AI to augment scientists' current practices, we also askedparticipants to reflect on concerns about AI. These findings can help guide theresponsible development of models and interfaces for scientific education,inquiry, and communication.", "output": "Scientists' Perspectives on the Potential for Generative AI in their Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The fast-growing demand for fully autonomous aerial operations in sharedspaces necessitates developing trustworthy agents that can safely andseamlessly navigate in crowded, dynamic spaces. In this work, we propose SocialRobot Tree Search (SoRTS), an algorithm for the safe navigation of mobilerobots in social domains. SoRTS aims to augment existing socially-awaretrajectory prediction policies with a Monte Carlo Tree Search planner forimproved downstream navigation of mobile robots. To evaluate the performance ofour method, we choose the use case of social navigation for general aviation.To aid this evaluation, within this work, we also introduce X-PlaneROS, ahigh-fidelity aerial simulator, to enable more research in full-scale aerialautonomy. By conducting a user study based on the assessments of 26 FAAcertified pilots, we show that SoRTS performs comparably to a competent humanpilot, significantly outperforming our baseline algorithm. We furthercomplement these results with self-play experiments in scenarios withincreasing complexity.", "output": "Learned Tree Search for Long-Horizon Social Robot Navigation in Shared Airspace."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a method to segment the visual field into independently movingregions, trained with no ground truth or supervision. It consists of anadversarial conditional encoder-decoder architecture based on Slot Attention,modified to use the image as context to decode optical flow without attemptingto reconstruct the image itself. In the resulting multi-modal representation,one modality (flow) feeds the encoder to produce separate latent codes (slots),whereas the other modality (image) conditions the decoder to generate the first(flow) from the slots. This design frees the representation from having toencode complex nuisance variability in the image due to, for instance,illumination and reflectance properties of the scene. Since customaryautoencoding based on minimizing the reconstruction error does not preclude theentire flow from being encoded into a single slot, we modify the loss to anadversarial criterion based on Contextual Information Separation. The resultingmin-max optimization fosters the separation of objects and their assignment todifferent attention slots, leading to Divided Attention, or DivA. DivAoutperforms recent unsupervised multi-object motion segmentation methods whiletripling run-time speed up to 104FPS and reducing the performance gap fromsupervised methods to 12% or less. DivA can handle different numbers of objectsand different image sizes at training and test time, is invariant topermutation of object labels, and does not require explicit regularization.", "output": "Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Frank-Wolfe algorithm is a popular method in structurally constrainedmachine learning applications, due to its fast per-iteration complexity.However, one major limitation of the method is a slow rate of convergence thatis difficult to accelerate due to erratic, zig-zagging step directions, evenasymptotically close to the solution. We view this as an artifact ofdiscretization; that is to say, the Frank-Wolfe emph{flow}, which is itstrajectory at asymptotically small step sizes, does not zig-zag, and reducingdiscretization error will go hand-in-hand in producing a more stabilizedmethod, with better convergence properties. We propose two improvements: amultistep Frank-Wolfe method that directly applies optimized higher-orderdiscretization schemes; and an LMO-averaging scheme with reduced discretizationerror, and whose local convergence rate over general convex sets acceleratesfrom a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.", "output": "Reducing Discretization Error in the Frank-Wolfe Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In response to innovations in machine learning (ML) models, productionworkloads changed radically and rapidly. TPU v4 is the fifth Google domainspecific architecture (DSA) and its third supercomputer for such ML models.Optical circuit switches (OCSes) dynamically reconfigure its interconnecttopology to improve scale, availability, utilization, modularity, deployment,security, power, and performance; users can pick a twisted 3D torus topology ifdesired. Much cheaper, lower power, and faster than Infiniband, OCSes andunderlying optical components are &lt;5% of system cost and &lt;3% of system power.Each TPU v4 includes SparseCores, dataflow processors that accelerate modelsthat rely on embeddings by 5x-7x yet use only 5% of die area and power.Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improvesperformance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chipsand thus ~10x faster overall, which along with OCS flexibility helps largelanguage models. For similar sized systems, it is ~4.3x-4.5x faster than theGraphcore IPU Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power thanthe Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computersof Google Cloud use ~3x less energy and produce ~20x less CO2e thancontemporary DSAs in a typical on-premise data center.", "output": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since the introduction of deep learning, a wide scope of representationproperties, such as decorrelation, whitening, disentanglement, rank, isotropy,and mutual information, have been studied to improve the quality ofrepresentation. However, manipulating such properties can be challenging interms of implementational effectiveness and general applicability. To addressthese limitations, we propose to regularize von Neumann entropy~(VNE) ofrepresentation. First, we demonstrate that the mathematical formulation of VNEis superior in effectively manipulating the eigenvalues of the representationautocorrelation matrix. Then, we demonstrate that it is widely applicable inimproving state-of-the-art algorithms or popular benchmark algorithms byinvestigating domain-generalization, meta-learning, self-supervised learning,and generative models. In addition, we formally establish theoreticalconnections with rank, disentanglement, and isotropy of representation.Finally, we provide discussions on the dimension control of VNE and therelationship with Shannon entropy. Code is available at:", "output": "VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning anticipation in Multi-Agent Reinforcement Learning (MARL) is areasoning paradigm where agents anticipate the learning steps of other agentsto improve cooperation among themselves. As MARL uses gradient-basedoptimization, learning anticipation requires using Higher-Order Gradients(HOG), with so-called HOG methods. Existing HOG methods are based on policyparameter anticipation, i.e., agents anticipate the changes in policyparameters of other agents. Currently, however, these existing HOG methods haveonly been applied to differentiable games or games with small state spaces. Inthis work, we demonstrate that in the case of non-differentiable games withlarge state spaces, existing HOG methods do not perform well and areinefficient due to their inherent limitations related to policy parameteranticipation and multiple sampling stages. To overcome these problems, wepropose Off-Policy Action Anticipation (OffPA2), a novel framework thatapproaches learning anticipation through action anticipation, i.e., agentsanticipate the changes in actions of other agents, via off-policy sampling. Wetheoretically analyze our proposed OffPA2 and employ it to develop multiple HOGmethods that are applicable to non-differentiable games with large statespaces. We conduct a large set of experiments and illustrate that our proposedHOG methods outperform the existing ones regarding efficiency and performance.", "output": "Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-Language models (VLMs) that use contrastive language-imagepre-training have shown promising zero-shot classification performance.However, their performance on imbalanced dataset is relatively poor, where thedistribution of classes in the training dataset is skewed, leading to poorperformance in predicting minority classes. For instance, CLIP achieved only 5%accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoderto VLMs to avoid OOM (out of memory) problem caused by large number of classesand capture nuanced features for tail classes. Then, we explore improvements ofVLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithmssuch as Focal Loss, Balanced SoftMax and Distribution Alignment. Experimentsdemonstrate that the performance of VLMs can be further boosted when used withdecoder and imbalanced methods. Specifically, our improved VLMs significantlyoutperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,and 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. Wefurther analyze the influence of pre-training data size, backbones, andtraining cost. Our study highlights the significance of imbalanced learningalgorithms in face of VLMs pre-trained by huge data. We release our code at", "output": "Exploring Vision-Language Models for Imbalanced Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Low-channel EEG devices are crucial for portable and entertainmentapplications. However, the low spatial resolution of EEG presents challenges indecoding low-channel motor imagery. This study introduces TSFF-Net, a novelnetwork architecture that integrates time-space-frequency features, effectivelycompensating for the limitations of single-mode feature extraction networksbased on time-series or time-frequency modalities. TSFF-Net comprises four maincomponents: time-frequency representation, time-frequency feature extraction,time-space feature extraction, and feature fusion and classification.Time-frequency representation and feature extraction transform raw EEG signalsinto time-frequency spectrograms and extract relevant features. The time-spacenetwork processes time-series EEG trials as input and extracts temporal-spatialfeatures. Feature fusion employs MMD loss to constrain the distribution oftime-frequency and time-space features in the Reproducing Kernel Hilbert Space,subsequently combining these features using a weighted fusion approach toobtain effective time-space-frequency features. Moreover, few studies haveexplored the decoding of three-channel motor imagery based on time-frequencyspectrograms. This study proposes a shallow, lightweight decoding architecture(TSFF-img) based on time-frequency spectrograms and compares its classificationperformance in low-channel motor imagery with other methods using two publiclyavailable datasets. Experimental results demonstrate that TSFF-Net not onlycompensates for the shortcomings of single-mode feature extraction networks inEEG decoding, but also outperforms other state-of-the-art methods. Overall,TSFF-Net offers considerable advantages in decoding low-channel motor imageryand provides valuable insights for algorithmically enhancing low-channel EEGdecoding.", "output": "Time-space-frequency feature Fusion for 3-channel motor imagery classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The cloud is still a popular platform for distributed deep learning (DL)training jobs since resource sharing in the cloud can improve resourceutilization and reduce overall costs. However, such sharing also bringsmultiple challenges for DL training jobs, e.g., high-priority jobs couldimpact, even interrupt, low-priority jobs. Meanwhile, most existing distributedDL training systems require users to configure the resources (i.e., the numberof nodes and resources like CPU and memory allocated to each node) of jobsmanually before job submission and can not adjust the job's resources duringthe runtime. The resource configuration of a job deeply affect this job'sperformance (e.g., training throughput, resource utilization, and completionrate). However, this usually leads to poor performance of jobs since users failto provide optimal resource configuration in most cases. system~is adistributed DL framework can auto-configure a DL job's initial resources anddynamically tune the job's resources to win the better performance. Withelastic capability, system~can effectively adjusts the resources of a job whenthere are performance issues detected or a job fails because of faults oreviction. Evaluations results show system~can outperform manual well-tunedresource configurations. Furthermore, in the production Kubernetes cluster ofcompany, system~reduces the medium of job completion time by 31%, andimproves the job completion rate by 6%, CPU utilization by 15%, and memoryutilization by 20% compared with manual configuration.", "output": "DLRover: An Elastic Deep Training Extension with Auto Job Resource Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT has become a global sensation. As ChatGPT and other Large LanguageModels (LLMs) emerge, concerns of misusing them in various ways increase, suchas disseminating fake news, plagiarism, manipulating public opinion, cheating,and fraud. Hence, distinguishing AI-generated from human-generated becomesincreasingly essential. Researchers have proposed various detectionmethodologies, ranging from basic binary classifiers to more complexdeep-learning models. Some detection techniques rely on statisticalcharacteristics or syntactic patterns, while others incorporate semantic orcontextual information to improve accuracy. The primary objective of this studyis to provide a comprehensive and contemporary assessment of the most recenttechniques in ChatGPT detection. Additionally, we evaluated other AI-generatedtext detection tools that do not specifically claim to detect ChatGPT-generatedcontent to assess their performance in detecting ChatGPT-generated content. Forour evaluation, we have curated a benchmark dataset consisting of prompts fromChatGPT and humans, including diverse questions from medical, open Q&amp;A, andfinance domains and user-generated responses from popular social networkingplatforms. The dataset serves as a reference to assess the performance ofvarious techniques in detecting ChatGPT-generated content. Our evaluationresults demonstrate that none of the existing methods can effectively detectChatGPT-generated content.", "output": "To ChatGPT, or not to ChatGPT: That is the question!."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Plastic Surgery In-Service Training Exam (PSITE) is an importantindicator of resident proficiency and serves as a useful benchmark forevaluating OpenAI's GPT. Unlike many of the simulated tests or practicequestions shown in the GPT-4 Technical Paper, the multiple-choice questionsevaluated here are authentic PSITE questions. These questions offer realisticclinical vignettes that a plastic surgeon commonly encounters in practice andscores highly correlate with passing the written boards required to become aBoard Certified Plastic Surgeon. Our evaluation shows dramatic improvement ofGPT-4 (without vision) over GPT-3.5 with both the 2022 and 2021 examsrespectively increasing the score from 8th to 88th percentile and 3rd to 99thpercentile. The final results of the 2023 PSITE are set to be released on April11, 2023, and this is an exciting moment to continue our research with a freshexam. Our evaluation pipeline is ready for the moment that the exam is releasedso long as we have access via OpenAI to the GPT-4 API. With multimodal input,we may achieve superhuman performance on the 2023.", "output": "GPT-4 to GPT-3.5: 'Hold My Scalpel' -- A Look at the Competency of OpenAI's GPT on the Plastic Surgery In-Service Training Exam."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Seasonal-trend decomposition is one of the most fundamental concepts in timeseries analysis that supports various downstream tasks, including time seriesanomaly detection and forecasting. However, existing decomposition methods relyon batch processing with a time complexity of O(W), where W is the number ofdata points within a time window. Therefore, they cannot always efficientlysupport real-time analysis that demands low processing delay. To address thischallenge, we propose OneShotSTL, an efficient and accurate algorithm that candecompose time series online with an update time complexity of O(1). OneShotSTLis more than $1,000$ times faster than the batch methods, with accuracycomparable to the best counterparts. Extensive experiments on real-worldbenchmark datasets for downstream time series anomaly detection and forecastingtasks demonstrate that OneShotSTL is from 10 to over 1,000 times faster thanthe state-of-the-art methods, while still providing comparable or even betteraccuracy.", "output": "OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series Anomaly Detection And Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked graph autoencoder (MGAE) has emerged as a promising self-supervisedgraph pre-training (SGP) paradigm due to its simplicity and effectiveness.However, existing efforts perform the mask-then-reconstruct operation in theraw data space as is done in computer vision (CV) and natural languageprocessing (NLP) areas, while neglecting the important non-Euclidean propertyof graph data. As a result, the highly unstable local connection structureslargely increase the uncertainty in inferring masked data and decrease thereliability of the exploited self-supervision signals, leading to inferiorrepresentations for downstream evaluations. To address this issue, we propose anovel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve thecertainty in inferring masked data and the reliability of the self-supervisionmechanism by further masking and reconstructing node samples in the high-orderlatent feature space. Through both theoretical and empirical analyses, we havediscovered that performing a joint mask-then-reconstruct strategy in bothlatent feature and raw data spaces could yield improved stability andperformance. To this end, we elaborately design a masked latent featurecompletion scheme, which predicts latent features of masked nodes under theguidance of high-order sample correlations that are hard to be observed fromthe raw data perspective. Specifically, we first adopt a latent featurepredictor to predict the masked latent features from the visible ones. Next, weencode the raw data of masked samples with a momentum graph encoder andsubsequently employ the resulting representations to improve predicted resultsthrough latent feature matching. Extensive experiments on seventeen datasetshave demonstrated the effectiveness and robustness of RARE againststate-of-the-art (SOTA) competitors across three downstream tasks.", "output": "RARE: Robust Masked Graph Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Skin lesion recognition using deep learning has made remarkable progress, andthere is an increasing need for deploying these systems in real-worldscenarios. However, recent research has revealed that deep neural networks forskin lesion recognition may overly depend on disease-irrelevant image artifacts(i.e. dark corners, dense hairs), leading to poor generalization in unseenenvironments. To address this issue, we propose a novel domain generalizationmethod called EPVT, which involves embedding prompts into the visiontransformer to collaboratively learn knowledge from diverse domains.Concretely, EPVT leverages a set of domain prompts, each of which plays as adomain expert, to capture domain-specific knowledge; and a shared prompt forgeneral knowledge over the entire dataset. To facilitate knowledge sharing andthe interaction of different prompts, we introduce a domain prompt generatorthat enables low-rank multiplicative updates between domain prompts and theshared prompt. A domain mixup strategy is additionally devised to reduce theco-occurring artifacts in each domain, which allows for more flexible decisionmargins and mitigates the issue of incorrectly assigned domain labels.Experiments on four out-of-distribution datasets and six different biased ISICdatasets demonstrate the superior generalization ability of EPVT in skin lesionrecognition across various environments. Our code and dataset will be releasedat ", "output": "EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning (ML) based time series forecasting models often require andassume certain degrees of stationarity in the data when producing forecasts.However, in many real-world situations, the data distributions are notstationary and they can change over time while reducing the accuracy of theforecasting models, which in the ML literature is known as concept drift.Handling concept drift in forecasting is essential for many ML methods in usenowadays, however, the prior work only proposes methods to handle concept driftin the classification domain. To fill this gap, we explore concept drifthandling methods in particular for Global Forecasting Models (GFM) whichrecently have gained popularity in the forecasting domain. We propose two newconcept drift handling methods, namely: Error Contribution Weighting (ECW) andGradient Descent Weighting (GDW), based on a continuous adaptive weightingconcept. These methods use two forecasting models which are separately trainedwith the most recent series and all series, and finally, the weighted averageof the forecasts provided by the two models are considered as the finalforecasts. Using LightGBM as the underlying base learner, in our evaluation onthree simulated datasets, the proposed models achieve significantly higheraccuracy than a set of statistical benchmarks and LightGBM baselines acrossfour evaluation metrics.", "output": "Handling Concept Drift in Global Time Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural processes (NPs) have brought the representation power of parametricdeep neural networks and the reliable uncertainty estimation of non-parametricGaussian processes together. Although recent development of NPs has shownsuccess in both regression and classification, how to adapt NPs to multimodaldata has not be carefully studied. For the first time, we propose a new modelof NP family for multimodal uncertainty estimation, namely Multimodal NeuralProcesses. In a holistic and principled way, we develop a dynamic contextmemory updated by the classification error, a multimodal Bayesian aggregationmechanism to aggregate multimodal representations, and a new attentionmechanism for calibrated predictions. In extensive empirical evaluation, ourmethod achieves the state-of-the-art multimodal uncertainty estimationperformance, showing its appealing ability of being robust against noisysamples and reliable in out-of-domain detection.", "output": "Multimodal Neural Processes for Uncertainty Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel definition of an algorithm and its corresponding algorithmlanguage called CoLweb. The merit of CoLweb [1] is that it makes algorithmdesign so versatile. That is, it forces us to a high-level, proof-carrying,distributed-style approach to algorithm design for both non-distributedcomputing and distributed one. We argue that this approach simplifies algorithmdesign. In addition, it unifies other approaches including recursivelogical/functional algorithms, imperative algorithms, object-orientedimperative algorithms, neural-nets, interaction nets, proof-carrying code, etc.As an application, we refine Horn clause definitions into two kinds:blind-univerally-quantified (BUQ) ones and parallel-universally-quantified(PUQ) ones. BUQ definitions corresponds to the traditional ones such as thosein Prolog where knowledgebase is $not$ expanding and its proof procedure isbased on the backward chaining. On the other hand, in PUQ definitions,knowledgebase is $expanding$ and its proof procedure leads to forward chainingand {it automatic memoization}.", "output": "Implementing Dynamic Programming in Computability Logic Web."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "XAI refers to the techniques and methods for building AI applications whichassist end users to interpret output and predictions of AI models. Black box AIapplications in high-stakes decision-making situations, such as medical domainhave increased the demand for transparency and explainability since wrongpredictions may have severe consequences. Model explainability andinterpretability are vital successful deployment of AI models in healthcarepractices. AI applications' underlying reasoning needs to be transparent toclinicians in order to gain their trust. This paper presents a systematicreview of XAI aspects and challenges in the healthcare domain. The primarygoals of this study are to review various XAI methods, their challenges, andrelated machine learning models in healthcare. The methods are discussed undersix categories: Features-oriented methods, global methods, concept models,surrogate models, local pixel-based methods, and human-centric methods. Mostimportantly, the paper explores XAI role in healthcare problems to clarify itsnecessity in safety-critical applications. The paper intends to establish acomprehensive understanding of XAI-related applications in the healthcare fieldby reviewing the related experimental results. To facilitate future researchfor filling research gaps, the importance of XAI models from differentviewpoints and their limitations are investigated.", "output": "A Brief Review of Explainable Artificial Intelligence in Healthcare."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work studies non-cooperative Multi-Agent Reinforcement Learning (MARL)where multiple agents interact in the same environment and whose goal is tomaximize the individual returns. Challenges arise when scaling up the number ofagents due to the resultant non-stationarity that the many agents introduce. Inorder to address this issue, Mean Field Games (MFG) rely on the symmetry andhomogeneity assumptions to approximate games with very large populations.Recently, deep Reinforcement Learning has been used to scale MFG to games withlarger number of states. Current methods rely on smoothing techniques such asaveraging the q-values or the updates on the mean-field distribution. This workpresents a different approach to stabilize the learning based on proximalupdates on the mean-field policy. We name our algorithm textit{Mean FieldProximal Policy Optimization (MF-PPO)}, and we empirically show theeffectiveness of our method in the OpenSpiel framework.", "output": "Regularization of the policy updates for stabilizing Mean Field Games."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model-agnostic meta-learning (MAML) is one of the most successfulmeta-learning algorithms. It has a bi-level optimization structure where theouter-loop process learns a shared initialization and the inner-loop processoptimizes task-specific weights. Although MAML relies on the standard gradientdescent in the inner-loop, recent studies have shown that controlling theinner-loop's gradient descent with a meta-learned preconditioner can bebeneficial. Existing preconditioners, however, cannot simultaneously adapt in atask-specific and path-dependent way. Additionally, they do not satisfy theRiemannian metric condition, which can enable the steepest descent learningwith preconditioned gradient. In this study, we propose Geometry-AdaptivePreconditioned gradient descent (GAP) that can overcome the limitations inMAML; GAP can efficiently meta-learn a preconditioner that is dependent ontask-specific parameters, and its preconditioner can be shown to be aRiemannian metric. Thanks to the two properties, the geometry-adaptivepreconditioner is effective for improving the inner-loop optimization.Experiment results show that GAP outperforms the state-of-the-art MAML familyand preconditioned gradient descent-MAML (PGD-MAML) family in a variety offew-shot learning tasks. Code is available at:", "output": "Meta-Learning with a Geometry-Adaptive Preconditioner."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised domain adaptation (UDA) addresses the problem of distributionshift between the unlabeled target domain and labelled source domain. While thesingle target domain adaptation (STDA) is well studied in both 2D and 3D visionliterature, multi-target domain adaptation (MTDA) is barely explored for 3Ddata despite its wide real-world applications such as autonomous drivingsystems for various geographical and climatic conditions. We establish an MTDAbaseline for 3D point cloud data by proposing to mix the featurerepresentations from all domains together to achieve better domain adaptationperformance by an ensemble average, which we call emph{{bf M}ixup {bfEns}emble {bf A}verage} or {bf emph{MEnsA}}. With the mixed representation,we use a domain classifier to improve at distinguishing the featurerepresentations of source domain from those of target domains in a sharedlatent space. In extensive empirical validations on the challenging PointDA-10dataset, we showcase a clear benefit of our simple method over previousunsupervised STDA and MTDA methods by large margins (up to $17.10%$ and$4.76%$ on averaged over all domain shifts). We make the code publiclyavailablehref{", "output": "\\emph{MEnsA}: Mix-up Ensemble Average for Unsupervised Multi Target Domain Adaptation on 3D Point Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As road accident cases are increasing due to the inattention of the driver,automated driver monitoring systems (DMS) have gained an increase inacceptance. In this report, we present a real-time DMS system that runs on ahardware-accelerator-based edge device. The system consists of an InfraRedcamera to record the driver footage and an edge device to process the data. Tosuccessfully port the deep learning models to run on the edge device takingfull advantage of the hardware accelerators, model surgery was performed. Thefinal DMS system achieves 63 frames per second (FPS) on the TI-TDA4VM edgedevice.", "output": "Real-time Driver Monitoring Systems on Edge AI Device."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-based delivery addresses, as the data foundation for logistics systems,contain abundant and crucial location information. How to effectively encodethe delivery address is a core task to boost the performance of downstreamtasks in the logistics system. Pre-trained Models (PTMs) designed for NaturalLanguage Process (NLP) have emerged as the dominant tools for encoding semanticinformation in text. Though promising, those NLP-based PTMs fall short ofencoding geographic knowledge in the delivery address, which considerably trimsdown the performance of delivery-related tasks in logistic systems such asCainiao. To tackle the above problem, we propose a domain-specific pre-trainedmodel, named G2PTL, a Geography-Graph Pre-trained model for delivery address inLogistics field. G2PTL combines the semantic learning capabilities of textpre-training with the geographical-relationship encoding abilities of graphmodeling. Specifically, we first utilize real-world logistics delivery data toconstruct a large-scale heterogeneous graph of delivery addresses, whichcontains abundant geographic knowledge and delivery information. Then, G2PTL ispre-trained with subgraphs sampled from the heterogeneous graph. Comprehensiveexperiments are conducted to demonstrate the effectiveness of G2PTL throughfour downstream tasks in logistics systems on real-world datasets. G2PTL hasbeen deployed in production in Cainiao's logistics system, which significantlyimproves the performance of delivery-related tasks.", "output": "G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Urban anomaly predictions, such as traffic accident prediction and crimeprediction, are of vital importance to smart city security and maintenance.Existing methods typically use deep learning to capture the intra-dependenciesin spatial and temporal dimensions. However, numerous key challenges remainunsolved, for instance, sparse zero-inflated data due to urban anomaliesoccurring with low frequency (which can lead to poor performance on real-worlddatasets), and both intra- and inter-dependencies of abnormal patterns acrossspatial, temporal, and semantic dimensions. Moreover, a unified approach topredict multiple kinds of anomaly is left to explore. In this paper, we proposeSTS to jointly capture the intra- and inter-dependencies between the patternsand the influential factors in three dimensions. Further, we use a multi-taskprediction module with a customized loss function to solve the zero-inflatedissue. To verify the effectiveness of the model, we apply it to two urbananomaly prediction tasks, crime prediction and traffic accident riskprediction, respectively. Experiments on two application scenarios with fourreal-world datasets demonstrate the superiority of STS, which outperformsstate-of-the-art methods in the mean absolute error and the root mean squareerror by 37.88% and 18.10% on zero-inflated datasets, and, 60.32% and 37.28% onnon-zero datasets, respectively.", "output": "Spatiotemporal and Semantic Zero-inflated Urban Anomaly Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cyber-physical systems (CPS) like autonomous vehicles, that utilize learningcomponents, are often sensitive to noise and out-of-distribution (OOD)instances encountered during runtime. As such, safety critical tasks dependupon OOD detection subsystems in order to restore the CPS to a known state orinterrupt execution to prevent safety from being compromised. However, it isdifficult to guarantee the performance of OOD detectors as it is difficult tocharacterize the OOD aspect of an instance, especially in high-dimensionalunstructured data.To distinguish between OOD data and data known to the learning componentthrough the training process, an emerging technique is to incorporatevariational autoencoders (VAE) within systems and apply classification oranomaly detection techniques on their latent spaces. The rationale for doing sois the reduction of the data domain size through the encoding process, whichbenefits real-time systems through decreased processing requirements,facilitates feature analysis for unstructured data and allows more explainabletechniques to be implemented.This study places probably approximately correct (PAC) based guarantees onOOD detection using the encoding process within VAEs to quantify image featuresand apply conformal constraints over them. This is used to bound the detectionerror on unfamiliar instances with user-defined confidence. The approach usedin this study is to empirically establish these bounds by sampling the latentprobability distribution and evaluating the error with respect to theconstraint violations that are encountered. The guarantee is then verifiedusing data generated from CARLA, an open-source driving simulator.", "output": "PAC-Based Formal Verification for Out-of-Distribution Data Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a novel multi-modal framework for Scene Text VisualQuestion Answering (STVQA), which requires models to read scene text in imagesfor question answering. Apart from text or visual objects, which could existindependently, scene text naturally links text and visual modalities togetherby conveying linguistic semantics while being a visual object in an imagesimultaneously. Different to conventional STVQA models which take thelinguistic semantics and visual semantics in scene text as two separatefeatures, in this paper, we propose a paradigm of \"Locate Then Generate\" (LTG),which explicitly unifies this two semantics with the spatial bounding box as abridge connecting them. Specifically, at first, LTG locates the region in animage that may contain the answer words with an answer location module (ALM)consisting of a region proposal network and a language refinement network, bothof which can transform to each other with one-to-one mapping via the scene textbounding box. Next, given the answer words selected by ALM, LTG generates areadable answer sequence with an answer generation module (AGM) based on apre-trained language model. As a benefit of the explicit alignment of thevisual and linguistic semantics, even without any scene text based pre-trainingtasks, LTG can boost the absolute accuracy by +6.06% and +6.92% on the TextVQAdataset and the ST-VQA dataset respectively, compared with a non-pre-trainingbaseline. We further demonstrate that LTG effectively unifies visual and textmodalities through the spatial bounding box connection, which isunderappreciated in previous methods.", "output": "Locate Then Generate: Bridging Vision and Language with Bounding Box for Scene-Text VQA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical Visual Question Answering (VQA) systems play a supporting role tounderstand clinic-relevant information carried by medical images. The questionsto a medical image include two categories: close-end (such as Yes/No question)and open-end. To obtain answers, the majority of the existing medical VQAmethods relies on classification approaches, while a few works attempt to usegeneration approaches or a mixture of the two. The classification approachesare relatively simple but perform poorly on long open-end questions. To bridgethis gap, in this paper, we propose a new Transformer based framework formedical VQA (named as Q2ATransformer), which integrates the advantages of boththe classification and the generation approaches and provides a unifiedtreatment for the close-end and open-end questions. Specifically, we introducean additional Transformer decoder with a set of learnable candidate answerembeddings to query the existence of each answer class to a givenimage-question pair. Through the Transformer attention, the candidate answerembeddings interact with the fused features of the image-question pair to makethe decision. In this way, despite being a classification-based approach, ourmethod provides a mechanism to interact with the answer information forprediction like the generation-based approaches. On the other hand, byclassification, we mitigate the task difficulty by reducing the search space ofanswers. Our method achieves new state-of-the-art performance on two medicalVQA benchmarks. Especially, for the open-end questions, we achieve 79.19% onVQA-RAD and 54.85% on PathVQA, with 16.09% and 41.45% absolute improvements,respectively.", "output": "Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Because of the inevitable cost and complexity of transformer and pre-trainedmodels, efficiency concerns are raised for long text classification. Meanwhile,in the highly sensitive domains, e.g., healthcare and legal long-text mining,potential model distrust, yet underrated and underexplored, may hatch vitalapprehension. Existing methods generally segment the long text, encode eachpiece with the pre-trained model, and use attention or RNNs to obtain long textrepresentation for classification. In this work, we propose a simple buteffective model, Segment-aWare multIdimensional PErceptron (SWIPE), to replaceattention/RNNs in the above framework. Unlike prior efforts, SWIPE caneffectively learn the label of the entire text with supervised training, whileperceive the labels of the segments and estimate their contributions to thelong-text labeling in an unsupervised manner. As a general classifier, SWIPEcan endorse different encoders, and it outperforms SOTA models in terms ofclassification accuracy and model efficiency. It is noteworthy that SWIPEachieves superior interpretability to transparentize long text classificationresults.", "output": "Multidimensional Perceptron for Efficient and Explainable Long Text Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual question answering (VQA) is a critical multimodal task in which anagent must answer questions according to the visual cue. Unfortunately,language bias is a common problem in VQA, which refers to the model generatinganswers only by associating with the questions while ignoring the visualcontent, resulting in biased results. We tackle the language bias problem byproposing a self-supervised counterfactual metric learning (SC-ML) method tofocus the image features better. SC-ML can adaptively select thequestion-relevant visual features to answer the question, reducing the negativeinfluence of question-irrelevant visual features on inferring answers. Inaddition, question-irrelevant visual features can be seamlessly incorporatedinto counterfactual training schemes to further boost robustness. Extensiveexperiments have proved the effectiveness of our method with improved resultson the VQA-CP dataset. Our code will be made publicly available.", "output": "SC-ML: Self-supervised Counterfactual Metric Learning for Debiased Visual Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural captioners are typically trained to mimic human-generated referenceswithout optimizing for any specific communication goal, leading to problemssuch as the generation of vague captions. In this paper, we show thatfine-tuning an out-of-the-box neural captioner with a self-superviseddiscriminative communication objective helps to recover a plain, visuallydescriptive language that is more informative about image contents. Given atarget image, the system must learn to produce a description that enables anout-of-the-box text-conditioned image retriever to identify such image among aset of candidates. We experiment with the popular ClipCap captioner, alsoreplicating the main results with BLIP. In terms of similarity to ground-truthhuman descriptions, the captions emerging from discriminative finetuning lagslightly behind those generated by the non-finetuned model, when the latter istrained and tested on the same caption dataset. However, when the model is usedwithout further tuning to generate captions for out-of-domain datasets, ourdiscriminatively-finetuned captioner generates descriptions that resemble humanreferences more than those produced by the same captioner without finetuning.We further show that, on the Conceptual Captions dataset, discriminativelyfinetuned captions are more helpful than either vanilla ClipCap captions orground-truth captions for human annotators tasked with an image discriminationtask.", "output": "Cross-Domain Image Captioning with Discriminative Finetuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inconsistency handling is an important issue in knowledge management.Especially in ontology engineering, logical inconsistencies may occur duringontology construction. A natural way to reason with an inconsistent ontology isto utilize the maximal consistent subsets of the ontology. However, previousstudies on selecting maximum consistent subsets have rarely considered thesemantics of the axioms, which may result in irrational inference. In thispaper, we propose a novel approach to reasoning with inconsistent ontologies indescription logics based on the embeddings of axioms. We first give a methodfor turning axioms into distributed semantic vectors to compute the semanticconnections between the axioms. We then define an embedding-based method forselecting the maximum consistent subsets and use it to define aninconsistency-tolerant inference relation. We show the rationality of ourinference relation by considering some logical properties. Finally, we conductexperiments on several ontologies to evaluate the reasoning power of ourinference relation. The experimental results show that our embedding-basedmethod can outperform existing inconsistency-tolerant reasoning methods basedon maximal consistent subsets.", "output": "An Embedding-based Approach to Inconsistency-tolerant Reasoning with Inconsistent Ontologies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we follow a data-centric philosophy and propose a novel motionannotation method based on the inherent representativeness of motion data in agiven dataset. Specifically, we propose a Representation-basedRepresentativeness Ranking R3 method that ranks all motion data in a givendataset according to their representativeness in a learned motionrepresentation space. We further propose a novel dual-level motion constrastivelearning method to learn the motion representation space in a more informativeway. Thanks to its high efficiency, our method is particularly responsive tofrequent requirements change and enables agile development of motion annotationmodels. Experimental results on the HDM05 dataset against state-of-the-artmethods demonstrate the superiority of our method.", "output": "Motion-R3: Fast and Accurate Motion Annotation via Representation-based Representativeness Ranking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the challenging task of training models for image-to-videodeblurring, which aims to recover a sequence of sharp images corresponding to agiven blurry image input. A critical issue disturbing the training of animage-to-video model is the ambiguity of the frame ordering since both theforward and backward sequences are plausible solutions. This paper proposes aneffective self-supervised ordering scheme that allows training high-qualityimage-to-video deblurring models. Unlike previous methods that rely onorder-invariant losses, we assign an explicit order for each video sequence,thus avoiding the order-ambiguity issue. Specifically, we map each videosequence to a vector in a latent high-dimensional space so that there exists ahyperplane such that for every video sequence, the vectors extracted from itand its reversed sequence are on different sides of the hyperplane. The side ofthe vectors will be used to define the order of the corresponding sequence.Last but not least, we propose a real-image dataset for the image-to-videodeblurring problem that covers a variety of popular domains, including face,hand, and street. Extensive experimental results confirm the effectiveness ofour method. Code and data are available at", "output": "HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years people have become increasingly reliant on social media toread news and get information, and some social media users post unsubstantiatedinformation to gain attention. Such information is known as rumours. Nowadays,rumour detection is receiving a growing amount of attention because of thepandemic of the New Coronavirus, which has led to a large number of rumoursbeing spread. In this paper, a Natural Language Processing (NLP) system isbuilt to predict rumours. The best model is applied to the COVID-19 tweets toconduct exploratory data analysis. The contribution of this study is twofold:(1) to compare rumours and facts using state-of-the-art natural languageprocessing models in two dimensions: language structure and propagation route.(2) An analysis of how rumours differ from facts in terms of their lexical useand the emotions they imply. This study shows that linguistic structure is abetter feature to distinguish rumours from facts compared to the propagationpath. In addition, rumour tweets contain more vocabulary related to politicsand negative emotions.", "output": "Rumour Detection and Analysis on Twitter."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video Instance Segmentation(VIS) aims at segmenting and categorizing objectsin videos from a closed set of training categories, lacking the generalizationability to handle novel categories in real-world videos. To address thislimitation, we make the following three contributions. First, we introduce thenovel task of Open-Vocabulary Video Instance Segmentation, which aims tosimultaneously segment, track, and classify objects in videos from open-setcategories, including novel categories unseen during training. Second, tobenchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video InstanceSegmentation dataset(LV-VIS), that contains well-annotated objects from 1,212diverse categories, significantly surpassing the category size of existingdatasets by more than one order of magnitude. Third, we propose an efficientMemory-Induced Vision-Language Transformer, MindVLT, to first achieveOpen-Vocabulary VIS in an end-to-end manner with near real-time inferencespeed. Extensive experiments on LV-VIS and four existing VIS datasetsdemonstrate the strong zero-shot generalization ability of MindVLT on novelcategories. We will release the dataset and code to facilitate futureendeavors.", "output": "Towards Open-Vocabulary Video Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The deep neural networks (DNNs) have freed the synthetic aperture radarautomatic target recognition (SAR ATR) from expertise-based feature designingand demonstrated superiority over conventional solutions. There has been shownthe unique deficiency of ground vehicle benchmarks in shapes of strongbackground correlation results in DNNs overfitting the clutter and beingnon-robust to unfamiliar surroundings. However, the gap between fixedbackground model training and varying background application remainsunderexplored. Inspired by contrastive learning, this letter proposes asolution called Contrastive Feature Alignment (CFA) aiming to learn invariantrepresentation for robust recognition. The proposed method contributes a mixedclutter variants generation strategy and a new inference branch equipped withchannel-weighted mean square error (CWMSE) loss for invariant representationlearning. In specific, the generation strategy is delicately designed to betterattract clutter-sensitive deviation in feature space. The CWMSE loss is furtherdevised to better contrast this deviation and align the deep features activatedby the original images and corresponding clutter variants. The proposed CFAcombines both classification and CWMSE losses to train the model jointly, whichallows for the progressive learning of invariant target representation.Extensive evaluations on the MSTAR dataset and six DNN models prove theeffectiveness of our proposal. The results demonstrated that the CFA-trainedmodels are capable of recognizing targets among unfamiliar surroundings thatare not included in the dataset, and are robust to varying signal-to-clutterratios.", "output": "Learning Invariant Representation via Contrastive Feature Alignment for Clutter Robust SAR Target Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop a contrastive framework for learning better prior distributionsfor Bayesian Neural Networks (BNNs) using unlabelled data. With this framework,we propose a practical BNN algorithm that offers the label-efficiency ofself-supervised learning and the principled uncertainty estimates of Bayesianmethods. Finally, we demonstrate the advantages of our approach fordata-efficient learning in semi-supervised and low-budget active learningproblems.", "output": "Incorporating Unlabelled Data into Bayesian Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For a natural language problem that requires some non-trivial reasoning tosolve, there are at least two ways to do it using a large language model (LLM).One is to ask it to solve it directly. The other is to use it to extract thefacts from the problem text and then use a theorem prover to solve it. In thisnote, we compare the two methods using ChatGPT and GPT4 on a series of logicword puzzles, and conclude that the latter is the right approach.", "output": "Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As multimedia content often contains noise from intrinsic defects of digitaldevices, image denoising is an important step for high-level vision recognitiontasks. Although several studies have developed the denoising field employingadvanced Transformers, these networks are too momory-intensive for real-worldapplications. Additionally, there is a lack of research on lightweight denosing(LWDN) with Transformers. To handle this, this work provides seven comparativebaseline Transformers for LWDN, serving as a foundation for future research. Wealso demonstrate the parts of randomly cropped patches significantly affect thedenoising performances during training. While previous studies have overlookedthis aspect, we aim to train our baseline Transformers in a truly fair manner.Furthermore, we conduct empirical analyses of various components to determinethe key considerations for constructing LWDN Transformers. Codes are availableat ", "output": "Exploration of Lightweight Single Image Denoising with Transformers and Truly Fair Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Shapley value is widely regarded as a trustworthy attribution metric.However, when people use Shapley values to explain the attribution of inputvariables of a deep neural network (DNN), it usually requires a very highcomputational cost to approximate relatively accurate Shapley values inreal-world applications. Therefore, we propose a novel network architecture,the HarsanyiNet, which makes inferences on the input sample and simultaneouslycomputes the exact Shapley values of the input variables in a single forwardpropagation. The HarsanyiNet is designed on the theoretical foundation that theShapley value can be reformulated as the redistribution of Harsanyiinteractions encoded by the network.", "output": "HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vertical federated learning (VFL) is a promising category of federatedlearning for the scenario where data is vertically partitioned and distributedamong parties. VFL enriches the description of samples using features fromdifferent parties to improve model capacity. Compared with horizontal federatedlearning, in most cases, VFL is applied in the commercial cooperation scenarioof companies. Therefore, VFL contains tremendous business values. In the pastfew years, VFL has attracted more and more attention in both academia andindustry. In this paper, we systematically investigate the current work of VFLfrom a layered perspective. From the hardware layer to the vertical federatedsystem layer, researchers contribute to various aspects of VFL. Moreover, theapplication of VFL has covered a wide range of areas, e.g., finance,healthcare, etc. At each layer, we categorize the existing work and explore thechallenges for the convenience of further research and development of VFL.Especially, we design a novel MOSP tree taxonomy to analyze the core componentof VFL, i.e., secure vertical federated machine learning algorithm. Ourtaxonomy considers four dimensions, i.e., machine learning model (M),protection object (O), security model (S), and privacy-preserving protocol (P),and provides a comprehensive investigation.", "output": "A Survey on Vertical Federated Learning: From a Layered Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Progress in 3D volumetric image analysis research is limited by the lack ofdatasets and most advances in analysis methods for volumetric images are basedon medical data. However, medical data do not necessarily resemble thecharacteristics of other volumetric images such as micro-CT. To promoteresearch in 3D volumetric image analysis beyond medical data, we have createdthe BugNIST dataset and made it freely available. BugNIST is an extensivedataset of micro-CT scans of 12 types of bugs, such as insects and larvae.BugNIST contains 9437 volumes where 9087 are of individual bugs and 350 aremixtures of bugs and other material. The goal of BugNIST is to benchmarkclassification and detection methods, and we have designed the detectionchallenge such that detection models are trained on scans of individual bugsand tested on bug mixtures. Models capable of solving this task will beindependent of the context, i.e., the surrounding material. This is a greatadvantage if the context is unknown or changing, as is often the case inmicro-CT. Our initial baseline analysis shows that current state-of-the-artdeep learning methods classify individual bugs very well, but has greatdifficulty with the detection challenge. Hereby, BugNIST enables research inimage analysis areas that until now have missed relevant data - bothclassification, detection, and hopefully more.", "output": "BugNIST -- A New Large Scale Volumetric 3D Image Dataset for Classification and Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Comprehending how the brain interacts with the external world throughgenerated neural signals is crucial for determining its working mechanism,treating brain diseases, and understanding intelligence. Although manytheoretical models have been proposed, they have thus far been difficult tointegrate and develop. In this study, we were inspired in part by grid cells increating a more general and robust grid module and constructing an interactiveand self-reinforcing cognitive system together with Bayesian reasoning, anapproach called space-division and exploration-exploitation with grid-feedback(Grid-SD2E). Here, a grid module can be used as an interaction medium betweenthe outside world and a system, as well as a self-reinforcement medium withinthe system. The space-division and exploration-exploitation (SD2E) receives the0/1 signals of a grid through its space-division (SD) module. The systemdescribed in this paper is also a theoretical model derived from experimentsconducted by other researchers and our experience on neural decoding. Herein,we analyse the rationality of the system based on the existing theories in bothneuroscience and cognitive science, and attempt to propose special and generalrules to explain the different interactions between people and between peopleand the external world. What's more, based on this model, the smallestcomputing unit is extracted, which is analogous to a single neuron in thebrain.", "output": "Grid-SD2E: A General Grid-Feedback in a System for Cognitive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate 3D human pose estimation is essential for sports analytics,coaching, and injury prevention. However, existing datasets for monocular poseestimation do not adequately capture the challenging and dynamic nature ofsports movements. In response, we introduce SportsPose, a large-scale 3D humanpose dataset consisting of highly dynamic sports movements. With more than176,000 3D poses from 24 different subjects performing 5 different sportsactivities, SportsPose provides a diverse and comprehensive set of 3D posesthat reflect the complex and dynamic nature of sports movements. Contrary toother markerless datasets we have quantitatively evaluated the precision ofSportsPose by comparing our poses with a commercial marker-based system andachieve a mean error of 34.5 mm across all evaluation sequences. This iscomparable to the error reported on the commonly used 3DPW dataset. We furtherintroduce a new metric, local movement, which describes the movement of thewrist and ankle joints in relation to the body. With this, we show thatSportsPose contains more movement than the Human3.6M and 3DPW datasets in theseextremum joints, indicating that our movements are more dynamic. The datasetwith accompanying code can be downloaded from our website. We hope thatSportsPose will allow researchers and practitioners to develop and evaluatemore effective models for the analysis of sports performance and injuryprevention. With its realistic and diverse dataset, SportsPose provides avaluable resource for advancing the state-of-the-art in pose estimation insports.", "output": "SportsPose -- A Dynamic 3D sports pose dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Evaluating the performance of heuristic optimisation algorithms is essentialto determine how well they perform under various conditions. Recently, the BIAStoolbox was introduced as a behaviour benchmark to detect structural bias (SB)in search algorithms. The toolbox can be used to identify biases in existingalgorithms, as well as to test for bias in newly developed algorithms. In thisarticle, we introduce a novel and explainable deep-learning expansion of theBIAS toolbox, called Deep-BIAS. Where the original toolbox uses 39 statisticaltests and a Random Forest model to predict the existence and type of SB, theDeep-BIAS method uses a trained deep-learning model to immediately detect thestrength and type of SB based on the raw performance distributions. Through aseries of experiments with a variety of structurally biased scenarios, wedemonstrate the effectiveness of Deep-BIAS. We also present the results ofusing the toolbox on 336 state-of-the-art optimisation algorithms, which showedthe presence of various types of structural bias, particularly towards thecentre of the objective space or exhibiting discretisation behaviour. TheDeep-BIAS method outperforms the BIAS toolbox both in detecting bias and forclassifying the type of SB. Furthermore, explanations can be derived using XAItechniques.", "output": "Deep-BIAS: Detecting Structural Bias using Explainable AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce HATELEXICON, a lexicon of slurs and targets of hate speech forthe countries of Brazil, Germany, India and Kenya, to aid training andinterpretability of models. We demonstrate how our lexicon can be used tointerpret model predictions, showing that models developed to classify extremespeech rely heavily on target words when making predictions. Further, wepropose a method to aid shot selection for training in low-resource settingsvia HATELEXICON. In few-shot learning, the selection of shots is of paramountimportance to model performance. In our work, we simulate a few-shot settingfor German and Hindi, using HASOC data for training and the MultilingualHateCheck (MHC) as a benchmark. We show that selecting shots based on ourlexicon leads to models performing better on MHC than models trained on shotssampled randomly. Thus, when given only a few training examples, using ourlexicon to select shots containing more sociocultural information leads tobetter few-shot performance.", "output": "Sociocultural knowledge is needed for selection of shots in hate speech detection tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate and robust trajectory predictions of road users are needed to enablesafe automated driving. To do this, machine learning models are often used,which can show erratic behavior when presented with previously unseen inputs.In this work, two environment-aware models (MotionCNN and MultiPath++) and twocommon baselines (Constant Velocity and an LSTM) are benchmarked for robustnessagainst various perturbations that simulate functional insufficiencies observedduring model deployment in a vehicle: unavailability of road information, latedetections, and noise. Results show significant performance degradation underthe presence of these perturbations, with errors increasing up to +1444.8% incommonly used trajectory prediction evaluation metrics. Training the modelswith similar perturbations effectively reduces performance degradation, witherror increases of up to +87.5%. We argue that despite being an effectivemitigation strategy, data augmentation through perturbations during trainingdoes not guarantee robustness towards unforeseen perturbations, sinceidentification of all possible on-road complications is unfeasible.Furthermore, degrading the inputs sometimes leads to more accurate predictions,suggesting that the models are unable to learn the true relationships betweenthe different elements in the data.", "output": "Robustness Benchmark of Road User Trajectory Prediction Models for Automated Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As influencers play considerable roles in social media marketing, companiesincrease the budget for influencer marketing. Hiring effective influencers iscrucial in social influencer marketing, but it is challenging to find the rightinfluencers among hundreds of millions of social media users. In this paper, wepropose InfluencerRank that ranks influencers by their effectiveness based ontheir posting behaviors and social relations over time. To represent theposting behaviors and social relations, the graph convolutional neural networksare applied to model influencers with heterogeneous networks during differenthistorical periods. By learning the network structure with the embedded nodefeatures, InfluencerRank can derive informative representations for influencersat each period. An attentive recurrent neural network finally distinguisheshighly effective influencers from other influencers by capturing the knowledgeof the dynamics of influencer representations over time. Extensive experimentshave been conducted on an Instagram dataset that consists of 18,397 influencerswith their 2,952,075 posts published within 12 months. The experimental resultsdemonstrate that InfluencerRank outperforms existing baseline methods. Anin-depth analysis further reveals that all of our proposed features and modelcomponents are beneficial to discover effective influencers.", "output": "InfluencerRank: Discovering Effective Influencers via Graph Convolutional Attentive Recurrent Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, significant advancements have been made in 3D generative models,however training these models across diverse domains is challenging andrequires an huge amount of training data and knowledge of pose distribution.Text-guided domain adaptation methods have allowed the generator to be adaptedto the target domains using text prompts, thereby obviating the need forassembling numerous data. Recently, DATID-3D presents impressive quality ofsamples in text-guided domain, preserving diversity in text by leveragingtext-to-image diffusion. However, adapting 3D generators to domains withsignificant domain gaps from the source domain still remains challenging due toissues in current text-to-image diffusion models as following: 1) shape-posetrade-off in diffusion-based translation, 2) pose bias, and 3) instance bias inthe target domain, resulting in inferior 3D shapes, low text-imagecorrespondence, and low intra-domain diversity in the generated samples. Toaddress these issues, we propose a novel pipeline called PODIA-3D, which usespose-preserved text-to-image diffusion-based domain adaptation for 3Dgenerative models. We construct a pose-preserved text-to-image diffusion modelthat allows the use of extremely high-level noise for significant domainchanges. We also propose specialized-to-general sampling strategies to improvethe details of the generated samples. Moreover, to overcome the instance bias,we introduce a text-guided debiasing method that improves intra-domaindiversity. Consequently, our method successfully adapts 3D generators acrosssignificant domain gaps. Our qualitative results and user study demonstratesthat our approach outperforms existing 3D text-guided domain adaptation methodsin terms of text-image correspondence, realism, diversity of rendered images,and sense of depth of 3D shapes in the generated samples", "output": "PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is an increasing adoption of machine learning for encoding data intovectors to serve online recommendation and search use cases. As a result,recent data management systems propose augmenting query processing with onlinevector similarity search. In this work, we explore vector similarity search inthe context of Knowledge Graphs (KGs). Motivated by the tasks of findingrelated KG queries and entities for past KG query workloads, we focus on hybridvector similarity search (hybrid queries for short) where part of the querycorresponds to vector similarity search and part of the query corresponds topredicates over relational attributes associated with the underlying datavectors. For example, given past KG queries for a song entity, we want toconstruct new queries for new song entities whose vector representations areclose to the vector representation of the entity in the past KG query. Butentities in a KG also have non-vector attributes such as a song associated withan artist, a genre, and a release date. Therefore, suggested entities must alsosatisfy query predicates over non-vector attributes beyond a vector-basedsimilarity predicate. While these tasks are central to KGs, our contributionsare generally applicable to hybrid queries. In contrast to prior works thatoptimize online queries, we focus on enabling efficient batch processing ofpast hybrid query workloads. We present our system, HQI, for high-throughputbatch processing of hybrid queries. We introduce a workload-aware vector datapartitioning scheme to tailor the vector index layout to the given workload anddescribe a multi-query optimization technique to reduce the overhead of vectorsimilarity computations. We evaluate our methods on industrial workloads anddemonstrate that HQI yields a 31x improvement in throughput for finding relatedKG queries compared to existing hybrid query processing approaches.", "output": "High-Throughput Vector Similarity Search in Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The main objective of this paper is to identify the major research areas ofChatGPT through term and keyword co-occurrence network mapping techniques. Forconducting the present study, total of 577 publications were retrieved from theLens database for the network visualization. The findings of the study showedthat chatgpt occurrence in maximum number of times followed by its relatedterms such as artificial intelligence, large language model, gpt, study etc.This study will be helpful to library and information science as well ascomputer or information technology professionals.", "output": "Network Visualization of ChatGPT Research: a study based on term and keyword co-occurrence network analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning-assisted edge intelligence enables privacy protection inmodern intelligent services. However, not Independent and IdenticallyDistributed (non-IID) distribution among edge clients can impair the localmodel performance. The existing single prototype-based strategy represents asample by using the mean of the feature space. However, feature spaces areusually not clustered, and a single prototype may not represent a sample well.Motivated by this, this paper proposes a multi-prototype federated contrastivelearning approach (MP-FedCL) which demonstrates the effectiveness of using amulti-prototype strategy over a single-prototype under non-IID settings,including both label and feature skewness. Specifically, a multi-prototypecomputation strategy based on textit{k-means} is first proposed to capturedifferent embedding representations for each class space, using multipleprototypes ($k$ centroids) to represent a class in the embedding space. In eachglobal round, the computed multiple prototypes and their respective modelparameters are sent to the edge server for aggregation into a global prototypepool, which is then sent back to all clients to guide their local training.Finally, local training for each client minimizes their own supervised learningtasks and learns from shared prototypes in the global prototype pool throughsupervised contrastive learning, which encourages them to learn knowledgerelated to their own class from others and reduces the absorption of unrelatedknowledge in each global iteration. Experimental results on MNIST, Digit-5,Office-10, and DomainNet show that our method outperforms multiple baselines,with an average test accuracy improvement of about 4.6% and 10.4% underfeature and label non-IID distributions, respectively.", "output": "MP-FedCL: Multi-Prototype Federated Contrastive Learning for Edge Intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text classification typically requires a substantial amount ofhuman-annotated data to serve as supervision, which is costly to obtain indynamic emerging domains. Certain methods seek to address this problem bysolely relying on the surface text of class names to serve as extremely weaksupervision. However, existing methods fail to account for single-classdocuments discussing multiple topics. Both topic diversity and vague sentencesmay introduce noise into the document's underlying representation andconsequently the precision of the predicted class. Furthermore, current workfocuses on text granularities (documents, sentences, or words) independently,which limits the degree of coarse- or fine-grained context that we can jointlyextract from all three to identify significant subtext for classification. Inorder to address this problem, we propose MEGClass, an extremelyweakly-supervised text classification method to exploit Mutually-Enhancing TextGranularities. Specifically, MEGClass constructs class-oriented sentence andclass representations based on keywords for performing a sentence-levelconfidence-weighted label ensemble in order to estimate a document's initialclass distribution. This serves as the target distribution for a multi-headattention network with a class-weighted contrastive loss. This network learnscontextualized sentence representations and weights to form documentrepresentations that reflect its original document and sentence-level topicdiversity. Retaining this heterogeneity allows MEGClass to select the mostclass-indicative documents to serve as iterative feedback for enhancing theclass representations. Finally, these top documents are used to fine-tune apre-trained text classifier. As demonstrated through extensive experiments onsix benchmark datasets, MEGClass outperforms other weakly and extremely weaklysupervised methods.", "output": "MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous cyber and cyber-physical systems need to perform decision-making,learning, and control in unknown environments. Such decision-making can besensitive to multiple factors, including modeling errors, changes in costs, andimpacts of events in the tails of probability distributions. Althoughmulti-agent reinforcement learning (MARL) provides a framework for learningbehaviors through repeated interactions with the environment by minimizing anaverage cost, it will not be adequate to overcome the above challenges. In thispaper, we develop a distributed MARL approach to solve decision-making problemsin unknown environments by learning risk-aware actions. We use the conditionalvalue-at-risk (CVaR) to characterize the cost function that is being minimized,and define a Bellman operator to characterize the value function associated toa given state-action pair. We prove that this operator satisfies a contractionproperty, and that it converges to the optimal value function. We then proposea distributed MARL algorithm called the CVaR QD-Learning algorithm, andestablish that value functions of individual agents reaches consensus. Weidentify several challenges that arise in the implementation of the CVaRQD-Learning algorithm, and present solutions to overcome these. We evaluate theCVaR QD-Learning algorithm through simulations, and demonstrate the effect of arisk parameter on value functions at consensus.", "output": "Risk-Aware Distributed Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we present Multi-Level Contrastive Learning for DensePrediction Task (MCL), an efficient self-supervised method for learningregion-level feature representation for dense prediction tasks. Our method ismotivated by the three key factors in detection: localization, scaleconsistency and recognition. To explicitly encode absolute position and scaleinformation, we propose a novel pretext task that assembles multi-scale imagesin a montage manner to mimic multi-object scenarios. Unlike the existingimage-level self-supervised methods, our method constructs a multi-levelcontrastive loss that considers each sub-region of the montage image as asingleton. Our method enables the neural network to learn regional semanticrepresentations for translation and scale consistency while reducingpre-training epochs to the same as supervised pre-training. Extensiveexperiments demonstrate that MCL consistently outperforms the recentstate-of-the-art methods on various datasets with significant margins. Inparticular, MCL obtains 42.5 AP$^mathrm{bb}$ and 38.3 AP$^mathrm{mk}$ on COCOwith the 1x schedule fintuning, when using Mask R-CNN with R50-FPN backbonepre-trained with 100 epochs. In comparison to MoCo, our method surpasses theirperformance by 4.0 AP$^mathrm{bb}$ and 3.1 AP$^mathrm{mk}$. Furthermore, weexplore the alignment between pretext task and downstream tasks. We extend ourpretext task to supervised pre-training, which achieves a similar performanceto self-supervised learning. This result demonstrates the importance of thealignment between pretext task and downstream tasks, indicating the potentialfor wider applicability of our method beyond self-supervised settings.", "output": "Multi-Level Contrastive Learning for Dense Prediction Task."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning image classification and image generation using the same set ofnetwork parameters is a challenging problem. Recent advanced approaches performwell in one task often exhibit poor performance in the other. This workintroduces an energy-based classifier and generator, namely EGC, which canachieve superior performance in both tasks using a single neural network.Unlike a conventional classifier that outputs a label given an image (i.e., aconditional distribution $p(y|mathbf{x})$), the forward pass in EGC is aclassifier that outputs a joint distribution $p(mathbf{x},y)$, enabling animage generator in its backward pass by marginalizing out the label $y$. Thisis done by estimating the energy and classification probability given a noisyimage in the forward pass, while denoising it using the score functionestimated in the backward pass. EGC achieves competitive generation resultscompared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUNChurch, while achieving superior classification accuracy and robustness againstadversarial attacks on CIFAR-10. This work represents the first successfulattempt to simultaneously excel in both tasks using a single set of networkparameters. We believe that EGC bridges the gap between discriminative andgenerative learning.", "output": "EGC: Image Generation and Classification via a Single Energy-Based Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural network can easily overfit to even noisy labels due to its highcapacity, which degrades the generalization performance of a model. To overcomethis issue, we propose a new approach for learning from noisy labels (LNL) viapost-training, which can significantly improve the generalization performanceof any pre-trained model on noisy label data. To this end, we rather exploitthe overfitting property of a trained model to identify mislabeled samples.Specifically, our post-training approach gradually removes samples with highinfluence on the decision boundary and refines the decision boundary to improvegeneralization performance. Our post-training approach creates great synergieswhen combined with the existing LNL methods. Experimental results on variousreal-world and synthetic benchmark datasets demonstrate the validity of ourapproach in diverse realistic scenarios.", "output": "Influential Rank: A New Perspective of Post-training for Robust Model against Noisy Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Bayesian Controller Fusion (BCF): a hybrid control strategy thatcombines the strengths of traditional hand-crafted controllers and model-freedeep reinforcement learning (RL). BCF thrives in the robotics domain, wherereliable but suboptimal control priors exist for many tasks, but RL fromscratch remains unsafe and data-inefficient. By fusing uncertainty-awaredistributional outputs from each system, BCF arbitrates control between them,exploiting their respective strengths. We study BCF on two real-world roboticstasks involving navigation in a vast and long-horizon environment, and acomplex reaching task that involves manipulability maximisation. For both thesedomains, simple handcrafted controllers exist that can solve the task at handin a risk-averse manner but do not necessarily exhibit the optimal solutiongiven limitations in analytical modelling, controller miscalibration and taskvariation. As exploration is naturally guided by the prior in the early stagesof training, BCF accelerates learning, while substantially improving beyond theperformance of the control prior, as the policy gains more experience. Moreimportantly, given the risk-aversity of the control prior, BCF ensures safeexploration and deployment, where the control prior naturally dominates theaction distribution in states unknown to the policy. We additionally show BCF'sapplicability to the zero-shot sim-to-real setting and its ability to deal without-of-distribution states in the real world. BCF is a promising approachtowards combining the complementary strengths of deep RL and traditionalrobotic control, surpassing what either can achieve independently. The code andsupplementary video material are made publicly available at", "output": "Bayesian Controller Fusion: Leveraging Control Priors in Deep Reinforcement Learning for Robotics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The capability of a mobile robot to efficiently and safely perform complexmissions is limited by its knowledge of the environment, namely the situation.Advanced reasoning, decision-making, and execution skills enable an intelligentagent to act autonomously in unknown environments. Situational Awareness (SA)is a fundamental capability of humans that has been deeply studied in variousfields, such as psychology, military, aerospace, and education. Nevertheless,it has yet to be considered in robotics, which has focused on singlecompartmentalized concepts such as sensing, spatial perception, sensor fusion,state estimation, and Simultaneous Localization and Mapping (SLAM). Hence, thepresent research aims to connect the broad multidisciplinary existing knowledgeto pave the way for a complete SA system for mobile robotics that we deemparamount for autonomy. To this aim, we define the principal components tostructure a robotic SA and their area of competence. Accordingly, this paperinvestigates each aspect of SA, surveying the state-of-the-art roboticsalgorithms that cover them, and discusses their current limitations.Remarkably, essential aspects of SA are still immature since the currentalgorithmic development restricts their performance to only specificenvironments. Nevertheless, Artificial Intelligence (AI), particularly DeepLearning (DL), has brought new methods to bridge the gap that maintains thesefields apart from the deployment to real-world scenarios. Furthermore, anopportunity has been discovered to interconnect the vastly fragmented space ofrobotic comprehension algorithms through the mechanism of Situational Graph(S-Graph), a generalization of the well-known scene graph. Therefore, wefinally shape our vision for the future of robotic Situational Awareness bydiscussing interesting recent research directions.", "output": "From SLAM to Situational Awareness: Challenges and Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning is a setting where machine learning models learn novelconcepts from continuously shifting training data, while simultaneouslyavoiding degradation of knowledge on previously seen classes which maydisappear from the training data for extended periods of time (a phenomenonknown as the catastrophic forgetting problem). Current approaches for continuallearning of a single expanding task (aka class-incremental continual learning)require extensive rehearsal of previously seen data to avoid this degradationof knowledge. Unfortunately, rehearsal comes at a cost to memory, and it mayalso violate data-privacy. Instead, we explore combining knowledge distillationand parameter regularization in new ways to achieve strong continual learningperformance without rehearsal. Specifically, we take a deep dive into commoncontinual learning techniques: prediction distillation, feature distillation,L2 parameter regularization, and EWC parameter regularization. We firstdisprove the common assumption that parameter regularization techniques failfor rehearsal-free continual learning of a single, expanding task. Next, weexplore how to leverage knowledge from a pre-trained model in rehearsal-freecontinual learning and find that vanilla L2 parameter regularizationoutperforms EWC parameter regularization and feature distillation. Finally, weexplore the recently popular ImageNet-R benchmark, and show that L2 parameterregularization implemented in self-attention blocks of a ViT transformeroutperforms recent popular prompting for continual learning methods.", "output": "A Closer Look at Rehearsal-Free Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study fair allocation of indivisible goods when agents have matroid rankvaluations. Our main contribution is a simple algorithm based on the colloquialYankee Swap procedure that computes provably fair and efficient Lorenzdominating allocations. While there exist polynomial time algorithms to computesuch allocations, our proposed method improves on them in two ways. (a) Ourapproach is easy to understand and does not use complex matroid optimizationalgorithms as subroutines. (b) Our approach is scalable; it is provably fasterthan all known algorithms to compute Lorenz dominating allocations. These twoproperties are key to the adoption of algorithms in any real fair allocationsetting; our contribution brings us one step closer to this goal.", "output": "Yankee Swap: a Fast and Simple Fair Allocation Mechanism for Matroid Rank Valuations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous driving, in recent years, has been receiving increasing attentionfor its potential to relieve drivers' burdens and improve the safety ofdriving. In modern autonomous driving pipelines, the perception system is anindispensable component, aiming to accurately estimate the status ofsurrounding environments and provide reliable observations for prediction andplanning. 3D object detection, which intelligently predicts the locations,sizes, and categories of the critical 3D objects near an autonomous vehicle, isan important part of a perception system. This paper reviews the advances in 3Dobject detection for autonomous driving. First, we introduce the background of3D object detection and discuss the challenges in this task. Second, we conducta comprehensive survey of the progress in 3D object detection from the aspectsof models and sensory inputs, including LiDAR-based, camera-based, andmulti-modal detection approaches. We also provide an in-depth analysis of thepotentials and challenges in each category of methods. Additionally, wesystematically investigate the applications of 3D object detection in drivingsystems. Finally, we conduct a performance analysis of the 3D object detectionapproaches, and we further summarize the research trends over the years andprospect the future directions of this area.", "output": "3D Object Detection for Autonomous Driving: A Comprehensive Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-scale architectures and attention modules have shown effectiveness inmany deep learning-based image de-raining methods. However, manually designingand integrating these two components into a neural network requires a bulk oflabor and extensive expertise. In this article, a high-performance multi-scaleattentive neural architecture search (MANAS) framework is technically developedfor image deraining. The proposed method formulates a new multi-scale attentionsearch space with multiple flexible modules that are favorite to the imagede-raining task. Under the search space, multi-scale attentive cells are built,which are further used to construct a powerful image de-raining network. Theinternal multiscale attentive architecture of the de-raining network issearched automatically through a gradient-based search algorithm, which avoidsthe daunting procedure of the manual design to some extent. Moreover, in orderto obtain a robust image de-raining model, a practical and effectivemulti-to-one training strategy is also presented to allow the de-rainingnetwork to get sufficient background information from multiple rainy imageswith the same background scene, and meanwhile, multiple loss functionsincluding external loss, internal loss, architecture regularization loss, andmodel complexity loss are jointly optimized to achieve robust de-rainingperformance and controllable model complexity. Extensive experimental resultson both synthetic and realistic rainy images, as well as the down-stream visionapplications (i.e., objection detection and segmentation) consistentlydemonstrate the superiority of our proposed method. The code is publiclyavailable at ", "output": "Multi-scale Attentive Image De-raining Networks via Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning (DL) has big-data processing capabilities that are as good, oreven better, than those of humans in many real-world domains, but at the costof high energy requirements that may be unsustainable in some applications andof errors, that, though infrequent, can be large. We hypothesise that afundamental weakness of DL lies in its intrinsic dependence onintegrate-and-fire point neurons that maximise information transmissionirrespective of whether it is relevant in the current context or not. Thisleads to unnecessary neural firing and to the feedforward transmission ofconflicting messages, which makes learning difficult and processing energyinefficient. Here we show how to circumvent these limitations by mimicking thecapabilities of context-sensitive neocortical neurons that receive input fromdiverse sources as a context to amplify and attenuate the transmission ofrelevant and irrelevant information, respectively. We demonstrate that a deepnetwork composed of such local processors seeks to maximise agreement betweenthe active neurons, thus restricting the transmission of conflictinginformation to higher levels and reducing the neural activity required toprocess large amounts of heterogeneous real-world data. As shown to be far moreeffective and efficient than current forms of DL, this two-point neuron studyoffers a possible step-change in transforming the cellular foundations of deepnetwork architectures.", "output": "Context-sensitive neocortical neurons transform the effectiveness and efficiency of neural information processing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lottery tickets (LTs) is able to discover accurate and sparse subnetworksthat could be trained in isolation to match the performance of dense networks.Ensemble, in parallel, is one of the oldest time-proven tricks in machinelearning to improve performance by combining the output of multiple independentmodels. However, the benefits of ensemble in the context of LTs will be dilutedsince ensemble does not directly lead to stronger sparse subnetworks, butleverages their predictions for a better decision. In this work, we firstobserve that directly averaging the weights of the adjacent learned subnetworkssignificantly boosts the performance of LTs. Encouraged by this observation, wefurther propose an alternative way to perform an 'ensemble' over thesubnetworks identified by iterative magnitude pruning via a simpleinterpolating strategy. We call our method Lottery Pools. In contrast to thenaive ensemble which brings no performance gains to each single subnetwork,Lottery Pools yields much stronger sparse subnetworks than the original LTswithout requiring any extra training or inference cost. Across various modernarchitectures on CIFAR-10/100 and ImageNet, we show that our method achievessignificant performance gains in both, in-distribution and out-of-distributionscenarios. Impressively, evaluated with VGG-16 and ResNet-18, the producedsparse subnetworks outperform the original LTs by up to 1.88% on CIFAR-100 and2.36% on CIFAR-100-C; the resulting dense network surpasses the pre-traineddense-model up to 2.22% on CIFAR-100 and 2.38% on CIFAR-100-C.", "output": "Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Intelligent robots rely on object detection models to perceive theenvironment. Following advances in deep learning security it has been revealedthat object detection models are vulnerable to adversarial attacks. However,prior research primarily focuses on attacking static images or offline videos.Therefore, it is still unclear if such attacks could jeopardize real-worldrobotic applications in dynamic environments. This paper bridges this gap bypresenting the first real-time online attack against object detection models.We devise three attacks that fabricate bounding boxes for nonexistent objectsat desired locations. The attacks achieve a success rate of about 90% withinabout 20 iterations. The demo video is available at", "output": "Adversarial Detection: Attacking Object Detection in Real Time."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a novel model called GAMMT (Generative Ambiguity Models usingMultiple Transformers) for sequential data that is based on sets ofprobabilities. Unlike conventional models, our approach acknowledges that thedata generation process of a sequence is not deterministic, but ratherambiguous and influenced by a set of probabilities. To capture this ambiguity,GAMMT employs multiple parallel transformers that are linked by a selectionmechanism, allowing for the approximation of ambiguous probabilities. Thegenerative nature of our approach also enables multiple representations ofinput tokens and sequences. While our models have not yet undergoneexperimental validation, we believe that our model has great potential toachieve high quality and diversity in modeling sequences with uncertain datageneration processes.", "output": "GAMMT: Generative Ambiguity Modeling Using Multiple Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While natural systems often present collective intelligence that allows themto self-organize and adapt to changes, the equivalent is missing in mostartificial systems. We explore the possibility of such a system in the contextof cooperative 2D push manipulations using mobile robots. Although conventionalworks demonstrate potential solutions for the problem in restricted settings,they have computational and learning difficulties. More importantly, thesesystems do not possess the ability to adapt when facing environmental changes.In this work, we show that by distilling a planner derived from adifferentiable soft-body physics simulator into an attention-based neuralnetwork, our multi-robot push manipulation system achieves better performancethan baselines. In addition, our system also generalizes to configurations notseen during training and is able to adapt toward task completions when externalturbulence and environmental changes are applied. Supplementary videos can befound on our project website:url{", "output": "Collective Intelligence for 2D Push Manipulation with Mobile Robots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graphs (KGs) have attracted more and more attentions because oftheir fundamental roles in many tasks. Quality evaluation for KGs is thuscrucial and indispensable. Existing methods in this field evaluate KGs byeither proposing new quality metrics from different dimensions or measuringperformances at KG construction stages. However, there are two major issueswith those methods. First, they highly rely on raw data in KGs, which makesKGs' internal information exposed during quality evaluation. Second, theyconsider more about the quality at data level instead of ability level, wherethe latter one is more important for downstream applications. To address theseissues, we propose a knowledge graph quality evaluation framework underincomplete information (QEII). The quality evaluation task is transformed intoan adversarial Q&amp;A game between two KGs. Winner of the game is thus consideredto have better qualities. During the evaluation process, no raw data isexposed, which ensures information protection. Experimental results on fourpairs of KGs demonstrate that, compared with baselines, the QEII implements areasonable quality evaluation at ability level under incomplete information.", "output": "Knowledge Graph Quality Evaluation under Incomplete Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Document Visual Question Answering (DocVQA) refers to the task of answeringquestions from document images. Existing work on DocVQA only considerssingle-page documents. However, in real scenarios documents are mostly composedof multiple pages that should be processed altogether. In this work we extendDocVQA to the multi-page scenario. For that, we first create a new dataset,MP-DocVQA, where questions are posed over multi-page documents instead ofsingle pages. Second, we propose a new hierarchical method, Hi-VT5, based onthe T5 architecture, that overcomes the limitations of current methods toprocess long multi-page documents. The proposed method is based on ahierarchical transformer architecture where the encoder summarizes the mostrelevant information of every page and then, the decoder takes this summarizedinformation to generate the final answer. Through extensive experimentation, wedemonstrate that our method is able, in a single stage, to answer the questionsand provide the page that contains the relevant information to find the answer,which can be used as a kind of explainability measure.", "output": "Hierarchical multimodal transformers for Multi-Page DocVQA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Test-time adaptation (TTA) is the problem of updating a pre-trained sourcemodel at inference time given test input(s) from a different target domain.Most existing TTA approaches assume the setting in which the target domain isstationary, i.e., all the test inputs come from a single target domain.However, in many practical settings, the test input distribution might exhibita lifelong/continual shift over time. Moreover, existing TTA approaches alsolack the ability to provide reliable uncertainty estimates, which is crucialwhen distribution shifts occur between the source and target domain. To addressthese issues, we present PETAL (Probabilistic lifElong Test-time Adaptationwith seLf-training prior), which solves lifelong TTA using a probabilisticapproach, and naturally results in (1) a student-teacher framework, where theteacher model is an exponential moving average of the student model, and (2)regularizing the model updates at inference time using the source model as aregularizer. To prevent model drift in the lifelong/continual TTA setting, wealso propose a data-driven parameter restoration technique which contributes toreducing the error accumulation and maintaining the knowledge of recent domainsby restoring only the irrelevant parameters. In terms of predictive error rateas well as uncertainty based metrics such as Brier score and negativelog-likelihood, our method achieves better results than the currentstate-of-the-art for online lifelong test-time adaptation across variousbenchmarks, such as CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCCdatasets. The source code for our approach is accessible at", "output": "A Probabilistic Framework for Lifelong Test-Time Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With more and more data being collected, data-driven modeling methods havebeen gaining in popularity in recent years. While physically sound, classicalgray-box models are often cumbersome to identify and scale, and their accuracymight be hindered by their limited expressiveness. On the other hand, classicalblack-box methods, typically relying on Neural Networks (NNs) nowadays, oftenachieve impressive performance, even at scale, by deriving statistical patternsfrom data. However, they remain completely oblivious to the underlying physicallaws, which may lead to potentially catastrophic failures if decisions forreal-world physical systems are based on them. Physically Consistent NeuralNetworks (PCNNs) were recently developed to address these aforementionedissues, ensuring physical consistency while still leveraging NNs to attainstate-of-the-art accuracy.In this work, we scale PCNNs to model building temperature dynamics andpropose a thorough comparison with classical gray-box and black-box methods.More precisely, we design three distinct PCNN extensions, thereby exemplifyingthe modularity and flexibility of the architecture, and formally prove theirphysical consistency. In the presented case study, PCNNs are shown to achievestate-of-the-art accuracy, even outperforming classical NN-based models despitetheir constrained structure. Our investigations furthermore provide a clearillustration of NNs achieving seemingly good performance while remainingcompletely physics-agnostic, which can be misleading in practice. While thisperformance comes at the cost of computational complexity, PCNNs on the otherhand show accuracy improvements of 17-35% compared to all other physicallyconsistent methods, paving the way for scalable physically consistent modelswith state-of-the-art performance.", "output": "Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To interpret deep neural networks, one main approach is to dissect the visualinput and find the prototypical parts responsible for the classification.However, existing methods often ignore the hierarchical relationship betweenthese prototypes, and thus can not explain semantic concepts at both higherlevel (e.g., water sports) and lower level (e.g., swimming). In this paperinspired by human cognition system, we leverage hierarchal information to dealwith uncertainty: When we observe water and human activity, but no definitiveaction it can be recognized as the water sports parent class. Only afterobserving a person swimming can we definitively refine it to the swimmingaction. To this end, we propose HIerarchical Prototype Explainer (HIPE) tobuild hierarchical relations between prototypes and classes. HIPE enables areasoning process for video action classification by dissecting the input videoframes on multiple levels of the class hierarchy, our method is also applicableto other video tasks. The faithfulness of our method is verified by reducingaccuracy-explainability trade off on ActivityNet and UCF-101 while providingmulti-level explanations.", "output": "Hierarchical Explanations for Video Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "D-Adaptation is an approach to automatically setting the learning rate whichasymptotically achieves the optimal rate of convergence for minimizing convexLipschitz functions, with no back-tracking or line searches, and no additionalfunction value or gradient evaluations per step. Our approach is the firsthyper-parameter free method for this class without additional multiplicativelog factors in the convergence rate. We present extensive experiments for SGDand Adam variants of our method, where the method automatically matcheshand-tuned learning rates across more than a dozen diverse machine learningproblems, including large-scale vision and language problems.An open-source implementation is available aturl{", "output": "Learning-Rate-Free Learning by D-Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world data generation often involves complex inter-dependencies amonginstances, violating the IID-data hypothesis of standard learning paradigms andposing a challenge for uncovering the geometric structures for learning desiredinstance representations. To this end, we introduce an energy constraineddiffusion model which encodes a batch of instances from a dataset intoevolutionary states that progressively incorporate other instances' informationby their interactions. The diffusion process is constrained by descent criteriaw.r.t.~a principled energy function that characterizes the global consistencyof instance representations over latent structures. We provide rigorous theorythat implies closed-form optimal estimates for the pairwise diffusion strengthamong arbitrary instance pairs, which gives rise to a new class of neuralencoders, dubbed as DIFFormer (diffusion-based Transformers), with twoinstantiations: a simple version with linear complexity for prohibitiveinstance numbers, and an advanced version for learning complex structures.Experiments highlight the wide applicability of our model as a general-purposeencoder backbone with superior performance in various tasks, such as nodeclassification on large graphs, semi-supervised image/text classification, andspatial-temporal dynamics prediction.", "output": "DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pre-training with offline data and online fine-tuning using reinforcementlearning is a promising strategy for learning control policies by leveragingthe best of both worlds in terms of sample efficiency and performance. Onenatural approach is to initialize the policy for online learning with the onetrained offline. In this work, we introduce a policy expansion scheme for thistask. After learning the offline policy, we use it as one candidate policy in apolicy set. We then expand the policy set with another policy which will beresponsible for further learning. The two policies will be composed in anadaptive manner for interacting with the environment. With this approach, thepolicy previously learned offline is fully retained during online learning,thus mitigating the potential issues such as destroying the useful behaviors ofthe offline policy in the initial stage of online learning while allowing theoffline policy participate in the exploration naturally in an adaptive manner.Moreover, new useful behaviors can potentially be captured by the newly addedpolicy through learning. Experiments are conducted on a number of tasks and theresults demonstrate the effectiveness of the proposed approach.", "output": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models have been demonstrated to be valuable in differentfields. ChatGPT, developed by OpenAI, has been trained using massive amounts ofdata and simulates human conversation by comprehending context and generatingappropriate responses. It has garnered significant attention due to its abilityto effectively answer a broad range of human inquiries, with fluent andcomprehensive answers surpassing prior public chatbots in both security andusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,which is the focus of this study. Eleven categories of failures, includingreasoning, factual errors, math, coding, and bias, are presented and discussed.The risks, limitations, and societal implications of ChatGPT are alsohighlighted. The goal of this study is to assist researchers and developers inenhancing future language models and chatbots.", "output": "A Categorical Archive of ChatGPT Failures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We explore the possibility of meta-learning for the language-independentunsupervised tokenization problem for English, Russian, and Chinese. Weimplement the meta-learning approach for automatic determination ofhyper-parameters of the unsupervised tokenization model proposed in earlierworks, relying on various human-independent fitness functions such asnormalised anti-entropy, compression factor and cross-split F1 score, as wellas additive and multiplicative composite combinations of the three metrics,testing them against the conventional F1 tokenization score. We find a fairlygood correlation between the latter and the additive combination of the formerthree metrics for English and Russian. In case of Chinese, we find asignificant correlation between the F 1 score and the compression factor. Ourresults suggest the possibility of robust unsupervised tokenization oflow-resource and dead languages and allow us to think about human languages interms of the evolution of efficient symbolic communication codes with differentstructural optimisation schemes that have evolved in different human cultures.", "output": "Self-tuning hyper-parameters for unsupervised cross-lingual tokenization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Task allocation plays a vital role in multi-robot autonomous cleaningsystems, where multiple robots work together to clean a large area. However,most current studies mainly focus on deterministic, single-task allocation forcleaning robots, without considering hybrid tasks in uncertain workingenvironments. Moreover, there is a lack of datasets and benchmarks for relevantresearch. In this paper, to address these problems, we formulate multi-robothybrid-task allocation under the uncertain cleaning environment as a robustoptimization problem. Firstly, we propose a novel robust mixed-integer linearprogramming model with practical constraints including the task orderconstraint for different tasks and the ability constraints of hybrid robots.Secondly, we establish a dataset of emph{100} instances made from floor plans,each of which has 2D manually-labeled images and a 3D model. Thirdly, weprovide comprehensive results on the collected dataset using three traditionaloptimization approaches and a deep reinforcement learning-based solver. Theevaluation results show that our solution meets the needs of multi-robotcleaning task allocation and the robust solver can protect the system fromworst-case scenarios with little additional cost. The benchmark will beavailable at{", "output": "Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous Cleaning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most models of visual attention are aimed at predicting either top-down orbottom-up control, as studied using different visual search and free-viewingtasks. We propose Human Attention Transformer (HAT), a single model predictingboth forms of attention control. HAT is the new state-of-the-art (SOTA) inpredicting the scanpath of fixations made during target-present andtarget-absent search, and matches or exceeds SOTA in the prediction of tasklessfree-viewing fixation scanpaths. HAT achieves this new SOTA by using a noveltransformer-based architecture and a simplified foveated retina thatcollectively create a spatio-temporal awareness akin to the dynamic visualworking memory of humans. Unlike previous methods that rely on a coarse grid offixation cells and experience information loss due to fixation discretization,HAT features a dense-prediction architecture and outputs a dense heatmap foreach fixation, thus avoiding discretizing fixations. HAT sets a new standard incomputational attention, which emphasizes both effectiveness and generality.HAT's demonstrated scope and applicability will likely inspire the developmentof new attention models that can better predict human behavior in variousattention-demanding scenarios.", "output": "Predicting Human Attention using Computational Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In decision-making methods, it is common to assume that the experts arehonest and professional. However, this is not the case when one or more expertsin the group decision making framework, such as the group analytic hierarchyprocess (GAHP), try to manipulate results in their favor. The aim of this paperis to introduce two heuristics in the GAHP, setting allowing to detect themanipulators and minimize their effect on the group consensus by diminishingtheir weights. The first heuristic is based on the assumption that manipulatorswill provide judgments which can be considered outliers with respect to thoseof the rest of the experts in the group. The second heuristic assumes thatdishonest judgments are less consistent than the average consistency of thegroup. Both approaches are illustrated with numerical examples and simulations.", "output": "Towards secure judgments aggregation in AHP."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce the range of oBERTa language models, aneasy-to-use set of language models which allows Natural Language Processing(NLP) practitioners to obtain between 3.8 and 24.3 times faster models withoutexpertise in model compression. Specifically, oBERTa extends existing work onpruning, knowledge distillation, and quantization and leverages frozenembeddings improves distillation and model initialization to deliver higheraccuracy on a broad range of transfer tasks. In generating oBERTa, we explorehow the highly optimized RoBERTa differs from the BERT for pruning duringpre-training and finetuning. We find it less amenable to compression duringfine-tuning. We explore the use of oBERTa on seven representative NLP tasks andfind that the improved compression techniques allow a pruned oBERTa model tomatch the performance of BERTbase and exceed the performance of Prune OFA Largeon the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,respectively faster in inference. We release our code, training regimes, andassociated model for broad usage to encourage usage and experimentation", "output": "oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A backdoor attack allows a malicious user to manipulate the environment orcorrupt the training data, thus inserting a backdoor into the trained agent.Such attacks compromise the RL system's reliability, leading to potentiallycatastrophic results in various key fields. In contrast, relatively limitedresearch has investigated effective defenses against backdoor attacks in RL.This paper proposes the Recovery Triggered States (RTS) method, a novelapproach that effectively protects the victim agents from backdoor attacks. RTSinvolves building a surrogate network to approximate the dynamics model.Developers can then recover the environment from the triggered state to a cleanstate, thereby preventing attackers from activating backdoors hidden in theagent by presenting the trigger. When training the surrogate to predict states,we incorporate agent action information to reduce the discrepancy between theactions taken by the agent on predicted states and the actions taken on realstates. RTS is the first approach to defend against backdoor attacks in asingle-agent setting. Our results show that using RTS, the cumulative rewardonly decreased by 1.41% under the backdoor attack.", "output": "Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Action understanding matters and attracts attention. It can be formed as themapping from the action physical space to the semantic space. Typically,researchers built action datasets according to idiosyncratic choices to defineclasses and push the envelope of benchmarks respectively. Thus, datasets areincompatible with each other like \"Isolated Islands\" due to semantic gaps andvarious class granularities, e.g., do housework in dataset A and wash plate indataset B. We argue that a more principled semantic space is an urgent need toconcentrate the community efforts and enable us to use all datasets together topursue generalizable action learning. To this end, we design a Poincare actionsemantic space given verb taxonomy hierarchy and covering massive actions. Byaligning the classes of previous datasets to our semantic space, we gather(image/video/skeleton/MoCap) datasets into a unified database in a unifiedlabel system, i.e., bridging \"isolated islands\" into a \"Pangea\". Accordingly,we propose a bidirectional mapping model between physical and semantic space tofully use Pangea. In extensive experiments, our system shows significantsuperiority, especially in transfer learning. Code and data will be madepublicly available.", "output": "From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Chat models, such as ChatGPT, have shown impressive capabilities and havebeen rapidly adopted across numerous domains. However, these models are onlyaccessible through a restricted API, creating barriers for new research andprogress in the field. We propose a pipeline that can automatically generate ahigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in aconversation with itself. Subsequently, we employ parameter-efficient tuning toenhance LLaMA, an open-source large language model. The resulting model, namedBaize, demonstrates good performance in multi-turn dialogues with guardrailsthat minimize potential risks. The Baize models and data are released forresearch purposes only at  An onlinedemo is also available at", "output": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The explosive growth of cyber attacks nowadays, such as malware, spam, andintrusions, caused severe consequences on society. Securing cyberspace hasbecome an utmost concern for organizations and governments. Traditional MachineLearning (ML) based methods are extensively used in detecting cyber threats,but they hardly model the correlations between real-world cyber entities. Inrecent years, with the proliferation of graph mining techniques, manyresearchers investigated these techniques for capturing correlations betweencyber entities and achieving high performance. It is imperative to summarizeexisting graph-based cybersecurity solutions to provide a guide for futurestudies. Therefore, as a key contribution of this paper, we provide acomprehensive review of graph mining for cybersecurity, including an overviewof cybersecurity tasks, the typical graph mining techniques, and the generalprocess of applying them to cybersecurity, as well as various solutions fordifferent cybersecurity tasks. For each task, we probe into relevant methodsand highlight the graph types, graph approaches, and task levels in theirmodeling. Furthermore, we collect open datasets and toolkits for graph-basedcybersecurity. Finally, we outlook the potential directions of this field forfuture research.", "output": "Graph Mining for Cybersecurity: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Throughout schooling, students are tested on reading comprehension andlogical reasoning. Students have developed various strategies for completingsuch exams, some of which are generally thought to outperform others. One suchstrategy involves emphasizing relative accuracy over absolute accuracy and cantheoretically produce the correct answer without full knowledge of theinformation required to solve the question. This paper examines theeffectiveness of applying such a strategy to train transfer learning models tosolve reading comprehension and logical reasoning questions. The models wereevaluated on the ReClor dataset, a challenging reading comprehension andlogical reasoning benchmark. While previous studies targeted logical reasoningskills, we focus on a general training method and model architecture. Wepropose the polytuplet loss function, an extension of the triplet lossfunction, to ensure prioritization of learning the relative correctness ofanswer choices over learning the true accuracy of each choice. Our resultsindicate that models employing polytuplet loss outperform existing baselinemodels. Although polytuplet loss is a promising alternative to othercontrastive loss functions, further research is required to quantify thebenefits it may present.", "output": "Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Parkinsons Disease (PD) is a neurodegenerative disorder resulting in motordeficits due to advancing degeneration of dopaminergic neurons. PD patientsreport experiencing tremor, rigidity, visual impairment, bradykinesia, andseveral cognitive deficits. Although Electroencephalography (EEG) indicatesabnormalities in PD patients, one major challenge is the lack of a consistent,accurate, and systemic biomarker for PD in order to closely monitor the diseasewith therapeutic treatments and medication. In this study, we collectedElectroencephalographic data from 15 PD patients and 16 Healthy Controls (HC).We first preprocessed every EEG signal using several techniques and extractedrelevant features using many feature extraction algorithms. Afterwards, weapplied several machine learning algorithms to classify PD versus HC. We foundthe most significant metrics to be achieved by the Random Forest ensemblelearning approach, with an accuracy, precision, recall, F1 score, and AUC of97.5%, 100%, 95%, 0.967, and 0.975, respectively. The results of this studyshow promise for exposing PD abnormalities using EEG during clinical diagnosis,and automating this process using signal processing techniques and MLalgorithms to evaluate the difference between healthy individuals and PDpatients.", "output": "Parkinsons Disease Detection via Resting-State Electroencephalography Using Signal Processing and Machine Learning Techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We conducted a prospective study to measure the clinical impact of anexplainable machine learning system on interobserver agreement in chestradiograph interpretation. The AI system, which we call as it VinDr-CXR whenused as a diagnosis-supporting tool, significantly improved the agreementbetween six radiologists with an increase of 1.5% in mean Fleiss' Kappa. Inaddition, we also observed that, after the radiologists consulted AI'ssuggestions, the agreement between each radiologist and the system wasremarkably increased by 3.3% in mean Cohen's Kappa. This work has been acceptedfor publication in IEEE Access and this paper is our short version submitted tothe Midwest Machine Learning Symposium (MMLS 2023), Chicago, IL, USA.", "output": "Evaluating the impact of an explainable machine learning system on the interobserver agreement in chest radiograph interpretation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we investigate the use of Fourier Neural Operators (FNOs) forimage classification in comparison to standard Convolutional Neural Networks(CNNs). Neural operators are a discretization-invariant generalization ofneural networks to approximate operators between infinite dimensional functionspaces. FNOs - which are neural operators with a specific parametrization -have been applied successfully in the context of parametric PDEs. We derive theFNO architecture as an example for continuous and Fr'echet-differentiableneural operators on Lebesgue spaces. We further show how CNNs can be convertedinto FNOs and vice versa and propose an interpolation-equivariant adaptation ofthe architecture.", "output": "Resolution-Invariant Image Classification based on Fourier Neural Operators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, online distillation has emerged as a powerful technique foradapting real-time deep neural networks on the fly using a slow, but accurateteacher model. However, a major challenge in online distillation iscatastrophic forgetting when the domain shifts, which occurs when the studentmodel is updated with data from the new domain and forgets previously learnedknowledge. In this paper, we propose a solution to this issue by leveraging thepower of continual learning methods to reduce the impact of domain shifts.Specifically, we integrate several state-of-the-art continual learning methodsin the context of online distillation and demonstrate their effectiveness inreducing catastrophic forgetting. Furthermore, we provide a detailed analysisof our proposed solution in the case of cyclic domain shifts. Our experimentalresults demonstrate the efficacy of our approach in improving the robustnessand accuracy of online distillation, with potential applications in domainssuch as video surveillance or autonomous driving. Overall, our work representsan important step forward in the field of online distillation and continuallearning, with the potential to significantly impact real-world applications.", "output": "Online Distillation with Continual Learning for Cyclic Domain Shifts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Thermal imaging has numerous advantages over regular visible-range imagingsince it performs well in low-light circumstances. Super-Resolution approachescan broaden their usefulness by replicating accurate high-resolution thermalpictures using measurements from low-cost, low-resolution thermal sensors.Because of the spectral range mismatch between the images, GuidedSuper-Resolution of thermal images utilizing visible range images is difficult.However, In case of failure to capture Visible Range Images can prevent theoperations of applications in critical areas. We present a novel data fusionframework and regularization technique for Guided Super Resolution of Thermalimages. The proposed architecture is computationally in-expensive andlightweight with the ability to maintain performance despite missing one of themodalities, i.e., high-resolution RGB image or the lower-resolution thermalimage, and is designed to be robust in the presence of missing data. Theproposed method presents a promising solution to the frequently occurringproblem of missing modalities in a real-world scenario. Code is available at", "output": "CoReFusion: Contrastive Regularized Fusion for Guided Thermal Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing image restoration methods mostly leverage the posterior distributionof natural images. However, they often assume known degradation and alsorequire supervised training, which restricts their adaptation to complex realapplications. In this work, we propose the Generative Diffusion Prior (GDP) toeffectively model the posterior distributions in an unsupervised samplingmanner. GDP utilizes a pre-train denoising diffusion generative model (DDPM)for solving linear inverse, non-linear, or blind problems. Specifically, GDPsystematically explores a protocol of conditional guidance, which is verifiedmore practical than the commonly used guidance way. Furthermore, GDP isstrength at optimizing the parameters of degradation model during the denoisingprocess, achieving blind image restoration. Besides, we devise hierarchicalguidance and patch-based methods, enabling the GDP to generate images ofarbitrary resolutions. Experimentally, we demonstrate GDP's versatility onseveral image datasets for linear problems, such as super-resolution,deblurring, inpainting, and colorization, as well as non-linear and blindissues, such as low-light enhancement and HDR image recovery. GDP outperformsthe current leading unsupervised methods on the diverse benchmarks inreconstruction quality and perceptual quality. Moreover, GDP also generalizeswell for natural images or synthesized images with arbitrary sizes from varioustasks out of the distribution of the ImageNet training set.", "output": "Generative Diffusion Prior for Unified Image Restoration and Enhancement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks have made huge progress in the last few decades.However, as the real-world data often exhibits a long-tailed distribution,vanilla deep models tend to be heavily biased toward the majority classes. Toaddress this problem, state-of-the-art methods usually adopt a mixture ofexperts (MoE) to focus on different parts of the long-tailed distribution.Experts in these methods are with the same model depth, which neglects the factthat different classes may have different preferences to be fit by models withdifferent depths. To this end, we propose a novel MoE-based method calledSelf-Heterogeneous Integration with Knowledge Excavation (SHIKE). We firstpropose Depth-wise Knowledge Fusion (DKF) to fuse features between differentshallow parts and the deep part in one network for each expert, which makesexperts more diverse in terms of representation. Based on DKF, we furtherpropose Dynamic Knowledge Transfer (DKT) to reduce the influence of the hardestnegative class that has a non-negligible impact on the tail classes in our MoEframework. As a result, the classification accuracy of long-tailed data can besignificantly improved, especially for the tail classes. SHIKE achieves thestate-of-the-art performance of 56.3%, 60.3%, 75.4%, and 41.9% on CIFAR100-LT(IF100), ImageNet-LT, iNaturalist 2018, and Places-LT, respectively.", "output": "Long-Tailed Visual Recognition via Self-Heterogeneous Integration with Knowledge Excavation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The main challenge of monocular 3D object detection is the accuratelocalization of 3D center. Motivated by a new and strong observation that thischallenge can be remedied by a 3D-space local-grid search scheme in an idealcase, we propose a stage-wise approach, which combines the information flowfrom 2D-to-3D (3D bounding box proposal generation with a single 2D image) and3D-to-2D (proposal verification by denoising with 3D-to-2D contexts) in atop-down manner. Specifically, we first obtain initial proposals fromoff-the-shelf backbone monocular 3D detectors. Then, we generate a 3D anchorspace by local-grid sampling from the initial proposals. Finally, we perform 3Dbounding box denoising at the 3D-to-2D proposal verification stage. Toeffectively learn discriminative features for denoising highly overlappedproposals, this paper presents a method of using the Perceiver I/O model tofuse the 3D-to-2D geometric information and the 2D appearance information. Withthe encoded latent representation of a proposal, the verification head isimplemented with a self-attention module. Our method, named as MonoXiver, isgeneric and can be easily adapted to any backbone monocular 3D detectors.Experimental results on the well-established KITTI dataset and the challenginglarge-scale Waymo dataset show that MonoXiver consistently achieves improvementwith limited computation overhead.", "output": "Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy-based models (EBM) have become increasingly popular within computervision. EBMs bring a probabilistic approach to training deep neural networks(DNN) and have been shown to enhance performance in areas such as calibration,out-of-distribution detection, and adversarial resistance. However, theseadvantages come at the cost of estimating input data probabilities, usuallyusing a Langevin based method such as Stochastic Gradient Langevin Dynamics(SGLD), which bring additional computational costs, require parameterization,caching methods for efficiency, and can run into stability and scaling issues.EBMs use dynamical methods to draw samples from the probability densityfunction (PDF) defined by the current state of the network and compare them tothe training data using a maximum log likelihood approach to learn the correctPDF.We propose a non-generative training approach, Non-Generative EBM (NG-EBM),that utilizes the {it{Approximate Mass}}, identified by Grathwohl et al., as aloss term to direct the training. We show that our NG-EBM training strategyretains many of the benefits of EBM in calibration, out-of-distributiondetection, and adversarial resistance, but without the computational complexityand overhead of the traditional approaches. In particular, the NG-EBM approachimproves the Expected Calibration Error by a factor of 2.5 for CIFAR10 and 7.5times for CIFAR100, when compared to traditionally trained models.", "output": "Non-Generative Energy Based Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of non-line-of-sight (NLOS) imaging is to image objects occludedfrom the camera's field of view using multiply scattered light. Recent workshave demonstrated the feasibility of two-bounce (2B) NLOS imaging by scanning alaser and measuring cast shadows of occluded objects in scenes with two relaysurfaces. In this work, we study the role of time-of-flight (ToF) measurements,ie transients, in 2B-NLOS under multiplexed illumination. Specifically, westudy how ToF information can reduce the number of measurements and spatialresolution needed for shape reconstruction. We present our findings withrespect to tradeoffs in (1) temporal resolution, (2) spatial resolution, and(3) number of image captures by studying SNR and recoverability as functions ofsystem parameters. This leads to a formal definition of the mathematicalconstraints for 2B lidar. We believe that our work lays an analyticalgroundwork for design of future NLOS imaging systems, especially as ToF sensorsbecome increasingly ubiquitous.", "output": "Role of Transients in Two-Bounce Non-Line-of-Sight Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")manipulations of multimedia content. Cheapfakes are known to be more prevalentthan deepfakes. Cheapfake media can be created using editing software forimage/video manipulations, or even without using any software, by simplyaltering the context of an image/video by sharing the media alongsidemisleading claims. This alteration of context is referred to as out-of-context(OOC) misuse of media. OOC media is much harder to detect than fake media,since the images and videos are not tampered. In this challenge, we focus ondetecting OOC images, and more specifically the misuse of real photographs withconflicting image captions in news items. The aim of this challenge is todevelop and benchmark models that can be used to detect whether given samples(news image and associated captions) are OOC, based on the recently compiledCOSMOS dataset.", "output": "Grand Challenge On Detecting Cheapfakes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training data is a critical requirement for machine learning tasks, andlabeled training data can be expensive to acquire, often requiring manual orsemi-automated data collection pipelines. For tracking applications, the datacollection involves drawing bounding boxes around the classes of interest oneach frame, and associate detections of the same \"instance\" over frames. In asemi-automated data collection pipeline, this can be achieved by running abaseline detection and tracking algorithm, and relying on manual correction toadd/remove/change bounding boxes on each frame, as well as resolving errors inthe associations over frames (track switches). In this paper, we propose a datacorrection pipeline to generate ground-truth data more efficiently in thissemi-automated scenario. Our method simplifies the trajectories from thetracking systems and let the annotator verify and correct the objects in thesampled keyframes. Once the objects in the keyframes are corrected, thebounding boxes in the other frames are obtained by interpolation. Our methodachieves substantial reduction in the number of frames requiring manualcorrection. In the MOT dataset, it reduces the number of frames by 30x whilemaintaining a HOTA score of 89.61% . Moreover, it reduces the number of framesby a factor of 10x while achieving a HOTA score of 79.24% in the SoccerNetdataset, and 85.79% in the DanceTrack dataset. The project code and data arepublicly released at", "output": "A Scale-Invariant Trajectory Simplification Method for Efficient Data Collection in Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model-based deep learning methods that combine imaging physics with learnedregularization priors have been emerging as powerful tools for parallel MRIacceleration. The main focus of this paper is to determine the utility of themonotone operator learning (MOL) framework in the parallel MRI setting. The MOLalgorithm alternates between a gradient descent step using a monotoneconvolutional neural network (CNN) and a conjugate gradient algorithm toencourage data consistency. The benefits of this approach include similarguarantees as compressive sensing algorithms including uniqueness, convergence,and stability, while being significantly more memory efficient than unrolledmethods. We validate the proposed scheme by comparing it with differentunrolled algorithms in the context of accelerated parallel MRI for static anddynamic settings.", "output": "Accelerated parallel MRI using memory efficient and robust monotone operator learning (MOL)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work investigates the unexplored usability of self-supervisedrepresentation learning in the direction of functional knowledge transfer. Inthis work, functional knowledge transfer is achieved by joint optimization ofself-supervised learning pseudo task and supervised learning task, improvingsupervised learning task performance. Recent progress in self-supervisedlearning uses a large volume of data, which becomes a constraint for itsapplications on small-scale datasets. This work shares a simple yet effectivejoint training framework that reinforces human-supervised task learning bylearning self-supervised representations just-in-time and vice versa.Experiments on three public datasets from different visual domains, IntelImage, CIFAR, and APTOS, reveal a consistent track of performance improvementson classification tasks during joint optimization. Qualitative analysis alsosupports the robustness of learnt representations. Source code and trainedmodels are available on GitHub.", "output": "Functional Knowledge Transfer with Self-supervised Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gaussian Mixture Models (GMM) do not adapt well to curved and stronglynonlinear data. However, we can use Gaussians in the curvilinear coordinatesystems to solve this problem. Moreover, such a solution allows for theadaptation of clusters to the complicated shapes defined by the family offunctions. But still, it is challenging to model clusters as closed curves(e.g., circles, ellipses, etc.). In this work, we propose a densityrepresentation of the closed curve, which can be used to detect the complicatedtemplates in the data. For this purpose, we define a new probabilitydistribution to model closed curves. Then we construct a mixture of suchdistributions and show that it can be effectively trained in the case of theone-dimensional closed curves.", "output": "Gaussian model for closed curves."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating the pose of an unseen object is the goal of the challengingone-shot pose estimation task. Previous methods have heavily relied on featurematching with great success. However, these methods are often inefficient andlimited by their reliance on pre-trained models that have not be designedspecifically for pose estimation. In this paper we propose PoseMatcher, anaccurate model free one-shot object pose estimator that overcomes theselimitations. We create a new training pipeline for object to image matchingbased on a three-view system: a query with a positive and negative templates.This simple yet effective approach emulates test time scenarios by cheaplyconstructing an approximation of the full object point cloud during training.To enable PoseMatcher to attend to distinct input modalities, an image and apointcloud, we introduce IO-Layer, a new attention layer that efficientlyaccommodates self and cross attention between the inputs. Moreover, we proposea pruning strategy where we iteratively remove redundant regions of the targetobject to further reduce the complexity and noise of the network whilemaintaining accuracy. Finally we redesign commonly used pose refinementstrategies, zoom and 2D offset refinements, and adapt them to the one-shotparadigm. We outperform all prior one-shot pose estimation methods on theLinemod and YCB-V datasets as well achieve results rivaling recentinstance-level methods. The source code and models are available at", "output": "PoseMatcher: One-shot 6D Object Pose Estimation by Deep Feature Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State estimation of oncoming vehicles: Earlier research has been based ondetermining states like position, velocity, orientation , angular velocity, etcof ego-vehicle. Our approach focuses on estimating the states of non-egovehicles which is crucial for Motion planning and decision-making. DynamicScene Based Localization: Our project will work on dynamic scenes like movingego (self) and non-ego vehicles. Previous methods were focused on staticenvironments.", "output": "Lidar based 3D Tracking and State Estimation of Dynamic Objects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a CNN fine-tuning method which enables users togive simultaneous feedback on two outputs: the classification itself and thevisual explanation for the classification. We present the effect of thisfeedback strategy in a skin lesion classification task and measure how CNNsreact to the two types of user feedback. To implement this approach, we proposea novel CNN architecture that integrates the Grad-CAM technique for explainingthe model's decision in the training loop. Using simulated user feedback, wefound that fine-tuning our model on both classification and explanationimproves visual explanation while preserving classification accuracy, thuspotentially increasing the trust of users in using CNN-based skin lesionclassifiers.", "output": "Fine-tuning of explainable CNNs for skin lesion classification based on dermatologists' feedback towards increasing trust."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The combination of the U-Net based deep learning models and Transformer is anew trend for medical image segmentation. U-Net can extract the detailed localsemantic and texture information and Transformer can learn the long-rangdependencies among pixels in the input image. However, directly adapting theTransformer for segmentation has ``token-flatten\" problem (flattens the localpatches into 1D tokens which losses the interaction among pixels within localpatches) and ``scale-sensitivity\" problem (uses a fixed scale to split theinput image into local patches). Compared to directly combining U-Net andTransformer, we propose a new global-local fashion combination of U-Net andTransformer, named U-Netmer, to solve the two problems. The proposed U-Netmersplits an input image into local patches. The global-context information amonglocal patches is learnt by the self-attention mechanism in Transformer andU-Net segments each local patch instead of flattening into tokens to solve the`token-flatten\" problem. The U-Netmer can segment the input image withdifferent patch sizes with the identical structure and the same parameter.Thus, the U-Netmer can be trained with different patch sizes to solve the``scale-sensitivity\" problem. We conduct extensive experiments in 7 publicdatasets on 7 organs (brain, heart, breast, lung, polyp, pancreas and prostate)and 4 imaging modalities (MRI, CT, ultrasound, and endoscopy) to show that theproposed U-Netmer can be generally applied to improve accuracy of medical imagesegmentation. These experimental results show that U-Netmer providesstate-of-the-art performance compared to baselines and other models. Inaddition, the discrepancy among the outputs of U-Netmer with different scalesis linearly correlated to the segmentation accuracy which can be considered asa confidence score to rank test images by difficulty without ground-truth.", "output": "U-Netmer: U-Net meets Transformer for medical image segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a method to segment the visual field into independently movingregions, trained with no ground truth or supervision. It consists of anadversarial conditional encoder-decoder architecture based on Slot Attention,modified to use the image as context to decode optical flow without attemptingto reconstruct the image itself. In the resulting multi-modal representation,one modality (flow) feeds the encoder to produce separate latent codes (slots),whereas the other modality (image) conditions the decoder to generate the first(flow) from the slots. This design frees the representation from having toencode complex nuisance variability in the image due to, for instance,illumination and reflectance properties of the scene. Since customaryautoencoding based on minimizing the reconstruction error does not preclude theentire flow from being encoded into a single slot, we modify the loss to anadversarial criterion based on Contextual Information Separation. The resultingmin-max optimization fosters the separation of objects and their assignment todifferent attention slots, leading to Divided Attention, or DivA. DivAoutperforms recent unsupervised multi-object motion segmentation methods whiletripling run-time speed up to 104FPS and reducing the performance gap fromsupervised methods to 12% or less. DivA can handle different numbers of objectsand different image sizes at training and test time, is invariant topermutation of object labels, and does not require explicit regularization.", "output": "Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since the introduction of deep learning, a wide scope of representationproperties, such as decorrelation, whitening, disentanglement, rank, isotropy,and mutual information, have been studied to improve the quality ofrepresentation. However, manipulating such properties can be challenging interms of implementational effectiveness and general applicability. To addressthese limitations, we propose to regularize von Neumann entropy~(VNE) ofrepresentation. First, we demonstrate that the mathematical formulation of VNEis superior in effectively manipulating the eigenvalues of the representationautocorrelation matrix. Then, we demonstrate that it is widely applicable inimproving state-of-the-art algorithms or popular benchmark algorithms byinvestigating domain-generalization, meta-learning, self-supervised learning,and generative models. In addition, we formally establish theoreticalconnections with rank, disentanglement, and isotropy of representation.Finally, we provide discussions on the dimension control of VNE and therelationship with Shannon entropy. Code is available at:", "output": "VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a method to learn a high-quality implicit 3D head avatar from amonocular RGB video captured in the wild. The learnt avatar is driven by aparametric face model to achieve user-controlled facial expressions and headposes. Our hybrid pipeline combines the geometry prior and dynamic tracking ofa 3DMM with a neural radiance field to achieve fine-grained control andphotorealism. To reduce over-smoothing and improve out-of-model expressionssynthesis, we propose to predict local features anchored on the 3DMM geometry.These learnt features are driven by 3DMM deformation and interpolated in 3Dspace to yield the volumetric radiance at a designated query point. We furthershow that using a Convolutional Neural Network in the UV space is critical inincorporating spatial context and producing representative local features.Extensive experiments show that we are able to reconstruct high-qualityavatars, with more accurate expression-dependent details, good generalizationto out-of-training expressions, and quantitatively superior renderings comparedto other state-of-the-art approaches.", "output": "Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video compression plays a significant role in IoT devices for the efficienttransport of visual data while satisfying all underlying bandwidth constraints.Deep learning-based video compression methods are rapidly replacing traditionalalgorithms and providing state-of-the-art results on edge devices. However,recently developed adversarial attacks demonstrate that digitally craftedperturbations can break the Rate-Distortion relationship of video compression.In this work, we present a real-world LED attack to target video compressionframeworks. Our physically realizable attack, dubbed NetFlick, can degrade thespatio-temporal correlation between successive frames by injecting flickeringtemporal perturbations. In addition, we propose universal perturbations thatcan downgrade performance of incoming video without prior knowledge of thecontents. Experimental results demonstrate that NetFlick can successfullydeteriorate the performance of video compression frameworks in both digital-and physical-settings and can be further extended to attack downstream videoclassification networks.", "output": "NetFlick: Adversarial Flickering Attacks on Deep Learning Based Video Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Metaverse through VR headsets is a rapidly growing concept, but the highcost of entry currently limits access for many users. This project aims toprovide an accessible entry point to the immersive Metaverse experience byleveraging web technologies. The platform developed allows users to engage withrendered avatars using only a web browser, microphone, and webcam. By employingthe WebGL and MediaPipe face tracking AI model from Google, the applicationgenerates real-time 3D face meshes for users. It uses a client-to-clientstreaming cluster to establish a connection, and clients negotiate SRTPprotocol through WebRTC for direct data streaming. Additionally, the projectaddresses backend challenges through an architecture that is serverless,distributive, auto-scaling, highly resilient, and secure. The platform offers ascalable, hardware-free solution for users to experience a near-immersiveMetaverse, with the potential for future integration with game server clusters.This project provides an important step toward a more inclusive Metaverseaccessible to a wider audience.", "output": "Virtual Avatar Stream: a cost-down approach to the Metaverse experience."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to its significant capability of modeling long-range dependencies, visiontransformer (ViT) has achieved promising success in both holistic and occludedperson re-identification (Re-ID) tasks. However, the inherent problems oftransformers such as the huge computational cost and memory footprint are stilltwo unsolved issues that will block the deployment of ViT based person Re-IDmodels on resource-limited edge devices. Our goal is to reduce both theinference complexity and model size without sacrificing the comparable accuracyon person Re-ID, especially for tasks with occlusion. To this end, we propose anovel attention map guided (AMG) transformer pruning method, which removes bothredundant tokens and heads with the guidance of the attention map in ahardware-friendly way. We first calculate the entropy in the key dimension andsum it up for the whole map, and the corresponding head parameters of maps withhigh entropy will be removed for model size reduction. Then we combine thesimilarity and first-order gradients of key tokens along the query dimensionfor token importance estimation and remove redundant key and value tokens tofurther reduce the inference complexity. Comprehensive experiments on OccludedDukeMTMC and Market-1501 demonstrate the effectiveness of our proposals. Forexample, our proposed pruning strategy on ViT-Base enjoystextup{textbf{29.4%}} textup{textbf{FLOPs}} savings withtextup{textbf{0.2%}} drop on Rank-1 and textup{textbf{0.4%}} improvementon mAP, respectively.", "output": "Attention Map Guided Transformer Pruning for Edge Device."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-Language models (VLMs) that use contrastive language-imagepre-training have shown promising zero-shot classification performance.However, their performance on imbalanced dataset is relatively poor, where thedistribution of classes in the training dataset is skewed, leading to poorperformance in predicting minority classes. For instance, CLIP achieved only 5%accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoderto VLMs to avoid OOM (out of memory) problem caused by large number of classesand capture nuanced features for tail classes. Then, we explore improvements ofVLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithmssuch as Focal Loss, Balanced SoftMax and Distribution Alignment. Experimentsdemonstrate that the performance of VLMs can be further boosted when used withdecoder and imbalanced methods. Specifically, our improved VLMs significantlyoutperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,and 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. Wefurther analyze the influence of pre-training data size, backbones, andtraining cost. Our study highlights the significance of imbalanced learningalgorithms in face of VLMs pre-trained by huge data. We release our code at", "output": "Exploring Vision-Language Models for Imbalanced Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art 3D object detectors are usually trained on large-scaledatasets with high-quality 3D annotations. However, such 3D annotations areoften expensive and time-consuming, which may not be practical for realapplications. A natural remedy is to adopt semi-supervised learning (SSL) byleveraging a limited amount of labeled samples and abundant unlabeled samples.Current pseudolabeling-based SSL object detection methods mainly adopt ateacher-student framework, with a single fixed threshold strategy to generatesupervision signals, which inevitably brings confused supervision when guidingthe student network training. Besides, the data augmentation of the point cloudin the typical teacher-student framework is too weak, and only contains basicdown sampling and flip-and-shift (i.e., rotate and scaling), which hinders theeffective learning of feature information. Hence, we address these issues byintroducing a novel approach of Hierarchical Supervision and Shuffle DataAugmentation (HSSDA), which is a simple yet effective teacher-studentframework. The teacher network generates more reasonable supervision for thestudent network by designing a dynamic dual-threshold strategy. Besides, theshuffle data augmentation strategy is designed to strengthen the featurerepresentation ability of the student network. Extensive experiments show thatHSSDA consistently outperforms the recent state-of-the-art methods on differentdatasets. The code will be released at ", "output": "Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated brain tumor segmentation based on deep learning (DL) has achievedpromising performance. However, it generally relies on annotated images formodel training, which is not always feasible in clinical settings. Therefore,the development of unsupervised DL-based brain tumor segmentation approacheswithout expert annotations is desired. Motivated by the success of promptlearning (PL) in natural language processing, we propose an approach tounsupervised brain tumor segmentation by designing image-based prompts thatallow indication of brain tumors, and this approach is dubbed as PL-based BrainTumor Segmentation (PL-BTS). Specifically, instead of directly training a modelfor brain tumor segmentation with a large amount of annotated data, we seek totrain a model that can answer the question: is a voxel in the input imageassociated with tumor-like hyper-/hypo-intensity? Such a model can be trainedby artificially generating tumor-like hyper-/hypo-intensity on images withouttumors with hand-crafted designs. Since the hand-crafted designs may be toosimplistic to represent all kinds of real tumors, the trained model may overfitthe simplistic hand-crafted task rather than actually answer the question ofabnormality. To address this problem, we propose the use of a validation task,where we generate a different hand-crafted task to monitor overfitting. Inaddition, we propose PL-BTS+ that further improves PL-BTS by exploitingunannotated images with brain tumors. Compared with competing unsupervisedmethods, the proposed method has achieved marked improvements on both publicand in-house datasets, and we have also demonstrated its possible extension toother brain lesion segmentation tasks.", "output": "Unsupervised Brain Tumor Segmentation with Image-based Prompts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works on 3D reconstruction from posed images have demonstrated thatdirect inference of scene-level 3D geometry without iterative optimization isfeasible using a deep neural network, showing remarkable promise and highefficiency. However, the reconstructed geometries, typically represented as a3D truncated signed distance function (TSDF), are often coarse without finegeometric details. To address this problem, we propose three effectivesolutions for improving the fidelity of inference-based 3D reconstructions. Wefirst present a resolution-agnostic TSDF supervision strategy to provide thenetwork with a more accurate learning signal during training, avoiding thepitfalls of TSDF interpolation seen in previous work. We then introduce a depthguidance strategy using multi-view depth estimates to enhance the scenerepresentation and recover more accurate surfaces. Finally, we develop a novelarchitecture for the final layers of the network, conditioning the output TSDFprediction on high-resolution image features in addition to coarse voxelfeatures, enabling sharper reconstruction of fine details. Our method producessmooth and highly accurate reconstructions, showing significant improvementsacross multiple depth and 3D reconstruction metrics.", "output": "FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, self-supervised learning (SSL) was shown to be vulnerable topatch-based data poisoning backdoor attacks. It was shown that an adversary canpoison a small part of the unlabeled data so that when a victim trains an SSLmodel on it, the final model will have a backdoor that the adversary canexploit. This work aims to defend self-supervised learning against suchattacks. We use a three-step defense pipeline, where we first train a model onthe poisoned data. In the second step, our proposed defense algorithm(PatchSearch) uses the trained model to search the training data for poisonedsamples and removes them from the training set. In the third step, a finalmodel is trained on the cleaned-up training set. Our results show thatPatchSearch is an effective defense. As an example, it improves a model'saccuracy on images containing the trigger from 38.2% to 63.7% which is veryclose to the clean model's accuracy, 64.6%. Moreover, we show that PatchSearchoutperforms baselines and state-of-the-art defense approaches including thoseusing additional clean, trusted data. Our code is available at", "output": "Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training a convolutional neural network (CNN) to detect infrared smalltargets in a fully supervised manner has gained remarkable research interestsin recent years, but is highly labor expensive since a large number ofper-pixel annotations are required. To handle this problem, in this paper, wemake the first attempt to achieve infrared small target detection withpoint-level supervision. Interestingly, during the training phase supervised bypoint labels, we discover that CNNs first learn to segment a cluster of pixelsnear the targets, and then gradually converge to predict groundtruth pointlabels. Motivated by this \"mapping degeneration\" phenomenon, we propose a labelevolution framework named label evolution with single point supervision (LESPS)to progressively expand the point label by leveraging the intermediatepredictions of CNNs. In this way, the network predictions can finallyapproximate the updated pseudo labels, and a pixel-level target mask can beobtained to train CNNs in an end-to-end manner. We conduct extensiveexperiments with insightful visualizations to validate the effectiveness of ourmethod. Experimental results show that CNNs equipped with LESPS can wellrecover the target masks from corresponding point labels, {and can achieve over70% and 95% of their fully supervised performance in terms of pixel-levelintersection over union (IoU) and object-level probability of detection (Pd),respectively. Code is available at ", "output": "Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection with Single Point Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In order to plan rapid response during disasters, first responder agenciesoften adopt `bring your own device' (BYOD) model with inexpensive mobile edgedevices (e.g., drones, robots, tablets) for complex video analyticsapplications, e.g., 3D reconstruction of a disaster scene. Unlike simpler videoapplications, widely used Multi-view Stereo (MVS) based 3D reconstructionapplications (e.g., openMVG/openMVS) are exceedingly time consuming, especiallywhen run on such computationally constrained mobile edge devices. Additionally,reducing the reconstruction latency of such inherently sequential algorithms ischallenging as unintelligent, application-agnostic strategies can drasticallydegrade the reconstruction (i.e., application outcome) quality making themuseless. In this paper, we aim to design a latency optimized MVS algorithmpipeline, with the objective to best balance the end-to-end latency andreconstruction quality by running the pipeline on a collaborative mobile edgeenvironment. The overall optimization approach is two-pronged where: (a)application optimizations introduce data-level parallelism by splitting thepipeline into high frequency and low frequency reconstruction components and(b) system optimizations incorporate task-level parallelism to the pipelines byrunning them opportunistically on available resources with online qualitycontrol in order to balance both latency and quality. Our evaluation on ahardware testbed using publicly available datasets shows upto ~54% reduction inlatency with negligible loss (~4-7%) in reconstruction quality.", "output": "End-to-End Latency Optimization of Multi-view 3D Reconstruction for Disaster Response."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fine-tuning a pre-trained model can leverage the semantic information fromlarge-scale pre-training data and mitigate the over-fitting problem ondownstream tasks with limited training examples. While the problem ofcatastrophic forgetting in backbone has been extensively studied, the potentialbias existing in a pre-trained model due to the corresponding pre-training taskand data, attracts less attention. In this work, we investigate this problem bydemonstrating that the obtained classifier after fine-tuning will be close tothat induced by the pre-trained model. To reduce the bias in the classifiereffectively, we introduce a reference distribution obtained from a fixed textclassifier, which can help regularize the learned vision classifier. Theproposed method, Text Supervised fine-tuning (TeS), is evaluated with diversepre-trained vision models including ResNet and ViT, and text encoders includingBERT and CLIP, on 11 downstream tasks. The consistent improvement with a clearmargin over distinct scenarios confirms the effectiveness of our proposal.", "output": "Improved Visual Fine-tuning with Natural Language Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Noise removal of images is an essential preprocessing procedure for manycomputer vision tasks. Currently, many denoising models based on deep neuralnetworks can perform well in removing the noise with known distributions (i.e.the additive Gaussian white noise). However eliminating real noise is still avery challenging task, since real-world noise often does not simply follow onesingle type of distribution, and the noise may spatially vary. In this paper,we present a new dual convolutional neural network (CNN) with attention forimage blind denoising, named as the DCANet. To the best of our knowledge, theproposed DCANet is the first work that integrates both the dual CNN andattention mechanism for image denoising. The DCANet is composed of a noiseestimation network, a spatial and channel attention module (SCAM), and a CNNwith a dual structure. The noise estimation network is utilized to estimate thespatial distribution and the noise level in an image. The noisy image and itsestimated noise are combined as the input of the SCAM, and a dual CNN containstwo different branches is designed to learn the complementary features toobtain the denoised image. The experimental results have verified that theproposed DCANet can suppress both synthetic and real noise effectively. Thecode of DCANet is available at ", "output": "DCANet: Dual Convolutional Neural Network with Attention for Image Blind Denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Skin lesion recognition using deep learning has made remarkable progress, andthere is an increasing need for deploying these systems in real-worldscenarios. However, recent research has revealed that deep neural networks forskin lesion recognition may overly depend on disease-irrelevant image artifacts(i.e. dark corners, dense hairs), leading to poor generalization in unseenenvironments. To address this issue, we propose a novel domain generalizationmethod called EPVT, which involves embedding prompts into the visiontransformer to collaboratively learn knowledge from diverse domains.Concretely, EPVT leverages a set of domain prompts, each of which plays as adomain expert, to capture domain-specific knowledge; and a shared prompt forgeneral knowledge over the entire dataset. To facilitate knowledge sharing andthe interaction of different prompts, we introduce a domain prompt generatorthat enables low-rank multiplicative updates between domain prompts and theshared prompt. A domain mixup strategy is additionally devised to reduce theco-occurring artifacts in each domain, which allows for more flexible decisionmargins and mitigates the issue of incorrectly assigned domain labels.Experiments on four out-of-distribution datasets and six different biased ISICdatasets demonstrate the superior generalization ability of EPVT in skin lesionrecognition across various environments. Our code and dataset will be releasedat ", "output": "EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning-based outlier (mismatched correspondence) rejection for robust 3Dregistration generally formulates the outlier removal as an inlier/outlierclassification problem. The core for this to be successful is to learn thediscriminative inlier/outlier feature representations. In this paper, wedevelop a novel variational non-local network-based outlier rejection frameworkfor robust alignment. By reformulating the non-local feature learning withvariational Bayesian inference, the Bayesian-driven long-range dependencies canbe modeled to aggregate discriminative geometric context information forinlier/outlier distinction. Specifically, to achieve such Bayesian-drivencontextual dependencies, each query/key/value component in our non-localnetwork predicts a prior feature distribution and a posterior one. Embeddedwith the inlier/outlier label, the posterior feature distribution islabel-dependent and discriminative. Thus, pushing the prior to be close to thediscriminative posterior in the training step enables the features sampled fromthis prior at test time to model high-quality long-range dependencies. Notably,to achieve effective posterior feature guidance, a specific probabilisticgraphical model is designed over our non-local model, which lets us derive avariational low bound as our optimization objective for model training.Finally, we propose a voting-based inlier searching strategy to cluster thehigh-quality hypothetical inliers for transformation estimation. Extensiveexperiments on 3DMatch, 3DLoMatch, and KITTI datasets verify the effectivenessof our method.", "output": "Robust Outlier Rejection for 3D Registration with Variational Bayes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Token-based masked generative models are gaining popularity for their fastinference time with parallel decoding. While recent token-based approachesachieve competitive performance to diffusion-based models, their generationperformance is still suboptimal as they sample multiple tokens simultaneouslywithout considering the dependence among them. We empirically investigate thisproblem and propose a learnable sampling model, Text-Conditioned TokenSelection (TCTS), to select optimal tokens via localized supervision with textinformation. TCTS improves not only the image quality but also the semanticalignment of the generated images with the given texts. To further improve theimage quality, we introduce a cohesive sampling strategy, Frequency AdaptiveSampling (FAS), to each group of tokens divided according to the self-attentionmaps. We validate the efficacy of TCTS combined with FAS with variousgenerative tasks, demonstrating that it significantly outperforms the baselinesin image-text alignment and image quality. Our text-conditioned samplingframework further reduces the original inference time by more than 50% withoutmodifying the original generative model.", "output": "Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bird's-Eye View (BEV) features are popular intermediate scene representationsshared by the 3D backbone and the detector head in LiDAR-based objectdetectors. However, little research has been done to investigate how toincorporate additional supervision on the BEV features to improve proposalgeneration in the detector head, while still balancing the number of powerful3D layers and efficient 2D network operations. This paper proposes a novelscene representation that encodes both the semantics and geometry of the 3Denvironment in 2D, which serves as a dense supervision signal for better BEVfeature learning. The key idea is to use auxiliary networks to predict acombination of explicit and implicit semantic probabilities by exploiting theircomplementary properties. Extensive experiments show that our simple yeteffective design can be easily integrated into most state-of-the-art 3D objectdetectors and consistently improves upon baseline models.", "output": "LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Marine ecosystems are vital for the planet's health, but human activitiessuch as climate change, pollution, and overfishing pose a constant threat tomarine species. Accurate classification and monitoring of these species can aidin understanding their distribution, population dynamics, and the impact ofhuman activities on them. However, classifying marine species can bechallenging due to their vast diversity and the complex underwater environment.With advancements in computer performance and GPU-based computing,deep-learning algorithms can now efficiently classify marine species, making iteasier to monitor and manage marine ecosystems. In this paper, we propose anoptimization to the MobileNetV2 model to achieve a 99.83% average validationaccuracy by highlighting specific guidelines for creating a dataset andaugmenting marine species images. This transfer learning algorithm can bedeployed successfully on a mobile application for on-site classification atfisheries.", "output": "FisHook -- An Optimized Approach to Marine Specie Classification using MobileNetV2."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The quality of point clouds is often limited by noise introduced during theircapture process. Consequently, a fundamental 3D vision task is the removal ofnoise, known as point cloud filtering or denoising. State-of-the-art learningbased methods focus on training neural networks to infer filtered displacementsand directly shift noisy points onto the underlying clean surfaces. In highnoise conditions, they iterate the filtering process. However, this iterativefiltering is only done at test time and is less effective at ensuring pointsconverge quickly onto the clean surfaces. We propose IterativePFN (iterativepoint cloud filtering network), which consists of multiple IterationModulesthat model the true iterative filtering process internally, within a singlenetwork. We train our IterativePFN network using a novel loss function thatutilizes an adaptive ground truth target at each iteration to capture therelationship between intermediate filtering results during training. Thisensures that the filtered results converge faster to the clean surfaces. Ourmethod is able to obtain better performance compared to state-of-the-artmethods. The source code can be found at:", "output": "IterativePFN: True Iterative Point Cloud Filtering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bird's eye view (BEV) perception is becoming increasingly important in thefield of autonomous driving. It uses multi-view camera data to learn atransformer model that directly projects the perception of the road environmentonto the BEV perspective. However, training a transformer model often requiresa large amount of data, and as camera data for road traffic is often private,it is typically not shared. Federated learning offers a solution that enablesclients to collaborate and train models without exchanging data. In this paper,we propose FedBEVT, a federated transformer learning approach for BEVperception. We address two common data heterogeneity issues in FedBEVT: (i)diverse sensor poses and (ii) varying sensor numbers in perception systems. Wepresent federated learning with camera-attentive personalization~(FedCaP) andadaptive multi-camera masking~(AMCM) to enhance the performance in real-worldscenarios. To evaluate our method in real-world settings, we create a datasetconsisting of four typical federated use cases. Our findings suggest thatFedBEVT outperforms the baseline approaches in all four use cases,demonstrating the potential of our approach for improving BEV perception inautonomous driving. We will make all codes and data publicly available.", "output": "FedBEVT: Federated Learning Bird's Eye View Perception Transformer in Road Traffic Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern data augmentation using a mixture-based technique can regularize themodels from overfitting to the training data in various computer visionapplications, but a proper data augmentation technique tailored for thepart-based Visible-Infrared person Re-IDentification (VI-ReID) models remainsunexplored. In this paper, we present a novel data augmentation technique,dubbed PartMix, that synthesizes the augmented samples by mixing the partdescriptors across the modalities to improve the performance of part-basedVI-ReID models. Especially, we synthesize the positive and negative sampleswithin the same and across different identities and regularize the backbonemodel through contrastive learning. In addition, we also present anentropy-based mining strategy to weaken the adverse impact of unreliablepositive and negative samples. When incorporated into existing part-basedVI-ReID model, PartMix consistently boosts the performance. We conductexperiments to demonstrate the effectiveness of our PartMix over the existingVI-ReID methods and provide ablation studies.", "output": "PartMix: Regularization Strategy to Learn Part Discovery for Visible-Infrared Person Re-identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model-agnostic meta-learning (MAML) is one of the most successfulmeta-learning algorithms. It has a bi-level optimization structure where theouter-loop process learns a shared initialization and the inner-loop processoptimizes task-specific weights. Although MAML relies on the standard gradientdescent in the inner-loop, recent studies have shown that controlling theinner-loop's gradient descent with a meta-learned preconditioner can bebeneficial. Existing preconditioners, however, cannot simultaneously adapt in atask-specific and path-dependent way. Additionally, they do not satisfy theRiemannian metric condition, which can enable the steepest descent learningwith preconditioned gradient. In this study, we propose Geometry-AdaptivePreconditioned gradient descent (GAP) that can overcome the limitations inMAML; GAP can efficiently meta-learn a preconditioner that is dependent ontask-specific parameters, and its preconditioner can be shown to be aRiemannian metric. Thanks to the two properties, the geometry-adaptivepreconditioner is effective for improving the inner-loop optimization.Experiment results show that GAP outperforms the state-of-the-art MAML familyand preconditioned gradient descent-MAML (PGD-MAML) family in a variety offew-shot learning tasks. Code is available at:", "output": "Meta-Learning with a Geometry-Adaptive Preconditioner."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised domain adaptation (UDA) addresses the problem of distributionshift between the unlabeled target domain and labelled source domain. While thesingle target domain adaptation (STDA) is well studied in both 2D and 3D visionliterature, multi-target domain adaptation (MTDA) is barely explored for 3Ddata despite its wide real-world applications such as autonomous drivingsystems for various geographical and climatic conditions. We establish an MTDAbaseline for 3D point cloud data by proposing to mix the featurerepresentations from all domains together to achieve better domain adaptationperformance by an ensemble average, which we call emph{{bf M}ixup {bfEns}emble {bf A}verage} or {bf emph{MEnsA}}. With the mixed representation,we use a domain classifier to improve at distinguishing the featurerepresentations of source domain from those of target domains in a sharedlatent space. In extensive empirical validations on the challenging PointDA-10dataset, we showcase a clear benefit of our simple method over previousunsupervised STDA and MTDA methods by large margins (up to $17.10%$ and$4.76%$ on averaged over all domain shifts). We make the code publiclyavailablehref{", "output": "\\emph{MEnsA}: Mix-up Ensemble Average for Unsupervised Multi Target Domain Adaptation on 3D Point Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As road accident cases are increasing due to the inattention of the driver,automated driver monitoring systems (DMS) have gained an increase inacceptance. In this report, we present a real-time DMS system that runs on ahardware-accelerator-based edge device. The system consists of an InfraRedcamera to record the driver footage and an edge device to process the data. Tosuccessfully port the deep learning models to run on the edge device takingfull advantage of the hardware accelerators, model surgery was performed. Thefinal DMS system achieves 63 frames per second (FPS) on the TI-TDA4VM edgedevice.", "output": "Real-time Driver Monitoring Systems on Edge AI Device."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have become a new SOTA generative modeling method in variousfields, for which there are multiple survey works that provide an overallsurvey. With the number of articles on diffusion models increasingexponentially in the past few years, there is an increasing need for surveys ofdiffusion models on specific fields. In this work, we are committed toconducting a survey on the graph diffusion models. Even though our focus is tocover the progress of diffusion models in graphs, we first briefly summarizehow other generative modeling methods are used for graphs. After that, weintroduce the mechanism of diffusion models in various forms, which facilitatesthe discussion on the graph diffusion models. The applications of graphdiffusion models mainly fall into the category of AI-generated content (AIGC)in science, for which we mainly focus on how graph diffusion models areutilized for generating molecules and proteins but also cover other cases,including materials design. Moreover, we discuss the issue of evaluatingdiffusion models in the graph domain and the existing challenges.", "output": "A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monitoring the movement and actions of humans in video in real-time is animportant task. We present a deep learning based algorithm for human actionrecognition for both RGB and thermal cameras. It is able to detect and trackhumans and recognize four basic actions (standing, walking, running, lying) inreal-time on a notebook with a NVIDIA GPU. For this, it combines state of theart components for object detection (Scaled YoloV4), optical flow (RAFT) andpose estimation (EvoSkeleton). Qualitative experiments on a set of tunnelvideos show that the proposed algorithm works robustly for both RGB and thermalvideo.", "output": "A real-time algorithm for human action recognition in RGB and thermal video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate lung nodule segmentation is crucial for early-stage lung cancerdiagnosis, as it can substantially enhance patient survival rates. Computedtomography (CT) images are widely employed for early diagnosis in lung noduleanalysis. However, the heterogeneity of lung nodules, size diversity, and thecomplexity of the surrounding environment pose challenges for developing robustnodule segmentation methods. In this study, we propose an efficient end-to-endframework, the multi-encoder-based self-adaptive hard attention network(MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Netcomprises three encoding paths, an attention block, and a decoder block,facilitating the integration of three types of inputs: CT slice patches,forward and backward maximum intensity projection (MIP) images, and region ofinterest (ROI) masks encompassing the nodule. By employing a novel adaptivehard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2Dsegmentation of lung nodules, focusing on the nodule region in each slice togenerate 3D volumetric segmentation of lung nodules. The proposed framework hasbeen comprehensively evaluated on the LIDC-IDRI dataset, the largest publiclyavailable dataset for lung nodule segmentation. The results demonstrate thatour approach is highly robust for various lung nodule types, outperformingprevious state-of-the-art techniques in terms of segmentation accuracy andcomputational complexity, rendering it suitable for real-time clinicalimplementation.", "output": "MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A biometric recognition system can operate in two distinct modes,identification or verification. In the first mode, the system recognizes anindividual by searching the enrolled templates of all the users for a match. Inthe second mode, the system validates a claimed identity by comparing the freshtemplate with the enrolled template for this identity. Both the experimentallydetermined false match rate and false non-match rate through recognitionthreshold adjustment define the recognition accuracy, and hence the security ofthe system. The biometric transformation schemes usually produce binarytemplates that are better handled by cryptographic schemes. One of therequirements for these transformation schemes is their irreversibility. In thiswork, we rely on probabilistic modelling to quantify the security strength ofbinary templates. We investigate the influence of template size, database sizeand threshold on the probability of having a near-collision, and we highlighttwo attacks on biometric systems. We discuss the choice of parameters throughthe generic presented attacks.", "output": "Untargeted Near-collision Attacks in Biometric Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With autonomous aerial vehicles enacting safety-critical missions, such asthe Mars Science Laboratory Curiosity rover's landing on Mars, the tasks ofautomatically identifying and reasoning about potentially hazardous landingsites is paramount. This paper presents a coupled perception-planning solutionwhich addresses the hazard detection, optimal landing trajectory generation,and contingency planning challenges encountered when landing in uncertainenvironments. Specifically, we develop and combine two novel algorithms,Hazard-Aware Landing Site Selection (HALSS) and Adaptive Deferred-DecisionTrajectory Optimization (Adaptive-DDTO), to address the perception and planningchallenges, respectively. The HALSS framework processes point cloud informationto identify feasible safe landing zones, while Adaptive-DDTO is a multi-targetcontingency planner that adaptively replans as new perception information isreceived. We demonstrate the efficacy of our approach using a simulated Martianenvironment and show that our coupled perception-planning method achievesgreater landing success whilst being more fuel efficient compared to anonadaptive DDTO approach.", "output": "HALO: Hazard-Aware Landing Optimization for Autonomous Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-channel time-series datasets are popular in the context of humanactivity recognition (HAR). On-body device (OBD) recordings of human movementsare often preferred for HAR applications not only for their reliability but asan approach for identity protection, e.g., in industrial settings.Contradictory, the gait activity is a biometric, as the cyclic movement isdistinctive and collectable. In addition, the gait cycle has proven to containsoft-biometric information of human groups, such as age and height. Thoughgeneral human movements have not been considered a biometric, they mightcontain identity information. This work investigates person and soft-biometricsidentification from OBD recordings of humans performing different activitiesusing deep architectures. Furthermore, we propose the use of attributerepresentation for soft-biometric identification. We evaluate the method onfour datasets of multi-channel time-series HAR, measuring the performance of aperson and soft-biometrics identification and its relation concerning performedactivities. We find that person identification is not limited to gait activity.The impact of activities on the identification performance was found to betraining and dataset specific. Soft-biometric based attribute representationshows promising results and emphasis the necessity of larger datasets.", "output": "Multi-Channel Time-Series Person and Soft-Biometric Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in deep learning have been pushing image denoising techniquesto a new level. In self-supervised image denoising, blind-spot network (BSN) isone of the most common methods. However, most of the existing BSN algorithmsuse a dot-based central mask, which is recognized as inefficient for imageswith large-scale spatially correlated noise. In this paper, we give thedefinition of large-noise and propose a multi-mask strategy using multipleconvolutional kernels masked in different shapes to further break the noisespatial correlation. Furthermore, we propose a novel self-supervised imagedenoising method that combines the multi-mask strategy with BSN (MM-BSN). Weshow that different masks can cause significant performance differences, andthe proposed MM-BSN can efficiently fuse the features extracted by multi-maskedlayers, while recovering the texture structures destroyed by multi-masking andinformation transmission. Our MM-BSN can be used to address the problem oflarge-noise denoising, which cannot be efficiently handled by other BSNmethods. Extensive experiments on public real-world datasets demonstrate thatthe proposed MM-BSN achieves state-of-the-art performance among self-supervisedand even unpaired image denoising methods for sRGB images denoising, withoutany labelling effort or prior knowledge. Code can be found in", "output": "MM-BSN: Self-Supervised Image Denoising for Real-World with Multi-Mask based on Blind-Spot Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Even though simultaneous optimization of similarity metrics represents astandard procedure in the field of semantic segmentation, surprisingly, thisdoes not hold true for image registration. To close this unexpected gap in theliterature, we investigate in a complex multi-modal 3D setting whethersimultaneous optimization of registration metrics, here implemented by means ofprimitive summation, can benefit image registration. We evaluate twochallenging datasets containing collections of pre- to post-operative and pre-to intra-operative Magnetic Resonance Imaging (MRI) of glioma. Employing theproposed optimization we demonstrate improved registration accuracy in terms ofTarget Registration Error (TRE) on expert neuroradiologists' landmarkannotations.", "output": "Primitive Simultaneous Optimization of Similarity Metrics for Image Registration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a novel multi-modal framework for Scene Text VisualQuestion Answering (STVQA), which requires models to read scene text in imagesfor question answering. Apart from text or visual objects, which could existindependently, scene text naturally links text and visual modalities togetherby conveying linguistic semantics while being a visual object in an imagesimultaneously. Different to conventional STVQA models which take thelinguistic semantics and visual semantics in scene text as two separatefeatures, in this paper, we propose a paradigm of \"Locate Then Generate\" (LTG),which explicitly unifies this two semantics with the spatial bounding box as abridge connecting them. Specifically, at first, LTG locates the region in animage that may contain the answer words with an answer location module (ALM)consisting of a region proposal network and a language refinement network, bothof which can transform to each other with one-to-one mapping via the scene textbounding box. Next, given the answer words selected by ALM, LTG generates areadable answer sequence with an answer generation module (AGM) based on apre-trained language model. As a benefit of the explicit alignment of thevisual and linguistic semantics, even without any scene text based pre-trainingtasks, LTG can boost the absolute accuracy by +6.06% and +6.92% on the TextVQAdataset and the ST-VQA dataset respectively, compared with a non-pre-trainingbaseline. We further demonstrate that LTG effectively unifies visual and textmodalities through the spatial bounding box connection, which isunderappreciated in previous methods.", "output": "Locate Then Generate: Bridging Vision and Language with Bounding Box for Scene-Text VQA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical Visual Question Answering (VQA) systems play a supporting role tounderstand clinic-relevant information carried by medical images. The questionsto a medical image include two categories: close-end (such as Yes/No question)and open-end. To obtain answers, the majority of the existing medical VQAmethods relies on classification approaches, while a few works attempt to usegeneration approaches or a mixture of the two. The classification approachesare relatively simple but perform poorly on long open-end questions. To bridgethis gap, in this paper, we propose a new Transformer based framework formedical VQA (named as Q2ATransformer), which integrates the advantages of boththe classification and the generation approaches and provides a unifiedtreatment for the close-end and open-end questions. Specifically, we introducean additional Transformer decoder with a set of learnable candidate answerembeddings to query the existence of each answer class to a givenimage-question pair. Through the Transformer attention, the candidate answerembeddings interact with the fused features of the image-question pair to makethe decision. In this way, despite being a classification-based approach, ourmethod provides a mechanism to interact with the answer information forprediction like the generation-based approaches. On the other hand, byclassification, we mitigate the task difficulty by reducing the search space ofanswers. Our method achieves new state-of-the-art performance on two medicalVQA benchmarks. Especially, for the open-end questions, we achieve 79.19% onVQA-RAD and 54.85% on PathVQA, with 16.09% and 41.45% absolute improvements,respectively.", "output": "Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, deep convolutional neural networks have shown fascinatingperformance in the field of image denoising. However, deeper networkarchitectures are often accompanied with large numbers of model parameters,leading to high training cost and long inference time, which limits theirapplication in practical denoising tasks. In this paper, we propose a noveldual convolutional blind denoising network with skip connection (DCBDNet),which is able to achieve a desirable balance between the denoising effect andnetwork complexity. The proposed DCBDNet consists of a noise estimation networkand a dual convolutional neural network (CNN). The noise estimation network isused to estimate the noise level map, which improves the flexibility of theproposed model. The dual CNN contains two branches: a u-shaped sub-network isdesigned for the upper branch, and the lower branch is composed of the dilatedconvolution layers. Skip connections between layers are utilized in both theupper and lower branches. The proposed DCBDNet was evaluated on severalsynthetic and real-world image denoising benchmark datasets. Experimentalresults have demonstrated that the proposed DCBDNet can effectively removegaussian noise in a wide range of levels, spatially variant noise and realnoise. With a simple model structure, our proposed DCBDNet still can obtaincompetitive denoising performance compared to the state-of-the-art imagedenoising models containing complex architectures. Namely, a favorabletrade-off between denoising performance and model complexity is achieved. Codesare available at ", "output": "Image Blind Denoising Using Dual Convolutional Neural Network with Skip Connection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, the development of deep learning has been pushing imagedenoising to a new level. Among them, self-supervised denoising is increasinglypopular because it does not require any prior knowledge. Most of the existingself-supervised methods are based on convolutional neural networks (CNN), whichare restricted by the locality of the receptive field and would cause colorshifts or textures loss. In this paper, we propose a novel Denoise Transformerfor real-world image denoising, which is mainly constructed with Context-awareDenoise Transformer (CADT) units and Secondary Noise Extractor (SNE) block.CADT is designed as a dual-branch structure, where the global branch uses awindow-based Transformer encoder to extract the global information, while thelocal branch focuses on the extraction of local features with small receptivefield. By incorporating CADT as basic components, we build a hierarchicalnetwork to directly learn the noise distribution information through residuallearning and obtain the first stage denoised output. Then, we design SNE in lowcomputation for secondary global noise extraction. Finally the blind spots arecollected from the Denoise Transformer output and reconstructed, forming thefinal denoised image. Extensive experiments on the real-world SIDD benchmarkachieve 50.62/0.990 for PSNR/SSIM, which is competitive with the currentstate-of-the-art method and only 0.17/0.001 lower. Visual comparisons on publicsRGB, Raw-RGB and greyscale datasets prove that our proposed DenoiseTransformer has a competitive performance, especially on blurred textures andlow-light images, without using additional knowledge, e.g., noise level ornoise type, regarding the underlying unknown noise.", "output": "Self-Supervised Image Denoising for Real-World Images with Context-aware Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contemporary segmentation methods are usually based on deep fullyconvolutional networks (FCNs). However, the layer-by-layer convolutions with agrowing receptive field is not good at capturing long-range contexts such aslane markers in the scene. In this paper, we address this issue by designing adistillation method that exploits label structure when training segmentationnetwork. The intuition is that the ground-truth lane annotations themselvesexhibit internal structure. We broadcast the structure hints throughout ateacher network, i.e., we train a teacher network that consumes a lane labelmap as input and attempts to replicate it as output. Then, the attention mapsof the teacher network are adopted as supervisors of the student segmentationnetwork. The teacher network, with label structure information embedded, knowsdistinctly where the convolution layers should pay visual attention into. Theproposed method is named as Label-guided Attention Distillation (LGAD). Itturns out that the student network learns significantly better with LGAD thanwhen learning alone. As the teacher network is deprecated after training, ourmethod do not increase the inference time. Note that LGAD can be easilyincorporated in any lane segmentation network.", "output": "Label-guided Attention Distillation for Lane Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual question answering (VQA) is a critical multimodal task in which anagent must answer questions according to the visual cue. Unfortunately,language bias is a common problem in VQA, which refers to the model generatinganswers only by associating with the questions while ignoring the visualcontent, resulting in biased results. We tackle the language bias problem byproposing a self-supervised counterfactual metric learning (SC-ML) method tofocus the image features better. SC-ML can adaptively select thequestion-relevant visual features to answer the question, reducing the negativeinfluence of question-irrelevant visual features on inferring answers. Inaddition, question-irrelevant visual features can be seamlessly incorporatedinto counterfactual training schemes to further boost robustness. Extensiveexperiments have proved the effectiveness of our method with improved resultson the VQA-CP dataset. Our code will be made publicly available.", "output": "SC-ML: Self-supervised Counterfactual Metric Learning for Debiased Visual Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Intensifying climate change will lead to more extreme weather events,including heavy rainfall and drought. Accurate stream flow prediction modelswhich are adaptable and robust to new circumstances in a changing climate willbe an important source of information for decisions on climate adaptationefforts, especially regarding mitigation of the risks of and damages associatedwith flooding. In this work we propose a machine learning-based approach forpredicting water flow intensities in inland watercourses based on the physicalcharacteristics of the catchment areas, obtained from geospatial data(including elevation and soil maps, as well as satellite imagery), in additionto temporal information about past rainfall quantities and temperaturevariations. We target the one-day-ahead regime, where a fully convolutionalneural network model receives spatio-temporal inputs and predicts the waterflow intensity in every coordinate of the spatial input for the subsequent day.To the best of our knowledge, we are the first to tackle the task of densewater flow intensity prediction; earlier works have considered predicting flowintensities at a sparse set of locations at a time. An extensive set of modelevaluations and ablations are performed, which empirically justify our variousdesign choices. Code and preprocessed data have been made publicly available at", "output": "Fully Convolutional Networks for Dense Water Flow Intensity Prediction in Swedish Catchment Areas."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural captioners are typically trained to mimic human-generated referenceswithout optimizing for any specific communication goal, leading to problemssuch as the generation of vague captions. In this paper, we show thatfine-tuning an out-of-the-box neural captioner with a self-superviseddiscriminative communication objective helps to recover a plain, visuallydescriptive language that is more informative about image contents. Given atarget image, the system must learn to produce a description that enables anout-of-the-box text-conditioned image retriever to identify such image among aset of candidates. We experiment with the popular ClipCap captioner, alsoreplicating the main results with BLIP. In terms of similarity to ground-truthhuman descriptions, the captions emerging from discriminative finetuning lagslightly behind those generated by the non-finetuned model, when the latter istrained and tested on the same caption dataset. However, when the model is usedwithout further tuning to generate captions for out-of-domain datasets, ourdiscriminatively-finetuned captioner generates descriptions that resemble humanreferences more than those produced by the same captioner without finetuning.We further show that, on the Conceptual Captions dataset, discriminativelyfinetuned captions are more helpful than either vanilla ClipCap captions orground-truth captions for human annotators tasked with an image discriminationtask.", "output": "Cross-Domain Image Captioning with Discriminative Finetuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A primary goal of class-incremental learning is to strike a balance betweenstability and plasticity, where models should be both stable enough to retainknowledge learned from previously seen classes, and plastic enough to learnconcepts from new classes. While previous works demonstrate strong performanceon class-incremental benchmarks, it is not clear whether their success comesfrom the models being stable, plastic, or a mixture of both. This paper aims toshed light on how effectively recent class-incremental learning algorithmsaddress the stability-plasticity trade-off. We establish analytical tools thatmeasure the stability and plasticity of feature representations, and employsuch tools to investigate models trained with various algorithms on large-scaleclass-incremental benchmarks. Surprisingly, we find that the majority ofclass-incremental learning algorithms heavily favor stability over plasticity,to the extent that the feature extractor of a model trained on the initial setof classes is no less effective than that of the final incremental model. Ourobservations not only inspire two simple algorithms that highlight theimportance of feature representation analysis, but also suggest thatclass-incremental learning approaches, in general, should strive for betterfeature representation learning.", "output": "On the Stability-Plasticity Dilemma of Class-Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model inversion (MI) attacks aim to infer and reconstruct private trainingdata by abusing access to a model. MI attacks have raised concerns about theleaking of sensitive information (e.g. private face images used in training aface recognition system). Recently, several algorithms for MI have beenproposed to improve the attack performance. In this work, we revisit MI, studytwo fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms,and propose solutions to these issues which lead to a significant boost inattack performance for all SOTA MI. In particular, our contributions aretwo-fold: 1) We analyze the optimization objective of SOTA MI algorithms, arguethat the objective is sub-optimal for achieving MI, and propose an improvedoptimization objective that boosts attack performance significantly. 2) Weanalyze \"MI overfitting\", show that it would prevent reconstructed images fromlearning semantics of training data, and propose a novel \"model augmentation\"idea to overcome this issue. Our proposed solutions are simple and improve allSOTA MI attack accuracy significantly. E.g., in the standard CelebA benchmark,our solutions improve accuracy by 11.8% and achieve for the first time over 90%attack accuracy. Our findings demonstrate that there is a clear risk of leakingsensitive information from deep learning models. We urge serious considerationto be given to the privacy implications. Our code, demo, and models areavailable at", "output": "Re-thinking Model Inversion Attacks Against Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we follow a data-centric philosophy and propose a novel motionannotation method based on the inherent representativeness of motion data in agiven dataset. Specifically, we propose a Representation-basedRepresentativeness Ranking R3 method that ranks all motion data in a givendataset according to their representativeness in a learned motionrepresentation space. We further propose a novel dual-level motion constrastivelearning method to learn the motion representation space in a more informativeway. Thanks to its high efficiency, our method is particularly responsive tofrequent requirements change and enables agile development of motion annotationmodels. Experimental results on the HDM05 dataset against state-of-the-artmethods demonstrate the superiority of our method.", "output": "Motion-R3: Fast and Accurate Motion Annotation via Representation-based Representativeness Ranking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Light scattering imposes a major obstacle for imaging objects seated deeplyin turbid media, such as biological tissues and foggy air. Diffuse opticaltomography (DOT) tackles scattering by volumetrically recovering the opticalabsorbance and has shown significance in medical imaging, remote sensing andautonomous driving. A conventional DOT reconstruction paradigm necessitatesdiscretizing the object volume into voxels at a pre-determined resolution formodelling diffuse light propagation and the resulting spatial resolution of thereconstruction is generally limited. We propose NeuDOT, a novel DOT schemebased on neural fields (NF) to continuously encode the optical absorbancewithin the volume and subsequently bridge the gap between model accuracy andhigh resolution. Comprehensive experiments demonstrate that NeuDOT achievessubmillimetre lateral resolution and resolves complex 3D objects at 14mm-depth, outperforming the state-of-the-art methods. NeuDOT is a non-invasive,high-resolution and computationally efficient tomographic method, and unlocksfurther applications of NF involving light scattering.", "output": "High-resolution tomographic reconstruction of optical absorbance through scattering media using neural fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the challenging task of training models for image-to-videodeblurring, which aims to recover a sequence of sharp images corresponding to agiven blurry image input. A critical issue disturbing the training of animage-to-video model is the ambiguity of the frame ordering since both theforward and backward sequences are plausible solutions. This paper proposes aneffective self-supervised ordering scheme that allows training high-qualityimage-to-video deblurring models. Unlike previous methods that rely onorder-invariant losses, we assign an explicit order for each video sequence,thus avoiding the order-ambiguity issue. Specifically, we map each videosequence to a vector in a latent high-dimensional space so that there exists ahyperplane such that for every video sequence, the vectors extracted from itand its reversed sequence are on different sides of the hyperplane. The side ofthe vectors will be used to define the order of the corresponding sequence.Last but not least, we propose a real-image dataset for the image-to-videodeblurring problem that covers a variety of popular domains, including face,hand, and street. Extensive experimental results confirm the effectiveness ofour method. Code and data are available at", "output": "HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning for medical imaging is limited by data scarcity and domainshift, which lead to biased training sets that do not accurately representdeployment conditions. A related practical problem is cross-modal segmentationwhere the objective is to segment unlabelled domains using previously labelledimages from other modalites, which is the context of the MICCAI CrossMoDA 2022challenge on vestibular schwannoma (VS) segmentation. In this context, wepropose a VS segmentation method that leverages conventional image-to-imagetranslation and segmentation using iterative self training combined to adedicated data augmentation technique called Generative Blending Augmentation(GBA). GBA is based on a one-shot 2D SinGAN generative model that allows torealistically diversify target tumor appearances in a downstream segmentationmodel, improving its generalization power at test time. Our solution rankedfirst on the VS segmentation task during the validation and test phase of theCrossModa 2022 challenge.", "output": "Cross-modal tumor segmentation using generative blending augmentation and self training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Video Instance Segmentation(VIS) aims at segmenting and categorizing objectsin videos from a closed set of training categories, lacking the generalizationability to handle novel categories in real-world videos. To address thislimitation, we make the following three contributions. First, we introduce thenovel task of Open-Vocabulary Video Instance Segmentation, which aims tosimultaneously segment, track, and classify objects in videos from open-setcategories, including novel categories unseen during training. Second, tobenchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video InstanceSegmentation dataset(LV-VIS), that contains well-annotated objects from 1,212diverse categories, significantly surpassing the category size of existingdatasets by more than one order of magnitude. Third, we propose an efficientMemory-Induced Vision-Language Transformer, MindVLT, to first achieveOpen-Vocabulary VIS in an end-to-end manner with near real-time inferencespeed. Extensive experiments on LV-VIS and four existing VIS datasetsdemonstrate the strong zero-shot generalization ability of MindVLT on novelcategories. We will release the dataset and code to facilitate futureendeavors.", "output": "Towards Open-Vocabulary Video Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The challenge of dynamic view synthesis from dynamic monocular videos, i.e.,synthesizing novel views for free viewpoints given a monocular video of adynamic scene captured by a moving camera, mainly lies in accurately modelingthe dynamic objects of a scene using limited 2D frames, each with a varyingtimestamp and viewpoint. Existing methods usually require pre-processed 2Doptical flow and depth maps by additional methods to supervise the network,making them suffer from the inaccuracy of the pre-processed supervision and theambiguity when lifting the 2D information to 3D. In this paper, we tackle thischallenge in an unsupervised fashion. Specifically, we decouple the motion ofthe dynamic objects into object motion and camera motion, respectivelyregularized by proposed unsupervised surface consistency and patch-basedmulti-view constraints. The former enforces the 3D geometric surfaces of movingobjects to be consistent over time, while the latter regularizes theirappearances to be consistent across different viewpoints. Such a fine-grainedmotion formulation can alleviate the learning difficulty for the network, thusenabling it to produce not only novel views with higher quality but also moreaccurate scene flows and depth than existing methods requiring extrasupervision. We will make the code publicly available.", "output": "Decoupling Dynamic Monocular Videos for Dynamic View Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The deep neural networks (DNNs) have freed the synthetic aperture radarautomatic target recognition (SAR ATR) from expertise-based feature designingand demonstrated superiority over conventional solutions. There has been shownthe unique deficiency of ground vehicle benchmarks in shapes of strongbackground correlation results in DNNs overfitting the clutter and beingnon-robust to unfamiliar surroundings. However, the gap between fixedbackground model training and varying background application remainsunderexplored. Inspired by contrastive learning, this letter proposes asolution called Contrastive Feature Alignment (CFA) aiming to learn invariantrepresentation for robust recognition. The proposed method contributes a mixedclutter variants generation strategy and a new inference branch equipped withchannel-weighted mean square error (CWMSE) loss for invariant representationlearning. In specific, the generation strategy is delicately designed to betterattract clutter-sensitive deviation in feature space. The CWMSE loss is furtherdevised to better contrast this deviation and align the deep features activatedby the original images and corresponding clutter variants. The proposed CFAcombines both classification and CWMSE losses to train the model jointly, whichallows for the progressive learning of invariant target representation.Extensive evaluations on the MSTAR dataset and six DNN models prove theeffectiveness of our proposal. The results demonstrated that the CFA-trainedmodels are capable of recognizing targets among unfamiliar surroundings thatare not included in the dataset, and are robust to varying signal-to-clutterratios.", "output": "Learning Invariant Representation via Contrastive Feature Alignment for Clutter Robust SAR Target Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-Language (V-L) models trained with contrastive learning to align thevisual and language modalities have been shown to be strong few-shot learners.Soft prompt learning is the method of choice for few-shot downstream adaptionaiming to bridge the modality gap caused by the distribution shift induced bythe new domain. While parameter-efficient, prompt learning still requiresaccess to the model weights and can be computationally infeasible for largemodels with billions of parameters. To address these shortcomings, in thiswork, we describe a black-box method for V-L few-shot adaptation that (a)operates on pre-computed image and text features and hence works without accessto the model's weights, (b) it is orders of magnitude faster at training time,(c) it is amenable to both supervised and unsupervised training, and (d) it canbe even used to align image and text features computed from uni-modal models.To achieve this, we propose Linear Feature Alignment (LFA), a simple linearapproach for V-L re-alignment in the target domain. LFA is initialized from aclosed-form solution to a least-squares problem and then it is iterativelyupdated by minimizing a re-ranking loss. Despite its simplicity, our approachcan even surpass soft-prompt learning methods as shown by extensive experimentson 11 image and 2 video datasets.", "output": "Black Box Few-Shot Adaptation for Vision-Language models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) aims to learn a single global model that enables thecentral server to help the model training in local clients without accessingtheir local data. The key challenge of FL is the heterogeneity of local data indifferent clients, such as heterogeneous label distribution and feature shift,which could lead to significant performance degradation of the learned models.Although many studies have been proposed to address the heterogeneous labeldistribution problem, few studies attempt to explore the feature shift issue.To address this issue, we propose a simple yet effective algorithm, namelytextbf{p}ersonalized textbf{Fed}erated learning with textbf{L}ocaltextbf{A}ttention (pFedLA), by incorporating the attention mechanism intopersonalized models of clients while keeping the attention blocksclient-specific. Specifically, two modules are proposed in pFedLA, i.e., thepersonalized single attention module and the personalized hybrid attentionmodule. In addition, the proposed pFedLA method is quite flexible and generalas it can be incorporated into any FL method to improve their performancewithout introducing additional communication costs. Extensive experimentsdemonstrate that the proposed pFedLA method can boost the performance ofstate-of-the-art FL methods on different tasks such as image classification andobject detection tasks.", "output": "Personalized Federated Learning with Local Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The interplay between structural and electrical changes in the heart aftermyocardial infarction (MI) plays a key role in the initiation and maintenanceof arrhythmia. The anatomical and electrophysiological properties of scar,border zone, and normal myocardium modify the electrocardiographic morphology,which is routinely analysed in clinical settings. However, the influence ofvarious MI properties on the QRS is not intuitively predictable.In this work,we have systematically investigated the effects of 17 post-MI scenarios,varying the location, size, transmural extent, and conductive level of scarringand border zone area, on the forward-calculated QRS. Additionally, we havecompared the contributions of different QRS score criteria for quantifyingpost-MI pathophysiology.The propagation of electrical activity in theventricles is simulated via a Eikonal model on a unified coordinate system.Theanalysis has been performed on 49 subjects, and the results imply that the QRSis capable of identifying MI, suggesting the feasibility of inverselyreconstructing infarct regions from QRS.There exist sensitivity variations ofdifferent QRS criteria for identifying 17 MI scenarios, which is informativefor solving the inverse problem.", "output": "Influence of Myocardial Infraction on QRS Properties: A Simulation Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the expensive costs of collecting labels in multi-label classificationdatasets, partially annotated multi-label classification has become an emergingfield in computer vision. One baseline approach to this task is to assumeunobserved labels as negative labels, but this assumption induces label noiseas a form of false negative. To understand the negative impact caused by falsenegative labels, we study how these labels affect the model's explanation. Weobserve that the explanation of two models, trained with full and partiallabels each, highlights similar regions but with different scaling, where thelatter tends to have lower attribution scores. Based on these findings, wepropose to boost the attribution scores of the model trained with partiallabels to make its explanation resemble that of the model trained with fulllabels. Even with the conceptually simple approach, the multi-labelclassification performance improves by a large margin in three differentdatasets on a single positive label setting and one on a large-scale partiallabel setting. Code is available at", "output": "Bridging the Gap between Model Explanations in Partially Annotated Multi-label Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As multimedia content often contains noise from intrinsic defects of digitaldevices, image denoising is an important step for high-level vision recognitiontasks. Although several studies have developed the denoising field employingadvanced Transformers, these networks are too momory-intensive for real-worldapplications. Additionally, there is a lack of research on lightweight denosing(LWDN) with Transformers. To handle this, this work provides seven comparativebaseline Transformers for LWDN, serving as a foundation for future research. Wealso demonstrate the parts of randomly cropped patches significantly affect thedenoising performances during training. While previous studies have overlookedthis aspect, we aim to train our baseline Transformers in a truly fair manner.Furthermore, we conduct empirical analyses of various components to determinethe key considerations for constructing LWDN Transformers. Codes are availableat ", "output": "Exploration of Lightweight Single Image Denoising with Transformers and Truly Fair Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Shapley value is widely regarded as a trustworthy attribution metric.However, when people use Shapley values to explain the attribution of inputvariables of a deep neural network (DNN), it usually requires a very highcomputational cost to approximate relatively accurate Shapley values inreal-world applications. Therefore, we propose a novel network architecture,the HarsanyiNet, which makes inferences on the input sample and simultaneouslycomputes the exact Shapley values of the input variables in a single forwardpropagation. The HarsanyiNet is designed on the theoretical foundation that theShapley value can be reformulated as the redistribution of Harsanyiinteractions encoded by the network.", "output": "HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Low-dose computed tomography (CT) images suffer from noise and artifacts dueto photon starvation and electronic noise. Recently, some works have attemptedto use diffusion models to address the over-smoothness and training instabilityencountered by previous deep-learning-based denoising models. However,diffusion models suffer from long inference times due to the large number ofsampling steps involved. Very recently, cold diffusion model generalizesclassical diffusion models and has greater flexibility. Inspired by the colddiffusion, this paper presents a novel COntextual eRror-modulated gEneralizedDiffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First,CoreDiff utilizes LDCT images to displace the random Gaussian noise and employsa novel mean-preserving degradation operator to mimic the physical process ofCT degradation, significantly reducing sampling steps thanks to the informativeLDCT images as the starting point of the sampling process. Second, to alleviatethe error accumulation problem caused by the imperfect restoration operator inthe sampling process, we propose a novel ContextuaL Error-modulAted RestorationNetwork (CLEAR-Net), which can leverage contextual information to constrain thesampling process from structural distortion and modulate time step embeddingfeatures for better alignment with the input at the next time step. Third, torapidly generalize to a new, unseen dose level with as few resources aspossible, we devise a one-shot learning framework to make CoreDiff generalizefaster and better using only a single LDCT image (un)paired with NDCT.Extensive experimental results on two datasets demonstrate that our CoreDiffoutperforms competing methods in denoising and generalization performance, witha clinically acceptable inference time.", "output": "CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human evaluation is critical for validating the performance of text-to-imagegenerative models, as this highly cognitive process requires deep comprehensionof text and images. However, our survey of 37 recent papers reveals that manyworks rely solely on automatic measures (e.g., FID) or perform poorly describedhuman evaluations that are not reliable or repeatable. This paper proposes astandardized and well-defined human evaluation protocol to facilitateverifiable and reproducible human evaluation in future works. In our pilot datacollection, we experimentally show that the current automatic measures areincompatible with human perception in evaluating the performance of thetext-to-image generation results. Furthermore, we provide insights fordesigning human evaluation experiments reliably and conclusively. Finally, wemake several resources publicly available to the community to facilitate easyand fast implementations.", "output": "Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large scale vision and language models can achieve impressive zero-shotrecognition performance by mapping class specific text queries to imagecontent. Two distinct challenges that remain however, are high sensitivity tothe choice of handcrafted class names that define queries, and the difficultyof adaptation to new, smaller datasets. Towards addressing these problems, wepropose to leverage available data to learn, for each class, an optimal wordembedding as a function of the visual content. By learning new word embeddingson an otherwise frozen model, we are able to retain zero-shot capabilities fornew classes, easily adapt models to new datasets, and adjust potentiallyerroneous, non-descriptive or ambiguous class names. We show that our solutioncan easily be integrated in image classification and object detectionpipelines, yields significant performance gains in multiple scenarios andprovides insights into model biases and labelling errors.", "output": "Learning to Name Classes for Vision and Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural fields are evolving towards a general-purpose continuousrepresentation for visual computing. Yet, despite their numerous appealingproperties, they are hardly amenable to signal processing. As a remedy, wepresent a method to perform general continuous convolutions with generalcontinuous signals such as neural fields. Observing that piecewise polynomialkernels reduce to a sparse set of Dirac deltas after repeated differentiation,we leverage convolution identities and train a repeated integral field toefficiently execute large-scale convolutions. We demonstrate our approach on avariety of data modalities and spatially-varying kernels.", "output": "Neural Field Convolutions by Repeated Differentiation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Progress in 3D volumetric image analysis research is limited by the lack ofdatasets and most advances in analysis methods for volumetric images are basedon medical data. However, medical data do not necessarily resemble thecharacteristics of other volumetric images such as micro-CT. To promoteresearch in 3D volumetric image analysis beyond medical data, we have createdthe BugNIST dataset and made it freely available. BugNIST is an extensivedataset of micro-CT scans of 12 types of bugs, such as insects and larvae.BugNIST contains 9437 volumes where 9087 are of individual bugs and 350 aremixtures of bugs and other material. The goal of BugNIST is to benchmarkclassification and detection methods, and we have designed the detectionchallenge such that detection models are trained on scans of individual bugsand tested on bug mixtures. Models capable of solving this task will beindependent of the context, i.e., the surrounding material. This is a greatadvantage if the context is unknown or changing, as is often the case inmicro-CT. Our initial baseline analysis shows that current state-of-the-artdeep learning methods classify individual bugs very well, but has greatdifficulty with the detection challenge. Hereby, BugNIST enables research inimage analysis areas that until now have missed relevant data - bothclassification, detection, and hopefully more.", "output": "BugNIST -- A New Large Scale Volumetric 3D Image Dataset for Classification and Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we explore massive pre-training on synthetic word images forenhancing the performance on four benchmark downstream handwriting analysistasks. To this end, we build a large synthetic dataset of word images renderedin several handwriting fonts, which offers a complete supervision signal. Weuse it to train a simple convolutional neural network (ConvNet) with a fullysupervised objective. The vector representations of the images obtained fromthe pre-trained ConvNet can then be considered as encodings of the handwritingstyle. We exploit such representations for Writer Retrieval, WriterIdentification, Writer Verification, and Writer Classification and demonstratethat our pre-training strategy allows extracting rich representations of thewriters' style that enable the aforementioned tasks with competitive resultswith respect to task-specific State-of-the-Art approaches.", "output": "Evaluating Synthetic Pre-Training for Handwriting Processing Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical images are extremely valuable for supporting medical diagnoses.However, in practice, low-quality (LQ) medical images, such as images that arehazy/blurry, have uneven illumination, or are out of focus, among others, areoften obtained during data acquisition. This leads to difficulties in thescreening and diagnosis of medical diseases. Several generative adversarialnetworks (GAN)-based image enhancement methods have been proposed and haveshown promising results. However, there is a quality-originality trade-offamong these methods in the sense that they produce visually pleasing resultsbut lose the ability to preserve originality, especially the structural inputs.Moreover, to our knowledge, there is no objective metric in evaluating thestructure preservation of medical image enhancement methods in unsupervisedsettings due to the unavailability of paired ground-truth data. In this study,we propose a framework for practical unsupervised medical image enhancementthat includes (1) a non-reference objective evaluation of structurepreservation for medical image enhancement tasks called Laplacian structuralsimilarity index measure (LaSSIM), which is based on SSIM and the Laplacianpyramid, and (2) a novel unsupervised GAN-based method called Laplacian medicalimage enhancement (LaMEGAN) to support the improvement of both originality andquality from LQ images. The LaSSIM metric does not require clean referenceimages and has been shown to be superior to SSIM in capturing image structuralchanges under image degradations, such as strong blurring on differentdatasets. The experiments demonstrated that our LaMEGAN achieves a satisfactorybalance between quality and originality, with robust structure preservationperformance while generating compelling visual results with very high imagequality scores. The code will be made available at", "output": "A Practical Framework for Unsupervised Structure Preservation Medical Image Enhancement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate 3D human pose estimation is essential for sports analytics,coaching, and injury prevention. However, existing datasets for monocular poseestimation do not adequately capture the challenging and dynamic nature ofsports movements. In response, we introduce SportsPose, a large-scale 3D humanpose dataset consisting of highly dynamic sports movements. With more than176,000 3D poses from 24 different subjects performing 5 different sportsactivities, SportsPose provides a diverse and comprehensive set of 3D posesthat reflect the complex and dynamic nature of sports movements. Contrary toother markerless datasets we have quantitatively evaluated the precision ofSportsPose by comparing our poses with a commercial marker-based system andachieve a mean error of 34.5 mm across all evaluation sequences. This iscomparable to the error reported on the commonly used 3DPW dataset. We furtherintroduce a new metric, local movement, which describes the movement of thewrist and ankle joints in relation to the body. With this, we show thatSportsPose contains more movement than the Human3.6M and 3DPW datasets in theseextremum joints, indicating that our movements are more dynamic. The datasetwith accompanying code can be downloaded from our website. We hope thatSportsPose will allow researchers and practitioners to develop and evaluatemore effective models for the analysis of sports performance and injuryprevention. With its realistic and diverse dataset, SportsPose provides avaluable resource for advancing the state-of-the-art in pose estimation insports.", "output": "SportsPose -- A Dynamic 3D sports pose dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a method for generating realistic pedestrian trajectories andfull-body animations that can be controlled to meet user-defined goals. We drawon recent advances in guided diffusion modeling to achieve test-timecontrollability of trajectories, which is normally only associated withrule-based systems. Our guided diffusion model allows users to constraintrajectories through target waypoints, speed, and specified social groups whileaccounting for the surrounding environment context. This trajectory diffusionmodel is integrated with a novel physics-based humanoid controller to form aclosed-loop, full-body pedestrian animation system capable of placing largecrowds in a simulated environment with varying terrains. We further proposeutilizing the value function learned during RL training of the animationcontroller to guide diffusion to produce trajectories better suited forparticular scenarios such as collision avoidance and traversing uneven terrain.Video results are available on the project page at .", "output": "Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel class incremental learning approach by incorporating afeature augmentation technique motivated by adversarial attacks. We employ aclassifier learned in the past to complement training examples rather thansimply play a role as a teacher for knowledge distillation towards subsequentmodels. The proposed approach has a unique perspective to utilize the previousknowledge in class incremental learning since it augments features of arbitrarytarget classes using examples in other classes via adversarial attacks on apreviously learned classifier. By allowing the cross-class featureaugmentations, each class in the old tasks conveniently populates samples inthe feature space, which alleviates the collapse of the decision boundariescaused by sample deficiency for the previous tasks, especially when the numberof stored exemplars is small. This idea can be easily incorporated intoexisting class incremental learning algorithms without any architecturemodification. Extensive experiments on the standard benchmarks show that ourmethod consistently outperforms existing class incremental learning methods bysignificant margins in various scenarios, especially under an environment withan extremely limited memory budget.", "output": "Cross-Class Feature Augmentation for Class Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, significant advancements have been made in 3D generative models,however training these models across diverse domains is challenging andrequires an huge amount of training data and knowledge of pose distribution.Text-guided domain adaptation methods have allowed the generator to be adaptedto the target domains using text prompts, thereby obviating the need forassembling numerous data. Recently, DATID-3D presents impressive quality ofsamples in text-guided domain, preserving diversity in text by leveragingtext-to-image diffusion. However, adapting 3D generators to domains withsignificant domain gaps from the source domain still remains challenging due toissues in current text-to-image diffusion models as following: 1) shape-posetrade-off in diffusion-based translation, 2) pose bias, and 3) instance bias inthe target domain, resulting in inferior 3D shapes, low text-imagecorrespondence, and low intra-domain diversity in the generated samples. Toaddress these issues, we propose a novel pipeline called PODIA-3D, which usespose-preserved text-to-image diffusion-based domain adaptation for 3Dgenerative models. We construct a pose-preserved text-to-image diffusion modelthat allows the use of extremely high-level noise for significant domainchanges. We also propose specialized-to-general sampling strategies to improvethe details of the generated samples. Moreover, to overcome the instance bias,we introduce a text-guided debiasing method that improves intra-domaindiversity. Consequently, our method successfully adapts 3D generators acrosssignificant domain gaps. Our qualitative results and user study demonstratesthat our approach outperforms existing 3D text-guided domain adaptation methodsin terms of text-image correspondence, realism, diversity of rendered images,and sense of depth of 3D shapes in the generated samples", "output": "PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot classification (FSC) entails learning novel classes given only a fewexamples per class after a pre-training (or meta-training) phase on a set ofbase classes. Recent works have shown that simply fine-tuning a pre-trainedVision Transformer (ViT) on new test classes is a strong approach for FSC.Fine-tuning ViTs, however, is expensive in time, compute and storage. This hasmotivated the design of parameter efficient fine-tuning (PEFT) methods whichfine-tune only a fraction of the Transformer's parameters. While these methodshave shown promise, inconsistencies in experimental conditions make itdifficult to disentangle their advantage from other experimental factorsincluding the feature extractor architecture, pre-trained initialization andfine-tuning algorithm, amongst others. In our paper, we conduct a large-scale,experimentally consistent, empirical analysis to study PEFTs for few-shot imageclassification. Through a battery of over 1.8k controlled experiments onlarge-scale few-shot benchmarks including Meta-Dataset (MD) and ORBIT, weuncover novel insights on PEFTs that cast light on their efficacy infine-tuning ViTs for few-shot classification. Through our controlled empiricalstudy, we have two main findings: (i) Fine-tuning just the LayerNorm parameters(which we call LN-Tune) during few-shot adaptation is an extremely strongbaseline across ViTs pre-trained with both self-supervised and supervisedobjectives, (ii) For self-supervised ViTs, we find that simply learning a setof scaling parameters for each attention matrix (which we call AttnScale) alongwith a domain-residual adapter (DRA) module leads to state-of-the-artperformance (while being $sim!$ 9$times$ more parameter-efficient) on MD.Our extensive empirical findings set strong baselines and call for rethinkingthe current design of PEFT methods for FSC.", "output": "Strong Baselines for Parameter Efficient Few-Shot Fine-tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning-assisted edge intelligence enables privacy protection inmodern intelligent services. However, not Independent and IdenticallyDistributed (non-IID) distribution among edge clients can impair the localmodel performance. The existing single prototype-based strategy represents asample by using the mean of the feature space. However, feature spaces areusually not clustered, and a single prototype may not represent a sample well.Motivated by this, this paper proposes a multi-prototype federated contrastivelearning approach (MP-FedCL) which demonstrates the effectiveness of using amulti-prototype strategy over a single-prototype under non-IID settings,including both label and feature skewness. Specifically, a multi-prototypecomputation strategy based on textit{k-means} is first proposed to capturedifferent embedding representations for each class space, using multipleprototypes ($k$ centroids) to represent a class in the embedding space. In eachglobal round, the computed multiple prototypes and their respective modelparameters are sent to the edge server for aggregation into a global prototypepool, which is then sent back to all clients to guide their local training.Finally, local training for each client minimizes their own supervised learningtasks and learns from shared prototypes in the global prototype pool throughsupervised contrastive learning, which encourages them to learn knowledgerelated to their own class from others and reduces the absorption of unrelatedknowledge in each global iteration. Experimental results on MNIST, Digit-5,Office-10, and DomainNet show that our method outperforms multiple baselines,with an average test accuracy improvement of about 4.6% and 10.4% underfeature and label non-IID distributions, respectively.", "output": "MP-FedCL: Multi-Prototype Federated Contrastive Learning for Edge Intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel domain generalization technique, referred to as RandomizedAdversarial Style Perturbation (RASP), which is motivated by the observationthat the characteristics of each domain are captured by the feature statisticscorresponding to style. The proposed algorithm perturbs the style of a featurein an adversarial direction towards a randomly selected class, and makes themodel learn against being misled by the unexpected styles observed in unseentarget domains. While RASP is effective to handle domain shifts, its naiveintegration into the training procedure might degrade the capability oflearning knowledge from source domains because it has no restriction on theperturbations of representations. This challenge is alleviated by NormalizedFeature Mixup (NFM), which facilitates the learning of the original featureswhile achieving robustness to perturbed representations via their mixup duringtraining. We evaluate the proposed algorithm via extensive experiments onvarious benchmarks and show that our approach improves domain generalizationperformance, especially in large-scale benchmarks.", "output": "Randomized Adversarial Style Perturbations for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents the AToMiC (Authoring Tools for Multimedia Content)dataset, designed to advance research in image/text cross-modal retrieval.While vision-language pretrained transformers have led to significantimprovements in retrieval effectiveness, existing research has relied onimage-caption datasets that feature only simplistic image-text relationshipsand underspecified user models of retrieval tasks. To address the gap betweenthese oversimplified settings and real-world applications for multimediacontent creation, we introduce a new approach for building retrieval testcollections. We leverage hierarchical structures and diverse domains of texts,styles, and types of images, as well as large-scale image-document associationsembedded in Wikipedia. We formulate two tasks based on a realistic user modeland validate our dataset through retrieval experiments using baseline models.AToMiC offers a testbed for scalable, diverse, and reproducible multimediaretrieval research. Finally, the dataset provides the basis for a dedicatedtrack at the 2023 Text Retrieval Conference (TREC), and is publicly availableat ", "output": "AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ethylene leakage detection has become one of the most important researchdirections in the field of target detection due to the fact that ethyleneleakage in the petrochemical industry is closely related to production safetyand environmental pollution. Under infrared conditions, there are many factorsthat affect the texture characteristics of ethylene, such as ethyleneconcentration, background, and so on. We find that the detection criteria usedin infrared imaging ethylene leakage detection research cannot fully reflectreal-world production conditions, which is not conducive to evaluate theperformance of current image-based target detection methods. Therefore, wecreate a new infrared image dataset of ethylene leakage with differentconcentrations and backgrounds, including 54275 images. We use the proposeddataset benchmark to evaluate seven advanced image-based target detectionalgorithms. Experimental results demonstrate the performance and limitations ofexisting algorithms, and the dataset benchmark has good versatility andeffectiveness.", "output": "Ethylene Leak Detection Based on Infrared Imaging: A Benchmark."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learned iterative reconstructions hold great promise to acceleratetomographic imaging with empirical robustness to model perturbations.Nevertheless, an adoption for photoacoustic tomography is hindered by the needto repeatedly evaluate the computational expensive forward model. Computationalfeasibility can be obtained by the use of fast approximate models, but a needto compensate model errors arises. In this work we advance the methodologicaland theoretical basis for model corrections in learned image reconstructions byembedding the model correction in a learned primal-dual framework. Here, themodel correction is jointly learned in data space coupled with a learnedupdating operator in image space within an unrolled end-to-end learnediterative reconstruction approach. The proposed formulation allows an extensionto a primal-dual deep equilibrium model providing fixed-point convergence aswell as reduced memory requirements for training. We provide theoretical andempirical insights into the proposed models with numerical validation in arealistic 2D limited-view setting. The model-corrected learned primal-dualmethods show excellent reconstruction quality with fast inference times andthus providing a methodological basis for real-time capable and scalableiterative reconstructions in photoacoustic tomography.", "output": "Model-corrected learned primal-dual models for fast limited-view photoacoustic tomography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-source Domain Generalization (DG) measures a classifier's ability togeneralize to new distributions of data it was not trained on, given severaltraining domains. While several multi-source DG methods have been proposed,they incur additional complexity during training by using domain labels. Recentwork has shown that a well-tuned Empirical Risk Minimization (ERM) trainingprocedure, that is simply minimizing the empirical risk on the source domains,can outperform most existing DG methods. We identify several key candidatetechniques to further improve ERM performance, such as better utilization oftraining data, model parameter selection, and weight-space regularization. Wecall the resulting method ERM++, and show it significantly improves theperformance of DG on five multi-source datasets by over 5% compared to standardERM, and beats state-of-the-art despite being less computationally expensive.Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset, achallenging DG benchmark. We hope that ERM++ becomes a strong baseline forfuture DG research. Code is released at", "output": "ERM++: An Improved Baseline for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present the USTC FLICAR Dataset, which is dedicated to thedevelopment of simultaneous localization and mapping and precise 3Dreconstruction of the workspace for heavy-duty autonomous aerial work robots.In recent years, numerous public datasets have played significant roles in theadvancement of autonomous cars and unmanned aerial vehicles (UAVs). However,these two platforms differ from aerial work robots: UAVs are limited in theirpayload capacity, while cars are restricted to two-dimensional movements. Tofill this gap, we create the Giraffe mapping robot based on a bucket truck,which is equipped with a variety of well-calibrated and synchronized sensors:four 3D LiDARs, two stereo cameras, two monocular cameras, Inertial MeasurementUnits (IMUs), and a GNSS/INS system. A laser tracker is used to record themillimeter-level ground truth positions. We also make its ground twin, theOkapi mapping robot, to gather data for comparison. The proposed datasetextends the typical autonomous driving sensing suite to aerial scenes.Therefore, the dataset is named FLICAR to denote flying cars. We believe thisdataset can also represent the flying car scenarios, specifically the takeoffand landing of VTOL (Vertical Takeoff and Landing) flying cars. The dataset isavailable for download at: ", "output": "USTC FLICAR: A Multisensor Fusion Dataset of LiDAR-Inertial-Camera for Heavy-duty Autonomous Aerial Work Robots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper addresses the robustness problem of visual-inertial stateestimation for underwater operations. Underwater robots operating in achallenging environment are required to know their pose at all times. Allvision-based localization schemes are prone to failure due to poor visibilityconditions, color loss, and lack of features. The proposed approach utilizes amodel of the robot's kinematics together with proprioceptive sensors tomaintain the pose estimate during visual-inertial odometry (VIO) failures.Furthermore, the trajectories from successful VIO and the ones from themodel-driven odometry are integrated in a coherent set that maintains aconsistent pose at all times. Health-monitoring tracks the VIO process ensuringtimely switches between the two estimators. Finally, loop closure isimplemented on the overall trajectory. The resulting framework is a robustestimator switching between model-based and visual-inertial odometry (SM/VIO).Experimental results from numerous deployments of the Aqua2 vehicle demonstratethe robustness of our approach over coral reefs and a shipwreck.", "output": "SM/VIO: Robust Underwater State Estimation Switching Between Model-based and Visual Inertial Odometry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose a few-shot colorectal tissue image generation methodfor addressing the scarcity of histopathological training data for rare cancertissues. Our few-shot generation method, named XM-GAN, takes one base and apair of reference tissue images as input and generates high-quality yet diverseimages. Within our XM-GAN, a novel controllable fusion block densely aggregateslocal regions of reference images based on their similarity to those in thebase image, resulting in locally consistent features. To the best of ourknowledge, we are the first to investigate few-shot generation in colorectaltissue images. We evaluate our few-shot colorectral tissue image generation byperforming extensive qualitative, quantitative and subject specialist(pathologist) based evaluations. Specifically, in specialist-based evaluation,pathologists could differentiate between our XM-GAN generated tissue images andreal images only 55% time. Moreover, we utilize these generated images as dataaugmentation to address the few-shot tissue image classification task,achieving a gain of 4.4% in terms of mean accuracy over the vanilla few-shotclassifier. Code: url{", "output": "Cross-modulated Few-shot Image Generation for Colorectal Tissue Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work introduces Differential Wavelet Amplifier (DWA), a drop-in modulefor wavelet-based image Super-Resolution (SR). DWA invigorates an approachrecently receiving less attention, namely Discrete Wavelet Transformation(DWT). DWT enables an efficient image representation for SR and reduces thespatial area of its input by a factor of 4, the overall model size, andcomputation cost, framing it as an attractive approach for sustainable ML. Ourproposed DWA model improves wavelet-based SR models by leveraging thedifference between two convolutional filters to refine relevant featureextraction in the wavelet domain, emphasizing local contrasts and suppressingcommon noise in the input signals. We show its effectiveness by integrating itinto existing SR models, e.g., DWSR and MWCNN, and demonstrate a clearimprovement in classical SR tasks. Moreover, DWA enables a direct applicationof DWSR and MWCNN to input image space, reducing the DWT representationchannel-wise since it omits traditional DWT.", "output": "DWA: Differential Wavelet Amplifier for Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A good metric, which promises a reliable comparison between solutions, isessential to a well-defined task. Unlike most vision tasks that have per-sampleground-truth, image synthesis targets generating emph{unseen} data and henceis usually evaluated with a distributional distance between one set of realsamples and another set of generated samples. This work provides an empiricalstudy on the evaluation of synthesis performance by taking the populargenerative adversarial networks (GANs) as a representative of generativemodels. In particular, we make in-depth analyses on how to represent a datapoint in the feature space, how to calculate a fair distance using selectedsamples, and how many instances to use from each set. Experiments on multipledatasets and settings suggest that (1) a group of models including bothCNN-based and ViT-based architectures serve as reliable and robust featureextractors, (2) Centered Kernel Alignment (CKA) enables better comparisonacross various extractors and hierarchical layers in one model, and (3) CKAshows satisfactory sample efficiency and complements existing metrics(textit{e.g.}, FID) in characterizing the similarity between two internal datacorrelations. These findings help us design a new measurement system, based onwhich we re-evaluate the state-of-the-art generative models in a consistent andreliable way.", "output": "Revisiting the Evaluation of Image Synthesis with GANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Animating virtual avatars with free-view control is crucial for variousapplications like virtual reality and digital entertainment. Previous studieshave attempted to utilize the representation power of the neural radiance field(NeRF) to reconstruct the human body from monocular videos. Recent workspropose to graft a deformation network into the NeRF to further model thedynamics of the human neural field for animating vivid human motions. However,such pipelines either rely on pose-dependent representations or fall short ofmotion coherency due to frame-independent optimization, making it difficult togeneralize to unseen pose sequences realistically. In this paper, we propose anovel framework MonoHuman, which robustly renders view-consistent andhigh-fidelity avatars under arbitrary novel poses. Our key insight is to modelthe deformation field with bi-directional constraints and explicitly leveragethe off-the-peg keyframe information to reason the feature correlations forcoherent results. Specifically, we first propose a Shared BidirectionalDeformation module, which creates a pose-independent generalizable deformationfield by disentangling backward and forward deformation correspondences intoshared skeletal motion weight and separate non-rigid motions. Then, we devise aForward Correspondence Search module, which queries the correspondence featureof keyframes to guide the rendering network. The rendered results are thusmulti-view consistent with high fidelity, even under challenging novel posesettings. Extensive experiments demonstrate the superiority of our proposedMonoHuman over state-of-the-art methods.", "output": "MonoHuman: Animatable Human Neural Field from Monocular Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Line segments are powerful features complementary to points. They offerstructural cues, robust to drastic viewpoint and illumination changes, and canbe present even in texture-less areas. However, describing and matching them ismore challenging compared to points due to partial occlusions, lack of texture,or repetitiveness. This paper introduces a new matching paradigm, where points,lines, and their descriptors are unified into a single wireframe structure. Wepropose GlueStick, a deep matching Graph Neural Network (GNN) that takes twowireframes from different images and leverages the connectivity informationbetween nodes to better glue them together. In addition to the increasedefficiency brought by the joint matching, we also demonstrate a large boost ofperformance when leveraging the complementary nature of these two features in asingle architecture. We show that our matching strategy outperforms thestate-of-the-art approaches independently matching line segments and points fora wide variety of datasets and tasks. The code is available at", "output": "GlueStick: Robust Image Matching by Sticking Points and Lines Together."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans can orient themselves in their 3D environments using simple 2D maps.Differently, algorithms for visual localization mostly rely on complex 3D pointclouds that are expensive to build, store, and maintain over time. We bridgethis gap by introducing OrienterNet, the first deep neural network that canlocalize an image with sub-meter accuracy using the same 2D semantic maps thathumans use. OrienterNet estimates the location and orientation of a query imageby matching a neural Bird's-Eye View with open and globally available maps fromOpenStreetMap, enabling anyone to localize anywhere such maps are available.OrienterNet is supervised only by camera poses but learns to perform semanticmatching with a wide range of map elements in an end-to-end manner. To enablethis, we introduce a large crowd-sourced dataset of images captured across 12cities from the diverse viewpoints of cars, bikes, and pedestrians. OrienterNetgeneralizes to new datasets and pushes the state of the art in both roboticsand AR scenarios. The code and trained model will be released publicly.", "output": "OrienterNet: Visual Localization in 2D Public Maps with Neural Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we present Multi-Level Contrastive Learning for DensePrediction Task (MCL), an efficient self-supervised method for learningregion-level feature representation for dense prediction tasks. Our method ismotivated by the three key factors in detection: localization, scaleconsistency and recognition. To explicitly encode absolute position and scaleinformation, we propose a novel pretext task that assembles multi-scale imagesin a montage manner to mimic multi-object scenarios. Unlike the existingimage-level self-supervised methods, our method constructs a multi-levelcontrastive loss that considers each sub-region of the montage image as asingleton. Our method enables the neural network to learn regional semanticrepresentations for translation and scale consistency while reducingpre-training epochs to the same as supervised pre-training. Extensiveexperiments demonstrate that MCL consistently outperforms the recentstate-of-the-art methods on various datasets with significant margins. Inparticular, MCL obtains 42.5 AP$^mathrm{bb}$ and 38.3 AP$^mathrm{mk}$ on COCOwith the 1x schedule fintuning, when using Mask R-CNN with R50-FPN backbonepre-trained with 100 epochs. In comparison to MoCo, our method surpasses theirperformance by 4.0 AP$^mathrm{bb}$ and 3.1 AP$^mathrm{mk}$. Furthermore, weexplore the alignment between pretext task and downstream tasks. We extend ourpretext task to supervised pre-training, which achieves a similar performanceto self-supervised learning. This result demonstrates the importance of thealignment between pretext task and downstream tasks, indicating the potentialfor wider applicability of our method beyond self-supervised settings.", "output": "Multi-Level Contrastive Learning for Dense Prediction Task."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning image classification and image generation using the same set ofnetwork parameters is a challenging problem. Recent advanced approaches performwell in one task often exhibit poor performance in the other. This workintroduces an energy-based classifier and generator, namely EGC, which canachieve superior performance in both tasks using a single neural network.Unlike a conventional classifier that outputs a label given an image (i.e., aconditional distribution $p(y|mathbf{x})$), the forward pass in EGC is aclassifier that outputs a joint distribution $p(mathbf{x},y)$, enabling animage generator in its backward pass by marginalizing out the label $y$. Thisis done by estimating the energy and classification probability given a noisyimage in the forward pass, while denoising it using the score functionestimated in the backward pass. EGC achieves competitive generation resultscompared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUNChurch, while achieving superior classification accuracy and robustness againstadversarial attacks on CIFAR-10. This work represents the first successfulattempt to simultaneously excel in both tasks using a single set of networkparameters. We believe that EGC bridges the gap between discriminative andgenerative learning.", "output": "EGC: Image Generation and Classification via a Single Energy-Based Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "High-fidelity human 3D models can now be learned directly from videos,typically by combining a template-based surface model with neuralrepresentations. However, obtaining a template surface requires expensivemulti-view capture systems, laser scans, or strictly controlled conditions.Previous methods avoid using a template but rely on a costly or ill-posedmapping from observation to canonical space. We propose a hybrid point-basedrepresentation for reconstructing animatable characters that does not requirean explicit surface model, while being generalizable to novel poses. For agiven video, our method automatically produces an explicit set of 3D pointsrepresenting approximate canonical geometry, and learns an articulateddeformation model that produces pose-dependent point transformations. Thepoints serve both as a scaffold for high-frequency neural features and ananchor for efficiently mapping between observation and canonical space. Wedemonstrate on established benchmarks that our representation overcomeslimitations of prior work operating in either canonical or in observationspace. Moreover, our automatic point extraction approach enables learningmodels of human and animal characters alike, matching the performance of themethods using rigged surface templates despite being more general. Projectwebsite: ", "output": "NPC: Neural Point Characters from Video."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern computer vision, images are typically represented as a fixeduniform grid with some stride and processed via a deep convolutional neuralnetwork. We argue that deforming the grid to better align with thehigh-frequency image content is a more effective strategy. We introduceemph{Deformable Grid} DefGrid, a learnable neural network module that predictslocation offsets of vertices of a 2-dimensional triangular grid, such that theedges of the deformed grid align with image boundaries. We showcase our DefGridin a variety of use cases, i.e., by inserting it as a module at various levelsof processing. We utilize DefGrid as an end-to-end emph{learnable geometricdownsampling} layer that replaces standard pooling methods for reducing featureresolution when feeding images into a deep CNN. We show significantly improvedresults at the same grid resolution compared to using CNNs on uniform grids forthe task of semantic segmentation. We also utilize DefGrid at the output layersfor the task of object mask annotation, and show that reasoning about objectboundaries on our predicted polygonal grid leads to more accurate results overexisting pixel-wise and curve-based approaches. We finally showcase DefGrid asa standalone module for unsupervised image partitioning, showing superiorperformance over existing approaches. Project website:<a href=\" http URL</a>", "output": "Beyond Fixed Grid: Learning Geometric Image Representation with a Deformable Grid."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing methods for enhancing dark images captured in a very low-lightenvironment assume that the intensity level of the optimal output image isknown and already included in the training set. However, this assumption oftendoes not hold, leading to output images that contain visual imperfections suchas dark regions or low contrast. To facilitate the training and evaluation ofadaptive models that can overcome this limitation, we have created a dataset of1500 raw images taken in both indoor and outdoor low-light conditions. Based onour dataset, we introduce a deep learning model capable of enhancing inputimages with a wide range of intensity levels at runtime, including ones thatare not seen during training. Our experimental results demonstrate that ourproposed dataset combined with our model can consistently and effectivelyenhance images across a wide range of diverse and challenging scenarios.", "output": "Adaptive Enhancement of Extreme Low-Light Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As research in deep neural networks advances, deep convolutional networksbecome promising for autonomous driving tasks. In particular, there is anemerging trend of employing end-to-end neural network models for autonomousdriving. However, previous research has shown that deep neural networkclassifiers are vulnerable to adversarial attacks. While for regression tasks,the effect of adversarial attacks is not as well understood. In this research,we devise two white-box targeted attacks against end-to-end autonomous drivingmodels. Our attacks manipulate the behavior of the autonomous driving system byperturbing the input image. In an average of 800 attacks with the same attackstrength (epsilon=1), the image-specific and image-agnostic attack deviates thesteering angle from the original output by 0.478 and 0.111, respectively, whichis much stronger than random noises that only perturbs the steering angle by0.002 (The steering angle ranges from [-1, 1]). Both attacks can be initiatedin real-time on CPUs without employing GPUs. Demo video:", "output": "Adversarial Driving: Attacking End-to-End Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural network can easily overfit to even noisy labels due to its highcapacity, which degrades the generalization performance of a model. To overcomethis issue, we propose a new approach for learning from noisy labels (LNL) viapost-training, which can significantly improve the generalization performanceof any pre-trained model on noisy label data. To this end, we rather exploitthe overfitting property of a trained model to identify mislabeled samples.Specifically, our post-training approach gradually removes samples with highinfluence on the decision boundary and refines the decision boundary to improvegeneralization performance. Our post-training approach creates great synergieswhen combined with the existing LNL methods. Experimental results on variousreal-world and synthetic benchmark datasets demonstrate the validity of ourapproach in diverse realistic scenarios.", "output": "Influential Rank: A New Perspective of Post-training for Robust Model against Noisy Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In many applications, we are constrained to learn classifiers from verylimited data (few-shot classification). The task becomes even more challengingif it is also required to identify samples from unknown categories (open-setclassification). Learning a good abstraction for a class with very few samplesis extremely difficult, especially under open-set settings. As a result,open-set recognition has received minimal attention in the few-shot setting.However, it is a critical task in many applications like environmentalmonitoring, where the number of labeled examples for each class is limited.Existing few-shot open-set recognition (FSOSR) methods rely on thresholdingschemes, with some considering uniform probability for open-class samples.However, this approach is often inaccurate, especially for fine-grainedcategorization, and makes them highly sensitive to the choice of a threshold.To address these concerns, we propose Reconstructing Exemplar-based Few-shotOpen-set ClaSsifier (ReFOCS). By using a novel exemplar reconstruction-basedmeta-learning strategy ReFOCS streamlines FSOSR eliminating the need for acarefully tuned threshold by learning to be self-aware of the openness of asample. The exemplars, act as class representatives and can be either providedin the training dataset or estimated in the feature domain. By testing on awide variety of datasets, we show ReFOCS to outperform multiplestate-of-the-art methods.", "output": "Reconstruction guided Meta-learning for Few Shot Open Set Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The capability of a mobile robot to efficiently and safely perform complexmissions is limited by its knowledge of the environment, namely the situation.Advanced reasoning, decision-making, and execution skills enable an intelligentagent to act autonomously in unknown environments. Situational Awareness (SA)is a fundamental capability of humans that has been deeply studied in variousfields, such as psychology, military, aerospace, and education. Nevertheless,it has yet to be considered in robotics, which has focused on singlecompartmentalized concepts such as sensing, spatial perception, sensor fusion,state estimation, and Simultaneous Localization and Mapping (SLAM). Hence, thepresent research aims to connect the broad multidisciplinary existing knowledgeto pave the way for a complete SA system for mobile robotics that we deemparamount for autonomy. To this aim, we define the principal components tostructure a robotic SA and their area of competence. Accordingly, this paperinvestigates each aspect of SA, surveying the state-of-the-art roboticsalgorithms that cover them, and discusses their current limitations.Remarkably, essential aspects of SA are still immature since the currentalgorithmic development restricts their performance to only specificenvironments. Nevertheless, Artificial Intelligence (AI), particularly DeepLearning (DL), has brought new methods to bridge the gap that maintains thesefields apart from the deployment to real-world scenarios. Furthermore, anopportunity has been discovered to interconnect the vastly fragmented space ofrobotic comprehension algorithms through the mechanism of Situational Graph(S-Graph), a generalization of the well-known scene graph. Therefore, wefinally shape our vision for the future of robotic Situational Awareness bydiscussing interesting recent research directions.", "output": "From SLAM to Situational Awareness: Challenges and Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-Object Tracking (MOT) has achieved aggressive progress and derived manyexcellent deep learning trackers. Meanwhile, most deep learning models areknown to be vulnerable to adversarial examples that are crafted with smallperturbations but could mislead the model prediction. In this work, we observethat the robustness on the MOT trackers is rarely studied, and it ischallenging to attack the MOT system since its mature association algorithmsare designed to be robust against errors during the tracking. To this end, weanalyze the vulnerability of popular MOT trackers and propose a noveladversarial attack method called Tracklet-Switch (TraSw) against the completetracking pipeline of MOT. The proposed TraSw can fool the advanced deeppedestrian trackers (i.e., FairMOT and ByteTrack), causing them fail to trackthe targets in the subsequent frames by perturbing very few frames. Experimentson the MOT-Challenge datasets (i.e., 2DMOT15, MOT17, and MOT20) show that TraSwcan achieve an extraordinarily high success attack rate of over 95% byattacking only four frames on average. To our knowledge, this is the first workon the adversarial attack against the pedestrian MOT trackers. Code isavailable at  .", "output": "Tracklet-Switch Adversarial Attack against Pedestrian Multi-Object Tracking Trackers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent 2D-to-3D human pose estimation works tend to utilize the graphstructure formed by the topology of the human skeleton. However, we argue thatthis skeletal topology is too sparse to reflect the body structure and sufferfrom serious 2D-to-3D ambiguity problem. To overcome these weaknesses, wepropose a novel graph convolution network architecture, Hierarchical GraphNetworks (HGN). It is based on denser graph topology generated by ourmulti-scale graph structure building strategy, thus providing more delicategeometric information. The proposed architecture contains three sparse-to-finerepresentation subnetworks organized in parallel, in which multi-scalegraph-structured features are processed and exchange information through anovel feature fusion strategy, leading to rich hierarchical representations. Wealso introduce a 3D coarse mesh constraint to further boost detail-relatedfeature learning. Extensive experiments demonstrate that our HGN achieves thestate-of-the art performance with reduced network parameters. Code is releasedat", "output": "Hierarchical Graph Networks for 3D Human Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Infrared small target super-resolution (SR) aims to recover reliable anddetailed high-resolution image with high-contrast targets from itslow-resolution counterparts. Since the infrared small target lacks color andfine structure information, it is significant to exploit the supplementaryinformation among sequence images to enhance the target. In this paper, wepropose the first infrared small target SR method named local motion andcontrast prior driven deep network (MoCoPnet) to integrate the domain knowledgeof infrared small target into deep network, which can mitigate the intrinsicfeature scarcity of infrared small targets. Specifically, motivated by thelocal motion prior in the spatio-temporal dimension, we propose a localspatio-temporal attention module to perform implicit frame alignment andincorporate the local spatio-temporal information to enhance the local features(especially for small targets). Motivated by the local contrast prior in thespatial dimension, we propose a central difference residual group toincorporate the central difference convolution into the feature extractionbackbone, which can achieve center-oriented gradient-aware feature extractionto further improve the target contrast. Extensive experiments have demonstratedthat our method can recover accurate spatial dependency and improve the targetcontrast. Comparative results show that MoCoPnet can outperform thestate-of-the-art video SR and single image SR methods in terms of both SRperformance and target enhancement. Based on the SR results, we furtherinvestigate the influence of SR on infrared small target detection and theexperimental results demonstrate that MoCoPnet promotes the detectionperformance. The code is available at ", "output": "Local Motion and Contrast Priors Driven Deep Network for Infrared Small Target Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose LiftReg, a 2D/3D deformable registration approach. LiftReg is adeep registration framework which is trained using sets of digitallyreconstructed radiographs (DRR) and computed tomography (CT) image pairs. Byusing simulated training data, LiftReg can use a high-quality CT-CT imagesimilarity measure, which helps the network to learn a high-quality deformationspace. To further improve registration quality and to address the inherentdepth ambiguities of very limited angle acquisitions, we propose to usefeatures extracted from the backprojected 2D images and a statisticaldeformation model. We test our approach on the DirLab lung registration datasetand show that it outperforms an existing learning-based pairwise registrationapproach.", "output": "LiftReg: Limited Angle 2D/3D Deformable Registration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite recent advances of AI, story understanding remains an open andunder-investigated problem. We collect, preprocess, and publicly release avideo-language story dataset, Synopses of Movie Narratives (SYMON), containing5,193 video summaries of popular movies and TV series. SYMON capturesnaturalistic story-telling videos for human audience made by human creators. Asa prototypical and naturalistic story dataset, SYMON features high coverage ofmultimodal story events, abundant mental-state descriptions, and large semanticgaps between the visual and the textual modalities. We establish benchmarks onvideo-text retrieval and zero-shot alignment on movie summary videos, whichshowcase the importance of in-domain data in story understanding. With SYMON,we hope to lay the groundwork for progress in multimodal story understanding.", "output": "Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classification networks can be used to localize and segment objects in imagesby means of class activation maps (CAMs). However, without pixel-levelannotations, classification networks are known to (1) mainly focus ondiscriminative regions, and (2) to produce diffuse CAMs without well-definedprediction contours. In this work, we approach both problems with twocontributions for improving CAM learning. First, we incorporate importancesampling based on the class-wise probability mass function induced by the CAMsto produce stochastic image-level class predictions. This results in CAMs whichactivate over a larger extent of objects. Second, we formulate a featuresimilarity loss term which aims to match the prediction contours with edges inthe image. As a third contribution, we conduct experiments on the PASCAL VOC2012 benchmark dataset to demonstrate that these modifications significantlyincrease the performance in terms of contour accuracy, while being comparableto current state-of-the-art methods in terms of region similarity.", "output": "Importance Sampling CAMs for Weakly-Supervised Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning is a setting where machine learning models learn novelconcepts from continuously shifting training data, while simultaneouslyavoiding degradation of knowledge on previously seen classes which maydisappear from the training data for extended periods of time (a phenomenonknown as the catastrophic forgetting problem). Current approaches for continuallearning of a single expanding task (aka class-incremental continual learning)require extensive rehearsal of previously seen data to avoid this degradationof knowledge. Unfortunately, rehearsal comes at a cost to memory, and it mayalso violate data-privacy. Instead, we explore combining knowledge distillationand parameter regularization in new ways to achieve strong continual learningperformance without rehearsal. Specifically, we take a deep dive into commoncontinual learning techniques: prediction distillation, feature distillation,L2 parameter regularization, and EWC parameter regularization. We firstdisprove the common assumption that parameter regularization techniques failfor rehearsal-free continual learning of a single, expanding task. Next, weexplore how to leverage knowledge from a pre-trained model in rehearsal-freecontinual learning and find that vanilla L2 parameter regularizationoutperforms EWC parameter regularization and feature distillation. Finally, weexplore the recently popular ImageNet-R benchmark, and show that L2 parameterregularization implemented in self-attention blocks of a ViT transformeroutperforms recent popular prompting for continual learning methods.", "output": "A Closer Look at Rehearsal-Free Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human motion transfer (HMT) aims to generate a video clip for the targetsubject by imitating the source subject's motion. Although previous methodshave achieved remarkable results in synthesizing good-quality videos, thosemethods omit the effects of individualized motion information from the sourceand target motions, textit{e.g.}, fine and high-frequency motion details, onthe realism of the motion in the generated video. To address this problem, wepropose an identity-preserved HMT network (textit{IDPres}), which follows thepipeline of the skeleton-based method. textit{IDpres} takes the individualizedmotion and skeleton information to enhance motion representations and improvethe reality of motions in the generated videos. With individualized motion, ourmethod focuses on fine-grained disentanglement and synthesis of motion. Inorder to improve the representation capability in latent space and facilitatethe training of textit{IDPres}, we design a training scheme, which allowstextit{IDPres} to disentangle different representations simultaneously andcontrol them to synthesize ideal motions accurately. Furthermore, to our bestknowledge, there are no available metrics for evaluating the proportion ofidentity information (both individualized motion and skeleton information) inthe generated video. Therefore, we propose a novel quantitative metric calledIdentity Score (textit{IDScore}) based on gait recognition. We also collecteda dataset with 101 subjects' solo-dance videos from the public domain, named$Dancer101$, to evaluate the method. The comprehensive experiments show theproposed method outperforms state-of-the-art methods in terms of reconstructionaccuracy and realistic motion.", "output": "An Identity-Preserved Framework for Human Motion Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an approach to learning regular spatial transformations betweenimage pairs in the context of medical image registration. Contrary tooptimization-based registration techniques and many modern learning-basedmethods, we do not directly penalize transformation irregularities but insteadpromote transformation regularity via an inverse consistency penalty. We use aneural network to predict a map between a source and a target image as well asthe map when swapping the source and target images. Different from existingapproaches, we compose these two resulting maps and regularize deviations ofthe $bf{Jacobian}$ of this composition from the identity matrix. Thisregularizer -- $texttt{GradICON}$ -- results in much better convergence whentraining registration models compared to promoting inverse consistency of thecomposition of maps directly while retaining the desirable implicitregularization effects of the latter. We achieve state-of-the-art registrationperformance on a variety of real-world medical image datasets using a singleset of hyperparameters and a single non-dataset-specific training protocol.", "output": "$\\texttt{GradICON}$: Approximate Diffeomorphisms via Gradient Inverse Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous driving, in recent years, has been receiving increasing attentionfor its potential to relieve drivers' burdens and improve the safety ofdriving. In modern autonomous driving pipelines, the perception system is anindispensable component, aiming to accurately estimate the status ofsurrounding environments and provide reliable observations for prediction andplanning. 3D object detection, which intelligently predicts the locations,sizes, and categories of the critical 3D objects near an autonomous vehicle, isan important part of a perception system. This paper reviews the advances in 3Dobject detection for autonomous driving. First, we introduce the background of3D object detection and discuss the challenges in this task. Second, we conducta comprehensive survey of the progress in 3D object detection from the aspectsof models and sensory inputs, including LiDAR-based, camera-based, andmulti-modal detection approaches. We also provide an in-depth analysis of thepotentials and challenges in each category of methods. Additionally, wesystematically investigate the applications of 3D object detection in drivingsystems. Finally, we conduct a performance analysis of the 3D object detectionapproaches, and we further summarize the research trends over the years andprospect the future directions of this area.", "output": "3D Object Detection for Autonomous Driving: A Comprehensive Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-scale architectures and attention modules have shown effectiveness inmany deep learning-based image de-raining methods. However, manually designingand integrating these two components into a neural network requires a bulk oflabor and extensive expertise. In this article, a high-performance multi-scaleattentive neural architecture search (MANAS) framework is technically developedfor image deraining. The proposed method formulates a new multi-scale attentionsearch space with multiple flexible modules that are favorite to the imagede-raining task. Under the search space, multi-scale attentive cells are built,which are further used to construct a powerful image de-raining network. Theinternal multiscale attentive architecture of the de-raining network issearched automatically through a gradient-based search algorithm, which avoidsthe daunting procedure of the manual design to some extent. Moreover, in orderto obtain a robust image de-raining model, a practical and effectivemulti-to-one training strategy is also presented to allow the de-rainingnetwork to get sufficient background information from multiple rainy imageswith the same background scene, and meanwhile, multiple loss functionsincluding external loss, internal loss, architecture regularization loss, andmodel complexity loss are jointly optimized to achieve robust de-rainingperformance and controllable model complexity. Extensive experimental resultson both synthetic and realistic rainy images, as well as the down-stream visionapplications (i.e., objection detection and segmentation) consistentlydemonstrate the superiority of our proposed method. The code is publiclyavailable at ", "output": "Multi-scale Attentive Image De-raining Networks via Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel and flexible attention based U-Net architecture referredto as \"Voxels-Intersecting Along Orthogonal Levels Attention U-Net\"(viola-Unet), for intracranial hemorrhage (ICH) segmentation task in theINSTANCE 2022 Data Challenge on non-contrast computed tomography (CT). Theperformance of ICH segmentation was improved by efficiently incorporating fusedspatially orthogonal and cross-channel features via our proposed Violaattention plugged into the U-Net decoding branches. The viola-Unet outperformedthe strong baseline nnU-Net models during both 5-fold cross validation andonline validation. Our solution was the winner of the challenge validationphase in terms of all four performance metrics (i.e., DSC, HD, NSD, and RVD).The code base, pretrained weights, and docker image of the viola-Unet AI toolare publicly available at url{", "output": "Voxels Intersecting along Orthogonal Levels Attention U-Net for Intracerebral Haemorrhage Segmentation in Head CT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in neural radiance fields (NeRFs) achieve state-of-the-artnovel view synthesis and facilitate dense estimation of scene properties.However, NeRFs often fail for large, unbounded scenes that are captured undervery sparse views with the scene content concentrated far away from the camera,as is typical for field robotics applications. In particular, NeRF-stylealgorithms perform poorly: (1) when there are insufficient views with littlepose diversity, (2) when scenes contain saturation and shadows, and (3) whenfinely sampling large unbounded scenes with fine structures becomescomputationally intensive.This paper proposes CLONeR, which significantly improves upon NeRF byallowing it to model large outdoor driving scenes that are observed from sparseinput sensor views. This is achieved by decoupling occupancy and color learningwithin the NeRF framework into separate Multi-Layer Perceptrons (MLPs) trainedusing LiDAR and camera data, respectively. In addition, this paper proposes anovel method to build differentiable 3D Occupancy Grid Maps (OGM) alongside theNeRF model, and leverage this occupancy grid for improved sampling of pointsalong a ray for volumetric rendering in metric space.Through extensive quantitative and qualitative experiments on scenes from theKITTI dataset, this paper demonstrates that the proposed method outperformsstate-of-the-art NeRF models on both novel view synthesis and dense depthprediction tasks when trained on sparse input data.", "output": "CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Intelligent robots rely on object detection models to perceive theenvironment. Following advances in deep learning security it has been revealedthat object detection models are vulnerable to adversarial attacks. However,prior research primarily focuses on attacking static images or offline videos.Therefore, it is still unclear if such attacks could jeopardize real-worldrobotic applications in dynamic environments. This paper bridges this gap bypresenting the first real-time online attack against object detection models.We devise three attacks that fabricate bounding boxes for nonexistent objectsat desired locations. The attacks achieve a success rate of about 90% withinabout 20 iterations. The demo video is available at", "output": "Adversarial Detection: Attacking Object Detection in Real Time."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning representative, robust and discriminative information from images isessential for effective person re-identification (Re-Id). In this paper, wepropose a compound approach for end-to-end discriminative deep feature learningfor person Re-Id based on both body and hand images. We carefully design theLocal-Aware Global Attention Network (LAGA-Net), a multi-branch deep networkarchitecture consisting of one branch for spatial attention, one branch forchannel attention, one branch for global feature representations and anotherbranch for local feature representations. The attention branches focus on therelevant features of the image while suppressing the irrelevant backgrounds. Inorder to overcome the weakness of the attention mechanisms, equivariant topixel shuffling, we integrate relative positional encodings into the spatialattention module to capture the spatial positions of pixels. The global branchintends to preserve the global context or structural information. For the thelocal branch, which intends to capture the fine-grained information, we performuniform partitioning to generate stripes on the conv-layer horizontally. Weretrieve the parts by conducting a soft partition without explicitlypartitioning the images or requiring external cues such as pose estimation. Aset of ablation study shows that each component contributes to the increasedperformance of the LAGA-Net. Extensive evaluations on four popular body-basedperson Re-Id benchmarks and two publicly available hand datasets demonstratethat our proposed method consistently outperforms existing state-of-the-artmethods.", "output": "Local-Aware Global Attention Network for Person Re-Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning fine-grained movements is a challenging topic in robotics,particularly in the context of robotic hands. One specific instance of thischallenge is the acquisition of fingerspelling sign language in robots. In thispaper, we propose an approach for learning dexterous motor imitation from videoexamples without additional information. To achieve this, we first build a URDFmodel of a robotic hand with a single actuator for each joint. We then leveragepre-trained deep vision models to extract the 3D pose of the hand from RGBvideos. Next, using state-of-the-art reinforcement learning algorithms formotion imitation (namely, proximal policy optimization and soft actor-critic),we train a policy to reproduce the movement extracted from the demonstrations.We identify the optimal set of hyperparameters for imitation based on areference motion. Finally, we demonstrate the generalizability of our approachby testing it on six different tasks, corresponding to fingerspelled letters.Our results show that our approach is able to successfully imitate thesefine-grained movements without additional information, highlighting itspotential for real-world applications in robotics.", "output": "Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A network based on complementary consistency training, called CC-Net, hasbeen proposed for semi-supervised left atrium image segmentation. CC-Netefficiently utilizes unlabeled data from the perspective of complementaryinformation to address the problem of limited ability of existingsemi-supervised segmentation algorithms to extract information from unlabeleddata. The complementary symmetric structure of CC-Net includes a main model andtwo auxiliary models. The complementary model inter-perturbations between themain and auxiliary models force consistency to form complementary consistency.The complementary information obtained by the two auxiliary models helps themain model to effectively focus on ambiguous areas, while enforcing consistencybetween the models is advantageous in obtaining decision boundaries with lowuncertainty. CC-Net has been validated on two public datasets. In the case ofspecific proportions of labeled data, compared with current advancedalgorithms, CC-Net has the best semi-supervised segmentation performance. Ourcode is publicly available at ", "output": "Complementary consistency semi-supervised learning for 3D left atrial image segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Coronavirus Disease 2019(COVID-19) spread rapidly around the world, causing aseries of severe health crises. Automated segmentation of lung infections basedon Deep Convolutional Neural Network(DCNN) from medical images such as CT,X-ray, etc, displayed a huge potential for accurate diagnosis and quantitativeanalysis. Most COVID-19 medical images show blurred boundaries, dense noisepoints, low contrast, and significant variation in the shape and size oflesions. Although various models based on UNet have been proposed, moreoptimisation is required to obtain accurate segmentation and meet complexcomputational needs. Furthermore, the existing COVID-19 infections segmentationDCNN based methods are only suitable for single modality medical images. Tosolve these problems, this paper proposes a symmetric Encoder-Decodersegmentation framework named MS-DCANet. We introduce Tokenized MLP block, anovel attention scheme that uses a shift-window mechanism similar to theTransformer to acquire self-attention and achieve local-to-global semanticdependency. MS-DCANet also uses several Dual Channel blocks and a Res-ASPPblock to expand the receptive field and extract multi-scale features. In alarge number of experiments on COVID-19 datasets using both X-ray and CTimages, MS-DCANet achieved state-of-the-art performance compared with otherUNet models. MS-DCANet can also improve trade-off accuracy and complexity. Toprove the proposed model's strong generalisability, we also apply MS-DCANet tothe segmentation of skin tumours from dermoscopy images and hand bone fromX-ray images with satisfactory results.", "output": "MS-DCANet: A MLP-based Multi-Scale Feature Framework For COVID-19 Infection Segmentation From Medical Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting the future motion of dynamic agents is of paramount importance toensuring safety and assessing risks in motion planning for autonomous robots.In this study, we propose a two-stage motion prediction method, called R-Pred,designed to effectively utilize both scene and interaction context using acascade of the initial trajectory proposal and trajectory refinement networks.The initial trajectory proposal network produces M trajectory proposalscorresponding to the M modes of the future trajectory distribution. Thetrajectory refinement network enhances each of the M proposals using 1)tube-query scene attention (TQSA) and 2) proposal-level interaction attention(PIA) mechanisms. TQSA uses tube-queries to aggregate local scene contextfeatures pooled from proximity around trajectory proposals of interest. PIAfurther enhances the trajectory proposals by modeling inter-agent interactionsusing a group of trajectory proposals selected by their distances fromneighboring agents. Our experiments conducted on Argoverse and nuScenesdatasets demonstrate that the proposed refinement network provides significantperformance improvements compared to the single-stage baseline and that R-Predachieves state-of-the-art performance in some categories of the benchmarks.", "output": "Two-Stage Context-Aware model for Predicting Future Motion of Dynamic Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of predicting the 3D shape, articulation, viewpoint,texture, and lighting of an articulated animal like a horse given a single testimage as input. We present a new method, dubbed MagicPony, that learns thispredictor purely from in-the-wild single-view images of the object category,with minimal assumptions about the topology of deformation. At its core is animplicit-explicit representation of articulated shape and appearance, combiningthe strengths of neural fields and meshes. In order to help the modelunderstand an object's shape and pose, we distil the knowledge captured by anoff-the-shelf self-supervised vision transformer and fuse it into the 3D model.To overcome local optima in viewpoint estimation, we further introduce a newviewpoint sampling scheme that comes at no additional training cost. MagicPonyoutperforms prior work on this challenging task and demonstrates excellentgeneralisation in reconstructing art, despite the fact that it is only trainedon real images.", "output": "MagicPony: Learning Articulated 3D Animals in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent LSS-based multi-view 3D object detection has made tremendous progress,by processing the features in Brid-Eye-View (BEV) via the convolutionaldetector. However, the typical convolution ignores the radial symmetry of theBEV features and increases the difficulty of the detector optimization. Topreserve the inherent property of the BEV features and ease the optimization,we propose an azimuth-equivariant convolution (AeConv) and anazimuth-equivariant anchor. The sampling grid of AeConv is always in the radialdirection, thus it can learn azimuth-invariant BEV features. The proposedanchor enables the detection head to learn predicting azimuth-irrelevanttargets. In addition, we introduce a camera-decoupled virtual depth to unifythe depth prediction for the images with different camera intrinsic parameters.The resultant detector is dubbed Azimuth-equivariant Detector (AeDet).Extensive experiments are conducted on nuScenes, and AeDet achieves a 62.0%NDS, surpassing the recent multi-view 3D object detectors such as PETRv2 andBEVDepth by a large margin. Project page: ", "output": "AeDet: Azimuth-invariant Multi-view 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a generalized few-shot segmentation framework with astraightforward training process and an easy-to-optimize inference phase. Inparticular, we propose a simple yet effective model based on the well-knownInfoMax principle, where the Mutual Information (MI) between the learnedfeature representations and their corresponding predictions is maximized. Inaddition, the terms derived from our MI-based formulation are coupled with aknowledge distillation term to retain the knowledge on base classes. With asimple training process, our inference model can be applied on top of anysegmentation network trained on base classes. The proposed inference yieldssubstantial improvements on the popular few-shot segmentation benchmarks,PASCAL-$5^i$ and COCO-$20^i$. Particularly, for novel classes, the improvementgains range from 7% to 26% (PASCAL-$5^i$) and from 3% to 12% (COCO-$20^i$) inthe 1-shot and 5-shot scenarios, respectively. Furthermore, we propose a morechallenging setting, where performance gaps are further exacerbated. Our codeis publicly available at ", "output": "A Strong Baseline for Generalized Few-Shot Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present a novel and effective framework, named 4K-NeRF, topursue high fidelity view synthesis on the challenging scenarios of ultra highresolutions, building on the methodology of neural radiance fields (NeRF). Therendering procedure of NeRF-based methods typically relies on a pixel-wisemanner in which rays (or pixels) are treated independently on both training andinference phases, limiting its representational ability on describing subtledetails, especially when lifting to a extremely high resolution. We address theissue by exploring ray correlation to enhance high-frequency details recovery.Particularly, we use the 3D-aware encoder to model geometric informationeffectively in a lower resolution space and recover fine details through the3D-aware decoder, conditioned on ray features and depths estimated by theencoder. Joint training with patch-based sampling further facilitates ourmethod incorporating the supervision from perception oriented regularizationbeyond pixel-wise loss. Benefiting from the use of geometry-aware localcontext, our method can significantly boost rendering quality on high-frequencydetails compared with modern NeRF methods, and achieve the state-of-the-artvisual quality on 4K ultra-high-resolution scenarios. Code Available aturl{", "output": "4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a new self-supervised method for pre-training the backbone of deepperception models operating on point clouds. The core idea is to train themodel on a pretext task which is the reconstruction of the surface on which the3D points are sampled, and to use the underlying latent vectors as input to theperception head. The intuition is that if the network is able to reconstructthe scene surface, given only sparse input points, then it probably alsocaptures some fragments of semantic information, that can be used to boost anactual perception task. This principle has a very simple formulation, whichmakes it both easy to implement and widely applicable to a large range of 3Dsensors and deep networks performing semantic segmentation or object detection.In fact, it supports a single-stream pipeline, as opposed to most contrastivelearning approaches, allowing training on limited resources. We conductedextensive experiments on various autonomous driving datasets, involving verydifferent kinds of lidars, for both semantic segmentation and object detection.The results show the effectiveness of our method to learn usefulrepresentations without any annotation, compared to existing approaches. Codeis available at ", "output": "ALSO: Automotive Lidar Self-supervision by Occupancy estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Document Visual Question Answering (DocVQA) refers to the task of answeringquestions from document images. Existing work on DocVQA only considerssingle-page documents. However, in real scenarios documents are mostly composedof multiple pages that should be processed altogether. In this work we extendDocVQA to the multi-page scenario. For that, we first create a new dataset,MP-DocVQA, where questions are posed over multi-page documents instead ofsingle pages. Second, we propose a new hierarchical method, Hi-VT5, based onthe T5 architecture, that overcomes the limitations of current methods toprocess long multi-page documents. The proposed method is based on ahierarchical transformer architecture where the encoder summarizes the mostrelevant information of every page and then, the decoder takes this summarizedinformation to generate the final answer. Through extensive experimentation, wedemonstrate that our method is able, in a single stage, to answer the questionsand provide the page that contains the relevant information to find the answer,which can be used as a kind of explainability measure.", "output": "Hierarchical multimodal transformers for Multi-Page DocVQA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose an adaptive multi-agent clustering recognition system that can beself-supervised driven, based on a temporal sequences continuous learningmechanism with adaptability. The system is designed to use some differentfunctional agents to build up a connection structure to improve adaptability tocope with environmental diverse demands, by predicting the input of the agentto drive the agent to achieve the act of clustering recognition of sequencesusing the traditional algorithmic approach. Finally, the feasibilityexperiments of video behavior clustering demonstrate the feasibility of thesystem to cope with dynamic situations. Our work is placedherefootnote{", "output": "Adaptive Multi-Agent Continuous Learning System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce ensembles of stochastic neural networks to approximate theBayesian posterior, combining stochastic methods such as dropout with deepensembles. The stochastic ensembles are formulated as families of distributionsand trained to approximate the Bayesian posterior with variational inference.We implement stochastic ensembles based on Monte Carlo dropout, DropConnect anda novel non-parametric version of dropout and evaluate them on a toy problemand CIFAR image classification. For both tasks, we test the quality of theposteriors directly against Hamiltonian Monte Carlo simulations. Our resultsshow that stochastic ensembles provide more accurate posterior estimates thanother popular baselines for Bayesian inference.", "output": "Bayesian posterior approximation with stochastic ensembles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation for extracting buildings and roads, from unmannedaerial vehicle (UAV) remote sensing images by deep learning becomes a moreefficient and convenient method than traditional manual segmentation insurveying and mapping field. In order to make the model lightweight and improvethe model accuracy, A Lightweight Network Using Object Attention (LOANet) forBuildings and Roads from UAV Aerial Remote Sensing Images is proposed. Theproposed network adopts an encoder-decoder architecture in which a LightweightDensely Connected Network (LDCNet) is developed as the encoder. In the decoderpart, the dual multi-scale context modules which consist of the Atrous SpatialPyramid Pooling module (ASPP) and the Object Attention Module (OAM) aredesigned to capture more context information from feature maps of UAV remotesensing images. Between ASPP and OAM, a Feature Pyramid Network (FPN) module isused to and fuse multi-scale features extracting from ASPP. A private datasetof remote sensing images taken by UAV which contains 2431 training sets, 945validation sets, and 475 test sets is constructed. The proposed model performswell on this dataset, with only 1.4M parameters and 5.48G floating-pointoperations (FLOPs), achieving a mean intersection-over-union ratio (mIoU) of71.12%. More extensive experiments on the public LoveDA dataset and CITY-OSMdataset to further verify the effectiveness of the proposed model withexcellent results on mIoU of 65.27% and 74.39%, respectively.", "output": "LOANet: A Lightweight Network Using Object Attention for Extracting Buildings and Roads from UAV Aerial Remote Sensing Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Test-time adaptation (TTA) is the problem of updating a pre-trained sourcemodel at inference time given test input(s) from a different target domain.Most existing TTA approaches assume the setting in which the target domain isstationary, i.e., all the test inputs come from a single target domain.However, in many practical settings, the test input distribution might exhibita lifelong/continual shift over time. Moreover, existing TTA approaches alsolack the ability to provide reliable uncertainty estimates, which is crucialwhen distribution shifts occur between the source and target domain. To addressthese issues, we present PETAL (Probabilistic lifElong Test-time Adaptationwith seLf-training prior), which solves lifelong TTA using a probabilisticapproach, and naturally results in (1) a student-teacher framework, where theteacher model is an exponential moving average of the student model, and (2)regularizing the model updates at inference time using the source model as aregularizer. To prevent model drift in the lifelong/continual TTA setting, wealso propose a data-driven parameter restoration technique which contributes toreducing the error accumulation and maintaining the knowledge of recent domainsby restoring only the irrelevant parameters. In terms of predictive error rateas well as uncertainty based metrics such as Brier score and negativelog-likelihood, our method achieves better results than the currentstate-of-the-art for online lifelong test-time adaptation across variousbenchmarks, such as CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCCdatasets. The source code for our approach is accessible at", "output": "A Probabilistic Framework for Lifelong Test-Time Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To interpret deep neural networks, one main approach is to dissect the visualinput and find the prototypical parts responsible for the classification.However, existing methods often ignore the hierarchical relationship betweenthese prototypes, and thus can not explain semantic concepts at both higherlevel (e.g., water sports) and lower level (e.g., swimming). In this paperinspired by human cognition system, we leverage hierarchal information to dealwith uncertainty: When we observe water and human activity, but no definitiveaction it can be recognized as the water sports parent class. Only afterobserving a person swimming can we definitively refine it to the swimmingaction. To this end, we propose HIerarchical Prototype Explainer (HIPE) tobuild hierarchical relations between prototypes and classes. HIPE enables areasoning process for video action classification by dissecting the input videoframes on multiple levels of the class hierarchy, our method is also applicableto other video tasks. The faithfulness of our method is verified by reducingaccuracy-explainability trade off on ActivityNet and UCF-101 while providingmulti-level explanations.", "output": "Hierarchical Explanations for Video Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper focuses on developing modern, efficient, lightweight models fordense predictions while trading off parameters, FLOPs, and performance.Inverted Residual Block (IRB) serves as the infrastructure for lightweightCNNs, but no counterpart has been recognized by attention-based studies. Thiswork rethinks lightweight infrastructure from efficient IRB and effectivecomponents of Transformer from a unified perspective, extending CNN-based IRBto attention-based models and abstracting a one-residual Meta Mobile Block(MMB) for lightweight model design. Following simple but effective designcriterion, we deduce a modern Inverted Residual Mobile Block (iRMB) and build aResNet-like Efficient MOdel (EMO) with only iRMB for down-stream tasks.Extensive experiments on ImageNet-1K, COCO2017, and ADE20K benchmarksdemonstrate the superiority of our EMO over state-of-the-art methods, e.g.,EMO-1M/2M/5M achieve 71.5, 75.1, and 78.4 Top-1 that surpass equal-orderCNN-/Attention-based models, while trading-off the parameter, efficiency, andaccuracy well: running 2.8-4.0 faster than EdgeNeXt on iPhone14. Code isavailable.", "output": "Rethinking Mobile Block for Efficient Attention-based Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There has been tremendous progress in large-scale text-to-image synthesisdriven by diffusion models enabling versatile downstream applications such as3D object synthesis from texts, image editing, and customized generation. Wepresent a generic approach using latent diffusion models as powerful imagepriors for various visual synthesis tasks. Existing methods that utilize suchpriors fail to use these models' full capabilities. To improve this, our coreideas are 1) a feature matching loss between features from different layers ofthe decoder to provide detailed guidance and 2) a KL divergence loss toregularize the predicted latent features and stabilize the training. Wedemonstrate the efficacy of our approach on three different applications,text-to-3D, StyleGAN adaptation, and layered image editing. Extensive resultsshow our method compares favorably against baselines.", "output": "Text-driven Visual Synthesis with Latent Diffusion Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Safe human-robot collaboration (HRC) has recently gained a lot of interestwith the emerging Industry 5.0 paradigm. Conventional robots are being replacedwith more intelligent and flexible collaborative robots (cobots). Safe andefficient collaboration between cobots and humans largely relies on the cobot'scomprehensive semantic understanding of the dynamic surrounding of industrialenvironments. Despite the importance of semantic understanding for suchapplications, 3D semantic segmentation of collaborative robot workspaces lackssufficient research and dedicated datasets. The performance limitation causedby insufficient datasets is called 'data hunger' problem. To overcome thiscurrent limitation, this work develops a new dataset specifically designed forthis use case, named \"COVERED\", which includes point-wise annotated pointclouds of a robotic cell. Lastly, we also provide a benchmark of currentstate-of-the-art (SOTA) algorithm performance on the dataset and demonstrate areal-time semantic segmentation of a collaborative robot workspace using amulti-LiDAR system. The promising results from using the trained Deep Networkson a real-time dynamically changing situation shows that we are on the righttrack. Our perception pipeline achieves 20Hz throughput with a prediction pointaccuracy of $&gt;$96% and $&gt;$92% mean intersection over union (mIOU) whilemaintaining an 8Hz throughput.", "output": "COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive self-supervised learning methods famously produce high qualitytransferable representations by learning invariances to different dataaugmentations. Invariances established during pre-training can be interpretedas strong inductive biases. However these may or may not be helpful, dependingon if they match the invariance requirements of downstream tasks or not. Thishas led to several attempts to learn task-specific invariances duringpre-training, however, these methods are highly compute intensive and tediousto train. We introduce the notion of amortised invariance learning forcontrastive self supervision. In the pre-training stage, we parameterize thefeature extractor by differentiable invariance hyper-parameters that controlthe invariances encoded by the representation. Then, for any downstream task,both linear readout and task-specific invariance requirements can beefficiently and effectively learned by gradient-descent. We evaluate the notionof amortised invariances for contrastive learning over two differentmodalities: vision and audio, on two widely-used contrastive learning methodsin vision: SimCLR and MoCo-v2 with popular architectures like ResNets andVision Transformers, and SimCLR with ResNet-18 for audio. We show that ouramortised features provide a reliable way to learn diverse downstream taskswith different invariance requirements, while using a single feature andavoiding task-specific pre-training. This provides an exciting perspective thatopens up new horizons in the field of general purpose representation learning.", "output": "Amortised Invariance Learning for Contrastive Self-Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Designing realistic digital humans is extremely complex. Most data-drivengenerative models used to simplify the creation of their underlying geometricshape do not offer control over the generation of local shape attributes. Inthis paper, we overcome this limitation by introducing a novel loss functiongrounded in spectral geometry and applicable to different neural-network-basedgenerative models of 3D head and body meshes. Encouraging the latent variablesof mesh variational autoencoders (VAEs) or generative adversarial networks(GANs) to follow the local eigenprojections of identity attributes, we improvelatent disentanglement and properly decouple the attribute creation.Experimental results show that our local eigenprojection disentangled (LED)models not only offer improved disentanglement with respect to thestate-of-the-art, but also maintain good generation capabilities with trainingtimes comparable to the vanilla implementations of the models.", "output": "3D Generative Model Latent Disentanglement via Local Eigenprojection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans constantly contact objects to move and perform tasks. Thus, detectinghuman-object contact is important for building human-centered artificialintelligence. However, there exists no robust method to detect contact betweenthe body and the scene from an image, and there exists no dataset to learn sucha detector. We fill this gap with HOT (\"Human-Object conTact\"), a new datasetof human-object contacts for images. To build HOT, we use two data sources: (1)We use the PROX dataset of 3D human meshes moving in 3D scenes, andautomatically annotate 2D image areas for contact via 3D mesh proximity andprojection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and asktrained annotators to draw polygons for the 2D image areas where contact takesplace. We also annotate the involved body part of the human body. We use ourHOT dataset to train a new contact detector, which takes a single color imageas input, and outputs 2D contact heatmaps as well as the body-part labels thatare in contact. This is a new and challenging task that extends currentfoot-ground or hand-object contact detectors to the full generality of thewhole body. The detector uses a part-attention branch to guide contactestimation through the context of the surrounding body parts and scene. Weevaluate our detector extensively, and quantitative results show that our modeloutperforms baselines, and that all components contribute to betterperformance. Results on images from an online repository show reasonabledetections and generalizability.", "output": "Detecting Human-Object Contact in Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To design fast neural networks, many works have been focusing on reducing thenumber of floating-point operations (FLOPs). We observe that such reduction inFLOPs, however, does not necessarily lead to a similar level of reduction inlatency. This mainly stems from inefficiently low floating-point operations persecond (FLOPS). To achieve faster networks, we revisit popular operators anddemonstrate that such low FLOPS is mainly due to frequent memory access of theoperators, especially the depthwise convolution. We hence propose a novelpartial convolution (PConv) that extracts spatial features more efficiently, bycutting down redundant computation and memory access simultaneously. Buildingupon our PConv, we further propose FasterNet, a new family of neural networks,which attains substantially higher running speed than others on a wide range ofdevices, without compromising on accuracy for various vision tasks. Forexample, on ImageNet-1k, our tiny FasterNet-T0 is $2.8times$, $3.3times$, and$2.4times$ faster than MobileViT-XXS on GPU, CPU, and ARM processors,respectively, while being $2.9%$ more accurate. Our large FasterNet-L achievesimpressive $83.5%$ top-1 accuracy, on par with the emerging Swin-B, whilehaving $36%$ higher inference throughput on GPU, as well as saving $37%$compute time on CPU. Code is available aturl{", "output": "Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores the task of detecting images generated by text-to-imagediffusion models. To evaluate this, we consider images generated from captionsin the MSCOCO and Wikimedia datasets using two state-of-the-art models: StableDiffusion and GLIDE. Our experiments show that it is possible to detect thegenerated images using simple Multi-Layer Perceptrons (MLPs), starting fromfeatures extracted by CLIP, or traditional Convolutional Neural Networks(CNNs). We also observe that models trained on images generated by StableDiffusion can detect images generated by GLIDE relatively well, however, thereverse is not true. Lastly, we find that incorporating the associated textualinformation with the images rarely leads to significant improvement indetection results but that the type of subject depicted in the image can have asignificant impact on performance. This work provides insights into thefeasibility of detecting generated images, and has implications for securityand privacy concerns in real-world applications. The code to reproduce ourresults is available at:", "output": "Detecting Images Generated by Diffusers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Videos depict the change of complex dynamical systems over time in the formof discrete image sequences. Generating controllable videos by learning thedynamical system is an important yet underexplored topic in the computer visioncommunity. This paper presents a novel framework, TiV-ODE, to generate highlycontrollable videos from a static image and a text caption. Specifically, ourframework leverages the ability of Neural Ordinary DifferentialEquations~(Neural ODEs) to represent complex dynamical systems as a set ofnonlinear ordinary differential equations. The resulting framework is capableof generating videos with both desired dynamics and content. Experimentsdemonstrate the ability of the proposed method in generating highlycontrollable and visually consistent videos, and its capability of modelingdynamical systems. Overall, this work is a significant step towards developingadvanced controllable video generation models that can handle complex anddynamic scenes.", "output": "Controllable Video Generation by Learning the Underlying Dynamical System with Neural ODE."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Local feature matching aims at establishing sparse correspondences between apair of images. Recently, detector-free methods present generally betterperformance but are not satisfactory in image pairs with large scaledifferences. In this paper, we propose Patch Area Transportation withSubdivision (PATS) to tackle this issue. Instead of building an expensive imagepyramid, we start by splitting the original image pair into equal-sized patchesand gradually resizing and subdividing them into smaller patches with the samescale. However, estimating scale differences between these patches isnon-trivial since the scale differences are determined by both relative cameraposes and scene structures, and thus spatially varying over image pairs.Moreover, it is hard to obtain the ground truth for real scenes. To this end,we propose patch area transportation, which enables learning scale differencesin a self-supervised manner. In contrast to bipartite graph matching, whichonly handles one-to-one matching, our patch area transportation can deal withmany-to-many relationships. PATS improves both matching accuracy and coverage,and shows superior performance in downstream tasks, such as relative poseestimation, visual localization, and optical flow estimation. The source codeis available at url{", "output": "PATS: Patch Area Transportation with Subdivision for Local Feature Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the remarkable achievement of recent underwater image restorationtechniques, the lack of labeled data has become a major hurdle for furtherprogress. In this work, we propose a mean-teacher based Semi-supervisedUnderwater Image Restoration (Semi-UIR) framework to incorporate the unlabeleddata into network training. However, the naive mean-teacher method suffers fromtwo main problems: (1) The consistency loss used in training might becomeineffective when the teacher's prediction is wrong. (2) Using L1 distance maycause the network to overfit wrong labels, resulting in confirmation bias. Toaddress the above problems, we first introduce a reliable bank to store the\"best-ever\" outputs as pseudo ground truth. To assess the quality of outputs,we conduct an empirical analysis based on the monotonicity property to selectthe most trustworthy NR-IQA method. Besides, in view of the confirmation biasproblem, we incorporate contrastive regularization to prevent the overfittingon wrong labels. Experimental results on both full-reference and non-referenceunderwater benchmarks demonstrate that our algorithm has obvious improvementover SOTA methods quantitatively and qualitatively. Code has been released at", "output": "Contrastive Semi-supervised Learning for Underwater Image Restoration via Reliable Bank."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most models of visual attention are aimed at predicting either top-down orbottom-up control, as studied using different visual search and free-viewingtasks. We propose Human Attention Transformer (HAT), a single model predictingboth forms of attention control. HAT is the new state-of-the-art (SOTA) inpredicting the scanpath of fixations made during target-present andtarget-absent search, and matches or exceeds SOTA in the prediction of tasklessfree-viewing fixation scanpaths. HAT achieves this new SOTA by using a noveltransformer-based architecture and a simplified foveated retina thatcollectively create a spatio-temporal awareness akin to the dynamic visualworking memory of humans. Unlike previous methods that rely on a coarse grid offixation cells and experience information loss due to fixation discretization,HAT features a dense-prediction architecture and outputs a dense heatmap foreach fixation, thus avoiding discretizing fixations. HAT sets a new standard incomputational attention, which emphasizes both effectiveness and generality.HAT's demonstrated scope and applicability will likely inspire the developmentof new attention models that can better predict human behavior in variousattention-demanding scenarios.", "output": "Predicting Human Attention using Computational Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of estimating 3D occupancy from surrounding view images is anexciting development in the field of autonomous driving, following the successof Birds Eye View (BEV) perception.This task provides crucial 3D attributes ofthe driving environment, enhancing the overall understanding and perception ofthe surrounding space. However, there is still a lack of a baseline to definethe task, such as network design, optimization, and evaluation. In this work,we present a simple attempt for 3D occupancy estimation, which is a CNN-basedframework designed to reveal several key factors for 3D occupancy estimation.In addition, we explore the relationship between 3D occupancy estimation andother related tasks, such as monocular depth estimation, stereo matching, andBEV perception (3D object detection and map segmentation), which could advancethe study on 3D occupancy estimation. For evaluation, we propose a simplesampling strategy to define the metric for occupancy evaluation, which isflexible for current public datasets. Moreover, we establish a new benchmark interms of the depth estimation metric, where we compare our proposed method withmonocular depth estimation methods on the DDAD and Nuscenes datasets.Therelevant code will be available in", "output": "A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the success of vision transformers (ViTs), they still suffer fromsignificant drops in accuracy in the presence of common corruptions, such asnoise or blur. Interestingly, we observe that the attention mechanism of ViTstends to rely on few important tokens, a phenomenon we call token overfocusing.More critically, these tokens are not robust to corruptions, often leading tohighly diverging attention patterns. In this paper, we intend to alleviate thisoverfocusing issue and make attention more stable through two generaltechniques: First, our Token-aware Average Pooling (TAP) module encourages thelocal neighborhood of each token to take part in the attention mechanism.Specifically, TAP learns average pooling schemes for each token such that theinformation of potentially important tokens in the neighborhood can adaptivelybe taken into account. Second, we force the output tokens to aggregateinformation from a diverse set of input tokens rather than focusing on just afew by using our Attention Diversification Loss (ADL). We achieve this bypenalizing high cosine similarity between the attention vectors of differenttokens. In experiments, we apply our methods to a wide range of transformerarchitectures and improve robustness significantly. For example, we improvecorruption robustness on ImageNet-C by 2.4% while simultaneously improvingaccuracy by 0.4% based on state-of-the-art robust architecture FAN. Also, whenfinetuning on semantic segmentation tasks, we improve robustness onCityScapes-C by 2.4% and ACDC by 3.1%.", "output": "Robustifying Token Attention for Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-Language models like CLIP have been widely adopted for various tasksdue to their impressive zero-shot capabilities. However, CLIP is not suitablefor extracting 3D geometric features as it was trained on only images and textby natural language supervision. We work on addressing this limitation andpropose a new framework termed CG3D (CLIP Goes 3D) where a 3D encoder islearned to exhibit zero-shot capabilities. CG3D is trained using triplets ofpointclouds, corresponding rendered 2D images, and texts using natural languagesupervision. To align the features in a multimodal embedding space, we utilizecontrastive loss on 3D features obtained from the 3D encoder, as well as visualand text features extracted from CLIP. We note that the natural images used totrain CLIP and the rendered 2D images in CG3D have a distribution shift.Attempting to train the visual and text encoder to account for this shiftresults in catastrophic forgetting and a notable decrease in performance. Tosolve this, we employ prompt tuning and introduce trainable parameters in theinput space to shift CLIP towards the 3D pre-training dataset utilized in CG3D.We extensively test our pre-trained CG3D framework and demonstrate itsimpressive capabilities in zero-shot, open scene understanding, and retrievaltasks. Further, it also serves as strong starting weights for fine-tuning indownstream 3D recognition tasks.", "output": "CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Post-training quantization (PTQ) is widely regarded as one of the mostefficient compression methods practically, benefitting from its data privacyand low computation costs. We argue that an overlooked problem of oscillationis in the PTQ methods. In this paper, we take the initiative to explore andpresent a theoretical proof to explain why such a problem is essential in PTQ.And then, we try to solve this problem by introducing a principled andgeneralized framework theoretically. In particular, we first formulate theoscillation in PTQ and prove the problem is caused by the difference in modulecapacity. To this end, we define the module capacity (ModCap) underdata-dependent and data-free scenarios, where the differentials betweenadjacent modules are used to measure the degree of oscillation. The problem isthen solved by selecting top-k differentials, in which the correspondingmodules are jointly optimized and quantized. Extensive experiments demonstratethat our method successfully reduces the performance drop and is generalized todifferent neural networks and PTQ methods. For example, with 2/4 bit ResNet-50quantization, our method surpasses the previous state-of-the-art method by1.9%. It becomes more significant on small model quantization, e.g. surpassesBRECQ method by 6.61% on MobileNetV2*0.5.", "output": "Solving Oscillation Problem in Post-Training Quantization Through a Theoretical Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a new, simple, and effective Self-supervisedSpatio-temporal Transformers (SPARTAN) approach to Group Activity Recognition(GAR) using unlabeled video data. Given a video, we create local and globalSpatio-temporal views with varying spatial patch sizes and frame rates. Theproposed self-supervised objective aims to match the features of thesecontrasting views representing the same video to be consistent with thevariations in spatiotemporal domains. To the best of our knowledge, theproposed mechanism is one of the first works to alleviate the weakly supervisedsetting of GAR using the encoders in video transformers. Furthermore, using theadvantage of transformer models, our proposed approach supports long-termrelationship modeling along spatio-temporal dimensions. The proposed SPARTANapproach performs well on two group activity recognition benchmarks, includingNBA and Volleyball datasets, by surpassing the state-of-the-art results by asignificant margin in terms of MCA and MPCA metrics.", "output": "SPARTAN: Self-supervised Spatiotemporal Transformers Approach to Group Activity Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Children's drawings have a wonderful inventiveness, creativity, and varietyto them. We present a system that automatically animates children's drawings ofthe human figure, is robust to the variance inherent in these depictions, andis simple and straightforward enough for anyone to use. We demonstrate thevalue and broad appeal of our approach by building and releasing the AnimatedDrawings Demo, a freely available public website that has been used by millionsof people around the world. We present a set of experiments exploring theamount of training data needed for fine-tuning, as well as a perceptual studydemonstrating the appeal of a novel twisted perspective retargeting technique.Finally, we introduce the Amateur Drawings Dataset, a first-of-its-kindannotated dataset, collected via the public demo, containing over 178,000amateur drawings and corresponding user-accepted character bounding boxes,segmentation masks, and joint location annotations.", "output": "A Method for Animating Children's Drawings of the Human Figure."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Content and style disentanglement is an effective way to achieve few-shotfont generation. It allows to transfer the style of the font image in a sourcedomain to the style defined with a few reference images in a target domain.However, the content feature extracted using a representative font might not beoptimal. In light of this, we propose a content fusion module (CFM) to projectthe content feature into a linear space defined by the content features ofbasis fonts, which can take the variation of content features caused bydifferent fonts into consideration. Our method also allows to optimize thestyle representation vector of reference images through a lightweight iterativestyle-vector refinement (ISR) strategy. Moreover, we treat the 1D projection ofa character image as a probability distribution and leverage the distancebetween two distributions as the reconstruction loss (namely projectedcharacter loss, PCL). Compared to L2 or L1 reconstruction loss, thedistribution distance pays more attention to the global shape of characters. Wehave evaluated our method on a dataset of 300 fonts with 6.5k characters each.Experimental results verify that our method outperforms existingstate-of-the-art few-shot font generation methods by a large margin. The sourcecode can be found at ", "output": "CF-Font: Content Fusion for Few-shot Font Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating grammatically and semantically correct captions in videocaptioning is a challenging task. The captions generated from the existingmethods are either word-by-word that do not align with grammatical structure ormiss key information from the input videos. To address these issues, weintroduce a novel global-local fusion network, with a Global-Local Fusion Block(GLFB) that encodes and fuses features from different parts of speech (POS)components with visual-spatial features. We use novel combinations of differentPOS components - 'determinant + subject', 'auxiliary verb', 'verb', and'determinant + object' for supervision of the POS blocks - Det + Subject, AuxVerb, Verb, and Det + Object respectively. The novel global-local fusionnetwork together with POS blocks helps align the visual features with languagedescription to generate grammatically and semantically correct captions.Extensive qualitative and quantitative experiments on benchmark MSVD and MSRVTTdatasets demonstrate that the proposed approach generates more grammaticallyand semantically correct captions compared to the existing methods, achievingthe new state-of-the-art. Ablations on the POS blocks and the GLFB demonstratethe impact of the contributions on the proposed method.", "output": "SEM-POS: Grammatically and Semantically Correct Video Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a multi-task representation learning framework tojointly estimate the identity, gender and age of individuals from their handimages for the purpose of criminal investigations since the hand images areoften the only available information in cases of serious crime such as sexualabuse. We investigate different up-to-date deep learning architectures andcompare their performance for joint estimation of identity, gender and age fromhand images of perpetrators of serious crime. To overcome the data imbalanceand simplify the age prediction, we create age groups for the age estimation.We make extensive evaluations and comparisons of both convolution-based andtransformer-based deep learning architectures on a publicly available 11k handsdataset. Our experimental analysis shows that it is possible to efficientlyestimate not only identity but also other attributes such as gender and age ofsuspects jointly from hand images for criminal investigations, which is crucialin assisting international police forces in the court to identify and convictabusers.", "output": "Joint Person Identity, Gender and Age Estimation from Hand Images using Deep Multi-Task Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The synergy of long-range dependencies from transformers and localrepresentations of image content from convolutional neural networks (CNNs) hasled to advanced architectures and increased performance for various medicalimage analysis tasks due to their complementary benefits. However, comparedwith CNNs, transformers require considerably more training data, due to alarger number of parameters and an absence of inductive bias. The need forincreasingly large datasets continues to be problematic, particularly in thecontext of medical imaging, where both annotation efforts and data protectionresult in limited data availability. In this work, inspired by the humandecision-making process of correlating new ``evidence'' with previouslymemorized ``experience'', we propose a Memorizing Vision Transformer (MoViT) toalleviate the need for large-scale datasets to successfully train and deploytransformer-based architectures. MoViT leverages an external memory structureto cache history attention snapshots during the training stage. To preventoverfitting, we incorporate an innovative memory update scheme, attentiontemporal moving average, to update the stored external memories with thehistorical moving average. For inference speedup, we design a prototypicalattention learning method to distill the external memory into smallerrepresentative subsets. We evaluate our method on a public histology imagedataset and an in-house MRI dataset, demonstrating that MoViT applied to variedmedical image analysis tasks, can outperform vanilla transformer models acrossvaried data regimes, especially in cases where only a small amount of annotateddata is available. More importantly, MoViT can reach a competitive performanceof ViT with only 3.0% of the training data.", "output": "MoViT: Memorizing Vision Transformers for Medical Image Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Anomaly detectors are widely used in industrial production to detect andlocalize unknown defects in query images. These detectors are trained onnominal images and have shown success in distinguishing anomalies from mostnormal samples. However, hard-nominal examples are scattered and far apart frommost normalities, they are often mistaken for anomalies by existing anomalydetectors. To address this problem, we propose a simple yet efficient method:textbf{H}ard Nominal textbf{E}xample-aware textbf{T}emplate textbf{M}utualtextbf{M}atching (HETMM). Specifically, textit{HETMM} aims to construct arobust prototype-based decision boundary, which can precisely distinguishbetween hard-nominal examples and anomalies, yielding fewer false-positive andmissed-detection rates. Moreover, textit{HETMM} mutually explores theanomalies in two directions between queries and the template set, and thus itis capable to capture the logical anomalies. This is a significant advantageover most anomaly detectors that frequently fail to detect logical anomalies.Additionally, to meet the speed-accuracy demands, we further proposetextbf{P}ixel-level textbf{T}emplate textbf{S}election (PTS) to streamlinethe original template set. textit{PTS} selects cluster centres andhard-nominal examples to form a tiny set, maintaining the original decisionboundaries. Comprehensive experiments on five real-world datasets demonstratethat our methods yield outperformance than existing advances under thereal-time inference speed. Furthermore, textit{HETMM} can be hot-updated byinserting novel samples, which may promptly address some incremental learningissues.", "output": "Hard Nominal Example-aware Template Mutual Matching for Industrial Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised learning of image classifiers distills human knowledge into aparametric model through pairs of images and corresponding labels (X,Y). Weargue that this simple and widely used representation of human knowledgeneglects rich auxiliary information from the annotation procedure, such as thetime-series of mouse traces and clicks left after image selection. Our insightis that such annotation byproducts Z provide approximate human attention thatweakly guides the model to focus on the foreground cues, reducing spuriouscorrelations and discouraging shortcut learning. To verify this, we createImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched withsample-wise annotation byproducts, collected by replicating the respectiveoriginal annotation tasks. We refer to the new paradigm of training models withannotation byproducts as learning using annotation byproducts (LUAB). We showthat a simple multitask loss for regressing Z together with Y already improvesthe generalisability and robustness of the learned models. Compared to theoriginal supervised learning, LUAB does not require extra annotation costs.ImageNet-AB and COCO-AB are at ", "output": "Neglected Free Lunch; Learning Image Classifiers Using Annotation Byproducts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weakly supervised video anomaly detection (WS-VAD) is a challenging problemthat aims to learn VAD models only with video-level annotations. In this work,we propose a Long-Short Temporal Co-teaching (LSTC) method to address theWS-VAD problem. It constructs two tubelet-based spatio-temporal transformernetworks to learn from short- and long-term video clips respectively. Eachnetwork is trained with respect to a multiple instance learning (MIL)-basedranking loss, together with a cross-entropy loss when clip-level pseudo labelsare available. A co-teaching strategy is adopted to train the two networks.That is, clip-level pseudo labels generated from each network are used tosupervise the other one at the next training round, and the two networks arelearned alternatively and iteratively. Our proposed method is able to betterdeal with the anomalies with varying durations as well as subtle anomalies.Extensive experiments on three public datasets demonstrate that our methodoutperforms state-of-the-art WS-VAD methods.", "output": "Long-Short Temporal Co-Teaching for Weakly Supervised Video Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper addresses the problem of rolling shutter correction in complexnonlinear and dynamic scenes with extreme occlusion. Existing methods sufferfrom two main drawbacks. Firstly, they face challenges in estimating theaccurate correction field due to the uniform velocity assumption, leading tosignificant image correction errors under complex motion. Secondly, the drasticocclusion in dynamic scenes prevents current solutions from achieving betterimage quality because of the inherent difficulties in aligning and aggregatingmultiple frames. To tackle these challenges, we model the curvilineartrajectory of pixels analytically and propose a geometry-based QuadraticRolling Shutter (QRS) motion solver, which precisely estimates the high-ordercorrection field of individual pixel. Besides, to reconstruct high-qualityocclusion frames in dynamic scenes, we present a 3D video architecture thateffectively Aligns and Aggregates multi-frame context, namely, RSA^2-Net. Weevaluate our method across a broad range of cameras and video sequences,demonstrating its significant superiority. Specifically, our method surpassesthe state-of-the-arts by +4.98, +0.77, and +4.33 of PSNR on Carla-RS,Fastec-RS, and BS-RSC datasets, respectively.", "output": "Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The article discusses a method for automating the process of cutting ahoneycomb block, and specifically obtaining points and cutting angles for therequired faces. The following requirements are taken into account in thecalculations: the allowable location of the cut plane is 0.4 of the length ofthe cell face, the cut plane must be perpendicular to the cell wall. Thealgorithm itself consists of two main stages: determining the honeycombstructure and searching for cut points. In the absence of significant defectsin honeycomb blocks (deformation of the cell profile and a dent on the edges ofthe cells), the structure determination algorithm works without significantinaccuracies. The results of the cut point search algorithm can be consideredsatisfactory.", "output": "Determination of cutting positions of honeycomb blocks using computer vision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Localizing people and recognizing their actions from videos is a challengingtask towards high-level video understanding. Existing methods are mostlytwo-stage based, with one stage for person bounding box generation and theother stage for action recognition. However, such two-stage methods aregenerally with low efficiency. We observe that directly unifying detection andaction recognition normally suffers from (i) inferior learning due to differentdesired properties of context representation for detection and actionrecognition; (ii) optimization difficulty with insufficient training data. Inthis work, we present a decoupled one-stage network dubbed DOAD, to mitigateabove issues and improve the efficiency for spatio-temporal action detection.To achieve it, we decouple detection and action recognition into two branches.Specifically, one branch focuses on detection representation for actordetection, and the other one for action recognition. For the action branch, wedesign a transformer-based module (TransPC) to model pairwise relationshipsbetween people and context. Different from commonly used vector-based dotproduct in self-attention, it is built upon a novel matrix-based key and valuefor Hadamard attention to model person-context information. It not onlyexploits relationships between person pairs but also takes into account contextand relative position information. The results on AVA and UCF101-24 datasetsshow that our method is competitive with two-stage state-of-the-art methodswith significant efficiency improvement.", "output": "DOAD: Decoupled One Stage Action Detection Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel, object-agnostic method for learning a universal policyfor dexterous object grasping from realistic point cloud observations andproprioceptive information under a table-top setting, namely UniDexGrasp++. Toaddress the challenge of learning the vision-based policy across thousands ofobject instances, we propose Geometry-aware Curriculum Learning (GeoCurriculum)and Geometry-aware iterative Generalist-Specialist Learning (GiGSL) whichleverage the geometry feature of the task and significantly improve thegeneralizability. With our proposed techniques, our final policy showsuniversal dexterous grasping on thousands of object instances with 85.4% and78.2% success rate on the train set and test set which outperforms thestate-of-the-art baseline UniDexGrasp by 11.7% and 11.3%, respectively.", "output": "UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Action understanding matters and attracts attention. It can be formed as themapping from the action physical space to the semantic space. Typically,researchers built action datasets according to idiosyncratic choices to defineclasses and push the envelope of benchmarks respectively. Thus, datasets areincompatible with each other like \"Isolated Islands\" due to semantic gaps andvarious class granularities, e.g., do housework in dataset A and wash plate indataset B. We argue that a more principled semantic space is an urgent need toconcentrate the community efforts and enable us to use all datasets together topursue generalizable action learning. To this end, we design a Poincare actionsemantic space given verb taxonomy hierarchy and covering massive actions. Byaligning the classes of previous datasets to our semantic space, we gather(image/video/skeleton/MoCap) datasets into a unified database in a unifiedlabel system, i.e., bridging \"isolated islands\" into a \"Pangea\". Accordingly,we propose a bidirectional mapping model between physical and semantic space tofully use Pangea. In extensive experiments, our system shows significantsuperiority, especially in transfer learning. Code and data will be madepublicly available.", "output": "From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recovering the physical attributes of an object's appearance from its imagescaptured under an unknown illumination is challenging yet essential forphoto-realistic rendering. Recent approaches adopt the emerging implicit scenerepresentations and have shown impressive results.However, they unanimouslyadopt a surface-based representation,and hence can not well handle scenes withvery complex geometry, translucent object and etc. In this paper, we propose toconduct inverse volume rendering, in contrast to surface-based, by representinga scene using microflake volume, which assumes the space is filled withinfinite small flakes and light reflects or scatters at each spatial locationaccording to microflake distributions. We further adopt the coordinate networksto implicitly encode the microflake volume, and develop a differentiablemicroflake volume renderer to train the network in an end-to-end way inprinciple.Our NeMF enables effective recovery of appearance attributes forhighly complex geometry and scattering object, enables high-quality relighting,material editing, and especially simulates volume rendering effects, such asscattering, which is infeasible for surface-based approaches.", "output": "NeMF: Inverse Volume Rendering with Neural Microflake Field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a study on the soft-Dice loss, one of the most popularloss functions in medical image segmentation, for situations where noise ispresent in target labels. In particular, the set of optimal solutions arecharacterized and sharp bounds on the volume bias of these solutions areprovided. It is further shown that a sequence of soft segmentations convergingto optimal soft-Dice also converges to optimal Dice when converted to hardsegmentations using thresholding. This is an important result because soft-Diceis often used as a proxy for maximizing the Dice metric. Finally, experimentsconfirming the theoretical results are provided.", "output": "Noisy Image Segmentation With Soft-Dice."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In vivo cardiac diffusion tensor imaging (cDTI) is a promising MagneticResonance Imaging (MRI) technique for evaluating the micro-structure ofmyocardial tissue in the living heart, providing insights into cardiac functionand enabling the development of innovative therapeutic strategies. However, theintegration of cDTI into routine clinical practice is challenging due to thetechnical obstacles involved in the acquisition, such as low signal-to-noiseratio and long scanning times. In this paper, we investigate and implementthree different types of deep learning-based MRI reconstruction models for cDTIreconstruction. We evaluate the performance of these models based onreconstruction quality assessment and diffusion tensor parameter assessment.Our results indicate that the models we discussed in this study can be appliedfor clinical use at an acceleration factor (AF) of $times 2$ and $times 4$,with the D5C5 model showing superior fidelity for reconstruction and the SwinMRmodel providing higher perceptual scores. There is no statistical differencewith the reference for all diffusion tensor parameters at AF $times 2$ or mostDT parameters at AF $times 4$, and the quality of most diffusion tensorparameter maps are visually acceptable. SwinMR is recommended as the optimalapproach for reconstruction at AF $times 2$ and AF $times 4$. However, webelieved the models discussed in this studies are not prepared for clinical useat a higher AF. At AF $times 8$, the performance of all models discussedremains limited, with only half of the diffusion tensor parameters beingrecovered to a level with no statistical difference from the reference. Somediffusion tensor parameter maps even provide wrong and misleading information.", "output": "Deep Learning-based Diffusion Tensor Cardiac Magnetic Resonance Reconstruction: A Comparison Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-set Unsupervised Video Domain Adaptation (OUVDA) deals with the task ofadapting an action recognition model from a labelled source domain to anunlabelled target domain that contains \"target-private\" categories, which arepresent in the target but absent in the source. In this work we deviate fromthe prior work of training a specialized open-set classifier or weightedadversarial learning by proposing to use pre-trained Language and Vision Models(CLIP). The CLIP is well suited for OUVDA due to its rich representation andthe zero-shot recognition capabilities. However, rejecting target-privateinstances with the CLIP's zero-shot protocol requires oracle knowledge aboutthe target-private label names. To circumvent the impossibility of theknowledge of label names, we propose AutoLabel that automatically discovers andgenerates object-centric compositional candidate target-private class names.Despite its simplicity, we show that CLIP when equipped with AutoLabel cansatisfactorily reject the target-private instances, thereby facilitating betteralignment between the shared classes of the two domains. The code is available.", "output": "AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised Relation Extraction (RE) aims to identify relations betweenentities in text, without having access to labeled data during training. Thissetting is particularly relevant for domain specific RE where no annotateddataset is available and for open-domain RE where the types of relations are apriori unknown. Although recent approaches achieve promising results, theyheavily depend on hyperparameters whose tuning would most often require labeleddata. To mitigate the reliance on hyperparameters, we propose PromptORE, a''Prompt-based Open Relation Extraction'' model. We adapt the novelprompt-tuning paradigm to work in an unsupervised setting, and use it to embedsentences expressing a relation. We then cluster these embeddings to discovercandidate relations, and we experiment different strategies to automaticallyestimate an adequate number of clusters. To the best of our knowledge,PromptORE is the first unsupervised RE model that does not need hyperparametertuning. Results on three general and specific domain datasets show thatPromptORE consistently outperforms state-of-the-art models with a relative gainof more than 40% in B 3 , V-measure and ARI. Qualitative analysis alsoindicates PromptORE's ability to identify semantically coherent clusters thatare very close to true relations.", "output": "PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The beekeeping sector has undergone considerable production variations overthe past years due to adverse weather conditions, occurring more frequently asclimate change progresses. These phenomena can be high-impact and cause theenvironment to be unfavorable to the bees' activity. We disentangle the honeyproduction drivers with tree-based methods and predict honey productionvariations for hives in Italy, one of the largest honey producers in Europe.The database covers hundreds of beehive data from 2019-2022 gathered withadvanced precision beekeeping techniques. We train and interpret the machinelearning models making them prescriptive other than just predictive. Superiorpredictive performances of tree-based methods compared to standard lineartechniques allow for better protection of bees' activity and assess potentiallosses for beekeepers for risk management.", "output": "A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural networks (NNs) playing the role of controllers have demonstratedimpressive empirical performances on challenging control problems. However, thepotential adoption of NN controllers in real-life applications also gives riseto a growing concern over the safety of these neural-network controlled systems(NNCSs), especially when used in safety-critical applications. In this work, wepresent POLAR-Express, an efficient and precise formal reachability analysistool for verifying the safety of NNCSs. POLAR-Express uses Taylor modelarithmetic to propagate Taylor models (TMs) across a neural networklayer-by-layer to compute an overapproximation of the neural-network function.It can be applied to analyze any feed-forward neural network with continuousactivation functions. We also present a novel approach to propagate TMs moreefficiently and precisely across ReLU activation functions. In addition,POLAR-Express provides parallel computation support for the layer-by-layerpropagation of TMs, thus significantly improving the efficiency and scalabilityover its earlier prototype POLAR. Across the comparison with six otherstate-of-the-art tools on a diverse set of benchmarks, POLAR-Express achievesthe best verification efficiency and tightness in the reachable set analysis.", "output": "POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose DoE2Vec, a variational autoencoder (VAE)-based methodology tolearn optimization landscape characteristics for downstream meta-learningtasks, e.g., automated selection of optimization algorithms. Principally, usinglarge training data sets generated with a random function generator, DoE2Vecself-learns an informative latent representation for any design of experiments(DoE). Unlike the classical exploratory landscape analysis (ELA) method, ourapproach does not require any feature engineering and is easily applicable forhigh dimensional search spaces. For validation, we inspect the quality oflatent reconstructions and analyze the latent representations using differentexperiments. The latent representations not only show promising potentials inidentifying similar (cheap-to-evaluate) surrogate functions, but also cansignificantly boost performances when being used complementary to the classicalELA features in classification tasks.", "output": "DoE2Vec: Deep-learning Based Features for Exploratory Landscape Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of dimensionality reduction and visualization of high-dimensionaldatasets remains a challenging problem since long. Modern high-throughputtechnologies produce newer high-dimensional datasets having multiple views withrelatively new data types. Visualization of these datasets require propermethodology that can uncover hidden patterns in the data without affecting thelocal and global structures within the data. To this end, however, very fewsuch methodology exist, which can realise this task. In this work, we haveintroduced a novel unsupervised deep neural network model, called NeuroDAVIS,for data visualization. NeuroDAVIS is capable of extracting important featuresfrom the data, without assuming any data distribution, and visualizeeffectively in lower dimension. It has been shown theoritically thatneighbourhood relationship of the data in high dimension remains preserved inlower dimension. The performance of NeuroDAVIS has been evaluated on a widevariety of synthetic and real high-dimensional datasets including numeric,textual, image and biological data. NeuroDAVIS has been highly competitiveagainst both t-Distributed Stochastic Neighbor Embedding (t-SNE) and UniformManifold Approximation and Projection (UMAP) with respect to visualizationquality, and preservation of data size, shape, and both local and globalstructure. It has outperformed Fast interpolation-based t-SNE (Fit-SNE), avariant of t-SNE, for most of the high-dimensional datasets as well. For thebiological datasets, besides t-SNE, UMAP and Fit-SNE, NeuroDAVIS has alsoperformed well compared to other state-of-the-art algorithms, like Potential ofHeat-diffusion for Affinity-based Trajectory Embedding (PHATE) and the siameseneural network-based method, called IVIS. Downstream classification andclustering analyses have also revealed favourable results forNeuroDAVIS-generated embeddings.", "output": "NeuroDAVIS: A neural network model for data visualization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The implementation of a multi-microgrid (MMG) system with multiple renewableenergy sources enables the facilitation of electricity trading. To tackle theenergy management problem of a MMG system, which consists of multiple renewableenergy microgrids belonging to different operating entities, this paperproposes a MMG collaborative optimization scheduling model based on amulti-agent centralized training distributed execution framework. To enhancethe generalization ability of dealing with various uncertainties, we alsopropose an improved multi-agent soft actor-critic (MASAC) algorithm, whichfacilitates en-ergy transactions between multi-agents in MMG, and employsautomated machine learning (AutoML) to optimize the MASAC hyperparameters tofurther improve the generalization of deep reinforcement learning (DRL). Thetest results demonstrate that the proposed method successfully achieves powercomplementarity between different entities, and reduces the MMG systemoperating cost. Additionally, the proposal significantly outperforms otherstate-of-the-art reinforcement learning algorithms with better economy andhigher calculation efficiency.", "output": "Multi-Microgrid Collaborative Optimization Scheduling Using an Improved Multi-Agent Soft Actor-Critic Algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rapid growth of data availability and usage, quantifying the addedvalue of each training data point has become a crucial process in the field ofartificial intelligence. The Shapley values have been recognized as aneffective method for data valuation, enabling efficient training setsummarization, acquisition, and outlier removal. In this paper, we introduce\"STI-KNN\", an innovative algorithm that calculates the exact pair-interactionShapley values for KNN models in O(t n^2) time, which is a significantimprovement over the O(2^n)$ time complexity of baseline methods. By usingSTI-KNN, we can efficiently and accurately evaluate the value of individualdata points, leading to improved training outcomes and ultimately enhancing theeffectiveness of artificial intelligence applications.", "output": "Optimizing Data Shapley Interaction Calculation from O(2^n) to O(t n^2) for KNN models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Abnormal event detection, which refers to mining unusual interactions amonginvolved entities, plays an important role in many real applications. Previousworks mostly over-simplify this task as detecting abnormal pair-wiseinteractions. However, real-world events may contain multi-typed attributedentities and complex interactions among them, which forms an AttributedHeterogeneous Information Network (AHIN). With the boom of social networks,abnormal event detection in AHIN has become an important, but seldom exploredtask. In this paper, we firstly study the unsupervised abnormal event detectionproblem in AHIN. The events are considered as star-schema instances of AHIN andare further modeled by hypergraphs. A novel hypergraph contrastive learningmethod, named AEHCL, is proposed to fully capture abnormal event patterns.AEHCL designs the intra-event and inter-event contrastive modules to exploitself-supervised AHIN information. The intra-event contrastive module capturesthe pair-wise and multivariate interaction anomalies within an event, and theinter-event module captures the contextual anomalies among events. These twomodules collaboratively boost the performance of each other and improve thedetection results. During the testing phase, a contrastive learning-basedabnormal event score function is further proposed to measure the abnormalitydegree of events. Extensive experiments on three datasets in differentscenarios demonstrate the effectiveness of AEHCL, and the results improvestate-of-the-art baselines up to 12.0% in Average Precision (AP) and 4.6% inArea Under Curve (AUC) respectively.", "output": "Abnormal Event Detection via Hypergraph Contrastive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we investigate the use of Fourier Neural Operators (FNOs) forimage classification in comparison to standard Convolutional Neural Networks(CNNs). Neural operators are a discretization-invariant generalization ofneural networks to approximate operators between infinite dimensional functionspaces. FNOs - which are neural operators with a specific parametrization -have been applied successfully in the context of parametric PDEs. We derive theFNO architecture as an example for continuous and Fr'echet-differentiableneural operators on Lebesgue spaces. We further show how CNNs can be convertedinto FNOs and vice versa and propose an interpolation-equivariant adaptation ofthe architecture.", "output": "Resolution-Invariant Image Classification based on Fourier Neural Operators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spiking Neural Networks (SNNs) have recently become more popular as abiologically plausible substitute for traditional Artificial Neural Networks(ANNs). SNNs are cost-efficient and deployment-friendly because they processinput in both spatial and temporal manners using binary spikes. However, weobserve that the information capacity in SNNs is affected by the number oftimesteps, leading to an accuracy-efficiency tradeoff. In this work, we study afine-grained adjustment of the number of timesteps in SNNs. Specifically, wetreat the number of timesteps as a variable conditioned on different inputsamples to reduce redundant timesteps for certain data. We call our methodSpiking Early-Exit Neural Networks (SEENNs). To determine the appropriatenumber of timesteps, we propose SEENN-I which uses a confidence scorethresholding to filter out the uncertain predictions, and SEENN-II whichdetermines the number of timesteps by reinforcement learning. Moreover, wedemonstrate that SEENN is compatible with both the directly trained SNN and theANN-SNN conversion. By dynamically adjusting the number of timesteps, our SEENNachieves a remarkable reduction in the average number of timesteps duringinference. For example, our SEENN-II ResNet-19 can achieve 96.1% accuracy withan average of 1.08 timesteps on the CIFAR-10 test dataset.", "output": "SEENN: Towards Temporal Spiking Early-Exit Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language modeling have shown impressive progress in generating compellingtext with good accuracy and high semantic coherence. An interesting researchdirection is to augment these powerful models for specific applications usingcontextual information. In this work, we explore multi-modal language modelingfor healthcare applications. We are interested in outcome prediction andpatient triage in hospital emergency department based on text information inchief complaints and vital signs recorded at triage. We adapt Perceiver - amodality-agnostic transformer-based model that has shown promising results inseveral applications. Since vital-sign modality is represented in tabularformat, we modified Perceiver position encoding to ensure permutationinvariance. We evaluated the multi-modal language model for the task ofdiagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In theexperimental analysis, we show that mutli-modality improves the predictionperformance compared with models trained solely on text or vital signs. Weidentified disease categories for which multi-modality leads to performanceimprovement and show that for these categories, vital signs have addedpredictive power. By analyzing the cross-attention layer, we show howmulti-modality contributes to model predictions. This work gives interestinginsights on the development of multi-modal language models for healthcareapplications.", "output": "Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An accurate solar wind speed model is important for space weatherpredictions, catastrophic event warnings, and other issues concerning solarwind - magnetosphere interaction. In this work, we construct a model based onconvolutional neural network (CNN) and Potential Field Source Surface (PFSS)magnetograms, considering a solar wind source surface of $R_{rmSS}=2.5R_odot$, aiming to predict the solar wind speed at the Lagrange 1 (L1)point of the Sun-Earth system. The input of our model consists of fourPotential Field Source Surface (PFSS) magnetograms at $R_{rm SS}$, which are7, 6, 5, and 4 days before the target epoch. Reduced magnetograms are used topromote the model's efficiency. We use the Global Oscillation Network Group(GONG) photospheric magnetograms and the potential field extrapolation model togenerate PFSS magnetograms at the source surface. The model providespredictions of the continuous test dataset with an averaged correlationcoefficient (CC) of 0.52 and a root mean square error (RMSE) of 80.8 km/s in aneight-fold validation training scheme with the time resolution of the data assmall as one hour. The model also has the potential to forecast high speedstreams of the solar wind, which can be quantified with a general threat scoreof 0.39.", "output": "Prediction of solar wind speed by applying convolutional neural network to potential field source surface (PFSS) magnetograms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Markov Neural Networks (GMNN) have recently been proposed to improveregular graph neural networks (GNN) by including label dependencies into thesemi-supervised node classification task. GMNNs do this in a theoreticallyprincipled way and use three kinds of information to predict labels. Just likeordinary GNNs, they use the node features and the graph structure but theymoreover leverage information from the labels of neighboring nodes to improvethe accuracy of their predictions. In this paper, we introduce a new datasetnamed WikiVitals which contains a graph of 48k mutually referred Wikipediaarticles classified into 32 categories and connected by 2.3M edges. Our aim isto rigorously evaluate the contributions of three distinct sources ofinformation to the prediction accuracy of GMNN for this dataset: the content ofthe articles, their connections with each other and the correlations amongtheir labels. For this purpose we adapt a method which was recently proposedfor performing fair comparisons of GNN performance using an appropriaterandomization over partitions and a clear separation of model selection andmodel assessment.", "output": "Fair Evaluation of Graph Markov Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Aims. The treatment of astronomical image time series has won increasingattention in recent years. Indeed, numerous surveys following up on transientobjects are in progress or under construction, such as the Vera RubinObservatory Legacy Survey for Space and Time (LSST), which is poised to producehuge amounts of these time series. The associated scientific topics areextensive, ranging from the study of objects in our galaxy to the observationof the most distant supernovae for measuring the expansion of the universe.With such a large amount of data available, the need for robust automatic toolsto detect and classify celestial objects is growing steadily. Methods. Thisstudy is based on the assumption that astronomical images contain moreinformation than light curves. In this paper, we propose a novel approach basedon deep learning for classifying different types of space objects directlyusing images. We named our approach ConvEntion, which stands for CONVolutionalattENTION. It is based on convolutions and transformers, which are newapproaches for the treatment of astronomical image time series. Our solutionintegrates spatio-temporal features and can be applied to various types ofimage datasets with any number of bands. Results. In this work, we solvedvarious problems the datasets tend to suffer from and we present new resultsfor classifications using astronomical image time series with an increase inaccuracy of 13%, compared to state-of-the-art approaches that use image timeseries, and a 12% increase, compared to approaches that use light curves.", "output": "Astronomical image time series classification using CONVolutional attENTION (ConvEntion)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data augmentation is essential when applying Machine Learning in small-dataregimes. It generates new samples following the observed data distributionwhile increasing their diversity and variability to help researchers andpractitioners improve their models' robustness and, thus, deploy them in thereal world. Nevertheless, its usage in tabular data still needs to be improved,as prior knowledge about the underlying data mechanism is seldom considered,limiting the fidelity and diversity of the generated data. Causal dataaugmentation strategies have been pointed out as a solution to handle thesechallenges by relying on conditional independence encoded in a causal graph. Inthis context, this paper experimentally analyzed the ADMG causal augmentationmethod considering different settings to support researchers and practitionersin understanding under which conditions prior knowledge helps generate new datapoints and, consequently, enhances the robustness of their models. The resultshighlighted that the studied method (a) is independent of the underlying modelmechanism, (b) requires a minimal number of observations that may bechallenging in a small-data regime to improve an ML model's accuracy, (c)propagates outliers to the augmented set degrading the performance of themodel, and (d) is sensitive to its hyperparameter's value.", "output": "A Guide for Practical Use of ADMG Causal Data Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, online distillation has emerged as a powerful technique foradapting real-time deep neural networks on the fly using a slow, but accurateteacher model. However, a major challenge in online distillation iscatastrophic forgetting when the domain shifts, which occurs when the studentmodel is updated with data from the new domain and forgets previously learnedknowledge. In this paper, we propose a solution to this issue by leveraging thepower of continual learning methods to reduce the impact of domain shifts.Specifically, we integrate several state-of-the-art continual learning methodsin the context of online distillation and demonstrate their effectiveness inreducing catastrophic forgetting. Furthermore, we provide a detailed analysisof our proposed solution in the case of cyclic domain shifts. Our experimentalresults demonstrate the efficacy of our approach in improving the robustnessand accuracy of online distillation, with potential applications in domainssuch as video surveillance or autonomous driving. Overall, our work representsan important step forward in the field of online distillation and continuallearning, with the potential to significantly impact real-world applications.", "output": "Online Distillation with Continual Learning for Cyclic Domain Shifts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pain is a common reason for accessing healthcare resources and is a growingarea of research, especially in its overlap with mental health. Mental healthelectronic health records are a good data source to study this overlap.However, much information on pain is held in the free text of these records,where mentions of pain present a unique natural language processing problem dueto its ambiguous nature. This project uses data from an anonymised mentalhealth electronic health records database. The data are used to train a machinelearning based classification algorithm to classify sentences as discussingpatient pain or not. This will facilitate the extraction of relevant paininformation from large databases, and the use of such outputs for furtherstudies on pain and mental health. 1,985 documents were manuallytriple-annotated for creation of gold standard training data, which was used totrain three commonly used classification algorithms. The best performing modelachieved an F1-score of 0.98 (95% CI 0.98-0.99).", "output": "Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The increase in abusive content on online social media platforms is impactingthe social life of online users. Use of offensive and hate speech has beenmaking so-cial media toxic. Homophobia and transphobia constitute offensivecomments against LGBT+ community. It becomes imperative to detect and handlethese comments, to timely flag or issue a warning to users indulging in suchbehaviour. However, automated detection of such content is a challenging task,more so in Dravidian languages which are identified as low resource languages.Motivated by this, the paper attempts to explore applicability of differentdeep learning mod-els for classification of the social media comments inMalayalam and Tamil lan-guages as homophobic, transphobic andnon-anti-LGBT+content. The popularly used deep learning models- ConvolutionalNeural Network (CNN), Long Short Term Memory (LSTM) using GloVe embedding andtransformer-based learning models (Multilingual BERT and IndicBERT) are appliedto the classification problem. Results obtained show that IndicBERT outperformsthe other imple-mented models, with obtained weighted average F1-score of 0.86and 0.77 for Malayalam and Tamil, respectively. Therefore, the present workconfirms higher performance of IndicBERT on the given task in selectedDravidian languages.", "output": "Detection of Homophobia & Transphobia in Dravidian Languages: Exploring Deep Learning Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Thermal imaging has numerous advantages over regular visible-range imagingsince it performs well in low-light circumstances. Super-Resolution approachescan broaden their usefulness by replicating accurate high-resolution thermalpictures using measurements from low-cost, low-resolution thermal sensors.Because of the spectral range mismatch between the images, GuidedSuper-Resolution of thermal images utilizing visible range images is difficult.However, In case of failure to capture Visible Range Images can prevent theoperations of applications in critical areas. We present a novel data fusionframework and regularization technique for Guided Super Resolution of Thermalimages. The proposed architecture is computationally in-expensive andlightweight with the ability to maintain performance despite missing one of themodalities, i.e., high-resolution RGB image or the lower-resolution thermalimage, and is designed to be robust in the presence of missing data. Theproposed method presents a promising solution to the frequently occurringproblem of missing modalities in a real-world scenario. Code is available at", "output": "CoReFusion: Contrastive Regularized Fusion for Guided Thermal Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous cyber agents may be developed by applying reinforcement and deepreinforcement learning (RL/DRL), where agents are trained in a representativeenvironment. The training environment must simulate with high-fidelity thenetwork Cyber Operations (CyOp) that the agent aims to explore. Given thecomplexity of net-work CyOps, a good simulator is difficult to achieve. Thiswork presents a systematic solution to automatically generate a high-fidelitysimulator in the Cyber Gym for Intelligent Learning (CyGIL). Throughrepresentation learning and continuous learning, CyGIL provides a unified CyOptraining environment where an emulated CyGIL-E automatically generates asimulated CyGIL-S. The simulator generation is integrated with the agenttraining process to further reduce the required agent training time. The agenttrained in CyGIL-S is transferrable directly to CyGIL-E showing fulltransferability to the emulated \"real\" network. Experimental results arepresented to demonstrate the CyGIL training performance. Enabling offline RL,the CyGIL solution presents a promising direction towards sim-to-real forleveraging RL agents in real-world cyber networks.", "output": "Unified Emulation-Simulation Training Environment for Autonomous Cyber Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Structured, or tabular, data is the most common format in data science. Whiledeep learning models have proven formidable in learning from unstructured datasuch as images or speech, they are less accurate than simpler approaches whenlearning from tabular data. In contrast, modern tree-based Machine Learning(ML) models shine in extracting relevant information from structured data. Anessential requirement in data science is to reduce model inference latency incases where, for example, models are used in a closed loop with simulation toaccelerate scientific discovery. However, the hardware acceleration communityhas mostly focused on deep neural networks and largely ignored other forms ofmachine learning. Previous work has described the use of an analog contentaddressable memory (CAM) component for efficiently mapping random forests. Inthis work, we focus on an overall analog-digital architecture implementing anovel increased precision analog CAM and a programmable network on chipallowing the inference of state-of-the-art tree-based ML models, such asXGBoost and CatBoost. Results evaluated in a single chip at 16nm technologyshow 119x lower latency at 9740x higher throughput compared with astate-of-the-art GPU, with a 19W peak power consumption.", "output": "X-TIME: An in-memory engine for accelerating machine learning on tabular data with CAMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We developed a prototype device for dynamic gaze and accommodationmeasurements based on 4 Purkinje reflections (PR) suitable for use in AR andophthalmology applications. PR1&amp;2 and PR3&amp;4 are used for accurate gaze andaccommodation measurements, respectively. Our eye model was developed in ZEMAXand matches the experiments well. Our model predicts the accommodation from 4diopters to 1 diopter with better than 0.25D accuracy. We performedrepeatability tests and obtained accurate gaze and accommodation estimationsfrom subjects. We are generating a large synthetic data set using physicallyaccurate models and machine learning.", "output": "Dynamic Accommodation Measurement using Purkinje Images and ML Algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy-based models (EBM) have become increasingly popular within computervision. EBMs bring a probabilistic approach to training deep neural networks(DNN) and have been shown to enhance performance in areas such as calibration,out-of-distribution detection, and adversarial resistance. However, theseadvantages come at the cost of estimating input data probabilities, usuallyusing a Langevin based method such as Stochastic Gradient Langevin Dynamics(SGLD), which bring additional computational costs, require parameterization,caching methods for efficiency, and can run into stability and scaling issues.EBMs use dynamical methods to draw samples from the probability densityfunction (PDF) defined by the current state of the network and compare them tothe training data using a maximum log likelihood approach to learn the correctPDF.We propose a non-generative training approach, Non-Generative EBM (NG-EBM),that utilizes the {it{Approximate Mass}}, identified by Grathwohl et al., as aloss term to direct the training. We show that our NG-EBM training strategyretains many of the benefits of EBM in calibration, out-of-distributiondetection, and adversarial resistance, but without the computational complexityand overhead of the traditional approaches. In particular, the NG-EBM approachimproves the Expected Calibration Error by a factor of 2.5 for CIFAR10 and 7.5times for CIFAR100, when compared to traditionally trained models.", "output": "Non-Generative Energy Based Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores the use of affine hulls of points as a means ofrepresenting data via learning in Reproducing Kernel Hilbert Spaces (RKHS),with the goal of partitioning the data space into geometric bodies that concealprivacy-sensitive information about individual data points, while preservingthe structure of the original learning problem. To this end, we introduce theKernel Affine Hull Machine (KAHM), which provides an effective way of computinga distance measure from the resulting bounded geometric body. KAHM is acritical building block in wide and deep autoencoders, which enable datarepresentation learning for classification applications. To ensureprivacy-preserving learning, we propose a novel method for generatingfabricated data, which involves smoothing differentially private data samplesthrough a transformation process. The resulting fabricated data guarantees notonly differential privacy but also ensures that the KAHM modeling error is notlarger than that of the original training data samples. We also address theaccuracy-loss issue that arises with differentially private classifiers byusing fabricated data. This approach results in a significant reduction in therisk of membership inference attacks while incurring only a marginal loss ofaccuracy. As an application, a KAHM based differentially private federatedlearning scheme is introduced featuring that the evaluation of globalclassifier requires only locally computed distance measures. Overall, ourfindings demonstrate the potential of KAHM as effective tool forprivacy-preserving learning and classification.", "output": "Kernel Affine Hull Machines for Differentially Private Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the field of sampling algorithms, MCMC (Markov Chain Monte Carlo) methodsare widely used when direct sampling is not possible. However, multimodality oftarget distributions often leads to slow convergence and mixing. One commonsolution is parallel tempering. Though highly effective in practice,theoretical guarantees on its performance are limited. In this paper, wepresent a new lower bound for parallel tempering on the spectral gap that has apolynomial dependence on all parameters except $log L$, where $(L + 1)$ is thenumber of levels. This improves the best existing bound which dependsexponentially on the number of modes. Moreover, we complement our result with ahypothetical upper bound on spectral gap that has an exponential dependence on$log L$, which shows that, in some sense, our bound is tight.", "output": "Improved Bound for Mixing Time of Parallel Tempering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study presents insights from interviews with nineteen Knowledge Graph(KG) practitioners who work in both enterprise and academic settings on a widevariety of use cases. Through this study, we identify critical challengesexperienced by KG practitioners when creating, exploring, and analyzing KGsthat could be alleviated through visualization design. Our findings revealthree major personas among KG practitioners - KG Builders, Analysts, andConsumers - each of whom have their own distinct expertise and needs. Wediscover that KG Builders would benefit from schema enforcers, while KGAnalysts need customizable query builders that provide interim query results.For KG Consumers, we identify a lack of efficacy for node-link diagrams, andthe need for tailored domain-specific visualizations to promote KG adoption andcomprehension. Lastly, we find that implementing KGs effectively in practicerequires both technical and social solutions that are not addressed withcurrent tools, technologies, and collaborative workflows. From the analysis ofour interviews, we distill several visualization research directions to improveKG usability, including knowledge cards that balance digestibility anddiscoverability, timeline views to track temporal changes, interfaces thatsupport organic discovery, and semantic explanations for AI and machinelearning predictions.", "output": "Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Empirical design in reinforcement learning is no small task. Running goodexperiments requires attention to detail and at times significant computationalresources. While compute resources available per dollar have continued to growrapidly, so have the scale of typical experiments in reinforcement learning. Itis now common to benchmark agents with millions of parameters against dozens oftasks, each using the equivalent of 30 days of experience. The scale of theseexperiments often conflict with the need for proper statistical evidence,especially when comparing algorithms. Recent studies have highlighted howpopular algorithms are sensitive to hyper-parameter settings and implementationdetails, and that common empirical practice leads to weak statistical evidence(Machado et al., 2018; Henderson et al., 2018). Here we take this one stepfurther.This manuscript represents both a call to action, and a comprehensiveresource for how to do good experiments in reinforcement learning. Inparticular, we cover: the statistical assumptions underlying common performancemeasures, how to properly characterize performance variation and stability,hypothesis testing, special considerations for comparing multiple agents,baseline and illustrative example construction, and how to deal withhyper-parameters and experimenter bias. Throughout we highlight common mistakesfound in the literature and the statistical consequences of those in exampleexperiments. The objective of this document is to provide answers on how we canuse our unprecedented compute to do good science in reinforcement learning, aswell as stay alert to potential pitfalls in our empirical design.", "output": "Empirical Design in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The intersection of machine learning and dynamical systems has generatedconsiderable interest recently. Neural Ordinary Differential Equations (NODEs)represent a rich overlap between these fields. In this paper, we develop acontinuous time neural network approach based on Delay Differential Equations(DDEs). Our model uses the adjoint sensitivity method to learn the modelparameters and delay directly from data. Our approach is inspired by that ofNODEs and extends earlier neural DDE models, which have assumed that the valueof the delay is known a priori. We perform a sensitivity analysis on ourproposed approach and demonstrate its ability to learn DDE parameters frombenchmark systems. We conclude our discussion with potential future directionsand applications.", "output": "Learning the Delay Using Neural Delay Differential Equations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Certain tasks such as determining whether a given integer can be divided by2, 3, or other prime numbers may be trivial for human beings, but can be lessstraightforward for computers in the absence of pre-specified algorithms. Inthis paper, we tested multiple deep learning architectures and featureengineering approaches, and evaluated the scenario of determining divisibilityof large finite integers (up to $2^{32}$) by small prime numbers. It turns outthat, regardless of the network frameworks or the complexity of the networkstructures (CNN, RNN, Transformer, etc.), the ability to predict the primenumber divisibility critically depends on the feature space fed into the deeplearning models. We also evaluated commercially available Automated MachineLearning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstratedthat they failed to address this issue unless appropriately engineered featureswere provided. We further proposed a closed form solution to the problem usingthe ordinary linear regression on Fourier series basis vectors, and showed itssuccess. Finally, we evaluated prompt-based learning using ChatGPT anddemonstrated its success on small primes and apparent failures on largerprimes. We conclude that feature engineering remains an important task toimprove the performance, increase the interpretability, and reduce thecomplexity of machine learning/deep learning models, even in the era of AutoMLand large-language models (LLMs).", "output": "On the Prime Number Divisibility by Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The training of neural networks is a complex, high-dimensional, non-convexand noisy optimization problem whose theoretical understanding is interestingboth from an applicative perspective and for fundamental reasons. A corechallenge is to understand the geometry and topography of the landscape thatguides the optimization. In this work, we employ standard Statistical Mechanicsmethods, namely, phase-space exploration using Langevin dynamics, to study thislandscape for an over-parameterized fully connected network performing aclassification task on random data. Analyzing the fluctuation statistics, inanalogy to thermal dynamics at a constant temperature, we infer a cleargeometric description of the low-loss region. We find that it is alow-dimensional manifold whose dimension can be readily obtained from thefluctuations. Furthermore, this dimension is controlled by the number of datapoints that reside near the classification decision boundary. Importantly, wefind that a quadratic approximation of the loss near the minimum isfundamentally inadequate due to the exponential nature of the decision boundaryand the flatness of the low-loss region. This causes the dynamics to sampleregions with higher curvature at higher temperatures, while producingquadratic-like statistics at any given temperature. We explain this behavior bya simplified loss model which is analytically tractable and reproduces theobserved fluctuation statistics.", "output": "Charting the Topography of the Neural Network Landscape with Thermal-Like Noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mathematical models for the generation of the action potential can improvethe understanding of physiological mechanisms that are consequence of theelectrical activity in neurons. In such models, some equations involvingempirically obtained functions of the membrane potential are usually defined.The best known of these models, the Hodgkin-Huxley model, is an example of thisparadigm since it defines the conductances of ion channels in terms of theopening and closing rates of each type of gate present in the channels. Thesefunctions need to be derived from laboratory measurements that are often veryexpensive and produce little data because they involve a time-space-independentmeasurement of the voltage in a single channel of the cell membrane. In thiswork, we investigate the possibility of finding the Hodgkin-Huxley model'sparametric functions using only two simple measurements (the membrane voltageas a function of time and the injected current that triggered that voltage) andapplying Deep Learning methods to estimate these functions. This would resultin an hybrid model of the action potential generation composed by the originalHodgkin-Huxley equations and an Artificial Neural Network that requires a smallset of easy-to-perform measurements to be trained. Experiments were carried outusing data generated from the original Hodgkin-Huxley model, and results showthat a simple two-layer artificial neural network (ANN) architecture trained ona minimal amount of data can learn to model some of the fundamental proprietiesof the action potential generation by estimating the model's rate functions.", "output": "Towards an Hybrid Hodgkin-Huxley Action Potential Generation Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Available evidence suggests that dynamic functional connectivity (dFC) cancapture time-varying abnormalities in brain activity in rs-fMRI data and has anatural advantage in uncovering mechanisms of abnormal brain activity inschizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysismodel called the temporal brain category graph convolutional network(temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysismodule, DSF-BrainNet, was designed to construct dynamic synchronizationfeatures. Subsequently, a revolutionary graph convolution method, TemporalConv,was proposed, based on the synchronous temporal properties of feature. Finally,the first modular abnormal hemispherical lateralization test tool in deeplearning based on rs-fMRI data, named CategoryPool, was proposed. This studywas validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% averageaccuracy, respectively, outperforming the baseline model and otherState-of-the-Art methods. The ablation results also demonstrate the advantagesof TemporalConv over the traditional edge feature convolution approach of graphconvolutional neural network (GCN) and the improvement of CategoryPool over theclassical graph pooling approach. Interestingly, this study showed that thelower order perceptual system and higher order network regions in the lefthemisphere are more severely dysfunctional than in the right hemisphere of SZ,and reaffirms the importance of the left medial superior frontal gyrus in SZ.Our core code is available at: ", "output": "Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model-based deep learning methods that combine imaging physics with learnedregularization priors have been emerging as powerful tools for parallel MRIacceleration. The main focus of this paper is to determine the utility of themonotone operator learning (MOL) framework in the parallel MRI setting. The MOLalgorithm alternates between a gradient descent step using a monotoneconvolutional neural network (CNN) and a conjugate gradient algorithm toencourage data consistency. The benefits of this approach include similarguarantees as compressive sensing algorithms including uniqueness, convergence,and stability, while being significantly more memory efficient than unrolledmethods. We validate the proposed scheme by comparing it with differentunrolled algorithms in the context of accelerated parallel MRI for static anddynamic settings.", "output": "Accelerated parallel MRI using memory efficient and robust monotone operator learning (MOL)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Objective. Research on brain-computer interfaces (BCIs) is advancing towardsrehabilitating severely disabled patients in the real world. Two key factorsfor successful decoding of user intentions are the size of implantedmicroelectrode arrays and a good online spike sorting algorithm. A small butdense microelectrode array with 3072 channels was recently developed fordecoding user intentions. The process of spike sorting determines the spikeactivity (SA) of different sources (neurons) from recorded neural data.Unfortunately, current spike sorting algorithms are unable to handle themassively increasing amount of data from dense microelectrode arrays, makingspike sorting a fragile component of the online BCI decoding framework.Approach. We proposed an adaptive and self-organized algorithm for online spikesorting, named Adaptive SpikeDeep-Classifier (Ada-SpikeDeepClassifier), whichuses SpikeDeeptector for channel selection, an adaptive background activityrejector (Ada-BAR) for discarding background events, and an adaptive spikeclassifier (Ada-Spike classifier) for classifying the SA of different neuralunits. Results. Our algorithm outperformed our previously publishedSpikeDeep-Classifier and eight other spike sorting algorithms, as evaluated ona human dataset and a publicly available simulated dataset. Significance. Theproposed algorithm is the first spike sorting algorithm that automaticallylearns the abrupt changes in the distribution of noise and SA. It is anartificial neural network-based algorithm that is well-suited for hardwareimplementation on neuromorphic chips that can be used for wearable invasiveBCIs.", "output": "Adaptive SpikeDeep-Classifier: Self-organizing and self-supervised machine learning algorithm for online spike sorting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Passive optical networks (PONs) have become a promising broadband accessnetwork solution. To ensure a reliable transmission, and to meet service levelagreements, PON systems have to be monitored constantly in order to quicklyidentify and localize networks faults. Typically, a service disruption in a PONsystem is mainly due to fiber cuts and optical network unit (ONU)transmitter/receiver failures. When the ONUs are located at different distancesfrom the optical line terminal (OLT), the faulty ONU or branch can beidentified by analyzing the recorded optical time domain reflectometry (OTDR)traces. However, faulty branch isolation becomes very challenging when thereflections originating from two or more branches with similar length overlap,which makes it very hard to discriminate the faulty branches given the globalbackscattered signal. Recently, machine learning (ML) based approaches haveshown great potential for managing optical faults in PON systems. Suchtechniques perform well when trained and tested with data derived from the samePON system. But their performance may severely degrade, if the PON system(adopted for the generation of the training data) has changed, e.g. by addingmore branches or varying the length difference between two neighboringbranches. etc. A re-training of the ML models has to be conducted for eachnetwork change, which can be time consuming. In this paper, to overcome theaforementioned issues, we propose a generic ML approach trained independentlyof the network architecture for identifying the faulty branch in PON systemsgiven OTDR signals for the cases of branches with close lengths. Such anapproach can be applied to an arbitrary PON system without requiring to bere-trained for each change of the network. The proposed approach is validatedusing experimental data derived from PON system.", "output": "Faulty Branch Identification in Passive Optical Networks using Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph-structured data are pervasive in the real-world such as socialnetworks, molecular graphs and transaction networks. Graph neural networks(GNNs) have achieved great success in representation learning on graphs,facilitating various downstream tasks. However, GNNs have several drawbackssuch as lacking interpretability, can easily inherit the bias of the trainingdata and cannot model the casual relations. Recently, counterfactual learningon graphs has shown promising results in alleviating these drawbacks. Variousgraph counterfactual learning approaches have been proposed for counterfactualfairness, explainability, link prediction and other applications on graphs. Tofacilitate the development of this promising direction, in this survey, wecategorize and comprehensively review papers on graph counterfactual learning.We divide existing methods into four categories based on research problemsstudied. For each category, we provide background and motivating examples, ageneral framework summarizing existing works and a detailed review of theseworks. We point out promising future research directions at the intersection ofgraph-structured data, counterfactual learning, and real-world applications. Tooffer a comprehensive view of resources for future studies, we compile acollection of open-source implementations, public datasets, and commonly-usedevaluation metrics. This survey aims to serve as a ``one-stop-shop'' forbuilding a unified understanding of graph counterfactual learning categoriesand current resources. We also maintain a repository for papers and resourcesand will keep updating the repository", "output": "Counterfactual Learning on Graphs: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We address the problem of learning linear system models from observingmultiple trajectories from different system dynamics. This frameworkencompasses a collaborative scenario where several systems seeking to estimatetheir dynamics are partitioned into clusters according to their systemsimilarity. Thus, the systems within the same cluster can benefit from theobservations made by the others. Considering this framework, we present analgorithm where each system alternately estimates its cluster identity andperforms an estimation of its dynamics. This is then aggregated to update themodel of each cluster. We show that under mild assumptions, our algorithmcorrectly estimates the cluster identities and achieves an approximate samplecomplexity that scales inversely with the number of systems in the cluster,thus facilitating a more efficient and personalized system identificationprocess.", "output": "Learning Personalized Models with Clustered System Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In material characterization, identifying defective areas on a materialsurface is fundamental. The conventional approach involves measuring therelevant physical properties point-by-point at the predetermined mesh gridpoints on the surface and determining the area at which the property does notreach the desired level. To identify defective areas more efficiently, wepropose adaptive mapping methods in which measurement resources are usedpreferentially to detect the boundaries of defective areas. We interpret thisproblem as an active-learning (AL) of the level set estimation (LSE) problem.The goal of AL-based LSE is to determine the level set of the physical propertyfunction defined on the surface with as small number of measurements aspossible. Furthermore, to handle the situations in which materials with similarspecifications are repeatedly produced, we introduce a transfer learningapproach so that the information of previously produced materials can beeffectively utilized. As a proof-of-concept, we applied the proposed methods tothe red-zone estimation problem of silicon wafers and demonstrated that wecould identify the defective areas with significantly lower measurement coststhan those of conventional methods.", "output": "Adaptive Defective Area Identification in Material Surface Using Active Transfer Learning-based Level Set Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While error backpropagation (BP) has dominated the training of nearly allmodern neural networks for a long time, it suffers from several biologicalplausibility issues such as the symmetric weight requirement and synchronousupdates. Feedback Alignment (FA) was proposed as an alternative to BP toaddress those dilemmas and has been demonstrated to be effective on varioustasks and network architectures. Despite its simplicity and effectiveness, asatisfying explanation of how FA works across different architectures is stilllacking. Here we propose a novel, architecture-agnostic theory of how FA worksthrough the lens of information theory: Instead of approximating gradientscalculated by BP with the same parameter, FA learns effective representationsby embedding target information into neural networks to be trained. We showthis through the analysis of FA dynamics in idealized settings and then via aseries of experiments. Based on the implications of this theory, we designedthree variants of FA and show their comparable performance on several tasks.These variants also account for some phenomena and theories in neurosciencesuch as predictive coding and representational drift.", "output": "Learning with augmented target information: An alternative theory of Feedback Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we consider the scenario-based two-stage stochastic DC optimalpower flow (OPF) problem for optimal and reliable dispatch when the load isfacing uncertainty. Although this problem is a linear program, it remainscomputationally challenging to solve due to the large number of scenariosneeded to accurately represent the uncertainties. To mitigate the computationalissues, many techniques have been proposed to approximate the second-stagedecisions so they can dealt more efficiently. The challenge of finding goodpolicies to approximate the second-stage decisions is that these solutions needto be feasible, which has been difficult to achieve with existing policies.To address these challenges, this paper proposes a learning method to solvethe two-stage problem in a more efficient and optimal way. A technique calledthe gauge map is incorporated into the learning architecture design toguarantee the learned solutions' feasibility to the network constraints.Namely, we can design policies that are feed forward functions that only outputfeasible solutions. Simulation results on standard IEEE systems show that,compared to iterative solvers and the widely used affine policy, our proposedmethod not only learns solutions of good quality but also accelerates thecomputation by orders of magnitude.", "output": "An Efficient Learning-Based Solver for Two-Stage DC Optimal Power Flow with Feasibility Guarantees."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop a predictive inference procedure that combines conformalprediction (CP) with unconditional quantile regression (QR) -- a commonly usedtool in econometrics that involves regressing the recentered influence function(RIF) of the quantile functional over input covariates. Unlike the morewidely-known conditional QR, unconditional QR explicitly captures the impact ofchanges in covariate distribution on the quantiles of the marginal distributionof outcomes. Leveraging this property, our procedure issues adaptive predictiveintervals with localized frequentist coverage guarantees. It operates byfitting a machine learning model for the RIFs using training data, and thenapplying the CP procedure for any test covariate with respect to a``hypothetical'' covariate distribution localized around the new instance.Experiments show that our procedure is adaptive to heteroscedasticity, providestransparent coverage guarantees that are relevant to the test instance at hand,and performs competitively with existing methods in terms of efficiency.", "output": "Conformalized Unconditional Quantile Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The fast-growing demand for fully autonomous aerial operations in sharedspaces necessitates developing trustworthy agents that can safely andseamlessly navigate in crowded, dynamic spaces. In this work, we propose SocialRobot Tree Search (SoRTS), an algorithm for the safe navigation of mobilerobots in social domains. SoRTS aims to augment existing socially-awaretrajectory prediction policies with a Monte Carlo Tree Search planner forimproved downstream navigation of mobile robots. To evaluate the performance ofour method, we choose the use case of social navigation for general aviation.To aid this evaluation, within this work, we also introduce X-PlaneROS, ahigh-fidelity aerial simulator, to enable more research in full-scale aerialautonomy. By conducting a user study based on the assessments of 26 FAAcertified pilots, we show that SoRTS performs comparably to a competent humanpilot, significantly outperforming our baseline algorithm. We furthercomplement these results with self-play experiments in scenarios withincreasing complexity.", "output": "Learned Tree Search for Long-Horizon Social Robot Navigation in Shared Airspace."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Missing data are an unavoidable complication in many machine learning tasks.When data are `missing at random' there exist a range of tools and techniquesto deal with the issue. However, as machine learning studies become moreambitious, and seek to learn from ever-larger volumes of heterogeneous data, anincreasingly encountered problem arises in which missing values exhibit anassociation or structure, either explicitly or implicitly. Such `structuredmissingness' raises a range of challenges that have not yet been systematicallyaddressed, and presents a fundamental hindrance to machine learning at scale.Here, we outline the current literature and propose a set of grand challengesin learning from data with structured missingness.", "output": "Learning from data with structured missingness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a method to segment the visual field into independently movingregions, trained with no ground truth or supervision. It consists of anadversarial conditional encoder-decoder architecture based on Slot Attention,modified to use the image as context to decode optical flow without attemptingto reconstruct the image itself. In the resulting multi-modal representation,one modality (flow) feeds the encoder to produce separate latent codes (slots),whereas the other modality (image) conditions the decoder to generate the first(flow) from the slots. This design frees the representation from having toencode complex nuisance variability in the image due to, for instance,illumination and reflectance properties of the scene. Since customaryautoencoding based on minimizing the reconstruction error does not preclude theentire flow from being encoded into a single slot, we modify the loss to anadversarial criterion based on Contextual Information Separation. The resultingmin-max optimization fosters the separation of objects and their assignment todifferent attention slots, leading to Divided Attention, or DivA. DivAoutperforms recent unsupervised multi-object motion segmentation methods whiletripling run-time speed up to 104FPS and reducing the performance gap fromsupervised methods to 12% or less. DivA can handle different numbers of objectsand different image sizes at training and test time, is invariant topermutation of object labels, and does not require explicit regularization.", "output": "Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Frank-Wolfe algorithm is a popular method in structurally constrainedmachine learning applications, due to its fast per-iteration complexity.However, one major limitation of the method is a slow rate of convergence thatis difficult to accelerate due to erratic, zig-zagging step directions, evenasymptotically close to the solution. We view this as an artifact ofdiscretization; that is to say, the Frank-Wolfe emph{flow}, which is itstrajectory at asymptotically small step sizes, does not zig-zag, and reducingdiscretization error will go hand-in-hand in producing a more stabilizedmethod, with better convergence properties. We propose two improvements: amultistep Frank-Wolfe method that directly applies optimized higher-orderdiscretization schemes; and an LMO-averaging scheme with reduced discretizationerror, and whose local convergence rate over general convex sets acceleratesfrom a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.", "output": "Reducing Discretization Error in the Frank-Wolfe Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In response to innovations in machine learning (ML) models, productionworkloads changed radically and rapidly. TPU v4 is the fifth Google domainspecific architecture (DSA) and its third supercomputer for such ML models.Optical circuit switches (OCSes) dynamically reconfigure its interconnecttopology to improve scale, availability, utilization, modularity, deployment,security, power, and performance; users can pick a twisted 3D torus topology ifdesired. Much cheaper, lower power, and faster than Infiniband, OCSes andunderlying optical components are &lt;5% of system cost and &lt;3% of system power.Each TPU v4 includes SparseCores, dataflow processors that accelerate modelsthat rely on embeddings by 5x-7x yet use only 5% of die area and power.Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improvesperformance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chipsand thus ~10x faster overall, which along with OCS flexibility helps largelanguage models. For similar sized systems, it is ~4.3x-4.5x faster than theGraphcore IPU Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power thanthe Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computersof Google Cloud use ~3x less energy and produce ~20x less CO2e thancontemporary DSAs in a typical on-premise data center.", "output": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since the introduction of deep learning, a wide scope of representationproperties, such as decorrelation, whitening, disentanglement, rank, isotropy,and mutual information, have been studied to improve the quality ofrepresentation. However, manipulating such properties can be challenging interms of implementational effectiveness and general applicability. To addressthese limitations, we propose to regularize von Neumann entropy~(VNE) ofrepresentation. First, we demonstrate that the mathematical formulation of VNEis superior in effectively manipulating the eigenvalues of the representationautocorrelation matrix. Then, we demonstrate that it is widely applicable inimproving state-of-the-art algorithms or popular benchmark algorithms byinvestigating domain-generalization, meta-learning, self-supervised learning,and generative models. In addition, we formally establish theoreticalconnections with rank, disentanglement, and isotropy of representation.Finally, we provide discussions on the dimension control of VNE and therelationship with Shannon entropy. Code is available at:", "output": "VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Agricultural irrigation is a significant contributor to freshwaterconsumption. However, the current irrigation systems used in the field are notefficient. They rely mainly on soil moisture sensors and the experience ofgrowers, but do not account for future soil moisture loss. Predicting soilmoisture loss is challenging because it is influenced by numerous factors,including soil texture, weather conditions, and plant characteristics. Thispaper proposes a solution to improve irrigation efficiency, which is calledDRLIC. DRLIC is a sophisticated irrigation system that uses deep reinforcementlearning (DRL) to optimize its performance. The system employs a neuralnetwork, known as the DRL control agent, which learns an optimal control policythat considers both the current soil moisture measurement and the future soilmoisture loss. We introduce an irrigation reward function that enables ourcontrol agent to learn from previous experiences. However, there may beinstances where the output of our DRL control agent is unsafe, such asirrigating too much or too little water. To avoid damaging the health of theplants, we implement a safety mechanism that employs a soil moisture predictorto estimate the performance of each action. If the predicted outcome is deemedunsafe, we perform a relatively-conservative action instead. To demonstrate thereal-world application of our approach, we developed an irrigation system thatcomprises sprinklers, sensing and control nodes, and a wireless network. Weevaluate the performance of DRLIC by deploying it in a testbed consisting ofsix almond trees. During a 15-day in-field experiment, we compared the waterconsumption of DRLIC with a widely-used irrigation scheme. Our results indicatethat DRLIC outperformed the traditional irrigation method by achieving a watersavings of up to 9.52%.", "output": "Optimizing Irrigation Efficiency using Deep Reinforcement Learning in the Field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The growing number of cyber-attacks against Industrial Control Systems (ICS)in recent years has elevated security concerns due to the potentialcatastrophic impact. Considering the complex nature of ICS, detecting acyber-attack in them is extremely challenging and requires advanced methodsthat can harness multiple data modalities. This research utilizes network andsensor modality data from ICS processed with a deep multi-modal cyber-attackdetection model for ICS. Results using the Secure Water Treatment (SWaT) systemshow that the proposed model can outperform existing single modality models andrecent works in the literature by achieving 0.99 precision, 0.98 recall, and0.98 f-measure, which shows the effectiveness of using both modalities in acombined model for detecting cyber-attacks.", "output": "A Deep Multi-Modal Cyber-Attack Detection in Industrial Control Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning anticipation in Multi-Agent Reinforcement Learning (MARL) is areasoning paradigm where agents anticipate the learning steps of other agentsto improve cooperation among themselves. As MARL uses gradient-basedoptimization, learning anticipation requires using Higher-Order Gradients(HOG), with so-called HOG methods. Existing HOG methods are based on policyparameter anticipation, i.e., agents anticipate the changes in policyparameters of other agents. Currently, however, these existing HOG methods haveonly been applied to differentiable games or games with small state spaces. Inthis work, we demonstrate that in the case of non-differentiable games withlarge state spaces, existing HOG methods do not perform well and areinefficient due to their inherent limitations related to policy parameteranticipation and multiple sampling stages. To overcome these problems, wepropose Off-Policy Action Anticipation (OffPA2), a novel framework thatapproaches learning anticipation through action anticipation, i.e., agentsanticipate the changes in actions of other agents, via off-policy sampling. Wetheoretically analyze our proposed OffPA2 and employ it to develop multiple HOGmethods that are applicable to non-differentiable games with large statespaces. We conduct a large set of experiments and illustrate that our proposedHOG methods outperform the existing ones regarding efficiency and performance.", "output": "Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Confusion matrices and derived metrics provide a comprehensive framework forthe evaluation of model performance in machine learning. These are well-knownand extensively employed in the supervised learning domain, particularlyclassification. Surprisingly, such a framework has not been fully explored inthe context of clustering validation. Indeed, just recently such a gap has beenbridged with the introduction of the Area Under the ROC Curve for Clustering(AUCC), an internal/relative Clustering Validation Index (CVI) that allows forclustering validation in real application scenarios. In this work we explorethe Area Under Precision-Recall Curve (and related metrics) in the context ofclustering validation. We show that these are not only appropriate as CVIs, butshould also be preferred in the presence of cluster imbalance. We perform acomprehensive evaluation of proposed and state-of-art CVIs on real andsimulated data sets. Our observations corroborate towards an unified validationframework for supervised and unsupervised learning, given that they areconsistent with existing guidelines established for the evaluation ofsupervised learning models.", "output": "Clustering Validation with The Area Under Precision-Recall Curves."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-Language models (VLMs) that use contrastive language-imagepre-training have shown promising zero-shot classification performance.However, their performance on imbalanced dataset is relatively poor, where thedistribution of classes in the training dataset is skewed, leading to poorperformance in predicting minority classes. For instance, CLIP achieved only 5%accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoderto VLMs to avoid OOM (out of memory) problem caused by large number of classesand capture nuanced features for tail classes. Then, we explore improvements ofVLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithmssuch as Focal Loss, Balanced SoftMax and Distribution Alignment. Experimentsdemonstrate that the performance of VLMs can be further boosted when used withdecoder and imbalanced methods. Specifically, our improved VLMs significantlyoutperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,and 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. Wefurther analyze the influence of pre-training data size, backbones, andtraining cost. Our study highlights the significance of imbalanced learningalgorithms in face of VLMs pre-trained by huge data. We release our code at", "output": "Exploring Vision-Language Models for Imbalanced Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Low-channel EEG devices are crucial for portable and entertainmentapplications. However, the low spatial resolution of EEG presents challenges indecoding low-channel motor imagery. This study introduces TSFF-Net, a novelnetwork architecture that integrates time-space-frequency features, effectivelycompensating for the limitations of single-mode feature extraction networksbased on time-series or time-frequency modalities. TSFF-Net comprises four maincomponents: time-frequency representation, time-frequency feature extraction,time-space feature extraction, and feature fusion and classification.Time-frequency representation and feature extraction transform raw EEG signalsinto time-frequency spectrograms and extract relevant features. The time-spacenetwork processes time-series EEG trials as input and extracts temporal-spatialfeatures. Feature fusion employs MMD loss to constrain the distribution oftime-frequency and time-space features in the Reproducing Kernel Hilbert Space,subsequently combining these features using a weighted fusion approach toobtain effective time-space-frequency features. Moreover, few studies haveexplored the decoding of three-channel motor imagery based on time-frequencyspectrograms. This study proposes a shallow, lightweight decoding architecture(TSFF-img) based on time-frequency spectrograms and compares its classificationperformance in low-channel motor imagery with other methods using two publiclyavailable datasets. Experimental results demonstrate that TSFF-Net not onlycompensates for the shortcomings of single-mode feature extraction networks inEEG decoding, but also outperforms other state-of-the-art methods. Overall,TSFF-Net offers considerable advantages in decoding low-channel motor imageryand provides valuable insights for algorithmically enhancing low-channel EEGdecoding.", "output": "Time-space-frequency feature Fusion for 3-channel motor imagery classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, haverecently attracted increasing interest, research enthusiasm, and businessdemand. However, their massive computation resources and huge memory footprintare inevitable challenges. To tackle this issue, we propose BCT, a framework ofblockwise compression for transformers without retraining, to lower deploymentthresholds. BCT achieves more fine-grained compression of the wholetransformer, including embedding, matrix multiplication, GELU, Softmax, layernormalization, and all the intermediate results. As a case, we compress anefficient model with BCT and evaluate it on several General LanguageUnderstanding Evaluation (GLUE) datasets. The results show that BCT can achievea less than 0.90% accuracy drop in most tasks.", "output": "Blockwise Compression of Transformer-based Models without Retraining."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT has become a global sensation. As ChatGPT and other Large LanguageModels (LLMs) emerge, concerns of misusing them in various ways increase, suchas disseminating fake news, plagiarism, manipulating public opinion, cheating,and fraud. Hence, distinguishing AI-generated from human-generated becomesincreasingly essential. Researchers have proposed various detectionmethodologies, ranging from basic binary classifiers to more complexdeep-learning models. Some detection techniques rely on statisticalcharacteristics or syntactic patterns, while others incorporate semantic orcontextual information to improve accuracy. The primary objective of this studyis to provide a comprehensive and contemporary assessment of the most recenttechniques in ChatGPT detection. Additionally, we evaluated other AI-generatedtext detection tools that do not specifically claim to detect ChatGPT-generatedcontent to assess their performance in detecting ChatGPT-generated content. Forour evaluation, we have curated a benchmark dataset consisting of prompts fromChatGPT and humans, including diverse questions from medical, open Q&amp;A, andfinance domains and user-generated responses from popular social networkingplatforms. The dataset serves as a reference to assess the performance ofvarious techniques in detecting ChatGPT-generated content. Our evaluationresults demonstrate that none of the existing methods can effectively detectChatGPT-generated content.", "output": "To ChatGPT, or not to ChatGPT: That is the question!."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For decades, track association has been a challenging problem in marinesurveillance, which involves the identification and association of vesselobservations over time. However, the Automatic Identification System (AIS) hasprovided a new opportunity for researchers to tackle this problem by offering alarge database of dynamic and geo-spatial information of marine vessels. Withthe availability of such large databases, researchers can now developsophisticated models and algorithms that leverage the increased availability ofdata to address the track association challenge effectively. Furthermore, withthe advent of deep learning, track association can now be approached as adata-intensive problem. In this study, we propose a Long Short-Term Memory(LSTM) based multi-model framework for track association. LSTM is a recurrentneural network architecture that is capable of processing multivariate temporaldata collected over time in a sequential manner, enabling it to predict currentvessel locations from historical observations. Based on these predictions, ageodesic distance based similarity metric is then utilized to associate theunclassified observations to their true tracks (vessels). We evaluate theperformance of our approach using standard performance metrics, such asprecision, recall, and F1 score, which provide a comprehensive summary of theaccuracy of the proposed framework.", "output": "Multi model LSTM architecture for Track Association based on Automatic Identification System Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a representative next-generation device/circuit technology beyond CMOS,diffractive optical neural networks (DONNs) have shown promising advantagesover conventional deep neural networks due to extreme fast computation speed(light speed) and low energy consumption. However, there is a mismatch, i.e.,significant prediction accuracy loss, between the DONN numerical modelling andphysical optical device deployment, because of the interpixel interactionwithin the diffractive layers. In this work, we propose a physics-awarediffractive optical neural network training framework to reduce the performancedifference between numerical modeling and practical deployment. Specifically,we propose the roughness modeling regularization in the training process andintegrate the physics-aware sparsification method to introduce sparsity to thephase masks to reduce sharp phase changes between adjacent pixels indiffractive layers. We further develop $2pi$ periodic optimization to reducethe roughness of the phase masks to preserve the performance of DONN.Experiment results demonstrate that, compared to state-of-the-arts, ourphysics-aware optimization can provide $35.7%$, $34.2%$, $28.1%$, and$27.3%$ reduction in roughness with only accuracy loss on MNIST, FMNIST,KMNIST, and EMNIST, respectively.", "output": "Physics-aware Roughness Optimization for Diffractive Optical Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data privacy concerns has made centralized training of data, which isscattered across silos, infeasible, leading to the need for collaborativelearning frameworks. To address that, two prominent frameworks emerged, i.e.,federated learning (FL) and split learning (SL). While FL has establishedvarious benchmark frameworks and research libraries, SL currently lacks aunified library despite its diversity in terms of label sharing, modelaggregation, and cut layer choice. This lack of standardization makes comparingSL paradigms difficult. To address this, we propose SLPerf, a unified researchframework and open research library for SL, and conduct extensive experimentson four widely-used datasets under both IID and Non-IID data settings. Ourcontributions include a comprehensive survey of recently proposed SL paradigms,a detailed benchmark comparison of different SL paradigms in differentsituations, and rich engineering take-away messages and research insights forimproving SL paradigms. SLPerf can facilitate SL algorithm development and fairperformance comparisons.", "output": "SLPerf: a Unified Framework for Benchmarking Split Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Seasonal-trend decomposition is one of the most fundamental concepts in timeseries analysis that supports various downstream tasks, including time seriesanomaly detection and forecasting. However, existing decomposition methods relyon batch processing with a time complexity of O(W), where W is the number ofdata points within a time window. Therefore, they cannot always efficientlysupport real-time analysis that demands low processing delay. To address thischallenge, we propose OneShotSTL, an efficient and accurate algorithm that candecompose time series online with an update time complexity of O(1). OneShotSTLis more than $1,000$ times faster than the batch methods, with accuracycomparable to the best counterparts. Extensive experiments on real-worldbenchmark datasets for downstream time series anomaly detection and forecastingtasks demonstrate that OneShotSTL is from 10 to over 1,000 times faster thanthe state-of-the-art methods, while still providing comparable or even betteraccuracy.", "output": "OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series Anomaly Detection And Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked graph autoencoder (MGAE) has emerged as a promising self-supervisedgraph pre-training (SGP) paradigm due to its simplicity and effectiveness.However, existing efforts perform the mask-then-reconstruct operation in theraw data space as is done in computer vision (CV) and natural languageprocessing (NLP) areas, while neglecting the important non-Euclidean propertyof graph data. As a result, the highly unstable local connection structureslargely increase the uncertainty in inferring masked data and decrease thereliability of the exploited self-supervision signals, leading to inferiorrepresentations for downstream evaluations. To address this issue, we propose anovel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve thecertainty in inferring masked data and the reliability of the self-supervisionmechanism by further masking and reconstructing node samples in the high-orderlatent feature space. Through both theoretical and empirical analyses, we havediscovered that performing a joint mask-then-reconstruct strategy in bothlatent feature and raw data spaces could yield improved stability andperformance. To this end, we elaborately design a masked latent featurecompletion scheme, which predicts latent features of masked nodes under theguidance of high-order sample correlations that are hard to be observed fromthe raw data perspective. Specifically, we first adopt a latent featurepredictor to predict the masked latent features from the visible ones. Next, weencode the raw data of masked samples with a momentum graph encoder andsubsequently employ the resulting representations to improve predicted resultsthrough latent feature matching. Extensive experiments on seventeen datasetshave demonstrated the effectiveness and robustness of RARE againststate-of-the-art (SOTA) competitors across three downstream tasks.", "output": "RARE: Robust Masked Graph Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Skin lesion recognition using deep learning has made remarkable progress, andthere is an increasing need for deploying these systems in real-worldscenarios. However, recent research has revealed that deep neural networks forskin lesion recognition may overly depend on disease-irrelevant image artifacts(i.e. dark corners, dense hairs), leading to poor generalization in unseenenvironments. To address this issue, we propose a novel domain generalizationmethod called EPVT, which involves embedding prompts into the visiontransformer to collaboratively learn knowledge from diverse domains.Concretely, EPVT leverages a set of domain prompts, each of which plays as adomain expert, to capture domain-specific knowledge; and a shared prompt forgeneral knowledge over the entire dataset. To facilitate knowledge sharing andthe interaction of different prompts, we introduce a domain prompt generatorthat enables low-rank multiplicative updates between domain prompts and theshared prompt. A domain mixup strategy is additionally devised to reduce theco-occurring artifacts in each domain, which allows for more flexible decisionmargins and mitigates the issue of incorrectly assigned domain labels.Experiments on four out-of-distribution datasets and six different biased ISICdatasets demonstrate the superior generalization ability of EPVT in skin lesionrecognition across various environments. Our code and dataset will be releasedat ", "output": "EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning (ML) based time series forecasting models often require andassume certain degrees of stationarity in the data when producing forecasts.However, in many real-world situations, the data distributions are notstationary and they can change over time while reducing the accuracy of theforecasting models, which in the ML literature is known as concept drift.Handling concept drift in forecasting is essential for many ML methods in usenowadays, however, the prior work only proposes methods to handle concept driftin the classification domain. To fill this gap, we explore concept drifthandling methods in particular for Global Forecasting Models (GFM) whichrecently have gained popularity in the forecasting domain. We propose two newconcept drift handling methods, namely: Error Contribution Weighting (ECW) andGradient Descent Weighting (GDW), based on a continuous adaptive weightingconcept. These methods use two forecasting models which are separately trainedwith the most recent series and all series, and finally, the weighted averageof the forecasts provided by the two models are considered as the finalforecasts. Using LightGBM as the underlying base learner, in our evaluation onthree simulated datasets, the proposed models achieve significantly higheraccuracy than a set of statistical benchmarks and LightGBM baselines acrossfour evaluation metrics.", "output": "Handling Concept Drift in Global Time Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Token-based masked generative models are gaining popularity for their fastinference time with parallel decoding. While recent token-based approachesachieve competitive performance to diffusion-based models, their generationperformance is still suboptimal as they sample multiple tokens simultaneouslywithout considering the dependence among them. We empirically investigate thisproblem and propose a learnable sampling model, Text-Conditioned TokenSelection (TCTS), to select optimal tokens via localized supervision with textinformation. TCTS improves not only the image quality but also the semanticalignment of the generated images with the given texts. To further improve theimage quality, we introduce a cohesive sampling strategy, Frequency AdaptiveSampling (FAS), to each group of tokens divided according to the self-attentionmaps. We validate the efficacy of TCTS combined with FAS with variousgenerative tasks, demonstrating that it significantly outperforms the baselinesin image-text alignment and image quality. Our text-conditioned samplingframework further reduces the original inference time by more than 50% withoutmodifying the original generative model.", "output": "Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural processes (NPs) have brought the representation power of parametricdeep neural networks and the reliable uncertainty estimation of non-parametricGaussian processes together. Although recent development of NPs has shownsuccess in both regression and classification, how to adapt NPs to multimodaldata has not be carefully studied. For the first time, we propose a new modelof NP family for multimodal uncertainty estimation, namely Multimodal NeuralProcesses. In a holistic and principled way, we develop a dynamic contextmemory updated by the classification error, a multimodal Bayesian aggregationmechanism to aggregate multimodal representations, and a new attentionmechanism for calibrated predictions. In extensive empirical evaluation, ourmethod achieves the state-of-the-art multimodal uncertainty estimationperformance, showing its appealing ability of being robust against noisysamples and reliable in out-of-domain detection.", "output": "Multimodal Neural Processes for Uncertainty Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the measurement model $Y = AX,$ where $X$ and, hence, $Y$ arerandom variables and $A$ is an a priori known tall matrix. At each timeinstance, a sample of one of $Y$'s coordinates is available, and the goal is toestimate $mu := mathbb{E}[X]$ via these samples. However, the challenge isthat a small but unknown subset of $Y$'s coordinates are controlled byadversaries with infinite power: they can return any real number each time theyare queried for a sample. For such an adversarial setting, we propose the firstasynchronous online algorithm that converges to $mu$ almost surely. We provethis result using a novel differential inclusion based two-timescale analysis.Two key highlights of our proof include: (a) the use of a novel Lyapunovfunction for showing that $mu$ is the unique global attractor for ouralgorithm's limiting dynamics, and (b) the use of martingale and stopping timetheory to show that our algorithm's iterates are almost surely bounded.", "output": "Online Learning with Adversaries: A Differential Inclusion Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Privacy and communication constraints are two major bottlenecks in federatedlearning (FL) and analytics (FA). We study the optimal accuracy of mean andfrequency estimation (canonical models for FL and FA respectively) under jointcommunication and $(varepsilon, delta)$-differential privacy (DP)constraints. We show that in order to achieve the optimal error under$(varepsilon, delta)$-DP, it is sufficient for each client to send$Thetaleft( n minleft(varepsilon, varepsilon^2right)right)$ bits for FLand $Thetaleft(logleft( nminleft(varepsilon, varepsilon^2right)right)right)$ bits for FA to the server, where $n$ is the number ofparticipating clients. Without compression, each client needs $O(d)$ bits and$log d$ bits for the mean and frequency estimation problems respectively(where $d$ corresponds to the number of trainable parameters in FL or thedomain size in FA), which means that we can get significant savings in theregime $ n minleft(varepsilon, varepsilon^2right) = o(d)$, which is oftenthe relevant regime in practice. Our algorithms leverage compression forprivacy amplification: when each client communicates only partial informationabout its sample, we show that privacy can be amplified by randomly selectingthe part contributed by each client.", "output": "Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study investigates the impact of spatiotemporal data dimensions on theprecision of a wind forecasting model developed using an artificial neuralnetwork. Although previous studies have shown that incorporating spatial datacan enhance the accuracy of wind forecasting models, few investigations haveexplored the extent of the improvement owing to different spatial scales inneural network-based predictive models. Additionally, there are limited studieson the optimal temporal length of the input data for these models. To addressthis gap, this study employs data with various spatiotemporal dimensions asinputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN)and assesses their predictive performance. The results indicate that usingspatial data of the surrounding area for 3D-CNN training can achieve betterpredictive performance than using only single-point information. Additionally,multi-time data had a more positive effect on the predictive performance thansingle-time data. To determine the reasons for this, correlation analyses wereused to determine the impact of the spatial and temporal sizes of the trainingdata on the prediction performance. The study found that as the autocorrelationcoefficient (ACC) decreased, meaning that there was less similarity over time,the prediction performance decreased. Furthermore, the spatial standarddeviation of the ACC also affects the prediction performance. A Pearsoncorrelation coefficient (PCC) analysis was conducted to examine the effect ofspace on the prediction performance. Through the PCC analysis, we show thatlocal geometric and seasonal wind conditions can influence the forecastcapability of a predictive model.", "output": "How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model-agnostic meta-learning (MAML) is one of the most successfulmeta-learning algorithms. It has a bi-level optimization structure where theouter-loop process learns a shared initialization and the inner-loop processoptimizes task-specific weights. Although MAML relies on the standard gradientdescent in the inner-loop, recent studies have shown that controlling theinner-loop's gradient descent with a meta-learned preconditioner can bebeneficial. Existing preconditioners, however, cannot simultaneously adapt in atask-specific and path-dependent way. Additionally, they do not satisfy theRiemannian metric condition, which can enable the steepest descent learningwith preconditioned gradient. In this study, we propose Geometry-AdaptivePreconditioned gradient descent (GAP) that can overcome the limitations inMAML; GAP can efficiently meta-learn a preconditioner that is dependent ontask-specific parameters, and its preconditioner can be shown to be aRiemannian metric. Thanks to the two properties, the geometry-adaptivepreconditioner is effective for improving the inner-loop optimization.Experiment results show that GAP outperforms the state-of-the-art MAML familyand preconditioned gradient descent-MAML (PGD-MAML) family in a variety offew-shot learning tasks. Code is available at:", "output": "Meta-Learning with a Geometry-Adaptive Preconditioner."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The solar atmosphere shows anomalous variation in temperature, starting fromthe 5500 K photosphere to the million-degree Kelvin corona. The corona itselfexpands into the interstellar medium as the free streaming solar wind, whichmodulates and impacts the near-Earth space weather. The precise source regionsof different structures in the solar wind, their formation height, and theheating of the solar atmosphere are inextricably linked and unsolved problemsin astrophysics. Observations suggest correlations between Coronal holes (CHs),which are cool, intensity deficit structures in the solar corona, withstructures in the solar wind. Observations also suggest the local plasmaheating in the corona through power-law distributed impulsive events. In thisthesis, we use narrowband photometric, spectroscopic, and disc-integratedemission of the solar atmosphere ranging from Near Ultraviolet to X-rays alongwith in-situ solar wind measurements to understand (i). the source regions ofthe solar wind, (ii). the underlying mechanism of solar coronal heating, and(iii). the differentiation in dynamics of CHs with the background Quiet Sun(QS) regions, which do not show any significant signature of the solar wind. Weleverage machine learning and numerical modeling tools to develop solar windforecasting codes using interpretable AI, inversion codes to infer theproperties of impulsive events and to understand the differences in thethermodynamics of CHs and QS regions. We finally present a unified scenario ofsolar wind emergence and heating in the solar atmosphere and discuss theimplications of inferences from this thesis.", "output": "Heating and dynamics of the Solar atmosphere."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As road accident cases are increasing due to the inattention of the driver,automated driver monitoring systems (DMS) have gained an increase inacceptance. In this report, we present a real-time DMS system that runs on ahardware-accelerator-based edge device. The system consists of an InfraRedcamera to record the driver footage and an edge device to process the data. Tosuccessfully port the deep learning models to run on the edge device takingfull advantage of the hardware accelerators, model surgery was performed. Thefinal DMS system achieves 63 frames per second (FPS) on the TI-TDA4VM edgedevice.", "output": "Real-time Driver Monitoring Systems on Edge AI Device."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the approximation capacity of some variation spaces corresponding toshallow ReLU$^k$ neural networks. It is shown that sufficiently smoothfunctions are contained in these spaces with finite variation norms. Forfunctions with less smoothness, the approximation rates in terms of thevariation norm are established. Using these results, we are able to prove theoptimal approximation rates in terms of the number of neurons for shallowReLU$^k$ neural networks. It is also shown how these results can be used toderive approximation bounds for deep neural networks and convolutional neuralnetworks (CNNs). As applications, we study convergence rates for nonparametricregression using three ReLU neural network models: shallow neural network,over-parameterized neural network, and CNN. In particular, we show that shallowneural networks can achieve the minimax optimal rates for learning H\"olderfunctions, which complements recent results for deep neural networks. It isalso proven that over-parameterized (deep or shallow) neural networks canachieve nearly optimal rates for nonparametric regression.", "output": "Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have become a new SOTA generative modeling method in variousfields, for which there are multiple survey works that provide an overallsurvey. With the number of articles on diffusion models increasingexponentially in the past few years, there is an increasing need for surveys ofdiffusion models on specific fields. In this work, we are committed toconducting a survey on the graph diffusion models. Even though our focus is tocover the progress of diffusion models in graphs, we first briefly summarizehow other generative modeling methods are used for graphs. After that, weintroduce the mechanism of diffusion models in various forms, which facilitatesthe discussion on the graph diffusion models. The applications of graphdiffusion models mainly fall into the category of AI-generated content (AIGC)in science, for which we mainly focus on how graph diffusion models areutilized for generating molecules and proteins but also cover other cases,including materials design. Moreover, we discuss the issue of evaluatingdiffusion models in the graph domain and the existing challenges.", "output": "A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasonably and effectively monitoring arrhythmias through ECG signals hassignificant implications for human health. With the development of deeplearning, numerous ECG classification algorithms based on deep learning haveemerged. However, most existing algorithms trade off high accuracy for complexmodels, resulting in high storage usage and power consumption. This alsoinevitably increases the difficulty of implementation on wearable ArtificialIntelligence-of-Things (AIoT) devices with limited resources. In this study, weproposed a universally applicable ultra-lightweight binary neural network(BNN)that is capable of 5-class and 17-class arrhythmia classification based on ECGsignals. Our BNN achieves 96.90% (full precision 97.09%) and 97.50% (fullprecision 98.00%) accuracy for 5-class and 17-class classification,respectively, with state-of-the-art storage usage (3.76 KB and 4.45 KB).Compared to other binarization works, our approach excels in supporting twomulti-classification modes while achieving the smallest known storage space.Moreover, our model achieves optimal accuracy in 17-class classification andboasts an elegantly simple network architecture. The algorithm we use isoptimized specifically for hardware implementation. Our research showcases thepotential of lightweight deep learning models in the healthcare industry,specifically in wearable medical devices, which hold great promise forimproving patient outcomes and quality of life. Code is available on:", "output": "Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Urban anomaly predictions, such as traffic accident prediction and crimeprediction, are of vital importance to smart city security and maintenance.Existing methods typically use deep learning to capture the intra-dependenciesin spatial and temporal dimensions. However, numerous key challenges remainunsolved, for instance, sparse zero-inflated data due to urban anomaliesoccurring with low frequency (which can lead to poor performance on real-worlddatasets), and both intra- and inter-dependencies of abnormal patterns acrossspatial, temporal, and semantic dimensions. Moreover, a unified approach topredict multiple kinds of anomaly is left to explore. In this paper, we proposeSTS to jointly capture the intra- and inter-dependencies between the patternsand the influential factors in three dimensions. Further, we use a multi-taskprediction module with a customized loss function to solve the zero-inflatedissue. To verify the effectiveness of the model, we apply it to two urbananomaly prediction tasks, crime prediction and traffic accident riskprediction, respectively. Experiments on two application scenarios with fourreal-world datasets demonstrate the superiority of STS, which outperformsstate-of-the-art methods in the mean absolute error and the root mean squareerror by 37.88% and 18.10% on zero-inflated datasets, and, 60.32% and 37.28% onnon-zero datasets, respectively.", "output": "Spatiotemporal and Semantic Zero-inflated Urban Anomaly Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In Graph Neural Networks (GNNs), hierarchical pooling operators generate acoarser representation of the input data by creating local summaries of thegraph structure and its vertex features. Considerable attention has beendevoted to studying the expressive power of message-passing (MP) layers inGNNs, while a study on how pooling operators affect the expressivity of a GNNis still lacking. Additionally, despite the recent advances in the design ofeffective pooling operators, there is not a principled criterion to comparethem. Our work aims to fill this gap by providing sufficient conditions for apooling operator to fully preserve the expressive power of the MP layers beforeit. These conditions serve as a universal and theoretically-grounded criterionfor choosing among existing pooling operators or designing new ones. Based onour theoretical findings, we reviewed several existing pooling operators andidentified those that fail to satisfy the expressiveness assumptions. Finally,we introduced an experimental setup to empirically measure the expressive powerof a GNN equipped with pooling layers, in terms of its capability to perform agraph isomorphism test.", "output": "The expressive power of pooling in Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate lung nodule segmentation is crucial for early-stage lung cancerdiagnosis, as it can substantially enhance patient survival rates. Computedtomography (CT) images are widely employed for early diagnosis in lung noduleanalysis. However, the heterogeneity of lung nodules, size diversity, and thecomplexity of the surrounding environment pose challenges for developing robustnodule segmentation methods. In this study, we propose an efficient end-to-endframework, the multi-encoder-based self-adaptive hard attention network(MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Netcomprises three encoding paths, an attention block, and a decoder block,facilitating the integration of three types of inputs: CT slice patches,forward and backward maximum intensity projection (MIP) images, and region ofinterest (ROI) masks encompassing the nodule. By employing a novel adaptivehard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2Dsegmentation of lung nodules, focusing on the nodule region in each slice togenerate 3D volumetric segmentation of lung nodules. The proposed framework hasbeen comprehensively evaluated on the LIDC-IDRI dataset, the largest publiclyavailable dataset for lung nodule segmentation. The results demonstrate thatour approach is highly robust for various lung nodule types, outperformingprevious state-of-the-art techniques in terms of segmentation accuracy andcomputational complexity, rendering it suitable for real-time clinicalimplementation.", "output": "MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficiently predicting properties of porous crystalline materials has greatpotential to accelerate the high throughput screening process for developingnew materials, as simulations carried out using first principles model areoften computationally expensive. To effectively make use of Deep Learningmethods to model these materials, we need to utilize the symmetries present inthe crystals, which are defined by their space group. Existing methods forcrystal property prediction either have symmetry constraints that are toorestrictive or only incorporate symmetries between unit cells. In addition,these models do not explicitly model the porous structure of the crystal. Inthis paper, we develop a model which incorporates the symmetries of the unitcell of a crystal in its architecture and explicitly models the porousstructure. We evaluate our model by predicting the heat of adsorption of CO$_2$for different configurations of the mordenite zeolite. Our results confirm thatour method performs better than existing methods for crystal propertyprediction and that the inclusion of pores results in a more efficient model.", "output": "Equivariant Networks for Porous Crystalline Materials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Identification of experimentally acquired mass spectra of unknown compoundspresents a~particular challenge because reliable spectral databases do notcover the potential chemical space with sufficient density. Therefore machinelearning based emph{de-novo} methods, which derive molecular structuredirectly from its mass spectrum gained attention recently. We present a~novelmethod in this family, addressing a~specific usecase of GC-EI-MS spectra, whichis particularly hard due to lack of additional information from the first stageof MS/MS experiments, on which the previously published methods rely. Weanalyze strengths and drawbacks or our approach and discuss future directions.", "output": "De-novo Identification of Small Molecules from Their GC-EI-MS Spectra."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A primary goal of class-incremental learning is to strike a balance betweenstability and plasticity, where models should be both stable enough to retainknowledge learned from previously seen classes, and plastic enough to learnconcepts from new classes. While previous works demonstrate strong performanceon class-incremental benchmarks, it is not clear whether their success comesfrom the models being stable, plastic, or a mixture of both. This paper aims toshed light on how effectively recent class-incremental learning algorithmsaddress the stability-plasticity trade-off. We establish analytical tools thatmeasure the stability and plasticity of feature representations, and employsuch tools to investigate models trained with various algorithms on large-scaleclass-incremental benchmarks. Surprisingly, we find that the majority ofclass-incremental learning algorithms heavily favor stability over plasticity,to the extent that the feature extractor of a model trained on the initial setof classes is no less effective than that of the final incremental model. Ourobservations not only inspire two simple algorithms that highlight theimportance of feature representation analysis, but also suggest thatclass-incremental learning approaches, in general, should strive for betterfeature representation learning.", "output": "On the Stability-Plasticity Dilemma of Class-Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Model inversion (MI) attacks aim to infer and reconstruct private trainingdata by abusing access to a model. MI attacks have raised concerns about theleaking of sensitive information (e.g. private face images used in training aface recognition system). Recently, several algorithms for MI have beenproposed to improve the attack performance. In this work, we revisit MI, studytwo fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms,and propose solutions to these issues which lead to a significant boost inattack performance for all SOTA MI. In particular, our contributions aretwo-fold: 1) We analyze the optimization objective of SOTA MI algorithms, arguethat the objective is sub-optimal for achieving MI, and propose an improvedoptimization objective that boosts attack performance significantly. 2) Weanalyze \"MI overfitting\", show that it would prevent reconstructed images fromlearning semantics of training data, and propose a novel \"model augmentation\"idea to overcome this issue. Our proposed solutions are simple and improve allSOTA MI attack accuracy significantly. E.g., in the standard CelebA benchmark,our solutions improve accuracy by 11.8% and achieve for the first time over 90%attack accuracy. Our findings demonstrate that there is a clear risk of leakingsensitive information from deep learning models. We urge serious considerationto be given to the privacy implications. Our code, demo, and models areavailable at", "output": "Re-thinking Model Inversion Attacks Against Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce the state-of-the-art deep learning Denoising DiffusionProbabilistic Model (DDPM) as a method to infer the volume or number density ofgiant molecular clouds (GMCs) from projected mass surface density maps. Weadopt magnetohydrodynamic simulations with different global magnetic fieldstrengths and large-scale dynamics, i.e., noncolliding and colliding GMCs. Wetrain a diffusion model on both mass surface density maps and theircorresponding mass-weighted number density maps from different viewing anglesfor all the simulations. We compare the diffusion model performance with a moretraditional empirical two-component and three-component power-law fittingmethod and with a more traditional neural network machine learning approach(CASI-2D). We conclude that the diffusion model achieves an order of magnitudeimprovement on the accuracy of predicting number density compared to that byother methods. We apply the diffusion method to some example astronomicalcolumn density maps of Taurus and the Infrared Dark Clouds (IRDCs) G28.37+0.07and G35.39-0.33 to produce maps of their mean volume densities.", "output": "Denoising Diffusion Probabilistic Models to Predict the Density of Molecular Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The contribution of this paper is a generalized formulation of correctionallearning using optimal transport, which is about how to optimally transport onemass distribution to another. Correctional learning is a framework developed toenhance the accuracy of parameter estimation processes by means of ateacher-student approach. In this framework, an expert agent, referred to asthe teacher, modifies the data used by a learning agent, known as the student,to improve its estimation process. The objective of the teacher is to alter thedata such that the student's estimation error is minimized, subject to a fixedintervention budget. Compared to existing formulations of correctionallearning, our novel optimal transport approach provides several benefits. Itallows for the estimation of more complex characteristics as well as theconsideration of multiple intervention policies for the teacher. We evaluateour approach on two theoretical examples, and on a human-robot interactionapplication in which the teacher's role is to improve the robots performance inan inverse reinforcement learning setting.", "output": "Optimal Transport for Correctional Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we study non-asymptotic bounds on correlation between two timerealizations of stable linear systems with isotropic Gaussian noise.Consequently, via sampling from a sub-trajectory and using emph{Talagrands'}inequality, we show that empirical averages of reward concentrate around steadystate (dynamical system mixes to when closed loop system is stable under linearfeedback policy ) reward , with high-probability. As opposed to common beliefof larger the spectral radius stronger the correlation between samples,emph{large discrepancy between algebraic and geometric multiplicity of systemeigenvalues leads to large invariant subspaces related to system-transitionmatrix}; once the system enters the large invariant subspace it will travelaway from origin for a while before coming close to a unit ball centered atorigin where an isotropic Gaussian noise can with high probability allow it toescape the current invariant subspace it resides in, leading toemph{bottlenecks} between different invariant subspaces that span$mathbb{R}^{n}$, to be precise : system initiated in a large invariantsubspace will be stuck there for a long-time: log-linear in dimension of theinvariant subspace and inversely to log of inverse of magnitude of theeigenvalue. In the problem of Ordinary Least Squares estimate of systemtransition matrix via a single trajectory, this phenomenon is even more evidentif spectrum of transition matrix associated to large invariant subspace isexplosive and small invariant subspaces correspond to stable eigenvalues. Ouranalysis provide first interpretable and geometric explanation into intricaciesof learning and concentration for random dynamical systems on continuous, highdimensional state space; exposing us to surprises in high dimensions", "output": "Learning and Concentration for High Dimensional Linear Gaussians: an Invariant Subspace Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable Artificial Intelligence (XAI) provides tools to helpunderstanding how the machine learning models work and reach a specificoutcome. It helps to increase the interpretability of models and makes themodels more trustworthy and transparent. In this context, many XAI methods wereproposed being SHAP and LIME the most popular. However, the proposed methodsassume that used predictors in the machine learning models are independentwhich in general is not necessarily true. Such assumption casts shadows on therobustness of the XAI outcomes such as the list of informative predictors.Here, we propose a simple, yet useful proxy that modifies the outcome of anyXAI feature ranking method allowing to account for the dependency among thepredictors. The proposed approach has the advantage of being model-agnostic aswell as simple to calculate the impact of each predictor in the model inpresence of collinearity.", "output": "Characterizing the contribution of dependent features in XAI methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While federated learning is promising for privacy-preserving collaborativelearning without revealing local data, it remains vulnerable to white-boxattacks and struggles to adapt to heterogeneous clients. Federated distillation(FD), built upon knowledge distillation--an effective technique fortransferring knowledge from a teacher model to student models--emerges as analternative paradigm, which provides enhanced privacy guarantees and addressesmodel heterogeneity. Nevertheless, challenges arise due to variations in localdata distributions and the absence of a well-trained teacher model, which leadsto misleading and ambiguous knowledge sharing that significantly degrades modelperformance. To address these issues, this paper proposes a selective knowledgesharing mechanism for FD, termed Selective-FD. It includes client-sideselectors and a server-side selector to accurately and precisely identifyknowledge from local and ensemble predictions, respectively. Empirical studies,backed by theoretical insights, demonstrate that our approach enhances thegeneralization capabilities of the FD framework and consistently outperformsbaseline methods. This study presents a promising direction for effectiveknowledge transfer in privacy-preserving collaborative learning.", "output": "Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predictive simulations are essential for applications ranging from weatherforecasting to material design. The veracity of these simulations hinges ontheir capacity to capture the effective system dynamics. Massively parallelsimulations predict the systems dynamics by resolving all spatiotemporalscales, often at a cost that prevents experimentation. On the other hand,reduced order models are fast but often limited by the linearization of thesystem dynamics and the adopted heuristic closures. We propose a novelsystematic framework that bridges large scale simulations and reduced ordermodels to extract and forecast adaptively the effective dynamics (AdaLED) ofmultiscale systems. AdaLED employs an autoencoder to identify reduced-orderrepresentations of the system dynamics and an ensemble of probabilisticrecurrent neural networks (RNNs) as the latent time-stepper. The frameworkalternates between the computational solver and the surrogate, acceleratinglearned dynamics while leaving yet-to-be-learned dynamics regimes to theoriginal solver. AdaLED continuously adapts the surrogate to the new dynamicsthrough online training. The transitions between the surrogate and thecomputational solver are determined by monitoring the prediction accuracy anduncertainty of the surrogate. The effectiveness of AdaLED is demonstrated onthree different systems - a Van der Pol oscillator, a 2D reaction-diffusionequation, and a 2D Navier-Stokes flow past a cylinder for varying Reynoldsnumbers (400 up to 1200), showcasing its ability to learn effective dynamicsonline, detect unseen dynamics regimes, and provide net speed-ups. To the bestof our knowledge, AdaLED is the first framework that couples a surrogate modelwith a computational solver to achieve online adaptive learning of effectivedynamics. It constitutes a potent tool for applications requiring manyexpensive simulations.", "output": "Adaptive learning of effective dynamics: Adaptive real-time, online modeling for complex systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-Language (V-L) models trained with contrastive learning to align thevisual and language modalities have been shown to be strong few-shot learners.Soft prompt learning is the method of choice for few-shot downstream adaptionaiming to bridge the modality gap caused by the distribution shift induced bythe new domain. While parameter-efficient, prompt learning still requiresaccess to the model weights and can be computationally infeasible for largemodels with billions of parameters. To address these shortcomings, in thiswork, we describe a black-box method for V-L few-shot adaptation that (a)operates on pre-computed image and text features and hence works without accessto the model's weights, (b) it is orders of magnitude faster at training time,(c) it is amenable to both supervised and unsupervised training, and (d) it canbe even used to align image and text features computed from uni-modal models.To achieve this, we propose Linear Feature Alignment (LFA), a simple linearapproach for V-L re-alignment in the target domain. LFA is initialized from aclosed-form solution to a least-squares problem and then it is iterativelyupdated by minimizing a re-ranking loss. Despite its simplicity, our approachcan even surpass soft-prompt learning methods as shown by extensive experimentson 11 image and 2 video datasets.", "output": "Black Box Few-Shot Adaptation for Vision-Language models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop a contrastive framework for learning better prior distributionsfor Bayesian Neural Networks (BNNs) using unlabelled data. With this framework,we propose a practical BNN algorithm that offers the label-efficiency ofself-supervised learning and the principled uncertainty estimates of Bayesianmethods. Finally, we demonstrate the advantages of our approach fordata-efficient learning in semi-supervised and low-budget active learningproblems.", "output": "Incorporating Unlabelled Data into Bayesian Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we derive sufficient conditions for the convergence of twopopular alternating minimisation algorithms for dictionary learning - theMethod of Optimal Directions (MOD) and Online Dictionary Learning (ODL), whichcan also be thought of as approximative K-SVD. We show that given awell-behaved initialisation that is either within distance at most $1/log(K)$to the generating dictionary or has a special structure ensuring that eachelement of the initialisation only points to one generating element, bothalgorithms will converge with geometric convergence rate to the generatingdictionary. This is done even for data models with non-uniform distributions onthe supports of the sparse coefficients. These allow the appearance frequencyof the dictionary elements to vary heavily and thus model real data moreclosely.", "output": "Convergence of alternating minimisation algorithms for dictionary learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spin systems are a powerful tool for modeling a wide range of physicalsystems. In this paper, we propose a novel framework for modeling spin systemsusing differentiable programming. Our approach enables us to efficientlysimulate spin systems, making it possible to model complex systems at scale.Specifically, we demonstrate the effectiveness of our technique by applying itto three different spin systems: the Ising model, the Potts model, and theCellular Potts model. Our simulations show that our framework offerssignificant speedup compared to traditional simulation methods, thanks to itsability to execute code efficiently across different hardware architectures,including Graphical Processing Units and Tensor Processing Units.", "output": "A differentiable programming framework for spin models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human-in-the-loop topic modelling incorporates users' knowledge into themodelling process, enabling them to refine the model iteratively. Recentresearch has demonstrated the value of user feedback, but there are stillissues to consider, such as the difficulty in tracking changes, comparingdifferent models and the lack of evaluation based on real-world examples ofuse. We developed a novel, interactive human-in-the-loop topic modeling systemwith a user-friendly interface that enables users compare and record every stepthey take, and a novel topic words suggestion feature to help users providefeedback that is faithful to the ground truth. Our system also supports notonly what traditional topic models can do, i.e., learning the topics from thewhole corpus, but also targeted topic modelling, i.e., learning topics forspecific aspects of the corpus. In this article, we provide an overview of thesystem and present the results of a series of user studies designed to assessthe value of the system in progressively more realistic applications of topicmodelling.", "output": "A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A major technique in learning-augmented online algorithms is combiningmultiple algorithms or predictors. Since the performance of each predictor mayvary over time, it is desirable to use not the single best predictor as abenchmark, but rather a dynamic combination which follows different predictorsat different times. We design algorithms that combine predictions and arecompetitive against such dynamic combinations for a wide class of onlineproblems, namely, metrical task systems. Against the best (in hindsight)unconstrained combination of $ell$ predictors, we obtain a competitive ratioof $O(ell^2)$, and show that this is best possible. However, for a benchmarkwith slightly constrained number of switches between different predictors, wecan get a $(1+epsilon)$-competitive algorithm. Moreover, our algorithms can beadapted to access predictors in a bandit-like fashion, querying only onepredictor at a time. An unexpected implication of one of our lower bounds is anew structural insight about covering formulations for the $k$-server problem.", "output": "Mixing predictions for online metric algorithms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents a novel loss function for learning nonlinear ModelPredictive Control policies via Imitation Learning. Standard approaches toImitation Learning neglect information about the expert and generally adopt aloss function based on the distance between expert and learned controls. Inthis work, we present a loss based on the Q-function directly embedding theperformance objectives and constraint satisfaction of the associated OptimalControl Problem (OCP). However, training a Neural Network with the Q-lossrequires solving the associated OCP for each new sample. To alleviate thecomputational burden, we derive a second Q-loss based on the Gauss-Newtonapproximation of the OCP resulting in a faster training time. We validate ourlosses against Behavioral Cloning, the standard approach to Imitation Learning,on the control of a nonlinear system with constraints. The final results showthat the Q-function-based losses significantly reduce the amount of constraintviolations while achieving comparable or better closed-loop costs.", "output": "Imitation Learning from Nonlinear MPC via the Exact Q-Loss and its Gauss-Newton Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) aims to learn a single global model that enables thecentral server to help the model training in local clients without accessingtheir local data. The key challenge of FL is the heterogeneity of local data indifferent clients, such as heterogeneous label distribution and feature shift,which could lead to significant performance degradation of the learned models.Although many studies have been proposed to address the heterogeneous labeldistribution problem, few studies attempt to explore the feature shift issue.To address this issue, we propose a simple yet effective algorithm, namelytextbf{p}ersonalized textbf{Fed}erated learning with textbf{L}ocaltextbf{A}ttention (pFedLA), by incorporating the attention mechanism intopersonalized models of clients while keeping the attention blocksclient-specific. Specifically, two modules are proposed in pFedLA, i.e., thepersonalized single attention module and the personalized hybrid attentionmodule. In addition, the proposed pFedLA method is quite flexible and generalas it can be incorporated into any FL method to improve their performancewithout introducing additional communication costs. Extensive experimentsdemonstrate that the proposed pFedLA method can boost the performance ofstate-of-the-art FL methods on different tasks such as image classification andobject detection tasks.", "output": "Personalized Federated Learning with Local Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose the use of machine learning techniques to find optimal quadraturerules for the construction of stiffness and mass matrices in isogeometricanalysis (IGA). We initially consider 1D spline spaces of arbitrary degreespanned over uniform and non-uniform knot sequences, and then the generatedoptimal rules are used for integration over higher-dimensional spaces usingtensor product sense. The quadrature rule search is posed as an optimizationproblem and solved by a machine learning strategy based on gradient-descent.However, since the optimization space is highly non-convex, the success of thesearch strongly depends on the number of quadrature points and the parameterinitialization. Thus, we use a dynamic programming strategy that initializesthe parameters from the optimal solution over the spline space with a lowernumber of knots. With this method, we found optimal quadrature rules for splinespaces when using IGA discretizations with up to 50 uniform elements andpolynomial degrees up to 8, showing the generality of the approach in thisscenario. For non-uniform partitions, the method also finds an optimal rule ina reasonable number of test cases. We also assess the generated optimal rulesin two practical case studies, namely, the eigenvalue problem of the Laplaceoperator and the eigenfrequency analysis of freeform curved beams, where thelatter problem shows the applicability of the method to curved geometries. Inparticular, the proposed method results in savings with respect to traditionalGaussian integration of up to 44% in 1D, 68% in 2D, and 82% in 3D spaces.", "output": "Machine Learning Discovery of Optimal Quadrature Rules for Isogeometric Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Shapley value is widely regarded as a trustworthy attribution metric.However, when people use Shapley values to explain the attribution of inputvariables of a deep neural network (DNN), it usually requires a very highcomputational cost to approximate relatively accurate Shapley values inreal-world applications. Therefore, we propose a novel network architecture,the HarsanyiNet, which makes inferences on the input sample and simultaneouslycomputes the exact Shapley values of the input variables in a single forwardpropagation. The HarsanyiNet is designed on the theoretical foundation that theShapley value can be reformulated as the redistribution of Harsanyiinteractions encoded by the network.", "output": "HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Low-dose computed tomography (CT) images suffer from noise and artifacts dueto photon starvation and electronic noise. Recently, some works have attemptedto use diffusion models to address the over-smoothness and training instabilityencountered by previous deep-learning-based denoising models. However,diffusion models suffer from long inference times due to the large number ofsampling steps involved. Very recently, cold diffusion model generalizesclassical diffusion models and has greater flexibility. Inspired by the colddiffusion, this paper presents a novel COntextual eRror-modulated gEneralizedDiffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First,CoreDiff utilizes LDCT images to displace the random Gaussian noise and employsa novel mean-preserving degradation operator to mimic the physical process ofCT degradation, significantly reducing sampling steps thanks to the informativeLDCT images as the starting point of the sampling process. Second, to alleviatethe error accumulation problem caused by the imperfect restoration operator inthe sampling process, we propose a novel ContextuaL Error-modulAted RestorationNetwork (CLEAR-Net), which can leverage contextual information to constrain thesampling process from structural distortion and modulate time step embeddingfeatures for better alignment with the input at the next time step. Third, torapidly generalize to a new, unseen dose level with as few resources aspossible, we devise a one-shot learning framework to make CoreDiff generalizefaster and better using only a single LDCT image (un)paired with NDCT.Extensive experimental results on two datasets demonstrate that our CoreDiffoutperforms competing methods in denoising and generalization performance, witha clinically acceptable inference time.", "output": "CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a new Deep Neural Network (DNN) testing algorithmcalled the Constrained Gradient Descent (CGD) method, and an implementation wecall CGDTest aimed at exposing security and robustness issues such asadversarial robustness and bias in DNNs. Our CGD algorithm is agradient-descent (GD) method, with the twist that the user can also specifylogical properties that characterize the kinds of inputs that the user maywant. This functionality sets CGDTest apart from other similar DNN testingtools since it allows users to specify logical constraints to test DNNs notonly for $ell_p$ ball-based adversarial robustness but, more importantly,includes richer properties such as disguised and flow adversarial constraints,as well as adversarial robustness in the NLP domain. We showcase the utilityand power of CGDTest via extensive experimentation in the context of vision andNLP domains, comparing against 32 state-of-the-art methods over these diversedomains. Our results indicate that CGDTest outperforms state-of-the-art testingtools for $ell_p$ ball-based adversarial robustness, and is significantlysuperior in testing for other adversarial robustness, with improvements in PAR2scores of over 1500% in some cases over the next best tool. Our evaluationshows that our CGD method outperforms competing methods we compared against interms of expressibility (i.e., a rich constraint language and concomitant toolsupport to express a wide variety of properties), scalability (i.e., can beapplied to very large real-world models with up to 138 million parameters), andgenerality (i.e., can be used to test a plethora of model architectures).", "output": "CGDTest: A Constrained Gradient Descent Algorithm for Testing Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents two direct parameterizations of stable and robust linearparameter-varying state-space (LPV-SS) models. The model parametrizationsguarantee a priori that for all parameter values during training, the allowedmodels are stable in the contraction sense or have their Lipschitz constantbounded by a user-defined value $gamma$. Furthermore, since theparametrizations are direct, the models can be trained using unconstrainedoptimization. The fact that the trained models are of the LPV-SS class makesthem useful for, e.g., further convex analysis or controller design. Theeffectiveness of the approach is demonstrated on an LPV identification problem.", "output": "Learning Stable and Robust Linear Parameter-Varying State-Space Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vertical federated learning (VFL) is a promising category of federatedlearning for the scenario where data is vertically partitioned and distributedamong parties. VFL enriches the description of samples using features fromdifferent parties to improve model capacity. Compared with horizontal federatedlearning, in most cases, VFL is applied in the commercial cooperation scenarioof companies. Therefore, VFL contains tremendous business values. In the pastfew years, VFL has attracted more and more attention in both academia andindustry. In this paper, we systematically investigate the current work of VFLfrom a layered perspective. From the hardware layer to the vertical federatedsystem layer, researchers contribute to various aspects of VFL. Moreover, theapplication of VFL has covered a wide range of areas, e.g., finance,healthcare, etc. At each layer, we categorize the existing work and explore thechallenges for the convenience of further research and development of VFL.Especially, we design a novel MOSP tree taxonomy to analyze the core componentof VFL, i.e., secure vertical federated machine learning algorithm. Ourtaxonomy considers four dimensions, i.e., machine learning model (M),protection object (O), security model (S), and privacy-preserving protocol (P),and provides a comprehensive investigation.", "output": "A Survey on Vertical Federated Learning: From a Layered Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Complete verification of deep neural networks (DNNs) can exactly determinewhether the DNN satisfies a desired trustworthy property (e.g., robustness,fairness) on an infinite set of inputs or not. Despite the tremendous progressto improve the scalability of complete verifiers over the years on individualDNNs, they are inherently inefficient when a deployed DNN is updated to improveits inference speed or accuracy. The inefficiency is because the expensiveverifier needs to be run from scratch on the updated DNN. To improveefficiency, we propose a new, general framework for incremental and completeDNN verification based on the design of novel theory, data structure, andalgorithms. Our contributions implemented in a tool named IVAN yield an overallgeometric mean speedup of 2.4x for verifying challenging MNIST and CIFAR10classifiers and a geometric mean speedup of 3.8x for the ACAS-XU classifiersover the state-of-the-art baselines.", "output": "Incremental Verification of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study approximation properties of single hidden layerneural networks with weights varying on finitely many directions and thresholdsfrom an open interval. We obtain a necessary and at the same time sufficientmeasure theoretic condition for density of such networks in the space ofcontinuous functions. Further, we prove a density result for neural networkswith a specifically constructed activation function and a fixed number ofneurons.", "output": "Measure theoretic results for approximation by neural networks with limited weights."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce HATELEXICON, a lexicon of slurs and targets of hate speech forthe countries of Brazil, Germany, India and Kenya, to aid training andinterpretability of models. We demonstrate how our lexicon can be used tointerpret model predictions, showing that models developed to classify extremespeech rely heavily on target words when making predictions. Further, wepropose a method to aid shot selection for training in low-resource settingsvia HATELEXICON. In few-shot learning, the selection of shots is of paramountimportance to model performance. In our work, we simulate a few-shot settingfor German and Hindi, using HASOC data for training and the MultilingualHateCheck (MHC) as a benchmark. We show that selecting shots based on ourlexicon leads to models performing better on MHC than models trained on shotssampled randomly. Thus, when given only a few training examples, using ourlexicon to select shots containing more sociocultural information leads tobetter few-shot performance.", "output": "Sociocultural knowledge is needed for selection of shots in hate speech detection tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a method for generating realistic pedestrian trajectories andfull-body animations that can be controlled to meet user-defined goals. We drawon recent advances in guided diffusion modeling to achieve test-timecontrollability of trajectories, which is normally only associated withrule-based systems. Our guided diffusion model allows users to constraintrajectories through target waypoints, speed, and specified social groups whileaccounting for the surrounding environment context. This trajectory diffusionmodel is integrated with a novel physics-based humanoid controller to form aclosed-loop, full-body pedestrian animation system capable of placing largecrowds in a simulated environment with varying terrains. We further proposeutilizing the value function learned during RL training of the animationcontroller to guide diffusion to produce trajectories better suited forparticular scenarios such as collision avoidance and traversing uneven terrain.Video results are available on the project page at .", "output": "Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel class incremental learning approach by incorporating afeature augmentation technique motivated by adversarial attacks. We employ aclassifier learned in the past to complement training examples rather thansimply play a role as a teacher for knowledge distillation towards subsequentmodels. The proposed approach has a unique perspective to utilize the previousknowledge in class incremental learning since it augments features of arbitrarytarget classes using examples in other classes via adversarial attacks on apreviously learned classifier. By allowing the cross-class featureaugmentations, each class in the old tasks conveniently populates samples inthe feature space, which alleviates the collapse of the decision boundariescaused by sample deficiency for the previous tasks, especially when the numberof stored exemplars is small. This idea can be easily incorporated intoexisting class incremental learning algorithms without any architecturemodification. Extensive experiments on the standard benchmarks show that ourmethod consistently outperforms existing class incremental learning methods bysignificant margins in various scenarios, especially under an environment withan extremely limited memory budget.", "output": "Cross-Class Feature Augmentation for Class Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present dual-attention neural biasing, an architecture designed to boostWake Words (WW) recognition and improve inference time latency on speechrecognition tasks. This architecture enables a dynamic switch for its runtimecompute paths by exploiting WW spotting to select which branch of its attentionnetworks to execute for an input audio frame. With this approach, weeffectively improve WW spotting accuracy while saving runtime compute cost asdefined by floating point operations (FLOPs). Using an in-house de-identifieddataset, we demonstrate that the proposed dual-attention network can reduce thecompute cost by $90%$ for WW audio frames, with only $1%$ increase in thenumber of parameters. This architecture improves WW F1 score by $16%$ relativeand improves generic rare word error rate by $3%$ relative compared to thebaselines.", "output": "Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The $texttt{torch-choice}$ is an open-source library for flexible, fastchoice modeling with Python and PyTorch. $texttt{torch-choice}$ provides a$texttt{ChoiceDataset}$ data structure to manage databases flexibly andmemory-efficiently. The paper demonstrates constructing a$texttt{ChoiceDataset}$ from databases of various formats and functionalitiesof $texttt{ChoiceDataset}$. The package implements two widely used models,namely the multinomial logit and nested logit models, and supportsregularization during model estimation. The package incorporates the option totake advantage of GPUs for estimation, allowing it to scale to massive datasetswhile being computationally efficient. Models can be initialized using eitherR-style formula strings or Python dictionaries. We conclude with a comparisonof the computational efficiencies of $texttt{torch-choice}$ and$texttt{mlogit}$ in R as (1) the number of observations increases, (2) thenumber of covariates increases, and (3) the expansion of item sets. Finally, wedemonstrate the scalability of $texttt{torch-choice}$ on large-scale datasets.", "output": "Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conspicuous progression in the field of machine learning and deep learninghave led the jump of highly realistic fake media, these media oftentimesreferred as deepfakes. Deepfakes are fabricated media which are generated bysophisticated AI that are at times very difficult to set apart from the realmedia. So far, this media can be uploaded to the various social mediaplatforms, hence advertising it to the world got easy, calling for anefficacious countermeasure. Thus, one of the optimistic counter steps againstdeepfake would be deepfake detection. To undertake this threat, researchers inthe past have created models to detect deepfakes based on ML/DL techniques likeConvolutional Neural Networks. This paper aims to explore differentmethodologies with an intention to achieve a cost-effective model with a higheraccuracy with different types of the datasets, which is to address thegeneralizability of the dataset.", "output": "Leveraging Deep Learning Approaches for Deepfake Detection: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Typical neural network trainings have substantial variance in test-setperformance between repeated runs, impeding hyperparameter comparison andtraining reproducibility. We present the following results towardsunderstanding this variation. (1) Despite having significant variance on theirtest-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings havevery little variance in their performance on the test-distributions from whichthose test-sets are sampled, suggesting that variance is less of a practicalissue than previously thought. (2) We present a simplifying statisticalassumption which closely approximates the structure of the test-set accuracydistribution. (3) We argue that test-set variance is inevitable in thefollowing two senses. First, we show that variance is largely caused by highsensitivity of the training process to initial conditions, rather than byspecific sources of randomness like the data order and augmentations. Second,we prove that variance is unavoidable given the observation that ensembles oftrained networks are well-calibrated. (4) We conduct preliminary studies ofdistribution-shift, fine-tuning, data augmentation and learning rate throughthe lens of variance between runs.", "output": "Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent advances in machine learning and deep neural networks have madethem attractive candidates for wireless communications functions such aschannel estimation, decoding, and downlink channel state information (CSI)compression. However, most of these neural networks are large and inefficientmaking it a barrier for deployment in practical wireless systems that requirelow-latency and low memory footprints for individual network functions. Tomitigate these limitations, we propose accelerated and compressed efficientneural networks for massive MIMO CSI feedback. Specifically, we have thoroughlyinvestigated the adoption of network pruning, post-training dynamic rangequantization, and weight clustering to optimize CSI feedback compression formassive MIMO systems. Furthermore, we have deployed the proposed modelcompression techniques on commodity hardware and demonstrated that in order toachieve inference gains, specialized libraries that accelerate computations forsparse neural networks are required. Our findings indicate that there isremarkable value in applying these model compression techniques and theproposed joint pruning and quantization approach reduced model size by 86.5%and inference time by 76.2% with minimal impact to model accuracy. Thesecompression methods are crucial to pave the way for practical adoption anddeployments of deep learning-based techniques in commercial wireless systems.", "output": "Accelerating and Compressing Deep Neural Networks for Massive MIMO CSI Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is an increasing adoption of machine learning for encoding data intovectors to serve online recommendation and search use cases. As a result,recent data management systems propose augmenting query processing with onlinevector similarity search. In this work, we explore vector similarity search inthe context of Knowledge Graphs (KGs). Motivated by the tasks of findingrelated KG queries and entities for past KG query workloads, we focus on hybridvector similarity search (hybrid queries for short) where part of the querycorresponds to vector similarity search and part of the query corresponds topredicates over relational attributes associated with the underlying datavectors. For example, given past KG queries for a song entity, we want toconstruct new queries for new song entities whose vector representations areclose to the vector representation of the entity in the past KG query. Butentities in a KG also have non-vector attributes such as a song associated withan artist, a genre, and a release date. Therefore, suggested entities must alsosatisfy query predicates over non-vector attributes beyond a vector-basedsimilarity predicate. While these tasks are central to KGs, our contributionsare generally applicable to hybrid queries. In contrast to prior works thatoptimize online queries, we focus on enabling efficient batch processing ofpast hybrid query workloads. We present our system, HQI, for high-throughputbatch processing of hybrid queries. We introduce a workload-aware vector datapartitioning scheme to tailor the vector index layout to the given workload anddescribe a multi-query optimization technique to reduce the overhead of vectorsimilarity computations. We evaluate our methods on industrial workloads anddemonstrate that HQI yields a 31x improvement in throughput for finding relatedKG queries compared to existing hybrid query processing approaches.", "output": "High-Throughput Vector Similarity Search in Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning-assisted edge intelligence enables privacy protection inmodern intelligent services. However, not Independent and IdenticallyDistributed (non-IID) distribution among edge clients can impair the localmodel performance. The existing single prototype-based strategy represents asample by using the mean of the feature space. However, feature spaces areusually not clustered, and a single prototype may not represent a sample well.Motivated by this, this paper proposes a multi-prototype federated contrastivelearning approach (MP-FedCL) which demonstrates the effectiveness of using amulti-prototype strategy over a single-prototype under non-IID settings,including both label and feature skewness. Specifically, a multi-prototypecomputation strategy based on textit{k-means} is first proposed to capturedifferent embedding representations for each class space, using multipleprototypes ($k$ centroids) to represent a class in the embedding space. In eachglobal round, the computed multiple prototypes and their respective modelparameters are sent to the edge server for aggregation into a global prototypepool, which is then sent back to all clients to guide their local training.Finally, local training for each client minimizes their own supervised learningtasks and learns from shared prototypes in the global prototype pool throughsupervised contrastive learning, which encourages them to learn knowledgerelated to their own class from others and reduces the absorption of unrelatedknowledge in each global iteration. Experimental results on MNIST, Digit-5,Office-10, and DomainNet show that our method outperforms multiple baselines,with an average test accuracy improvement of about 4.6% and 10.4% underfeature and label non-IID distributions, respectively.", "output": "MP-FedCL: Multi-Prototype Federated Contrastive Learning for Edge Intelligence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Processing-in-memory (PIM) promises to alleviate the data movement bottleneckin modern computing systems. However, current real-world PIM systems have theinherent disadvantage that their hardware is more constrained than inconventional processors (CPU, GPU), due to the difficulty and cost of buildingprocessing elements near or inside the memory. As a result, general-purpose PIMarchitectures support fairly limited instruction sets and struggle to executecomplex operations such as transcendental functions and other hard-to-calculateoperations (e.g., square root). These operations are particularly important forsome modern workloads, e.g., activation functions in machine learningapplications.In order to provide support for transcendental (and other hard-to-calculate)functions in general-purpose PIM systems, we present emph{TransPimLib}, alibrary that provides CORDIC-based and LUT-based methods for trigonometricfunctions, hyperbolic functions, exponentiation, logarithm, square root, etc.We develop an implementation of TransPimLib for the UPMEM PIM architecture andperform a thorough evaluation of TransPimLib's methods in terms of performanceand accuracy, using microbenchmarks and three full workloads (Blackscholes,Sigmoid, Softmax). We open-source all our code and datasetsat~url{", "output": "TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel domain generalization technique, referred to as RandomizedAdversarial Style Perturbation (RASP), which is motivated by the observationthat the characteristics of each domain are captured by the feature statisticscorresponding to style. The proposed algorithm perturbs the style of a featurein an adversarial direction towards a randomly selected class, and makes themodel learn against being misled by the unexpected styles observed in unseentarget domains. While RASP is effective to handle domain shifts, its naiveintegration into the training procedure might degrade the capability oflearning knowledge from source domains because it has no restriction on theperturbations of representations. This challenge is alleviated by NormalizedFeature Mixup (NFM), which facilitates the learning of the original featureswhile achieving robustness to perturbed representations via their mixup duringtraining. We evaluate the proposed algorithm via extensive experiments onvarious benchmarks and show that our approach improves domain generalizationperformance, especially in large-scale benchmarks.", "output": "Randomized Adversarial Style Perturbations for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learned iterative reconstructions hold great promise to acceleratetomographic imaging with empirical robustness to model perturbations.Nevertheless, an adoption for photoacoustic tomography is hindered by the needto repeatedly evaluate the computational expensive forward model. Computationalfeasibility can be obtained by the use of fast approximate models, but a needto compensate model errors arises. In this work we advance the methodologicaland theoretical basis for model corrections in learned image reconstructions byembedding the model correction in a learned primal-dual framework. Here, themodel correction is jointly learned in data space coupled with a learnedupdating operator in image space within an unrolled end-to-end learnediterative reconstruction approach. The proposed formulation allows an extensionto a primal-dual deep equilibrium model providing fixed-point convergence aswell as reduced memory requirements for training. We provide theoretical andempirical insights into the proposed models with numerical validation in arealistic 2D limited-view setting. The model-corrected learned primal-dualmethods show excellent reconstruction quality with fast inference times andthus providing a methodological basis for real-time capable and scalableiterative reconstructions in photoacoustic tomography.", "output": "Model-corrected learned primal-dual models for fast limited-view photoacoustic tomography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-source Domain Generalization (DG) measures a classifier's ability togeneralize to new distributions of data it was not trained on, given severaltraining domains. While several multi-source DG methods have been proposed,they incur additional complexity during training by using domain labels. Recentwork has shown that a well-tuned Empirical Risk Minimization (ERM) trainingprocedure, that is simply minimizing the empirical risk on the source domains,can outperform most existing DG methods. We identify several key candidatetechniques to further improve ERM performance, such as better utilization oftraining data, model parameter selection, and weight-space regularization. Wecall the resulting method ERM++, and show it significantly improves theperformance of DG on five multi-source datasets by over 5% compared to standardERM, and beats state-of-the-art despite being less computationally expensive.Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset, achallenging DG benchmark. We hope that ERM++ becomes a strong baseline forfuture DG research. Code is released at", "output": "ERM++: An Improved Baseline for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Electrocardiogram (ECG) measures the electrical cardiac activitygenerated by the heart to detect abnormal heartbeat and heart attack. However,the irregular occurrence of the abnormalities demands continuous monitoring ofheartbeats. Machine learning techniques are leveraged to automate the task toreduce labor work needed during monitoring. In recent years, many companieshave launched products with ECG monitoring and irregular heartbeat alert. Amongall classification algorithms, the time series-based algorithm dynamic timewarping (DTW) is widely adopted to undertake the ECG classification task.Though progress has been achieved, the DTW-based ECG classification also bringsa new attacking vector of leaking the patients' diagnosis results. This papershows that the ECG input samples' labels can be stolen via a side-channelattack, Flush+Reload. In particular, we first identify the vulnerability of DTWfor ECG classification, i.e., the correlation between warping path choice andprediction results. Then we implement an attack that leverages Flush+Reload tomonitor the warping path selection with known ECG data and then build apredictor for constructing the relation between warping path selection andlabels of input ECG samples. Based on experiments, we find that theFlush+Reload-based inference leakage can achieve an 84.0% attacking successrate to identify the labels of the two samples in DTW.", "output": "Side Channel-Assisted Inference Leakage from Machine Learning-based ECG Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work introduces Differential Wavelet Amplifier (DWA), a drop-in modulefor wavelet-based image Super-Resolution (SR). DWA invigorates an approachrecently receiving less attention, namely Discrete Wavelet Transformation(DWT). DWT enables an efficient image representation for SR and reduces thespatial area of its input by a factor of 4, the overall model size, andcomputation cost, framing it as an attractive approach for sustainable ML. Ourproposed DWA model improves wavelet-based SR models by leveraging thedifference between two convolutional filters to refine relevant featureextraction in the wavelet domain, emphasizing local contrasts and suppressingcommon noise in the input signals. We show its effectiveness by integrating itinto existing SR models, e.g., DWSR and MWCNN, and demonstrate a clearimprovement in classical SR tasks. Moreover, DWA enables a direct applicationof DWSR and MWCNN to input image space, reducing the DWT representationchannel-wise since it omits traditional DWT.", "output": "DWA: Differential Wavelet Amplifier for Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum many-body physics simulation has important impacts on understandingfundamental science and has applications to quantum materials design andquantum technology. However, due to the exponentially growing size of theHilbert space with respect to the particle number, a direct simulation isintractable. While representing quantum states with tensor networks and neuralnetworks are the two state-of-the-art methods for approximate simulations, eachhas its own limitations in terms of expressivity and optimization. To addressthese challenges, we develop a novel architecture, Autoregressive NeuralTensorNet (ANTN), which bridges tensor networks and autoregressive neuralnetworks. We show that Autoregressive Neural TensorNet parameterizes normalizedwavefunctions with exact sampling, generalizes the expressivity of tensornetworks and autoregressive neural networks, and inherits a variety ofsymmetries from autoregressive neural networks. We demonstrate our approach onthe 2D $J_1$-$J_2$ Heisenberg model with different systems sizes and couplingparameters, outperforming both tensor networks and autoregressive neuralnetworks. Our work opens up new opportunities for both scientific simulationsand machine learning applications.", "output": "Autoregressive Neural TensorNet: Bridging Neural Networks and Tensor Networks for Quantum Many-Body Simulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Particle localization and -classification constitute two of the mostfundamental problems in computational microscopy. In recent years, deeplearning based approaches have been introduced for these tasks with greatsuccess. A key shortcoming of these supervised learning methods is their needfor large training data sets, typically generated from particle models inconjunction with complex numerical forward models simulating the physics oftransmission electron microscopes. Computer implementations of such forwardmodels are computationally extremely demanding and limit the scope of theirapplicability. In this paper we propose a simple method for simulating theforward operator of an electron microscope based on additive noise and NeuralStyle Transfer techniques. We evaluate the method on localization andclassification tasks using one of the established state-of-the-artarchitectures showing performance on par with the benchmark. In contrast toprevious approaches, our method accelerates the data generation process by afactor of 750 while using 33 times less memory and scales well to typicaltransmission electron microscope detector sizes. It utilizes GPU accelerationand parallel processing. It can be used as a stand-alone method to adapt atraining data set or as a data augmentation technique. The source code isavailable at ", "output": "FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning image classification and image generation using the same set ofnetwork parameters is a challenging problem. Recent advanced approaches performwell in one task often exhibit poor performance in the other. This workintroduces an energy-based classifier and generator, namely EGC, which canachieve superior performance in both tasks using a single neural network.Unlike a conventional classifier that outputs a label given an image (i.e., aconditional distribution $p(y|mathbf{x})$), the forward pass in EGC is aclassifier that outputs a joint distribution $p(mathbf{x},y)$, enabling animage generator in its backward pass by marginalizing out the label $y$. Thisis done by estimating the energy and classification probability given a noisyimage in the forward pass, while denoising it using the score functionestimated in the backward pass. EGC achieves competitive generation resultscompared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUNChurch, while achieving superior classification accuracy and robustness againstadversarial attacks on CIFAR-10. This work represents the first successfulattempt to simultaneously excel in both tasks using a single set of networkparameters. We believe that EGC bridges the gap between discriminative andgenerative learning.", "output": "EGC: Image Generation and Classification via a Single Energy-Based Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tensor decomposition methods have proven effective in various applications,including compression and acceleration of neural networks. At the same time,the problem of determining optimal decomposition ranks, which present thecrucial parameter controlling the compression-accuracy trade-off, is stillacute. In this paper, we introduce MARS -- a new efficient method for theautomatic selection of ranks in general tensor decompositions. During training,the procedure learns binary masks over decomposition cores that \"select\" theoptimal tensor structure. The learning is performed via relaxed maximum aposteriori (MAP) estimation in a specific Bayesian model and can be naturallyembedded into the standard neural network training routine. Diverse experimentsdemonstrate that MARS achieves better results compared to previous works invarious tasks.", "output": "MARS: Masked Automatic Ranks Selection in Tensor Decompositions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer networks have revolutionized NLP representation learning sincethey were introduced. Though a great effort has been made to explain therepresentation in transformers, it is widely recognized that our understandingis not sufficient. One important reason is that there lack enough visualizationtools for detailed analysis. In this paper, we propose to use dictionarylearning to open up these \"black boxes\" as linear superpositions of transformerfactors. Through visualization, we demonstrate the hierarchical semanticstructures captured by the transformer factors, e.g., word-level polysemydisambiguation, sentence-level pattern formation, and long-range dependency.While some of these patterns confirm the conventional prior linguisticknowledge, the rest are relatively unexpected, which may provide new insights.We hope this visualization tool can bring further knowledge and a betterunderstanding of how transformer networks work. The code is available at", "output": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We demonstrate when and how an entire left-infinite orbit of an underlyingdynamical system or observations from such left-infinite orbits can be uniquelyrepresented by a pair of elements in a different space, a phenomenon which wecall textit{causal embedding}. The collection of such pairs is derived from adriven dynamical system and is used to learn a function which together with thedriven system would: (i). determine a system that is topologically conjugate tothe underlying system (ii). enable forecasting the underlying system's dynamicssince the conjugacy is computable and universal, i.e., it does not depend onthe underlying system (iii). guarantee an attractor containing the image of thecausally embedded object even if there is an error made in learning thefunction. By accomplishing these we herald a new forecasting scheme that beatsthe existing reservoir computing schemes that often lead to poor long-termconsistency as there is no guarantee of the existence of a learnable function,and overcomes the challenges of stability in Takens delay embedding. Weillustrate accurate modeling of underlying systems where previously knowntechniques have failed.", "output": "Universal set of Observables for Forecasting Physical Systems through Causal Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classic and deep generalized canonical correlation analysis (GCCA) algorithmsseek low-dimensional common representations of data entities from multiple``views'' (e.g., audio and image) using linear transformations and neuralnetworks, respectively. When the views are acquired and stored at differentcomputing agents (e.g., organizations and edge devices) and data sharing isundesired due to privacy or communication cost considerations, federatedlearning-based GCCA is well-motivated. In federated learning, the views arekept locally at the agents and only derived, limited information exchange witha central server is allowed. However, applying existing GCCA algorithms ontosuch federated learning settings may incur prohibitively high communicationoverhead. This work puts forth a communication-efficient federated learningframework for both linear and deep GCCA under the maximum variance (MAX-VAR)formulation. The overhead issue is addressed by aggressively compressing (viaquantization) the exchanging information between the computing agents and acentral controller. Compared to the unquantized version, our empirical studyshows that the proposed algorithm enjoys a substantial reduction ofcommunication overheads with virtually no loss in accuracy and convergencespeed. Rigorous convergence analyses are also presented, which is a nontrivialeffort. Generic federated optimization results do not cover the special problemstructure of GCCA. Our result shows that the proposed algorithms for bothlinear and deep GCCA converge to critical points at a sublinear rate, evenunder heavy quantization and stochastic approximations. In addition, in thelinear MAX-VAR case, the quantized algorithm approaches a global optimum in ageometric rate under reasonable conditions. Synthetic and real-data experimentsare used to showcase the effectiveness of the proposed approach.", "output": "Communication-Efficient Federated Linear and Deep Generalized Canonical Correlation Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop a new framework for embedding joint probability distributions intensor product reproducing kernel Hilbert spaces (RKHS). Our frameworkaccommodates a low-dimensional, normalized and positive model of aRadon-Nikodym derivative, which we estimate from sample sizes of up to severalmillion data points, alleviating the inherent limitations of RKHS modeling.Well-defined normalized and positive conditional distributions are naturalby-products to our approach. The embedding is fast to compute and accommodateslearning problems ranging from prediction to classification. Our theoreticalfindings are supplemented by favorable numerical results.", "output": "Adaptive joint distribution learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Parameter learning for high-dimensional, partially observed, and nonlinearstochastic processes is a methodological challenge. Spatiotemporal diseasetransmission systems provide examples of such processes giving rise to openinference problems. We propose the iterated block particle filter (IBPF)algorithm for learning high-dimensional parameters over graphical state spacemodels with general state spaces, measures, transition densities and graphstructure. Theoretical performance guarantees are obtained on beating the curseof dimensionality (COD), algorithm convergence, and likelihood maximization.Experiments on a highly nonlinear and non-Gaussian spatiotemporal model formeasles transmission reveal that the iterated ensemble Kalman filter algorithm(Li et al. (2020)) is ineffective and the iterated filtering algorithm (Ionideset al. (2015)) suffers from the COD, while our IBPF algorithm beats CODconsistently across various experiments with different metrics.", "output": "Iterated Block Particle Filter for High-dimensional Parameter Learning: Beating the Curse of Dimensionality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting drug-drug interactions (DDI) is the problem of predicting sideeffects (unwanted outcomes) of a pair of drugs using drug information and knownside effects of many pairs. This problem can be formulated as predicting labels(i.e. side effects) for each pair of nodes in a DDI graph, of which nodes aredrugs and edges are interacting drugs with known labels. State-of-the-artmethods for this problem are graph neural networks (GNNs), which leverageneighborhood information in the graph to learn node representations. For DDI,however, there are many labels with complicated relationships due to the natureof side effects. Usual GNNs often fix labels as one-hot vectors that do notreflect label relationships and potentially do not obtain the highestperformance in the difficult cases of infrequent labels. In this paper, weformulate DDI as a hypergraph where each hyperedge is a triple: two nodes fordrugs and one node for a label. We then present CentSmoothie, a hypergraphneural network that learns representations of nodes and labels altogether witha novel central-smoothing formulation. We empirically demonstrate theperformance advantages of CentSmoothie in simulations as well as real datasets.", "output": "Central-Smoothing Hypergraph Neural Networks for Predicting Drug-Drug Interactions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Social media has become extremely influential when it comes to policy makingin modern societies, especially in the western world, where platforms such asTwitter allow users to follow politicians, thus making citizens more involvedin political discussion. In the same vein, politicians use Twitter to expresstheir opinions, debate among others on current topics and promote theirpolitical agendas aiming to influence voter behaviour. In this paper, weattempt to analyse tweets of politicians from three European countries andexplore the virality of their tweets. Previous studies have shown that tweetsconveying negative sentiment are likely to be retweeted more frequently. Byutilising state-of-the-art pre-trained language models, we performed sentimentanalysis on hundreds of thousands of tweets collected from members ofparliament in Greece, Spain and the United Kingdom, including devolvedadministrations. We achieved this by systematically exploring and analysing thedifferences between influential and less popular tweets. Our analysis indicatesthat politicians' negatively charged tweets spread more widely, especially inmore recent times, and highlights interesting differences between politicalparties as well as between politicians and the general population.", "output": "Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite recent advances of AI, story understanding remains an open andunder-investigated problem. We collect, preprocess, and publicly release avideo-language story dataset, Synopses of Movie Narratives (SYMON), containing5,193 video summaries of popular movies and TV series. SYMON capturesnaturalistic story-telling videos for human audience made by human creators. Asa prototypical and naturalistic story dataset, SYMON features high coverage ofmultimodal story events, abundant mental-state descriptions, and large semanticgaps between the visual and the textual modalities. We establish benchmarks onvideo-text retrieval and zero-shot alignment on movie summary videos, whichshowcase the importance of in-domain data in story understanding. With SYMON,we hope to lay the groundwork for progress in multimodal story understanding.", "output": "Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the complexity of the reachability problem for (deep) neuralnetworks: does it compute valid output given some valid input? It was recentlyclaimed that the problem is NP-complete for general neural networks andspecifications over the input/output dimension given by conjunctions of linearinequalities. We recapitulate the proof and repair some flaws in the originalupper and lower bound proofs. Motivated by the general result, we show thatNP-hardness already holds for restricted classes of simple specifications andneural networks. Allowing for a single hidden layer and an output dimension ofone as well as neural networks with just one negative, zero and one positiveweight or bias is sufficient to ensure NP-hardness. Additionally, we give athorough discussion and outlook of possible extensions for this direction ofresearch on neural network verification.", "output": "Reachability In Simple Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The acquisition of labels for supervised learning can be expensive. In orderto improve the sample-efficiency of neural network regression, we study activelearning methods that adaptively select batches of unlabeled data for labeling.We present a framework for constructing such methods out of (network-dependent)base kernels, kernel transformations and selection methods. Our frameworkencompasses many existing Bayesian methods based on Gaussian Processapproximations of neural networks as well as non-Bayesian methods.Additionally, we propose to replace the commonly used last-layer features withsketched finite-width Neural Tangent Kernels, and to combine them with a novelclustering method. To evaluate different methods, we introduce an open-sourcebenchmark consisting of 15 large tabular regression data sets. Our proposedmethod outperforms the state-of-the-art on our benchmark, scales to large datasets, and works out-of-the-box without adjusting the network architecture ortraining code. We provide open-source code that includes efficientimplementations of all kernels, kernel transformations, and selection methods,and can be used for reproducing our results.", "output": "A Framework and Benchmark for Deep Batch Active Learning for Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Classification networks can be used to localize and segment objects in imagesby means of class activation maps (CAMs). However, without pixel-levelannotations, classification networks are known to (1) mainly focus ondiscriminative regions, and (2) to produce diffuse CAMs without well-definedprediction contours. In this work, we approach both problems with twocontributions for improving CAM learning. First, we incorporate importancesampling based on the class-wise probability mass function induced by the CAMsto produce stochastic image-level class predictions. This results in CAMs whichactivate over a larger extent of objects. Second, we formulate a featuresimilarity loss term which aims to match the prediction contours with edges inthe image. As a third contribution, we conduct experiments on the PASCAL VOC2012 benchmark dataset to demonstrate that these modifications significantlyincrease the performance in terms of contour accuracy, while being comparableto current state-of-the-art methods in terms of region similarity.", "output": "Importance Sampling CAMs for Weakly-Supervised Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning is a setting where machine learning models learn novelconcepts from continuously shifting training data, while simultaneouslyavoiding degradation of knowledge on previously seen classes which maydisappear from the training data for extended periods of time (a phenomenonknown as the catastrophic forgetting problem). Current approaches for continuallearning of a single expanding task (aka class-incremental continual learning)require extensive rehearsal of previously seen data to avoid this degradationof knowledge. Unfortunately, rehearsal comes at a cost to memory, and it mayalso violate data-privacy. Instead, we explore combining knowledge distillationand parameter regularization in new ways to achieve strong continual learningperformance without rehearsal. Specifically, we take a deep dive into commoncontinual learning techniques: prediction distillation, feature distillation,L2 parameter regularization, and EWC parameter regularization. We firstdisprove the common assumption that parameter regularization techniques failfor rehearsal-free continual learning of a single, expanding task. Next, weexplore how to leverage knowledge from a pre-trained model in rehearsal-freecontinual learning and find that vanilla L2 parameter regularizationoutperforms EWC parameter regularization and feature distillation. Finally, weexplore the recently popular ImageNet-R benchmark, and show that L2 parameterregularization implemented in self-attention blocks of a ViT transformeroutperforms recent popular prompting for continual learning methods.", "output": "A Closer Look at Rehearsal-Free Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The notion of neural collapse refers to several emergent phenomena that havebeen empirically observed across various canonical classification problems.During the terminal phase of training a deep neural network, the featureembedding of all examples of the same class tend to collapse to a singlerepresentation, and the features of different classes tend to separate as muchas possible. Neural collapse is often studied through a simplified model,called the unconstrained feature representation, in which the model is assumedto have \"infinite expressivity\" and can map each data point to any arbitraryrepresentation. In this work, we propose a more realistic variant of theunconstrained feature representation that takes the limited expressivity of thenetwork into account. Empirical evidence suggests that the memorization ofnoisy data points leads to a degradation (dilation) of the neural collapse.Using a model of the memorization-dilation (M-D) phenomenon, we show onemechanism by which different losses lead to different performances of thetrained network on noisy data. Our proofs reveal why label smoothing, amodification of cross-entropy empirically observed to produce a regularizationeffect, leads to improved generalization in classification tasks.", "output": "Memorization-Dilation: Modeling Neural Collapse Under Label Noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning (DL) has big-data processing capabilities that are as good, oreven better, than those of humans in many real-world domains, but at the costof high energy requirements that may be unsustainable in some applications andof errors, that, though infrequent, can be large. We hypothesise that afundamental weakness of DL lies in its intrinsic dependence onintegrate-and-fire point neurons that maximise information transmissionirrespective of whether it is relevant in the current context or not. Thisleads to unnecessary neural firing and to the feedforward transmission ofconflicting messages, which makes learning difficult and processing energyinefficient. Here we show how to circumvent these limitations by mimicking thecapabilities of context-sensitive neocortical neurons that receive input fromdiverse sources as a context to amplify and attenuate the transmission ofrelevant and irrelevant information, respectively. We demonstrate that a deepnetwork composed of such local processors seeks to maximise agreement betweenthe active neurons, thus restricting the transmission of conflictinginformation to higher levels and reducing the neural activity required toprocess large amounts of heterogeneous real-world data. As shown to be far moreeffective and efficient than current forms of DL, this two-point neuron studyoffers a possible step-change in transforming the cellular foundations of deepnetwork architectures.", "output": "Context-sensitive neocortical neurons transform the effectiveness and efficiency of neural information processing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently proposed BERT-based evaluation metrics for text generation performwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,relating to information correctness. We argue that this stems (in part) fromthe fact that they are models of semantic similarity. In contrast, we developevaluation metrics based on Natural Language Inference (NLI), which we deem amore appropriate modeling. We design a preference-based adversarial attackframework and show that our NLI based metrics are much more robust to theattacks than the recent BERT-based metrics. On standard benchmarks, our NLIbased metrics outperform existing summarization metrics, but perform below SOTAMT metrics. However, when combining existing metrics with our NLI metrics, weobtain both higher adversarial robustness (15%-30%) and higher quality metricsas measured on standard benchmarks (+5% to 30%).", "output": "MENLI: Robust Evaluation Metrics from Natural Language Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lottery tickets (LTs) is able to discover accurate and sparse subnetworksthat could be trained in isolation to match the performance of dense networks.Ensemble, in parallel, is one of the oldest time-proven tricks in machinelearning to improve performance by combining the output of multiple independentmodels. However, the benefits of ensemble in the context of LTs will be dilutedsince ensemble does not directly lead to stronger sparse subnetworks, butleverages their predictions for a better decision. In this work, we firstobserve that directly averaging the weights of the adjacent learned subnetworkssignificantly boosts the performance of LTs. Encouraged by this observation, wefurther propose an alternative way to perform an 'ensemble' over thesubnetworks identified by iterative magnitude pruning via a simpleinterpolating strategy. We call our method Lottery Pools. In contrast to thenaive ensemble which brings no performance gains to each single subnetwork,Lottery Pools yields much stronger sparse subnetworks than the original LTswithout requiring any extra training or inference cost. Across various modernarchitectures on CIFAR-10/100 and ImageNet, we show that our method achievessignificant performance gains in both, in-distribution and out-of-distributionscenarios. Impressively, evaluated with VGG-16 and ResNet-18, the producedsparse subnetworks outperform the original LTs by up to 1.88% on CIFAR-100 and2.36% on CIFAR-100-C; the resulting dense network surpasses the pre-traineddense-model up to 2.22% on CIFAR-100 and 2.38% on CIFAR-100-C.", "output": "Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Electrical Distribution Systems are extensively penetrated with theDistributed Energy Resources (DERs) to cater the energy demands with generalperception that it enhances the system resiliency. However, it may be adversefor the grid operation due to various factors like its intermittentavailability, dynamics in weather condition, introduction of nonlinearity,complexity etc. This needs a detailed understanding of system resiliency thatour method proposes here. We introduce a methodology using complex networktheory to identify the resiliency of distribution system when incorporated withSolar PV generation under various undesirable configurations. Complexcorrelated networks for different conditions were obtained and various networkparameters were computed for identifying the resiliency of those networks. Theproposed methodology identifies the hosting capacity of solar panels in thesystem while maintaining the resiliency under different unwanted conditionshence helps to obtain an optimal allocation topology for solar panels in thesystem. The proposed method also identifies the critical nodes that are highlysensitive to the changes and could drive the system into non-resiliency. Thisframework was demonstrated on IEEE-123 Test Feeder system with time-series datagenerated using GridLAB-D and variety of analysis were performed using complexnetwork and machine learning models.", "output": "A methodology for identifying resiliency in renewable electrical distribution system using complex network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training deep neural networks (DNNs) is an important and challengingoptimization problem in machine learning due to its non-convexity andnon-separable structure. The alternating minimization (AM) approaches split thecomposition structure of DNNs and have drawn great interest in the deeplearning and optimization communities. In this paper, we propose a unifiedframework for analyzing the convergence rate of AM-type network trainingmethods. Our analysis is based on the non-monotone $j$-step sufficient decreaseconditions and the Kurdyka-Lojasiewicz (KL) property, which relaxes therequirement of designing descent algorithms. We show the detailed localconvergence rate if the KL exponent $theta$ varies in $[0,1)$. Moreover, thelocal R-linear convergence is discussed under a stronger $j$-step sufficientdecrease condition.", "output": "Convergence Rates of Training Deep Neural Networks via Alternating Minimization Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning fine-grained movements is a challenging topic in robotics,particularly in the context of robotic hands. One specific instance of thischallenge is the acquisition of fingerspelling sign language in robots. In thispaper, we propose an approach for learning dexterous motor imitation from videoexamples without additional information. To achieve this, we first build a URDFmodel of a robotic hand with a single actuator for each joint. We then leveragepre-trained deep vision models to extract the 3D pose of the hand from RGBvideos. Next, using state-of-the-art reinforcement learning algorithms formotion imitation (namely, proximal policy optimization and soft actor-critic),we train a policy to reproduce the movement extracted from the demonstrations.We identify the optimal set of hyperparameters for imitation based on areference motion. Finally, we demonstrate the generalizability of our approachby testing it on six different tasks, corresponding to fingerspelled letters.Our results show that our approach is able to successfully imitate thesefine-grained movements without additional information, highlighting itspotential for real-world applications in robotics.", "output": "Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Collision avoidance in the presence of dynamic obstacles in unknownenvironments is one of the most critical challenges for unmanned systems. Inthis paper, we present a method that identifies obstacles in terms ofellipsoids to estimate linear and angular obstacle velocities. Our proposedmethod is based on the idea of any object can be approximately expressed byellipsoids. To achieve this, we propose a method based on variational Bayesianestimation of Gaussian mixture model, the Kyachiyan algorithm, and a refinementalgorithm. Our proposed method does not require knowledge of the number ofclusters and can operate in real-time, unlike existing optimization-basedmethods. In addition, we define an ellipsoid-based feature vector to matchobstacles given two timely close point frames. Our method can be applied to anyenvironment with static and dynamic obstacles, including the ones with rotatingobstacles. We compare our algorithm with other clustering methods and show thatwhen coupled with a trajectory planner, the overall system can efficientlytraverse unknown environments in the presence of dynamic obstacles.", "output": "Obstacle Identification and Ellipsoidal Decomposition for Fast Motion Planning in Unknown Dynamic Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) has emerged as a means of distributed learning usinglocal data stored at clients with a coordinating server. Recent studies showedthat FL can suffer from poor performance and slower convergence when trainingdata at clients are not independent and identically distributed. Here weconsider a new complementary approach to mitigating this performancedegradation by allowing the server to perform auxiliary learning from a smalldataset. Our analysis and experiments show that this new approach can achievesignificant improvements in both model accuracy and convergence time even whenthe server dataset is small and its distribution differs from that of theaggregated data from all clients.", "output": "Federated Learning with Server Learning: Enhancing Performance for Non-IID Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wasserstein distributionally robust optimization (textsf{WDRO}) is a popularmodel to enhance the robustness of machine learning with ambiguous data.However, the complexity of textsf{WDRO} can be prohibitive in practice sincesolving its ``minimax'' formulation requires a great amount of computation.Recently, several fast textsf{WDRO} training algorithms for some specificmachine learning tasks (e.g., logistic regression) have been developed.However, the research on designing efficient algorithms for general large-scaletextsf{WDRO}s is still quite limited, to the best of our knowledge.textit{Coreset} is an important tool for compressing large dataset, and thusit has been widely applied to reduce the computational complexities for manyoptimization problems. In this paper, we introduce a unified framework toconstruct the $epsilon$-coreset for the general textsf{WDRO} problems. Thoughit is challenging to obtain a conventional coreset for textsf{WDRO} due to theuncertainty issue of ambiguous data, we show that we can compute a ``dualcoreset'' by using the strong duality property of textsf{WDRO}. Also, theerror introduced by the dual coreset can be theoretically guaranteed for theoriginal textsf{WDRO} objective. To construct the dual coreset, we propose anovel grid sampling approach that is particularly suitable for the dualformulation of textsf{WDRO}. Finally, we implement our coreset approach andillustrate its effectiveness for several textsf{WDRO} problems in theexperiments.", "output": "Coresets for Wasserstein Distributionally Robust Optimization Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Analytical explorations on complex networks and cubes (i.e.,multi-dimensional datasets) are currently two separate research fields withdifferent strategies. To gain more insights into cube dynamics via uniquenetwork-domain methodologies and to obtain abundant synthetic networks, we needa transformation approach from cubes into associated networks. To this end, wepropose FGM, a fast generic model converting cubes into interrelated networks,whereby samples are remodeled into nodes and network dynamics are guided underthe concept of nearest-neighbor searching. Through comparison with previousmodels, we show that FGM can cost-efficiently generate networks exhibitingtypical patterns more closely aligned to factual networks, such as moreauthentic degree distribution, power-law average nearest-neighbor degreedependency, and the influence decay phenomenon we consider vital for networks.Furthermore, we evaluate the networks that FGM generates through various cubes.Results show that FGM is resilient to input perturbations, producing networkswith consistent fine properties.", "output": "From Cubes to Networks: Fast Generic Model for Synthetic Networks Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent advent of play-to-earn (P2E) systems in massively multiplayeronline role-playing games (MMORPGs) has made in-game goods interchangeable withreal-world values more than ever before. The goods in the P2E MMORPGs can bedirectly exchanged with cryptocurrencies such as Bitcoin, Ethereum, or Klaytnvia blockchain networks. Unlike traditional in-game goods, once they had beenwritten to the blockchains, P2E goods cannot be restored by the game operationteams even with chargeback fraud such as payment fraud, cancellation, orrefund. To tackle the problem, we propose a novel chargeback fraud predictionmethod, PU GNN, which leverages graph attention networks with PU loss tocapture both the players' in-game behavior with P2E token transaction patterns.With the adoption of modified GraphSMOTE, the proposed model handles theimbalanced distribution of labels in chargeback fraud datasets. The conductedexperiments on three real-world P2E MMORPG datasets demonstrate that PU GNNachieves superior performances over previously suggested methods.", "output": "PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a novel model called GAMMT (Generative Ambiguity Models usingMultiple Transformers) for sequential data that is based on sets ofprobabilities. Unlike conventional models, our approach acknowledges that thedata generation process of a sequence is not deterministic, but ratherambiguous and influenced by a set of probabilities. To capture this ambiguity,GAMMT employs multiple parallel transformers that are linked by a selectionmechanism, allowing for the approximation of ambiguous probabilities. Thegenerative nature of our approach also enables multiple representations ofinput tokens and sequences. While our models have not yet undergoneexperimental validation, we believe that our model has great potential toachieve high quality and diversity in modeling sequences with uncertain datageneration processes.", "output": "GAMMT: Generative Ambiguity Modeling Using Multiple Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Algorithmic fairness plays an increasingly critical role in machine learningresearch. Several group fairness notions and algorithms have been proposed.However, the fairness guarantee of existing fair classification methods mainlydepends on specific data distributional assumptions, often requiring largesample sizes, and fairness could be violated when there is a modest number ofsamples, which is often the case in practice. In this paper, we propose FaiREE,a fair classification algorithm that can satisfy group fairness constraintswith finite-sample and distribution-free theoretical guarantees. FaiREE can beadapted to satisfy various group fairness notions (e.g., Equality ofOpportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimalaccuracy. These theoretical guarantees are further supported by experiments onboth synthetic and real data. FaiREE is shown to have favorable performanceover state-of-the-art algorithms.", "output": "FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While natural systems often present collective intelligence that allows themto self-organize and adapt to changes, the equivalent is missing in mostartificial systems. We explore the possibility of such a system in the contextof cooperative 2D push manipulations using mobile robots. Although conventionalworks demonstrate potential solutions for the problem in restricted settings,they have computational and learning difficulties. More importantly, thesesystems do not possess the ability to adapt when facing environmental changes.In this work, we show that by distilling a planner derived from adifferentiable soft-body physics simulator into an attention-based neuralnetwork, our multi-robot push manipulation system achieves better performancethan baselines. In addition, our system also generalizes to configurations notseen during training and is able to adapt toward task completions when externalturbulence and environmental changes are applied. Supplementary videos can befound on our project website:url{", "output": "Collective Intelligence for 2D Push Manipulation with Mobile Robots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Any quantum computing application, once encoded as a quantum circuit, must becompiled before being executable on a quantum computer. Similar to classicalcompilation, quantum compilation is a sequential process with many compilationsteps and numerous possible optimization passes. Despite the similarities, thedevelopment of compilers for quantum computing is still in its infancy --lacking mutual consolidation on the best sequence of passes, compatibility,adaptability, and flexibility. In this work, we take advantage of decades ofclassical compiler optimization and propose a reinforcement learning frameworkfor developing optimized quantum circuit compilation flows. Through distinctconstraints and a unifying interface, the framework supports the combination oftechniques from different compilers and optimization tools in a singlecompilation flow. Experimental evaluations show that the proposed framework --set up with a selection of compilation passes from IBM's Qiskit andQuantinuum's TKET -- significantly outperforms both individual compilers in 73%of cases regarding the expected fidelity. The framework is available on GitHub( as part of the Munich Quantum Toolkit(MQT).", "output": "Compiler Optimization for Quantum Computing Using Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a new self-supervised method for pre-training the backbone of deepperception models operating on point clouds. The core idea is to train themodel on a pretext task which is the reconstruction of the surface on which the3D points are sampled, and to use the underlying latent vectors as input to theperception head. The intuition is that if the network is able to reconstructthe scene surface, given only sparse input points, then it probably alsocaptures some fragments of semantic information, that can be used to boost anactual perception task. This principle has a very simple formulation, whichmakes it both easy to implement and widely applicable to a large range of 3Dsensors and deep networks performing semantic segmentation or object detection.In fact, it supports a single-stream pipeline, as opposed to most contrastivelearning approaches, allowing training on limited resources. We conductedextensive experiments on various autonomous driving datasets, involving verydifferent kinds of lidars, for both semantic segmentation and object detection.The results show the effectiveness of our method to learn usefulrepresentations without any annotation, compared to existing approaches. Codeis available at ", "output": "ALSO: Automotive Lidar Self-supervision by Occupancy estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce ensembles of stochastic neural networks to approximate theBayesian posterior, combining stochastic methods such as dropout with deepensembles. The stochastic ensembles are formulated as families of distributionsand trained to approximate the Bayesian posterior with variational inference.We implement stochastic ensembles based on Monte Carlo dropout, DropConnect anda novel non-parametric version of dropout and evaluate them on a toy problemand CIFAR image classification. For both tasks, we test the quality of theposteriors directly against Hamiltonian Monte Carlo simulations. Our resultsshow that stochastic ensembles provide more accurate posterior estimates thanother popular baselines for Bayesian inference.", "output": "Bayesian posterior approximation with stochastic ensembles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Test-time adaptation (TTA) is the problem of updating a pre-trained sourcemodel at inference time given test input(s) from a different target domain.Most existing TTA approaches assume the setting in which the target domain isstationary, i.e., all the test inputs come from a single target domain.However, in many practical settings, the test input distribution might exhibita lifelong/continual shift over time. Moreover, existing TTA approaches alsolack the ability to provide reliable uncertainty estimates, which is crucialwhen distribution shifts occur between the source and target domain. To addressthese issues, we present PETAL (Probabilistic lifElong Test-time Adaptationwith seLf-training prior), which solves lifelong TTA using a probabilisticapproach, and naturally results in (1) a student-teacher framework, where theteacher model is an exponential moving average of the student model, and (2)regularizing the model updates at inference time using the source model as aregularizer. To prevent model drift in the lifelong/continual TTA setting, wealso propose a data-driven parameter restoration technique which contributes toreducing the error accumulation and maintaining the knowledge of recent domainsby restoring only the irrelevant parameters. In terms of predictive error rateas well as uncertainty based metrics such as Brier score and negativelog-likelihood, our method achieves better results than the currentstate-of-the-art for online lifelong test-time adaptation across variousbenchmarks, such as CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCCdatasets. The source code for our approach is accessible at", "output": "A Probabilistic Framework for Lifelong Test-Time Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hierarchical methods in reinforcement learning have the potential to reducethe amount of decisions that the agent needs to perform when learning newtasks. However, finding reusable useful temporal abstractions that facilitatefast learning remains a challenging problem. Recently, several deep learningapproaches were proposed to learn such temporal abstractions in the form ofoptions in an end-to-end manner. In this work, we point out severalshortcomings of these methods and discuss their potential negativeconsequences. Subsequently, we formulate the desiderata for reusable optionsand use these to frame the problem of learning options as a gradient-basedmeta-learning problem. This allows us to formulate an objective that explicitlyincentivizes options which allow a higher-level decision maker to adjust in fewsteps to different tasks. Experimentally, we show that our method is able tolearn transferable components which accelerate learning and performs betterthan existing prior methods developed for this setting. Additionally, weperform ablations to quantify the impact of using gradient-based meta-learningas well as other proposed changes.", "output": "Reusable Options through Gradient-based Meta Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With more and more data being collected, data-driven modeling methods havebeen gaining in popularity in recent years. While physically sound, classicalgray-box models are often cumbersome to identify and scale, and their accuracymight be hindered by their limited expressiveness. On the other hand, classicalblack-box methods, typically relying on Neural Networks (NNs) nowadays, oftenachieve impressive performance, even at scale, by deriving statistical patternsfrom data. However, they remain completely oblivious to the underlying physicallaws, which may lead to potentially catastrophic failures if decisions forreal-world physical systems are based on them. Physically Consistent NeuralNetworks (PCNNs) were recently developed to address these aforementionedissues, ensuring physical consistency while still leveraging NNs to attainstate-of-the-art accuracy.In this work, we scale PCNNs to model building temperature dynamics andpropose a thorough comparison with classical gray-box and black-box methods.More precisely, we design three distinct PCNN extensions, thereby exemplifyingthe modularity and flexibility of the architecture, and formally prove theirphysical consistency. In the presented case study, PCNNs are shown to achievestate-of-the-art accuracy, even outperforming classical NN-based models despitetheir constrained structure. Our investigations furthermore provide a clearillustration of NNs achieving seemingly good performance while remainingcompletely physics-agnostic, which can be misleading in practice. While thisperformance comes at the cost of computational complexity, PCNNs on the otherhand show accuracy improvements of 17-35% compared to all other physicallyconsistent methods, paving the way for scalable physically consistent modelswith state-of-the-art performance.", "output": "Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To interpret deep neural networks, one main approach is to dissect the visualinput and find the prototypical parts responsible for the classification.However, existing methods often ignore the hierarchical relationship betweenthese prototypes, and thus can not explain semantic concepts at both higherlevel (e.g., water sports) and lower level (e.g., swimming). In this paperinspired by human cognition system, we leverage hierarchal information to dealwith uncertainty: When we observe water and human activity, but no definitiveaction it can be recognized as the water sports parent class. Only afterobserving a person swimming can we definitively refine it to the swimmingaction. To this end, we propose HIerarchical Prototype Explainer (HIPE) tobuild hierarchical relations between prototypes and classes. HIPE enables areasoning process for video action classification by dissecting the input videoframes on multiple levels of the class hierarchy, our method is also applicableto other video tasks. The faithfulness of our method is verified by reducingaccuracy-explainability trade off on ActivityNet and UCF-101 while providingmulti-level explanations.", "output": "Hierarchical Explanations for Video Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The prominent role of social media in people's daily lives has made them moreinclined to receive news through social networks than traditional sources. Thisshift in public behavior has opened doors for some to diffuse fake news onsocial media; and subsequently cause negative economic, political, and socialconsequences as well as distrust among the public.There are many proposed methods to solve the rumor detection problem, most ofwhich do not take full advantage of the heterogeneous nature of newspropagation networks. With this intention, we considered a previously proposedarchitecture as our baseline and performed the idea of structural featureextraction from the heterogeneous rumor propagation over its architecture usingthe concept of meta path-based embeddings. We named our model Meta Path-basedGlobal Local Attention Network (MGLAN). Extensive experimental analysis onthree state-of-the-art datasets has demonstrated that MGLAN outperforms othermodels by capturing node-level discrimination to different node types.", "output": "A Meta Path-based Approach for Rumor Detection on Social Media."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) has gained increasing interest in recent years as adistributed on-device learning paradigm. However, multiple challenges remain tobe addressed for deploying FL in real-world Internet-of-Things (IoT) networkswith hierarchies. Although existing works have proposed various approaches toaccount data heterogeneity, system heterogeneity, unexpected stragglers andscalibility, none of them provides a systematic solution to address all of thechallenges in a hierarchical and unreliable IoT network. In this paper, wepropose an asynchronous and hierarchical framework (Async-HFL) for performingFL in a common three-tier IoT network architecture. In response to the largelyvaried delays, Async-HFL employs asynchronous aggregations at both the gatewayand the cloud levels thus avoids long waiting time. To fully unleash thepotential of Async-HFL in converging speed under system heterogeneities andstragglers, we design device selection at the gateway level and device-gatewayassociation at the cloud level. Device selection chooses edge devices totrigger local training in real-time while device-gateway association determinesthe network topology periodically after several cloud epochs, both satisfyingbandwidth limitation. We evaluate Async-HFL's convergence speedup usinglarge-scale simulations based on ns-3 and a network topology from NYCMesh. Ourresults show that Async-HFL converges 1.08-1.31x faster in wall-clock time andsaves up to 21.6% total communication cost compared to state-of-the-artasynchronous FL algorithms (with client selection). We further validateAsync-HFL on a physical deployment and observe robust convergence underunexpected stragglers.", "output": "Async-HFL: Efficient and Robust Asynchronous Federated Learning in Hierarchical IoT Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "D-Adaptation is an approach to automatically setting the learning rate whichasymptotically achieves the optimal rate of convergence for minimizing convexLipschitz functions, with no back-tracking or line searches, and no additionalfunction value or gradient evaluations per step. Our approach is the firsthyper-parameter free method for this class without additional multiplicativelog factors in the convergence rate. We present extensive experiments for SGDand Adam variants of our method, where the method automatically matcheshand-tuned learning rates across more than a dozen diverse machine learningproblems, including large-scale vision and language problems.An open-source implementation is available aturl{", "output": "Learning-Rate-Free Learning by D-Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-world data generation often involves complex inter-dependencies amonginstances, violating the IID-data hypothesis of standard learning paradigms andposing a challenge for uncovering the geometric structures for learning desiredinstance representations. To this end, we introduce an energy constraineddiffusion model which encodes a batch of instances from a dataset intoevolutionary states that progressively incorporate other instances' informationby their interactions. The diffusion process is constrained by descent criteriaw.r.t.~a principled energy function that characterizes the global consistencyof instance representations over latent structures. We provide rigorous theorythat implies closed-form optimal estimates for the pairwise diffusion strengthamong arbitrary instance pairs, which gives rise to a new class of neuralencoders, dubbed as DIFFormer (diffusion-based Transformers), with twoinstantiations: a simple version with linear complexity for prohibitiveinstance numbers, and an advanced version for learning complex structures.Experiments highlight the wide applicability of our model as a general-purposeencoder backbone with superior performance in various tasks, such as nodeclassification on large graphs, semi-supervised image/text classification, andspatial-temporal dynamics prediction.", "output": "DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language Models (LMs) have been shown to leak information about training datathrough sentence-level membership inference and reconstruction attacks.Understanding the risk of LMs leaking Personally Identifiable Information (PII)has received less attention, which can be attributed to the false assumptionthat dataset curation techniques such as scrubbing are sufficient to preventPII leakage. Scrubbing techniques reduce but do not prevent the risk of PIIleakage: in practice scrubbing is imperfect and must balance the trade-offbetween minimizing disclosure and preserving the utility of the dataset. On theother hand, it is unclear to which extent algorithmic defenses such asdifferential privacy, designed to guarantee sentence- or user-level privacy,prevent PII disclosure. In this work, we introduce rigorous game-baseddefinitions for three types of PII leakage via black-box extraction, inference,and reconstruction attacks with only API access to an LM. We empiricallyevaluate the attacks against GPT-2 models fine-tuned with and without defenseson three domains: case law, health care, and e-mails. Our main contributionsare (i) novel attacks that can extract up to 10$times$ more PII sequences thanexisting attacks, (ii) showing that sentence-level differential privacy reducesthe risk of PII disclosure but still leaks about 3% of PII sequences, and (iii)a subtle connection between record-level membership inference and PIIreconstruction.", "output": "Analyzing Leakage of Personally Identifiable Information in Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graphs consisting of vocal nodes (\"the vocal minority\") and silent nodes(\"the silent majority\"), namely VS-Graph, are ubiquitous in the real world. Thevocal nodes tend to have abundant features and labels. In contrast, silentnodes only have incomplete features and rare labels, e.g., the description andpolitical tendency of politicians (vocal) are abundant while not for ordinarypeople (silent) on the twitter's social network. Predicting the silent majorityremains a crucial yet challenging problem. However, most existingmessage-passing based GNNs assume that all nodes belong to the same domain,without considering the missing features and distribution-shift betweendomains, leading to poor ability to deal with VS-Graph. To combat the abovechallenges, we propose Knowledge Transferable Graph Neural Network (KT-GNN),which models distribution shifts during message passing and representationlearning by transferring knowledge from vocal nodes to silent nodes.Specifically, we design the domain-adapted \"feature completion and messagepassing mechanism\" for node representation learning while preserving domaindifference. And a knowledge transferable classifier based on KL-divergence isfollowed. Comprehensive experiments on real-world scenarios (i.e., companyfinancial risk assessment and political elections) demonstrate the superiorperformance of our method. Our source code has been open sourced.", "output": "Predicting the Silent Majority on Graphs: Knowledge Transferable Graph Neural Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models have been demonstrated to be valuable in differentfields. ChatGPT, developed by OpenAI, has been trained using massive amounts ofdata and simulates human conversation by comprehending context and generatingappropriate responses. It has garnered significant attention due to its abilityto effectively answer a broad range of human inquiries, with fluent andcomprehensive answers surpassing prior public chatbots in both security andusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,which is the focus of this study. Eleven categories of failures, includingreasoning, factual errors, math, coding, and bias, are presented and discussed.The risks, limitations, and societal implications of ChatGPT are alsohighlighted. The goal of this study is to assist researchers and developers inenhancing future language models and chatbots.", "output": "A Categorical Archive of ChatGPT Failures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent approaches build on implicit neural representations (INRs) to proposegenerative models over function spaces. However, they are computationallycostly when dealing with inference tasks, such as missing data imputation, ordirectly cannot tackle them. In this work, we propose a novel deep generativemodel, named VAMoH. VAMoH combines the capabilities of modeling continuousfunctions using INRs and the inference capabilities of Variational Autoencoders(VAEs). In addition, VAMoH relies on a normalizing flow to define the prior,and a mixture of hypernetworks to parametrize the data log-likelihood. Thisgives VAMoH a high expressive capability and interpretability. Throughexperiments on a diverse range of data types, such as images, voxels, andclimate data, we show that VAMoH can effectively learn rich distributions overcontinuous functions. Furthermore, it can perform inference-related tasks, suchas conditional super-resolution generation and in-painting, as well or betterthan previous approaches, while being less computationally demanding.", "output": "Variational Mixture of HyperGenerators for Learning Distributions Over Functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When performing causal discovery, assumptions have to be made on how the truecausal mechanism corresponds to the underlying joint probability distribution.These assumptions are labeled as causal razors in this work. We review numerouscausal razors that appeared in the literature, and offer a comprehensivelogical comparison of them. In particular, we scrutinize an unpopular causalrazor, namely parameter minimality, in multinomial causal models and itslogical relations with other well-studied causal razors. Our logical resultposes a dilemma in selecting a reasonable scoring criterion for score-basedcasual search algorithms.", "output": "Causal Razors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We revisit the original approach of using deep learning and neural networksto solve differential equations by incorporating the knowledge of the equation.This is done by adding a dedicated term to the loss function during theoptimization procedure in the training process. The so-called physics-informedneural networks (PINNs) are tested on a variety of academic ordinarydifferential equations in order to highlight the benefits and drawbacks of thisapproach with respect to standard integration methods. We focus on thepossibility to use the least possible amount of data into the training process.The principles of PINNs for solving differential equations by enforcingphysical laws via penalizing terms are reviewed. A tutorial on a simpleequation model illustrates how to put into practice the method for ordinarydifferential equations. Benchmark tests show that a very small amount oftraining data is sufficient to predict the solution when the non linearity ofthe problem is weak. However, this is not the case in strongly non linearproblems where a priori knowledge of training data over some partial or thewhole time integration interval is necessary.", "output": "Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Designing realistic digital humans is extremely complex. Most data-drivengenerative models used to simplify the creation of their underlying geometricshape do not offer control over the generation of local shape attributes. Inthis paper, we overcome this limitation by introducing a novel loss functiongrounded in spectral geometry and applicable to different neural-network-basedgenerative models of 3D head and body meshes. Encouraging the latent variablesof mesh variational autoencoders (VAEs) or generative adversarial networks(GANs) to follow the local eigenprojections of identity attributes, we improvelatent disentanglement and properly decouple the attribute creation.Experimental results show that our local eigenprojection disentangled (LED)models not only offer improved disentanglement with respect to thestate-of-the-art, but also maintain good generation capabilities with trainingtimes comparable to the vanilla implementations of the models.", "output": "3D Generative Model Latent Disentanglement via Local Eigenprojection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in learning-based control leverage deep functionapproximators, such as neural networks, to model the evolution of controlleddynamical systems over time. However, the problem of learning a dynamics modeland a stabilizing controller persists, since the synthesis of a stabilizingfeedback law for known nonlinear systems is a difficult task, let alone forcomplex parametric representations that must be fit to data. To this end, wepropose Control with Inherent Lyapunov Stability (CoILS), a method for jointlylearning parametric representations of a nonlinear dynamics model and astabilizing controller from data. To do this, our approach simultaneouslylearns a parametric Lyapunov function which intrinsically constrains thedynamics model to be stabilizable by the learned controller. In addition to thestabilizability of the learned dynamics guaranteed by our novel construction,we show that the learned controller stabilizes the true dynamics under certainassumptions on the fidelity of the learned dynamics. Finally, we demonstratethe efficacy of CoILS on a variety of simulated nonlinear dynamical systems.", "output": "Data-Driven Control with Inherent Lyapunov Stability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have demonstrated their significant potential tobe applied for addressing various application tasks. However, traditionalrecommender systems continue to face great challenges such as poorinteractivity and explainability, which actually also hinder their broaddeployment in real-world systems. To address these limitations, this paperproposes a novel paradigm called Chat-Rec (ChatGPT Augmented RecommenderSystem) that innovatively augments LLMs for building conversational recommendersystems by converting user profiles and historical interactions into prompts.Chat-Rec is demonstrated to be effective in learning user preferences andestablishing connections between users and products through in-contextlearning, which also makes the recommendation process more interactive andexplainable. What's more, within the Chat-Rec framework, user's preferences cantransfer to different products for cross-domain recommendations, andprompt-based injection of information into LLMs can also handle the cold-startscenarios with new items. In our experiments, Chat-Rec effectively improve theresults of top-k recommendations and performs better in zero-shot ratingprediction task. Chat-Rec offers a novel approach to improving recommendersystems and presents new practical scenarios for the implementation of AIGC (AIgenerated content) in recommender system studies.", "output": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study multiclass online prediction where the learner can predict using alist of multiple labels (as opposed to just one label in the traditionalsetting). We characterize learnability in this model using the $b$-aryLittlestone dimension. This dimension is a variation of the classicalLittlestone dimension with the difference that binary mistake trees arereplaced with $(k+1)$-ary mistake trees, where $k$ is the number of labels inthe list. In the agnostic setting, we explore different scenarios depending onwhether the comparator class consists of single-labeled or multi-labeledfunctions and its tradeoff with the size of the lists the algorithm uses. Wefind that it is possible to achieve negative regret in some cases and provide acomplete characterization of when this is possible. As part of our work, weadapt classical algorithms such as Littlestone's SOA and Rosenblatt'sPerceptron to predict using lists of labels. We also establish combinatorialresults for list-learnable classes, including an list online version of theSauer-Shelah-Perles Lemma. We state our results within the framework of patternclasses -- a generalization of hypothesis classes which can represent adaptivehypotheses (i.e. functions with memory), and model data-dependent assumptionssuch as linear classification with margin.", "output": "List Online Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Researchers often represent relations in multi-variate correlational datausing Gaussian graphical models, which require regularization to sparsify themodels. Acknowledging that they often study the modular structure of theinferred network, we suggest integrating it in the cross-validation of theregularization strength to balance under- and overfitting. Using synthetic andreal data, we show that this approach allows us to better recover and infermodular structure in noisy data compared with the graphical lasso, a standardapproach using the Gaussian log-likelihood when cross-validating theregularization strength.", "output": "Module-based regularization improves Gaussian graphical models when observing noisy data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given a graph with a subset of labeled nodes, we are interested in thequality of the averaging estimator which for an unlabeled node predicts theaverage of the observations of its labeled neighbors. We rigorously studyconcentration properties, variance bounds and risk bounds in this context.While the estimator itself is very simple we believe that our results willcontribute towards the theoretical understanding of learning on graphs throughmore sophisticated methods such as Graph Neural Networks.", "output": "The Graphical Nadaraya-Watson Estimator on Latent Position Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Supervised learning of image classifiers distills human knowledge into aparametric model through pairs of images and corresponding labels (X,Y). Weargue that this simple and widely used representation of human knowledgeneglects rich auxiliary information from the annotation procedure, such as thetime-series of mouse traces and clicks left after image selection. Our insightis that such annotation byproducts Z provide approximate human attention thatweakly guides the model to focus on the foreground cues, reducing spuriouscorrelations and discouraging shortcut learning. To verify this, we createImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched withsample-wise annotation byproducts, collected by replicating the respectiveoriginal annotation tasks. We refer to the new paradigm of training models withannotation byproducts as learning using annotation byproducts (LUAB). We showthat a simple multitask loss for regressing Z together with Y already improvesthe generalisability and robustness of the learned models. Compared to theoriginal supervised learning, LUAB does not require extra annotation costs.ImageNet-AB and COCO-AB are at ", "output": "Neglected Free Lunch; Learning Image Classifiers Using Annotation Byproducts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we introduce the range of oBERTa language models, aneasy-to-use set of language models which allows Natural Language Processing(NLP) practitioners to obtain between 3.8 and 24.3 times faster models withoutexpertise in model compression. Specifically, oBERTa extends existing work onpruning, knowledge distillation, and quantization and leverages frozenembeddings improves distillation and model initialization to deliver higheraccuracy on a broad range of transfer tasks. In generating oBERTa, we explorehow the highly optimized RoBERTa differs from the BERT for pruning duringpre-training and finetuning. We find it less amenable to compression duringfine-tuning. We explore the use of oBERTa on seven representative NLP tasks andfind that the improved compression techniques allow a pruned oBERTa model tomatch the performance of BERTbase and exceed the performance of Prune OFA Largeon the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,respectively faster in inference. We release our code, training regimes, andassociated model for broad usage to encourage usage and experimentation", "output": "oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A backdoor attack allows a malicious user to manipulate the environment orcorrupt the training data, thus inserting a backdoor into the trained agent.Such attacks compromise the RL system's reliability, leading to potentiallycatastrophic results in various key fields. In contrast, relatively limitedresearch has investigated effective defenses against backdoor attacks in RL.This paper proposes the Recovery Triggered States (RTS) method, a novelapproach that effectively protects the victim agents from backdoor attacks. RTSinvolves building a surrogate network to approximate the dynamics model.Developers can then recover the environment from the triggered state to a cleanstate, thereby preventing attackers from activating backdoors hidden in theagent by presenting the trigger. When training the surrogate to predict states,we incorporate agent action information to reduce the discrepancy between theactions taken by the agent on predicted states and the actions taken on realstates. RTS is the first approach to defend against backdoor attacks in asingle-agent setting. Our results show that using RTS, the cumulative rewardonly decreased by 1.41% under the backdoor attack.", "output": "Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We combine concepts from multilevel solvers for partial differentialequations (PDEs) with neural network based deep learning and propose a newmethodology for the efficient numerical solution of high-dimensional parametricPDEs. An in-depth theoretical analysis shows that the proposed architecture isable to approximate multigrid V-cycles to arbitrary precision with the numberof weights only depending logarithmically on the resolution of the finest mesh.As a consequence, approximation bounds for the solution of parametric PDEs byneural networks that are independent on the (stochastic) parameter dimensioncan be derived. The performance of the proposed method is illustrated onhigh-dimensional parametric linear elliptic PDEs that are common benchmarkproblems in uncertainty quantification. We find substantial improvements overstate-of-the-art deep learning-based solvers. As particularly challengingexamples, random conductivity with high-dimensional non-affine Gaussian fieldsin 100 parameter dimensions and a random cookie problem are examined. Due tothe multilevel structure of our method, the amount of training samples can bereduced on finer levels, hence significantly lowering the generation time fortraining data and the training time of our method.", "output": "Multilevel CNNs for Parametric PDEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The dynamics of systems of many degrees of freedom evolving on multiplescales are often modeled in terms of stochastic differential equations. Usuallythe structural form of these equations is unknown and the only manifestation ofthe system's dynamics are observations at discrete points in time. Despitetheir widespread use, accurately inferring these systems from sparse-in-timeobservations remains challenging. Conventional inference methods either focuson the temporal structure of observations, neglecting the geometry of thesystem's invariant density, or use geometric approximations of the invariantdensity, which are limited to conservative driving forces. To address theselimitations, here, we introduce a novel approach that reconciles these twoperspectives. We propose a path augmentation scheme that employs data-drivencontrol to account for the geometry of the invariant system's density.Non-parametric inference on the augmented paths, enables efficientidentification of the underlying deterministic forces of systems observed atlow sampling rates.", "output": "Geometric constraints improve inference of sparsely observed stochastic dynamics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Action understanding matters and attracts attention. It can be formed as themapping from the action physical space to the semantic space. Typically,researchers built action datasets according to idiosyncratic choices to defineclasses and push the envelope of benchmarks respectively. Thus, datasets areincompatible with each other like \"Isolated Islands\" due to semantic gaps andvarious class granularities, e.g., do housework in dataset A and wash plate indataset B. We argue that a more principled semantic space is an urgent need toconcentrate the community efforts and enable us to use all datasets together topursue generalizable action learning. To this end, we design a Poincare actionsemantic space given verb taxonomy hierarchy and covering massive actions. Byaligning the classes of previous datasets to our semantic space, we gather(image/video/skeleton/MoCap) datasets into a unified database in a unifiedlabel system, i.e., bridging \"isolated islands\" into a \"Pangea\". Accordingly,we propose a bidirectional mapping model between physical and semantic space tofully use Pangea. In extensive experiments, our system shows significantsuperiority, especially in transfer learning. Code and data will be madepublicly available.", "output": "From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the finite horizon optimal steering of the joint stateprobability distribution subject to the angular velocity dynamics governed bythe Euler equation. The problem and its solution amounts to controlling thespin of a rigid body via feedback, and is of practical importance, for example,in angular stabilization of a spacecraft with stochastic initial and terminalstates. We clarify how this problem is an instance of the optimal masstransport (OMT) problem with bilinear prior drift. We deduce both static anddynamic versions of the Eulerian OMT, and provide analytical and numericalresults for the synthesis of the optimal controller.", "output": "Optimal Mass Transport over the Euler Equation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Throughout schooling, students are tested on reading comprehension andlogical reasoning. Students have developed various strategies for completingsuch exams, some of which are generally thought to outperform others. One suchstrategy involves emphasizing relative accuracy over absolute accuracy and cantheoretically produce the correct answer without full knowledge of theinformation required to solve the question. This paper examines theeffectiveness of applying such a strategy to train transfer learning models tosolve reading comprehension and logical reasoning questions. The models wereevaluated on the ReClor dataset, a challenging reading comprehension andlogical reasoning benchmark. While previous studies targeted logical reasoningskills, we focus on a general training method and model architecture. Wepropose the polytuplet loss function, an extension of the triplet lossfunction, to ensure prioritization of learning the relative correctness ofanswer choices over learning the true accuracy of each choice. Our resultsindicate that models employing polytuplet loss outperform existing baselinemodels. Although polytuplet loss is a promising alternative to othercontrastive loss functions, further research is required to quantify thebenefits it may present.", "output": "Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models."}]