[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the past few years, Differentiable Neural Architecture Search (DNAS)rapidly imposed itself as the trending approach to automate the discovery ofdeep neural network architectures. This rise is mainly due to the popularity ofDARTS, one of the first major DNAS methods. In contrast with previous worksbased on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster byseveral orders of magnitude and uses fewer computational resources. In thiscomprehensive survey, we focus specifically on DNAS and review recentapproaches in this field. Furthermore, we propose a novel challenge-basedtaxonomy to classify DNAS methods. We also discuss the contributions brought toDNAS in the past few years and its impact on the global NAS field. Finally, weconclude by giving some insights into future research directions for the DNASfield.", "output": "Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Materials possessing flexible physico-chemical properties that adapton-demand to the hostile environmental conditions of deep space will becomeessential in defining the future of space exploration. A promising venue forinspiration towards the design of environment-specific materials is in theintricate micro-architectures and lattice geometry found throughout nature.However, the immense design space covered by such irregular topologies ischallenging to probe analytically. For this reason, most synthetic latticematerials have to date been based on periodic architectures instead. Here, wepropose a computational approach using a graph representation for both regularand irregular lattice materials. Our method uses differentiable message passingalgorithms to calculate mechanical properties, and therefore allows usingautomatic differentiation to adjust both the geometric structure and attributesof individual lattice elements to design materials with desired properties. Theintroduced methodology is applicable to any system representable as aheterogeneous graph, including other types of materials.", "output": "Differentiable graph-structured models for inverse design of lattice materials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of temporal relation extraction is to infer the temporal relationbetween two events in the document. Supervised models are dominant in thistask. In this work, we investigate ChatGPT's ability on zero-shot temporalrelation extraction. We designed three different prompt techniques to breakdown the task and evaluate ChatGPT. Our experiments show that ChatGPT'sperformance has a large gap with that of supervised methods and can heavilyrely on the design of prompts. We further demonstrate that ChatGPT can infermore small relation classes correctly than supervised methods. The currentshortcomings of ChatGPT on temporal relation extraction are also discussed inthis paper. We found that ChatGPT cannot keep consistency during temporalinference and it fails in actively long-dependency temporal inference.", "output": "Zero-shot Temporal Relation Extraction with ChatGPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Physically-based rendering (PBR) is key for immersive rendering effects usedwidely in the industry to showcase detailed realistic scenes from computergraphics assets. A well-known caveat is that producing the same iscomputationally heavy and relies on complex capture devices. Inspired by thesuccess in quality and efficiency of recent volumetric neural rendering, wewant to develop a physically-based neural shader to eliminate device dependencyand significantly boost performance. However, no existing lighting and materialmodels in the current neural rendering approaches can accurately represent thecomprehensive lighting models and BRDFs properties required by the PBR process.Thus, this paper proposes a novel lighting representation that models directand indirect light locally through a light sampling strategy in a learned lightsampling field. We also propose BRDF models to separately representsurface/subsurface scattering details to enable complex objects such astranslucent material (i.e., skin, jade). We then implement our proposedrepresentations with an end-to-end physically-based neural face skin shader,which takes a standard face asset (i.e., geometry, albedo map, and normal map)and an HDRI for illumination as inputs and generates a photo-realisticrendering as output. Extensive experiments showcase the quality and efficiencyof our PBR face skin shader, indicating the effectiveness of our proposedlighting and material representations.", "output": "Light Sampling Field and BRDF Representation for Physically-based Neural Rendering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the user targeting and expanding of new shows on a video platform, the keypoint is how their embeddings are generated. It's supposed to be personalizedfrom the perspective of both users and shows. Furthermore, the pursue of bothinstant (click) and long-time (view time) rewards, and the cold-start problemfor new shows bring additional challenges. Such a problem is suitable forprocessing by heterogeneous graph models, because of the natural graphstructure of data. But real-world networks usually have billions of nodes andvarious types of edges. Few existing methods focus on handling large-scale dataand exploiting different types of edges, especially the latter. In this paper,we propose a two-stage audience expansion scheme based on an edge-promptedheterogeneous graph network which can take different double-sided interactionsand features into account. In the offline stage, to construct the graph, userIDs and specific side information combinations of the shows are chosen to bethe nodes, and click/co-click relations and view time are used to build theedges. Embeddings and clustered user groups are then calculated. When new showsarrive, their embeddings and subsequent matching users can be produced within aconsistent space. In the online stage, posterior data including click/viewusers are employed as seeds to look for similar users. The results on thepublic datasets and our billion-scale data demonstrate the accuracy andefficiency of our approach.", "output": "Audience Expansion for Multi-show Release Based on an Edge-prompted Heterogeneous Graph Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning causal relationships solely from observational data providesinsufficient information about the underlying causal mechanism and the searchspace of possible causal graphs. As a result, often the search space can growexponentially for approaches such as Greedy Equivalence Search (GES) that usesa score-based approach to search the space of equivalence classes of graphs.Prior causal information such as the presence or absence of a causal edge canbe leveraged to guide the discovery process towards a more restricted andaccurate search space. In this study, we present KGS, a knowledge-guided greedyscore-based causal discovery approach that uses observational data andstructural priors (causal edges) as constraints to learn the causal graph. KGSis a novel application of knowledge constraints that can leverage any of thefollowing prior edge information between any two variables: the presence of adirected edge, the absence of an edge, and the presence of an undirected edge.We extensively evaluate KGS across multiple settings in both synthetic andbenchmark real-world datasets. Our experimental results demonstrate thatstructural priors of any type and amount are helpful and guide the searchprocess towards an improved performance and early convergence.", "output": "KGS: Causal Discovery Using Knowledge-guided Greedy Equivalence Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in deep learning have accelerated its use in variousapplications, such as cellular image analysis and molecular discovery. Inmolecular discovery, a generative adversarial network (GAN), which comprises adiscriminator to distinguish generated molecules from existing molecules and agenerator to generate new molecules, is one of the premier technologies due toits ability to learn from a large molecular data set efficiently and generatenovel molecules that preserve similar properties. However, differentpharmaceutical companies may be unwilling or unable to share their local datasets due to the geo-distributed and sensitive nature of molecular data sets,making it impossible to train GANs in a centralized manner. In this paper, wepropose a Graph convolutional network in Generative Adversarial Networks viaFederated learning (GraphGANFed) framework, which integrates graphconvolutional neural Network (GCN), GAN, and federated learning (FL) as a wholesystem to generate novel molecules without sharing local data sets. InGraphGANFed, the discriminator is implemented as a GCN to better capturefeatures from molecules represented as molecular graphs, and FL is used totrain both the discriminator and generator in a distributive manner to preservedata privacy. Extensive simulations are conducted based on the three bench-markdata sets to demonstrate the feasibility and effectiveness of GraphGANFed. Themolecules generated by GraphGANFed can achieve high novelty (=100) anddiversity (&gt; 0.9). The simulation results also indicate that 1) a lowercomplexity discriminator model can better avoid mode collapse for a smallerdata set, 2) there is a tradeoff among different evaluation metrics, and 3)having the right dropout ratio of the generator and discriminator can avoidmode collapse.", "output": "GraphGANFed: A Federated Generative Framework for Graph-Structured Molecules Towards Efficient Drug Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Brain-inspired hyperdimensional computing (HDC) has been recently considereda promising learning approach for resource-constrained devices. However,existing approaches use static encoders that are never updated during thelearning process. Consequently, it requires a very high dimensionality toachieve adequate accuracy, severely lowering the encoding and trainingefficiency. In this paper, we propose DistHD, a novel dynamic encodingtechnique for HDC adaptive learning that effectively identifies and regeneratesdimensions that mislead the classification and compromise the learning quality.Our proposed algorithm DistHD successfully accelerates the learning process andachieves the desired accuracy with considerably lower dimensionality.", "output": "DistHD: A Learner-Aware Dynamic Encoding Method for Hyperdimensional Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) is an area of significant research interest, andsafe RL in particular is attracting attention due to its ability to handlesafety-driven constraints that are crucial for real-world applications of RLalgorithms. This work proposes a novel approach to RL training, called controlinvariant set (CIS) enhanced RL, which leverages the benefits of CIS to improvestability guarantees and sampling efficiency. The approach consists of twolearning stages: offline and online. In the offline stage, CIS is incorporatedinto the reward design, initial state sampling, and state reset procedures. Inthe online stage, RL is retrained whenever the state is outside of CIS, whichserves as a stability criterion. A backup table that utilizes the explicit formof CIS is obtained to ensure the online stability. To evaluate the proposedapproach, we apply it to a simulated chemical reactor. The results show asignificant improvement in sampling efficiency during offline training andclosed-loop stability in the online implementation.", "output": "Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a self-supervised shared encoder model that achieves strongresults on several visual, language and multimodal benchmarks while being data,memory and run-time efficient. We make three key contributions. First, incontrast to most existing works, we use a single transformer with all theencoder layers processing both the text and the image modalities. Second, wepropose a stage-wise training strategy where the model is first trained onimages, then jointly with unimodal text and image datasets and finally jointlywith text and text-image datasets. Third, to preserve information across boththe modalities, we propose a training pipeline that learns simultaneously fromgradient updates of different modalities at each training update step. Theresults on downstream text-only, image-only and multimodal tasks show that ourmodel is competitive with several strong models while using fewer parametersand lesser pre-training data. For example, MoMo performs competitively withFLAVA on multimodal (+3.1), image-only (+1.1) and text-only (-0.1) tasksdespite having 2/5th the number of parameters and using 1/3rd the image-texttraining pairs. Finally, we ablate various design choices and further show thatincreasing model size produces significant performance gains indicatingpotential for substantial improvements with larger models using our approach.", "output": "MoMo: A shared encoder Model for text, image and multi-Modal representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integration of heterogeneous and high-dimensional multi-omics data isbecoming increasingly important in understanding genetic data. Each omicstechnique only provides a limited view of the underlying biological process andintegrating heterogeneous omics layers simultaneously would lead to a morecomprehensive and detailed understanding of diseases and phenotypes. However,one obstacle faced when performing multi-omics data integration is theexistence of unpaired multi-omics data due to instrument sensitivity and cost.Studies may fail if certain aspects of the subjects are missing or incomplete.In this paper, we propose a deep learning method for multi-omics integrationwith incomplete data by Cross-omics Linked unified embedding with ContrastiveLearning and Self Attention (CLCLSA). Utilizing complete multi-omics data assupervision, the model employs cross-omics autoencoders to learn the featurerepresentation across different types of biological data. The multi-omicscontrastive learning, which is used to maximize the mutual information betweendifferent types of omics, is employed before latent feature concatenation. Inaddition, the feature-level self-attention and omics-level self-attention areemployed to dynamically identify the most informative features for multi-omicsdata integration. Extensive experiments were conducted on four publicmulti-omics datasets. The experimental results indicated that the proposedCLCLSA outperformed the state-of-the-art approaches for multi-omics dataclassification using incomplete multi-omics data.", "output": "CLCLSA: Cross-omics Linked embedding with Contrastive Learning and Self Attention for multi-omics integration with incomplete multi-omics data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic neural network is an emerging research topic in deep learning. Withadaptive inference, dynamic models can achieve remarkable accuracy andcomputational efficiency. However, it is challenging to design a powerfuldynamic detector, because of no suitable dynamic architecture and exitingcriterion for object detection. To tackle these difficulties, we propose adynamic framework for object detection, named DynamicDet. Firstly, we carefullydesign a dynamic architecture based on the nature of the object detection task.Then, we propose an adaptive router to analyze the multi-scale information andto decide the inference route automatically. We also present a noveloptimization strategy with an exiting criterion based on the detection lossesfor our dynamic detectors. Last, we present a variable-speed inferencestrategy, which helps to realize a wide range of accuracy-speed trade-offs withonly one dynamic detector. Extensive experiments conducted on the COCObenchmark demonstrate that the proposed DynamicDet achieves newstate-of-the-art accuracy-speed trade-offs. For instance, with comparableaccuracy, the inference speed of our dynamic detector Dy-YOLOv7-W6 surpassesYOLOv7-E6 by 12%, YOLOv7-D6 by 17%, and YOLOv7-E6E by 39%. The code isavailable at ", "output": "DynamicDet: A Unified Dynamic Architecture for Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent researches on unsupervised person re-identification~(reID) havedemonstrated that pre-training on unlabeled person images achieves superiorperformance on downstream reID tasks than pre-training on ImageNet. However,those pre-trained methods are specifically designed for reID and sufferflexible adaption to other pedestrian analysis tasks. In this paper, we proposeVAL-PAT, a novel framework that learns transferable representations to enhancevarious pedestrian analysis tasks with multimodal information. To train ourframework, we introduce three learning objectives, emph{i.e.,} self-supervisedcontrastive learning, image-text contrastive learning and multi-attributeclassification. The self-supervised contrastive learning facilitates thelearning of the intrinsic pedestrian properties, while the image-textcontrastive learning guides the model to focus on the appearance information ofpedestrians.Meanwhile, multi-attribute classification encourages the model torecognize attributes to excavate fine-grained pedestrian information. We firstperform pre-training on LUPerson-TA dataset, where each image contains text andattribute annotations, and then transfer the learned representations to variousdownstream tasks, including person reID, person attribute recognition andtext-based person search. Extensive experiments demonstrate that our frameworkfacilitates the learning of general pedestrian representations and thus leadsto promising results on various pedestrian analysis tasks.", "output": "Learning Transferable Pedestrian Representation from Multimodal Information Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene text editing is a challenging task that involves modifying or insertingspecified texts in an image while maintaining its natural and realisticappearance. Most previous approaches to this task rely on style-transfer modelsthat crop out text regions and feed them into image transfer models, such asGANs. However, these methods are limited in their ability to change text styleand are unable to insert texts into images. Recent advances in diffusion modelshave shown promise in overcoming these limitations with text-conditional imageediting. However, our empirical analysis reveals that state-of-the-artdiffusion models struggle with rendering correct text and controlling textstyle. To address these problems, we propose DIFFSTE to improve pre-traineddiffusion models with a dual encoder design, which includes a character encoderfor better text legibility and an instruction encoder for better style control.An instruction tuning framework is introduced to train our model to learn themapping from the text instruction to the corresponding image with either thespecified style or the style of the surrounding texts in the background. Such atraining method further brings our method the zero-shot generalization abilityto the following three scenarios: generating text with unseen font variation,e.g., italic and bold, mixing different fonts to construct a new font, andusing more relaxed forms of natural language as the instructions to guide thegeneration task. We evaluate our approach on five datasets and demonstrate itssuperior performance in terms of text correctness, image naturalness, and stylecontrollability. Our code is publicly available.", "output": "Improving Diffusion Models for Scene Text Editing with Dual Encoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dialogue Acts (DAs) can be used to explain what expert tutors do and whatstudents know during the tutoring process. Most empirical studies adopt therandom sampling method to obtain sentence samples for manual annotation of DAs,which are then used to train DA classifiers. However, these studies have paidlittle attention to sample informativeness, which can reflect the informationquantity of the selected samples and inform the extent to which a classifiercan learn patterns. Notably, the informativeness level may vary among thesamples and the classifier might only need a small amount of low informativesamples to learn the patterns. Random sampling may overlook sampleinformativeness, which consumes human labelling costs and contributes less totraining the classifiers. As an alternative, researchers suggest employingstatistical sampling methods of Active Learning (AL) to identify theinformative samples for training the classifiers. However, the use of ALmethods in educational DA classification tasks is under-explored. In thispaper, we examine the informativeness of annotated sentence samples. Then, thestudy investigates how the AL methods can select informative samples to supportDA classifiers in the AL sampling process. The results reveal that mostannotated sentences present low informativeness in the training dataset and thepatterns of these sentences can be easily captured by the DA classifier. Wealso demonstrate how AL methods can reduce the cost of manual annotation in theAL sampling process.", "output": "Does Informativeness Matter? Active Learning for Educational Dialogue Act Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study evaluates the potential of a large language model for aiding ingeneration of semantic feature norms - a critical tool for evaluatingconceptual structure in cognitive science. Building from an existinghuman-generated dataset, we show that machine-verified norms capture aspects ofconceptual structure beyond what is expressed in human norms alone, and betterexplain human judgments of semantic similarity amongst items that are distallyrelated. The results suggest that LLMs can greatly enhance traditional methodsof semantic feature norm verification, with implications for our understandingof conceptual representation in humans and machines.", "output": "Semantic Feature Verification in FLAN-T5."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the last few years, large language models (LLMs) have emerged as themost important breakthroughs in natural language processing (NLP) thatfundamentally transform research and developments in the field. ChatGPTrepresents one of the most exciting LLM systems developed recently to showcaseimpressive skills for language generation and highly attract public attention.Among various exciting applications discovered for ChatGPT in English, themodel can process and generate texts for multiple languages due to itsmultilingual training data. Given the broad adoption of ChatGPT for English indifferent problems and areas, a natural question is whether ChatGPT can also beapplied effectively for other languages or it is necessary to develop morelanguage-specific technologies. The answer to this question requires a thoroughevaluation of ChatGPT over multiple tasks with diverse languages and largedatasets (i.e., beyond reported anecdotes), which is still missing or limitedin current research. Our work aims to fill this gap for the evaluation ofChatGPT and similar LLMs to provide more comprehensive information formultilingual NLP applications. While this work will be an ongoing effort toinclude additional experiments in the future, our current paper evaluatesChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,low, and extremely low resources. We also focus on the zero-shot learningsetting for ChatGPT to improve reproducibility and better simulate theinteractions of general users. Compared to the performance of previous models,our extensive experimental results demonstrate a worse performance of ChatGPTfor different NLP tasks and languages, calling for further research to developbetter models and understanding for multilingual learning.", "output": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, multi-interest models, which extract interests of a user asmultiple representation vectors, have shown promising performances forsequential recommendation. However, none of existing multi-interestrecommendation models consider the Out-Of-Distribution (OOD) generalizationproblem, in which interest distribution may change. Considering multipleinterests of a user are usually highly correlated, the model has chance tolearn spurious correlations between noisy interests and target items. Once thedata distribution changes, the correlations among interests may also change,and the spurious correlations will mislead the model to make wrong predictions.To tackle with above OOD generalization problem, we propose a novelmulti-interest network, named DEep Stable Multi-Interest Learning (DESMIL),which attempts to de-correlate the extracted interests in the model, and thusspurious correlations can be eliminated. DESMIL applies an attentive module toextract multiple interests, and then selects the most important one for makingfinal predictions. Meanwhile, DESMIL incorporates a weighted correlationestimation loss based on Hilbert-Schmidt Independence Criterion (HSIC), withwhich training samples are weighted, to minimize the correlations amongextracted interests. Extensive experiments have been conducted under both OODand random settings, and up to 36.8% and 21.7% relative improvements areachieved respectively.", "output": "Deep Stable Multi-Interest Learning for Out-of-distribution Sequential Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spiking neural networks (SNNs) are well known as the brain-inspired modelswith high computing efficiency, due to a key component that they utilize spikesas information units, close to the biological neural systems. Although spikingbased models are energy efficient by taking advantage of discrete spikesignals, their performance is limited by current network structures and theirtraining methods. As discrete signals, typical SNNs cannot apply the gradientdescent rules directly into parameters adjustment as artificial neural networks(ANNs). Aiming at this limitation, here we propose a novel method ofconstructing deep SNN models with knowledge distillation (KD) that uses ANN asteacher model and SNN as student model. Through ANN-SNN joint trainingalgorithm, the student SNN model can learn rich feature information from theteacher ANN model through the KD method, yet it avoids training SNN fromscratch when communicating with non-differentiable spikes. Our method can notonly build a more efficient deep spiking structure feasibly and reasonably, butuse few time steps to train whole model compared to direct training or ANN toSNN methods. More importantly, it has a superb ability of noise immunity forvarious types of artificial noises and natural signals. The proposed novelmethod provides efficient ways to improve the performance of SNN throughconstructing deeper structures in a high-throughput fashion, with potentialusage for light and efficient brain-inspired computing of practical scenarios.", "output": "Constructing Deep Spiking Neural Networks from Artificial Neural Networks with Knowledge Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern multi-agent reinforcement learning (RL) algorithms hold greatpotential for solving a variety of real-world problems. However, they do notfully exploit cross-agent knowledge to reduce sample complexity and improveperformance. Although transfer RL supports knowledge sharing, it ishyperparameter sensitive and complex. To solve this problem, we propose a novelmulti-agent policy reciprocity (PR) framework, where each agent can fullyexploit cross-agent policies even in mismatched states. We then define anadjacency space for mismatched states and design a plug-and-play module forvalue iteration, which enables agents to infer more precise returns. To improvethe scalability of PR, deep PR is proposed for continuous control tasks.Moreover, theoretical analysis shows that agents can asymptotically reachconsensus through individual perceived rewards and converge to an optimal valuefunction, which implies the stability and effectiveness of PR, respectively.Experimental results on discrete and continuous environments demonstrate thatPR outperforms various existing RL and transfer RL methods.", "output": "Multi-agent Policy Reciprocity with Theoretical Guarantee."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies how to keep a vision backbone effective while removingtoken mixers in its basic building blocks. Token mixers, as self-attention forvision transformers (ViTs), are intended to perform information communicationbetween different spatial tokens but suffer from considerable computationalcost and latency. However, directly removing them will lead to an incompletemodel structure prior, and thus brings a significant accuracy drop. To thisend, we first develop an RepIdentityFormer base on the re-parameterizing idea,to study the token mixer free model architecture. And we then explore theimproved learning paradigm to break the limitation of simple token mixer freebackbone, and summarize the empirical practice into 5 guidelines. Equipped withthe proposed optimization strategy, we are able to build an extremely simplevision backbone with encouraging performance, while enjoying the highefficiency during inference. Extensive experiments and ablative analysis alsodemonstrate that the inductive bias of network architecture, can beincorporated into simple network structure with appropriate optimizationstrategy. We hope this work can serve as a starting point for the explorationof optimization-driven efficient network design. Project page:", "output": "RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a deep learning method for accurately localizing the center of asingle corneal reflection (CR) in an eye image. Unlike previous approaches, weuse a convolutional neural network (CNN) that was trained solely usingsimulated data. Using only simulated data has the benefit of completelysidestepping the time-consuming process of manual annotation that is requiredfor supervised training on real eye images. To systematically evaluate theaccuracy of our method, we first tested it on images with simulated CRs placedon different backgrounds and embedded in varying levels of noise. Second, wetested the method on high-quality videos captured from real eyes. Our methodoutperformed state-of-the-art algorithmic methods on real eye images with a 35%reduction in terms of spatial precision, and performed on par withstate-of-the-art on simulated images in terms of spatial accuracy.We concludethat our method provides a precise method for CR center localization andprovides a solution to the data availability problem which is one of theimportant common roadblocks in the development of deep learning models for gazeestimation. Due to the superior CR center localization and ease of application,our method has the potential to improve the accuracy and precision of CR-basedeye trackers", "output": "Precise localization of corneal reflections in eye images using deep learning trained on synthetic data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Finding an efficient way to adapt robot trajectory is a priority to improveoverall performance of robots. One approach for trajectory planning is throughtransferring human-like skills to robots by Learning from Demonstrations (LfD).The human demonstration is considered the target motion to mimic. However,human motion is typically optimal for human embodiment but not for robotsbecause of the differences between human biomechanics and robot dynamics. TheDynamic Movement Primitives (DMP) framework is a viable solution for thislimitation of LfD, but it requires tuning the second-order dynamics in theformulation. Our contribution is introducing a systematic method to extract thedynamic features from human demonstration to auto-tune the parameters in theDMP framework. In addition to its use with LfD, another utility of the proposedmethod is that it can readily be used in conjunction with ReinforcementLearning (RL) for robot training. In this way, the extracted featuresfacilitate the transfer of human skills by allowing the robot to explore thepossible trajectories more efficiently and increasing robot compliancesignificantly. We introduced a methodology to extract the dynamic features frommultiple trajectories based on the optimization of human-likeness andsimilarity in the parametric space. Our method was implemented into an actualhuman-robot setup to extract human dynamic features and used to regenerate therobot trajectories following both LfD and RL with DMP. It resulted in a stableperformance of the robot, maintaining a high degree of human-likeness based onaccumulated distance error as good as the best heuristic tuning.", "output": "Human-Robot Skill Transfer with Enhanced Compliance via Dynamic Movement Primitives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable AI has become a popular tool for validating machine learningmodels. Mismatches between the explained model's decision strategy and theuser's domain knowledge (e.g. Clever Hans effects) have also been recognized asa starting point for improving faulty models. However, it is less clear what todo when the user and the explanation agree. In this paper, we demonstrate thatacceptance of explanations by the user is not a guarantee for a ML model tofunction well, in particular, some Clever Hans effects may remain undetected.Such hidden flaws of the model can nevertheless be mitigated, and wedemonstrate this by contributing a new method, Explanation-Guided ExposureMinimization (EGEM), that premptively prunes variations in the ML model thathave not been the subject of positive explanation feedback. Experiments onnatural image data demonstrate that our approach leads to models that stronglyreduce their reliance on hidden Clever Hans strategies, and consequentlyachieve higher accuracy on new data.", "output": "Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As data-driven intelligent systems advance, the need for reliable andtransparent decision-making mechanisms has become increasingly important.Therefore, it is essential to integrate uncertainty quantification and modelexplainability approaches to foster trustworthy business and operationalprocess analytics. This study explores how model uncertainty can be effectivelycommunicated in global and local post-hoc explanation approaches, such asPartial Dependence Plots (PDP) and Individual Conditional Expectation (ICE)plots. In addition, this study examines appropriate visualization analyticsapproaches to facilitate such methodological integration. By combining thesetwo research directions, decision-makers can not only justify the plausibilityof explanation-driven actionable insights but also validate their reliability.Finally, the study includes expert interviews to assess the suitability of theproposed approach and designed interface for a real-world predictive processmonitoring problem in the manufacturing domain.", "output": "Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study focuses on long-term forecasting (LTF) on continuous-time dynamicgraph networks (CTDGNs), which is important for real-world modeling. ExistingCTDGNs are effective for modeling temporal graph data due to their ability tocapture complex temporal dependencies but perform poorly on LTF due to thesubstantial requirement for historical data, which is not practical in mostcases. To relieve this problem, a most intuitive way is data augmentation. Inthis study, we propose textbf{underline{U}ncertainty underline{M}askedunderline{M}ixunderline{U}p (UmmU)}: a plug-and-play module that conductsuncertainty estimation to introduce uncertainty into the embedding ofintermediate layer of CTDGNs, and perform masked mixup to further enhance theuncertainty of the embedding to make it generalize to more situations. UmmU canbe easily inserted into arbitrary CTDGNs without increasing the number ofparameters. We conduct comprehensive experiments on three real-world dynamicgraph datasets, the results demonstrate that UmmU can effectively improve thelong-term forecasting performance for CTDGNs.", "output": "Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face anti-spoofing (FAS) is an essential mechanism for safeguarding theintegrity of automated face recognition systems. Despite substantialadvancements, the generalization of existing approaches to real-worldapplications remains challenging. This limitation can be attributed to thescarcity and lack of diversity in publicly available FAS datasets, which oftenleads to overfitting during training or saturation during testing. In terms ofquantity, the number of spoof subjects is a critical determinant. Most datasetscomprise fewer than 2,000 subjects. With regard to diversity, the majority ofdatasets consist of spoof samples collected in controlled environments usingrepetitive, mechanical processes. This data collection methodology results inhomogenized samples and a dearth of scenario diversity. To address theseshortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, alarge-scale, diverse FAS dataset collected in unconstrained settings. Ourdataset encompasses 853,729 images of 321,751 spoof subjects and 529,571 imagesof 148,169 live subjects, representing a substantial increase in quantity.Moreover, our dataset incorporates spoof data obtained from the internet,spanning a wide array of scenarios and various commercial sensors, including 17presentation attacks (PAs) that encompass both 2D and 3D forms. This novel datacollection strategy markedly enhances FAS data diversity. Leveraging the WFASdataset and Protocol 1 (Known-Type), we host the Wild Face Anti-SpoofingChallenge at the CVPR2023 workshop. Additionally, we meticulously evaluaterepresentative methods using Protocol 1 and Protocol 2 (Unknown-Type). Throughan in-depth examination of the challenge outcomes and benchmark baselines, weprovide insightful analyses and propose potential avenues for future research.The dataset is released under Insightface.", "output": "Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a recent trend for using the novel Artificial Intelligence ChatGPTchatbox, which provides detailed responses and articulate answers across manydomains of knowledge. However, in many cases it returns plausible-sounding butincorrect or inaccurate responses, whereas it does not provide evidence.Therefore, any user has to further search for checking the accuracy of theanswer or/and for finding more information about the entities of the response.At the same time there is a high proliferation of RDF Knowledge Graphs (KGs)over any real domain, that offer high quality structured data. For enabling thecombination of ChatGPT and RDF KGs, we present a research prototype, calledGPToLODS, which is able to enrich any ChatGPT response with more informationfrom hundreds of RDF KGs. In particular, it identifies and annotates eachentity of the response with statistics and hyperlinks to LODsyndesis KG (whichcontains integrated data from 400 RDF KGs and over 412 million entities). Inthis way, it is feasible to enrich the content of entities and to perform factchecking and validation for the facts of the response at real time.", "output": "Using Multiple RDF Knowledge Graphs for Enriching ChatGPT Responses."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series classification (TSC) is a challenging task due to the diversityof types of feature that may be relevant for different classification tasks,including trends, variance, frequency, magnitude, and various patterns. Toaddress this challenge, several alternative classes of approach have beendeveloped, including similarity-based, features and intervals, shapelets,dictionary, kernel, neural network, and hybrid approaches. While kernel, neuralnetwork, and hybrid approaches perform well overall, some specializedapproaches are better suited for specific tasks. In this paper, we propose anew similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), whichoutperforms previous state-of-the-art similarity-based classifiers across theUCR benchmark and outperforms state-of-the-art kernel, neural network, andhybrid methods on specific datasets in the benchmark that are best addressed bysimilarity-base methods. PF 2.0 incorporates three recent advances in timeseries similarity measures -- (1) computationally efficient early abandoningand pruning to speedup elastic similarity computations; (2) a new elasticsimilarity measure, Amerced Dynamic Time Warping (ADTW); and (3) cost functiontuning. It rationalizes the set of similarity measures employed, reducing theeight base measures of the original PF to three and using the first derivativetransform with all similarity measures, rather than a limited subset. We haveimplemented both PF 1.0 and PF 2.0 in a single C++ framework, making the PFframework more efficient.", "output": "Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In our study, we apply different statistical models to nowcast quarterly GDPgrowth for the US economy. Using the monthly FRED-MD database, we compare thenowcasting performance of the dynamic factor model (DFM) and four artificialneural networks (ANNs): the multilayer perceptron (MLP), the one-dimensionalconvolutional neural network (1D CNN), the long short-term memory network(LSTM), and the gated recurrent unit (GRU). The empirical analysis presents theresults from two distinctively different evaluation periods. The first (2010:Q1-- 2019:Q4) is characterized by balanced economic growth, while the second(2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. Accordingto our results, longer input sequences result in more accurate nowcasts inperiods of balanced economic growth. However, this effect ceases above arelatively low threshold value of around six quarters (eighteen months). Duringperiods of economic turbulence (e.g., during the COVID-19 recession), longertraining sequences do not help the models' predictive performance; instead,they seem to weaken their generalization capability. Our results show that 1DCNN, with the same parameters, generates accurate nowcasts in both of ourevaluation periods. Consequently, first in the literature, we propose the useof this specific neural network architecture for economic nowcasting.", "output": "GDP nowcasting with artificial neural networks: How much does long-term memory matter?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-Driven Response Regime Exploration and Identification (DR$^2$EI) is anovel and fully data-driven method for identifying and classifying responseregimes of a dynamical system without requiring human intervention. Thisapproach is a valuable tool for exploring and discovering response regimes incomplex dynamical systems, especially when the governing equations and thenumber of response regimes are unknown, and the system is expensive to sample.Additionally, the method is useful for order reduction, as it can be used toidentify the most dominant response regimes of a given dynamical system.DR$^2$EI utilizes unsupervised learning algorithms to transform the system'sresponse into an embedding space that facilitates regime classification. Anactive sequential sampling approach based on Gaussian Process Regression (GPR)is used to efficiently sample the parameter space, quantify uncertainty, andprovide optimal trade-offs between exploration and exploitation. Theperformance of the DR$^2$EI method was evaluated by analyzing three establisheddynamical systems: the mathematical pendulum, the Lorenz system, and theDuffing oscillator. The method was shown to effectively identify a variety ofresponse regimes with both similar and distinct topological features andfrequency content, demonstrating its versatility in capturing a wide range ofbehaviors. While it may not be possible to guarantee that all possible regimeswill be identified, the method provides an automated and efficient means forexploring the parameter space of a dynamical system and identifying itsunderlying \"sufficiently dominant\" response regimes without prior knowledge ofthe system's equations or behavior.", "output": "Data-Driven Response Regime Exploration and Identification for Dynamical Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation assigns category labels to each pixel in an image,enabling breakthroughs in fields such as autonomous driving and robotics. DeepNeural Networks have achieved high accuracies in semantic segmentation butrequire large training datasets. Some domains have difficulties building suchdatasets due to rarity, privacy concerns, and the need for skilled annotators.Few-Shot Learning (FSL) has emerged as a new research stream that allows modelsto learn new tasks from a few samples. This contribution provides an overviewof FSL in semantic segmentation (FSS), proposes a new taxonomy, and describescurrent limitations and outlooks.", "output": "Few Shot Semantic Segmentation: a review of methodologies and open challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In federated learning, benign participants aim to optimize a global modelcollaboratively. However, the risk of textit{privacy leakage} cannot beignored in the presence of textit{semi-honest} adversaries. Existing researchhas focused either on designing protection mechanisms or on inventing attackingmechanisms. While the battle between defenders and attackers seemsnever-ending, we are concerned with one critical question: is it possible toprevent potential attacks in advance? To address this, we propose the firstgame-theoretic framework that considers both FL defenders and attackers interms of their respective payoffs, which include computational costs, FL modelutilities, and privacy leakage risks. We name this game the Federated LearningSecurity Game (FLSG), in which neither defenders nor attackers are aware of allparticipants' payoffs.To handle the textit{incomplete information} inherent in this situation, wepropose associating the FLSG with an textit{oracle} that has two primaryresponsibilities. First, the oracle provides lower and upper bounds of thepayoffs for the players. Second, the oracle acts as a correlation device,privately providing suggested actions to each player. With this novelframework, we analyze the optimal strategies of defenders and attackers.Furthermore, we derive and demonstrate conditions under which the attacker, asa rational decision-maker, should always follow the oracle's suggestiontextit{not to attack}.", "output": "A Game-theoretic Framework for Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present new Recurrent Neural Network (RNN) cells for image classificationusing a Neural Architecture Search (NAS) approach called DARTS. We areinterested in the ReNet architecture, which is a RNN based approach presentedas an alternative for convolutional and pooling steps. ReNet can be definedusing any standard RNN cells, such as LSTM and GRU. One limitation is thatstandard RNN cells were designed for one dimensional sequential data and notfor two dimensions like it is the case for image classification. We overcomethis limitation by using DARTS to find new cell designs. We compare our resultswith ReNet that uses GRU and LSTM cells. Our found cells outperform thestandard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicategeneralizability, as we derived the RNN cell designs from CIFAR-10 withoutperforming a new cell search for SVHN.", "output": "DartsReNet: Exploring new RNN cells in ReNet architectures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpretability of AI models allows for user safety checks to build trust inthese models. In particular, decision trees (DTs) provide a global view on thelearned model and clearly outlines the role of the features that are criticalto classify a given data. However, interpretability is hindered if the DT istoo large. To learn compact trees, a Reinforcement Learning (RL) framework hasbeen recently proposed to explore the space of DTs. A given supervisedclassification task is modeled as a Markov decision problem (MDP) and thenaugmented with additional actions that gather information about the features,equivalent to building a DT. By appropriately penalizing these actions, the RLagent learns to optimally trade-off size and performance of a DT. However, todo so, this RL agent has to solve a partially observable MDP. The maincontribution of this paper is to prove that it is sufficient to solve a fullyobservable problem to learn a DT optimizing the interpretability-performancetrade-off. As such any planning or RL algorithm can be used. We demonstrate theeffectiveness of this approach on a set of classical supervised classificationdatasets and compare our approach with other interpretability-performanceoptimizing methods.", "output": "Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is desirable to predict the behavior of traffic participants conditionedon different planned trajectories of the autonomous vehicle. This allows thedownstream planner to estimate the impact of its decisions. Recent approachesfor conditional behavior prediction rely on a regression decoder, meaning thatcoordinates or polynomial coefficients are regressed. In this work we revisitset-based trajectory prediction, where the probability of each trajectory in apredefined trajectory set is determined by a classification model, andfirst-time employ it to the task of conditional behavior prediction. We proposeRESET, which combines a new metric-driven algorithm for trajectory setgeneration with a graph-based encoder. For unconditional prediction, RESETachieves comparable performance to a regression-based approach. Due to thenature of set-based approaches, it has the advantageous property of being ableto predict a flexible number of trajectories without influencing runtime orcomplexity. For conditional prediction, RESET achieves reasonable results withlate fusion of the planned trajectory, which was not observed forregression-based approaches before. This means that RESET is computationallylightweight to combine with a planner that proposes multiple future plans ofthe autonomous vehicle, as large parts of the forward pass can be reused.", "output": "RESET: Revisiting Trajectory Sets for Conditional Behavior Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Homographs, words with the same spelling but different meanings, remainchallenging in Neural Machine Translation (NMT). While recent works leveragevarious word embedding approaches to differentiate word sense in NMT, they donot focus on the pivotal components in resolving ambiguities of homographs inNMT: the hidden states of an encoder. In this paper, we propose a novelapproach to tackle homographic issues of NMT in the latent space. We firsttrain an encoder (aka \"HDR-encoder\") to learn universal sentencerepresentations in a natural language inference (NLI) task. We furtherfine-tune the encoder using homograph-based synset sentences from WordNet,enabling it to learn word-level homographic disambiguation representations(HDR). The pre-trained HDR-encoder is subsequently integrated with atransformer-based NMT in various schemes to improve translation accuracy.Experiments on four translation directions demonstrate the effectiveness of theproposed method in enhancing the performance of NMT systems in the BLEU scores(up to +2.3 compared to a solid baseline). The effects can be verified by othermetrics (F1, precision, and recall) of translation accuracy in an additionaldisambiguation task. Visualization methods like heatmaps, T-SNE and translationexamples are also utilized to demonstrate the effects of the proposed method.", "output": "Learning Homographic Disambiguation Representation for Neural Machine Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of approaches for trajectory prediction requires metrics tovalidate and compare their performance. Currently established metrics are basedon Euclidean distance, which means that errors are weighted equally in alldirections. Euclidean metrics are insufficient for structured environments likeroads, since they do not properly capture the agent's intent relative to theunderlying lane. In order to provide a reasonable assessment of trajectoryprediction approaches with regard to the downstream planning task, we propose anew metric that is lane distance-based: Lane Miss Rate (LMR). For thecalculation of LMR, the ground-truth and predicted endpoints are assigned tolane segments, more precisely their centerlines. Measured by the distance alongthe lane segments, predictions that are within a certain threshold distance tothe ground-truth count as hits, otherwise they count as misses. LMR is thendefined as the ratio of sequences that yield a miss. Our results on threestate-of-the-art trajectory prediction models show that LMR preserves the orderof Euclidean distance-based metrics. In contrast to the Euclidean Miss Rate,qualitative results show that LMR yields misses for sequences where predictionsare located on wrong lanes. Hits on the other hand result for sequences wherepredictions are located on the correct lane. This means that LMR implicitlyweights Euclidean error relative to the lane and goes into the direction ofcapturing intents of traffic agents. The source code of LMR for Argoverse 1 ispublicly available.", "output": "LMR: Lane Distance-Based Metric for Trajectory Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Finding a balance between collaboration and competition is crucial forartificial agents in many real-world applications. We investigate this using aMulti-Agent Reinforcement Learning (MARL) setup on the back of a high-impactproblem. The accumulation and yearly growth of plastic in the ocean causeirreparable damage to many aspects of oceanic health and the marina system. Toprevent further damage, we need to find ways to reduce macroplastics from knownplastic patches in the ocean. Here we propose a Graph Neural Network (GNN)based communication mechanism that increases the agents' observation space. Inour custom environment, agents control a plastic collecting vessel. Thecommunication mechanism enables agents to develop a communication protocolusing a binary signal. While the goal of the agent collective is to clean up asmuch as possible, agents are rewarded for the individual amount ofmacroplastics collected. Hence agents have to learn to communicate effectivelywhile maintaining high individual performance. We compare our proposedcommunication mechanism with a multi-agent baseline without the ability tocommunicate. Results show communication enables collaboration and increasescollective performance significantly. This means agents have learned theimportance of communication and found a balance between collaboration andcompetition.", "output": "Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural network (GNN) models are increasingly being used for theclassification of electroencephalography (EEG) data. However, GNN-baseddiagnosis of neurological disorders, such as Alzheimer's disease (AD), remainsa relatively unexplored area of research. Previous studies have relied onfunctional connectivity methods to infer brain graph structures and used simpleGNN architectures for the diagnosis of AD. In this work, we propose a noveladaptive gated graph convolutional network (AGGCN) that can provide explainablepredictions. AGGCN adaptively learns graph structures by combiningconvolution-based node feature enhancement with a well-known correlation-basedmeasure of functional connectivity. Furthermore, the gated graph convolutioncan dynamically weigh the contribution of various spatial scales. The proposedmodel achieves high accuracy in both eyes-closed and eyes-open conditions,indicating the stability of learned representations. Finally, we demonstratethat the proposed AGGCN model generates consistent explanations of itspredictions that might be relevant for further study of AD-related alterationsof brain networks.", "output": "Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the design of sample-efficient algorithms for reinforcement learningin the presence of rich, high-dimensional observations, formalized via theBlock MDP problem. Existing algorithms suffer from either 1) computationalintractability, 2) strong statistical assumptions that are not necessarilysatisfied in practice, or 3) suboptimal sample complexity. We address theseissues by providing the first computationally efficient algorithm that attainsrate-optimal sample complexity with respect to the desired accuracy level, withminimal statistical assumptions. Our algorithm, MusIK, combines systematicexploration with representation learning based on multi-step inversekinematics, a learning objective in which the aim is to predict the learner'sown action from the current observation and observations in the (potentiallydistant) future. MusIK is simple and flexible, and can efficiently takeadvantage of general-purpose function approximation. Our analysis leveragesseveral new techniques tailored to non-optimistic exploration algorithms, whichwe anticipate will find broader use.", "output": "Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion Models (DMs) are powerful generative models that add Gaussian noiseto the data and learn to remove it. We wanted to determine which noisedistribution (Gaussian or non-Gaussian) led to better generated data in DMs.Since DMs do not work by design with non-Gaussian noise, we built a frameworkthat allows reversing a diffusion process with non-Gaussian location-scalenoise. We use that framework to show that the Gaussian distribution performsthe best over a wide range of other distributions (Laplace, Uniform, t,Generalized-Gaussian).", "output": "Diffusion models with location-scale noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the slowing down of Moore's law, augmenting complementary-metal-oxidesemiconductor (CMOS) transistors with emerging nanotechnologies (X) is becomingincreasingly important. In this paper, we demonstrate how stochastic magnetictunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combinedwith versatile Field Programmable Gate Arrays (FPGA) to design anenergy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Ourheterogeneous computer successfully performs probabilistic inference andasynchronous Boltzmann learning despite device-to-device variations in sMTJs. Acomprehensive comparison using a CMOS predictive process design kit (PDK)reveals that digital CMOS-based p-bits emulating high-quality randomness useover 10,000 transistors with the energy per generated random number beingroughly two orders of magnitude greater than the sMTJ-based p-bits thatdissipate only 2 fJ. Scaled and integrated versions of our approach cansignificantly advance probabilistic computing and its applications in variousdomains, including probabilistic machine learning, optimization, and quantumsimulation.", "output": "CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper focuses on the continuous control of the unmanned aerial vehicle(UAV) based on a deep reinforcement learning method for a large-scale 3Dcomplex environment. The purpose is to make the UAV reach any target point froma certain starting point, and the flying height and speed are variable duringnavigation. In this work, we propose a deep reinforcement learning (DRL)-basedmethod combined with human-in-the-loop, which allows the UAV to avoid obstaclesautomatically during flying. We design multiple reward functions based on therelevant domain knowledge to guide UAV navigation. The role ofhuman-in-the-loop is to dynamically change the reward function of the UAV indifferent situations to suit the obstacle avoidance of the UAV better. Weverify the success rate and average step size on urban, rural, and forestscenarios, and the experimental results show that the proposed method canreduce the training convergence time and improve the efficiency and accuracy ofnavigation tasks. The code is available on the website", "output": "UAV Obstacle Avoidance by Human-in-the-Loop Reinforcement in Arbitrary 3D Environment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning (ML) models can fail in unexpected ways in the real world,but not all model failures are equal. With finite time and resources, MLpractitioners are forced to prioritize their model debugging and improvementefforts. Through interviews with 13 ML practitioners at Apple, we found thatpractitioners construct small targeted test sets to estimate an error's nature,scope, and impact on users. We built on this insight in a case study withmachine translation models, and developed Angler, an interactive visualanalytics tool to help practitioners prioritize model improvements. In a userstudy with 7 machine translation experts, we used Angler to understandprioritization practices when the input space is infinite, and obtainingreliable signals of model quality is expensive. Our study revealed thatparticipants could form more interesting and user-focused hypotheses forprioritization by analyzing quantitative summary statistics and qualitativelyassessing data by reading sentences.", "output": "Angler: Helping Machine Translation Practitioners Prioritize Model Improvements."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider modeling a binary response variable together with a set ofcovariates for two groups under observational data. The grouping variable canbe the confounding variable (the common cause of treatment and outcome),gender, case/control, ethnicity, etc. Given the covariates and a binary latentvariable, the goal is to construct two directed acyclic graphs (DAGs), whilesharing some common parameters. The set of nodes, which represent thevariables, are the same for both groups but the directed edges between nodes,which represent the causal relationships between the variables, can bepotentially different. For each group, we also estimate the effect size foreach node. We assume that each group follows a Gaussian distribution under itsDAG. Given the parent nodes, the joint distribution of DAG is conditionallyindependent due to the Markov property of DAGs. We introduce the concept ofGaussian DAG-probit model under two groups and hence doubly Gaussian DAG-probitmodel. To estimate the skeleton of the DAGs and the model parameters, we tooksamples from the posterior distribution of doubly Gaussian DAG-probit model viaMCMC method. We validated the proposed method using a comprehensive simulationexperiment and applied it on two real datasets. Furthermore, we validated theresults of the real data analysis using well-known experimental studies to showthe value of the proposed grouping variable in the causality domain.", "output": "Bayesian Causal Inference in Doubly Gaussian DAG-probit Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A new approach called NAF (the Neural Attention Forest) for solvingregression and classification tasks under tabular training data is proposed.The main idea behind the proposed NAF model is to introduce the attentionmechanism into the random forest by assigning attention weights calculated byneural networks of a specific form to data in leaves of decision trees and tothe random forest itself in the framework of the Nadaraya-Watson kernelregression. In contrast to the available models like the attention-based randomforest, the attention weights and the Nadaraya-Watson regression arerepresented in the form of neural networks whose weights can be regarded astrainable parameters. The first part of neural networks with shared weights istrained for all trees and computes attention weights of data in leaves. Thesecond part aggregates outputs of the tree networks and aims to minimize thedifference between the random forest prediction and the truth target value froma training set. The neural network is trained in an end-to-end manner. Thecombination of the random forest and neural networks implementing the attentionmechanism forms a transformer for enhancing the forest predictions. Numericalexperiments with real datasets illustrate the proposed method. The codeimplementing the approach is publicly available.", "output": "Neural Attention Forests: Transformer-Based Forest Improvement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The scale of the global edge AI market continues to grow. The currenttechnical challenges that hinder the large-scale replication of edge AI aremainly small samples on the edge and heterogeneity of edge data. In addition,edge AI customers often have requirements for data security compliance andoffline autonomy of edge AI services. Based on the lifelong learning method inthe academic world, we formally define the problem of edge-cloud collaborativelifelong learning for the first time, and release the industry's firstopen-source edge-cloud collaborative lifelong learning. Edge-cloudcollaborative lifelong learning adapts to data heterogeneity at different edgelocations through (1) multi-task transfer learning to achieve accurateprediction of \"thousands of people and thousands of faces\"; (2) incrementalprocessing of unknown tasks, the more systems learn and the smarter systems arewith small samples, gradually realize AI engineering and automation; (3) Usethe cloud-side knowledge base to remember new situational knowledge to avoidcatastrophic forgetting; (4) The edge-cloud collaborative architecture enablesdata security compliance and edge AI services to be offline autonomy whileapplying cloud resources. This work hopes to help fundamentally solve theabove-mentioned challenges of edge-cloud collaborative machine learning.", "output": "KubeEdge-Sedna v0.3: Towards Next-Generation Automatically Customized AI Engineering Scheme."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Acquiring knowledge about object interactions and affordances can facilitatescene understanding and human-robot collaboration tasks. As humans tend to useobjects in many different ways depending on the scene and the objects'availability, learning object affordances in everyday-life scenarios is achallenging task, particularly in the presence of an open set of interactionsand objects. We address the problem of affordance categorization forclass-agnostic objects with an open set of interactions; we achieve this bylearning similarities between object interactions in an unsupervised way andthus inducing clusters of object affordances. A novel depth-informedqualitative spatial representation is proposed for the construction of ActivityGraphs (AGs), which abstract from the continuous representation ofspatio-temporal interactions in RGB-D videos. These AGs are clustered to obtaingroups of objects with similar affordances. Our experiments in a real-worldscenario demonstrate that our method learns to create object affordanceclusters with a high V-measure even in cluttered scenes. The proposed approachhandles object occlusions by capturing effectively possible interactions andwithout imposing any object or scene constraints.", "output": "Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Logic locking has been proposed to safeguard intellectual property (IP)during chip fabrication. Logic locking techniques protect hardware IP by makinga subset of combinational modules in a design dependent on a secret key that iswithheld from untrusted parties. If an incorrect secret key is used, a set ofdeterministic errors is produced in locked modules, restricting unauthorizeduse. A common target for logic locking is neural accelerators, especially asmachine-learning-as-a-service becomes more prevalent. In this work, we explorehow logic locking can be used to compromise the security of a neuralaccelerator it protects. Specifically, we show how the deterministic errorscaused by incorrect keys can be harnessed to produce neural-trojan-stylebackdoors. To do so, we first outline a motivational attack scenario where acarefully chosen incorrect key, which we call a trojan key, producesmisclassifications for an attacker-specified input class in a lockedaccelerator. We then develop a theoretically-robust attack methodology toautomatically identify trojan keys. To evaluate this attack, we launch it onseveral locked accelerators. In our largest benchmark accelerator, our attackidentified a trojan key that caused a 74% decrease in classification accuracyfor attacker-specified trigger inputs, while degrading accuracy by only 1.7%for other inputs on average.", "output": "Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic perception of human behaviors during social interactions is crucialfor AR/VR applications, and an essential component is estimation of plausible3D human pose and shape of our social partners from the egocentric view. One ofthe biggest challenges of this task is severe body truncation due to closesocial distances in egocentric scenarios, which brings large pose ambiguitiesfor unseen body parts. To tackle this challenge, we propose a novelscene-conditioned diffusion method to model the body pose distribution.Conditioned on the 3D scene geometry, the diffusion model generates bodies inplausible human-scene interactions, with the sampling guided by a physics-basedcollision score to further resolve human-scene inter-penetrations. Theclassifier-free training enables flexible sampling with different conditionsand enhanced diversity. A visibility-aware graph convolution model guided byper-joint visibility serves as the diffusion denoiser to incorporateinter-joint dependencies and per-body-part control. Extensive evaluations showthat our method generates bodies in plausible interactions with 3D scenes,achieving both superior accuracy for visible joints and diversity for invisiblebody parts. The code will be available at", "output": "Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works demonstrate a remarkable ability to customize text-to-imagediffusion models while only providing a few example images. What happens if youtry to customize such models using multiple, fine-grained concepts in asequential (i.e., continual) manner? In our work, we show that recentstate-of-the-art customization of text-to-image models suffer from catastrophicforgetting when new concepts arrive sequentially. Specifically, when adding anew concept, the ability to generate high quality images of past, similarconcepts degrade. To circumvent this forgetting, we propose a new method,C-LoRA, composed of a continually self-regularized low-rank adaptation in crossattention layers of the popular Stable Diffusion model. Furthermore, we usecustomization prompts which do not include the word of the customized object(i.e., \"person\" for a human face dataset) and are initialized as completelyrandom embeddings. Importantly, our method induces only marginal additionalparameter costs and requires no storage of user data for replay. We show thatC-LoRA not only outperforms several baselines for our proposed setting oftext-to-image continual customization, which we refer to as ContinualDiffusion, but that we achieve a new state-of-the-art in the well-establishedrehearsal-free continual learning setting for image classification. The highachieving performance of C-LoRA in two separate domains positions it as acompelling solution for a wide range of applications, and we believe it hassignificant potential for practical impact.", "output": "Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning at multiple levels of temporal abstraction is one of the keyattributes of intelligence. In reinforcement learning, this is often modeledthrough temporally extended courses of actions called options. Options allowagents to make predictions and to operate at different levels of abstractionwithin an environment. Nevertheless, approaches based on the options frameworkoften start with the assumption that a reasonable set of options is knownbeforehand. When this is not the case, there are no definitive answers forwhich options one should consider. In this paper, we argue that the successorrepresentation (SR), which encodes states based on the pattern of statevisitation that follows them, can be seen as a natural substrate for thediscovery and use of temporal abstractions. To support our claim, we take a bigpicture view of recent results, showing how the SR can be used to discoveroptions that facilitate either temporally-extended exploration or planning. Wecast these results as instantiations of a general framework for optiondiscovery in which the agent's representation is used to identify usefuloptions, which are then used to further improve its representation. Thisresults in a virtuous, never-ending, cycle in which both the representation andthe options are constantly refined based on each other. Beyond option discoveryitself, we also discuss how the SR allows us to augment a set of options into acombinatorially large counterpart without additional learning. This is achievedthrough the combination of previously learned options. Our empirical evaluationfocuses on options discovered for exploration and on the use of the SR tocombine them. The results of our experiments shed light on important designdecisions involved in the definition of options and demonstrate the synergy ofdifferent methods based on the SR, such as eigenoptions and the optionkeyboard.", "output": "Temporal Abstraction in Reinforcement Learning with the Successor Representation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A cross-domain recommendation has shown promising results in solvingdata-sparsity and cold-start problems. Despite such progress, existing methodsfocus on domain-shareable information (overlapped users or same contexts) for aknowledge transfer, and they fail to generalize well without such requirements.To deal with these problems, we suggest utilizing review texts that are generalto most e-commerce systems. Our model (named SER) uses three text analysismodules, guided by a single domain discriminator for disentangledrepresentation learning. Here, we suggest a novel optimization strategy thatcan enhance the quality of domain disentanglement, and also debilitatesdetrimental information of a source domain. Also, we extend the encodingnetwork from a single to multiple domains, which has proven to be powerful forreview-based recommender systems. Extensive experiments and ablation studiesdemonstrate that our method is efficient, robust, and scalable compared to thestate-of-the-art single and cross-domain recommendation methods.", "output": "Review-Based Domain Disentanglement without Duplicate Users or Contexts for Cross-Domain Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline estimation of the dynamical model of a Markov Decision Process (MDP)is a non-trivial task that greatly depends on the data available in thelearning phase. Sometimes the dynamics of the model is invariant with respectto some transformations of the current state and action. Recent works showedthat an expert-guided pipeline relying on Density Estimation methods as DeepNeural Network based Normalizing Flows effectively detects this structure indeterministic environments, both categorical and continuous-valued. Theacquired knowledge can be exploited to augment the original data set, leadingeventually to a reduction in the distributional shift between the true and thelearned model. Such data augmentation technique can be exploited as apreliminary process to be executed before adopting an Offline ReinforcementLearning architecture, increasing its performance. In this work we extend theparadigm to also tackle non-deterministic MDPs, in particular, 1) we propose adetection threshold in categorical environments based on statistical distances,and 2) we show that the former results lead to a performance improvement whensolving the learned MDP and then applying the optimized policy in the realenvironment.", "output": "Data Augmentation through Expert-guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neuro-symbolic approaches to artificial intelligence, which combine neuralnetworks with classical symbolic techniques, are growing in prominence,necessitating formal approaches to reason about their correctness. We propose anovel modelling formalism called neuro-symbolic concurrent stochastic games(NS-CSGs), which comprise probabilistic finite-state agents interacting in ashared continuous-state environment observed through perception mechanismsimplemented as neural networks (NNs). We focus on the class of NS-CSGs withBorel state spaces and prove the existence and measurability of the valuefunction for zero-sum discounted cumulative rewards under piecewise-constantrestrictions on the components of this class of models. To compute values andsynthesise strategies, we present, for the first time, implementable valueiteration (VI) and policy iteration (PI) algorithms to solve a class ofcontinuous-state CSGs. These require a finite representation of the pre-imageof the environment's NN perception mechanism and rely on finite abstractrepresentations of value functions and strategies closed under VI or PI. First,we introduce a Borel measurable piecewise-constant (B-PWC) representation ofvalue functions, extend minimax backups to this representation and proposeB-PWC VI. Second, we introduce two novel representations for the valuefunctions and strategies, constant-piecewise-linear (CON-PWL) andconstant-piecewise-constant (CON-PWC) respectively, and proposeMinimax-action-free PI by extending a recent PI method based on alternatingplayer choices for finite state spaces to Borel state spaces, which does notrequire normal-form games to be solved. We illustrate our approach with adynamic vehicle parking example by generating approximately optimal strategiesusing a prototype implementation of the B-PWC VI algorithm.", "output": "Strategy Synthesis for Zero-Sum Neuro-Symbolic Concurrent Stochastic Games."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Parameterized quantum circuits can be used as quantum neural networks andhave the potential to outperform their classical counterparts when trained foraddressing learning problems. To date, much of the results on their performanceon practical problems are heuristic in nature. In particular, the convergencerate for the training of quantum neural networks is not fully understood. Here,we analyze the dynamics of gradient descent for the training error of a classof variational quantum machine learning models. We define wide quantum neuralnetworks as parameterized quantum circuits in the limit of a large number ofqubits and variational parameters. We then find a simple analytic formula thatcaptures the average behavior of their loss function and discuss theconsequences of our findings. For example, for random quantum circuits, wepredict and characterize an exponential decay of the residual training error asa function of the parameters of the system. We finally validate our analyticresults with numerical experiments.", "output": "Analytic theory for the dynamics of wide quantum neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a model for multi-objective optimization, a credo, for agents in asystem that are configured into multiple groups (i.e., teams). Our model ofcredo regulates how agents optimize their behavior for the groups they belongto. We evaluate credo in the context of challenging social dilemmas withreinforcement learning agents. Our results indicate that the interests ofteammates, or the entire system, are not required to be fully aligned forachieving globally beneficial outcomes. We identify two scenarios without fullcommon interest that achieve high equality and significantly higher meanpopulation rewards compared to when the interests of all agents are aligned.", "output": "The Importance of Credo in Multiagent Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the booming demand for machine learning applications, it has beenrecognized that the number of knowledgeable data scientists can not scale withthe growing data volumes and application needs in our digital world. Inresponse to this demand, several automated machine learning (AutoML) frameworkshave been developed to fill the gap of human expertise by automating theprocess of building machine learning pipelines. Each framework comes withdifferent heuristics-based design decisions. In this study, we present acomprehensive evaluation and comparison of the performance characteristics ofsix popular AutoML frameworks, namely, AutoWeka, AutoSKlearn, TPOT, Recipe,ATM, and SmartML, across 100 data sets from established AutoML benchmarksuites. Our experimental evaluation considers different aspects for itscomparison, including the performance impact of several design decisions,including time budget, size of search space, meta-learning, and ensembleconstruction. The results of our study reveal various interesting insights thatcan significantly guide and impact the design of AutoML frameworks.", "output": "AutoMLBench: A Comprehensive Experimental Evaluation of Automated Machine Learning Frameworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In decoding linear block codes, it was shown that noticeable reliabilitygains can be achieved by introducing learnable parameters to the BeliefPropagation (BP) decoder. Despite the success of these methods, there are twokey open problems. The first is the lack of interpretation of the learnedweights, and the other is the lack of analysis for non-AWGN channels. In thiswork, we aim to bridge this gap by providing insights into the weights learnedand their connection to the structure of the underlying code. We show that theweights are heavily influenced by the distribution of short cycles in the code.We next look at the performance of these decoders in non-AWGN channels, bothsynthetic and over-the-air channels, and study the complexity vs. performancetrade-offs, demonstrating that increasing the number of parameters helpssignificantly in complex channels. Finally, we show that the decoders withlearned weights achieve higher reliability than those with weights optimizedanalytically under the Gaussian approximation.", "output": "Interpreting Neural Min-Sum Decoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the success of convolutional neural networks (CNNs) in numerouscomputer vision tasks and their extraordinary generalization performances,several attempts to predict the generalization errors of CNNs have only beenlimited to a posteriori analyses thus far. A priori theories explaining thegeneralization performances of deep neural networks have mostly ignored theconvolutionality aspect and do not specify why CNNs are able to seeminglyovercome curse of dimensionality on computer vision tasks like imageclassification where the image dimensions are in thousands. Our work attemptsto explain the generalization performance of CNNs on image classification underthe hypothesis that CNNs operate on the domain of image patches. Ours is thefirst work we are aware of to derive an a priori error bound for thegeneralization error of CNNs and we present both quantitative and qualitativeevidences in the support of our theory. Our patch-based theory also offersexplanation for why data augmentation techniques like Cutout, CutMix and randomcropping are effective in improving the generalization error of CNNs.", "output": "CNNs Avoid Curse of Dimensionality by Learning on Patches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monumental advancements in artificial intelligence (AI) have lured theinterest of doctors, lenders, judges, and other professionals. While thesehigh-stakes decision-makers are optimistic about the technology, those familiarwith AI systems are wary about the lack of transparency of its decision-makingprocesses. Perturbation-based post hoc explainers offer a model agnostic meansof interpreting these systems while only requiring query-level access. However,recent work demonstrates that these explainers can be fooled adversarially.This discovery has adverse implications for auditors, regulators, and othersentinels. With this in mind, several natural questions arise - how can weaudit these black box systems? And how can we ascertain that the auditee iscomplying with the audit in good faith? In this work, we rigorously formalizethis problem and devise a defense against adversarial attacks onperturbation-based explainers. We propose algorithms for the detection(CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by ournovel conditional anomaly detection approach, KNN-CAD. We demonstrate that ourapproach successfully detects whether a black box system adversarially concealsits decision-making process and mitigates the adversarial attack on real-worlddata for the prevalent explainers, LIME and SHAP.", "output": "Unfooling Perturbation-Based Post Hoc Explainers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatiotemporal predictive learning aims to generate future frames by learningfrom historical frames. In this paper, we investigate existing methods andpresent a general framework of spatiotemporal predictive learning, in which thespatial encoder and decoder capture intra-frame features and the middletemporal module catches inter-frame correlations. While the mainstream methodsemploy recurrent units to capture long-term temporal dependencies, they sufferfrom low computational efficiency due to their unparallelizable architectures.To parallelize the temporal module, we propose the Temporal Attention Unit(TAU), which decomposes the temporal attention into intra-frame staticalattention and inter-frame dynamical attention. Moreover, while the mean squarederror loss focuses on intra-frame errors, we introduce a novel differentialdivergence regularization to take inter-frame variations into account.Extensive experiments demonstrate that the proposed method enables the derivedmodel to achieve competitive performance on various spatiotemporal predictionbenchmarks.", "output": "Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent Language Models (LMs) achieve breakthrough performance in codegeneration when trained on human-authored problems, even solving somecompetitive-programming problems. Self-play has proven useful in games such asGo, and thus it is natural to ask whether LMs can generate their owninstructive programming problems to improve their performance. We show that itis possible for an LM to synthesize programming problems and solutions, whichare filtered for correctness by a Python interpreter. The LM's performance isthen seen to improve when it is fine-tuned on its own synthetic problems andverified solutions; thus the model 'improves itself' using the Pythoninterpreter. Problems are specified formally as programming puzzles [Schusteret al., 2021], a code-based problem format where solutions can easily beverified for correctness by execution. In experiments on publicly-availableLMs, test accuracy more than doubles. This work demonstrates the potential forcode LMs, with an interpreter, to generate instructive problems and improvetheir own performance.", "output": "Language Models Can Teach Themselves to Program Better."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While instance-level explanation of GNN is a well-studied problem with plentyof approaches being developed, providing a global explanation for the behaviourof a GNN is much less explored, despite its potential in interpretability anddebugging. Existing solutions either simply list local explanations for a givenclass, or generate a synthetic prototypical graph with maximal score for agiven class, completely missing any combinatorial aspect that the GNN couldhave learned. In this work, we propose GLGExplainer (Global Logic-based GNNExplainer), the first Global Explainer capable of generating explanations asarbitrary Boolean combinations of learned graphical concepts. GLGExplainer is afully differentiable architecture that takes local explanations as inputs andcombines them into a logic formula over graphical concepts, represented asclusters of local explanations. Contrary to existing solutions, GLGExplainerprovides accurate and human-interpretable global explanations that areperfectly aligned with ground-truth explanations (on synthetic data) or matchexisting domain knowledge (on real-world data). Extracted formulas are faithfulto the model predictions, to the point of providing insights into someoccasionally incorrect rules learned by the model, making GLGExplainer apromising diagnostic tool for learned GNNs.", "output": "Global Explainability of GNNs via Logic Combination of Learned Concepts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has alimitation dealing with long inputs due to its attention mechanism. Longformer,ETC and BigBird addressed this issue and effectively solved the quadraticdependency problem. However we find that these models are not sufficient, andpropose LittleBird, a novel model based on BigBird with improved speed andmemory footprint while maintaining accuracy. In particular, we devise a moreflexible and efficient position representation method based on Attention withLinear Biases (ALiBi). We also show that replacing the method of globalinformation represented in the BigBird with pack and unpack attention is moreeffective. The proposed model can work on long inputs even after beingpre-trained on short inputs, and can be trained efficiently reusing existingpre-trained language model for short inputs. This is a significant benefit forlow-resource languages where large amounts of long text data are difficult toobtain. As a result, our experiments show that LittleBird works very well in avariety of languages, achieving high performance in question answering tasks,particularly in KorQuAD2.0, Korean Question Answering Dataset for longparagraphs.", "output": "LittleBird: Efficient Faster & Longer Transformer for Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Steganography is the practice of encoding secret information into innocuouscontent in such a manner that an adversarial third party would not realize thatthere is hidden meaning. While this problem has classically been studied insecurity literature, recent advances in generative models have led to a sharedinterest among security and machine learning researchers in developing scalablesteganography techniques. In this work, we show that a steganography procedureis perfectly secure under Cachin (1998)'s information theoretic-model ofsteganography if and only if it is induced by a coupling. Furthermore, we showthat, among perfectly secure procedures, a procedure is maximally efficient ifand only if it is induced by a minimum entropy coupling. These insights yieldwhat are, to the best of our knowledge, the first steganography algorithms toachieve perfect security guarantees with non-trivial efficiency; additionally,these algorithms are highly scalable. To provide empirical validation, wecompare a minimum entropy coupling-based approach to three modern baselines --arithmetic coding, Meteor, and adaptive dynamic grouping -- using GPT-2,WaveRNN, and Image Transformer as communication channels. We find that theminimum entropy coupling-based approach achieves superior encoding efficiency,despite its stronger security constraints. In aggregate, these results suggestthat it may be natural to view information-theoretic steganography through thelens of minimum entropy coupling.", "output": "Perfectly Secure Steganography Using Minimum Entropy Coupling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Researchers are solving the challenges of spatial-temporal prediction bycombining Federated Learning (FL) and graph models with respect to theconstrain of privacy and security. In order to make better use of the power ofgraph model, some researchs also combine split learning(SL). However, there arestill several issues left unattended: 1) Clients might not be able to accessthe server during inference phase; 2) The graph of clients designed manually inthe server model may not reveal the proper relationship between clients. Thispaper proposes a new GNN-oriented split federated learning method, named node{bfseries M}asking and {bfseries M}ulti-granularity {bfseries M}essagepassing-based Federated Graph Model (M$^3$FGM) for the above issues. For thefirst issue, the server model of M$^3$FGM employs a MaskNode layer to simulatethe case of clients being offline. We also redesign the decoder of the clientmodel using a dual-sub-decoders structure so that each client model can use itslocal data to predict independently when offline. As for the second issue, anew GNN layer named Multi-Granularity Message Passing (MGMP) layer enables eachclient node to perceive global and local information. We conducted extensiveexperiments in two different scenarios on two real traffic datasets. Resultsshow that M$^3$FGM outperforms the baselines and variant models, achieves thebest results in both datasets and scenarios.", "output": "M3FGM:a node masking and multi-granularity message passing-based federated graph model for spatial-temporal data prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In intensive care units (ICUs), critically ill patients are monitored withelectroencephalograms (EEGs) to prevent serious brain injury. The number ofpatients who can be monitored is constrained by the availability of trainedphysicians to read EEGs, and EEG interpretation can be subjective and prone tointer-observer variability. Automated deep learning systems for EEG couldreduce human bias and accelerate the diagnostic process. However, black boxdeep learning models are untrustworthy, difficult to troubleshoot, and lackaccountability in real-world applications, leading to a lack of trust andadoption by clinicians. To address these challenges, we propose a novelinterpretable deep learning model that not only predicts the presence ofharmful brainwave patterns but also provides high-quality case-basedexplanations of its decisions. Our model performs better than the correspondingblack box model, despite being constrained to be interpretable. The learned 2Dembedded space provides the first global overview of the structure ofictal-interictal-injury continuum brainwave patterns. The ability to understandhow our model arrived at its decisions will not only help clinicians todiagnose and treat harmful brain activities more accurately but also increasetheir trust and adoption of machine learning models in clinical practice; thiscould be an integral component of the ICU neurologists' standard workflow.", "output": "Interpretable Machine Learning System to EEG Patterns on the Ictal-Interictal-Injury Continuum."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While large pretrained language models (PLMs) demonstrate incredible fluencyand performance on many natural language tasks, recent work has shown thatwell-performing PLMs are very sensitive to what prompts are feed into them.Even when prompts are semantically identical, language models may give verydifferent answers. When considering safe and trustworthy deployments of PLMs wewould like their outputs to be consistent under prompts that mean the samething or convey the same intent. While some work has looked into howstate-of-the-art PLMs address this need, they have been limited to onlyevaluating lexical equality of single- or multi-word answers and do not addressconsistency of generative text sequences. In order to understand consistency ofPLMs under text generation settings, we develop a measure of semanticconsistency that allows the comparison of open-ended text outputs. We implementseveral versions of this consistency metric to evaluate the performance of anumber of PLMs on paraphrased versions of questions in the TruthfulQA dataset,we find that our proposed metrics are considerably more consistent thantraditional metrics embodying lexical consistency, and also correlate withhuman evaluation of output consistency to a higher degree.", "output": "Measuring Reliability of Large Language Models through Semantic Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past few years, large knowledge bases have been constructed to storemassive amounts of knowledge. However, these knowledge bases are highlyincomplete. To solve this problem, we propose a web-based question answeringsystem system with multimodal fusion of unstructured and structuredinformation, to fill in missing information for knowledge bases. To utilizeunstructured information from the Web for knowledge base completion, we designa web-based question answering system using multimodal features and questiontemplates to extract missing facts, which can achieve good performance withvery few questions. To help improve extraction quality, the question answeringsystem employs structured information from knowledge bases, such as entitytypes and entity-to-entity relatedness.", "output": "Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Constraint programming (CP) is a powerful technique for solving constraintsatisfaction and optimization problems. In CP solvers, the variable orderingstrategy used to select which variable to explore first in the solving processhas a significant impact on solver effectiveness. To address this issue, wepropose a novel variable ordering strategy based on supervised learning, whichwe evaluate in the context of job shop scheduling problems. Our learning-basedmethods predict the optimal solution of a problem instance and use thepredicted solution to order variables for CP solvers. added[]{Unliketraditional variable ordering methods, our methods can learn from thecharacteristics of each problem instance and customize the variable orderingstrategy accordingly, leading to improved solver performance.} Our experimentsdemonstrate that training machine learning models is highly efficient and canachieve high accuracy. Furthermore, our learned variable ordering methodsperform competitively when compared to four existing methods. Finally, wedemonstrate that hybridising the machine learning-based variable orderingmethods with traditional domain-based methods is beneficial.", "output": "Enhancing Constraint Programming via Supervised Learning for Job Shop Scheduling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graphs (KGs) have attracted more and more attentions because oftheir fundamental roles in many tasks. Quality evaluation for KGs is thuscrucial and indispensable. Existing methods in this field evaluate KGs byeither proposing new quality metrics from different dimensions or measuringperformances at KG construction stages. However, there are two major issueswith those methods. First, they highly rely on raw data in KGs, which makesKGs' internal information exposed during quality evaluation. Second, theyconsider more about the quality at data level instead of ability level, wherethe latter one is more important for downstream applications. To address theseissues, we propose a knowledge graph quality evaluation framework underincomplete information (QEII). The quality evaluation task is transformed intoan adversarial Q&amp;A game between two KGs. Winner of the game is thus consideredto have better qualities. During the evaluation process, no raw data isexposed, which ensures information protection. Experimental results on fourpairs of KGs demonstrate that, compared with baselines, the QEII implements areasonable quality evaluation at ability level under incomplete information.", "output": "Knowledge Graph Quality Evaluation under Incomplete Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past few years, large knowledge bases have been constructed to storemassive amounts of knowledge. However, these knowledge bases are highlyincomplete, for example, over 70% of people in Freebase have no known place ofbirth. To solve this problem, we propose a query-driven knowledge basecompletion system with multimodal fusion of unstructured and structuredinformation. To effectively fuse unstructured information from the Web andstructured information in knowledge bases to achieve good performance, oursystem builds multimodal knowledge graphs based on question answering and ruleinference. We propose a multimodal path fusion algorithm to rank candidateanswers based on different paths in the multimodal knowledge graphs, achievingmuch better performance than question answering, rule inference and a baselinefusion algorithm. To improve system efficiency, query-driven techniques areutilized to reduce the runtime of our system, providing fast responses to userqueries. Extensive experiments have been conducted to demonstrate theeffectiveness and efficiency of our system.", "output": "Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous cars are indispensable when humans go further down the hands-freeroute. Although existing literature highlights that the acceptance of theautonomous car will increase if it drives in a human-like manner, sparseresearch offers the naturalistic experience from a passenger's seat perspectiveto examine the human likeness of current autonomous cars. The present studytested whether the AI driver could create a human-like ride experience forpassengers based on 69 participants' feedback in a real-road scenario. Wedesigned a ride experience-based version of the non-verbal Turing test forautomated driving. Participants rode in autonomous cars (driven by either humanor AI drivers) as a passenger and judged whether the driver was human or AI.The AI driver failed to pass our test because passengers detected the AI driverabove chance. In contrast, when the human driver drove the car, the passengers'judgement was around chance. We further investigated how human passengersascribe humanness in our test. Based on Lewin's field theory, we advanced acomputational model combining signal detection theory with pre-trained languagemodels to predict passengers' humanness rating behaviour. We employed affectivetransition between pre-study baseline emotions and corresponding post-stageemotions as the signal strength of our model. Results showed that thepassengers' ascription of humanness would increase with the greater affectivetransition. Our study suggested an important role of affective transition inpassengers' ascription of humanness, which might become a future direction forautonomous driving.", "output": "Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing pre-training methods for extractive Question Answering (QA) generatecloze-like queries different from natural questions in syntax structure, whichcould overfit pre-trained models to simple keyword matching. In order toaddress this problem, we propose a novel Momentum Contrastive pRe-training fOrqueStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSSintroduces a momentum contrastive learning framework to align the answerprobability between cloze-like and natural query-passage sample pairs. Hence,the pre-trained models can better transfer the knowledge learned in cloze-likesamples to answering natural questions. Experimental results on threebenchmarking QA datasets show that our method achieves noticeable improvementcompared with all baselines in both supervised and zero-shot scenarios.", "output": "Momentum Contrastive Pre-training for Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Some recent artificial neural networks (ANNs) claim to model aspects ofprimate neural and human performance data. Their success in object recognitionis, however, dependent on exploiting low-level features for solving visualtasks in a way that humans do not. As a result, out-of-distribution oradversarial input is often challenging for ANNs. Humans instead learn abstractpatterns and are mostly unaffected by many extreme image distortions. Weintroduce a set of novel image transforms inspired by neurophysiologicalfindings and evaluate humans and ANNs on an object recognition task. We showthat machines perform better than humans for certain transforms and struggle toperform at par with humans on others that are easy for humans. We quantify thedifferences in accuracy for humans and machines and find a ranking ofdifficulty for our transforms for human data. We also suggest how certaincharacteristics of human visual processing can be adapted to improve theperformance of ANNs for our difficult-for-machines transforms.", "output": "Extreme Image Transformations Affect Humans and Machines Differently."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building large AI fleets to support the rapidly growing DL workloads is anactive research topic for modern cloud providers. Generating accuratebenchmarks plays an essential role in designing the fast-paced software andhardware solutions in this space. Two fundamental challenges to make thisscalable are (i) workload representativeness and (ii) the ability to quicklyincorporate changes to the fleet into the benchmarks.To overcome these issues, we propose Mystique, an accurate and scalableframework for production AI benchmark generation. It leverages the PyTorchexecution trace (ET), a new feature that captures the runtime information of AImodels at the granularity of operators, in a graph format, together with theirmetadata. By sourcing fleet ETs, we can build AI benchmarks that are portableand representative. Mystique is scalable, due to its lightweight datacollection, in terms of runtime overhead and instrumentation effort. It is alsoadaptive because ET composability allows flexible control on benchmarkcreation.We evaluate our methodology on several production AI models, and show thatbenchmarks generated with Mystique closely resemble original AI models, both inexecution time and system-level metrics. We also showcase the portability ofthe generated benchmarks across platforms, and demonstrate several use casesenabled by the fine-grained composability of the execution trace.", "output": "Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-view image compression plays a critical role in 3D-relatedapplications. Existing methods adopt a predictive coding architecture, whichrequires joint encoding to compress the corresponding disparity as well asresidual information. This demands collaboration among cameras and enforces theepipolar geometric constraint between different views, which makes itchallenging to deploy these methods in distributed camera systems with randomlyoverlapping fields of view. Meanwhile, distributed source coding theoryindicates that efficient data compression of correlated sources can be achievedby independent encoding and joint decoding, which motivates us to design alearning-based distributed multi-view image coding (LDMIC) framework. Withindependent encoders, LDMIC introduces a simple yet effective joint contexttransfer module based on the cross-attention mechanism at the decoder toeffectively capture the global inter-view correlations, which is insensitive tothe geometric relationships between images. Experimental results show thatLDMIC significantly outperforms both traditional and learning-based MIC methodswhile enjoying fast encoding speed. Code will be released at", "output": "LDMIC: Learning-based Distributed Multi-view Image Coding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of fairly allocating a set of indivisible goods amongagents with bivalued submodular valuations -- each good provides a marginalgain of either $a$ or $b$ ($a &lt; b$) and goods have decreasing marginal gains.This is a natural generalization of two well-studied valuation classes --bivalued additive valuations and binary submodular valuations. We present asimple sequential algorithmic framework, based on the recently introducedYankee Swap mechanism, that can be adapted to compute a variety of solutionconcepts, including leximin, max Nash welfare (MNW) and $p$-mean welfaremaximizing allocations when $a$ divides $b$. This result is complemented by anexisting result on the computational intractability of leximin and MNWallocations when $a$ does not divide $b$.We further examine leximin and MNW allocations with respect to two well-knownproperties -- envy freeness and the maximin share guarantee. On envy freeness,we show that neither the leximin nor the MNW allocation is guaranteed to beenvy free up to one good (EF1). This is surprising since for the simplerclasses of bivalued additive valuations and binary submodular valuations, MNWallocations are known to be envy free up to any good (EFX). On the maximinshare guarantee, we show that MNW and leximin allocations guarantee each agent$frac14$ and $frac{a}{b+3a}$ of their maximin share respectively when $a$divides $b$. This fraction improves to $frac13$ and $frac{a}{b+2a}$respectively when agents have bivalued additive valuations.", "output": "Dividing Good and Better Items Among Agents with Submodular Valuations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models (LMs) have been instrumental for the rapid advance of naturallanguage processing. This paper studies continual pre-training of LMs, inparticular, continual domain-adaptive pre-training (or continual DAP-training).Existing research has shown that further pre-training an LM using a domaincorpus to adapt the LM to the domain can improve the end-task performance inthe domain. This paper proposes a novel method to continually DAP-train an LMwith a sequence of unlabeled domain corpora to adapt the LM to these domains toimprove their end-task performances. The key novelty of our method is asoft-masking mechanism that directly controls the update to the LM. A novelproxy is also proposed to preserve the general knowledge in the original LM.Additionally, it contrasts the representations of the previously learned domainknowledge (including the general knowledge in the pre-trained LM) and theknowledge from the current full network to achieve knowledge integration. Themethod not only overcomes catastrophic forgetting, but also achieves knowledgetransfer to improve end-task performances. Empirical evaluation demonstratesthe effectiveness of the proposed method.", "output": "Continual Pre-training of Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive learning methods have been applied to a range of domains andmodalities by training models to identify similar \"views\" of data points.However, specialized scientific modalities pose a challenge for this paradigm,as identifying good views for each scientific instrument is complex andtime-intensive. In this paper, we focus on applying contrastive learningapproaches to a variety of remote sensing datasets. We show that Viewmakernetworks, a recently proposed method for generating views, are promising forproducing views in this setting without requiring extensive domain knowledgeand trial and error. We apply Viewmaker to four multispectral imaging problems,each with a different format, finding that Viewmaker can outperform cropping-and reflection-based methods for contrastive learning in every case whenevaluated on downstream classification tasks. This provides additional evidencethat domain-agnostic methods can empower contrastive learning to scale toreal-world scientific domains. Open source code can be found at", "output": "Multispectral Contrastive Learning with Viewmaker Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in generative models such as GPT may be used to fabricateindistinguishable fake customer reviews at a much lower cost, thus posingchallenges for social media platforms to detect these machine-generated fakereviews. We propose to leverage the high-quality elite restaurant reviewsverified by Yelp to generate fake reviews from the OpenAI GPT review creatorand ultimately fine-tune a GPT output detector to predict fake reviews thatsignificantly outperform existing solutions. We further apply the model topredict non-elite reviews and identify the patterns across several dimensions,such as review, user and restaurant characteristics, and writing style. We showthat social media platforms are continuously challenged by machine-generatedfake reviews, although they may implement detection systems to filter outsuspicious reviews.", "output": "Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) is a machine learning approach that trains agentsto maximize cumulative rewards through interactions with environments. Theintegration of RL with deep learning has recently resulted in impressiveachievements in a wide range of challenging tasks, including board games,arcade games, and robot control. Despite these successes, there remain severalcrucial challenges, including brittle convergence properties caused bysensitive hyperparameters, difficulties in temporal credit assignment with longtime horizons and sparse rewards, a lack of diverse exploration, especially incontinuous search space scenarios, difficulties in credit assignment inmulti-agent reinforcement learning, and conflicting objectives for rewards.Evolutionary computation (EC), which maintains a population of learning agents,has demonstrated promising performance in addressing these limitations. Thisarticle presents a comprehensive survey of state-of-the-art methods forintegrating EC into RL, referred to as evolutionary reinforcement learning(EvoRL). We categorize EvoRL methods according to key research fields in RL,including hyperparameter optimization, policy search, exploration, rewardshaping, meta-RL, and multi-objective RL. We then discuss future researchdirections in terms of efficient methods, benchmarks, and scalable platforms.This survey serves as a resource for researchers and practitioners interestedin the field of EvoRL, highlighting the important challenges and opportunitiesfor future research. With the help of this survey, researchers andpractitioners can develop more efficient methods and tailored benchmarks forEvoRL, further advancing this promising cross-disciplinary research field.", "output": "Evolutionary Reinforcement Learning: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite excellent average-case performance of many image classifiers, theirperformance can substantially deteriorate on semantically coherent subgroups ofthe data that were under-represented in the training data. These systematicerrors can impact both fairness for demographic minority groups as well asrobustness and safety under domain shift. A major challenge is to identify suchsubgroups with subpar performance when the subgroups are not annotated andtheir occurrence is very rare. We leverage recent advances in text-to-imagemodels and search in the space of textual descriptions of subgroups (\"prompts\")for subgroups where the target model has low performance on theprompt-conditioned synthesized data. To tackle the exponentially growing numberof subgroups, we employ combinatorial testing. We denote this procedure asPromptAttack as it can be interpreted as an adversarial attack in a promptspace. We study subgroup coverage and identifiability with PromptAttack in acontrolled setting and find that it identifies systematic errors with highaccuracy. Thereupon, we apply PromptAttack to ImageNet classifiers and identifynovel systematic errors on rare subgroups.", "output": "Identification of Systematic Errors of Image Classifiers on Rare Subgroups."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sign language recognition (SLR) is a weakly supervised task that annotatessign videos as textual glosses. Recent studies show that insufficient trainingcaused by the lack of large-scale available sign datasets becomes the mainbottleneck for SLR. Most SLR works thereby adopt pretrained visual modules anddevelop two mainstream solutions. The multi-stream architectures extendmulti-cue visual features, yielding the current SOTA performances but requiringcomplex designs and might introduce potential noise. Alternatively, theadvanced single-cue SLR frameworks using explicit cross-modal alignment betweenvisual and textual modalities are simple and effective, potentially competitivewith the multi-cue framework. In this work, we propose a novel contrastivevisual-textual transformation for SLR, CVT-SLR, to fully explore the pretrainedknowledge of both the visual and language modalities. Based on the single-cuecross-modal alignment framework, we propose a variational autoencoder (VAE) forpretrained contextual knowledge while introducing the complete pretrainedlanguage module. The VAE implicitly aligns visual and textual modalities whilebenefiting from pretrained contextual knowledge as the traditional contextualmodule. Meanwhile, a contrastive cross-modal alignment algorithm is designed toexplicitly enhance the consistency constraints. Extensive experiments on publicdatasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLRconsistently outperforms existing single-cue methods and even outperforms SOTAmulti-cue methods.", "output": "CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "L'evy walks and other theoretical models of optimal foraging have beensuccessfully used to describe real-world scenarios, attracting attention inseveral fields such as economy, physics, ecology, and evolutionary biology.However, it remains unclear in most cases which strategies maximize foragingefficiency and whether such strategies can be learned by living organisms. Toaddress these questions, we model foragers as reinforcement learning agents. Wefirst prove theoretically that maximizing rewards in our reinforcement learningmodel is equivalent to optimizing foraging efficiency. We then show withnumerical experiments that our agents learn foraging strategies whichoutperform the efficiency of known strategies such as L'evy walks.", "output": "Optimal foraging strategies can be learned and outperform L\\'evy walks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sensing technologies deployed in the workplace can collect detailed dataabout individual activities and group interactions that are otherwise difficultto capture. A hopeful application of these technologies is that they can helpbusinesses and workers optimize productivity and wellbeing. However, given theinherent and structural power dynamics in the workplace, the prevalent approachof accepting tacit compliance to monitor work activities rather than seekingworkers' meaningful consent raises privacy and ethical concerns. This paperunpacks a range of challenges that workers face when consenting to workplacewellbeing technologies. Using a hypothetical case to prompt reflection amongsix multi-stakeholder focus groups involving 15 participants, we exploredparticipants' expectations and capacity to consent to workplace sensingtechnologies. We sketched possible interventions that could better support moremeaningful consent to workplace wellbeing technologies by drawing on criticalcomputing and feminist scholarship -- which reframes consent from a purelyindividual choice to a structural condition experienced at the individual levelthat needs to be freely given, reversible, informed, enthusiastic, and specific(FRIES). The focus groups revealed that workers are vulnerable to meaninglessconsent -- dynamics that undo the value of data gathered in the name of\"wellbeing,\" as well as an erosion of autonomy in the workplace. Tomeaningfully consent, participants wanted changes to how the technology worksand is being used, as well as to the policies and practices surrounding thetechnology. Our mapping of what prevents workers from meaningfully consentingto workplace wellbeing technologies (challenges) and what they require to do so(interventions) underscores that the lack of meaningful consent is a structuralproblem requiring socio-technical solutions.", "output": "Can Workers Meaningfully Consent to Workplace Wellbeing Technologies?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have shown great promise in text-guided image styletransfer, but there is a trade-off between style transformation and contentpreservation due to their stochastic nature. Existing methods requirecomputationally expensive fine-tuning of diffusion models or additional neuralnetwork. To address this, here we propose a zero-shot contrastive loss fordiffusion models that doesn't require additional fine-tuning or auxiliarynetworks. By leveraging patch-wise contrastive loss between generated samplesand original image embeddings in the pre-trained diffusion model, our methodcan generate images with the same semantic content as the source image in azero-shot manner. Our approach outperforms existing methods while preservingcontent and requiring no additional training, not only for image style transferbut also for image-to-image translation and manipulation. Our experimentalresults validate the effectiveness of our proposed method.", "output": "Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cognitive psychology delves on understanding perception, attention, memory,language, problem-solving, decision-making, and reasoning. Large languagemodels (LLMs) are emerging as potent tools increasingly capable of performinghuman-level tasks. The recent development in the form of GPT-4 and itsdemonstrated success in tasks complex to humans exam and complex problems hasled to an increased confidence in the LLMs to become perfect instruments ofintelligence. Although GPT-4 report has shown performance on some cognitivepsychology tasks, a comprehensive assessment of GPT-4, via the existingwell-established datasets is required. In this study, we focus on theevaluation of GPT-4's performance on a set of cognitive psychology datasetssuch as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand howGPT-4 processes and integrates cognitive psychology with contextualinformation, providing insight into the underlying cognitive processes thatenable its ability to generate the responses. We show that GPT-4 exhibits ahigh level of accuracy in cognitive psychology tasks relative to the priorstate-of-the-art models. Our results strengthen the already availableassessments and confidence on GPT-4's cognitive psychology abilities. It hassignificant potential to revolutionize the field of AI, by enabling machines tobridge the gap between human and machine reasoning.", "output": "Mind meets machine: Unravelling GPT-4's cognitive psychology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial intelligence (AI) researchers have been developing and refininglarge language models (LLMs) that exhibit remarkable capabilities across avariety of domains and tasks, challenging our understanding of learning andcognition. The latest model developed by OpenAI, GPT-4, was trained using anunprecedented scale of compute and data. In this paper, we report on ourinvestigation of an early version of GPT-4, when it was still in activedevelopment by OpenAI. We contend that (this early version of) GPT-4 is part ofa new cohort of LLMs (along with ChatGPT and Google's PaLM for example) thatexhibit more general intelligence than previous AI models. We discuss therising capabilities and implications of these models. We demonstrate that,beyond its mastery of language, GPT-4 can solve novel and difficult tasks thatspan mathematics, coding, vision, medicine, law, psychology and more, withoutneeding any special prompting. Moreover, in all of these tasks, GPT-4'sperformance is strikingly close to human-level performance, and often vastlysurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4'scapabilities, we believe that it could reasonably be viewed as an early (yetstill incomplete) version of an artificial general intelligence (AGI) system.In our exploration of GPT-4, we put special emphasis on discovering itslimitations, and we discuss the challenges ahead for advancing towards deeperand more comprehensive versions of AGI, including the possible need forpursuing a new paradigm that moves beyond next-word prediction. We concludewith reflections on societal influences of the recent technological leap andfuture research directions.", "output": "Sparks of Artificial General Intelligence: Early experiments with GPT-4."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have demonstrated remarkable capabilities innatural language understanding and generation across various domains, includingmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-artLLM, on medical competency examinations and benchmark datasets. GPT-4 is ageneral-purpose model that is not specialized for medical problems throughtraining or engineered to solve clinical tasks. Our analysis covers two sets ofofficial practice materials for the USMLE, a three-step examination programused to assess clinical competency and grant licensure in the United States. Wealso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyondmeasuring model performance, experiments were conducted to investigate theinfluence of test questions containing both text and images on modelperformance, probe for memorization of content during training, and studyprobability calibration, which is of critical importance in high-stakesapplications like medicine. Our results show that GPT-4, without anyspecialized prompt crafting, exceeds the passing score on USMLE by over 20points and outperforms earlier general-purpose models (GPT-3.5) as well asmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tunedversion of Flan-PaLM 540B). In addition, GPT-4 is significantly bettercalibrated than GPT-3.5, demonstrating a much-improved ability to predict thelikelihood that its answers are correct. We also explore the behavior of themodel qualitatively through a case study that shows the ability of GPT-4 toexplain medical reasoning, personalize explanations to students, andinteractively craft new counterfactual scenarios around a medical case.Implications of the findings are discussed for potential uses of GPT-4 inmedical education, assessment, and clinical practice, with appropriateattention to challenges of accuracy and safety.", "output": "Capabilities of GPT-4 on Medical Challenge Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The incorporation of artificial intelligence (AI) technology, and inparticular natural language processing (NLP), is becoming increasingly vitalfor the development of immersive and interactive metaverse experiences. Onesuch artificial intelligence tool that is gaining traction in the metaverse isChatGPT, a large language model trained by OpenAI. The article delves into thepros and cons of utilizing ChatGPT for metaverse-based education,entertainment, personalization, and support. Dynamic and personalizedexperiences are possible with this technology, but there are also legitimateprivacy, bias, and ethical issues to consider. This article aims to helpreaders understand the possible influence of ChatGPT on the metaverse and howit may be used to effectively create a more immersive and engaging virtualenvironment by evaluating these opportunities and obstacles.", "output": "Unleashing ChatGPT on the Metaverse: Savior or Destroyer?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection is a fundamental task in computer vision, which has beengreatly progressed through developing large and intricate deep learning models.However, the lack of transparency is a big challenge that may not allow thewidespread adoption of these models. Explainable artificial intelligence is afield of research where methods are developed to help users understand thebehavior, decision logics, and vulnerabilities of AI-based systems. Black-boxexplanation refers to explaining decisions of an AI system without havingaccess to its internals. In this paper, we design and implement a black-boxexplanation method named Black-box Object Detection Explanation by Masking(BODEM) through adopting a new masking approach for AI-based object detectionsystems. We propose local and distant masking to generate multiple versions ofan input image. Local masks are used to disturb pixels within a target objectto figure out how the object detector reacts to these changes, while distantmasks are used to assess how the detection model's decisions are affected bydisturbing pixels outside the object. A saliency map is then created byestimating the importance of pixels through measuring the difference betweenthe detection output before and after masking. Finally, a heatmap is createdthat visualizes how important pixels within the input image are to the detectedobjects. The experimentations on various object detection datasets and modelsshowed that BODEM can be effectively used to explain the behavior of objectdetectors and reveal their vulnerabilities. This makes BODEM suitable forexplaining and validating AI based object detection systems in black-boxsoftware testing scenarios. Furthermore, we conducted data augmentationexperiments that showed local masks produced by BODEM can be used for furthertraining the object detectors and improve their detection accuracy androbustness.", "output": "Model-agnostic explainable artificial intelligence for object detection in image data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language is essentially a complex, intricate system of human expressionsgoverned by grammatical rules. It poses a significant challenge to developcapable AI algorithms for comprehending and grasping a language. As a majorapproach, language modeling has been widely studied for language understandingand generation in the past two decades, evolving from statistical languagemodels to neural language models. Recently, pre-trained language models (PLMs)have been proposed by pre-training Transformer models over large-scale corpora,showing strong capabilities in solving various NLP tasks. Since researchershave found that model scaling can lead to performance improvement, they furtherstudy the scaling effect by increasing the model size to an even larger size.Interestingly, when the parameter scale exceeds a certain level, these enlargedlanguage models not only achieve a significant performance improvement but alsoshow some special abilities that are not present in small-scale languagemodels. To discriminate the difference in parameter scale, the researchcommunity has coined the term large language models (LLM) for the PLMs ofsignificant size. Recently, the research on LLMs has been largely advanced byboth academia and industry, and a remarkable progress is the launch of ChatGPT,which has attracted widespread attention from society. The technical evolutionof LLMs has been making an important impact on the entire AI community, whichwould revolutionize the way how we develop and use AI algorithms. In thissurvey, we review the recent advances of LLMs by introducing the background,key findings, and mainstream techniques. In particular, we focus on four majoraspects of LLMs, namely pre-training, adaptation tuning, utilization, andcapacity evaluation. Besides, we also summarize the available resources fordeveloping LLMs and discuss the remaining issues for future directions.", "output": "A Survey of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce HATELEXICON, a lexicon of slurs and targets of hate speech forthe countries of Brazil, Germany, India and Kenya, to aid training andinterpretability of models. We demonstrate how our lexicon can be used tointerpret model predictions, showing that models developed to classify extremespeech rely heavily on target words when making predictions. Further, wepropose a method to aid shot selection for training in low-resource settingsvia HATELEXICON. In few-shot learning, the selection of shots is of paramountimportance to model performance. In our work, we simulate a few-shot settingfor German and Hindi, using HASOC data for training and the MultilingualHateCheck (MHC) as a benchmark. We show that selecting shots based on ourlexicon leads to models performing better on MHC than models trained on shotssampled randomly. Thus, when given only a few training examples, using ourlexicon to select shots containing more sociocultural information leads tobetter few-shot performance.", "output": "Sociocultural knowledge is needed for selection of shots in hate speech detection tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The amount of data has growing significance in exploring cutting-edgematerials and a number of datasets have been generated either by hand orautomated approaches. However, the materials science field struggles toeffectively utilize the abundance of data, especially in applied disciplineswhere materials are evaluated based on device performance rather than theirproperties. This article presents a new natural language processing (NLP) taskcalled structured information inference (SII) to address the complexities ofinformation extraction at the device level in materials science. Weaccomplished this task by tuning GPT-3 on an existing perovskite solar cellFAIR (Findable, Accessible, Interoperable, Reusable) dataset with 91.8%F1-score and extended the dataset with data published since its release. Theproduced data is formatted and normalized, enabling its direct utilization asinput in subsequent data analysis. This feature empowers materials scientiststo develop models by selecting high-quality review articles within theirdomain. Additionally, we designed experiments to predict the electricalperformance of solar cells and design materials or devices with targetedparameters using large language models (LLMs). Our results demonstratecomparable performance to traditional machine learning methods without featureselection, highlighting the potential of LLMs to acquire scientific knowledgeand design new materials akin to materials scientists.", "output": "Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We analyze the data-dependent capacity of neural networks and assessanomalies in inputs from the perspective of networks during inference. Thenotion of data-dependent capacity allows for analyzing the knowledge base of amodel populated by learned features from training data. We define purview asthe additional capacity necessary to characterize inference samples that differfrom the training data. To probe the purview of a network, we utilize gradientsto measure the amount of change required for the model to characterize thegiven inputs more accurately. To eliminate the dependency on ground-truthlabels in generating gradients, we introduce confounding labels that areformulated by combining multiple categorical labels. We demonstrate that ourgradient-based approach can effectively differentiate inputs that cannot beaccurately represented with learned features. We utilize our approach inapplications of detecting anomalous inputs, including out-of-distribution,adversarial, and corrupted samples. Our approach requires no hyperparametertuning or additional data processing and outperforms state-of-the-art methodsby up to 2.7%, 19.8%, and 35.6% of AUROC scores, respectively.", "output": "Probing the Purview of Neural Networks via Gradient Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The retriever-reader framework is popular for open-domain question answering(ODQA), where a retriever samples for the reader a set of relevant candidatepassages from a large corpus. A key assumption behind this method is that highrelevance scores from the retriever likely indicate high answerability from thereader, which implies a high probability that the retrieved passages containanswers to a given question. In this work, we empirically dispel this beliefand observe that recent dense retrieval models based on DPR often rankunanswerable counterfactual passages higher than their answerable originalpassages. To address such answer-unawareness in dense retrievers, we seek touse counterfactual samples as additional training resources to bettersynchronize the relevance measurement of DPR with the answerability ofquestion-passage pairs. Specifically, we present counterfactually-PivotingContrastive Learning (PiCL), a novel representation learning approach forpassage retrieval that leverages counterfactual samples as pivots betweenpositive and negative samples in their learned embedding space. We incorporatePiCL into the retriever training to show the effectiveness of PiCL on ODQAbenchmarks and the robustness of the learned models.", "output": "Revisiting Dense Retrieval with Unanswerable Counterfactuals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The role of an expert in the decision-making process is crucial, as the finalrecommendation depends on his disposition, clarity of mind, experience, andknowledge of the problem. However, the recommendation also depends on theirhonesty. But what if the expert is dishonest? Then, the answer on how difficultit is to manipulate in a given case becomes essential. In the presented work,we consider manipulation of a ranking obtained by comparing alternatives inpairs. More specifically, we propose an algorithm for finding an almost optimalway to swap the positions of two selected alternatives. Thanks to this, it ispossible to determine how difficult such manipulation is in a given case.Theoretical considerations are illustrated by a practical example.", "output": "Almost optimal manipulation of a pair of alternatives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Sat-NeRF, a modified implementation of the recently introducedShadow Neural Radiance Field (S-NeRF) model. This method is able to synthesizenovel views from a sparse set of satellite images of a scene, while accountingfor the variation in lighting present in the pictures. The trained model canalso be used to accurately estimate the surface elevation of the scene, whichis often a desirable quantity for satellite observation applications. S-NeRFimproves on the standard Neural Radiance Field (NeRF) method by considering theradiance as a function of the albedo and the irradiance. Both these quantitiesare output by fully connected neural network branches of the model, and thelatter is considered as a function of the direct light from the sun and thediffuse color from the sky. The implementations were run on a dataset ofsatellite images, augmented using a zoom-and-crop technique. A hyperparameterstudy for NeRF was carried out, leading to intriguing observations on themodel's convergence. Finally, both NeRF and S-NeRF were run until 100k epochsin order to fully fit the data and produce their best possible predictions. Thecode related to this article can be found aturl{", "output": "NeRF applied to satellite imagery for surface reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electronic Health Records (EHR) are generated from clinical routine carerecording valuable information of broad patient populations, which provideplentiful opportunities for improving patient management and interventionstrategies in clinical practice. To exploit the enormous potential of EHR data,a popular EHR data analysis paradigm in machine learning is EHR representationlearning, which first leverages the individual patient's EHR data to learninformative representations by a backbone, and supports diverse health-caredownstream tasks grounded on the representations. Unfortunately, such aparadigm fails to access the in-depth analysis of patients' relevance, which isgenerally known as cohort studies in clinical practice. Specifically, patientsin the same cohort tend to share similar characteristics, implying theirresemblance in medical conditions such as symptoms or diseases. In this paper,we propose a universal COhort Representation lEarning (CORE) framework toaugment EHR utilization by leveraging the fine-grained cohort information amongpatients. In particular, CORE first develops an explicit patient modeling taskbased on the prior knowledge of patients' diagnosis codes, which measures thelatent relevance among patients to adaptively divide the cohorts for eachpatient. Based on the constructed cohorts, CORE recodes the pre-extracted EHRdata representation from intra- and inter-cohort perspectives, yieldingaugmented EHR data representation learning. CORE is readily applicable todiverse backbone models, serving as a universal plug-in framework to infusecohort information into healthcare methods for boosted performance. We conductan extensive experimental evaluation on two real-world datasets, and theexperimental results demonstrate the effectiveness and generalizability ofCORE.", "output": "Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Appropriately regulating artificial intelligence is an increasingly urgentpolicy challenge. Legislatures and regulators lack the specialized knowledgerequired to best translate public demands into legal requirements. Overrelianceon industry self-regulation fails to hold producers and users of AI systemsaccountable to democratic demands. Regulatory markets, in which governmentsrequire the targets of regulation to purchase regulatory services from aprivate regulator, are proposed. This approach to AI regulation could overcomethe limitations of both command-and-control regulation and self-regulation.Regulatory market could enable governments to establish policy priorities forthe regulation of AI, whilst relying on market forces and industry R&amp;D effortsto pioneer the methods of regulation that best achieve policymakers' statedobjectives.", "output": "Regulatory Markets: The Future of AI Governance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent large-scale generative modeling has attained unprecedentedperformance especially in producing high-fidelity images driven by textprompts. Text inversion (TI), alongside the text-to-image model backbones, isproposed as an effective technique in personalizing the generation when theprompts contain user-defined, unseen or long-tail concept tokens. Despite that,we find and show that the deployment of TI remains full of \"dark-magics\" -- toname a few, the harsh requirement of additional datasets, arduous human effortsin the loop and lack of robustness. In this work, we propose a much-enhancedversion of TI, dubbed Controllable Textual Inversion (COTI), in resolving allthe aforementioned problems and in turn delivering a robust, data-efficient andeasy-to-use framework. The core to COTI is a theoretically-guided lossobjective instantiated with a comprehensive and novel weighted scoringmechanism, encapsulated by an active-learning paradigm. The extensive resultsshow that COTI significantly outperforms the prior TI-related approaches with a26.05 decrease in the FID score and a 23.00% boost in the R-precision.", "output": "Controllable Textual Inversion for Personalized Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised point cloud shape correspondence aims to obtain densepoint-to-point correspondences between point clouds without manually annotatedpairs. However, humans and some animals have bilateral symmetry and variousorientations, which lead to severe mispredictions of symmetrical parts.Besides, point cloud noise disrupts consistent representations for point cloudand thus degrades the shape correspondence accuracy. To address the aboveissues, we propose a Self-Ensembling ORientation-aware Network termed SE-ORNet.The key of our approach is to exploit an orientation estimation module with adomain adaptive discriminator to align the orientations of point cloud pairs,which significantly alleviates the mispredictions of symmetrical parts.Additionally, we design a selfensembling framework for unsupervised point cloudshape correspondence. In this framework, the disturbances of point cloud noiseare overcome by perturbing the inputs of the student and teacher networks withdifferent data augmentations and constraining the consistency of predictions.Extensive experiments on both human and animal datasets show that our SE-ORNetcan surpass state-of-the-art unsupervised point cloud shape correspondencemethods.", "output": "SE-ORNet: Self-Ensembling Orientation-aware Network for Unsupervised Point Cloud Shape Correspondence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Foundation models have taken over natural language processing and imagegeneration domains due to the flexibility of prompting. With the recentintroduction of the Segment Anything Model (SAM), this prompt-driven paradigmhas entered image segmentation with a hitherto unexplored abundance ofcapabilities. The purpose of this paper is to conduct an initial evaluation ofthe out-of-the-box zero-shot capabilities of SAM for medical imagesegmentation, by evaluating its performance on an abdominal CT organsegmentation task, via point or bounding box based prompting. We show that SAMgeneralizes well to CT data, making it a potential catalyst for the advancementof semi-automatic segmentation tools for clinicians. We believe that thisfoundation model, while not reaching state-of-the-art segmentation performancein our investigations, can serve as a highly potent starting point for furtheradaptations of such models to the intricacies of the medical domain. Keywords:medical image segmentation, SAM, foundation models, zero-shot learning", "output": "SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The transferability of adversarial examples is a crucial aspect of evaluatingthe robustness of deep learning systems, particularly in black-box scenarios.Although several methods have been proposed to enhance cross-modeltransferability, little attention has been paid to the transferability ofadversarial examples across different tasks. This issue has become increasinglyrelevant with the emergence of foundational multi-task AI systems such asVisual ChatGPT, rendering the utility of adversarial samples generated by asingle task relatively limited. Furthermore, these systems often entailinferential functions beyond mere recognition-like tasks. To address this gap,we propose a novel Visual Relation-based cross-task Adversarial Patchgeneration method called VRAP, which aims to evaluate the robustness of variousvisual tasks, especially those involving visual reasoning, such as VisualQuestion Answering and Image Captioning. VRAP employs scene graphs to combineobject recognition-based deception with predicate-based relations elimination,thereby disrupting the visual reasoning information shared among inferentialtasks. Our extensive experiments demonstrate that VRAP significantly surpassesprevious methods in terms of black-box transferability across diverse visualreasoning tasks.", "output": "Boosting Cross-task Transferability of Adversarial Patches with Visual Relations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sign Language Recognition (SLR) systems aim to be embedded in video streamplatforms to recognize the sign performed in front of a camera. SLR researchhas taken advantage of recent advances in pose estimation models to useskeleton sequences estimated from videos instead of RGB information to predictsigns. This approach can make HAR-related tasks less complex and more robust todiverse backgrounds, lightning conditions, and physical appearances. In thiswork, we explore the use of a spatio-temporal skeleton representation such asTree Structure Skeleton Image (TSSI) as an alternative input to improve theaccuracy of skeleton-based models for SLR. TSSI converts a skeleton sequenceinto an RGB image where the columns represent the joints of the skeleton in adepth-first tree traversal order, the rows represent the temporal evolution ofthe joints, and the three channels represent the (x, y, z) coordinates of thejoints. We trained a DenseNet-121 using this type of input and compared it withother skeleton-based deep learning methods using a large-scale American SignLanguage (ASL) dataset, WLASL. Our model (SL-TSSI-DenseNet) overcomes thestate-of-the-art of other skeleton-based models. Moreover, when including dataaugmentation our proposal achieves better results than both skeleton-based andRGB-based models. We evaluated the effectiveness of our model on the AnkaraUniversity Turkish Sign Language (TSL) dataset, AUTSL, and a Mexican SignLanguage (LSM) dataset. On the AUTSL dataset, the model achieves similarresults to the state-of-the-art of other skeleton-based models. On the LSMdataset, the model achieves higher results than the baseline. Code has beenmade available at: ", "output": "Isolated Sign Language Recognition based on Tree Structure Skeleton Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the past few years, Differentiable Neural Architecture Search (DNAS)rapidly imposed itself as the trending approach to automate the discovery ofdeep neural network architectures. This rise is mainly due to the popularity ofDARTS, one of the first major DNAS methods. In contrast with previous worksbased on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster byseveral orders of magnitude and uses fewer computational resources. In thiscomprehensive survey, we focus specifically on DNAS and review recentapproaches in this field. Furthermore, we propose a novel challenge-basedtaxonomy to classify DNAS methods. We also discuss the contributions brought toDNAS in the past few years and its impact on the global NAS field. Finally, weconclude by giving some insights into future research directions for the DNASfield.", "output": "Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present MONET, a new multimodal dataset captured using a thermal cameramounted on a drone that flew over rural areas, and recorded human and vehicleactivities. We captured MONET to study the problem of object localisation andbehaviour understanding of targets undergoing large-scale variations and beingrecorded from different and moving viewpoints. Target activities occur in twodifferent land sites, each with unique scene structures and clutteredbackgrounds. MONET consists of approximately 53K images featuring 162K manuallyannotated bounding boxes. Each image is timestamp-aligned with drone metadatathat includes information about attitudes, speed, altitude, and GPScoordinates. MONET is different from previous thermal drone datasets because itfeatures multimodal data, including rural scenes captured with thermal camerascontaining both person and vehicle targets, along with trajectory informationand metadata. We assessed the difficulty of the dataset in terms of transferlearning between the two sites and evaluated nine object detection algorithmsto identify the open challenges associated with this type of data. Projectpage: ", "output": "The MONET dataset: Multimodal drone thermal dataset recorded in rural scenarios."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conventional image sensors digitize high-resolution images at fast framerates, producing a large amount of data that needs to be transmitted off thesensor for further processing. This is challenging for perception systemsoperating on edge devices, because communication is power inefficient andinduces latency. Fueled by innovations in stacked image sensor fabrication,emerging sensor-processors offer programmability and minimal processingcapabilities directly on the sensor. We exploit these capabilities bydeveloping an efficient recurrent neural network architecture, PixelRNN, thatencodes spatio-temporal features on the sensor using purely binary operations.PixelRNN reduces the amount of data to be transmitted off the sensor by afactor of 64x compared to conventional systems while offering competitiveaccuracy for hand gesture recognition and lip reading tasks. We experimentallyvalidate PixelRNN using a prototype implementation on the SCAMP-5sensor-processor platform.", "output": "PixelRNN: In-pixel Recurrent Neural Networks for End-to-end-optimized Perception with Neural Sensors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional neural networks (CNN) have become the predominant model forimage segmentation tasks. Most CNN segmentation architectures resize spatialdimensions by a fixed factor of two to aggregate spatial context. Recent workhas explored using other resizing factors to improve model accuracy forspecific applications. However, finding the appropriate rescaling factor mostoften involves training a separate network for many different factors andcomparing the performance of each model. The computational burden of thesemodels means that in practice it is rarely done, and when done only a fewdifferent scaling factors are considered.In this work, we present a hypernetwork strategy that can be used to easilyand rapidly generate the Pareto frontier for the trade-off between accuracy andefficiency as the rescaling factor varies. We show how to train a singlehypernetwork that generates CNN parameters conditioned on a rescaling factor.This enables a user to quickly choose a rescaling factor that appropriatelybalances accuracy and computational efficiency for their particular needs. Wefocus on image segmentation tasks, and demonstrate the value of this approachacross various domains. We also find that, for a given rescaling factor, oursingle hypernetwork outperforms CNNs trained with fixed rescaling factors.", "output": "Amortized Learning of Dynamic Feature Scaling for Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clouds and haze often occlude optical satellite images, hindering continuous,dense monitoring of the Earth's surface. Although modern deep learning methodscan implicitly learn to ignore such occlusions, explicit cloud removal aspre-processing enables manual interpretation and allows training models whenonly few annotations are available. Cloud removal is challenging due to thewide range of occlusion scenarios -- from scenes partially visible throughhaze, to completely opaque cloud coverage. Furthermore, integratingreconstructed images in downstream applications would greatly benefit fromtrustworthy quality assessment. In this paper, we introduce UnCRtainTS, amethod for multi-temporal cloud removal combining a novel attention-basedarchitecture, and a formulation for multivariate uncertainty prediction. Thesetwo components combined set a new state-of-the-art performance in terms ofimage reconstruction on two public cloud removal datasets. Additionally, weshow how the well-calibrated predicted uncertainties enable a precise controlof the reconstruction quality.", "output": "UnCRtainTS: Uncertainty Quantification for Cloud Removal in Optical Satellite Time Series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The burgeoning field of camouflaged object detection (COD) seeks to identifyobjects that blend into their surroundings. Despite the impressive performanceof recent models, we have identified a limitation in their robustness, whereexisting methods may misclassify salient objects as camouflaged ones, despitethese two characteristics being contradictory. This limitation may stem fromlacking multi-pattern training images, leading to less saliency robustness. Toaddress this issue, we introduce CamDiff, a novel approach inspired byAI-Generated Content (AIGC) that overcomes the scarcity of multi-patterntraining images. Specifically, we leverage the latent diffusion model tosynthesize salient objects in camouflaged scenes, while using the zero-shotimage classification ability of the Contrastive Language-Image Pre-training(CLIP) model to prevent synthesis failures and ensure the synthesized objectaligns with the input prompt. Consequently, the synthesized image retains itsoriginal camouflage label while incorporating salient objects, yieldingcamouflage samples with richer characteristics. The results of user studiesshow that the salient objects in the scenes synthesized by our frameworkattract the user's attention more; thus, such samples pose a greater challengeto the existing COD models. Our approach enables flexible editing and efficientlarge-scale dataset generation at a low cost. It significantly enhances CODbaselines' training and testing phases, emphasizing robustness across diversedomains. Our newly-generated datasets and source code are available at", "output": "CamDiff: Camouflage Image Augmentation via Diffusion Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Physically-based rendering (PBR) is key for immersive rendering effects usedwidely in the industry to showcase detailed realistic scenes from computergraphics assets. A well-known caveat is that producing the same iscomputationally heavy and relies on complex capture devices. Inspired by thesuccess in quality and efficiency of recent volumetric neural rendering, wewant to develop a physically-based neural shader to eliminate device dependencyand significantly boost performance. However, no existing lighting and materialmodels in the current neural rendering approaches can accurately represent thecomprehensive lighting models and BRDFs properties required by the PBR process.Thus, this paper proposes a novel lighting representation that models directand indirect light locally through a light sampling strategy in a learned lightsampling field. We also propose BRDF models to separately representsurface/subsurface scattering details to enable complex objects such astranslucent material (i.e., skin, jade). We then implement our proposedrepresentations with an end-to-end physically-based neural face skin shader,which takes a standard face asset (i.e., geometry, albedo map, and normal map)and an HDRI for illumination as inputs and generates a photo-realisticrendering as output. Extensive experiments showcase the quality and efficiencyof our PBR face skin shader, indicating the effectiveness of our proposedlighting and material representations.", "output": "Light Sampling Field and BRDF Representation for Physically-based Neural Rendering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computational Pathology (CoPath) is an interdisciplinary science thataugments developments of computational approaches to analyze and model medicalhistopathology images. The main objective for CoPath is to developinfrastructure and workflows of digital diagnostics as an assistive CAD systemfor clinical pathology facilitating transformational changes in the diagnosisand treatment of cancer diseases. With evergrowing developments in deeplearning and computer vision algorithms, and the ease of the data flow fromdigital pathology, currently CoPath is witnessing a paradigm shift. Despite thesheer volume of engineering and scientific works being introduced for cancerimage analysis, there is still a considerable gap of adopting and integratingthese algorithms in clinical practice. This raises a significant questionregarding the direction and trends that are undertaken in CoPath. In thisarticle we provide a comprehensive review of more than 700 papers to addressthe challenges faced in problem design all-the-way to the application andimplementation viewpoints. We have catalogued each paper into a model-card byexamining the key works and challenges faced to layout the current landscape inCoPath. We hope this helps the community to locate relevant works andfacilitate understanding of the field's future directions. In a nutshell, weoversee the CoPath developments in cycle of stages which are required to becohesively linked together to address the challenges associated with suchmultidisciplinary science. We overview this cycle from different perspectivesof data-centric, model-centric, and application-centric problems. We finallysketch remaining challenges and provide directions for future technicaldevelopments and clinical integration of CoPath.", "output": "Computational Pathology: A Survey Review and The Way Forward."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixture of Experts (MoE) are rising in popularity as a means to trainextremely large-scale models, yet allowing for a reasonable computational costat inference time. Recent state-of-the-art approaches usually assume a largenumber of experts, and require training all experts jointly, which often leadto training instabilities such as the router collapsing In contrast, in thiswork, we propose to revisit the simple single-gate MoE, which allows for morepractical training. Key to our work are (i) a base model branch acting both asan early-exit and an ensembling regularization scheme, (ii) a simple andefficient asynchronous training pipeline without router collapse issues, andfinally (iii) a per-sample clustering-based initialization. We showexperimentally that the proposed model obtains efficiency-to-accuracytrade-offs comparable with other more complex MoE, and outperforms non-mixturebaselines. This showcases the merits of even a simple single-gate MoE, andmotivates further exploration in this area.", "output": "Revisiting Single-gated Mixtures of Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a self-supervised shared encoder model that achieves strongresults on several visual, language and multimodal benchmarks while being data,memory and run-time efficient. We make three key contributions. First, incontrast to most existing works, we use a single transformer with all theencoder layers processing both the text and the image modalities. Second, wepropose a stage-wise training strategy where the model is first trained onimages, then jointly with unimodal text and image datasets and finally jointlywith text and text-image datasets. Third, to preserve information across boththe modalities, we propose a training pipeline that learns simultaneously fromgradient updates of different modalities at each training update step. Theresults on downstream text-only, image-only and multimodal tasks show that ourmodel is competitive with several strong models while using fewer parametersand lesser pre-training data. For example, MoMo performs competitively withFLAVA on multimodal (+3.1), image-only (+1.1) and text-only (-0.1) tasksdespite having 2/5th the number of parameters and using 1/3rd the image-texttraining pairs. Finally, we ablate various design choices and further show thatincreasing model size produces significant performance gains indicatingpotential for substantial improvements with larger models using our approach.", "output": "MoMo: A shared encoder Model for text, image and multi-Modal representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate camera-to-lidar calibration is a requirement for sensor data fusionin many 3D perception tasks. In this paper, we present SceneCalib, a novelmethod for simultaneous self-calibration of extrinsic and intrinsic parametersin a system containing multiple cameras and a lidar sensor. Existing methodstypically require specially designed calibration targets and human operators,or they only attempt to solve for a subset of calibration parameters. Weresolve these issues with a fully automatic method that requires no explicitcorrespondences between camera images and lidar point clouds, allowing forrobustness to many outdoor environments. Furthermore, the full system isjointly calibrated with explicit cross-camera constraints to ensure thatcamera-to-camera and camera-to-lidar extrinsic parameters are consistent.", "output": "SceneCalib: Automatic Targetless Calibration of Cameras and Lidars in Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image classifiers are information-discarding machines, by design. Yet, howthese models discard information remains mysterious. We hypothesize that oneway for image classifiers to reach high accuracy is to first zoom to the mostdiscriminative region in the image and then extract features from there topredict image labels. We study six popular networks ranging from AlexNet toCLIP and find that proper framing of the input image can lead to the correctclassification of 98.91% of ImageNet images. Furthermore, we explore thepotential and limits of zoom transforms in image classification and uncoverpositional biases in various datasets, especially a strong center bias in twopopular datasets: ImageNet-A and ObjectNet. Finally, leveraging our insightsinto the potential of zoom, we propose a state-of-the-art test-timeaugmentation (TTA) technique that improves classification accuracy by forcingmodels to explicitly perform zoom-in operations before making predictions. Ourmethod is more interpretable, accurate, and faster than MEMO, astate-of-the-art TTA method. Additionally, we propose ImageNet-Hard, a newbenchmark where zooming in alone often does not help state-of-the-art modelsbetter label images.", "output": "Zoom is what you need: An empirical study of the power of zoom and spatial biases in image classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of continual learning has attracted rising attention in recentyears. However, few works have questioned the commonly used learning setup,based on a task curriculum of random class. This differs significantly fromhuman continual learning, which is guided by taxonomic curricula. In this work,we propose the Taxonomic Class Incremental Learning (TCIL) problem. In TCIL,the task sequence is organized based on a taxonomic class tree. We unifyexisting approaches to CIL and taxonomic learning as parameter inheritanceschemes and introduce a new such scheme for the TCIL learning. This enables theincremental transfer of knowledge from ancestor to descendant class of a classtaxonomy through parameter inheritance. Experiments on CIFAR-100 andImageNet-100 show the effectiveness of the proposed TCIL method, whichoutperforms existing SOTA methods by 2% in terms of final accuracy on CIFAR-100and 3% on ImageNet-100.", "output": "Taxonomic Class Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human pose estimation has seen widespread use of transformer models in recentyears. Pose transformers benefit from the self-attention map, which capturesthe correlation between human joint tokens and the image. However, trainingsuch models is computationally expensive. The recent token-Pruned PoseTransformer (PPT) solves this problem by pruning the background tokens of theimage, which are usually less informative. However, although it improvesefficiency, PPT inevitably leads to worse performance than TokenPose due to thepruning of tokens. To overcome this problem, we present a novel method calledDistilling Pruned-Token Transformer for human pose estimation (DPPT). Ourmethod leverages the output of a pre-trained TokenPose to supervise thelearning process of PPT. We also establish connections between the internalstructure of pose transformers and PPT, such as attention maps and jointfeatures. Our experimental results on the MPII datasets show that our DPPT cansignificantly improve PCK compared to previous PPT models while still reducingcomputational complexity.", "output": "Distilling Token-Pruned Pose Transformer for 2D Human Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic neural network is an emerging research topic in deep learning. Withadaptive inference, dynamic models can achieve remarkable accuracy andcomputational efficiency. However, it is challenging to design a powerfuldynamic detector, because of no suitable dynamic architecture and exitingcriterion for object detection. To tackle these difficulties, we propose adynamic framework for object detection, named DynamicDet. Firstly, we carefullydesign a dynamic architecture based on the nature of the object detection task.Then, we propose an adaptive router to analyze the multi-scale information andto decide the inference route automatically. We also present a noveloptimization strategy with an exiting criterion based on the detection lossesfor our dynamic detectors. Last, we present a variable-speed inferencestrategy, which helps to realize a wide range of accuracy-speed trade-offs withonly one dynamic detector. Extensive experiments conducted on the COCObenchmark demonstrate that the proposed DynamicDet achieves newstate-of-the-art accuracy-speed trade-offs. For instance, with comparableaccuracy, the inference speed of our dynamic detector Dy-YOLOv7-W6 surpassesYOLOv7-E6 by 12%, YOLOv7-D6 by 17%, and YOLOv7-E6E by 39%. The code isavailable at ", "output": "DynamicDet: A Unified Dynamic Architecture for Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent researches on unsupervised person re-identification~(reID) havedemonstrated that pre-training on unlabeled person images achieves superiorperformance on downstream reID tasks than pre-training on ImageNet. However,those pre-trained methods are specifically designed for reID and sufferflexible adaption to other pedestrian analysis tasks. In this paper, we proposeVAL-PAT, a novel framework that learns transferable representations to enhancevarious pedestrian analysis tasks with multimodal information. To train ourframework, we introduce three learning objectives, emph{i.e.,} self-supervisedcontrastive learning, image-text contrastive learning and multi-attributeclassification. The self-supervised contrastive learning facilitates thelearning of the intrinsic pedestrian properties, while the image-textcontrastive learning guides the model to focus on the appearance information ofpedestrians.Meanwhile, multi-attribute classification encourages the model torecognize attributes to excavate fine-grained pedestrian information. We firstperform pre-training on LUPerson-TA dataset, where each image contains text andattribute annotations, and then transfer the learned representations to variousdownstream tasks, including person reID, person attribute recognition andtext-based person search. Extensive experiments demonstrate that our frameworkfacilitates the learning of general pedestrian representations and thus leadsto promising results on various pedestrian analysis tasks.", "output": "Learning Transferable Pedestrian Representation from Multimodal Information Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, panoramic images can be easily obtained by panoramic cameras.However, when the panoramic camera orientation is tilted, a non-uprightpanoramic image will be captured. Existing upright adjustment models focus onhow to estimate more accurate camera orientation, and attribute imagereconstruction to offline or post-processing tasks. To this end, we propose anonline end-to-end network for upright adjustment. Our network is designed toreconstruct the image while finding the angle. Our network consists of threemodules: orientation estimation, LUT online generation, and uprightreconstruction. Direction estimation estimates the tilt angle of the panoramicimage. Then, a converter block with upsampling function is designed to generateangle to LUT. This module can output corresponding online LUT for differentinput angles. Finally, a lightweight generative adversarial network (GAN) aimsto generate upright images from shallow features. The experimental results showthat in terms of angles, we have improved the accuracy of small angle errors.In terms of image reconstruction, In image reconstruction, we have achieved thefirst real-time online upright reconstruction of panoramic images using deeplearning networks.", "output": "An End-to-End Network for Upright Adjustment of Panoramic Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Biometric authentication service providers often claim that it is notpossible to reverse-engineer a user's raw biometric sample, such as afingerprint or a face image, from its mathematical (feature-space)representation. In this paper, we investigate this claim on the specificexample of deep neural network (DNN) embeddings. Inversion of DNN embeddingshas been investigated for explaining deep image representations or synthesizingnormalized images. Existing studies leverage full access to all layers of theoriginal model, as well as all possible information on the original dataset.For the biometric authentication use case, we need to investigate this underadversarial settings where an attacker has access to a feature-spacerepresentation but no direct access to the exact original dataset nor theoriginal learned model. Instead, we assume varying degree of attacker'sbackground knowledge about the distribution of the dataset as well as theoriginal learned model (architecture and training process). In these cases, weshow that the attacker can exploit off-the-shelf DNN models and publicdatasets, to mimic the behaviour of the original learned model to varyingdegrees of success, based only on the obtained representation and attacker'sprior knowledge. We propose a two-pronged attack that first infers the originalDNN by exploiting the model footprint on the embedding, and then reconstructsthe raw data by using the inferred model. We show the practicality of theattack on popular DNNs trained for two prominent biometric modalities, face andfingerprint recognition. The attack can effectively infer the originalrecognition model (mean accuracy 83% for faces, 86% for fingerprints), andcan craft effective biometric reconstructions that are successfullyauthenticated with 1-vs-1 authentication accuracy of up to 92% for somemodels.", "output": "On the Adversarial Inversion of Deep Biometric Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optical aberrations of optical systems cause significant degradation ofimaging quality. Aberration correction by sophisticated lens designs andspecial glass materials generally incurs high cost of manufacturing and theincrease in the weight of optical systems, thus recent work has shifted toaberration correction with deep learning-based post-processing. Thoughreal-world optical aberrations vary in degree, existing methods cannoteliminate variable-degree aberrations well, especially for the severe degreesof degradation. Also, previous methods use a single feed-forward neural networkand suffer from information loss in the output. To address the issues, wepropose a novel aberration correction method with an invertible architecture byleveraging its information-lossless property. Within the architecture, wedevelop conditional invertible blocks to allow the processing of aberrationswith variable degrees. Our method is evaluated on both a synthetic dataset fromphysics-based imaging simulation and a real captured dataset. Quantitative andqualitative experimental results demonstrate that our method outperformscompared methods in correcting variable-degree optical aberrations.", "output": "Neural Invertible Variable-degree Optical Aberrations Correction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study aims to determine a predictive model to learn students probabilityto pass their courses taken at the earliest stage of the semester. Tosuccessfully discover a good predictive model with high acceptability,accurate, and precision rate which delivers a useful outcome for decisionmaking in education systems, in improving the processes of conveying knowledgeand uplifting students academic performance, the proponent applies and strictlyfollowed the CRISP-DM (Cross-Industry Standard Process for Data Mining)methodology. This study employs classification for data mining techniques, anddecision tree for algorithm. With the utilization of the newly discoveredpredictive model, the prediction of students probabilities to pass the currentcourses they take gives 0.7619 accuracy, 0.8333 precision, 0.8823 recall, and0.8571 f1 score, which shows that the model used in the prediction is reliable,accurate, and recommendable. Considering the indicators and the results, it canbe noted that the prediction model used in this study is highly acceptable. Thedata mining techniques provides effective and efficient innovative tools inanalyzing and predicting student performances. The model used in this studywill greatly affect the way educators understand and identify the weakness oftheir students in the class, the way they improved the effectiveness of theirlearning processes gearing to their students, bring down academic failurerates, and help institution administrators modify their learning systemoutcomes. Further study for the inclusion of some students demographicinformation, vast amount of data within the dataset, automated and manualprocess of predictive criteria indicators where the students can regulate towhich criteria, they must improve more for them to pass their courses taken atthe end of the semester as early as midterm period are highly needed.", "output": "A Predictive Model using Machine Learning Algorithm in Identifying Students Probability on Passing Semestral Course."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Scene text editing is a challenging task that involves modifying or insertingspecified texts in an image while maintaining its natural and realisticappearance. Most previous approaches to this task rely on style-transfer modelsthat crop out text regions and feed them into image transfer models, such asGANs. However, these methods are limited in their ability to change text styleand are unable to insert texts into images. Recent advances in diffusion modelshave shown promise in overcoming these limitations with text-conditional imageediting. However, our empirical analysis reveals that state-of-the-artdiffusion models struggle with rendering correct text and controlling textstyle. To address these problems, we propose DIFFSTE to improve pre-traineddiffusion models with a dual encoder design, which includes a character encoderfor better text legibility and an instruction encoder for better style control.An instruction tuning framework is introduced to train our model to learn themapping from the text instruction to the corresponding image with either thespecified style or the style of the surrounding texts in the background. Such atraining method further brings our method the zero-shot generalization abilityto the following three scenarios: generating text with unseen font variation,e.g., italic and bold, mixing different fonts to construct a new font, andusing more relaxed forms of natural language as the instructions to guide thegeneration task. We evaluate our approach on five datasets and demonstrate itssuperior performance in terms of text correctness, image naturalness, and stylecontrollability. Our code is publicly available.", "output": "Improving Diffusion Models for Scene Text Editing with Dual Encoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Camera localization is a classical computer vision task that serves variousArtificial Intelligence and Robotics applications. With the rapid developmentsof Deep Neural Networks (DNNs), end-to-end visual localization methods areprosperous in recent years. In this work, we focus on the scene coordinateprediction ones and propose a network architecture named as Structure GuidanceLearning (SGL) which utilizes the receptive branch and the structure branch toextract both high-level and low-level features to estimate the 3D coordinates.We design a confidence strategy to refine and filter the predicted 3Dobservations, which enables us to estimate the camera poses by employing thePerspective-n-Point (PnP) with RANSAC. In the training part, we design theBundle Adjustment trainer to help the network fit the scenes better.Comparisons with some state-of-the-art (SOTA) methods and sufficient ablationexperiments confirm the validity of our proposed architecture.", "output": "SGL: Structure Guidance Learning for Camera Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider solving ill-posed imaging inverse problems without access to animage prior or ground-truth examples. An overarching challenge in these inverseproblems is that an infinite number of images, including many that areimplausible, are consistent with the observed measurements. Thus, image priorsare required to reduce the space of possible solutions to more desireablereconstructions. However, in many applications it is difficult or potentiallyimpossible to obtain example images to construct an image prior. Henceinaccurate priors are often used, which inevitably result in biased solutions.Rather than solving an inverse problem using priors that encode the spatialstructure of any one image, we propose to solve a set of inverse problemsjointly by incorporating prior constraints on the collective structure of theunderlying images. The key assumption of our work is that the underlying imageswe aim to reconstruct share common, low-dimensional structure. We show thatsuch a set of inverse problems can be solved simultaneously without the use ofa spatial image prior by instead inferring a shared image generator with alow-dimensional latent space. The parameters of the generator and latentembeddings are found by maximizing a proxy for the Evidence Lower Bound (ELBO).Once identified, the generator and latent embeddings can be combined to providereconstructed images for each inverse problem. The framework we propose canhandle general forward model corruptions, and we show that measurements derivedfrom only a small number of ground-truth images ($leqslant 150$) aresufficient for \"prior-free\" image reconstruction. We demonstrate our approachon a variety of convex and non-convex inverse problems, ranging from denoising,phase retrieval, and black hole video reconstruction.", "output": "Ill-Posed Image Reconstruction Without an Image Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Audiovisual representation learning typically relies on the correspondencebetween sight and sound. However, there are often multiple audio tracks thatcan correspond with a visual scene. Consider, for example, differentconversations on the same crowded street. The effect of such counterfactualpairs on audiovisual representation learning has not been previously explored.To investigate this, we use dubbed versions of movies to augment cross-modalcontrastive learning. Our approach learns to represent alternate audio tracks,differing only in speech content, similarly to the same video. Our results showthat dub-augmented training improves performance on a range of auditory andaudiovisual tasks, without significantly affecting linguistic task performanceoverall. We additionally compare this approach to a strong baseline where weremove speech before pretraining, and find that dub-augmented training is moreeffective, including for paralinguistic and audiovisual tasks where speechremoval leads to worse performance. These findings highlight the importance ofconsidering speech variation when learning scene-level audiovisualcorrespondences and suggest that dubbed audio can be a useful augmentationtechnique for training audiovisual models toward more robust performance.", "output": "Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "77% of adults over 50 want to age in place today, presenting a majorchallenge to ensuring adequate nutritional intake. It has been reported thatone in four older adults that are 65 years or older are malnourished and giventhe direct link between malnutrition and decreased quality of life, there havebeen numerous studies conducted on how to efficiently track nutritional intakeof food. Recent advancements in machine learning and computer vision showpromise of automated nutrition tracking methods of food, but require a largehigh-quality dataset in order to accurately identify the nutrients from thefood on the plate. Unlike existing datasets, a collection of 3D models withnutritional information allow for view synthesis to create an infinite numberof 2D images for any given viewpoint/camera angle along with the associatednutritional information. In this paper, we develop a methodology for collectinghigh-quality 3D models for food items with a particular focus on speed andconsistency, and introduce NutritionVerse-3D, a large-scale high-qualityhigh-resolution dataset of 105 3D food models, in conjunction with theirassociated weight, food name, and nutritional value. These models allow forlarge quantity food intake scenes, diverse and customizable scene layout, andan infinite number of camera settings and lighting conditions.NutritionVerse-3D is publicly available as a part of an open initiative toaccelerate machine learning for nutrition sensing.", "output": "NutritionVerse-3D: A 3D Food Model Dataset for Nutritional Intake Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the growth in capabilities of generative models, there has been growinginterest in using photo-realistic renders of common 3D food items to improvedownstream tasks such as food printing, nutrition prediction, or management offood wastage. Despite 3D modelling capabilities being more accessible than everdue to the success of NeRF based view-synthesis, such rendering methods stillstruggle to correctly capture thin food objects, often generating meshes withsignificant holes. In this study, we present an optimized strategy for enablingimproved rendering of thin 3D food models, and demonstrate qualitativeimprovements in rendering quality. Our method generates the 3D model mesh via aproposed thin-object-optimized differentiable reconstruction method and tailorsthe strategy at both the data collection and training stages to better handlethin objects. While simple, we find that this technique can be employed forquick and highly consistent capturing of thin 3D objects.", "output": "NutritionVerse-Thin: An Optimized Strategy for Enabling Improved Rendering of 3D Thin Food Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Segment Anything Model (SAM) is a new image segmentation tool trainedwith the largest segmentation dataset at this time. The model has demonstratedthat it can create high-quality masks for image segmentation with goodpromptability and generalizability. However, the performance of the model onmedical images requires further validation. To assist with the development,assessment, and utilization of SAM on medical images, we introduce Segment AnyMedical Model (SAMM), an extension of SAM on 3D Slicer, a widely-usedopen-source image processing and visualization software that has beenextensively used in the medical imaging community. This open-source extensionto 3D Slicer and its demonstrations are posted on GitHub( SAMM achieves 0.6-second latency of acomplete cycle and can infer image masks in nearly real-time.", "output": "SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, a new form of magnetic resonance imaging (MRI) called syntheticcorrelated diffusion (CDI$^s$) imaging was introduced and showed considerablepromise for clinical decision support for cancers such as prostate cancer whencompared to current gold-standard MRI techniques. However, the efficacy forCDI$^s$ for other forms of cancers such as breast cancer has not been aswell-explored nor have CDI$^s$ data been previously made publicly available.Motivated to advance efforts in the development of computer-aided clinicaldecision support for breast cancer using CDI$^s$, we introduce Cancer-Net BCa,a multi-institutional open-source benchmark dataset of volumetric CDI$^s$imaging data of breast cancer patients. Cancer-Net BCa contains CDI$^s$volumetric images from a pre-treatment cohort of 253 patients across teninstitutions, along with detailed annotation metadata (the lesion type, geneticsubtype, longest diameter on the MRI (MRLD), the Scarff-Bloom-Richardson (SBR)grade, and the post-treatment breast cancer pathologic complete response (pCR)to neoadjuvant chemotherapy). We further examine the demographic and tumourdiversity of the Cancer-Net BCa dataset to gain deeper insights into potentialbiases. Cancer-Net BCa is publicly available as a part of a global open-sourceinitiative dedicated to accelerating advancement in machine learning to aidclinicians in the fight against cancer.", "output": "A Multi-Institutional Open-Source Benchmark Dataset for Breast Cancer Clinical Decision Support using Synthetic Correlated Diffusion Imaging Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Movie story analysis requires understanding characters' emotions and mentalstates. Towards this goal, we formulate emotion understanding as predicting adiverse and multi-label set of emotions at the level of a movie scene and foreach character. We propose EmoTx, a multimodal Transformer-based architecturethat ingests videos, multiple characters, and dialog utterances to make jointpredictions. By leveraging annotations from the MovieGraphs dataset, we aim topredict classic emotions (e.g. happy, angry) and other mental states (e.g.honest, helpful). We conduct experiments on the most frequently occurring 10and 25 labels, and a mapping that clusters 181 labels to 26. Ablation studiesand comparison against adapted state-of-the-art emotion recognition approachesshows the effectiveness of EmoTx. Analyzing EmoTx's self-attention scoresreveals that expressive emotions often look at character tokens while othermental states rely on video and dialog cues.", "output": "How you feelin'? Learning Emotions and Mental States in Movie Scenes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) enables multiple sites to collaboratively trainpowerful deep models without compromising data privacy and security. Thestatistical heterogeneity (e.g., non-IID data and domain shifts) is a primaryobstacle in FL, impairing the generalization performance of the global model.Weakly supervised segmentation, which uses sparsely-grained (i.e., point-,bounding box-, scribble-, block-wise) supervision, is increasingly being paidattention to due to its great potential of reducing annotation costs. However,there may exist label heterogeneity, i.e., different annotation forms acrosssites. In this paper, we propose a novel personalized FL framework for medicalimage segmentation, named FedICRA, which uniformly leverages heterogeneous weaksupervision via adaptIve Contrastive Representation and Aggregation.Concretely, to facilitate personalized modeling and to avoid confusion, achannel selection based site contrastive representation module is employed toadaptively cluster intra-site embeddings and separate inter-site ones. Toeffectively integrate the common knowledge from the global model with theunique knowledge from each local model, an adaptive aggregation module isapplied for updating and initializing local models at the element level.Additionally, a weakly supervised objective function that leverages amultiscale tree energy loss and a gated CRF loss is employed to generate moreprecise pseudo-labels and further boost the segmentation performance. Throughextensive experiments on two distinct medical image segmentation tasks ofdifferent modalities, the proposed FedICRA demonstrates overwhelmingperformance over other state-of-the-art personalized FL methods. Itsperformance even approaches that of fully supervised training on centralizeddata. Our code and data are available at ", "output": "Unifying and Personalizing Weakly-supervised Federated Medical Image Segmentation via Adaptive Representation and Aggregation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face anti-spoofing (FAS) based on domain generalization (DG) has beenrecently studied to improve the generalization on unseen scenarios. Previousmethods typically rely on domain labels to align the distribution of eachdomain for learning domain-invariant representations. However, artificialdomain labels are coarse-grained and subjective, which cannot reflect realdomain distributions accurately. Besides, such domain-aware methods focus ondomain-level alignment, which is not fine-grained enough to ensure that learnedrepresentations are insensitive to domain styles. To address these issues, wepropose a novel perspective for DG FAS that aligns features on the instancelevel without the need for domain labels. Specifically, Instance-Aware DomainGeneralization framework is proposed to learn the generalizable feature byweakening the features' sensitivity to instance-specific styles. Concretely, wepropose Asymmetric Instance Adaptive Whitening to adaptively eliminate thestyle-sensitive feature correlation, boosting the generalization. Moreover,Dynamic Kernel Generator and Categorical Style Assembly are proposed to firstextract the instance-specific features and then generate the style-diversifiedfeatures with large style shifts, respectively, further facilitating thelearning of style-insensitive features. Extensive experiments and analysisdemonstrate the superiority of our method over state-of-the-art competitors.Code will be publicly available at ", "output": "Instance-Aware Domain Generalization for Face Anti-Spoofing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce the task of 3D visual grounding in large-scale dynamic scenesbased on natural linguistic descriptions and online captured multi-modal visualdata, including 2D images and 3D LiDAR point clouds. We present a novel method,WildRefer, for this task by fully utilizing the appearance features in images,the location and geometry features in point clouds, and the dynamic features inconsecutive input frames to match the semantic features in language. Inparticular, we propose two novel datasets, STRefer and LifeRefer, which focuson large-scale human-centric daily-life scenarios with abundant 3D object andnatural language annotations. Our datasets are significant for the research of3D visual grounding in the wild and has huge potential to boost the developmentof autonomous driving and service robots. Extensive comparisons and ablationstudies illustrate that our method achieves state-of-the-art performance on twoproposed datasets. Code and dataset will be released when the paper ispublished.", "output": "WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since the differences in viewing range, resolution and relative position, themulti-modality sensing module composed of infrared and visible cameras needs tobe registered so as to have more accurate scene perception. In practice, manualcalibration-based registration is the most widely used process, and it isregularly calibrated to maintain accuracy, which is time-consuming andlabor-intensive. To cope with these problems, we propose a scene-adaptiveinfrared and visible image registration. Specifically, in regard of thediscrepancy between multi-modality images, an invertible translation process isdeveloped to establish a modality-invariant domain, which comprehensivelyembraces the feature intensity and distribution of both infrared and visiblemodalities. We employ homography to simulate the deformation between differentplanes and develop a hierarchical framework to rectify the deformation inferredfrom the proposed latent representation in a coarse-to-fine manner. For that,the advanced perception ability coupled with the residual estimation conduciveto the regression of sparse offsets, and the alternate correlation searchfacilitates a more accurate correspondence matching. Moreover, we propose thefirst ground truth available misaligned infrared and visible image dataset,involving three synthetic sets and one real-world set. Extensive experimentsvalidate the effectiveness of the proposed method against thestate-of-the-arts, advancing the subsequent applications.", "output": "Modality-Invariant Representation for Infrared and Visible Image Registration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive Language-Image Pre-training (CLIP) is a powerful multimodal largevision model that has demonstrated significant benefits for downstream tasks,including many zero-shot learning and text-guided vision tasks. However, wenotice some severe problems regarding the model's explainability, whichundermines its credibility and impedes related tasks. Specifically, we findCLIP prefers the background regions than the foregrounds according to thepredicted similarity map, which contradicts human understanding. Besides, thereare obvious noisy activations on the visualization results at irrelevantpositions. To address these two issues, we conduct in-depth analyses and revealthe reasons with new findings and evidences. Based on these insights, wepropose the CLIP Surgery, a method that enables surgery-like modifications forthe inference architecture and features, for better explainability andenhancement in multiple open-vocabulary tasks. The proposed method hassignificantly improved the explainability of CLIP for both convolutionalnetworks and vision transformers, surpassing existing methods by large margins.Besides, our approach also demonstrates remarkable improvements inopen-vocabulary segmentation and multi-label recognition tasks. For examples,the mAP improvement on NUS-Wide multi-label recognition is 4.41% without anyadditional training, and our CLIP Surgery surpasses the state-of-the-art methodby 8.74% at mIoU on Cityscapes open-vocabulary semantic segmentation.Furthermore, our method benefits other tasks including multimodal visualizationand interactive segmentation like Segment Anything Model (SAM). The code isavailable at ", "output": "CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper studies how to keep a vision backbone effective while removingtoken mixers in its basic building blocks. Token mixers, as self-attention forvision transformers (ViTs), are intended to perform information communicationbetween different spatial tokens but suffer from considerable computationalcost and latency. However, directly removing them will lead to an incompletemodel structure prior, and thus brings a significant accuracy drop. To thisend, we first develop an RepIdentityFormer base on the re-parameterizing idea,to study the token mixer free model architecture. And we then explore theimproved learning paradigm to break the limitation of simple token mixer freebackbone, and summarize the empirical practice into 5 guidelines. Equipped withthe proposed optimization strategy, we are able to build an extremely simplevision backbone with encouraging performance, while enjoying the highefficiency during inference. Extensive experiments and ablative analysis alsodemonstrate that the inductive bias of network architecture, can beincorporated into simple network structure with appropriate optimizationstrategy. We hope this work can serve as a starting point for the explorationof optimization-driven efficient network design. Project page:", "output": "RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most urban applications necessitate building footprints in the form ofconcise vector graphics with sharp boundaries rather than pixel-wise rasterimages. This need contrasts with the majority of existing methods, whichtypically generate over-smoothed footprint polygons. Editing theseautomatically produced polygons can be inefficient, if not more time-consumingthan manual digitization. This paper introduces a semi-automatic approach forbuilding footprint extraction through semantically-sensitive superpixels andneural graph networks. Drawing inspiration from object-based classificationtechniques, we first learn to generate superpixels that are not onlyboundary-preserving but also semantically-sensitive. The superpixels respondexclusively to building boundaries rather than other natural objects, whilesimultaneously producing semantic segmentation of the buildings. Theseintermediate superpixel representations can be naturally considered as nodeswithin a graph. Consequently, graph neural networks are employed to model theglobal interactions among all superpixels and enhance the representativeness ofnode features for building segmentation. Classical approaches are utilized toextract and regularize boundaries for the vectorized building footprints.Utilizing minimal clicks and straightforward strokes, we efficiently accomplishaccurate segmentation outcomes, eliminating the necessity for editing polygonvertices. Our proposed approach demonstrates superior precision and efficacy,as validated by experimental assessments on various public benchmark datasets.We observe a 10% enhancement in the metric for superpixel clustering and an8% increment in vector graphics evaluation, when compared with establishedtechniques. Additionally, we have devised an optimized and sophisticatedpipeline for interactive editing, poised to further augment the overall qualityof the results.", "output": "SuperpixelGraph: Semi-automatic generation of building footprint through semantic-sensitive superpixel and neural graph networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rail detection, essential for railroad anomaly detection, aims to identifythe railroad region in video frames. Although various studies on rail detectionexist, neither an open benchmark nor a high-speed network is available in thecommunity, making algorithm comparison and development difficult. Inspired bythe growth of lane detection, we propose a rail database and a row-based raildetection method. In detail, we make several contributions: (i) We present areal-world railway dataset, Rail-DB, with 7432 pairs of images and annotations.The images are collected from different situations in lighting, roadstructures, and views. The rails are labeled with polylines, and the images arecategorized into nine scenes. The Rail-DB is expected to facilitate theimprovement of rail detection algorithms. (ii) We present an efficientrow-based rail detection method, Rail-Net, containing a lightweightconvolutional backbone and an anchor classifier. Specifically, we formulate theprocess of rail detection as a row-based selecting problem. This strategyreduces the computational cost compared to alternative segmentation methods.(iii) We evaluate the Rail-Net on Rail-DB with extensive experiments, includingcross-scene settings and network backbones ranging from ResNet to VisionTransformers. Our method achieves promising performance in terms of both speedand accuracy. Notably, a lightweight version could achieve 92.77% accuracy and312 frames per second. The Rail-Net outperforms the traditional method by50.65% and the segmentation one by 5.86%. The database and code are availableat: ", "output": "Rail Detection: An Efficient Row-based Network and A New Benchmark."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Inverse path tracing has recently been applied to joint material and lightingestimation, given geometry and multi-view HDR observations of an indoor scene.However, it has two major limitations: path tracing is expensive to compute,and ambiguities exist between reflection and emission. We propose a novelFactorized Inverse Path Tracing (FIPT) method which utilizes a factored lighttransport formulation and finds emitters driven by rendering errors. Ouralgorithm enables accurate material and lighting optimization faster thanprevious work, and is more effective at resolving ambiguities. The exhaustiveexperiments on synthetic scenes show that our method (1) outperformsstate-of-the-art indoor inverse rendering and relighting methods particularlyin the presence of complex illumination effects; (2) speeds up inverse pathtracing optimization to less than an hour. We further demonstrate robustness tonoisy inputs through material and lighting estimates that allow plausiblerelighting in a real scene. The source code is available at:", "output": "Factorized Inverse Path Tracing for Efficient and Accurate Material-Lighting Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a deep learning method for accurately localizing the center of asingle corneal reflection (CR) in an eye image. Unlike previous approaches, weuse a convolutional neural network (CNN) that was trained solely usingsimulated data. Using only simulated data has the benefit of completelysidestepping the time-consuming process of manual annotation that is requiredfor supervised training on real eye images. To systematically evaluate theaccuracy of our method, we first tested it on images with simulated CRs placedon different backgrounds and embedded in varying levels of noise. Second, wetested the method on high-quality videos captured from real eyes. Our methodoutperformed state-of-the-art algorithmic methods on real eye images with a 35%reduction in terms of spatial precision, and performed on par withstate-of-the-art on simulated images in terms of spatial accuracy.We concludethat our method provides a precise method for CR center localization andprovides a solution to the data availability problem which is one of theimportant common roadblocks in the development of deep learning models for gazeestimation. Due to the superior CR center localization and ease of application,our method has the potential to improve the accuracy and precision of CR-basedeye trackers", "output": "Precise localization of corneal reflections in eye images using deep learning trained on synthetic data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) have shown exciting performance in various tasks,yet suffer generalization failures when meeting unknown target domains. One ofthe most promising approaches to achieve domain generalization (DG) isgenerating unseen data, e.g., mixup, to cover the unknown target data. However,existing works overlook the challenges induced by the simultaneous appearanceof changes in both the semantic and distribution space. Accordingly, such achallenge makes source distributions hard to fit for DNNs. To mitigate thehard-fitting issue, we propose to perform a semantic-aware mixup (SAM) fordomain generalization, where whether to perform mixup depends on the semanticand domain information. The feasibility of SAM shares the same spirits with theFourier-based mixup. Namely, the Fourier phase spectrum is expected to containsemantics information (relating to labels), while the Fourier amplitude retainsother information (relating to style information). Built upon the insight, SAMapplies different mixup strategies to the Fourier phase spectrum and amplitudeinformation. For instance, SAM merely performs mixup on the amplitude spectrumwhen both the semantic and domain information changes. Consequently, theoverwhelmingly large change can be avoided. We validate the effectiveness ofSAM using image classification tasks on several DG benchmarks.", "output": "Semantic-Aware Mixup for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Social group detection is a crucial aspect of various robotic applications,including robot navigation and human-robot interactions. To date, a range ofmodel-based techniques have been employed to address this challenge, such asthe F-formation and trajectory similarity frameworks. However, these approachesoften fail to provide reliable results in crowded and dynamic scenarios. Recentadvancements in this area have mainly focused on learning-based methods, suchas deep neural networks that use visual content or human pose. Although visualcontent-based methods have demonstrated promising performance on large-scaledatasets, their computational complexity poses a significant barrier to theirpractical use in real-time applications. To address these issues, we propose asimple and efficient framework for social group detection. Our approachexplores the impact of motion trajectory on social grouping and utilizes anovel, reliable, and fast data-driven method. We formulate the individuals in ascene as a graph, where the nodes are represented by LSTM-encoded trajectoriesand the edges are defined by the distances between each pair of tracks. Ourframework employs a modified graph transformer module and graph clusteringlosses to detect social groups. Our experiments on the popular JRDBAct datasetreveal noticeable improvements in performance, with relative improvementsranging from 2% to 11%. Furthermore, our framework is significantly faster,with up to 12x faster inference times compared to state-of-the-art methodsunder the same computation resources. These results demonstrate that ourproposed method is suitable for real-time robotic applications.", "output": "Real-time Trajectory-based Social Group Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We have recently seen tremendous progress in diffusion advances forgenerating realistic human motions. Yet, they largely disregard the richmulti-human interactions. In this paper, we present InterGen, an effectivediffusion-based approach that incorporates human-to-human interactions into themotion diffusion process, which enables layman users to customize high-qualitytwo-person interaction motions, with only text guidance. We first contribute amultimodal dataset, named InterHuman. It consists of about 107M frames fordiverse two-person interactions, with accurate skeletal motions and 16,756natural language descriptions. For the algorithm side, we carefully tailor themotion diffusion model to our two-person interaction setting. To handle thesymmetry of human identities during interactions, we propose two cooperativetransformer-based denoisers that explicitly share weights, with a mutualattention mechanism to further connect the two denoising processes. Then, wepropose a novel representation for motion input in our interaction diffusionmodel, which explicitly formulates the global relations between the twoperformers in the world frame. We further introduce two novel regularizationterms to encode spatial relations, equipped with a corresponding damping schemeduring the training of our interaction diffusion model. Extensive experimentsvalidate the effectiveness and generalizability of InterGen. Notably, it cangenerate more diverse and compelling two-person motions than previous methodsand enables various downstream applications for human interactions.", "output": "InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recovering whole-body mesh by inferring the abstract pose and shapeparameters from visual content can obtain 3D bodies with realistic structures.However, the inferring process is highly non-linear and suffers from image-meshmisalignment, resulting in inaccurate reconstruction. In contrast, 3D keypointestimation methods utilize the volumetric representation to achieve pixel-levelaccuracy but may predict unrealistic body structures. To address these issues,this paper presents a novel hybrid inverse kinematics solution, HybrIK, thatintegrates the merits of 3D keypoint estimation and body mesh recovery in aunified framework. HybrIK directly transforms accurate 3D joints to body-partrotations via twist-and-swing decomposition. The swing rotations areanalytically solved with 3D joints, while the twist rotations are derived fromvisual cues through neural networks. To capture comprehensive whole-bodydetails, we further develop a holistic framework, HybrIK-X, which enhancesHybrIK with articulated hands and an expressive face. HybrIK-X is fast andaccurate by solving the whole-body pose with a one-stage model. Experimentsdemonstrate that HybrIK and HybrIK-X preserve both the accuracy of 3D jointsand the realistic structure of the parametric human model, leading topixel-aligned whole-body mesh recovery. The proposed method significantlysurpasses the state-of-the-art methods on various benchmarks for body-only,hand-only, and whole-body scenarios. Code and results can be found at", "output": "HybrIK-X: Hybrid Analytical-Neural Inverse Kinematics for Whole-body Mesh Recovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-attention modules have demonstrated remarkable capabilities in capturinglong-range relationships and improving the performance of point cloud tasks.However, point cloud objects are typically characterized by complex,disordered, and non-Euclidean spatial structures with multiple scales, andtheir behavior is often dynamic and unpredictable. The current self-attentionmodules mostly rely on dot product multiplication and dimension alignment amongquery-key-value features, which cannot adequately capture the multi-scalenon-Euclidean structures of point cloud objects. To address these problems,this paper proposes a self-attention plug-in module with its variants,Multi-scale Geometry-aware Transformer (MGT). MGT processes point cloud datawith multi-scale local and global geometric information in the following threeaspects. At first, the MGT divides point cloud data into patches with multiplescales. Secondly, a local feature extractor based on sphere mapping is proposedto explore the geometry inner each patch and generate a fixed-lengthrepresentation for each patch. Thirdly, the fixed-length representations arefed into a novel geodesic-based self-attention to capture the globalnon-Euclidean geometry between patches. Finally, all the modules are integratedinto the framework of MGT with an end-to-end training scheme. Experimentalresults demonstrate that the MGT vastly increases the capability of capturingmulti-scale geometry using the self-attention mechanism and achieves strongcompetitive performance on mainstream point cloud benchmarks.", "output": "Multi-scale Geometry-aware Transformer for 3D Point Cloud Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pseudo depth maps are depth map predicitions which are used as ground truthduring training. In this paper we leverage pseudo depth maps in order tosegment objects of classes that have never been seen during training. Thisrenders our object segmentation task an open world task. The pseudo depth mapsare generated using pretrained networks, which have either been trained withthe full intention to generalize to downstream tasks (LeRes and MiDaS), orwhich have been trained in an unsupervised fashion on video sequences(MonodepthV2). In order to tell our network which object to segment, we providethe network with a single click on the object's surface on the pseudo depth mapof the image as input. We test our approach on two different scenarios: Onewithout the RGB image and one where the RGB image is part of the input. Ourresults demonstrate a considerably better generalization performance from seento unseen object types when depth is used. On the Semantic Boundaries Datasetwe achieve an improvement from $61.57$ to $69.79$ IoU score on unseen classes,when only using half of the training classes during training and performing thesegmentation on depth maps only.", "output": "Impact of Pseudo Depth on Open World Object Segmentation with Minimal User Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We demonstrate universal polarization transformers based on an engineereddiffractive volume, which can synthesize a large set of arbitrarily-selected,complex-valued polarization scattering matrices between the polarization statesat different positions within its input and output field-of-views (FOVs). Thisframework comprises 2D arrays of linear polarizers with diverse angles, whichare positioned between isotropic diffractive layers, each containing tens ofthousands of diffractive features with optimizable transmission coefficients.We demonstrate that, after its deep learning-based training, this diffractivepolarization transformer could successfully implement N_i x N_o = 10,000different spatially-encoded polarization scattering matrices with negligibleerror within a single diffractive volume, where N_i and N_o represent thenumber of pixels in the input and output FOVs, respectively. We experimentallyvalidated this universal polarization transformation framework in the terahertzpart of the spectrum by fabricating wire-grid polarizers and integrating themwith 3D-printed diffractive layers to form a physical polarization transformeroperating at 0.75 mm wavelength. Through this set-up, we demonstrated anall-optical polarization permutation operation of spatially-varyingpolarization fields, and simultaneously implemented distinct spatially-encodedpolarization scattering matrices between the input and output FOVs of a compactdiffractive processor that axially spans 200 wavelengths. This framework opensup new avenues for developing novel optical devices for universal polarizationcontrol, and may find various applications in, e.g., remote sensing, medicalimaging, security, material inspection and machine vision.", "output": "Universal Polarization Transformations: Spatial programming of polarization scattering matrices using a deep learning-designed diffractive polarization transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable AI has become a popular tool for validating machine learningmodels. Mismatches between the explained model's decision strategy and theuser's domain knowledge (e.g. Clever Hans effects) have also been recognized asa starting point for improving faulty models. However, it is less clear what todo when the user and the explanation agree. In this paper, we demonstrate thatacceptance of explanations by the user is not a guarantee for a ML model tofunction well, in particular, some Clever Hans effects may remain undetected.Such hidden flaws of the model can nevertheless be mitigated, and wedemonstrate this by contributing a new method, Explanation-Guided ExposureMinimization (EGEM), that premptively prunes variations in the ML model thathave not been the subject of positive explanation feedback. Experiments onnatural image data demonstrate that our approach leads to models that stronglyreduce their reliance on hidden Clever Hans strategies, and consequentlyachieve higher accuracy on new data.", "output": "Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The retrieval of 3D objects has gained significant importance in recent yearsdue to its broad range of applications in computer vision, computer graphics,virtual reality, and augmented reality. However, the retrieval of 3D objectspresents significant challenges due to the intricate nature of 3D models, whichcan vary in shape, size, and texture, and have numerous polygons and vertices.To this end, we introduce a novel SHREC challenge track that focuses onretrieving relevant 3D animal models from a dataset using sketch queries andexpedites accessing 3D models through available sketches. Furthermore, a newdataset named ANIMAR was constructed in this study, comprising a collection of711 unique 3D animal models and 140 corresponding sketch queries. Our contestrequires participants to retrieve 3D models based on complex and detailedsketches. We receive satisfactory results from eight teams and 204 runs.Although further improvement is necessary, the proposed task has the potentialto incentivize additional research in the domain of 3D object retrieval,potentially yielding benefits for a wide range of applications. We also provideinsights into potential areas of future research, such as improving techniquesfor feature extraction and matching, and creating more diverse datasets toevaluate retrieval performance.", "output": "SketchANIMAR: Sketch-based 3D Animal Fine-Grained Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to incrementally learn new classes from limited samples iscrucial to the development of artificial intelligence systems for real clinicalapplication. Although existing incremental learning techniques have attemptedto address this issue, they still struggle with only few labeled data,particularly when the samples are from varied domains. In this paper, weexplore the cross-domain few-shot incremental learning (CDFSCIL) problem.CDFSCIL requires models to learn new classes from very few labeled samplesincrementally, and the new classes may be vastly different from the targetspace. To counteract this difficulty, we propose a cross-domain enhancementconstraint and cross-domain data augmentation method. Experiments on MedMNISTshow that the classification performance of this method is better than othersimilar incremental learning methods.", "output": "Few-shot Class-incremental Learning for Cross-domain Disease Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The human visual system processes images with varied degrees of resolution,with the fovea, a small portion of the retina, capturing the highest acuityregion, which gradually declines toward the field of view's periphery. However,the majority of existing object localization methods rely on images acquired byimage sensors with space-invariant resolution, ignoring biological attentionmechanisms.As a region of interest pooling, this study employs a fixation predictionmodel that emulates human objective-guided attention of searching for a givenclass in an image. The foveated pictures at each fixation point are thenclassified to determine whether the target is present or absent in the scene.Throughout this two-stage pipeline method, we investigate the varying resultsobtained by utilizing high-level or panoptic features and provide aground-truth label function for fixation sequences that is smoother,considering in a better way the spatial structure of the problem.Finally, we present a novel dual task model capable of performing fixationprediction and detection simultaneously, allowing knowledge transfer betweenthe two tasks. We conclude that, due to the complementary nature of both tasks,the training process benefited from the sharing of knowledge, resulting in animprovement in performance when compared to the previous approach's baselinescores.", "output": "Learning to search for and detect objects in foveal images using deep learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Meta AI Research approaches a general, promptable Segment AnythingModel (SAM) pre-trained on an unprecedentedly large segmentation dataset(SA-1B). Without a double, the emergence of SAM will yield significant benefitsfor a wide array of practical image segmentation applications. In this study,we conduct a series of intriguing investigations into the performance of SAMacross various applications, particularly in the fields of natural images,agriculture, manufacturing, remote sensing, and healthcare. We analyze anddiscuss the benefits and limitations of SAM and provide an outlook on futuredevelopment of segmentation tasks. Note that our work does not intend topropose new algorithms or theories, but rather provide a comprehensive view ofSAM in practice. This work is expected to provide insights that facilitatefuture research activities toward generic segmentation.", "output": "Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face anti-spoofing (FAS) is an essential mechanism for safeguarding theintegrity of automated face recognition systems. Despite substantialadvancements, the generalization of existing approaches to real-worldapplications remains challenging. This limitation can be attributed to thescarcity and lack of diversity in publicly available FAS datasets, which oftenleads to overfitting during training or saturation during testing. In terms ofquantity, the number of spoof subjects is a critical determinant. Most datasetscomprise fewer than 2,000 subjects. With regard to diversity, the majority ofdatasets consist of spoof samples collected in controlled environments usingrepetitive, mechanical processes. This data collection methodology results inhomogenized samples and a dearth of scenario diversity. To address theseshortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, alarge-scale, diverse FAS dataset collected in unconstrained settings. Ourdataset encompasses 853,729 images of 321,751 spoof subjects and 529,571 imagesof 148,169 live subjects, representing a substantial increase in quantity.Moreover, our dataset incorporates spoof data obtained from the internet,spanning a wide array of scenarios and various commercial sensors, including 17presentation attacks (PAs) that encompass both 2D and 3D forms. This novel datacollection strategy markedly enhances FAS data diversity. Leveraging the WFASdataset and Protocol 1 (Known-Type), we host the Wild Face Anti-SpoofingChallenge at the CVPR2023 workshop. Additionally, we meticulously evaluaterepresentative methods using Protocol 1 and Protocol 2 (Unknown-Type). Throughan in-depth examination of the challenge outcomes and benchmark baselines, weprovide insightful analyses and propose potential avenues for future research.The dataset is released under Insightface.", "output": "Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Representation learning aims to discover individual salient features of adomain in a compact and descriptive form that strongly identifies the uniquecharacteristics of a given sample respective to its domain. Existing works invisual style representation literature have tried to disentangle style fromcontent during training explicitly. A complete separation between these has yetto be fully achieved. Our paper aims to learn a representation of visualartistic style more strongly disentangled from the semantic content depicted inan image. We use Neural Style Transfer (NST) to measure and drive the learningsignal and achieve state-of-the-art representation learning on explicitlydisentangled metrics. We show that strongly addressing the disentanglement ofstyle and content leads to large gains in style-specific metrics, encoding farless semantic information and achieving state-of-the-art accuracy in downstreammultimodal applications.", "output": "ALADIN-NST: Self-supervised disentangled representation learning of artistic style through Neural Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The task of collaborative human pose forecasting stands for predicting thefuture poses of multiple interacting people, given those in previous frames.Predicting two people in interaction, instead of each separately, promisesbetter performance, due to their body-body motion correlations. But the taskhas remained so far primarily unexplored.In this paper, we review the progress in human pose forecasting and providean in-depth assessment of the single-person practices that perform best for2-body collaborative motion forecasting. Our study confirms the positive impactof frequency input representations, space-time separable and fully-learnableinteraction adjacencies for the encoding GCN and FC decoding. Othersingle-person practices do not transfer to 2-body, so the proposed best ones donot include hierarchical body modeling or attention-based interaction encoding.We further contribute a novel initialization procedure for the 2-body spatialinteraction parameters of the encoder, which benefits performance andstability. Altogether, our proposed 2-body pose forecasting best practicesyield a performance improvement of 21.9% over the state-of-the-art on the mostrecent ExPI dataset, whereby the novel initialization accounts for 3.5%. Seeour project page at ", "output": "Best Practices for 2-Body Pose Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Year after year, the demand for ever-better smartphone photos continues togrow, in particular in the domain of portrait photography. Manufacturers thususe perceptual quality criteria throughout the development of smartphonecameras. This costly procedure can be partially replaced by automatedlearning-based methods for image quality assessment (IQA). Due to itssubjective nature, it is necessary to estimate and guarantee the consistency ofthe IQA process, a characteristic lacking in the mean opinion scores (MOS)widely used for crowdsourcing IQA. In addition, existing blind IQA (BIQA)datasets pay little attention to the difficulty of cross-content assessment,which may degrade the quality of annotations. This paper introduces PIQ23, aportrait-specific IQA dataset of 5116 images of 50 predefined scenariosacquired by 100 smartphones, covering a high variety of brands, models, and usecases. The dataset includes individuals of various genders and ethnicities whohave given explicit and informed consent for their photographs to be used inpublic research. It is annotated by pairwise comparisons (PWC) collected fromover 30 image quality experts for three image attributes: face detailpreservation, face target exposure, and overall image quality. An in-depthstatistical analysis of these annotations allows us to evaluate theirconsistency over PIQ23. Finally, we show through an extensive comparison withexisting baselines that semantic information (image context) can be used toimprove IQA predictions. The dataset along with the proposed statisticalanalysis and BIQA algorithms are available:", "output": "An Image Quality Assessment Dataset for Portraits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works on personalized text-to-image generation usually learn to bind aspecial token with specific subjects or styles of a few given images by tuningits embedding through gradient descent. It is natural to question whether wecan optimize the textual inversions by only accessing the process of modelinference. As only requiring the forward computation to determine the textualinversion retains the benefits of less GPU memory, simple deployment, andsecure access for scalable models. In this paper, we introduce aemph{gradient-free} framework to optimize the continuous textual inversion inan iterative evolutionary strategy. Specifically, we first initialize anappropriate token embedding for textual inversion with the consideration ofvisual and text vocabulary information. Then, we decompose the optimization ofevolutionary strategy into dimension reduction of searching space andnon-convex gradient-free optimization in subspace, which significantlyaccelerates the optimization process with negligible performance loss.Experiments in several applications demonstrate that the performance oftext-to-image model equipped with our proposed gradient-free method iscomparable to that of gradient-based counterparts with variant GPU/CPUplatforms, flexible employment, as well as computational efficiency.", "output": "Gradient-Free Textual Inversion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Power lines pose a significant safety threat to unmanned aerial vehicles(UAVs) operating at low altitudes. However, detecting power lines in aerialimages is challenging due to the small size of the foreground data (i.e., powerlines) and the abundance of background information. To address this challenge,we propose DUFormer, a semantic segmentation algorithm designed specificallyfor power line detection in aerial images. We assume that performing sufficientfeature extraction with a convolutional neural network (CNN) that has a stronginductive bias is beneficial for training an efficient Transformer model. Tothis end, we propose a heavy token encoder responsible for overlapping featurere-mining and tokenization. The encoder comprises a pyramid CNN featureextraction module and a power line feature enhancement module. Followingsufficient feature extraction for power lines, the feature fusion is carriedout, and then the Transformer block is used for global modeling. The finalsegmentation result is obtained by fusing local and global features in thedecode head. Additionally, we demonstrate the significance of the jointmulti-weight loss function in power line segmentation. The experimental resultsdemonstrate that our proposed method achieves the state-of-the-art performancein power line segmentation on the publicly available TTPLA dataset.", "output": "DUFormer: A Novel Architecture for Power Line Segmentation of Aerial Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study uses domain randomization to generate a synthetic RGB-D datasetfor training multimodal instance segmentation models, aiming to achievecolour-agnostic hand localization in cluttered industrial environments. Domainrandomization is a simple technique for addressing the \"reality gap\" byrandomly rendering unrealistic features in a simulation scene to force theneural network to learn essential domain features. We provide a new syntheticdataset for various hand detection applications in industrial environments, aswell as ready-to-use pretrained instance segmentation models. To achieve robustresults in a complex unstructured environment, we use multimodal input thatincludes both colour and depth information, which we hypothesize helps toimprove the accuracy of the model prediction. In order to test this assumption,we analyze the influence of each modality and their synergy. The evaluatedmodels were trained solely on our synthetic dataset; yet we show that ourapproach enables the models to outperform corresponding models trained onexisting state-of-the-art datasets in terms of Average Precision andProbability-based Detection Quality.", "output": "HaDR: Applying Domain Randomization for Generating Synthetic Multimodal Dataset for Hand Instance Segmentation in Cluttered Industrial Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation assigns category labels to each pixel in an image,enabling breakthroughs in fields such as autonomous driving and robotics. DeepNeural Networks have achieved high accuracies in semantic segmentation butrequire large training datasets. Some domains have difficulties building suchdatasets due to rarity, privacy concerns, and the need for skilled annotators.Few-Shot Learning (FSL) has emerged as a new research stream that allows modelsto learn new tasks from a few samples. This contribution provides an overviewof FSL in semantic segmentation (FSS), proposes a new taxonomy, and describescurrent limitations and outlooks.", "output": "Few Shot Semantic Segmentation: a review of methodologies and open challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present new Recurrent Neural Network (RNN) cells for image classificationusing a Neural Architecture Search (NAS) approach called DARTS. We areinterested in the ReNet architecture, which is a RNN based approach presentedas an alternative for convolutional and pooling steps. ReNet can be definedusing any standard RNN cells, such as LSTM and GRU. One limitation is thatstandard RNN cells were designed for one dimensional sequential data and notfor two dimensions like it is the case for image classification. We overcomethis limitation by using DARTS to find new cell designs. We compare our resultswith ReNet that uses GRU and LSTM cells. Our found cells outperform thestandard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicategeneralizability, as we derived the RNN cell designs from CIFAR-10 withoutperforming a new cell search for SVHN.", "output": "DartsReNet: Exploring new RNN cells in ReNet architectures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper investigates the performance of diffusion models for video anomalydetection (VAD) within the most challenging but also the most operationalscenario in which the data annotations are not used. As being sparse, diverse,contextual, and often ambiguous, detecting abnormal events precisely is a veryambitious task. To this end, we rely only on the information-richspatio-temporal data, and the reconstruction power of the diffusion models suchthat a high reconstruction error is utilized to decide the abnormality.Experiments performed on two large-scale video anomaly detection datasetsdemonstrate the consistent improvement of the proposed method over thestate-of-the-art generative models while in some cases our method achievesbetter scores than the more complex models. This is the first study using adiffusion model and examining its parameters' influence to present guidance forVAD in surveillance scenarios.", "output": "Exploring Diffusion Models for Unsupervised Video Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is desirable to predict the behavior of traffic participants conditionedon different planned trajectories of the autonomous vehicle. This allows thedownstream planner to estimate the impact of its decisions. Recent approachesfor conditional behavior prediction rely on a regression decoder, meaning thatcoordinates or polynomial coefficients are regressed. In this work we revisitset-based trajectory prediction, where the probability of each trajectory in apredefined trajectory set is determined by a classification model, andfirst-time employ it to the task of conditional behavior prediction. We proposeRESET, which combines a new metric-driven algorithm for trajectory setgeneration with a graph-based encoder. For unconditional prediction, RESETachieves comparable performance to a regression-based approach. Due to thenature of set-based approaches, it has the advantageous property of being ableto predict a flexible number of trajectories without influencing runtime orcomplexity. For conditional prediction, RESET achieves reasonable results withlate fusion of the planned trajectory, which was not observed forregression-based approaches before. This means that RESET is computationallylightweight to combine with a planner that proposes multiple future plans ofthe autonomous vehicle, as large parts of the forward pass can be reused.", "output": "RESET: Revisiting Trajectory Sets for Conditional Behavior Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability of convolutional neural networks (CNNs) to recognize objectsregardless of their position in the image is due to thetranslation-equivariance of the convolutional operation. Group-equivariant CNNstransfer this equivariance to other transformations of the input. Dealingappropriately with objects and object parts of different scale is challenging,and scale can vary for multiple reasons such as the underlying object size orthe resolution of the imaging modality. In this paper, we propose ascale-equivariant convolutional network layer for three-dimensional data thatguarantees scale-equivariance in 3D CNNs. Scale-equivariance lifts the burdenof having to learn each possible scale separately, allowing the neural networkto focus on higher-level learning goals, which leads to better results andbetter data-efficiency. We provide an overview of the theoretical foundationsand scientific work on scale-equivariant neural networks in the two-dimensionaldomain. We then transfer the concepts from 2D to the three-dimensional spaceand create a scale-equivariant convolutional layer for 3D data. Using theproposed scale-equivariant layer, we create a scale-equivariant U-Net formedical image segmentation and compare it with a non-scale-equivariant baselinemethod. Our experiments demonstrate the effectiveness of the proposed method inachieving scale-equivariance for 3D medical image analysis. We publish our codeat  for furtherresearch and application.", "output": "Scale-Equivariant Deep Learning for 3D Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "StyleGANs are at the forefront of controllable image generation as theyproduce a latent space that is semantically disentangled, making it suitablefor image editing and manipulation. However, the performance of StyleGANsseverely degrades when trained via class-conditioning on large-scalelong-tailed datasets. We find that one reason for degradation is the collapseof latents for each class in the $mathcal{W}$ latent space. With NoisyTwins,we first introduce an effective and inexpensive augmentation strategy for classembeddings, which then decorrelates the latents based on self-supervision inthe $mathcal{W}$ space. This decorrelation mitigates collapse, ensuring thatour method preserves intra-class diversity with class-consistency in imagegeneration. We show the effectiveness of our approach on large-scale real-worldlong-tailed datasets of ImageNet-LT and iNaturalist 2019, where our methodoutperforms other methods by $sim 19%$ on FID, establishing a newstate-of-the-art.", "output": "NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Remarkable advances have been achieved recently in learning neuralrepresentations that characterize object geometry, while generating texturedobjects suitable for downstream applications and 3D rendering remains at anearly stage. In particular, reconstructing textured geometry from images ofreal objects is a significant challenge -- reconstructed geometry is ofteninexact, making realistic texturing a significant challenge. We presentMesh2Tex, which learns a realistic object texture manifold from uncorrelatedcollections of 3D object geometry and photorealistic RGB images, by leveraginga hybrid mesh-neural-field texture representation. Our texture representationenables compact encoding of high-resolution textures as a neural field in thebarycentric coordinate system of the mesh faces. The learned texture manifoldenables effective navigation to generate an object texture for a given 3Dobject geometry that matches to an input RGB image, which maintains robustnesseven under challenging real-world scenarios where the mesh geometryapproximates an inexact match to the underlying geometry in the RGB image.Mesh2Tex can effectively generate realistic object textures for an object meshto match real images observations towards digitization of real environments,significantly improving over previous state of the art.", "output": "Mesh2Tex: Generating Mesh Textures from Image Queries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of approaches for trajectory prediction requires metrics tovalidate and compare their performance. Currently established metrics are basedon Euclidean distance, which means that errors are weighted equally in alldirections. Euclidean metrics are insufficient for structured environments likeroads, since they do not properly capture the agent's intent relative to theunderlying lane. In order to provide a reasonable assessment of trajectoryprediction approaches with regard to the downstream planning task, we propose anew metric that is lane distance-based: Lane Miss Rate (LMR). For thecalculation of LMR, the ground-truth and predicted endpoints are assigned tolane segments, more precisely their centerlines. Measured by the distance alongthe lane segments, predictions that are within a certain threshold distance tothe ground-truth count as hits, otherwise they count as misses. LMR is thendefined as the ratio of sequences that yield a miss. Our results on threestate-of-the-art trajectory prediction models show that LMR preserves the orderof Euclidean distance-based metrics. In contrast to the Euclidean Miss Rate,qualitative results show that LMR yields misses for sequences where predictionsare located on wrong lanes. Hits on the other hand result for sequences wherepredictions are located on the correct lane. This means that LMR implicitlyweights Euclidean error relative to the lane and goes into the direction ofcapturing intents of traffic agents. The source code of LMR for Argoverse 1 ispublicly available.", "output": "LMR: Lane Distance-Based Metric for Trajectory Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern image retrieval methods typically rely on fine-tuning pre-trainedencoders to extract image-level descriptors. However, the most widely usedmodels are pre-trained on ImageNet-1K with limited classes. The pre-trainedfeature representation is therefore not universal enough to generalize well tothe diverse open-world classes. In this paper, we first cluster the large-scaleLAION400M into one million pseudo classes based on the joint textual and visualfeatures extracted by the CLIP model. Due to the confusion of labelgranularity, the automatically clustered dataset inevitably contains heavyinter-class conflict. To alleviate such conflict, we randomly select partialinter-class prototypes to construct the margin-based softmax loss. To furtherenhance the low-dimensional feature representation, we randomly select partialfeature dimensions when calculating the similarities between embeddings andclass-wise prototypes. The dual random partial selections are with respect tothe class dimension and the feature dimension of the prototype matrix, makingthe classification conflict-robust and the feature embedding compact. Ourmethod significantly outperforms state-of-the-art unsupervised and supervisedimage retrieval approaches on multiple benchmarks. The code and pre-trainedmodels are released to facilitate future research", "output": "Unicom: Universal and Compact Representation Learning for Image Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual Place Recognition is a task that aims to predict the coordinates of animage (called query) based solely on visual clues. Most commonly, a retrievalapproach is adopted, where the query is matched to the most similar images froma large database of geotagged photos, using learned global descriptors. Despiterecent advances, recognizing the same place when the query comes from asignificantly different distribution is still a major hurdle for state of theart retrieval methods. Examples are heavy illumination changes (e.g. night-timeimages) or substantial occlusions (e.g. transient objects). In this work weexplore whether re-ranking methods based on spatial verification can tacklethese challenges, following the intuition that local descriptors are inherentlymore robust than global features to domain shifts. To this end, we provide anew, comprehensive benchmark on current state of the art models. We alsointroduce two new demanding datasets with night and occluded queries, to bematched against a city-wide database. Code and datasets are available at", "output": "Are Local Features All You Need for Cross-Domain Visual Place Recognition?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The prevalence of breast cancer continues to grow, affecting about 300,000females in the United States in 2023. However, there are different levels ofseverity of breast cancer requiring different treatment strategies, and hence,grading breast cancer has become a vital component of breast cancer diagnosisand treatment planning. Specifically, the gold-standard Scarff-Bloom-Richardson(SBR) grade has been shown to consistently indicate a patient's response tochemotherapy. Unfortunately, the current method to determine the SBR graderequires removal of some cancer cells from the patient which can lead to stressand discomfort along with costly expenses. In this paper, we study the efficacyof deep learning for breast cancer grading based on synthetic correlateddiffusion (CDI$^s$) imaging, a new magnetic resonance imaging (MRI) modalityand found that it achieves better performance on SBR grade prediction comparedto those learnt using gold-standard imaging modalities. Hence, we introduceCancer-Net BCa-S, a volumetric deep radiomics approach for predicting SBR gradebased on volumetric CDI$^s$ data. Given the promising results, this proposedmethod to identify the severity of the cancer would allow for better treatmentdecisions without the need for a biopsy. Cancer-Net BCa-S has been madepublicly available as part of a global open-source initiative for advancingmachine learning for cancer care.", "output": "Cancer-Net BCa-S: Breast Cancer Grade Prediction using Volumetric Deep Radiomic Features from Synthetic Correlated Diffusion Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image segmentation is an increasingly popular area of research inmedical imaging processing and analysis. However, many researchers who are newto the field struggle with basic concepts. This tutorial paper aims to providean overview of the fundamental concepts of medical imaging, with a focus onMagnetic Resonance and Computerized Tomography. We will also discuss deeplearning algorithms, tools, and frameworks used for segmentation tasks, andsuggest best practices for method development and image analysis. Our tutorialincludes sample tasks using public data, and accompanying code is available onGitHub ( By sharingour insights gained from years of experience in the field and learning fromrelevant literature, we hope to assist researchers in overcoming the initialchallenges they may encounter in this exciting and important area of research.", "output": "Automated computed tomography and magnetic resonance imaging segmentation using deep learning: a beginner's guide."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked image modeling (MIM) has attracted much research attention due to itspromising potential for learning scalable visual representations. In typicalapproaches, models usually focus on predicting specific contents of maskedpatches, and their performances are highly related to pre-defined maskstrategies. Intuitively, this procedure can be considered as training a student(the model) on solving given problems (predict masked patches). However, weargue that the model should not only focus on solving given problems, but alsostand in the shoes of a teacher to produce a more challenging problem byitself. To this end, we propose Hard Patches Mining (HPM), a brand-newframework for MIM pre-training. We observe that the reconstruction loss cannaturally be the metric of the difficulty of the pre-training task. Therefore,we introduce an auxiliary loss predictor, predicting patch-wise losses firstand deciding where to mask next. It adopts a relative relationship learningstrategy to prevent overfitting to exact reconstruction loss values.Experiments under various settings demonstrate the effectiveness of HPM inconstructing masked images. Furthermore, we empirically find that solelyintroducing the loss prediction objective leads to powerful representations,verifying the efficacy of the ability to be aware of where is hard toreconstruct.", "output": "Hard Patches Mining for Masked Image Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multiscale video transformers have been explored in a wide variety of visiontasks. To date, however, the multiscale processing has been confined to theencoder or decoder alone. We present a unified multiscale encoder-decodertransformer that is focused on dense prediction tasks in videos. Multiscalerepresentation at both encoder and decoder yields key benefits of implicitextraction of spatiotemporal features (i.e. without reliance on input opticalflow) as well as temporal consistency at encoding and coarseto-fine detectionfor high-level (e.g. object) semantics to guide precise localization atdecoding. Moreover, we propose a transductive learning scheme throughmany-to-many label propagation to provide temporally consistent predictions. Weshowcase our Multiscale Encoder-Decoder Video Transformer (MED-VT) on AutomaticVideo Object Segmentation (AVOS) and actor/action segmentation, where weoutperform state-of-the-art approaches on multiple benchmarks using only rawimages, without using optical flow.", "output": "MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sign languages are used as a primary language by approximately 70 millionD/deaf people world-wide. However, most communication technologies operate inspoken and written languages, creating inequities in access. To help tacklethis problem, we release ASL Citizen, the largest Isolated Sign LanguageRecognition (ISLR) dataset to date, collected with consent and containing83,912 videos for 2,731 distinct signs filmed by 52 signers in a variety ofenvironments. We propose that this dataset be used for sign language dictionaryretrieval for American Sign Language (ASL), where a user demonstrates a sign totheir own webcam with the aim of retrieving matching signs from a dictionary.We show that training supervised machine learning classifiers with our datasetgreatly advances the state-of-the-art on metrics relevant for dictionaryretrieval, achieving, for instance, 62% accuracy and a recall-at-10 of 90%,evaluated entirely on videos of users who are not present in the training orvalidation sets. An accessible PDF of this article is available at", "output": "ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Variational autoencoders (VAEs) are powerful generative modelling methods,however they suffer from blurry generated samples and reconstructions comparedto the images they have been trained on. Significant research effort has beenspent to increase the generative capabilities by creating more flexible modelsbut often flexibility comes at the cost of higher complexity and computationalcost. Several works have focused on altering the reconstruction term of theevidence lower bound (ELBO), however, often at the expense of losing themathematical link to maximizing the likelihood of the samples under the modeleddistribution. Here we propose a new formulation of the reconstruction term forthe VAE that specifically penalizes the generation of blurry images while atthe same time still maximizing the ELBO under the modeled distribution. We showthe potential of the proposed loss on three different data sets, where itoutperforms several recently proposed reconstruction losses for VAEs.", "output": "Explicitly Minimizing the Blur Error of Variational Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual localization is a core component in many applications, includingaugmented reality (AR). Localization algorithms compute the camera pose of aquery image w.r.t. a scene representation, which is typically built fromimages. This often requires capturing and storing large amounts of data,followed by running Structure-from-Motion (SfM) algorithms. An interesting, andunderexplored, source of data for building scene representations are 3D modelsthat are readily available on the Internet, e.g., hand-drawn CAD models, 3Dmodels generated from building footprints, or from aerial images. These modelsallow to perform visual localization right away without the time-consumingscene capturing and model building steps. Yet, it also comes with challenges asthe available 3D models are often imperfect reflections of reality. E.g., themodels might only have generic or no textures at all, might only provide asimple approximation of the scene geometry, or might be stretched. This paperstudies how the imperfections of these models affect localization accuracy. Wecreate a new benchmark for this task and provide a detailed experimentalevaluation based on multiple 3D models per scene. We show that 3D models fromthe Internet show promise as an easy-to-obtain scene representation. At thesame time, there is significant room for improvement for visual localizationpipelines. To foster research on this interesting and challenging task, werelease our benchmark at v-pnk.github.io/cadloc.", "output": "Visual Localization using Imperfect 3D Models from the Internet."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continuous mid-air hand gesture recognition based on captured hand posestreams is fundamental for human-computer interaction, particularly in AR / VR.However, many of the methods proposed to recognize heterogeneous hand gesturesare tested only on the classification task, and the real-time low-latencygesture segmentation in a continuous stream is not well addressed in theliterature. For this task, we propose the On-Off deep Multi-View Multi-Taskparadigm (OO-dMVMT). The idea is to exploit multiple time-local views relatedto hand pose and movement to generate rich gesture descriptions, along withusing heterogeneous tasks to achieve high accuracy. OO-dMVMT extends theclassical MVMT paradigm, where all of the multiple tasks have to be active ateach time, by allowing specific tasks to switch on/off depending on whetherthey can apply to the input. We show that OO-dMVMT defines the new SotA oncontinuous/online 3D skeleton-based gesture recognition in terms of gestureclassification accuracy, segmentation accuracy, false positives, and decisionlatency while maintaining real-time operation.", "output": "OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for Real-time 3D Hand Gesture Classification and Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperspectral image (HSI) classification is an important topic in the fieldof remote sensing, and has a wide range of applications in Earth science. HSIscontain hundreds of continuous bands, which are characterized by high dimensionand high correlation between adjacent bands. The high dimension and redundancyof HSI data bring great difficulties to HSI classification. In recent years, alarge number of HSI feature extraction and classification methods based on deeplearning have been proposed. However, their ability to model the globalrelationships among samples in both spatial and spectral domains is stilllimited. In order to solve this problem, an HSI classification method withspectral-spatial diffusion models is proposed. The proposed method realizes thereconstruction of spectral-spatial distribution of the training samples withthe forward and reverse spectral-spatial diffusion process, thus modeling theglobal spatial-spectral relationship between samples. Then, we use thespectral-spatial denoising network of the reverse process to extract theunsupervised diffusion features. Features extracted by the spectral-spatialdiffusion models can achieve cross-sample perception from the reconstructeddistribution of the training samples, thus obtaining better classificationperformance. Experiments on three public HSI datasets show that the proposedmethod can achieve better performance than the state-of-the-art methods. Thesource code and the pre-trained spectral-spatial diffusion model will bepublicly available at ", "output": "SpectralDiff: Hyperspectral Image Classification with Spectral-Spatial Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present ImageReward -- the first general-purpose text-to-image humanpreference reward model -- to address various prevalent issues in generativemodels and align them with human values and preferences. Its training is basedon our systematic annotation pipeline that covers both the rating and rankingcomponents, collecting a dataset of 137k expert comparisons to date. In humanevaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by38.6%), making it a promising automatic metric for evaluating and improvingtext-to-image synthesis. The reward model is publicly available via thetexttt{image-reward} package at url{", "output": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Acquiring knowledge about object interactions and affordances can facilitatescene understanding and human-robot collaboration tasks. As humans tend to useobjects in many different ways depending on the scene and the objects'availability, learning object affordances in everyday-life scenarios is achallenging task, particularly in the presence of an open set of interactionsand objects. We address the problem of affordance categorization forclass-agnostic objects with an open set of interactions; we achieve this bylearning similarities between object interactions in an unsupervised way andthus inducing clusters of object affordances. A novel depth-informedqualitative spatial representation is proposed for the construction of ActivityGraphs (AGs), which abstract from the continuous representation ofspatio-temporal interactions in RGB-D videos. These AGs are clustered to obtaingroups of objects with similar affordances. Our experiments in a real-worldscenario demonstrate that our method learns to create object affordanceclusters with a high V-measure even in cluttered scenes. The proposed approachhandles object occlusions by capturing effectively possible interactions andwithout imposing any object or scene constraints.", "output": "Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, the success of large-scale vision-language models (VLMs)such as CLIP has led to their increased usage in various computer vision tasks.These models enable zero-shot inference through carefully crafted instructionaltext prompts without task-specific supervision. However, the potential of VLMsfor generalization tasks in remote sensing (RS) has not been fully realized. Toaddress this research gap, we propose a novel image-conditioned prompt learningstrategy called the Visual Attention Parameterized Prompts Learning Network(APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learningin RS scene classification and disentangles visual style and content primitivesfor domain generalization tasks. To achieve this, APPLeNet combines visualcontent features obtained from different layers of the vision encoder and styleproperties obtained from feature statistics of domain-specific batches. Anattention-driven injection module is further introduced to generate visualtokens from this information. We also introduce an anti-correlation regularizerto ensure discrimination among the token embeddings, as this visual informationis combined with the textual tokens. To validate APPLeNet, we curated fouravailable RS benchmarks and introduced experimental protocols and datasets forthree domain generalization tasks. Our results consistently outperform therelevant literature and code is available at", "output": "APPLeNet: Visual Attention Parameterized Prompt Learning for Few-Shot Remote Sensing Image Generalization using CLIP."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The swift and precise detection of vehicles holds significant researchsignificance in intelligent transportation systems (ITS). However, currentvehicle detection algorithms encounter challenges such as high computationalcomplexity, low detection rate, and limited feasibility on mobile devices. Toaddress these issues, this paper proposes a lightweight vehicle detectionalgorithm for YOLOv7-tiny called Ghost-YOLOv7. The model first scales the widthmultiple to 0.5 and replaces the standard convolution of the backbone networkwith Ghost convolution to achieve a lighter network and improve the detectionspeed; secondly, a Ghost bi-directional feature pyramid network (Ghost-BiFPN)neck network is designed to enhance feature extraction capability of thealgorithm and enrich semantic information; thirdly, a Ghost Decouoled Head(GDH) is employed for accurate prediction of vehicle location and class,enhancing model accuracy; finally, a coordinate attention mechanism isintroduced in the output layer to suppress environmental interference, and theWIoU loss function is employed to enhance the detection accuracy further.Experimental results on the PASCAL VOC dataset demonstrate that Ghost-YOLOv7outperforms the original YOLOv7-tiny model, achieving a 29.8% reduction incomputation, 37.3% reduction in the number of parameters, 35.1% reduction inmodel weights, and 1.1% higher mean average precision (mAP), while achieving adetection speed of 428 FPS. These results validate the effectiveness of theproposed method.", "output": "Fast vehicle detection algorithm based on lightweight YOLO7-tiny."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the realm of 3D-computer vision applications, point cloud few-shotlearning plays a critical role. However, it poses an arduous challenge due tothe sparsity, irregularity, and unordered nature of the data. Current methodsrely on complex local geometric extraction techniques such as convolution,graph, and attention mechanisms, along with extensive data-driven pre-trainingtasks. These approaches contradict the fundamental goal of few-shot learning,which is to facilitate efficient learning. To address this issue, we proposeGPr-Net (Geometric Prototypical Network), a lightweight and computationallyefficient geometric prototypical network that captures the intrinsic topologyof point clouds and achieves superior performance. Our proposed method, IGI++(Intrinsic Geometry Interpreter++) employs vector-based hand-crafted intrinsicgeometry interpreters and Laplace vectors to extract and evaluate point cloudmorphology, resulting in improved representations for FSL (Few-Shot Learning).Additionally, Laplace vectors enable the extraction of valuable features frompoint clouds with fewer points. To tackle the distribution drift challenge infew-shot metric learning, we leverage hyperbolic space and demonstrate that ourapproach handles intra and inter-class variance better than existing pointcloud few-shot learning methods. Experimental results on the ModelNet40 datasetshow that GPr-Net outperforms state-of-the-art methods in few-shot learning onpoint clouds, achieving utmost computational efficiency that is $170times$better than all existing works. The code is publicly available at", "output": "GPr-Net: Geometric Prototypical Network for Point Cloud Few-Shot Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computer vision applications in transportation logistics and warehousing havea huge potential for process automation. We present a structured literaturereview on research in the field to help leverage this potential. All literatureis categorized w.r.t. the application, i.e. the task it tackles and w.r.t. thecomputer vision techniques that are used. Regarding applications, we subdividethe literature in two areas: Monitoring, i.e. observing and retrieving relevantinformation from the environment, and manipulation, where approaches are usedto analyze and interact with the environment. In addition to that, we point outdirections for future research and link to recent developments in computervision that are suitable for application in logistics. Finally, we present anoverview of existing datasets and industrial solutions. We conclude that whilealready many research areas have been investigated, there is still hugepotential for future research. The results of our analysis are also availableonline at ", "output": "Literature Review: Computer Vision Applications in Transportation Logistics and Warehousing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The most recent efforts in video matting have focused on eliminating trimapdependency since trimap annotations are expensive and trimap-based methods areless adaptable for real-time applications. Despite the latest tripmap-freemethods showing promising results, their performance often degrades whendealing with highly diverse and unstructured videos. We address this limitationby introducing Adaptive Matting for Dynamic Videos, termed AdaM, which is aframework designed for simultaneously differentiating foregrounds frombackgrounds and capturing alpha matte details of human subjects in theforeground. Two interconnected network designs are employed to achieve thisgoal: (1) an encoder-decoder network that produces alpha mattes andintermediate masks which are used to guide the transformer in adaptivelydecoding foregrounds and backgrounds, and (2) a transformer network in whichlong- and short-term attention combine to retain spatial and temporal contexts,facilitating the decoding of foreground details. We benchmark and study ourmethods on recently introduced datasets, showing that our model notablyimproves matting realism and temporal coherence in complex real-world videosand achieves new best-in-class generalizability. Further details and examplesare available at ", "output": "Adaptive Human Matting for Dynamic Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the difficulty in collecting large-scale and perfectly aligned pairedtraining data for Under-Display Camera (UDC) image restoration, previousmethods resort to monitor-based image systems or simulation-based methods,sacrificing the realness of the data and introducing domain gaps. In this work,we revisit the classic stereo setup for training data collection -- capturingtwo images of the same scene with one UDC and one standard camera. The key ideais to \"copy\" details from a high-quality reference image and \"paste\" them onthe UDC image. While being able to generate real training pairs, this settingis susceptible to spatial misalignment due to perspective and depth of fieldchanges. The problem is further compounded by the large domain discrepancybetween the UDC and normal images, which is unique to UDC restoration. In thispaper, we mitigate the non-trivial domain discrepancy and spatial misalignmentthrough a novel Transformer-based framework that generates well-aligned yethigh-quality target data for the corresponding UDC input. This is made possiblethrough two carefully designed components, namely, the Domain Alignment Module(DAM) and Geometric Alignment Module (GAM), which encourage robust and accuratediscovery of correspondence between the UDC and normal views. Extensiveexperiments show that high-quality and well-aligned pseudo UDC training pairsare beneficial for training a robust restoration network. Code and the datasetare available at ", "output": "Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose $textbf{VidStyleODE}$, a spatiotemporally continuous disentangled$textbf{Vid}$eo representation based upon $textbf{Style}$GAN andNeural-$textbf{ODE}$s. Effective traversal of the latent space learned byGenerative Adversarial Networks (GANs) has been the basis for recentbreakthroughs in image editing. However, the applicability of such advancementsto the video domain has been hindered by the difficulty of representing andcontrolling videos in the latent space of GANs. In particular, videos arecomposed of content (i.e., appearance) and complex motion components thatrequire a special mechanism to disentangle and control. To achieve this,VidStyleODE encodes the video content in a pre-trained StyleGAN $mathcal{W}_+$space and benefits from a latent ODE component to summarize the spatiotemporaldynamics of the input video. Our novel continuous video generation process thencombines the two to generate high-quality and temporally consistent videos withvarying frame rates. We show that our proposed method enables a variety ofapplications on real videos: text-guided appearance manipulation, motionmanipulation, image animation, and video interpolation and extrapolation.Project website: ", "output": "VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a new annotation method called Sparse Annotation (SA) forcrowd counting, which reduces human labeling efforts by sparsely labelingindividuals in an image. We argue that sparse labeling can reduce theredundancy of full annotation and capture more diverse information from distantindividuals that is not fully captured by Partial Annotation methods. Besides,we propose a point-based Progressive Point Matching network (PPM) to betterexplore the crowd from the whole image with sparse annotation, which includes aProposal Matching Network (PMN) and a Performance Restoration Network (PRN).The PMN generates pseudo-point samples using a basic point classifier, whilethe PRN refines the point classifier with the pseudo points to maximizeperformance. Our experimental results show that PPM outperforms previoussemi-supervised crowd counting methods with the same amount of annotation by alarge margin and achieves competitive performance with state-of-the-artfully-supervised methods.", "output": "Crowd Counting with Sparse Annotation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Segmenting anything is a ground-breaking step toward artificial generalintelligence, and the Segment Anything Model (SAM) greatly fosters thefoundation models for computer vision. We could not be more excited to probethe performance traits of SAM. In particular, exploring situations in which SAMdoes not perform well is interesting. In this report, we choose three concealedscenes, i.e., camouflaged animals, industrial defects, and medical lesions, toevaluate SAM under unprompted settings. Our main observation is that SAM looksunskilled in concealed scenes.", "output": "SAM Struggles in Concealed Scenes -- Empirical Study on \"Segment Anything\"."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic perception of human behaviors during social interactions is crucialfor AR/VR applications, and an essential component is estimation of plausible3D human pose and shape of our social partners from the egocentric view. One ofthe biggest challenges of this task is severe body truncation due to closesocial distances in egocentric scenarios, which brings large pose ambiguitiesfor unseen body parts. To tackle this challenge, we propose a novelscene-conditioned diffusion method to model the body pose distribution.Conditioned on the 3D scene geometry, the diffusion model generates bodies inplausible human-scene interactions, with the sampling guided by a physics-basedcollision score to further resolve human-scene inter-penetrations. Theclassifier-free training enables flexible sampling with different conditionsand enhanced diversity. A visibility-aware graph convolution model guided byper-joint visibility serves as the diffusion denoiser to incorporateinter-joint dependencies and per-body-part control. Extensive evaluations showthat our method generates bodies in plausible interactions with 3D scenes,achieving both superior accuracy for visible joints and diversity for invisiblebody parts. The code will be available at", "output": "Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present DreamPose, a diffusion-based method for generating animatedfashion videos from still images. Given an image and a sequence of human bodyposes, our method synthesizes a video containing both human and fabric motion.To achieve this, we transform a pretrained text-to-image model (StableDiffusion) into a pose-and-image guided video synthesis model, using a novelfinetuning strategy, a set of architectural changes to support the addedconditioning signals, and techniques to encourage temporal consistency. Wefine-tune on a collection of fashion videos from the UBC Fashion dataset. Weevaluate our method on a variety of clothing styles and poses, and demonstratethat our method produces state-of-the-art results on fashion video animation.Video results are available on our project page.", "output": "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works demonstrate a remarkable ability to customize text-to-imagediffusion models while only providing a few example images. What happens if youtry to customize such models using multiple, fine-grained concepts in asequential (i.e., continual) manner? In our work, we show that recentstate-of-the-art customization of text-to-image models suffer from catastrophicforgetting when new concepts arrive sequentially. Specifically, when adding anew concept, the ability to generate high quality images of past, similarconcepts degrade. To circumvent this forgetting, we propose a new method,C-LoRA, composed of a continually self-regularized low-rank adaptation in crossattention layers of the popular Stable Diffusion model. Furthermore, we usecustomization prompts which do not include the word of the customized object(i.e., \"person\" for a human face dataset) and are initialized as completelyrandom embeddings. Importantly, our method induces only marginal additionalparameter costs and requires no storage of user data for replay. We show thatC-LoRA not only outperforms several baselines for our proposed setting oftext-to-image continual customization, which we refer to as ContinualDiffusion, but that we achieve a new state-of-the-art in the well-establishedrehearsal-free continual learning setting for image classification. The highachieving performance of C-LoRA in two separate domains positions it as acompelling solution for a wide range of applications, and we believe it hassignificant potential for practical impact.", "output": "Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present RECLIP (Resource-efficient CLIP), a simple method that minimizescomputational resource footprint for CLIP (Contrastive Language ImagePretraining). Inspired by the notion of coarse-to-fine in computer vision, weleverage small images to learn from large-scale language supervisionefficiently, and finetune the model with high-resolution data in the end. Sincethe complexity of the vision transformer heavily depends on input image size,our approach significantly reduces the training resource requirements both intheory and in practice. Using the same batch size and training epoch, RECLIPachieves highly competitive zero-shot classification and image text retrievalaccuracy with 6 to 8$times$ less computational resources and 7 to 9$times$fewer FLOPs than the baseline. Compared to the state-of-the-art contrastivelearning methods, RECLIP demonstrates 5 to 59$times$ training resource savingswhile maintaining highly competitive zero-shot classification and retrievalperformance. We hope this work will pave the path for the broader researchcommunity to explore language supervised pretraining in more resource-friendlysettings.", "output": "RECLIP: Resource-efficient CLIP by Training with Small Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We analyze the spaces of images encoded by generative neural networks of theBigGAN architecture. We find that generic multiplicative perturbations ofneural network parameters away from the photo-realistic point often lead tonetworks generating images which appear as \"artistic renditions\" of thecorresponding objects. This demonstrates an emergence of aesthetic propertiesdirectly from the structure of the photo-realistic visual environment asencoded in its neural network parametrization. Moreover, modifying a deepsemantic part of the neural network leads to the appearance of symbolic visualrepresentations. None of the considered networks had any access to images ofhuman-made art.", "output": "Aesthetics and neural network image representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most feedforward convolutional neural networks spend roughly the same effortsfor each pixel. Yet human visual recognition is an interaction between eyemovements and spatial attention, which we will have several glimpses of anobject in different regions. Inspired by this observation, we propose anend-to-end trainable Multi-Glimpse Network (MGNet) which aims to tackle thechallenges of high computation and the lack of robustness based on recurrentdownsampled attention mechanism. Specifically, MGNet sequentially selectstask-relevant regions of an image to focus on and then adaptively combines allcollected information for the final prediction. MGNet expresses strongresistance against adversarial attacks and common corruptions with lesscomputation. Also, MGNet is inherently more interpretable as it explicitlyinforms us where it focuses during each iteration. Our experiments onImageNet100 demonstrate the potential of recurrent downsampled attentionmechanisms to improve a single feedforward manner. For example, MGNet improves4.76% accuracy on average in common corruptions with only 36.9% computationalcost. Moreover, while the baseline incurs an accuracy drop to 7.6%, MGNetmanages to maintain 44.2% accuracy in the same PGD attack strength withResNet-50 backbone. Our code is available at", "output": "Multi-Glimpse Network: A Robust and Efficient Classification Architecture based on Recurrent Downsampled Attention."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a combined scaling method - named BASIC - that achieves 85.7%top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning fromany labeled ImageNet example. This accuracy surpasses best published similarmodels - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significantimprovements in robustness benchmarks. For instance, on 5 test sets withnatural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, ourmodel achieves 84.3% top-1 average accuracy, only a small drop from itsoriginal ImageNet accuracy. To achieve these results, we scale up thecontrastive learning framework of CLIP and ALIGN in three dimensions: datasize, model size, and batch size. Our dataset has 6.6B noisy image-text pairs,which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs thanALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and4x more than ALIGN. We encountered two main challenges with the scaling rulesof BASIC. First, the main challenge with implementing the combined scalingrules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. Toovercome the memory limit, we propose two simple methods which make use ofgradient checkpointing and model parallelism. Second, while increasing thedataset size and the model size has been the defacto method to improve theperformance of deep learning models like BASIC, the effect of a largecontrastive batch size on such contrastive-trained image-text models is notwell-understood. To shed light on the benefits of large contrastive batchsizes, we develop a theoretical framework which shows that larger contrastivebatch sizes lead to smaller generalization gaps for image-text models such asBASIC.", "output": "Combined Scaling for Zero-shot Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of structure from motion is concerned with recovering3-dimensional structure of an object from a set of 2-dimensional images.Generally, all information can be uniquely recovered if enough images and imagepoints are provided, but there are certain cases where unique recovery isimpossible; these are called critical configurations. In this paper we use analgebraic approach to study the critical configurations for two projectivecameras. We show that all critical configurations lie on quadric surfaces, andclassify exactly which quadrics constitute a critical configuration. The paperalso describes the relation between the different reconstructions when uniquereconstruction is impossible.", "output": "Critical configurations for two projective views, a new approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The generic object detection (GOD) task has been successfully tackled byrecent deep neural networks, trained by an avalanche of annotated trainingsamples from some common classes. However, it is still non-trivial togeneralize these object detectors to the novel long-tailed object classes,which have only few labeled training samples. To this end, the Few-Shot ObjectDetection (FSOD) has been topical recently, as it mimics the humans' ability oflearning to learn, and intelligently transfers the learned generic objectknowledge from the common heavy-tailed, to the novel long-tailed objectclasses. Especially, the research in this emerging field has been flourishingin recent years with various benchmarks, backbones, and methodologies proposed.To review these FSOD works, there are several insightful FSOD survey articles[58, 59, 74, 78] that systematically study and compare them as the groups offine-tuning/transfer learning, and meta-learning methods. In contrast, wereview the existing FSOD algorithms from a new perspective under a new taxonomybased on their contributions, i.e., data-oriented, model-oriented, andalgorithm-oriented. Thus, a comprehensive survey with performance comparison isconducted on recent achievements of FSOD. Furthermore, we also analyze thetechnical challenges, the merits and demerits of these methods, and envisionthe future directions of FSOD. Specifically, we give an overview of FSOD,including the problem definition, common datasets, and evaluation protocols.The taxonomy is then proposed that groups FSOD methods into three types.Following this taxonomy, we provide a systematic review of the advances inFSOD. Finally, further discussions on performance, challenges, and futuredirections are presented.", "output": "Recent Few-Shot Object Detection Algorithms: A Survey with Performance Comparison."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the success of convolutional neural networks (CNNs) in numerouscomputer vision tasks and their extraordinary generalization performances,several attempts to predict the generalization errors of CNNs have only beenlimited to a posteriori analyses thus far. A priori theories explaining thegeneralization performances of deep neural networks have mostly ignored theconvolutionality aspect and do not specify why CNNs are able to seeminglyovercome curse of dimensionality on computer vision tasks like imageclassification where the image dimensions are in thousands. Our work attemptsto explain the generalization performance of CNNs on image classification underthe hypothesis that CNNs operate on the domain of image patches. Ours is thefirst work we are aware of to derive an a priori error bound for thegeneralization error of CNNs and we present both quantitative and qualitativeevidences in the support of our theory. Our patch-based theory also offersexplanation for why data augmentation techniques like Cutout, CutMix and randomcropping are effective in improving the generalization error of CNNs.", "output": "CNNs Avoid Curse of Dimensionality by Learning on Patches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatiotemporal predictive learning aims to generate future frames by learningfrom historical frames. In this paper, we investigate existing methods andpresent a general framework of spatiotemporal predictive learning, in which thespatial encoder and decoder capture intra-frame features and the middletemporal module catches inter-frame correlations. While the mainstream methodsemploy recurrent units to capture long-term temporal dependencies, they sufferfrom low computational efficiency due to their unparallelizable architectures.To parallelize the temporal module, we propose the Temporal Attention Unit(TAU), which decomposes the temporal attention into intra-frame staticalattention and inter-frame dynamical attention. Moreover, while the mean squarederror loss focuses on intra-frame errors, we introduce a novel differentialdivergence regularization to take inter-frame variations into account.Extensive experiments demonstrate that the proposed method enables the derivedmodel to achieve competitive performance on various spatiotemporal predictionbenchmarks.", "output": "Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Oversight AI is an emerging concept in radiology where the AI forms asymbiosis with radiologists by continuously supporting radiologists in theirdecision-making. Recent advances in vision-language models sheds a light on thelong-standing problems of the oversight AI by the understanding both visual andtextual concepts and their semantic correspondences. However, there have beenlimited successes in the application of vision-language models in the medicaldomain, as the current vision-language models and learning strategies forphotographic images and captions call for the web-scale data corpus of imageand text pairs which was not often feasible in the medical domain. To addressthis, here we present a model dubbed Medical Cross-attention Vision-Languagemodel (Medical X-VL), leveraging the key components to be tailored for themedical domain. Our medical X-VL model is based on the following components:self-supervised uni-modal models in medical domain and fusion encoder to bridgethem, momentum distillation, sentence-wise contrastive learning for medicalreports, and the sentence similarity-adjusted hard negative mining. Weexperimentally demonstrated that our model enables various zero-shot tasks foroversight AI, ranging from the zero-shot classification to zero-shot errorcorrection. Our model outperformed the current state-of-the-art models in twodifferent medical image database, suggesting the novel clinical usage of ouroversight AI model for monitoring human errors. Our method was especiallysuccessful in the data-limited setting, which is frequently encountered in theclinics, suggesting the potential widespread applicability in medical domain.", "output": "Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural implicit representations are drawing a lot of attention from therobotics community recently, as they are expressive, continuous and compact.However, city-scale continual implicit dense mapping based on sparse LiDARinput is still an under-explored challenge. To this end, we successfully builda city-scale continual neural mapping system with a panoptic representationthat consists of environment-level and instance-level modelling. Given a streamof sparse LiDAR point cloud, it maintains a dynamic generative model that maps3D coordinates to signed distance field (SDF) values. To address the difficultyof representing geometric information at different levels in city-scale space,we propose a tailored three-layer sampling strategy to dynamically sample theglobal, local and near-surface domains. Meanwhile, to realize high fidelitymapping of instance under incomplete observation, category-specific prior isintroduced to better model the geometric details. We evaluate on the publicSemanticKITTI dataset and demonstrate the significance of the newly proposedthree-layer sampling strategy and panoptic representation, using bothquantitative and qualitative results. Codes and model will be publiclyavailable.", "output": "City-scale Incremental Neural Mapping with Three-layer Sampling and Panoptic Representation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern 3D semantic instance segmentation approaches predominantly rely onspecialized voting mechanisms followed by carefully designed geometricclustering techniques. Building on the successes of recent Transformer-basedmethods for object detection and image segmentation, we propose the firstTransformer-based approach for 3D semantic instance segmentation. We show thatwe can leverage generic Transformer building blocks to directly predictinstance masks from 3D point clouds. In our model called Mask3D each objectinstance is represented as an instance query. Using Transformer decoders, theinstance queries are learned by iteratively attending to point cloud featuresat multiple scales. Combined with point features, the instance queries directlyyield all instance masks in parallel. Mask3D has several advantages overcurrent state-of-the-art approaches, since it neither relies on (1) votingschemes which require hand-selected geometric properties (such as centers) nor(2) geometric grouping mechanisms requiring manually-tuned hyper-parameters(e.g. radii) and (3) enables a loss that directly optimizes instance masks.Mask3D sets a new state-of-the-art on ScanNet test (+6.2 mAP), S3DIS 6-fold(+10.1 mAP), STPLS3D (+11.2 mAP) and ScanNet200 test (+12.4 mAP).", "output": "Mask3D: Mask Transformer for 3D Semantic Instance Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we first extend the recent Masked Auto-Encoder (MAE) modelfrom a single modality to audio-visual multi-modalities. Subsequently, wepropose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combiningcontrastive learning and masked data modeling, two major self-supervisedlearning frameworks, to learn a joint and coordinated audio-visualrepresentation. Our experiments show that the contrastive audio-visualcorrespondence learning objective not only enables the model to performaudio-visual retrieval tasks, but also helps the model learn a better jointrepresentation. As a result, our fully self-supervised pretrained CAV-MAEachieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with theprevious best supervised pretrained model on AudioSet in the audio-visual eventclassification task. Code and pretrained models are at", "output": "Contrastive Audio-Visual Masked Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "During image editing, existing deep generative models tend to re-synthesizethe entire output from scratch, including the unedited regions. This leads to asignificant waste of computation, especially for minor editing operations. Inthis work, we present Spatially Sparse Inference (SSI), a general-purposetechnique that selectively performs computation for edited regions andaccelerates various generative models, including both conditional GANs anddiffusion models. Our key observation is that users tend to gradually changethe input image. This motivates us to cache and reuse the feature maps of theoriginal image. Given an edited image, we sparsely apply the convolutionalfilters to the edited regions while reusing the cached features for theunedited areas. Based on our algorithm, we further propose Sparse IncrementalGenerative Engine (SIGE) to convert the computation reduction to latencyreduction on off-the-shelf hardware. With about $1%$-area edits, our methodreduces the computation of DDPM by $7.5times$, Stable Diffusion by$8.2times$, and GauGAN by $18times$ while preserving the visual fidelity.With SIGE, we accelerate the inference time of DDPM by $3.0times$ on NVIDIARTX 3090 and $6.6times$ on Apple M1 Pro CPU, Stable Diffusion by $7.2times$on 3090, and GauGAN by $5.6times$ on 3090 and $14times$ on M1 Pro CPU.", "output": "Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In intensive care units (ICUs), critically ill patients are monitored withelectroencephalograms (EEGs) to prevent serious brain injury. The number ofpatients who can be monitored is constrained by the availability of trainedphysicians to read EEGs, and EEG interpretation can be subjective and prone tointer-observer variability. Automated deep learning systems for EEG couldreduce human bias and accelerate the diagnostic process. However, black boxdeep learning models are untrustworthy, difficult to troubleshoot, and lackaccountability in real-world applications, leading to a lack of trust andadoption by clinicians. To address these challenges, we propose a novelinterpretable deep learning model that not only predicts the presence ofharmful brainwave patterns but also provides high-quality case-basedexplanations of its decisions. Our model performs better than the correspondingblack box model, despite being constrained to be interpretable. The learned 2Dembedded space provides the first global overview of the structure ofictal-interictal-injury continuum brainwave patterns. The ability to understandhow our model arrived at its decisions will not only help clinicians todiagnose and treat harmful brain activities more accurately but also increasetheir trust and adoption of machine learning models in clinical practice; thiscould be an integral component of the ICU neurologists' standard workflow.", "output": "Interpretable Machine Learning System to EEG Patterns on the Ictal-Interictal-Injury Continuum."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The progress in modelling time series and, more generally, sequences ofstructured data has recently revamped research in anomaly detection. The taskstands for identifying abnormal behaviors in financial series, IT systems,aerospace measurements, and the medical domain, where anomaly detection may aidin isolating cases of depression and attend the elderly. Anomaly detection intime series is a complex task since anomalies are rare due to highly non-lineartemporal correlations and since the definition of anomalous is sometimessubjective. Here we propose the novel use of Hyperbolic uncertainty for AnomalyDetection (HypAD). HypAD learns self-supervisedly to reconstruct the inputsignal. We adopt best practices from the state-of-the-art to encode thesequence by an LSTM, jointly learned with a decoder to reconstruct the signal,with the aid of GAN critics. Uncertainty is estimated end-to-end by means of ahyperbolic neural network. By using uncertainty, HypAD may assess whether it iscertain about the input signal but it fails to reconstruct it because this isanomalous; or whether the reconstruction error does not necessarily implyanomaly, as the model is uncertain, e.g. a complex but regular input signal.The novel key idea is that a detectable anomaly is one where the model iscertain but it predicts wrongly. HypAD outperforms the current state-of-the-artfor univariate anomaly detection on established benchmarks based on data fromNASA, Yahoo, Numenta, Amazon, and Twitter. It also yields state-of-the-artperformance on a multivariate dataset of anomaly activities in elderly homeresidences, and it outperforms the baseline on SWaT. Overall, HypAD yields thelowest false alarms at the best performance rate, thanks to successfullyidentifying detectable anomalies.", "output": "Are we certain it's anomalous?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the context of biometrics, matching confidence refers to the confidencethat a given matching decision is correct. Since many biometric systems operatein critical decision-making processes, such as in forensics investigations,accurately and reliably stating the matching confidence becomes of highimportance. Previous works on biometric confidence estimation can welldifferentiate between high and low confidence, but lack interpretability.Therefore, they do not provide accurate probabilistic estimates of thecorrectness of a decision. In this work, we propose a probabilisticinterpretable comparison (PIC) score that accurately reflects the probabilitythat the score originates from samples of the same identity. We prove that theproposed approach provides optimal matching confidence. Contrary to otherapproaches, it can also optimally combine multiple samples in a joint PIC scorewhich further increases the recognition and confidence estimation performance.In the experiments, the proposed PIC approach is compared against all biometricconfidence estimation methods available on four publicly available databasesand five state-of-the-art face recognition systems. The results demonstratethat PIC has a significantly more accurate probabilistic interpretation thansimilar approaches and is highly effective for multi-biometric recognition. Thecode is publicly-available.", "output": "PIC-Score: Probabilistic Interpretable Comparison Score for Optimal Matching Confidence in Single- and Multi-Biometric (Face) Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have revealed that, beyond conventional accuracy, calibrationshould also be considered for training modern deep neural networks. To addressmiscalibration during learning, some methods have explored different penaltyfunctions as part of the learning objective, alongside a standardclassification loss, with a hyper-parameter controlling the relativecontribution of each term. Nevertheless, these methods share two majordrawbacks: 1) the scalar balancing weight is the same for all classes,hindering the ability to address different intrinsic difficulties or imbalanceamong classes; and 2) the balancing weight is usually fixed without an adaptivestrategy, which may prevent from reaching the best compromise between accuracyand calibration, and requires hyper-parameter search for each application. Wepropose Class Adaptive Label Smoothing (CALS) for calibrating deep networks,which allows to learn class-wise multipliers during training, yielding apowerful alternative to common label smoothing penalties. Our method builds ona general Augmented Lagrangian approach, a well-established technique inconstrained optimization, but we introduce several modifications to tailor itfor large-scale, class-adaptive training. Comprehensive evaluation and multiplecomparisons on a variety of benchmarks, including standard and long-tailedimage classification, semantic segmentation, and text classification,demonstrate the superiority of the proposed method. The code is available at", "output": "Class Adaptive Network Calibration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face animation has received a lot of attention from researchers in recentyears due to its wide range of promising applications. Many face animationmodels based on optical flow or deep neural networks have achieved greatsuccess. However, these models are likely to fail in animated scenarios withsignificant view changes, resulting in unrealistic or distorted faces. One ofthe possible reasons is that such models lack prior knowledge of human facesand are not proficient to imagine facial regions they have never seen before.In this paper, we propose a flexible and generic approach to improve theperformance of face animation without additional training. We use multiplesource images as input as compensation for the lack of prior knowledge offaces. The effectiveness of our method is experimentally demonstrated, wherethe proposed method successfully supplements the baseline method.", "output": "Face Animation with Multiple Source Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic image colorization is a challenging problem. Due to the highillness and multi-modal uncertainty, directly training a deep neural networkusually leads to incorrect semantic colors and low color richness. Recenttransformer-based methods can deliver better results, but they often rely onmanually designed priors, which are hard to implement and suffer from poorgeneralization ability. Moreover, they tend to introduce serious color bleedingeffects since color attention is performed on single-scale features, thus failto exploit sufficient semantic information. To address these issues, we proposeDDColor, a new end-to-end method with dual decoders for image colorization. Ourapproach includes a multi-scale image decoder and a transformer-based colordecoder. The former restores the spatial resolution of the image, while thelatter establishes the correlation between color and semantic representationsvia cross-attention. Rather than using additional priors, our two decoders worktogether to leverage multi-scale image features to guide optimization ofadaptive color queries, significantly alleviating color bleeding effects. Inaddition, a simple yet effective colorfulness loss is introduced to furtherenhance the color richness of generated results. Our extensive experimentsdemonstrate that DDColor achieves significantly superior performance toexisting state-of-the-art works both quantitatively and qualitatively. Codeswill be made publicly available at ", "output": "DDColor: Towards Photo-Realistic and Semantic-Aware Image Colorization via Dual Decoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Some recent artificial neural networks (ANNs) claim to model aspects ofprimate neural and human performance data. Their success in object recognitionis, however, dependent on exploiting low-level features for solving visualtasks in a way that humans do not. As a result, out-of-distribution oradversarial input is often challenging for ANNs. Humans instead learn abstractpatterns and are mostly unaffected by many extreme image distortions. Weintroduce a set of novel image transforms inspired by neurophysiologicalfindings and evaluate humans and ANNs on an object recognition task. We showthat machines perform better than humans for certain transforms and struggle toperform at par with humans on others that are easy for humans. We quantify thedifferences in accuracy for humans and machines and find a ranking ofdifficulty for our transforms for human data. We also suggest how certaincharacteristics of human visual processing can be adapted to improve theperformance of ANNs for our difficult-for-machines transforms.", "output": "Extreme Image Transformations Affect Humans and Machines Differently."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding event relationships in videos requires a model to understandthe underlying structures of events (i.e. the event type, the associatedargument roles, and corresponding entities) and factual knowledge forreasoning. Structural symbolic representation (SSR) based methods directly takeevent types and associated argument roles/entities as inputs to performreasoning. However, the state-of-the-art video event-relation prediction systemshows the necessity of using continuous feature vectors from input videos;existing methods based solely on SSR inputs fail completely, even when givenoracle event types and argument roles. In this paper, we conduct an extensiveempirical analysis to answer the following questions: 1) why SSR-based methodfailed; 2) how to understand the evaluation setting of video event relationprediction properly; 3) how to uncover the potential of SSR-based methods. Wefirst identify suboptimal training settings as causing the failure of previousSSR-based video event prediction models. Then through qualitative andquantitative analysis, we show how evaluation that takes only video as inputsis currently unfeasible, as well as the reliance on oracle event information toobtain an accurate evaluation. Based on these findings, we propose to furthercontextualize the SSR-based model to an Event-Sequence Model and equip it withmore factual knowledge through a simple yet effective way of reformulatingexternal visual commonsense knowledge bases into an event-relation predictionpretraining dataset. The resultant new state-of-the-art model eventuallyestablishes a 25% Macro-accuracy performance boost.", "output": "In Defense of Structural Symbolic Representation for Video Event-Relation Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-target multi-camera tracking (MTMCT) of vehicles, i.e. trackingvehicles across multiple cameras, is a crucial application for the developmentof smart city and intelligent traffic system. The main challenges of MTMCT ofvehicles include the intra-class variability of the same vehicle andinter-class similarity between different vehicles and how to associate the samevehicle accurately across different cameras under large search space. Previousmethods for MTMCT usually use hierarchical clustering of trajectories toconduct cross camera association. However, the search space can be large anddoes not take spatial and temporal information into consideration. In thispaper, we proposed a transformer-based camera link model with spatial andtemporal filtering to conduct cross camera tracking. Achieving 73.68% IDF1 onthe Nvidia Cityflow V2 dataset test set, showing the effectiveness of ourcamera link model on multi-target multi-camera tracking.", "output": "Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-view image compression plays a critical role in 3D-relatedapplications. Existing methods adopt a predictive coding architecture, whichrequires joint encoding to compress the corresponding disparity as well asresidual information. This demands collaboration among cameras and enforces theepipolar geometric constraint between different views, which makes itchallenging to deploy these methods in distributed camera systems with randomlyoverlapping fields of view. Meanwhile, distributed source coding theoryindicates that efficient data compression of correlated sources can be achievedby independent encoding and joint decoding, which motivates us to design alearning-based distributed multi-view image coding (LDMIC) framework. Withindependent encoders, LDMIC introduces a simple yet effective joint contexttransfer module based on the cross-attention mechanism at the decoder toeffectively capture the global inter-view correlations, which is insensitive tothe geometric relationships between images. Experimental results show thatLDMIC significantly outperforms both traditional and learning-based MIC methodswhile enjoying fast encoding speed. Code will be released at", "output": "LDMIC: Learning-based Distributed Multi-view Image Coding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive learning methods have been applied to a range of domains andmodalities by training models to identify similar \"views\" of data points.However, specialized scientific modalities pose a challenge for this paradigm,as identifying good views for each scientific instrument is complex andtime-intensive. In this paper, we focus on applying contrastive learningapproaches to a variety of remote sensing datasets. We show that Viewmakernetworks, a recently proposed method for generating views, are promising forproducing views in this setting without requiring extensive domain knowledgeand trial and error. We apply Viewmaker to four multispectral imaging problems,each with a different format, finding that Viewmaker can outperform cropping-and reflection-based methods for contrastive learning in every case whenevaluated on downstream classification tasks. This provides additional evidencethat domain-agnostic methods can empower contrastive learning to scale toreal-world scientific domains. Open source code can be found at", "output": "Multispectral Contrastive Learning with Viewmaker Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Digital twin is a problem of augmenting real objects with their digitalcounterparts. It can underpin a wide range of applications in augmented reality(AR), autonomy, and UI/UX. A critical component in a good digital-twin systemis real-time, accurate 3D object tracking. Most existing works solve 3D objecttracking through the lens of robotic grasping, employ older generations ofdepth sensors, and measure performance metrics that may not apply to otherdigital-twin applications such as in AR. In this work, we create a novel RGB-Ddataset, called Digital Twin Tracking Dataset (DTTD), to enable furtherresearch of the problem and extend potential solutions towards longer rangesand mm localization accuracy. To reduce point cloud noise from the inputsource, we select the latest Microsoft Azure Kinect as the state-of-the-arttime-of-flight (ToF) camera. In total, 103 scenes of 10 common off-the-shelfobjects with rich textures are recorded, with each frame annotated with aper-pixel semantic segmentation and ground-truth object poses provided by acommercial motion capturing system. Through extensive experiments withmodel-level and dataset-level analysis, we demonstrate that DTTD can helpresearchers develop future object tracking methods and analyze new challenges.The dataset, data generation, annotation, and model evaluation pipeline aremade publicly available as open source code at:", "output": "Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Field (NeRF) has broken new ground in the novel viewsynthesis due to its simple concept and state-of-the-art quality. However, itsuffers from severe performance degradation unless trained with a dense set ofimages with different camera poses, which hinders its practical applications.Although previous methods addressing this problem achieved promising results,they relied heavily on the additional training resources, which goes againstthe philosophy of sparse-input novel-view synthesis pursuing the trainingefficiency. In this work, we propose MixNeRF, an effective training strategyfor novel view synthesis from sparse inputs by modeling a ray with a mixturedensity model. Our MixNeRF estimates the joint distribution of RGB colors alongthe ray samples by modeling it with mixture of distributions. We also propose anew task of ray depth estimation as a useful training objective, which ishighly correlated with 3D scene geometry. Moreover, we remodel the colors withregenerated blending weights based on the estimated ray depth and furtherimproves the robustness for colors and viewpoints. Our MixNeRF outperformsother state-of-the-art methods in various standard benchmarks with superiorefficiency of training and inference.", "output": "MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite excellent average-case performance of many image classifiers, theirperformance can substantially deteriorate on semantically coherent subgroups ofthe data that were under-represented in the training data. These systematicerrors can impact both fairness for demographic minority groups as well asrobustness and safety under domain shift. A major challenge is to identify suchsubgroups with subpar performance when the subgroups are not annotated andtheir occurrence is very rare. We leverage recent advances in text-to-imagemodels and search in the space of textual descriptions of subgroups (\"prompts\")for subgroups where the target model has low performance on theprompt-conditioned synthesized data. To tackle the exponentially growing numberof subgroups, we employ combinatorial testing. We denote this procedure asPromptAttack as it can be interpreted as an adversarial attack in a promptspace. We study subgroup coverage and identifiability with PromptAttack in acontrolled setting and find that it identifies systematic errors with highaccuracy. Thereupon, we apply PromptAttack to ImageNet classifiers and identifynovel systematic errors on rare subgroups.", "output": "Identification of Systematic Errors of Image Classifiers on Rare Subgroups."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sign language recognition (SLR) is a weakly supervised task that annotatessign videos as textual glosses. Recent studies show that insufficient trainingcaused by the lack of large-scale available sign datasets becomes the mainbottleneck for SLR. Most SLR works thereby adopt pretrained visual modules anddevelop two mainstream solutions. The multi-stream architectures extendmulti-cue visual features, yielding the current SOTA performances but requiringcomplex designs and might introduce potential noise. Alternatively, theadvanced single-cue SLR frameworks using explicit cross-modal alignment betweenvisual and textual modalities are simple and effective, potentially competitivewith the multi-cue framework. In this work, we propose a novel contrastivevisual-textual transformation for SLR, CVT-SLR, to fully explore the pretrainedknowledge of both the visual and language modalities. Based on the single-cuecross-modal alignment framework, we propose a variational autoencoder (VAE) forpretrained contextual knowledge while introducing the complete pretrainedlanguage module. The VAE implicitly aligns visual and textual modalities whilebenefiting from pretrained contextual knowledge as the traditional contextualmodule. Meanwhile, a contrastive cross-modal alignment algorithm is designed toexplicitly enhance the consistency constraints. Extensive experiments on publicdatasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLRconsistently outperforms existing single-cue methods and even outperforms SOTAmulti-cue methods.", "output": "CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate morphological classification of white blood cells (WBCs) is animportant step in the diagnosis of leukemia, a disease in which nonfunctionalblast cells accumulate in the bone marrow. Recently, deep convolutional neuralnetworks (CNNs) have been successfully used to classify leukocytes by trainingthem on single-cell images from a specific domain. Most CNN models assume thatthe distributions of the training and test data are similar, i.e., the data areindependently and identically distributed. Therefore, they are not robust todifferent staining procedures, magnifications, resolutions, scanners, orimaging protocols, as well as variations in clinical centers or patientcohorts. In addition, domain-specific data imbalances affect the generalizationperformance of classifiers. Here, we train a robust CNN for WBC classificationby addressing cross-domain data imbalance and domain shifts. To this end, weuse two loss functions and demonstrate their effectiveness inout-of-distribution (OOD) generalization. Our approach achieves the best F1macro score compared to other existing methods and is able to consider rarecell types. This is the first demonstration of imbalanced domain generalizationin hematological cytomorphology and paves the way for robust single cellclassification methods for the application in laboratories and clinics.", "output": "Imbalanced Domain Generalization for Robust Single Cell Classification in Hematological Cytomorphology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Radiologists have different training and clinical experiences, which mayresult in various segmentation annotations for lung nodules, causingsegmentation uncertainty. Conventional methods usually select a singleannotation as the learning target or try to learn a latent space of variousannotations, but these approaches waste the valuable information of consensusor disagreements ingrained in the multiple annotations. In this paper, wepropose an Uncertainty-Aware Attention Mechanism (UAAM) that utilizes consensusand disagreements among multiple annotations to facilitate better segmentation.To achieve this, we introduce the Multi-Confidence Mask (MCM), which is acombination of a Low-Confidence (LC) Mask and a High-Confidence (HC) Mask. TheLC mask indicates regions with a low segmentation confidence, which may causedifferent segmentation options among radiologists. Following UAAM, we furtherdesign an Uncertainty-Guide Segmentation Network (UGS-Net), which containsthree modules: a Feature Extracting Module that captures a general feature of alung nodule, an Uncertainty-Aware Module that produces three features for theannotations' union, intersection, and annotation set, and an Intersection-UnionConstraining Module that uses distances between the three features to balancethe predictions of final segmentation, LC mask, and HC mask. To fullydemonstrate the performance of our method, we propose a Complex NoduleValidation on LIDC-IDRI, which tests UGS-Net's segmentation performance on lungnodules that are difficult to segment using U-Net. Experimental resultsdemonstrate that our method can significantly improve the segmentationperformance on nodules with poor segmentation by U-Net.", "output": "Lung Nodule Segmentation and Low-Confidence Region Prediction with Uncertainty-Aware Attention Mechanism."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have shown great promise in text-guided image styletransfer, but there is a trade-off between style transformation and contentpreservation due to their stochastic nature. Existing methods requirecomputationally expensive fine-tuning of diffusion models or additional neuralnetwork. To address this, here we propose a zero-shot contrastive loss fordiffusion models that doesn't require additional fine-tuning or auxiliarynetworks. By leveraging patch-wise contrastive loss between generated samplesand original image embeddings in the pre-trained diffusion model, our methodcan generate images with the same semantic content as the source image in azero-shot manner. Our approach outperforms existing methods while preservingcontent and requiring no additional training, not only for image style transferbut also for image-to-image translation and manipulation. Our experimentalresults validate the effectiveness of our proposed method.", "output": "Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Imagining the future trajectory is the key for robots to make sound planningand successfully reach their goals. Therefore, text-conditioned videoprediction (TVP) is an essential task to facilitate general robot policylearning, i.e., predicting future video frames with a given languageinstruction and reference frames. It is a highly challenging task to groundtask-level goals specified by instructions and high-fidelity frames together,requiring large-scale data and computation. To tackle this task and empowerrobots with the ability to foresee the future, we propose a sample andcomputation-efficient model, named textbf{Seer}, by inflating the pretrainedtext-to-image (T2I) stable diffusion models along the temporal axis. We inflatethe denoising U-Net and language conditioning model with two novel techniques,Autoregressive Spatial-Temporal Attention and Frame Sequential Text Decomposer,to propagate the rich prior knowledge in the pretrained T2I models across theframes. With the well-designed architecture, Seer makes it possible to generatehigh-fidelity, coherent, and instruction-aligned video frames by fine-tuning afew layers on a small amount of data. The experimental results on SomethingSomething V2 (SSv2) and Bridgedata datasets demonstrate our superior videoprediction performance with around 210-hour training on 4 RTX 3090 GPUs:decreasing the FVD of the current SOTA model from 290 to 200 on SSv2 andachieving at least 70% preference in the human evaluation.", "output": "Seer: Language Instructed Video Prediction with Latent Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modeling sounds emitted from physical object interactions is critical forimmersive perceptual experiences in real and virtual worlds. Traditionalmethods of impact sound synthesis use physics simulation to obtain a set ofphysics parameters that could represent and synthesize the sound. However, theyrequire fine details of both the object geometries and impact locations, whichare rarely available in the real world and can not be applied to synthesizeimpact sounds from common videos. On the other hand, existing video-driven deeplearning-based approaches could only capture the weak correspondence betweenvisual content and impact sounds since they lack of physics knowledge. In thiswork, we propose a physics-driven diffusion model that can synthesizehigh-fidelity impact sound for a silent video clip. In addition to the videocontent, we propose to use additional physics priors to guide the impact soundsynthesis procedure. The physics priors include both physics parameters thatare directly estimated from noisy real-world impact sound examples withoutsophisticated setup and learned residual parameters that interpret the soundenvironment via neural networks. We further implement a novel diffusion modelwith specific training and inference strategies to combine physics priors andvisual information for impact sound synthesis. Experimental results show thatour model outperforms several existing systems in generating realistic impactsounds. More importantly, the physics-based representations are fullyinterpretable and transparent, thus enabling us to perform sound editingflexibly.", "output": "Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection is a fundamental task in computer vision, which has beengreatly progressed through developing large and intricate deep learning models.However, the lack of transparency is a big challenge that may not allow thewidespread adoption of these models. Explainable artificial intelligence is afield of research where methods are developed to help users understand thebehavior, decision logics, and vulnerabilities of AI-based systems. Black-boxexplanation refers to explaining decisions of an AI system without havingaccess to its internals. In this paper, we design and implement a black-boxexplanation method named Black-box Object Detection Explanation by Masking(BODEM) through adopting a new masking approach for AI-based object detectionsystems. We propose local and distant masking to generate multiple versions ofan input image. Local masks are used to disturb pixels within a target objectto figure out how the object detector reacts to these changes, while distantmasks are used to assess how the detection model's decisions are affected bydisturbing pixels outside the object. A saliency map is then created byestimating the importance of pixels through measuring the difference betweenthe detection output before and after masking. Finally, a heatmap is createdthat visualizes how important pixels within the input image are to the detectedobjects. The experimentations on various object detection datasets and modelsshowed that BODEM can be effectively used to explain the behavior of objectdetectors and reveal their vulnerabilities. This makes BODEM suitable forexplaining and validating AI based object detection systems in black-boxsoftware testing scenarios. Furthermore, we conducted data augmentationexperiments that showed local masks produced by BODEM can be used for furthertraining the object detectors and improve their detection accuracy androbustness.", "output": "Model-agnostic explainable artificial intelligence for object detection in image data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale diffusion models like Stable Diffusion are powerful and findvarious real-world applications while customizing such models by fine-tuning isboth memory and time inefficient. Motivated by the recent progress in naturallanguage processing, we investigate parameter-efficient tuning in largediffusion models by inserting small learnable modules (termed adapters). Inparticular, we decompose the design space of adapters into orthogonal factors-- the input position, the output position as well as the function form, andperform Analysis of Variance (ANOVA), a classical statistical approach foranalyzing the correlation between discrete (design options) and continuousvariables (evaluation metrics). Our analysis suggests that the input positionof adapters is the critical factor influencing the performance of downstreamtasks. Then, we carefully study the choice of the input position, and we findthat putting the input position after the cross-attention block can lead to thebest performance, validated by additional visualization analyses. Finally, weprovide a recipe for parameter-efficient tuning in diffusion models, which iscomparable if not superior to the fully fine-tuned baseline (e.g., DreamBooth)with only 0.75 % extra parameters, across various customized tasks.", "output": "A Closer Look at Parameter-Efficient Tuning in Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, deep learning has been widely used in SAR ATR and achievedexcellent performance on the MSTAR dataset. However, due to constrained imagingconditions, MSTAR has data biases such as background correlation, i.e.,background clutter properties have a spurious correlation with target classes.Deep learning can overfit clutter to reduce training errors. Therefore, thedegree of overfitting for clutter reflects the non-causality of deep learningin SAR ATR. Existing methods only qualitatively analyze this phenomenon. Inthis paper, we quantify the contributions of different regions to targetrecognition based on the Shapley value. The Shapley value of clutter measuresthe degree of overfitting. Moreover, we explain how data bias and model biascontribute to non-causality. Concisely, data bias leads to comparablesignal-to-clutter ratios and clutter textures in training and test sets. Andvarious model structures have different degrees of overfitting for thesebiases. The experimental results of various models under standard operatingconditions on the MSTAR dataset support our conclusions. Our code is availableat ", "output": "Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While neural networks are capable of achieving human-like performance in manytasks such as image classification, the impressive performance of each model islimited to its own dataset. Source-free domain adaptation (SFDA) was introducedto address knowledge transfer between different domains in the absence ofsource data, thus, increasing data privacy. Diversity in representation spacecan be vital to a model`s adaptability in varied and difficult domains. Inunsupervised SFDA, the diversity is limited to learning a single hypothesis onthe source or learning multiple hypotheses with a shared feature extractor.Motivated by the improved predictive performance of ensembles, we propose anovel unsupervised SFDA algorithm that promotes representational diversitythrough the use of separate feature extractors with Distinct BackboneArchitectures (DBA). Although diversity in feature space is increased, theunconstrained mutual information (MI) maximization may potentially introduceamplification of weak hypotheses. Thus we introduce the Weak HypothesisPenalization (WHP) regularizer as a mitigation strategy. Our work proposesPenalized Diversity (PD) where the synergy of DBA and WHP is applied tounsupervised source-free domain adaptation for covariate shift. In addition, PDis augmented with a weighted MI maximization objective for label distributionshift. Empirical results on natural, synthetic, and medical domains demonstratethe effectiveness of PD under different distributional shifts.", "output": "Source-free Domain Adaptation Requires Penalized Diversity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We analyze the data-dependent capacity of neural networks and assessanomalies in inputs from the perspective of networks during inference. Thenotion of data-dependent capacity allows for analyzing the knowledge base of amodel populated by learned features from training data. We define purview asthe additional capacity necessary to characterize inference samples that differfrom the training data. To probe the purview of a network, we utilize gradientsto measure the amount of change required for the model to characterize thegiven inputs more accurately. To eliminate the dependency on ground-truthlabels in generating gradients, we introduce confounding labels that areformulated by combining multiple categorical labels. We demonstrate that ourgradient-based approach can effectively differentiate inputs that cannot beaccurately represented with learned features. We utilize our approach inapplications of detecting anomalous inputs, including out-of-distribution,adversarial, and corrupted samples. Our approach requires no hyperparametertuning or additional data processing and outperforms state-of-the-art methodsby up to 2.7%, 19.8%, and 35.6% of AUROC scores, respectively.", "output": "Probing the Purview of Neural Networks via Gradient Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatial attention has been widely used to improve the performance ofconvolutional neural networks by allowing them to focus on importantinformation. However, it has certain limitations. In this paper, we propose anew perspective on the effectiveness of spatial attention, which is that it cansolve the problem of convolutional kernel parameter sharing. Despite this, theinformation contained in the attention map generated by spatial attention isnot sufficient for large-size convolutional kernels. Therefore, we introduce anew attention mechanism called Receptive-Field Attention (RFA). While previousattention mechanisms such as the Convolutional Block Attention Module (CBAM)and Coordinate Attention (CA) only focus on spatial features, they cannot fullyaddress the issue of convolutional kernel parameter sharing. In contrast, RFAnot only focuses on the receptive-field spatial feature but also provideseffective attention weights for large-size convolutional kernels. TheReceptive-Field Attention convolutional operation (RFAConv), developed by RFA,represents a new approach to replace the standard convolution operation. Itoffers nearly negligible increment of computational cost and parameters, whilesignificantly improving network performance. We conducted a series ofexperiments on ImageNet-1k, MS COCO, and VOC datasets, which demonstrated thesuperiority of our approach in various tasks including classification, objectdetection, and semantic segmentation. Of particular importance, we believe thatit is time to shift focus from spatial features to receptive-field spatialfeatures for current spatial attention mechanisms. By doing so, we can furtherimprove network performance and achieve even better results. The code andpre-trained models for the relevant tasks can be found at", "output": "RFAConv: Innovating Spatital Attention and Standard Convolutional Operation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of the convolutional neural network based prior in imaging inverseproblems has become increasingly popular. Current state-of-the-art methods,however, can easily result in severe overfitting, which makes a number of earlystopping techniques necessary to eliminate the overfitting problem. To motivateour work, we review some existing approaches to image priors. We find that thedeep image prior in combined with the handcrafted prior has an outstandingperformance in terms of interpretability and representability. We propose amulti-code deep image prior, a multiple latent codes variant of the deep imageprior, which can be utilized to eliminate overfitting and is also robust to thedifferent numbers of the latent codes. Due to the non-differentiability of thehandcrafted prior, we use the alternative direction method of multipliers(ADMM) algorithm. We compare the performance of the proposed method on an imagedenoising problem and a highly ill-posed CT reconstruction problem against theexisting state-of-the-art methods, including PnP-DIP, DIP-VBTV and ADMM DIP-WTVmethods. For the CelebA dataset denoising, we obtain 1.46 dB peak signal tonoise ratio improvement against all compared methods. For the CTreconstruction, the corresponding average improvement of three test images is4.3 dB over DIP, and 1.7 dB over ADMM DIP-WTV, and 1.2 dB over PnP-DIP alongwith a significant improvement in the structural similarity index.", "output": "Multi-code deep image prior based plug-and-play ADMM for image denoising and CT reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Sat-NeRF, a modified implementation of the recently introducedShadow Neural Radiance Field (S-NeRF) model. This method is able to synthesizenovel views from a sparse set of satellite images of a scene, while accountingfor the variation in lighting present in the pictures. The trained model canalso be used to accurately estimate the surface elevation of the scene, whichis often a desirable quantity for satellite observation applications. S-NeRFimproves on the standard Neural Radiance Field (NeRF) method by considering theradiance as a function of the albedo and the irradiance. Both these quantitiesare output by fully connected neural network branches of the model, and thelatter is considered as a function of the direct light from the sun and thediffuse color from the sky. The implementations were run on a dataset ofsatellite images, augmented using a zoom-and-crop technique. A hyperparameterstudy for NeRF was carried out, leading to intriguing observations on themodel's convergence. Finally, both NeRF and S-NeRF were run until 100k epochsin order to fully fit the data and produce their best possible predictions. Thecode related to this article can be found aturl{", "output": "NeRF applied to satellite imagery for surface reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning with large-scale unlabeled data has become a powerful tool forpre-training Visual Transformers (VTs). However, prior works tend to overlookthat, in real-world scenarios, the input data may be corrupted and unreliable.Pre-training VTs on such corrupted data can be challenging, especially when wepre-train via the masked autoencoding approach, where both the inputs andmasked ``ground truth\" targets can potentially be unreliable in this case. Toaddress this limitation, we introduce the Token Boosting Module (TBM) as aplug-and-play component for VTs that effectively allows the VT to learn toextract clean and robust features during masked autoencoding pre-training. Weprovide theoretical analysis to show how TBM improves model pre-training withmore robust and generalizable representations, thus benefiting downstreamtasks. We conduct extensive experiments to analyze TBM's effectiveness, andresults on four corrupted datasets demonstrate that TBM consistently improvesperformance on downstream tasks.", "output": "Token Boosting for Robust Self-Supervised Visual Transformer Pre-training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Referring expression segmentation aims to segment an object described by alanguage expression from an image. Despite the recent progress on this task,existing models tackling this task may not be able to fully capture semanticsand visual representations of individual concepts, which limits theirgeneralization capability, especially when handling novel compositions oflearned concepts. In this work, through the lens of meta learning, we propose aMeta Compositional Referring Expression Segmentation (MCRES) framework toenhance model compositional generalization performance. Specifically, to handlevarious levels of novel compositions, our framework first uses training data toconstruct a virtual training set and multiple virtual testing sets, where datasamples in each virtual testing set contain a level of novel compositionsw.r.t. the virtual training set. Then, following a novel meta optimizationscheme to optimize the model to obtain good testing performance on the virtualtesting sets after training on the virtual training set, our framework caneffectively drive the model to better capture semantics and visualrepresentations of individual concepts, and thus obtain robust generalizationperformance even when handling novel compositions. Extensive experiments onthree benchmark datasets demonstrate the effectiveness of our framework.", "output": "Meta Compositional Referring Expression Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Kinship recognition aims to determine whether the subjects in two facialimages are kin or non-kin, which is an emerging and challenging problem.However, most previous methods focus on heuristic designs without consideringthe spatial correlation between face images. In this paper, we aim to learndiscriminative kinship representations embedded with the relation informationbetween face components (e.g., eyes, nose, etc.). To achieve this goal, wepropose the Face Componential Relation Network, which learns the relationshipbetween face components among images with a cross-attention mechanism, whichautomatically learns the important facial regions for kinship recognition.Moreover, we propose Face Componential Relation Network (FaCoRNet), whichadapts the loss function by the guidance from cross-attention to learn morediscriminative feature representations. The proposedMainMethodAbbr~outperforms previous state-of-the-art methods by large marginsfor the largest public kinship recognition FIW benchmark. The code will bepublicly released upon acceptance.", "output": "Kinship Representation Learning with Face Componential Relation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent large-scale generative modeling has attained unprecedentedperformance especially in producing high-fidelity images driven by textprompts. Text inversion (TI), alongside the text-to-image model backbones, isproposed as an effective technique in personalizing the generation when theprompts contain user-defined, unseen or long-tail concept tokens. Despite that,we find and show that the deployment of TI remains full of \"dark-magics\" -- toname a few, the harsh requirement of additional datasets, arduous human effortsin the loop and lack of robustness. In this work, we propose a much-enhancedversion of TI, dubbed Controllable Textual Inversion (COTI), in resolving allthe aforementioned problems and in turn delivering a robust, data-efficient andeasy-to-use framework. The core to COTI is a theoretically-guided lossobjective instantiated with a comprehensive and novel weighted scoringmechanism, encapsulated by an active-learning paradigm. The extensive resultsshow that COTI significantly outperforms the prior TI-related approaches with a26.05 decrease in the FID score and a 23.00% boost in the R-precision.", "output": "Controllable Textual Inversion for Personalized Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, the deployment of deep learning based applications on edge devicesis an essential task owing to the increasing demands on intelligent services.However, the limited computing resources on edge nodes make the modelsvulnerable to attacks, such that the predictions made by models are unreliable.In this paper, we investigate latency attacks on deep learning applications.Unlike common adversarial attacks for misclassification, the goal of latencyattacks is to increase the inference time, which may stop applications fromresponding to the requests within a reasonable time. This kind of attack isubiquitous for various applications, and we use object detection to demonstratehow such kind of attacks work. We also design a framework named Overload togenerate latency attacks at scale. Our method is based on a newly formulatedoptimization problem and a novel technique, called spatial attention, toincrease the inference time of object detection. We have conducted experimentsusing YOLOv5 models on Nvidia NX. The experimental results show that withlatency attacks, the inference time of a single image can be increased tentimes longer in reference to the normal setting. Moreover, comparing toexisting methods, our attacking method is simpler and more effective.", "output": "Overload: Latency Attacks on Object Detection for Edge Devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A fundamental task in robotics is to navigate between two locations. Inparticular, real-world navigation can require long-horizon planning usinghigh-dimensional RGB images, which poses a substantial challenge for end-to-endlearning-based approaches. Current semi-parametric methods instead achievelong-horizon navigation by combining learned modules with a topological memoryof the environment, often represented as a graph over previously collectedimages. However, using these graphs in practice typically involves tuning anumber of pruning heuristics to avoid spurious edges, limit runtime memoryusage and allow reasonably fast graph queries. In this work, we presentOne-4-All (O4A), a method leveraging self-supervised and manifold learning toobtain a graph-free, end-to-end navigation pipeline in which the goal isspecified as an image. Navigation is achieved by greedily minimizing apotential function defined continuously over the O4A latent space. Our systemis trained offline on non-expert exploration sequences of RGB data andcontrols, and does not require any depth or pose measurements. We show that O4Acan reach long-range goals in 8 simulated Gibson indoor environments, andfurther demonstrate successful real-world navigation using a Jackal UGVplatform.", "output": "One-4-All: Neural Potential Fields for Embodied Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Foundation models have taken over natural language processing and imagegeneration domains due to the flexibility of prompting. With the recentintroduction of the Segment Anything Model (SAM), this prompt-driven paradigmhas entered image segmentation with a hitherto unexplored abundance ofcapabilities. The purpose of this paper is to conduct an initial evaluation ofthe out-of-the-box zero-shot capabilities of SAM for medical imagesegmentation, by evaluating its performance on an abdominal CT organsegmentation task, via point or bounding box based prompting. We show that SAMgeneralizes well to CT data, making it a potential catalyst for the advancementof semi-automatic segmentation tools for clinicians. We believe that thisfoundation model, while not reaching state-of-the-art segmentation performancein our investigations, can serve as a highly potent starting point for furtheradaptations of such models to the intricacies of the medical domain. Keywords:medical image segmentation, SAM, foundation models, zero-shot learning", "output": "SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past few years, Federated Learning (FL) has become a populardistributed machine learning paradigm. FL involves a group of clients withdecentralized data who collaborate to learn a common model under thecoordination of a centralized server, with the goal of protecting clients'privacy by ensuring that local datasets never leave the clients and that theserver only performs model aggregation. However, in realistic scenarios, theserver may be able to collect a small amount of data that approximately mimicsthe population distribution and has stronger computational ability to performthe learning process. To address this, we focus on the hybrid FL framework inthis paper. While previous hybrid FL work has shown that the alternativetraining of clients and server can increase convergence speed, it has focusedon the scenario where clients fully participate and ignores the negative effectof partial participation. In this paper, we provide theoretical analysis ofhybrid FL under clients' partial participation to validate that partialparticipation is the key constraint on convergence speed. We then propose a newalgorithm called FedCLG, which investigates the two-fold role of the server inhybrid FL. Firstly, the server needs to process the training steps using itssmall amount of local datasets. Secondly, the server's calculated gradientneeds to guide the participated clients' training and the server's aggregation.We validate our theoretical findings through numerical experiments, which showthat our proposed method FedCLG outperforms state-of-the-art methods.", "output": "Accelerating Hybrid Federated Learning Convergence under Partial Participation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Variational inference (VI) seeks to approximate a target distribution $pi$by an element of a tractable family of distributions. Of key interest instatistics and machine learning is Gaussian VI, which approximates $pi$ byminimizing the Kullback-Leibler (KL) divergence to $pi$ over the space ofGaussians. In this work, we develop the (Stochastic) Forward-Backward GaussianVariational Inference (FB-GVI) algorithm to solve Gaussian VI. Our approachexploits the composite structure of the KL divergence, which can be written asthe sum of a smooth term (the potential) and a non-smooth term (the entropy)over the Bures-Wasserstein (BW) space of Gaussians endowed with the Wassersteindistance. For our proposed algorithm, we obtain state-of-the-art convergenceguarantees when $pi$ is log-smooth and log-concave, as well as the firstconvergence guarantees to first-order stationary solutions when $pi$ is onlylog-smooth.", "output": "Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The transferability of adversarial examples is a crucial aspect of evaluatingthe robustness of deep learning systems, particularly in black-box scenarios.Although several methods have been proposed to enhance cross-modeltransferability, little attention has been paid to the transferability ofadversarial examples across different tasks. This issue has become increasinglyrelevant with the emergence of foundational multi-task AI systems such asVisual ChatGPT, rendering the utility of adversarial samples generated by asingle task relatively limited. Furthermore, these systems often entailinferential functions beyond mere recognition-like tasks. To address this gap,we propose a novel Visual Relation-based cross-task Adversarial Patchgeneration method called VRAP, which aims to evaluate the robustness of variousvisual tasks, especially those involving visual reasoning, such as VisualQuestion Answering and Image Captioning. VRAP employs scene graphs to combineobject recognition-based deception with predicate-based relations elimination,thereby disrupting the visual reasoning information shared among inferentialtasks. Our extensive experiments demonstrate that VRAP significantly surpassesprevious methods in terms of black-box transferability across diverse visualreasoning tasks.", "output": "Boosting Cross-task Transferability of Adversarial Patches with Visual Relations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the past few years, Differentiable Neural Architecture Search (DNAS)rapidly imposed itself as the trending approach to automate the discovery ofdeep neural network architectures. This rise is mainly due to the popularity ofDARTS, one of the first major DNAS methods. In contrast with previous worksbased on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster byseveral orders of magnitude and uses fewer computational resources. In thiscomprehensive survey, we focus specifically on DNAS and review recentapproaches in this field. Furthermore, we propose a novel challenge-basedtaxonomy to classify DNAS methods. We also discuss the contributions brought toDNAS in the past few years and its impact on the global NAS field. Finally, weconclude by giving some insights into future research directions for the DNASfield.", "output": "Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Materials possessing flexible physico-chemical properties that adapton-demand to the hostile environmental conditions of deep space will becomeessential in defining the future of space exploration. A promising venue forinspiration towards the design of environment-specific materials is in theintricate micro-architectures and lattice geometry found throughout nature.However, the immense design space covered by such irregular topologies ischallenging to probe analytically. For this reason, most synthetic latticematerials have to date been based on periodic architectures instead. Here, wepropose a computational approach using a graph representation for both regularand irregular lattice materials. Our method uses differentiable message passingalgorithms to calculate mechanical properties, and therefore allows usingautomatic differentiation to adjust both the geometric structure and attributesof individual lattice elements to design materials with desired properties. Theintroduced methodology is applicable to any system representable as aheterogeneous graph, including other types of materials.", "output": "Differentiable graph-structured models for inverse design of lattice materials."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tuning tensor program generation involves searching for various possibleprogram transformation combinations for a given program on target hardware tooptimize the tensor program execution. It is already a complex process becauseof the massive search space and exponential combinations of transformationsmake auto-tuning tensor program generation more challenging, especially when wehave a heterogeneous target. In this research, we attempt to address theseproblems by learning the joint neural network and hardware features andtransferring them to the new target hardware. We extensively study the existingstate-of-the-art dataset, TenSet, perform comparative analysis on the testsplit strategies and propose methodologies to prune the dataset. We adopt anattention-inspired approach for tuning the tensor programs enabling them toembed neural network and hardware-specific features. Our approach could prunethe dataset up to 45% of the baseline without compromising the PairwiseComparison Accuracy (PCA). Further, the proposed methodology can achieve on-paror improved mean inference time with 25%-40% of the baseline tuning time acrossdifferent networks and target hardware.", "output": "Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing novice-friendly machine learning (ML) modeling tools center around asolo user experience, where a single user collects only their own data to builda model. However, solo modeling experiences limit valuable opportunities forencountering alternative ideas and approaches that can arise when learners worktogether; consequently, it often precludes encountering critical issues in MLaround data representation and diversity that can surface when differentperspectives are manifested in a group-constructed data set. To address thisissue, we created Co-ML -- a tablet-based app for learners to collaborativelybuild ML image classifiers through an end-to-end, iterative model-buildingprocess. In this paper, we illustrate the feasibility and potential richness ofcollaborative modeling by presenting an in-depth case study of a family (twochildren 11 and 14-years-old working with their parents) using Co-ML in afacilitated introductory ML activity at home. We share the Co-ML system designand contribute a discussion of how using Co-ML in a collaborative activityenabled beginners to collectively engage with dataset design considerationsunderrepresented in prior work such as data diversity, class imbalance, anddata quality. We discuss how a distributed collaborative process, in whichindividuals can take on different model-building responsibilities, provides arich context for children and adults to learn ML dataset design.", "output": "Collaborative Machine Learning Model Building with Families Using Co-ML."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional neural networks (CNN) have become the predominant model forimage segmentation tasks. Most CNN segmentation architectures resize spatialdimensions by a fixed factor of two to aggregate spatial context. Recent workhas explored using other resizing factors to improve model accuracy forspecific applications. However, finding the appropriate rescaling factor mostoften involves training a separate network for many different factors andcomparing the performance of each model. The computational burden of thesemodels means that in practice it is rarely done, and when done only a fewdifferent scaling factors are considered.In this work, we present a hypernetwork strategy that can be used to easilyand rapidly generate the Pareto frontier for the trade-off between accuracy andefficiency as the rescaling factor varies. We show how to train a singlehypernetwork that generates CNN parameters conditioned on a rescaling factor.This enables a user to quickly choose a rescaling factor that appropriatelybalances accuracy and computational efficiency for their particular needs. Wefocus on image segmentation tasks, and demonstrate the value of this approachacross various domains. We also find that, for a given rescaling factor, oursingle hypernetwork outperforms CNNs trained with fixed rescaling factors.", "output": "Amortized Learning of Dynamic Feature Scaling for Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In fetal ultrasound screening, Doppler images on the umbilical artery (UA)are important for monitoring blood supply through the umbilical cord. However,to capture UA Doppler images, a number of steps need to be done correctly:placing the gate at a proper location in the ultrasound image to obtain bloodflow waveforms, and judging the Doppler waveform quality. Both of these rely onthe operator's experience. The shortage of experienced sonographers thuscreates a demand for machine assistance. We propose an automatic system to fillthis gap. Using a modified Faster R-CNN we obtain an algorithm that suggestsDoppler flow gate locations. We subsequently assess the Doppler waveformquality. We validate the proposed system on 657 scans from a nationalultrasound screening database. The experimental results demonstrate that oursystem is useful in guiding operators for UA Doppler image capture and qualityassessment.", "output": "An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sequential recommendation models, models that learn from chronologicaluser-item interactions, outperform traditional recommendation models in manysettings. Despite the success of sequential recommendation models, theirrobustness has recently come into question. Two properties unique to the natureof sequential recommendation models may impair their robustness - the cascadeeffects induced during training and the model's tendency to rely too heavily ontemporal information. To address these vulnerabilities, we proposeCascade-guided Adversarial training, a new adversarial training procedure thatis specifically designed for sequential recommendation models. Our approachharnesses the intrinsic cascade effects present in sequential modeling toproduce strategic adversarial perturbations to item embeddings during training.Experiments on training state-of-the-art sequential models on four publicdatasets from different domains show that our training approach producessuperior model ranking accuracy and superior model robustness to real itemreplacement perturbations when compared to both standard model training andgeneric adversarial training.", "output": "Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixture of Experts (MoE) are rising in popularity as a means to trainextremely large-scale models, yet allowing for a reasonable computational costat inference time. Recent state-of-the-art approaches usually assume a largenumber of experts, and require training all experts jointly, which often leadto training instabilities such as the router collapsing In contrast, in thiswork, we propose to revisit the simple single-gate MoE, which allows for morepractical training. Key to our work are (i) a base model branch acting both asan early-exit and an ensembling regularization scheme, (ii) a simple andefficient asynchronous training pipeline without router collapse issues, andfinally (iii) a per-sample clustering-based initialization. We showexperimentally that the proposed model obtains efficiency-to-accuracytrade-offs comparable with other more complex MoE, and outperforms non-mixturebaselines. This showcases the merits of even a simple single-gate MoE, andmotivates further exploration in this area.", "output": "Revisiting Single-gated Mixtures of Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in deep learning have accelerated its use in variousapplications, such as cellular image analysis and molecular discovery. Inmolecular discovery, a generative adversarial network (GAN), which comprises adiscriminator to distinguish generated molecules from existing molecules and agenerator to generate new molecules, is one of the premier technologies due toits ability to learn from a large molecular data set efficiently and generatenovel molecules that preserve similar properties. However, differentpharmaceutical companies may be unwilling or unable to share their local datasets due to the geo-distributed and sensitive nature of molecular data sets,making it impossible to train GANs in a centralized manner. In this paper, wepropose a Graph convolutional network in Generative Adversarial Networks viaFederated learning (GraphGANFed) framework, which integrates graphconvolutional neural Network (GCN), GAN, and federated learning (FL) as a wholesystem to generate novel molecules without sharing local data sets. InGraphGANFed, the discriminator is implemented as a GCN to better capturefeatures from molecules represented as molecular graphs, and FL is used totrain both the discriminator and generator in a distributive manner to preservedata privacy. Extensive simulations are conducted based on the three bench-markdata sets to demonstrate the feasibility and effectiveness of GraphGANFed. Themolecules generated by GraphGANFed can achieve high novelty (=100) anddiversity (&gt; 0.9). The simulation results also indicate that 1) a lowercomplexity discriminator model can better avoid mode collapse for a smallerdata set, 2) there is a tradeoff among different evaluation metrics, and 3)having the right dropout ratio of the generator and discriminator can avoidmode collapse.", "output": "GraphGANFed: A Federated Generative Framework for Graph-Structured Molecules Towards Efficient Drug Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a scalable machine learning (ML) framework for predictingintensive properties and particularly classifying phases of many-body systems.Scalability and transferability are central to the unprecedented computationalefficiency of ML methods. In general, linear-scaling computation can beachieved through the divide and conquer approach, and the locality of physicalproperties is key to partitioning the system into sub-domains that can besolved separately. Based on the locality assumption, ML model is developed forthe prediction of intensive properties of a finite-size block. Predictions oflarge-scale systems can then be obtained by averaging results of the ML modelfrom randomly sampled blocks of the system. We show that the applicability ofthis approach depends on whether the block-size of the ML model is greater thanthe characteristic length scale of the system. In particular, in the case ofphase identification across a critical point, the accuracy of the ML predictionis limited by the diverging correlation length. The two-dimensional Ising modelis used to demonstrate the proposed framework. We obtain an intriguing scalingrelation between the prediction accuracy and the ratio of ML block size overthe spin-spin correlation length. Implications for practical applications arealso discussed.", "output": "Machine learning for structure-property relationships: Scalability and limitations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Brain-inspired hyperdimensional computing (HDC) has been recently considereda promising learning approach for resource-constrained devices. However,existing approaches use static encoders that are never updated during thelearning process. Consequently, it requires a very high dimensionality toachieve adequate accuracy, severely lowering the encoding and trainingefficiency. In this paper, we propose DistHD, a novel dynamic encodingtechnique for HDC adaptive learning that effectively identifies and regeneratesdimensions that mislead the classification and compromise the learning quality.Our proposed algorithm DistHD successfully accelerates the learning process andachieves the desired accuracy with considerably lower dimensionality.", "output": "DistHD: A Learner-Aware Dynamic Encoding Method for Hyperdimensional Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) is an area of significant research interest, andsafe RL in particular is attracting attention due to its ability to handlesafety-driven constraints that are crucial for real-world applications of RLalgorithms. This work proposes a novel approach to RL training, called controlinvariant set (CIS) enhanced RL, which leverages the benefits of CIS to improvestability guarantees and sampling efficiency. The approach consists of twolearning stages: offline and online. In the offline stage, CIS is incorporatedinto the reward design, initial state sampling, and state reset procedures. Inthe online stage, RL is retrained whenever the state is outside of CIS, whichserves as a stability criterion. A backup table that utilizes the explicit formof CIS is obtained to ensure the online stability. To evaluate the proposedapproach, we apply it to a simulated chemical reactor. The results show asignificant improvement in sampling efficiency during offline training andclosed-loop stability in the online implementation.", "output": "Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large foundation language models have shown their versatility in being ableto be adapted to perform a wide variety of downstream tasks, such as textgeneration, sentiment analysis, semantic search etc. However, training suchlarge foundational models is a non-trivial exercise that requires a significantamount of compute power and expertise from machine learning and systemsexperts. As models get larger, these demands are only increasing. Sparsity is apromising technique to relieve the compute requirements for training. However,sparsity introduces new challenges in training the sparse model to the samequality as the dense counterparts. Furthermore, sparsity drops the operationintensity and introduces irregular memory access patterns that makes itchallenging to efficiently utilize compute resources. This paper demonstratesan end-to-end training flow on a large language model - 13 billion GPT - usingsparsity and dataflow. The dataflow execution model and architecture enablesefficient on-chip irregular memory accesses as well as native kernel fusion andpipelined parallelism that helps recover device utilization. We show that wecan successfully train GPT 13B to the same quality as the dense GPT 13B model,while achieving an end-end speedup of 4.5x over dense A100 baseline.", "output": "Training Large Language Models Efficiently with Sparsity and Dataflow."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning, as a popular paradigm for collaborative training, isvulnerable against privacy attacks. Different privacy levels regarding users'attitudes need to be satisfied locally, while a strict privacy guarantee forthe global model is also required centrally. Personalized Local DifferentialPrivacy (PLDP) is suitable for preserving users' varying local privacy, yetonly provides a central privacy guarantee equivalent to the worst-case localprivacy level. Thus, achieving strong central privacy as well as personalizedlocal privacy with a utility-promising model is a challenging problem. In thiswork, a general framework (APES) is built up to strengthen model privacy underpersonalized local privacy by leveraging the privacy amplification effect ofthe shuffle model. To tighten the privacy bound, we quantify the heterogeneouscontributions to the central privacy user by user. The contributions arecharacterized by the ability of generating \"echos\" from the perturbation ofeach user, which is carefully measured by proposed methods Neighbor Divergenceand Clip-Laplace Mechanism. Furthermore, we propose a refined framework(S-APES) with the post-sparsification technique to reduce privacy loss inhigh-dimension scenarios. To the best of our knowledge, the impact of shufflingon personalized local privacy is considered for the first time. We provide astrong privacy amplification effect, and the bound is tighter than the baselineresult based on existing methods for uniform local privacy. Experimentsdemonstrate that our frameworks ensure comparable or higher accuracy for theglobal model.", "output": "Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We assess the ability of large language models (LLMs) to answer causalquestions by analyzing their strengths and weaknesses against three types ofcausal question. We believe that current LLMs can answer causal questions withexisting causal knowledge as combined domain experts. However, they are not yetable to provide satisfactory answers for discovering new knowledge or forhigh-stakes decision-making tasks with high precision. We discuss possiblefuture directions and opportunities, such as enabling explicit and implicitcausal modules as well as deep causal-aware LLMs. These will not only enableLLMs to answer many different types of causal questions for greater impact butalso enable LLMs to be more trustworthy and efficient in general.", "output": "Understanding Causality with Large Language Models: Feasibility and Opportunities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic differentiation variational inference (ADVI) offers fast andeasy-to-use posterior approximation in multiple modern probabilisticprogramming languages. However, its stochastic optimizer lacks clearconvergence criteria and requires tuning parameters. Moreover, ADVI inheritsthe poor posterior uncertainty estimates of mean-field variational Bayes(MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues.DADVI replaces the intractable MFVB objective with a fixed Monte Carloapproximation, a technique known in the stochastic optimization literature asthe ``sample average approximation'' (SAA). By optimizing an approximate butdeterministic objective, DADVI can use off-the-shelf second-order optimization,and, unlike standard mean-field ADVI, is amenable to more accurate posteriorlinear response (LR) covariance estimates. In contrast to existing worst-casetheory, we show that, on certain classes of common statistical problems, DADVIand the SAA can perform well with relatively few samples even in very highdimensions, though we also show that such favorable results cannot extend tovariational approximations that are too expressive relative to mean-field ADVI.We show on a variety of real-world problems that DADVI reliably finds goodsolutions with default settings (unlike ADVI) and, together with LRcovariances, is typically faster and more accurate than standard ADVI.", "output": "Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integration of heterogeneous and high-dimensional multi-omics data isbecoming increasingly important in understanding genetic data. Each omicstechnique only provides a limited view of the underlying biological process andintegrating heterogeneous omics layers simultaneously would lead to a morecomprehensive and detailed understanding of diseases and phenotypes. However,one obstacle faced when performing multi-omics data integration is theexistence of unpaired multi-omics data due to instrument sensitivity and cost.Studies may fail if certain aspects of the subjects are missing or incomplete.In this paper, we propose a deep learning method for multi-omics integrationwith incomplete data by Cross-omics Linked unified embedding with ContrastiveLearning and Self Attention (CLCLSA). Utilizing complete multi-omics data assupervision, the model employs cross-omics autoencoders to learn the featurerepresentation across different types of biological data. The multi-omicscontrastive learning, which is used to maximize the mutual information betweendifferent types of omics, is employed before latent feature concatenation. Inaddition, the feature-level self-attention and omics-level self-attention areemployed to dynamically identify the most informative features for multi-omicsdata integration. Extensive experiments were conducted on four publicmulti-omics datasets. The experimental results indicated that the proposedCLCLSA outperformed the state-of-the-art approaches for multi-omics dataclassification using incomplete multi-omics data.", "output": "CLCLSA: Cross-omics Linked embedding with Contrastive Learning and Self Attention for multi-omics integration with incomplete multi-omics data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the MEMA framework for the easy and quick derivation of efficientinference runtimes that minimize external memory accesses for matrixmultiplication on TinyML systems. The framework accounts for hardware resourceconstraints and problem sizes in analytically determining optimized schedulesand kernels that minimize memory accesses. MEMA provides a solution to awell-known problem in the current practice, that is, optimal schedules tend tobe found only through a time consuming and heuristic search of a largescheduling space. We compare the performance of runtimes derived from MEMA toexisting state-of-the-art libraries on ARM-based TinyML systems. For example,for neural network benchmarks on the ARM Cortex-M4, we achieve up to a 1.8xspeedup and 44% energy reduction over CMSIS-NN.", "output": "MEMA Runtime Framework: Minimizing External Memory Accesses for TinyML on Microcontrollers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of continual learning has attracted rising attention in recentyears. However, few works have questioned the commonly used learning setup,based on a task curriculum of random class. This differs significantly fromhuman continual learning, which is guided by taxonomic curricula. In this work,we propose the Taxonomic Class Incremental Learning (TCIL) problem. In TCIL,the task sequence is organized based on a taxonomic class tree. We unifyexisting approaches to CIL and taxonomic learning as parameter inheritanceschemes and introduce a new such scheme for the TCIL learning. This enables theincremental transfer of knowledge from ancestor to descendant class of a classtaxonomy through parameter inheritance. Experiments on CIFAR-100 andImageNet-100 show the effectiveness of the proposed TCIL method, whichoutperforms existing SOTA methods by 2% in terms of final accuracy on CIFAR-100and 3% on ImageNet-100.", "output": "Taxonomic Class Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Biometric authentication service providers often claim that it is notpossible to reverse-engineer a user's raw biometric sample, such as afingerprint or a face image, from its mathematical (feature-space)representation. In this paper, we investigate this claim on the specificexample of deep neural network (DNN) embeddings. Inversion of DNN embeddingshas been investigated for explaining deep image representations or synthesizingnormalized images. Existing studies leverage full access to all layers of theoriginal model, as well as all possible information on the original dataset.For the biometric authentication use case, we need to investigate this underadversarial settings where an attacker has access to a feature-spacerepresentation but no direct access to the exact original dataset nor theoriginal learned model. Instead, we assume varying degree of attacker'sbackground knowledge about the distribution of the dataset as well as theoriginal learned model (architecture and training process). In these cases, weshow that the attacker can exploit off-the-shelf DNN models and publicdatasets, to mimic the behaviour of the original learned model to varyingdegrees of success, based only on the obtained representation and attacker'sprior knowledge. We propose a two-pronged attack that first infers the originalDNN by exploiting the model footprint on the embedding, and then reconstructsthe raw data by using the inferred model. We show the practicality of theattack on popular DNNs trained for two prominent biometric modalities, face andfingerprint recognition. The attack can effectively infer the originalrecognition model (mean accuracy 83% for faces, 86% for fingerprints), andcan craft effective biometric reconstructions that are successfullyauthenticated with 1-vs-1 authentication accuracy of up to 92% for somemodels.", "output": "On the Adversarial Inversion of Deep Biometric Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study aims to determine a predictive model to learn students probabilityto pass their courses taken at the earliest stage of the semester. Tosuccessfully discover a good predictive model with high acceptability,accurate, and precision rate which delivers a useful outcome for decisionmaking in education systems, in improving the processes of conveying knowledgeand uplifting students academic performance, the proponent applies and strictlyfollowed the CRISP-DM (Cross-Industry Standard Process for Data Mining)methodology. This study employs classification for data mining techniques, anddecision tree for algorithm. With the utilization of the newly discoveredpredictive model, the prediction of students probabilities to pass the currentcourses they take gives 0.7619 accuracy, 0.8333 precision, 0.8823 recall, and0.8571 f1 score, which shows that the model used in the prediction is reliable,accurate, and recommendable. Considering the indicators and the results, it canbe noted that the prediction model used in this study is highly acceptable. Thedata mining techniques provides effective and efficient innovative tools inanalyzing and predicting student performances. The model used in this studywill greatly affect the way educators understand and identify the weakness oftheir students in the class, the way they improved the effectiveness of theirlearning processes gearing to their students, bring down academic failurerates, and help institution administrators modify their learning systemoutcomes. Further study for the inclusion of some students demographicinformation, vast amount of data within the dataset, automated and manualprocess of predictive criteria indicators where the students can regulate towhich criteria, they must improve more for them to pass their courses taken atthe end of the semester as early as midterm period are highly needed.", "output": "A Predictive Model using Machine Learning Algorithm in Identifying Students Probability on Passing Semestral Course."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dialogue Acts (DAs) can be used to explain what expert tutors do and whatstudents know during the tutoring process. Most empirical studies adopt therandom sampling method to obtain sentence samples for manual annotation of DAs,which are then used to train DA classifiers. However, these studies have paidlittle attention to sample informativeness, which can reflect the informationquantity of the selected samples and inform the extent to which a classifiercan learn patterns. Notably, the informativeness level may vary among thesamples and the classifier might only need a small amount of low informativesamples to learn the patterns. Random sampling may overlook sampleinformativeness, which consumes human labelling costs and contributes less totraining the classifiers. As an alternative, researchers suggest employingstatistical sampling methods of Active Learning (AL) to identify theinformative samples for training the classifiers. However, the use of ALmethods in educational DA classification tasks is under-explored. In thispaper, we examine the informativeness of annotated sentence samples. Then, thestudy investigates how the AL methods can select informative samples to supportDA classifiers in the AL sampling process. The results reveal that mostannotated sentences present low informativeness in the training dataset and thepatterns of these sentences can be easily captured by the DA classifier. Wealso demonstrate how AL methods can reduce the cost of manual annotation in theAL sampling process.", "output": "Does Informativeness Matter? Active Learning for Educational Dialogue Act Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study evaluates the potential of a large language model for aiding ingeneration of semantic feature norms - a critical tool for evaluatingconceptual structure in cognitive science. Building from an existinghuman-generated dataset, we show that machine-verified norms capture aspects ofconceptual structure beyond what is expressed in human norms alone, and betterexplain human judgments of semantic similarity amongst items that are distallyrelated. The results suggest that LLMs can greatly enhance traditional methodsof semantic feature norm verification, with implications for our understandingof conceptual representation in humans and machines.", "output": "Semantic Feature Verification in FLAN-T5."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the Seismic Laboratory for Imaging and Modeling/Monitoring (SLIM)open-source software framework for computational geophysics and, moregenerally, inverse problems involving the wave-equation (e.g., seismic andmedical ultrasound), regularization with learned priors, and learned neuralsurrogates for multiphase flow simulations. By integrating multiple layers ofabstraction, our software is designed to be both readable and scalable. Thisallows researchers to easily formulate their problems in an abstract fashionwhile exploiting the latest developments in high-performance computing. Weillustrate and demonstrate our design principles and their benefits by means ofbuilding a scalable prototype for permeability inversion from time-lapsecrosswell seismic data, which aside from coupling of wave physics andmultiphase flow, involves machine learning.", "output": "Learned multiphysics inversion with differentiable programming and machine learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Audiovisual representation learning typically relies on the correspondencebetween sight and sound. However, there are often multiple audio tracks thatcan correspond with a visual scene. Consider, for example, differentconversations on the same crowded street. The effect of such counterfactualpairs on audiovisual representation learning has not been previously explored.To investigate this, we use dubbed versions of movies to augment cross-modalcontrastive learning. Our approach learns to represent alternate audio tracks,differing only in speech content, similarly to the same video. Our results showthat dub-augmented training improves performance on a range of auditory andaudiovisual tasks, without significantly affecting linguistic task performanceoverall. We additionally compare this approach to a strong baseline where weremove speech before pretraining, and find that dub-augmented training is moreeffective, including for paralinguistic and audiovisual tasks where speechremoval leads to worse performance. These findings highlight the importance ofconsidering speech variation when learning scene-level audiovisualcorrespondences and suggest that dubbed audio can be a useful augmentationtechnique for training audiovisual models toward more robust performance.", "output": "Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Segment Anything Model (SAM) is a new image segmentation tool trainedwith the largest segmentation dataset at this time. The model has demonstratedthat it can create high-quality masks for image segmentation with goodpromptability and generalizability. However, the performance of the model onmedical images requires further validation. To assist with the development,assessment, and utilization of SAM on medical images, we introduce Segment AnyMedical Model (SAMM), an extension of SAM on 3D Slicer, a widely-usedopen-source image processing and visualization software that has beenextensively used in the medical imaging community. This open-source extensionto 3D Slicer and its demonstrations are posted on GitHub( SAMM achieves 0.6-second latency of acomplete cycle and can infer image masks in nearly real-time.", "output": "SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern multi-agent reinforcement learning (RL) algorithms hold greatpotential for solving a variety of real-world problems. However, they do notfully exploit cross-agent knowledge to reduce sample complexity and improveperformance. Although transfer RL supports knowledge sharing, it ishyperparameter sensitive and complex. To solve this problem, we propose a novelmulti-agent policy reciprocity (PR) framework, where each agent can fullyexploit cross-agent policies even in mismatched states. We then define anadjacency space for mismatched states and design a plug-and-play module forvalue iteration, which enables agents to infer more precise returns. To improvethe scalability of PR, deep PR is proposed for continuous control tasks.Moreover, theoretical analysis shows that agents can asymptotically reachconsensus through individual perceived rewards and converge to an optimal valuefunction, which implies the stability and effectiveness of PR, respectively.Experimental results on discrete and continuous environments demonstrate thatPR outperforms various existing RL and transfer RL methods.", "output": "Multi-agent Policy Reciprocity with Theoretical Guarantee."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The digital transformation of automation places new demands on dataacquisition and processing in industrial processes. Logical relationshipsbetween acquired data and cyclic process sequences must be correctlyinterpreted and evaluated. To solve this problem, a novel approach based onevolutionary algorithms is proposed to self optimise the system logic ofcomplex processes. Based on the genetic results, a programme code for thesystem implementation is derived by decoding the solution. This is achieved bya flexible system structure with an upstream, intermediate and downstream unit.In the intermediate unit, a directed learning process interacts with a systemreplica and an evaluation function in a closed loop. The code generationstrategy is represented by redundancy and priority, sequencing and performancederivation. The presented approach is evaluated on an industrial liquid stationprocess subject to a multi-objective optimisation problem.", "output": "Self Optimisation and Automatic Code Generation by Evolutionary Algorithms in PLC based Controlling Processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We prove a few representer theorems for a localised version of theregularised and multiview support vector machine learning problem introduced byH.Q.~Minh, L.~Bazzani, and V.~Murino, textit{Journal of Machine LearningResearch}, textbf{17}(2016) 1--72, that involves operator valued positivesemidefinite kernels and their reproducing kernel Hilbert spaces. The resultsconcern general cases when convex or nonconvex loss functions and finite orinfinite dimensional input spaces are considered. We show that the generalframework allows infinite dimensional input spaces and nonconvex loss functionsfor some special cases, in particular in case the loss functions are G^ateauxdifferentiable. Detailed calculations are provided for the exponential leastsquares loss functions that leads to partially nonlinear problems.", "output": "Localisation of Regularised and Multiview Support Vector Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable AI has become a popular tool for validating machine learningmodels. Mismatches between the explained model's decision strategy and theuser's domain knowledge (e.g. Clever Hans effects) have also been recognized asa starting point for improving faulty models. However, it is less clear what todo when the user and the explanation agree. In this paper, we demonstrate thatacceptance of explanations by the user is not a guarantee for a ML model tofunction well, in particular, some Clever Hans effects may remain undetected.Such hidden flaws of the model can nevertheless be mitigated, and wedemonstrate this by contributing a new method, Explanation-Guided ExposureMinimization (EGEM), that premptively prunes variations in the ML model thathave not been the subject of positive explanation feedback. Experiments onnatural image data demonstrate that our approach leads to models that stronglyreduce their reliance on hidden Clever Hans strategies, and consequentlyachieve higher accuracy on new data.", "output": "Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, Dynamic Graph (DG) representations have been increasinglyused for modeling dynamic systems due to their ability to integrate bothtopological and temporal information in a compact representation. Dynamicgraphs allow to efficiently handle applications such as social networkprediction, recommender systems, traffic forecasting or electroencephalographyanalysis, that can not be adressed using standard numeric representations. As adirect consequence of the emergence of dynamic graph representations, dynamicgraph learning has emerged as a new machine learning problem, combiningchallenges from both sequential/temporal data processing and static graphlearning. In this research area, Dynamic Graph Neural Network (DGNN) has becamethe state of the art approach and plethora of models have been proposed in thevery recent years. This paper aims at providing a review of problems and modelsrelated to dynamic graph learning. The various dynamic graph supervisedlearning settings are analysed and discussed. We identify the similarities anddifferences between existing models with respect to the way time information ismodeled. Finally, general guidelines for a DGNN designer when faced with adynamic graph learning problem are provided.", "output": "Dynamic Graph Representation Learning with Neural Networks: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to incrementally learn new classes from limited samples iscrucial to the development of artificial intelligence systems for real clinicalapplication. Although existing incremental learning techniques have attemptedto address this issue, they still struggle with only few labeled data,particularly when the samples are from varied domains. In this paper, weexplore the cross-domain few-shot incremental learning (CDFSCIL) problem.CDFSCIL requires models to learn new classes from very few labeled samplesincrementally, and the new classes may be vastly different from the targetspace. To counteract this difficulty, we propose a cross-domain enhancementconstraint and cross-domain data augmentation method. Experiments on MedMNISTshow that the classification performance of this method is better than othersimilar incremental learning methods.", "output": "Few-shot Class-incremental Learning for Cross-domain Disease Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As data-driven intelligent systems advance, the need for reliable andtransparent decision-making mechanisms has become increasingly important.Therefore, it is essential to integrate uncertainty quantification and modelexplainability approaches to foster trustworthy business and operationalprocess analytics. This study explores how model uncertainty can be effectivelycommunicated in global and local post-hoc explanation approaches, such asPartial Dependence Plots (PDP) and Individual Conditional Expectation (ICE)plots. In addition, this study examines appropriate visualization analyticsapproaches to facilitate such methodological integration. By combining thesetwo research directions, decision-makers can not only justify the plausibilityof explanation-driven actionable insights but also validate their reliability.Finally, the study includes expert interviews to assess the suitability of theproposed approach and designed interface for a real-world predictive processmonitoring problem in the manufacturing domain.", "output": "Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study focuses on long-term forecasting (LTF) on continuous-time dynamicgraph networks (CTDGNs), which is important for real-world modeling. ExistingCTDGNs are effective for modeling temporal graph data due to their ability tocapture complex temporal dependencies but perform poorly on LTF due to thesubstantial requirement for historical data, which is not practical in mostcases. To relieve this problem, a most intuitive way is data augmentation. Inthis study, we propose textbf{underline{U}ncertainty underline{M}askedunderline{M}ixunderline{U}p (UmmU)}: a plug-and-play module that conductsuncertainty estimation to introduce uncertainty into the embedding ofintermediate layer of CTDGNs, and perform masked mixup to further enhance theuncertainty of the embedding to make it generalize to more situations. UmmU canbe easily inserted into arbitrary CTDGNs without increasing the number ofparameters. We conduct comprehensive experiments on three real-world dynamicgraph datasets, the results demonstrate that UmmU can effectively improve thelong-term forecasting performance for CTDGNs.", "output": "Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the geometry of linear networks with one-dimensional convolutionallayers. The function spaces of these networks can be identified withsemi-algebraic families of polynomials admitting sparse factorizations. Weanalyze the impact of the network's architecture on the function space'sdimension, boundary, and singular points. We also describe the critical pointsof the network's parameterization map. Furthermore, we study the optimizationproblem of training a network with the squared error loss. We prove that forarchitectures where all strides are larger than one and generic data, thenon-zero critical points of that optimization problem are smooth interiorpoints of the function space. This property is known to be false for denselinear networks and linear convolutional networks with stride one.", "output": "Function Space and Critical Points of Linear Convolutional Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this article we identify a general class of high-dimensional continuousfunctions that can be approximated by deep neural networks (DNNs) with therectified linear unit (ReLU) activation without the curse of dimensionality. Inother words, the number of DNN parameters grows at most polynomially in theinput dimension and the approximation error. The functions in our class can beexpressed as a potentially unbounded number of compositions of specialfunctions which include products, maxima, and certain parallelized Lipschitzcontinuous functions.", "output": "Deep neural network approximation of composite functions without the curse of dimensionality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series classification (TSC) is a challenging task due to the diversityof types of feature that may be relevant for different classification tasks,including trends, variance, frequency, magnitude, and various patterns. Toaddress this challenge, several alternative classes of approach have beendeveloped, including similarity-based, features and intervals, shapelets,dictionary, kernel, neural network, and hybrid approaches. While kernel, neuralnetwork, and hybrid approaches perform well overall, some specializedapproaches are better suited for specific tasks. In this paper, we propose anew similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), whichoutperforms previous state-of-the-art similarity-based classifiers across theUCR benchmark and outperforms state-of-the-art kernel, neural network, andhybrid methods on specific datasets in the benchmark that are best addressed bysimilarity-base methods. PF 2.0 incorporates three recent advances in timeseries similarity measures -- (1) computationally efficient early abandoningand pruning to speedup elastic similarity computations; (2) a new elasticsimilarity measure, Amerced Dynamic Time Warping (ADTW); and (3) cost functiontuning. It rationalizes the set of similarity measures employed, reducing theeight base measures of the original PF to three and using the first derivativetransform with all similarity measures, rather than a limited subset. We haveimplemented both PF 1.0 and PF 2.0 in a single C++ framework, making the PFframework more efficient.", "output": "Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In our study, we apply different statistical models to nowcast quarterly GDPgrowth for the US economy. Using the monthly FRED-MD database, we compare thenowcasting performance of the dynamic factor model (DFM) and four artificialneural networks (ANNs): the multilayer perceptron (MLP), the one-dimensionalconvolutional neural network (1D CNN), the long short-term memory network(LSTM), and the gated recurrent unit (GRU). The empirical analysis presents theresults from two distinctively different evaluation periods. The first (2010:Q1-- 2019:Q4) is characterized by balanced economic growth, while the second(2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. Accordingto our results, longer input sequences result in more accurate nowcasts inperiods of balanced economic growth. However, this effect ceases above arelatively low threshold value of around six quarters (eighteen months). Duringperiods of economic turbulence (e.g., during the COVID-19 recession), longertraining sequences do not help the models' predictive performance; instead,they seem to weaken their generalization capability. Our results show that 1DCNN, with the same parameters, generates accurate nowcasts in both of ourevaluation periods. Consequently, first in the literature, we propose the useof this specific neural network architecture for economic nowcasting.", "output": "GDP nowcasting with artificial neural networks: How much does long-term memory matter?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurately inferring Gene Regulatory Networks (GRNs) is a critical andchallenging task in biology. GRNs model the activatory and inhibitoryinteractions between genes and are inherently causal in nature. To accuratelyidentify GRNs, perturbational data is required. However, most GRN discoverymethods only operate on observational data. Recent advances in neuralnetwork-based causal discovery methods have significantly improved causaldiscovery, including handling interventional data, improvements in performanceand scalability. However, applying state-of-the-art (SOTA) causal discoverymethods in biology poses challenges, such as noisy data and a large number ofsamples. Thus, adapting the causal discovery methods is necessary to handlethese challenges. In this paper, we introduce DiscoGen, a neural network-basedGRN discovery method that can denoise gene expression measurements and handleinterventional data. We demonstrate that our model outperforms SOTA neuralnetwork-based causal discovery methods.", "output": "DiscoGen: Learning to Discover Gene Regulatory Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In federated learning, benign participants aim to optimize a global modelcollaboratively. However, the risk of textit{privacy leakage} cannot beignored in the presence of textit{semi-honest} adversaries. Existing researchhas focused either on designing protection mechanisms or on inventing attackingmechanisms. While the battle between defenders and attackers seemsnever-ending, we are concerned with one critical question: is it possible toprevent potential attacks in advance? To address this, we propose the firstgame-theoretic framework that considers both FL defenders and attackers interms of their respective payoffs, which include computational costs, FL modelutilities, and privacy leakage risks. We name this game the Federated LearningSecurity Game (FLSG), in which neither defenders nor attackers are aware of allparticipants' payoffs.To handle the textit{incomplete information} inherent in this situation, wepropose associating the FLSG with an textit{oracle} that has two primaryresponsibilities. First, the oracle provides lower and upper bounds of thepayoffs for the players. Second, the oracle acts as a correlation device,privately providing suggested actions to each player. With this novelframework, we analyze the optimal strategies of defenders and attackers.Furthermore, we derive and demonstrate conditions under which the attacker, asa rational decision-maker, should always follow the oracle's suggestiontextit{not to attack}.", "output": "A Game-theoretic Framework for Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present new Recurrent Neural Network (RNN) cells for image classificationusing a Neural Architecture Search (NAS) approach called DARTS. We areinterested in the ReNet architecture, which is a RNN based approach presentedas an alternative for convolutional and pooling steps. ReNet can be definedusing any standard RNN cells, such as LSTM and GRU. One limitation is thatstandard RNN cells were designed for one dimensional sequential data and notfor two dimensions like it is the case for image classification. We overcomethis limitation by using DARTS to find new cell designs. We compare our resultswith ReNet that uses GRU and LSTM cells. Our found cells outperform thestandard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicategeneralizability, as we derived the RNN cell designs from CIFAR-10 withoutperforming a new cell search for SVHN.", "output": "DartsReNet: Exploring new RNN cells in ReNet architectures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpretability of AI models allows for user safety checks to build trust inthese models. In particular, decision trees (DTs) provide a global view on thelearned model and clearly outlines the role of the features that are criticalto classify a given data. However, interpretability is hindered if the DT istoo large. To learn compact trees, a Reinforcement Learning (RL) framework hasbeen recently proposed to explore the space of DTs. A given supervisedclassification task is modeled as a Markov decision problem (MDP) and thenaugmented with additional actions that gather information about the features,equivalent to building a DT. By appropriately penalizing these actions, the RLagent learns to optimally trade-off size and performance of a DT. However, todo so, this RL agent has to solve a partially observable MDP. The maincontribution of this paper is to prove that it is sufficient to solve a fullyobservable problem to learn a DT optimizing the interpretability-performancetrade-off. As such any planning or RL algorithm can be used. We demonstrate theeffectiveness of this approach on a set of classical supervised classificationdatasets and compare our approach with other interpretability-performanceoptimizing methods.", "output": "Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is desirable to predict the behavior of traffic participants conditionedon different planned trajectories of the autonomous vehicle. This allows thedownstream planner to estimate the impact of its decisions. Recent approachesfor conditional behavior prediction rely on a regression decoder, meaning thatcoordinates or polynomial coefficients are regressed. In this work we revisitset-based trajectory prediction, where the probability of each trajectory in apredefined trajectory set is determined by a classification model, andfirst-time employ it to the task of conditional behavior prediction. We proposeRESET, which combines a new metric-driven algorithm for trajectory setgeneration with a graph-based encoder. For unconditional prediction, RESETachieves comparable performance to a regression-based approach. Due to thenature of set-based approaches, it has the advantageous property of being ableto predict a flexible number of trajectories without influencing runtime orcomplexity. For conditional prediction, RESET achieves reasonable results withlate fusion of the planned trajectory, which was not observed forregression-based approaches before. This means that RESET is computationallylightweight to combine with a planner that proposes multiple future plans ofthe autonomous vehicle, as large parts of the forward pass can be reused.", "output": "RESET: Revisiting Trajectory Sets for Conditional Behavior Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability of convolutional neural networks (CNNs) to recognize objectsregardless of their position in the image is due to thetranslation-equivariance of the convolutional operation. Group-equivariant CNNstransfer this equivariance to other transformations of the input. Dealingappropriately with objects and object parts of different scale is challenging,and scale can vary for multiple reasons such as the underlying object size orthe resolution of the imaging modality. In this paper, we propose ascale-equivariant convolutional network layer for three-dimensional data thatguarantees scale-equivariance in 3D CNNs. Scale-equivariance lifts the burdenof having to learn each possible scale separately, allowing the neural networkto focus on higher-level learning goals, which leads to better results andbetter data-efficiency. We provide an overview of the theoretical foundationsand scientific work on scale-equivariant neural networks in the two-dimensionaldomain. We then transfer the concepts from 2D to the three-dimensional spaceand create a scale-equivariant convolutional layer for 3D data. Using theproposed scale-equivariant layer, we create a scale-equivariant U-Net formedical image segmentation and compare it with a non-scale-equivariant baselinemethod. Our experiments demonstrate the effectiveness of the proposed method inachieving scale-equivariance for 3D medical image analysis. We publish our codeat  for furtherresearch and application.", "output": "Scale-Equivariant Deep Learning for 3D Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "StyleGANs are at the forefront of controllable image generation as theyproduce a latent space that is semantically disentangled, making it suitablefor image editing and manipulation. However, the performance of StyleGANsseverely degrades when trained via class-conditioning on large-scalelong-tailed datasets. We find that one reason for degradation is the collapseof latents for each class in the $mathcal{W}$ latent space. With NoisyTwins,we first introduce an effective and inexpensive augmentation strategy for classembeddings, which then decorrelates the latents based on self-supervision inthe $mathcal{W}$ space. This decorrelation mitigates collapse, ensuring thatour method preserves intra-class diversity with class-consistency in imagegeneration. We show the effectiveness of our approach on large-scale real-worldlong-tailed datasets of ImageNet-LT and iNaturalist 2019, where our methodoutperforms other methods by $sim 19%$ on FID, establishing a newstate-of-the-art.", "output": "NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of approaches for trajectory prediction requires metrics tovalidate and compare their performance. Currently established metrics are basedon Euclidean distance, which means that errors are weighted equally in alldirections. Euclidean metrics are insufficient for structured environments likeroads, since they do not properly capture the agent's intent relative to theunderlying lane. In order to provide a reasonable assessment of trajectoryprediction approaches with regard to the downstream planning task, we propose anew metric that is lane distance-based: Lane Miss Rate (LMR). For thecalculation of LMR, the ground-truth and predicted endpoints are assigned tolane segments, more precisely their centerlines. Measured by the distance alongthe lane segments, predictions that are within a certain threshold distance tothe ground-truth count as hits, otherwise they count as misses. LMR is thendefined as the ratio of sequences that yield a miss. Our results on threestate-of-the-art trajectory prediction models show that LMR preserves the orderof Euclidean distance-based metrics. In contrast to the Euclidean Miss Rate,qualitative results show that LMR yields misses for sequences where predictionsare located on wrong lanes. Hits on the other hand result for sequences wherepredictions are located on the correct lane. This means that LMR implicitlyweights Euclidean error relative to the lane and goes into the direction ofcapturing intents of traffic agents. The source code of LMR for Argoverse 1 ispublicly available.", "output": "LMR: Lane Distance-Based Metric for Trajectory Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) is a popular way of edge computing that doesn'tcompromise users' privacy. Current FL paradigms assume that data only resideson the edge, while cloud servers only perform model averaging. However, inreal-life situations such as recommender systems, the cloud server has theability to store historical and interactive features. In this paper, ourproposed Edge-Cloud Collaborative Knowledge Transfer Framework (ECCT) bridgesthe gap between the edge and cloud, enabling bi-directional knowledge transferbetween both, sharing feature embeddings and prediction logits. ECCTconsolidates various benefits, including enhancing personalization, enablingmodel heterogeneity, tolerating training asynchronization, and relievingcommunication burdens. Extensive experiments on public and industrial datasetsdemonstrate ECCT's effectiveness and potential for use in academia andindustry.", "output": "Edge-cloud Collaborative Learning with Federated and Centralized Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Finding a balance between collaboration and competition is crucial forartificial agents in many real-world applications. We investigate this using aMulti-Agent Reinforcement Learning (MARL) setup on the back of a high-impactproblem. The accumulation and yearly growth of plastic in the ocean causeirreparable damage to many aspects of oceanic health and the marina system. Toprevent further damage, we need to find ways to reduce macroplastics from knownplastic patches in the ocean. Here we propose a Graph Neural Network (GNN)based communication mechanism that increases the agents' observation space. Inour custom environment, agents control a plastic collecting vessel. Thecommunication mechanism enables agents to develop a communication protocolusing a binary signal. While the goal of the agent collective is to clean up asmuch as possible, agents are rewarded for the individual amount ofmacroplastics collected. Hence agents have to learn to communicate effectivelywhile maintaining high individual performance. We compare our proposedcommunication mechanism with a multi-agent baseline without the ability tocommunicate. Results show communication enables collaboration and increasescollective performance significantly. This means agents have learned theimportance of communication and found a balance between collaboration andcompetition.", "output": "Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural network (GNN) models are increasingly being used for theclassification of electroencephalography (EEG) data. However, GNN-baseddiagnosis of neurological disorders, such as Alzheimer's disease (AD), remainsa relatively unexplored area of research. Previous studies have relied onfunctional connectivity methods to infer brain graph structures and used simpleGNN architectures for the diagnosis of AD. In this work, we propose a noveladaptive gated graph convolutional network (AGGCN) that can provide explainablepredictions. AGGCN adaptively learns graph structures by combiningconvolution-based node feature enhancement with a well-known correlation-basedmeasure of functional connectivity. Furthermore, the gated graph convolutioncan dynamically weigh the contribution of various spatial scales. The proposedmodel achieves high accuracy in both eyes-closed and eyes-open conditions,indicating the stability of learned representations. Finally, we demonstratethat the proposed AGGCN model generates consistent explanations of itspredictions that might be relevant for further study of AD-related alterationsof brain networks.", "output": "Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quality control (QC) has long been considered essential to guarantee thereliability of neuroimaging studies. It is particularly important for fetalbrain MRI, where large and unpredictable fetal motion can lead to substantialartifacts in the acquired images. Existing methods for fetal brain qualityassessment operate at the textit{slice} level, and fail to get a comprehensivepicture of the quality of an image, that can only be achieved by looking at thetextit{entire} brain volume. In this work, we propose FetMRQC, a machinelearning framework for automated image quality assessment tailored to fetalbrain MRI, which extracts an ensemble of quality metrics that are then used topredict experts' ratings. Based on the manual ratings of more than 1000low-resolution stacks acquired across two different institutions, we show that,compared with existing quality metrics, FetMRQC is able to generalizeout-of-domain, while being interpretable and data efficient. We also release anovel manual quality rating tool designed to facilitate and optimize qualityrating of fetal brain images.Our tool, along with all the code to generate, train and evaluate the modelwill be released upon acceptance of the paper.", "output": "FetMRQC: Automated Quality Control for fetal brain MRI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the design of sample-efficient algorithms for reinforcement learningin the presence of rich, high-dimensional observations, formalized via theBlock MDP problem. Existing algorithms suffer from either 1) computationalintractability, 2) strong statistical assumptions that are not necessarilysatisfied in practice, or 3) suboptimal sample complexity. We address theseissues by providing the first computationally efficient algorithm that attainsrate-optimal sample complexity with respect to the desired accuracy level, withminimal statistical assumptions. Our algorithm, MusIK, combines systematicexploration with representation learning based on multi-step inversekinematics, a learning objective in which the aim is to predict the learner'sown action from the current observation and observations in the (potentiallydistant) future. MusIK is simple and flexible, and can efficiently takeadvantage of general-purpose function approximation. Our analysis leveragesseveral new techniques tailored to non-optimistic exploration algorithms, whichwe anticipate will find broader use.", "output": "Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most real-world networks evolve over time. Existing literature proposesmodels for dynamic networks that are either unlabeled or assumed to have asingle membership structure. On the other hand, a new family of MixedMembership Stochastic Block Models (MMSBM) allows to model static labelednetworks under the assumption of mixed-membership clustering. In this work, wepropose to extend this later class of models to infer dynamic labeled networksunder a mixed membership assumption. Our approach takes the form of a temporalprior on the model's parameters. It relies on the single assumption thatdynamics are not abrupt. We show that our method significantly differs fromexisting approaches, and allows to model more complex systems --dynamic labelednetworks. We demonstrate the robustness of our method with several experimentson both synthetic and real-world datasets. A key interest of our approach isthat it needs very few training data to yield good results. The performancegain under challenging conditions broadens the variety of possible applicationsof automated learning tools --as in social sciences, which comprise many fieldswhere small datasets are a major obstacle to the introduction of machinelearning methods.", "output": "Dynamic Mixed Membership Stochastic Block Model for Weighted Labeled Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data augmentation forms the cornerstone of many modern machine learningtraining pipelines; yet, the mechanisms by which it works are not clearlyunderstood. Much of the research on data augmentation (DA) has focused onimproving existing techniques, examining its regularization effects in thecontext of neural network over-fitting, or investigating its impact onfeatures. Here, we undertake a holistic examination of the effect of DA onthree different classifiers, convolutional neural networks, support vectormachines, and logistic regression models, which are commonly used in supervisedclassification of imbalanced data. We support our examination with testing onthree image and five tabular datasets. Our research indicates that DA, whenapplied to imbalanced data, produces substantial changes in model weights,support vectors and feature selection; even though it may only yield relativelymodest changes to global metrics, such as balanced accuracy or F1 measure. Wehypothesize that DA works by facilitating variances in data, so that machinelearning models can associate changes in the data with labels. By diversifyingthe range of feature amplitudes that a model must recognize to predict a label,DA improves a model's capacity to generalize when learning with imbalanceddata.", "output": "Towards Understanding How Data Augmentation Works with Imbalanced Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Surface electromyogram (EMG) can be employed as an interface signal forvarious devices and software via pattern recognition. In EMG-based patternrecognition, the classifier should not only be accurate, but also output anappropriate confidence (i.e., probability of correctness) for its prediction.If the confidence accurately reflects the likelihood of true correctness, thenit will be useful in various application tasks, such as motion rejection andonline adaptation. The aim of this paper is to identify the types ofclassifiers that provide higher accuracy and better confidence in EMG patternrecognition. We evaluate the performance of various discriminative andgenerative classifiers on four EMG datasets, both visually and quantitatively.The analysis results show that while a discriminative classifier based on adeep neural network exhibits high accuracy, it outputs a confidence thatdiffers from true probabilities. By contrast, a scale mixture model-basedclassifier, which is a generative classifier that can account for uncertaintyin EMG variance, exhibits superior performance in terms of both accuracy andconfidence.", "output": "Evaluating Classifier Confidence for Surface EMG Pattern Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion Models (DMs) are powerful generative models that add Gaussian noiseto the data and learn to remove it. We wanted to determine which noisedistribution (Gaussian or non-Gaussian) led to better generated data in DMs.Since DMs do not work by design with non-Gaussian noise, we built a frameworkthat allows reversing a diffusion process with non-Gaussian location-scalenoise. We use that framework to show that the Gaussian distribution performsthe best over a wide range of other distributions (Laplace, Uniform, t,Generalized-Gaussian).", "output": "Diffusion models with location-scale noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Note-level automatic music transcription is one of the most representativemusic information retrieval (MIR) tasks and has been studied for variousinstruments to understand music. However, due to the lack of high-qualitylabeled data, transcription of many instruments is still a challenging task. Inparticular, in the case of singing, it is difficult to find accurate notes dueto its expressiveness in pitch, timbre, and dynamics. In this paper, we proposea method of finding note onsets of singing voice more accurately by leveragingthe linguistic characteristics of singing, which are not seen in otherinstruments. The proposed model uses mel-scaled spectrogram and phoneticposteriorgram (PPG), a frame-wise likelihood of phoneme, as an input of theonset detection network while PPG is generated by the pre-trained network withsinging and speech data. To verify how linguistic features affect onsetdetection, we compare the evaluation results through the dataset with differentlanguages and divide onset types for detailed analysis. Our approachsubstantially improves the performance of singing transcription and thereforeemphasizes the importance of linguistic features in singing analysis.", "output": "A Phoneme-Informed Neural Network Model for Note-Level Singing Transcription."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Variational autoencoders (VAEs) are powerful generative modelling methods,however they suffer from blurry generated samples and reconstructions comparedto the images they have been trained on. Significant research effort has beenspent to increase the generative capabilities by creating more flexible modelsbut often flexibility comes at the cost of higher complexity and computationalcost. Several works have focused on altering the reconstruction term of theevidence lower bound (ELBO), however, often at the expense of losing themathematical link to maximizing the likelihood of the samples under the modeleddistribution. Here we propose a new formulation of the reconstruction term forthe VAE that specifically penalizes the generation of blurry images while atthe same time still maximizing the ELBO under the modeled distribution. We showthe potential of the proposed loss on three different data sets, where itoutperforms several recently proposed reconstruction losses for VAEs.", "output": "Explicitly Minimizing the Blur Error of Variational Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the slowing down of Moore's law, augmenting complementary-metal-oxidesemiconductor (CMOS) transistors with emerging nanotechnologies (X) is becomingincreasingly important. In this paper, we demonstrate how stochastic magnetictunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combinedwith versatile Field Programmable Gate Arrays (FPGA) to design anenergy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Ourheterogeneous computer successfully performs probabilistic inference andasynchronous Boltzmann learning despite device-to-device variations in sMTJs. Acomprehensive comparison using a CMOS predictive process design kit (PDK)reveals that digital CMOS-based p-bits emulating high-quality randomness useover 10,000 transistors with the energy per generated random number beingroughly two orders of magnitude greater than the sMTJ-based p-bits thatdissipate only 2 fJ. Scaled and integrated versions of our approach cansignificantly advance probabilistic computing and its applications in variousdomains, including probabilistic machine learning, optimization, and quantumsimulation.", "output": "CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning (ML) models can fail in unexpected ways in the real world,but not all model failures are equal. With finite time and resources, MLpractitioners are forced to prioritize their model debugging and improvementefforts. Through interviews with 13 ML practitioners at Apple, we found thatpractitioners construct small targeted test sets to estimate an error's nature,scope, and impact on users. We built on this insight in a case study withmachine translation models, and developed Angler, an interactive visualanalytics tool to help practitioners prioritize model improvements. In a userstudy with 7 machine translation experts, we used Angler to understandprioritization practices when the input space is infinite, and obtainingreliable signals of model quality is expensive. Our study revealed thatparticipants could form more interesting and user-focused hypotheses forprioritization by analyzing quantitative summary statistics and qualitativelyassessing data by reading sentences.", "output": "Angler: Helping Machine Translation Practitioners Prioritize Model Improvements."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Localizing behaviors of neural networks to a subset of the network'scomponents or a subset of interactions between components is a natural firststep towards analyzing network mechanisms and possible failure modes. Existingwork is often qualitative and ad-hoc, and there is no consensus on theappropriate way to evaluate localization claims. We introduce path patching, atechnique for expressing and quantitatively testing a natural class ofhypotheses expressing that behaviors are localized to a set of paths. We refinean explanation of induction heads, characterize a behavior of GPT-2, and opensource a framework for efficiently running similar experiments.", "output": "Localizing Model Behavior with Path Patching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Methods such as chain-of-thought prompting and self-consistency have pushedthe frontier of language model reasoning performance with no additionaltraining. To further improve performance, we propose a prompt ensembling methodfor large language models, which uses a small dataset to construct a set of fewshot prompts that together comprise a ``boosted prompt ensemble''. The few shotexamples for each prompt are chosen in a stepwise fashion to be ``hard''examples on which the previous step's ensemble is uncertain. We show that thisoutperforms single-prompt output-space ensembles and bagged prompt-spaceensembles on the GSM8k and AQuA datasets, among others. We propose bothtrain-time and test-time versions of boosted prompting that use differentlevels of available annotation and conduct a detailed empirical study of ouralgorithm.", "output": "Boosted Prompt Ensembles for Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present ImageReward -- the first general-purpose text-to-image humanpreference reward model -- to address various prevalent issues in generativemodels and align them with human values and preferences. Its training is basedon our systematic annotation pipeline that covers both the rating and rankingcomponents, collecting a dataset of 137k expert comparisons to date. In humanevaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by38.6%), making it a promising automatic metric for evaluating and improvingtext-to-image synthesis. The reward model is publicly available via thetexttt{image-reward} package at url{", "output": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A new approach called NAF (the Neural Attention Forest) for solvingregression and classification tasks under tabular training data is proposed.The main idea behind the proposed NAF model is to introduce the attentionmechanism into the random forest by assigning attention weights calculated byneural networks of a specific form to data in leaves of decision trees and tothe random forest itself in the framework of the Nadaraya-Watson kernelregression. In contrast to the available models like the attention-based randomforest, the attention weights and the Nadaraya-Watson regression arerepresented in the form of neural networks whose weights can be regarded astrainable parameters. The first part of neural networks with shared weights istrained for all trees and computes attention weights of data in leaves. Thesecond part aggregates outputs of the tree networks and aims to minimize thedifference between the random forest prediction and the truth target value froma training set. The neural network is trained in an end-to-end manner. Thecombination of the random forest and neural networks implementing the attentionmechanism forms a transformer for enhancing the forest predictions. Numericalexperiments with real datasets illustrate the proposed method. The codeimplementing the approach is publicly available.", "output": "Neural Attention Forests: Transformer-Based Forest Improvement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study presents a machine learning (ML) pipeline for clinical dataclassification in the context of a 30-day readmission problem, along with afairness audit on subgroups based on sensitive attributes. A range of ML modelsare used for classification and the fairness audit is conducted on the modelpredictions. The fairness audit uncovers disparities in equal opportunity,predictive parity, false positive rate parity, and false negative rate paritycriteria on the MIMIC III dataset based on attributes such as gender,ethnicity, language, and insurance group. The results identify disparities inthe model's performance across different groups and highlights the need forbetter fairness and bias mitigation strategies. The study suggests the need forcollaborative efforts among researchers, policymakers, and practitioners toaddress bias and fairness in artificial intelligence (AI) systems.", "output": "Auditing ICU Readmission Rates in an Clinical Database: An Analysis of Risk Factors and Clinical Outcomes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Acquiring knowledge about object interactions and affordances can facilitatescene understanding and human-robot collaboration tasks. As humans tend to useobjects in many different ways depending on the scene and the objects'availability, learning object affordances in everyday-life scenarios is achallenging task, particularly in the presence of an open set of interactionsand objects. We address the problem of affordance categorization forclass-agnostic objects with an open set of interactions; we achieve this bylearning similarities between object interactions in an unsupervised way andthus inducing clusters of object affordances. A novel depth-informedqualitative spatial representation is proposed for the construction of ActivityGraphs (AGs), which abstract from the continuous representation ofspatio-temporal interactions in RGB-D videos. These AGs are clustered to obtaingroups of objects with similar affordances. Our experiments in a real-worldscenario demonstrate that our method learns to create object affordanceclusters with a high V-measure even in cluttered scenes. The proposed approachhandles object occlusions by capturing effectively possible interactions andwithout imposing any object or scene constraints.", "output": "Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Physics-informed neural networks (PINNs) have proven a suitable mathematicalscaffold for solving inverse ordinary (ODE) and partial differential equations(PDE). Typical inverse PINNs are formulated as soft-constrained multi-objectiveoptimization problems with several hyperparameters. In this work, wedemonstrate that inverse PINNs can be framed in terms of maximum-likelihoodestimators (MLE) to allow explicit error propagation from interpolation to thephysical model space through Taylor expansion, without the need ofhyperparameter tuning. We explore its application to high-dimensional coupledODEs constrained by differential algebraic equations that are common intransient chemical and biological kinetics. Furthermore, we show thatsingular-value decomposition (SVD) of the ODE coupling matrices (reactionstoichiometry matrix) provides reduced uncorrelated subspaces in which PINNssolutions can be represented and over which residuals can be projected.Finally, SVD bases serve as preconditioners for the inversion of covariancematrices in this hyperparameter-free robust application of MLE to``kinetics-informed neural networks''.", "output": "Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computer vision applications in transportation logistics and warehousing havea huge potential for process automation. We present a structured literaturereview on research in the field to help leverage this potential. All literatureis categorized w.r.t. the application, i.e. the task it tackles and w.r.t. thecomputer vision techniques that are used. Regarding applications, we subdividethe literature in two areas: Monitoring, i.e. observing and retrieving relevantinformation from the environment, and manipulation, where approaches are usedto analyze and interact with the environment. In addition to that, we point outdirections for future research and link to recent developments in computervision that are suitable for application in logistics. Finally, we present anoverview of existing datasets and industrial solutions. We conclude that whilealready many research areas have been investigated, there is still hugepotential for future research. The results of our analysis are also availableonline at ", "output": "Literature Review: Computer Vision Applications in Transportation Logistics and Warehousing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite their potential in real-world applications, multi-agent reinforcementlearning (MARL) algorithms often suffer from high sample complexity. To addressthis issue, we present a novel model-based MARL algorithm, BiLL (Bi-LevelLatent Variable Model-based Learning), that learns a bi-level latent variablemodel from high-dimensional inputs. At the top level, the model learns latentrepresentations of the global state, which encode global information relevantto behavior learning. At the bottom level, it learns latent representations foreach agent, given the global latent representations from the top level. Themodel generates latent trajectories to use for policy learning. We evaluate ouralgorithm on complex multi-agent tasks in the challenging SMAC and Flatlandenvironments. Our algorithm outperforms state-of-the-art model-free andmodel-based baselines in sample efficiency, including on two extremelychallenging Super Hard SMAC maps.", "output": "Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Heart disorder has just overtaken cancer as the world's biggest cause ofmortality. Several cardiac failures, heart disease mortality, and diagnosticcosts can all be reduced with early identification and treatment. Medical datais collected in large quantities by the healthcare industry, but it is not wellmined. The discovery of previously unknown patterns and connections in thisinformation can help with an improved decision when it comes to forecastingheart disorder risk. In the proposed study, we constructed an ML-baseddiagnostic system for heart illness forecasting, using a heart disorderdataset. We used data preprocessing techniques like outlier detection andremoval, checking and removing missing entries, feature normalization,cross-validation, nine classification algorithms like RF, MLP, KNN, ETC, XGB,SVC, ADB, DT, and GBM, and eight classifier measuring performance metrics likeramification accuracy, precision, F1 score, specificity, ROC, sensitivity,log-loss, and Matthews' correlation coefficient, as well as eightclassification performance evaluations. Our method can easily differentiatebetween people who have cardiac disease and those are normal. Receiveroptimistic curves and also the region under the curves were determined by everyclassifier. Most of the classifiers, pretreatment strategies, validationmethods, and performance assessment metrics for classification models have beendiscussed in this study. The performance of the proposed scheme has beenconfirmed, utilizing all of its capabilities. In this work, the impact ofclinical decision support systems was evaluated using a stacked ensembleapproach that included these nine algorithms", "output": "An Improved Heart Disease Prediction Using Stacked Ensemble Method."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Parkinson's disease (PD) is the most widespread movement condition and thesecond most common neurodegenerative disorder, following Alzheimer's. Movementsymptoms and imaging techniques are the most popular ways to diagnose thisdisease. However, they are not accurate and fast and may only be accessible toa few people. This study provides an autonomous system, i.e., PD-ADSV, fordiagnosing PD based on voice signals, which uses four machine learningclassifiers and the hard voting ensemble method to achieve the highestaccuracy. PD-ADSV is developed using Python and the Gradio web framework.", "output": "PD-ADSV: An Automated Diagnosing System Using Voice Signals and Hard Voting Ensemble Method for Parkinson's Disease."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a new annotation method called Sparse Annotation (SA) forcrowd counting, which reduces human labeling efforts by sparsely labelingindividuals in an image. We argue that sparse labeling can reduce theredundancy of full annotation and capture more diverse information from distantindividuals that is not fully captured by Partial Annotation methods. Besides,we propose a point-based Progressive Point Matching network (PPM) to betterexplore the crowd from the whole image with sparse annotation, which includes aProposal Matching Network (PMN) and a Performance Restoration Network (PRN).The PMN generates pseudo-point samples using a basic point classifier, whilethe PRN refines the point classifier with the pseudo points to maximizeperformance. Our experimental results show that PPM outperforms previoussemi-supervised crowd counting methods with the same amount of annotation by alarge margin and achieves competitive performance with state-of-the-artfully-supervised methods.", "output": "Crowd Counting with Sparse Annotation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works demonstrate a remarkable ability to customize text-to-imagediffusion models while only providing a few example images. What happens if youtry to customize such models using multiple, fine-grained concepts in asequential (i.e., continual) manner? In our work, we show that recentstate-of-the-art customization of text-to-image models suffer from catastrophicforgetting when new concepts arrive sequentially. Specifically, when adding anew concept, the ability to generate high quality images of past, similarconcepts degrade. To circumvent this forgetting, we propose a new method,C-LoRA, composed of a continually self-regularized low-rank adaptation in crossattention layers of the popular Stable Diffusion model. Furthermore, we usecustomization prompts which do not include the word of the customized object(i.e., \"person\" for a human face dataset) and are initialized as completelyrandom embeddings. Importantly, our method induces only marginal additionalparameter costs and requires no storage of user data for replay. We show thatC-LoRA not only outperforms several baselines for our proposed setting oftext-to-image continual customization, which we refer to as ContinualDiffusion, but that we achieve a new state-of-the-art in the well-establishedrehearsal-free continual learning setting for image classification. The highachieving performance of C-LoRA in two separate domains positions it as acompelling solution for a wide range of applications, and we believe it hassignificant potential for practical impact.", "output": "Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bayesian neural networks (BNN) are the probabilistic model that combines thestrengths of both neural network (NN) and stochastic processes. As a result,BNN can combat overfitting and perform well in applications where data islimited. Earthquake rupture study is such a problem where data is insufficient,and scientists have to rely on many trial and error numerical or physicalmodels. Lack of resources and computational expenses, often, it becomes hard todetermine the reasons behind the earthquake rupture. In this work, a BNN hasbeen used (1) to combat the small data problem and (2) to find out theparameter combinations responsible for earthquake rupture and (3) to estimatethe uncertainty associated with earthquake rupture. Two thousand rupturesimulations are used to train and test the model. A simple 2D rupture geometryis considered where the fault has a Gaussian geometric heterogeneity at thecenter, and eight parameters vary in each simulation. The test F1-score of BNN(0.8334), which is 2.34% higher than plain NN score. Results show that theparameters of rupture propagation have higher uncertainty than the rupturearrest. Normal stresses play a vital role in determining rupture propagationand are also the highest source of uncertainty, followed by the dynamicfriction coefficient. Shear stress has a moderate role, whereas the geometricfeatures such as the width and height of the fault are least significant anduncertain.", "output": "Estimating uncertainty of earthquake rupture using Bayesian neural network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Intensive algorithmic efforts have been made to enable the rapid improvementsof certificated robustness for complex ML models recently. However, currentrobustness certification methods are only able to certify under a limitedperturbation radius. Given that existing pure data-driven statisticalapproaches have reached a bottleneck, in this paper, we propose to integratestatistical ML models with knowledge (expressed as logical rules) as areasoning component using Markov logic networks (MLN, so as to further improvethe overall certified robustness. This opens new research questions aboutcertifying the robustness of such a paradigm, especially the reasoningcomponent (e.g., MLN). As the first step towards understanding these questions,we first prove that the computational complexity of certifying the robustnessof MLN is #P-hard. Guided by this hardness result, we then derive the firstcertified robustness bound for MLN by carefully analyzing different modelregimes. Finally, we conduct extensive experiments on five datasets includingboth high-dimensional images and natural language texts, and we show that thecertified robustness with knowledge-based logical reasoning indeedsignificantly outperforms that of the state-of-the-arts.", "output": "Improving Certified Robustness via Statistical Learning with Logical Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The convergence rates for convex and non-convex optimization methods dependon the choice of a host of constants, including step sizes, Lyapunov functionconstants and momentum constants. In this work we propose the use of factorialpowers as a flexible tool for defining constants that appear in convergenceproofs. We list a number of remarkable properties that these sequences enjoy,and show how they can be applied to convergence proofs to simplify or improvethe convergence rates of the momentum method, accelerated gradient and thestochastic variance reduced method (SVRG).", "output": "The Power of Factorial Powers: New Parameter settings for (Stochastic) Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Great advances in deep neural networks (DNNs) have led to state-of-the-artperformance on a wide range of tasks. However, recent studies have shown thatDNNs are vulnerable to adversarial attacks, which have brought great concernswhen deploying these models to safety-critical applications such as autonomousdriving. Different defense approaches have been proposed against adversarialattacks, including: a) empirical defenses, which can usually be adaptivelyattacked again without providing robustness certification; and b) certifiablyrobust approaches, which consist of robustness verification providing the lowerbound of robust accuracy against any attacks under certain conditions andcorresponding robust training approaches. In this paper, we systematizecertifiably robust approaches and related practical and theoreticalimplications and findings. We also provide the first comprehensive benchmark onexisting robustness verification and training approaches on different datasets.In particular, we 1) provide a taxonomy for the robustness verification andtraining approaches, as well as summarize the methodologies for representativealgorithms, 2) reveal the characteristics, strengths, limitations, andfundamental connections among these approaches, 3) discuss current researchprogresses, theoretical barriers, main challenges, and future directions forcertifiably robust approaches for DNNs, and 4) provide an open-sourced unifiedplatform to evaluate 20+ representative certifiably robust approaches.", "output": "SoK: Certified Robustness for Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Missing time-series data is a prevalent practical problem. Imputation methodsin time-series data often are applied to the full panel data with the purposeof training a model for a downstream out-of-sample task. For example, infinance, imputation of missing returns may be applied prior to training aportfolio optimization model. Unfortunately, this practice may result in alook-ahead-bias in the future performance on the downstream task. There is aninherent trade-off between the look-ahead-bias of using the full data set forimputation and the larger variance in the imputation from using only thetraining data. By connecting layers of information revealed in time, we proposea Bayesian posterior consensus distribution which optimally controls thevariance and look-ahead-bias trade-off in the imputation. We demonstrate thebenefit of our methodology both in synthetic and real financial data.", "output": "Time-Series Imputation with Wasserstein Interpolation for Optimal Look-Ahead-Bias and Variance Tradeoff."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "NLP pipelines with limited or no labeled data, rely on unsupervised methodsfor document processing. Unsupervised approaches typically depend on clusteringof terms or documents. In this paper, we introduce a novel clusteringalgorithm, Vec2GC (Vector to Graph Communities), an end-to-end pipeline tocluster terms or documents for any given text corpus. Our method uses communitydetection on a weighted graph of the terms or documents, created using textrepresentation learning. Vec2GC clustering algorithm is a density basedapproach, that supports hierarchical clustering as well.", "output": "Vec2GC -- A Graph Based Clustering Method for Text Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The success of deep learning has sparked interest in improving relationaltable tasks, like data preparation and search, with table representation modelstrained on large table corpora. Existing table corpora primarily contain tablesextracted from HTML pages, limiting the capability to represent offlinedatabase tables. To train and evaluate high-capacity models for applicationsbeyond the Web, we need resources with tables that resemble relational databasetables. Here we introduce GitTables, a corpus of 1M relational tables extractedfrom GitHub. Our continuing curation aims at growing the corpus to at least 10Mtables. Analyses of GitTables show that its structure, content, and topicalcoverage differ significantly from existing table corpora. We annotate tablecolumns in GitTables with semantic types, hierarchical relations anddescriptions from Schema.org and DBpedia. The evaluation of our annotationpipeline on the T2Dv2 benchmark illustrates that our approach provides resultson par with human annotations. We present three applications of GitTables,demonstrating its value for learned semantic type detection models, schemacompletion methods, and benchmarks for table-to-KG matching, data search, andpreparation. We make the corpus and code available at", "output": "GitTables: A Large-Scale Corpus of Relational Tables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is widely perceived that leveraging the success of modern machine learningtechniques to mobile devices and wireless networks has the potential ofenabling important new services. This, however, poses significant challenges,essentially due to that both data and processing power are highly distributedin a wireless network. In this paper, we develop a learning algorithm and anarchitecture that make use of multiple data streams and processing units, notonly during the training phase but also during the inference phase. Inparticular, the analysis reveals how inference propagates and fuses across anetwork. We study the design criterion of our proposed method and its bandwidthrequirements. Also, we discuss implementation aspects using neural networks intypical wireless radio access; and provide experiments that illustrate benefitsover state-of-the-art techniques.", "output": "In-Network Learning: Distributed Training and Inference in Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The paper discusses derivative-free optimization (DFO), which involvesminimizing a function without access to gradients or directional derivatives,only function evaluations. Classical DFO methods, which mimic gradient-basedmethods, such as Nelder-Mead and direct search have limited scalability forhigh-dimensional problems. Zeroth-order methods have been gaining popularitydue to the demands of large-scale machine learning applications, and the paperfocuses on the selection of the step size $alpha_k$ in these methods. Theproposed approach, called Curvature-Aware Random Search (CARS), uses first- andsecond-order finite difference approximations to compute a candidate$alpha_{+}$. We prove that for strongly convex objective functions, CARSconverges linearly provided that the search direction is drawn from adistribution satisfying very mild conditions. We also present a CubicRegularized variant of CARS, named CARS-CR, which converges in a rate of$mathcal{O}(k^{-1})$ without the assumption of strong convexity. Numericalexperiments show that CARS and CARS-CR match or exceed the state-of-the-arts onbenchmark problem sets.", "output": "Curvature-Aware Derivative-Free Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning at multiple levels of temporal abstraction is one of the keyattributes of intelligence. In reinforcement learning, this is often modeledthrough temporally extended courses of actions called options. Options allowagents to make predictions and to operate at different levels of abstractionwithin an environment. Nevertheless, approaches based on the options frameworkoften start with the assumption that a reasonable set of options is knownbeforehand. When this is not the case, there are no definitive answers forwhich options one should consider. In this paper, we argue that the successorrepresentation (SR), which encodes states based on the pattern of statevisitation that follows them, can be seen as a natural substrate for thediscovery and use of temporal abstractions. To support our claim, we take a bigpicture view of recent results, showing how the SR can be used to discoveroptions that facilitate either temporally-extended exploration or planning. Wecast these results as instantiations of a general framework for optiondiscovery in which the agent's representation is used to identify usefuloptions, which are then used to further improve its representation. Thisresults in a virtuous, never-ending, cycle in which both the representation andthe options are constantly refined based on each other. Beyond option discoveryitself, we also discuss how the SR allows us to augment a set of options into acombinatorially large counterpart without additional learning. This is achievedthrough the combination of previously learned options. Our empirical evaluationfocuses on options discovered for exploration and on the use of the SR tocombine them. The results of our experiments shed light on important designdecisions involved in the definition of options and demonstrate the synergy ofdifferent methods based on the SR, such as eigenoptions and the optionkeyboard.", "output": "Temporal Abstraction in Reinforcement Learning with the Successor Representation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a combined scaling method - named BASIC - that achieves 85.7%top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning fromany labeled ImageNet example. This accuracy surpasses best published similarmodels - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significantimprovements in robustness benchmarks. For instance, on 5 test sets withnatural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, ourmodel achieves 84.3% top-1 average accuracy, only a small drop from itsoriginal ImageNet accuracy. To achieve these results, we scale up thecontrastive learning framework of CLIP and ALIGN in three dimensions: datasize, model size, and batch size. Our dataset has 6.6B noisy image-text pairs,which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs thanALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and4x more than ALIGN. We encountered two main challenges with the scaling rulesof BASIC. First, the main challenge with implementing the combined scalingrules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. Toovercome the memory limit, we propose two simple methods which make use ofgradient checkpointing and model parallelism. Second, while increasing thedataset size and the model size has been the defacto method to improve theperformance of deep learning models like BASIC, the effect of a largecontrastive batch size on such contrastive-trained image-text models is notwell-understood. To shed light on the benefits of large contrastive batchsizes, we develop a theoretical framework which shows that larger contrastivebatch sizes lead to smaller generalization gaps for image-text models such asBASIC.", "output": "Combined Scaling for Zero-shot Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Markov decision process (MDP) provides a mathematical framework formodeling sequential decision-making problems, many of which are crucial tosecurity and safety, such as autonomous driving and robot control. The rapiddevelopment of artificial intelligence research has created efficient methodsfor solving MDPs, such as deep neural networks (DNNs), reinforcement learning(RL), and imitation learning (IL). However, these popular models solving MDPsare neither thoroughly tested nor rigorously reliable.We present MDPFuzz, the first blackbox fuzz testing framework for modelssolving MDPs. MDPFuzz forms testing oracles by checking whether the targetmodel enters abnormal and dangerous states. During fuzzing, MDPFuzz decideswhich mutated state to retain by measuring if it can reduce cumulative rewardsor form a new state sequence. We design efficient techniques to quantify the\"freshness\" of a state sequence using Gaussian mixture models (GMMs) anddynamic expectation-maximization (DynEM). We also prioritize states with highpotential of revealing crashes by estimating the local sensitivity of targetmodels over states.MDPFuzz is evaluated on five state-of-the-art models for solving MDPs,including supervised DNN, RL, IL, and multi-agent RL. Our evaluation includesscenarios of autonomous driving, aircraft collision avoidance, and two gamesthat are often used to benchmark RL. During a 12-hour run, we find over 80crash-triggering state sequences on each model. We show inspiring findings thatcrash-triggering states, though they look normal, induce distinct neuronactivation patterns compared with normal states. We further develop an abnormalbehavior detector to harden all the evaluated models and repair them with thefindings of MDPFuzz to significantly enhance their robustness withoutsacrificing accuracy.", "output": "MDPFuzz: Testing Models Solving Markov Decision Processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline estimation of the dynamical model of a Markov Decision Process (MDP)is a non-trivial task that greatly depends on the data available in thelearning phase. Sometimes the dynamics of the model is invariant with respectto some transformations of the current state and action. Recent works showedthat an expert-guided pipeline relying on Density Estimation methods as DeepNeural Network based Normalizing Flows effectively detects this structure indeterministic environments, both categorical and continuous-valued. Theacquired knowledge can be exploited to augment the original data set, leadingeventually to a reduction in the distributional shift between the true and thelearned model. Such data augmentation technique can be exploited as apreliminary process to be executed before adopting an Offline ReinforcementLearning architecture, increasing its performance. In this work we extend theparadigm to also tackle non-deterministic MDPs, in particular, 1) we propose adetection threshold in categorical environments based on statistical distances,and 2) we show that the former results lead to a performance improvement whensolving the learned MDP and then applying the optimized policy in the realenvironment.", "output": "Data Augmentation through Expert-guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A Bayesian Network is a directed acyclic graph (DAG) on a set of $n$ randomvariables (the vertices); a Bayesian Network Distribution (BND) is aprobability distribution on the random variables that is Markovian on thegraph. A finite $k$-mixture of such models is graphically represented by alarger graph which has an additional \"hidden\" (or \"latent\") random variable$U$, ranging in ${1,ldots,k}$, and a directed edge from $U$ to every othervertex. Models of this type are fundamental to causal inference, where $U$models an unobserved confounding effect of multiple populations, obscuring thecausal relationships in the observable DAG. By solving the mixture problem andrecovering the joint probability distribution on $U$, traditionallyunidentifiable causal relationships become identifiable. Using a reduction tothe more well-studied \"product\" case on empty graphs, we give the firstalgorithm to learn mixtures of non-empty DAGs.", "output": "Causal Inference Despite Limited Global Confounding via Mixture Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Full-field traffic state information (i.e., flow, speed, and density) iscritical for the successful operation of Intelligent Transportation Systems(ITS) on freeways. However, incomplete traffic information tends to be directlycollected from traffic detectors that are insufficiently installed in mostareas, which is a major obstacle to the popularization of ITS. To tackle thisissue, this paper introduces an innovative traffic state estimation (TSE)framework that hybrid regression machine learning techniques (e.g., artificialneural network (ANN), random forest (RF), and support vector machine (SVM))with a traffic physics model (e.g., second-order macroscopic traffic flowmodel) using limited information from traffic sensors as inputs to constructaccurate and full-field estimated traffic state for freeway systems. To examinethe effectiveness of the proposed TSE framework, this paper conducted empiricalstudies on a real-world data set collected from a stretch of I-15 freeway inSalt Lake City, Utah. Experimental results show that the proposed method hasbeen proved to estimate full-field traffic information accurately. Hence, theproposed method could provide accurate and full-field traffic information, thusproviding the basis for the popularization of ITS.", "output": "A Hybrid Physics Machine Learning Approach for Macroscopic Traffic State Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hybrid modelling reduces the misspecification of expert models by combiningthem with machine learning (ML) components learned from data. Similarly to manyML algorithms, hybrid model performance guarantees are limited to the trainingdistribution. Leveraging the insight that the expert model is usually valideven outside the training domain, we overcome this limitation by introducing ahybrid data augmentation strategy termed textit{expert augmentation}. Based ona probabilistic formalization of hybrid modelling, we demonstrate that expertaugmentation, which can be incorporated into existing hybrid systems, improvesgeneralization. We empirically validate the expert augmentation on threecontrolled experiments modelling dynamical systems with ordinary and partialdifferential equations. Finally, we assess the potential real-worldapplicability of expert augmentation on a dataset of a real double pendulum.", "output": "Robust Hybrid Learning With Expert Augmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Combining Federated Learning (FL) with a Trusted Execution Environment (TEE)is a promising approach for realizing privacy-preserving FL, which has garneredsignificant academic attention in recent years. Implementing the TEE on theserver side enables each round of FL to proceed without exposing the client'sgradient information to untrusted servers. This addresses usability gaps inexisting secure aggregation schemes as well as utility gaps in differentiallyprivate FL. However, to address the issue using a TEE, the vulnerabilities ofserver-side TEEs need to be considered -- this has not been sufficientlyinvestigated in the context of FL. The main technical contribution of thisstudy is the analysis of the vulnerabilities of TEE in FL and the defense.First, we theoretically analyze the leakage of memory access patterns,revealing the risk of sparsified gradients, which are commonly used in FL toenhance communication efficiency and model accuracy. Second, we devise aninference attack to link memory access patterns to sensitive information in thetraining dataset. Finally, we propose an oblivious yet efficient aggregationalgorithm to prevent memory access pattern leakage. Our experiments onreal-world data demonstrate that the proposed method functions efficiently inpractical scales.", "output": "OLIVE: Oblivious Federated Learning on Trusted Execution Environment against the risk of sparsification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised speech recognition models require considerable labeledtraining data for learning high-fidelity representations for Automatic SpeechRecognition (ASR) which is computationally demanding and time-consuming. Weconsider the task of identifying an optimal subset of data for efficientfine-tuning in self-supervised speech models for ASR. We discover that thedataset pruning strategies used in vision tasks for sampling the mostinformative examples do not perform better than random subset selection onfine-tuning self-supervised ASR. We then present the COWERAGE algorithm forrepresentative subset selection in self-supervised ASR. COWERAGE is based onour finding that ensuring the coverage of examples based on training Word ErrorRate (WER) in the early training epochs leads to better generalizationperformance. Extensive experiments with the wav2vec 2.0 and HuBERT model onTIMIT, Librispeech, and LJSpeech datasets show the effectiveness of COWERAGEand its transferability across models, with up to 17% relative WER improvementover existing dataset pruning methods and random sampling. We also demonstratethat the coverage of training instances in terms of WER values ensures theinclusion of phonemically diverse examples, leading to better test accuracy inself-supervised speech recognition models.", "output": "Representative Subset Selection for Efficient Fine-Tuning in Self-Supervised Speech Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Parameterized quantum circuits can be used as quantum neural networks andhave the potential to outperform their classical counterparts when trained foraddressing learning problems. To date, much of the results on their performanceon practical problems are heuristic in nature. In particular, the convergencerate for the training of quantum neural networks is not fully understood. Here,we analyze the dynamics of gradient descent for the training error of a classof variational quantum machine learning models. We define wide quantum neuralnetworks as parameterized quantum circuits in the limit of a large number ofqubits and variational parameters. We then find a simple analytic formula thatcaptures the average behavior of their loss function and discuss theconsequences of our findings. For example, for random quantum circuits, wepredict and characterize an exponential decay of the residual training error asa function of the parameters of the system. We finally validate our analyticresults with numerical experiments.", "output": "Analytic theory for the dynamics of wide quantum neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a new consistency-based approach for defining and solvingnonnegative/positive matrix and tensor completion problems. The novelty of theframework is that instead of artificially making the problem well-posed in theform of an application-arbitrary optimization problem, e.g., minimizing a bulkstructural measure such as rank or norm, we show that a singleproperty/constraint: preserving unit-scale consistency, guarantees theexistence of both a solution and, under relatively weak support assumptions,uniqueness. The framework and solution algorithms also generalize directly totensors of arbitrary dimensions while maintaining computational complexity thatis linear in problem size for fixed dimension d. In the context of recommendersystem (RS) applications, we prove that two reasonable properties that shouldbe expected to hold for any solution to the RS problem are sufficient to permituniqueness guarantees to be established within our framework. Key theoreticalcontributions include a general unit-consistent tensor-completion frameworkwith proofs of its properties, e.g., consensus-order and fairness, andalgorithms with optimal runtime and space complexities, e.g., O(1)term-completion with preprocessing complexity that is linear in the number ofknown terms of the matrix/tensor. From a practical perspective, the seamlessability of the framework to generalize to exploit high-dimensional structuralrelationships among key state variables, e.g., user and product attributes,offers a means for extracting significantly more information than is possiblefor alternative methods that cannot generalize beyond direct user-productrelationships. Finally, we propose our consensus ordering property as anadmissibility criterion for any proposed RS method.", "output": "Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the booming demand for machine learning applications, it has beenrecognized that the number of knowledgeable data scientists can not scale withthe growing data volumes and application needs in our digital world. Inresponse to this demand, several automated machine learning (AutoML) frameworkshave been developed to fill the gap of human expertise by automating theprocess of building machine learning pipelines. Each framework comes withdifferent heuristics-based design decisions. In this study, we present acomprehensive evaluation and comparison of the performance characteristics ofsix popular AutoML frameworks, namely, AutoWeka, AutoSKlearn, TPOT, Recipe,ATM, and SmartML, across 100 data sets from established AutoML benchmarksuites. Our experimental evaluation considers different aspects for itscomparison, including the performance impact of several design decisions,including time budget, size of search space, meta-learning, and ensembleconstruction. The results of our study reveal various interesting insights thatcan significantly guide and impact the design of AutoML frameworks.", "output": "AutoMLBench: A Comprehensive Experimental Evaluation of Automated Machine Learning Frameworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the success of convolutional neural networks (CNNs) in numerouscomputer vision tasks and their extraordinary generalization performances,several attempts to predict the generalization errors of CNNs have only beenlimited to a posteriori analyses thus far. A priori theories explaining thegeneralization performances of deep neural networks have mostly ignored theconvolutionality aspect and do not specify why CNNs are able to seeminglyovercome curse of dimensionality on computer vision tasks like imageclassification where the image dimensions are in thousands. Our work attemptsto explain the generalization performance of CNNs on image classification underthe hypothesis that CNNs operate on the domain of image patches. Ours is thefirst work we are aware of to derive an a priori error bound for thegeneralization error of CNNs and we present both quantitative and qualitativeevidences in the support of our theory. Our patch-based theory also offersexplanation for why data augmentation techniques like Cutout, CutMix and randomcropping are effective in improving the generalization error of CNNs.", "output": "CNNs Avoid Curse of Dimensionality by Learning on Patches."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge Distillation (KD) is a prominent neural model compression techniquethat heavily relies on teacher network predictions to guide the training of astudent model. Considering the ever-growing size of pre-trained language models(PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it isevident that in KD, deploying the teacher network during training adds to thememory and computational requirements of training. In the computer visionliterature, the necessity of the teacher network is put under scrutiny byshowing that KD is a label regularization technique that can be replaced withlighter teacher-free variants such as the label-smoothing technique. However,to the best of our knowledge, this issue is not investigated in NLP. Therefore,this work concerns studying different label regularization techniques andwhether we actually need them to improve the fine-tuning of smaller PLMnetworks on downstream tasks. In this regard, we did a comprehensive set ofexperiments on different PLMs such as BERT, RoBERTa, and GPT with more than 600distinct trials and ran each configuration five times. This investigation ledto a surprising observation that KD and other label regularization techniquesdo not play any meaningful role over regular fine-tuning when the student modelis pre-trained. We further explore this phenomenon in different settings of NLPand computer vision tasks and demonstrate that pre-training itself acts as akind of regularization, and additional label regularization is unnecessary.", "output": "Do we need Label Regularization to Fine-tune Pre-trained Language Models?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monumental advancements in artificial intelligence (AI) have lured theinterest of doctors, lenders, judges, and other professionals. While thesehigh-stakes decision-makers are optimistic about the technology, those familiarwith AI systems are wary about the lack of transparency of its decision-makingprocesses. Perturbation-based post hoc explainers offer a model agnostic meansof interpreting these systems while only requiring query-level access. However,recent work demonstrates that these explainers can be fooled adversarially.This discovery has adverse implications for auditors, regulators, and othersentinels. With this in mind, several natural questions arise - how can weaudit these black box systems? And how can we ascertain that the auditee iscomplying with the audit in good faith? In this work, we rigorously formalizethis problem and devise a defense against adversarial attacks onperturbation-based explainers. We propose algorithms for the detection(CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by ournovel conditional anomaly detection approach, KNN-CAD. We demonstrate that ourapproach successfully detects whether a black box system adversarially concealsits decision-making process and mitigates the adversarial attack on real-worlddata for the prevalent explainers, LIME and SHAP.", "output": "Unfooling Perturbation-Based Post Hoc Explainers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training is one of the most popular methods for training methodsrobust to adversarial attacks, however, it is not well-understood from atheoretical perspective. We prove and existence, regularity, and minimaxtheorems for adversarial surrogate risks. Our results explain some empiricalobservations on adversarial robustness from prior work and suggest newdirections in algorithm development. Furthermore, our results extend previouslyknown existence and minimax theorems for the adversarial classification risk tosurrogate risks.", "output": "Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have revealed an essential paradigm in designing loss functionsthat differentiate individual losses vs. aggregate losses. The individual lossmeasures the quality of the model on a sample, while the aggregate losscombines individual losses/scores over each training sample. Both have a commonprocedure that aggregates a set of individual values to a single numericalvalue. The ranking order reflects the most fundamental relation amongindividual values in designing losses. In addition, decomposability, in which aloss can be decomposed into an ensemble of individual terms, becomes asignificant property of organizing losses/scores. This survey provides asystematic and comprehensive review of rank-based decomposable losses inmachine learning. Specifically, we provide a new taxonomy of loss functionsthat follows the perspectives of aggregate loss and individual loss. Weidentify the aggregator to form such losses, which are examples of setfunctions. We organize the rank-based decomposable losses into eightcategories. Following these categories, we review the literature on rank-basedaggregate losses and rank-based individual losses. We describe general formulasfor these losses and connect them with existing research topics. We alsosuggest future research directions spanning unexplored, remaining, and emergingissues in rank-based decomposable losses.", "output": "Rank-based Decomposable Losses in Machine Learning: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent Language Models (LMs) achieve breakthrough performance in codegeneration when trained on human-authored problems, even solving somecompetitive-programming problems. Self-play has proven useful in games such asGo, and thus it is natural to ask whether LMs can generate their owninstructive programming problems to improve their performance. We show that itis possible for an LM to synthesize programming problems and solutions, whichare filtered for correctness by a Python interpreter. The LM's performance isthen seen to improve when it is fine-tuned on its own synthetic problems andverified solutions; thus the model 'improves itself' using the Pythoninterpreter. Problems are specified formally as programming puzzles [Schusteret al., 2021], a code-based problem format where solutions can easily beverified for correctness by execution. In experiments on publicly-availableLMs, test accuracy more than doubles. This work demonstrates the potential forcode LMs, with an interpreter, to generate instructive problems and improvetheir own performance.", "output": "Language Models Can Teach Themselves to Program Better."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Oversight AI is an emerging concept in radiology where the AI forms asymbiosis with radiologists by continuously supporting radiologists in theirdecision-making. Recent advances in vision-language models sheds a light on thelong-standing problems of the oversight AI by the understanding both visual andtextual concepts and their semantic correspondences. However, there have beenlimited successes in the application of vision-language models in the medicaldomain, as the current vision-language models and learning strategies forphotographic images and captions call for the web-scale data corpus of imageand text pairs which was not often feasible in the medical domain. To addressthis, here we present a model dubbed Medical Cross-attention Vision-Languagemodel (Medical X-VL), leveraging the key components to be tailored for themedical domain. Our medical X-VL model is based on the following components:self-supervised uni-modal models in medical domain and fusion encoder to bridgethem, momentum distillation, sentence-wise contrastive learning for medicalreports, and the sentence similarity-adjusted hard negative mining. Weexperimentally demonstrated that our model enables various zero-shot tasks foroversight AI, ranging from the zero-shot classification to zero-shot errorcorrection. Our model outperformed the current state-of-the-art models in twodifferent medical image database, suggesting the novel clinical usage of ouroversight AI model for monitoring human errors. Our method was especiallysuccessful in the data-limited setting, which is frequently encountered in theclinics, suggesting the potential widespread applicability in medical domain.", "output": "Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We conduct the first comprehensive meta-analysis of deterministic solarforecasting based on skill score, screening 1,447 papers from Google Scholarand reviewing the full texts of 320 papers for data extraction. A database of4,687 points was built and analyzed with multivariate adaptive regressionspline modelling, partial dependence plots, and linear regression. The marginalimpacts on skill score of ten factors were quantified. The analysis shows thenon-linearity and complex interaction between variables in the database.Forecast horizon has a central impact and dominates other factors' impacts.Therefore, the analysis of solar forecasts should be done separately for eachhorizon. Climate zone variables have statistically significant correlation withskill score. Regarding inputs, historical data and spatial temporal informationare highly helpful. For intra-day, sky and satellite images show the mostimportance. For day-ahead, numerical weather predictions and locally measuredmeteorological data are very efficient. All forecast models were compared.Ensemble-hybrid models achieve the most accurate forecasts for all horizons.Hybrid models show superiority for intra-hour while image-based methods are themost efficient for intra-day forecasts. More training data can enhance skillscore. However, over-fitting is observed when there is too much training data(longer than 2000 days). There has been a substantial improvement in solarforecast accuracy, especially in recent years. More improvement is observed forintra-hour and intra-day than day-ahead forecasts. By controlling for the keydifferences between forecasts, including location variables, our findings canbe applied globally.", "output": "A Meta-Analysis of Solar Forecasting Based on Skill Score."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electrical Distribution Systems are extensively penetrated with DistributedEnergy Resources (DERs) to cater the energy demands with the general perceptionthat it enhances the system's resilience. However, integration of DERs mayadversely affect the grid operation and affect the system resilience due tovarious factors like their intermittent availability, dynamics of weatherconditions, non-linearity, complexity, number of malicious threats, andimproved reliability requirements of consumers. This paper proposes amethodology to evaluate the planning and operational resilience of powerdistribution systems under extreme events and determines the withstandcapability of the electrical network. The proposed framework is developed byeffectively employing the complex network theory. Correlated networks forundesirable configurations are developed from the time series data of activepower monitored at nodes of the electrical network. For these correlatednetworks, computed the network parameters such as clustering coefficient,assortative coefficient, average degree and power law exponent for theanticipation; and percolation threshold for the determination of the networkwithstand capability under extreme conditions. The proposed methodology is alsosuitable for identifying the hosting capacity of solar panels in the systemwhile maintaining resilience under different unfavourable conditions andidentifying the most critical nodes of the system that could drive the systeminto non-resilience. This framework is demonstrated on IEEE 123 node testfeeder by generating active power time-series data for a variety of electricalconditions using simulation software, GridLAB-D. The percolation thresholdresulted as an effective metric for the determination of the planning andoperational resilience of the power distribution system.", "output": "Evaluating the Planning and Operational Resilience of Electrical Distribution Systems with Distributed Energy Resources using Complex Network Theory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning from Demonstration (LfD) approaches empower end-users to teachrobots novel tasks via demonstrations of the desired behaviors, democratizingaccess to robotics. However, current LfD frameworks are not capable of fastadaptation to heterogeneous human demonstrations nor the large-scale deploymentin ubiquitous robotics applications. In this paper, we propose a novel LfDframework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Ourapproach (1) leverages learned strategies to construct policy mixtures for fastadaptation to new demonstrations, allowing for quick end-user personalization,(2) distills common knowledge across demonstrations, achieving accurate taskinference; and (3) expands its model only when needed in lifelong deployments,maintaining a concise set of prototypical strategies that can approximate allbehaviors via policy mixtures. We empirically validate that FLAIR achievesadaptability (i.e., the robot adapts to heterogeneous, user-specific taskpreferences), efficiency (i.e., the robot achieves sample-efficientadaptation), and scalability (i.e., the model grows sublinearly with the numberof demonstrations while maintaining high performance). FLAIR surpassesbenchmarks across three control tasks with an average 57% improvement in policyreturns and an average 78% fewer episodes required for demonstration modelingusing policy mixtures. Finally, we demonstrate the success of FLAIR in a tabletennis task and find users rate FLAIR as having higher task (p&lt;.05) andpersonalization (p&lt;.05) performance.", "output": "Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series analysis is of immense importance in extensive applications, suchas weather forecasting, anomaly detection, and action recognition. This paperfocuses on temporal variation modeling, which is the common key problem ofextensive analysis tasks. Previous methods attempt to accomplish this directlyfrom the 1D time series, which is extremely challenging due to the intricatetemporal patterns. Based on the observation of multi-periodicity in timeseries, we ravel out the complex temporal variations into the multipleintraperiod- and interperiod-variations. To tackle the limitations of 1D timeseries in representation capability, we extend the analysis of temporalvariations into the 2D space by transforming the 1D time series into a set of2D tensors based on multiple periods. This transformation can embed theintraperiod- and interperiod-variations into the columns and rows of the 2Dtensors respectively, making the 2D-variations to be easily modeled by 2Dkernels. Technically, we propose the TimesNet with TimesBlock as a task-generalbackbone for time series analysis. TimesBlock can discover themulti-periodicity adaptively and extract the complex temporal variations fromtransformed 2D tensors by a parameter-efficient inception block. Our proposedTimesNet achieves consistent state-of-the-art in five mainstream time seriesanalysis tasks, including short- and long-term forecasting, imputation,classification, and anomaly detection. Code is available at this repository:", "output": "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While instance-level explanation of GNN is a well-studied problem with plentyof approaches being developed, providing a global explanation for the behaviourof a GNN is much less explored, despite its potential in interpretability anddebugging. Existing solutions either simply list local explanations for a givenclass, or generate a synthetic prototypical graph with maximal score for agiven class, completely missing any combinatorial aspect that the GNN couldhave learned. In this work, we propose GLGExplainer (Global Logic-based GNNExplainer), the first Global Explainer capable of generating explanations asarbitrary Boolean combinations of learned graphical concepts. GLGExplainer is afully differentiable architecture that takes local explanations as inputs andcombines them into a logic formula over graphical concepts, represented asclusters of local explanations. Contrary to existing solutions, GLGExplainerprovides accurate and human-interpretable global explanations that areperfectly aligned with ground-truth explanations (on synthetic data) or matchexisting domain knowledge (on real-world data). Extracted formulas are faithfulto the model predictions, to the point of providing insights into someoccasionally incorrect rules learned by the model, making GLGExplainer apromising diagnostic tool for learned GNNs.", "output": "Global Explainability of GNNs via Logic Combination of Learned Concepts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In current research, machine and deep learning solutions for theclassification of temporal data are shifting from single-channel datasets(univariate) to problems with multiple channels of information (multivariate).The majority of these works are focused on the method novelty and architecture,and the format of the input data is often treated implicitly. Particularly,multivariate datasets are often treated as a stack of univariate time series interms of input preprocessing, with scaling methods applied across each channelseparately. In this evaluation, we aim to demonstrate that the additionalchannel dimension is far from trivial and different approaches to scaling canlead to significantly different results in the accuracy of a solution. To thatend, we test seven different data transformation methods on four differenttemporal dimensions and study their effect on the classification accuracy offive recent methods. We show that, for the large majority of tested datasets,the best transformation-dimension configuration leads to an increase in theaccuracy compared to the result of each model with the same hyperparameters andno scaling, ranging from 0.16 to 76.79 percentage points. We also show that ifwe keep the transformation method constant, there is a statisticallysignificant difference in accuracy results when applying it across differentdimensions, with accuracy differences ranging from 0.23 to 47.79 percentagepoints. Finally, we explore the relation of the transformation methods anddimensions to the classifiers, and we conclude that there is no prominentgeneral trend, and the optimal configuration is dataset- andclassifier-specific.", "output": "An Empirical Evaluation of Multivariate Time Series Classification with Input Transformation across Different Dimensions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent self-supervised pre-training methods on Heterogeneous InformationNetworks (HINs) have shown promising competitiveness over traditionalsemi-supervised Heterogeneous Graph Neural Networks (HGNNs). Unfortunately,their performance heavily depends on careful customization of variousstrategies for generating high-quality positive examples and negative examples,which notably limits their flexibility and generalization ability. In thiswork, we present SHGP, a novel Self-supervised Heterogeneous Graph Pre-trainingapproach, which does not need to generate any positive examples or negativeexamples. It consists of two modules that share the same attention-aggregationscheme. In each iteration, the Att-LPA module produces pseudo-labels throughstructural clustering, which serve as the self-supervision signals to guide theAtt-HGNN module to learn object embeddings and attention coefficients. The twomodules can effectively utilize and enhance each other, promoting the model tolearn discriminative embeddings. Extensive experiments on four real-worlddatasets demonstrate the superior effectiveness of SHGP againststate-of-the-art unsupervised baselines and even semi-supervised baselines. Werelease our source code at: ", "output": "Self-supervised Heterogeneous Graph Pre-training Based on Structural Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Researchers are solving the challenges of spatial-temporal prediction bycombining Federated Learning (FL) and graph models with respect to theconstrain of privacy and security. In order to make better use of the power ofgraph model, some researchs also combine split learning(SL). However, there arestill several issues left unattended: 1) Clients might not be able to accessthe server during inference phase; 2) The graph of clients designed manually inthe server model may not reveal the proper relationship between clients. Thispaper proposes a new GNN-oriented split federated learning method, named node{bfseries M}asking and {bfseries M}ulti-granularity {bfseries M}essagepassing-based Federated Graph Model (M$^3$FGM) for the above issues. For thefirst issue, the server model of M$^3$FGM employs a MaskNode layer to simulatethe case of clients being offline. We also redesign the decoder of the clientmodel using a dual-sub-decoders structure so that each client model can use itslocal data to predict independently when offline. As for the second issue, anew GNN layer named Multi-Granularity Message Passing (MGMP) layer enables eachclient node to perceive global and local information. We conducted extensiveexperiments in two different scenarios on two real traffic datasets. Resultsshow that M$^3$FGM outperforms the baselines and variant models, achieves thebest results in both datasets and scenarios.", "output": "M3FGM:a node masking and multi-granularity message passing-based federated graph model for spatial-temporal data prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "During image editing, existing deep generative models tend to re-synthesizethe entire output from scratch, including the unedited regions. This leads to asignificant waste of computation, especially for minor editing operations. Inthis work, we present Spatially Sparse Inference (SSI), a general-purposetechnique that selectively performs computation for edited regions andaccelerates various generative models, including both conditional GANs anddiffusion models. Our key observation is that users tend to gradually changethe input image. This motivates us to cache and reuse the feature maps of theoriginal image. Given an edited image, we sparsely apply the convolutionalfilters to the edited regions while reusing the cached features for theunedited areas. Based on our algorithm, we further propose Sparse IncrementalGenerative Engine (SIGE) to convert the computation reduction to latencyreduction on off-the-shelf hardware. With about $1%$-area edits, our methodreduces the computation of DDPM by $7.5times$, Stable Diffusion by$8.2times$, and GauGAN by $18times$ while preserving the visual fidelity.With SIGE, we accelerate the inference time of DDPM by $3.0times$ on NVIDIARTX 3090 and $6.6times$ on Apple M1 Pro CPU, Stable Diffusion by $7.2times$on 3090, and GauGAN by $5.6times$ on 3090 and $14times$ on M1 Pro CPU.", "output": "Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In intensive care units (ICUs), critically ill patients are monitored withelectroencephalograms (EEGs) to prevent serious brain injury. The number ofpatients who can be monitored is constrained by the availability of trainedphysicians to read EEGs, and EEG interpretation can be subjective and prone tointer-observer variability. Automated deep learning systems for EEG couldreduce human bias and accelerate the diagnostic process. However, black boxdeep learning models are untrustworthy, difficult to troubleshoot, and lackaccountability in real-world applications, leading to a lack of trust andadoption by clinicians. To address these challenges, we propose a novelinterpretable deep learning model that not only predicts the presence ofharmful brainwave patterns but also provides high-quality case-basedexplanations of its decisions. Our model performs better than the correspondingblack box model, despite being constrained to be interpretable. The learned 2Dembedded space provides the first global overview of the structure ofictal-interictal-injury continuum brainwave patterns. The ability to understandhow our model arrived at its decisions will not only help clinicians todiagnose and treat harmful brain activities more accurately but also increasetheir trust and adoption of machine learning models in clinical practice; thiscould be an integral component of the ICU neurologists' standard workflow.", "output": "Interpretable Machine Learning System to EEG Patterns on the Ictal-Interictal-Injury Continuum."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past few years, large knowledge bases have been constructed to storemassive amounts of knowledge. However, these knowledge bases are highlyincomplete. To solve this problem, we propose a web-based question answeringsystem system with multimodal fusion of unstructured and structuredinformation, to fill in missing information for knowledge bases. To utilizeunstructured information from the Web for knowledge base completion, we designa web-based question answering system using multimodal features and questiontemplates to extract missing facts, which can achieve good performance withvery few questions. To help improve extraction quality, the question answeringsystem employs structured information from knowledge bases, such as entitytypes and entity-to-entity relatedness.", "output": "Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The progress in modelling time series and, more generally, sequences ofstructured data has recently revamped research in anomaly detection. The taskstands for identifying abnormal behaviors in financial series, IT systems,aerospace measurements, and the medical domain, where anomaly detection may aidin isolating cases of depression and attend the elderly. Anomaly detection intime series is a complex task since anomalies are rare due to highly non-lineartemporal correlations and since the definition of anomalous is sometimessubjective. Here we propose the novel use of Hyperbolic uncertainty for AnomalyDetection (HypAD). HypAD learns self-supervisedly to reconstruct the inputsignal. We adopt best practices from the state-of-the-art to encode thesequence by an LSTM, jointly learned with a decoder to reconstruct the signal,with the aid of GAN critics. Uncertainty is estimated end-to-end by means of ahyperbolic neural network. By using uncertainty, HypAD may assess whether it iscertain about the input signal but it fails to reconstruct it because this isanomalous; or whether the reconstruction error does not necessarily implyanomaly, as the model is uncertain, e.g. a complex but regular input signal.The novel key idea is that a detectable anomaly is one where the model iscertain but it predicts wrongly. HypAD outperforms the current state-of-the-artfor univariate anomaly detection on established benchmarks based on data fromNASA, Yahoo, Numenta, Amazon, and Twitter. It also yields state-of-the-artperformance on a multivariate dataset of anomaly activities in elderly homeresidences, and it outperforms the baseline on SWaT. Overall, HypAD yields thelowest false alarms at the best performance rate, thanks to successfullyidentifying detectable anomalies.", "output": "Are we certain it's anomalous?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As data becomes increasingly vital, a company would be very cautious aboutreleasing data, because the competitors could use it to train high-performancemodels, thereby posing a tremendous threat to the company's commercialcompetence. To prevent training good models on the data, we could addimperceptible perturbations to it. Since such perturbations aim at hurting theentire training process, they should reflect the vulnerability of DNN training,rather than that of a single model. Based on this new idea, we seek perturbedexamples that are always unrecognized (never correctly classified) in training.In this paper, we uncover them by model checkpoints' gradients, forming theproposed self-ensemble protection (SEP), which is very effective because (1)learning on examples ignored during normal training tends to yield DNNsignoring normal examples; (2) checkpoints' cross-model gradients are close toorthogonal, meaning that they are as diverse as DNNs with differentarchitectures. That is, our amazing performance of ensemble only requires thecomputation of training one model. By extensive experiments with 9 baselines on3 datasets and 5 architectures, SEP is verified to be a new state-of-the-art,e.g., our small $ell_infty=2/255$ perturbations reduce the accuracy of aCIFAR-10 ResNet18 from 94.56% to 14.68%, compared to 41.35% by the best-knownmethod. Code is available at ", "output": "Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past few years, large knowledge bases have been constructed to storemassive amounts of knowledge. However, these knowledge bases are highlyincomplete, for example, over 70% of people in Freebase have no known place ofbirth. To solve this problem, we propose a query-driven knowledge basecompletion system with multimodal fusion of unstructured and structuredinformation. To effectively fuse unstructured information from the Web andstructured information in knowledge bases to achieve good performance, oursystem builds multimodal knowledge graphs based on question answering and ruleinference. We propose a multimodal path fusion algorithm to rank candidateanswers based on different paths in the multimodal knowledge graphs, achievingmuch better performance than question answering, rule inference and a baselinefusion algorithm. To improve system efficiency, query-driven techniques areutilized to reduce the runtime of our system, providing fast responses to userqueries. Extensive experiments have been conducted to demonstrate theeffectiveness and efficiency of our system.", "output": "Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present hierarchical policy blending as optimal transport (HiPBOT). HiPBOThierarchically adjusts the weights of low-level reactive expert policies ofdifferent agents by adding a look-ahead planning layer on the parameter space.The high-level planner renders policy blending as unbalanced optimal transportconsolidating the scaling of the underlying Riemannian motion policies. As aresult, HiPBOT effectively decides the priorities between expert policies andagents, ensuring the task's success and guaranteeing safety. Experimentalresults in several application scenarios, from low-dimensional navigation tohigh-dimensional whole-body control, show the efficacy and efficiency ofHiPBOT. Our method outperforms state-of-the-art baselines -- either adoptingprobabilistic inference or defining a tree structure of experts -- paving theway for new applications of optimal transport to robot control. More materialat ", "output": "Hierarchical Policy Blending As Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce the Bayesian Compiler Optimization framework (BaCO), a generalpurpose autotuner for modern compilers targeting CPUs, GPUs, and FPGAs. BaCOprovides the flexibility needed to handle the requirements of modern autotuningtasks. Particularly, it deals with permutation, ordered, and continuousparameter types along with both known and unknown parameter constraints. Toreason about these parameter types and efficiently deliver high-quality code,BaCO uses Bayesian optimiza tion algorithms specialized towards the autotuningdomain. We demonstrate BaCO's effectiveness on three modern compiler systems:TACO, RISE &amp; ELEVATE, and HPVM2FPGA for CPUs, GPUs, and FPGAs respectively. Forthese domains, BaCO outperforms current state-of-the-art autotuners bydelivering on average 1.36x-1.56x faster code with a tiny search budget, andBaCO is able to reach expert-level performance 2.9x-3.9x faster.", "output": "BaCO: A Fast and Portable Bayesian Compiler Optimization Framework."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Some recent artificial neural networks (ANNs) claim to model aspects ofprimate neural and human performance data. Their success in object recognitionis, however, dependent on exploiting low-level features for solving visualtasks in a way that humans do not. As a result, out-of-distribution oradversarial input is often challenging for ANNs. Humans instead learn abstractpatterns and are mostly unaffected by many extreme image distortions. Weintroduce a set of novel image transforms inspired by neurophysiologicalfindings and evaluate humans and ANNs on an object recognition task. We showthat machines perform better than humans for certain transforms and struggle toperform at par with humans on others that are easy for humans. We quantify thedifferences in accuracy for humans and machines and find a ranking ofdifficulty for our transforms for human data. We also suggest how certaincharacteristics of human visual processing can be adapted to improve theperformance of ANNs for our difficult-for-machines transforms.", "output": "Extreme Image Transformations Affect Humans and Machines Differently."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "$L_{p}$-norm regularization schemes such as $L_{0}$, $L_{1}$, and$L_{2}$-norm regularization and $L_{p}$-norm-based regularization techniquessuch as weight decay and group LASSO compute a quantity which de pends on modelweights considered in isolation from one another. This paper describes a novelregularizer which is not based on an $L_{p}$-norm. In contrast with$L_{p}$-norm-based regularization, this regularizer is concerned with thespatial arrangement of weights within a weight matrix. This regularizer is anadditive term for the loss function and is differentiable, simple and fast tocompute, scale-invariant, requires a trivial amount of additional memory, andcan easily be parallelized. Empirically this method yields approximately a oneorder-of-magnitude improvement in the number of nonzero model parameters at agiven level of accuracy.", "output": "A Novel Sparse Regularizer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven approaches coupled with physical knowledge are powerfultechniques to model systems. The goal of such models is to efficiently solvefor the underlying field by combining measurements with known physical laws. Asmany systems contain unknown elements, such as missing parameters, noisy data,or incomplete physical laws, this is widely approached as an uncertaintyquantification problem. The common techniques to handle all the variablestypically depend on the numerical scheme used to approximate the posterior, andit is desirable to have a method which is independent of any suchdiscretization. Information field theory (IFT) provides the tools necessary toperform statistics over fields that are not necessarily Gaussian. We extend IFTto physics-informed IFT (PIFT) by encoding the functional priors withinformation about the physical laws which describe the field. The posteriorsderived from this PIFT remain independent of any numerical scheme and cancapture multiple modes, allowing for the solution of problems which areill-posed. We demonstrate our approach through an analytical example involvingthe Klein-Gordon equation. We then develop a variant of stochastic gradientLangevin dynamics to draw samples from the joint posterior over the field andmodel parameters. We apply our method to numerical examples with variousdegrees of model-form error and to inverse problems involving nonlineardifferential equations. As an addendum, the method is equipped with a metricwhich allows the posterior to automatically quantify model-form uncertainty.Because of this, our numerical experiments show that the method remains robustto even an incorrect representation of the physics given sufficient data. Wenumerically demonstrate that the method correctly identifies when the physicscannot be trusted, in which case it automatically treats learning the field asa regression problem.", "output": "Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-view image compression plays a critical role in 3D-relatedapplications. Existing methods adopt a predictive coding architecture, whichrequires joint encoding to compress the corresponding disparity as well asresidual information. This demands collaboration among cameras and enforces theepipolar geometric constraint between different views, which makes itchallenging to deploy these methods in distributed camera systems with randomlyoverlapping fields of view. Meanwhile, distributed source coding theoryindicates that efficient data compression of correlated sources can be achievedby independent encoding and joint decoding, which motivates us to design alearning-based distributed multi-view image coding (LDMIC) framework. Withindependent encoders, LDMIC introduces a simple yet effective joint contexttransfer module based on the cross-attention mechanism at the decoder toeffectively capture the global inter-view correlations, which is insensitive tothe geometric relationships between images. Experimental results show thatLDMIC significantly outperforms both traditional and learning-based MIC methodswhile enjoying fast encoding speed. Code will be released at", "output": "LDMIC: Learning-based Distributed Multi-view Image Coding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of online learning in a two-player decentralizedcooperative Stackelberg game. In each round, the leader first takes an action,followed by the follower who takes their action after observing the leader'smove. The goal of the leader is to learn to minimize the cumulative regretbased on the history of interactions. Differing from the traditionalformulation of repeated Stackelberg games, we assume the follower isomniscient, with full knowledge of the true reward, and that they alwaysbest-respond to the leader's actions. We analyze the sample complexity ofregret minimization in this repeated Stackelberg game. We show that dependingon the reward structure, the existence of the omniscient follower may changethe sample complexity drastically, from constant to exponential, even forlinear cooperative Stackelberg games. This poses unique challenges for thelearning process of the leader and the subsequent regret analysis.", "output": "Online Learning in Stackelberg Games with an Omniscient Follower."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Statistical optimality benchmarking is crucial for analyzing and designingtime series classification (TSC) algorithms. This study proposes to benchmarkthe optimality of TSC algorithms in distinguishing diffusion processes by thelikelihood ratio test (LRT). The LRT is an optimal classifier by theNeyman-Pearson lemma. The LRT benchmarks are computationally efficient becausethe LRT does not need training, and the diffusion processes can be efficientlysimulated and are flexible to reflect the specific features of real-worldapplications. We demonstrate the benchmarking with three widely-used TSCalgorithms: random forest, ResNet, and ROCKET. These algorithms can achieve theLRT optimality for univariate time series and multivariate Gaussian processes.However, these model-agnostic algorithms are suboptimal in classifyinghigh-dimensional nonlinear multivariate time series. Additionally, the LRTbenchmark provides tools to analyze the dependence of classification accuracyon the time length, dimension, temporal sampling frequency, and randomness ofthe time series.", "output": "Benchmarking optimality of time series classification methods in distinguishing diffusions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models (LMs) have been instrumental for the rapid advance of naturallanguage processing. This paper studies continual pre-training of LMs, inparticular, continual domain-adaptive pre-training (or continual DAP-training).Existing research has shown that further pre-training an LM using a domaincorpus to adapt the LM to the domain can improve the end-task performance inthe domain. This paper proposes a novel method to continually DAP-train an LMwith a sequence of unlabeled domain corpora to adapt the LM to these domains toimprove their end-task performances. The key novelty of our method is asoft-masking mechanism that directly controls the update to the LM. A novelproxy is also proposed to preserve the general knowledge in the original LM.Additionally, it contrasts the representations of the previously learned domainknowledge (including the general knowledge in the pre-trained LM) and theknowledge from the current full network to achieve knowledge integration. Themethod not only overcomes catastrophic forgetting, but also achieves knowledgetransfer to improve end-task performances. Empirical evaluation demonstratesthe effectiveness of the proposed method.", "output": "Continual Pre-training of Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in generative models such as GPT may be used to fabricateindistinguishable fake customer reviews at a much lower cost, thus posingchallenges for social media platforms to detect these machine-generated fakereviews. We propose to leverage the high-quality elite restaurant reviewsverified by Yelp to generate fake reviews from the OpenAI GPT review creatorand ultimately fine-tune a GPT output detector to predict fake reviews thatsignificantly outperform existing solutions. We further apply the model topredict non-elite reviews and identify the patterns across several dimensions,such as review, user and restaurant characteristics, and writing style. We showthat social media platforms are continuously challenged by machine-generatedfake reviews, although they may implement detection systems to filter outsuspicious reviews.", "output": "Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) have proven to be powerful in many graph-basedapplications. However, they fail to generalize well under heterophilic setups,where neighbor nodes have different labels. To address this challenge, weemploy a confidence ratio as a hyper-parameter, assuming that some of the edgesare disassortative (heterophilic). Here, we propose a two-phased algorithm.Firstly, we determine edge coefficients through subgraph matching using asupplementary module. Then, we apply GNNs with a modified label propagationmechanism to utilize the edge coefficients effectively. Specifically, oursupplementary module identifies a certain proportion of task-irrelevant edgesbased on a given confidence ratio. Using the remaining edges, we employ thewidely used optimal transport to measure the similarity between two nodes withtheir subgraphs. Finally, using the coefficients as supplementary informationon GNNs, we improve the label propagation mechanism which can prevent two nodeswith smaller weights from being closer. The experiments on benchmark datasetsshow that our model alleviates over-smoothing and improves performance.", "output": "Finding Heterophilic Neighbors via Confidence-based Subgraph Matching for Semi-supervised Node Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) is a machine learning approach that trains agentsto maximize cumulative rewards through interactions with environments. Theintegration of RL with deep learning has recently resulted in impressiveachievements in a wide range of challenging tasks, including board games,arcade games, and robot control. Despite these successes, there remain severalcrucial challenges, including brittle convergence properties caused bysensitive hyperparameters, difficulties in temporal credit assignment with longtime horizons and sparse rewards, a lack of diverse exploration, especially incontinuous search space scenarios, difficulties in credit assignment inmulti-agent reinforcement learning, and conflicting objectives for rewards.Evolutionary computation (EC), which maintains a population of learning agents,has demonstrated promising performance in addressing these limitations. Thisarticle presents a comprehensive survey of state-of-the-art methods forintegrating EC into RL, referred to as evolutionary reinforcement learning(EvoRL). We categorize EvoRL methods according to key research fields in RL,including hyperparameter optimization, policy search, exploration, rewardshaping, meta-RL, and multi-objective RL. We then discuss future researchdirections in terms of efficient methods, benchmarks, and scalable platforms.This survey serves as a resource for researchers and practitioners interestedin the field of EvoRL, highlighting the important challenges and opportunitiesfor future research. With the help of this survey, researchers andpractitioners can develop more efficient methods and tailored benchmarks forEvoRL, further advancing this promising cross-disciplinary research field.", "output": "Evolutionary Reinforcement Learning: A Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite excellent average-case performance of many image classifiers, theirperformance can substantially deteriorate on semantically coherent subgroups ofthe data that were under-represented in the training data. These systematicerrors can impact both fairness for demographic minority groups as well asrobustness and safety under domain shift. A major challenge is to identify suchsubgroups with subpar performance when the subgroups are not annotated andtheir occurrence is very rare. We leverage recent advances in text-to-imagemodels and search in the space of textual descriptions of subgroups (\"prompts\")for subgroups where the target model has low performance on theprompt-conditioned synthesized data. To tackle the exponentially growing numberof subgroups, we employ combinatorial testing. We denote this procedure asPromptAttack as it can be interpreted as an adversarial attack in a promptspace. We study subgroup coverage and identifiability with PromptAttack in acontrolled setting and find that it identifies systematic errors with highaccuracy. Thereupon, we apply PromptAttack to ImageNet classifiers and identifynovel systematic errors on rare subgroups.", "output": "Identification of Systematic Errors of Image Classifiers on Rare Subgroups."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of finding the optimal value of n in the n-steptemporal difference (TD) algorithm. We find the optimal n by resorting to themodel-free optimization technique of simultaneous perturbation stochasticapproximation (SPSA). We adopt a one-simulation SPSA procedure that isoriginally for continuous optimization to the discrete optimization frameworkbut incorporates a cyclic perturbation sequence. We prove the convergence ofour proposed algorithm, SDPSA, and show that it finds the optimal value of n inn-step TD. Through experiments, we show that the optimal value of n is achievedwith SDPSA for any arbitrary initial value of the same.", "output": "n-Step Temporal Difference Learning with Optimal n."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have shown great promise in text-guided image styletransfer, but there is a trade-off between style transformation and contentpreservation due to their stochastic nature. Existing methods requirecomputationally expensive fine-tuning of diffusion models or additional neuralnetwork. To address this, here we propose a zero-shot contrastive loss fordiffusion models that doesn't require additional fine-tuning or auxiliarynetworks. By leveraging patch-wise contrastive loss between generated samplesand original image embeddings in the pre-trained diffusion model, our methodcan generate images with the same semantic content as the source image in azero-shot manner. Our approach outperforms existing methods while preservingcontent and requiring no additional training, not only for image style transferbut also for image-to-image translation and manipulation. Our experimentalresults validate the effectiveness of our proposed method.", "output": "Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks leverage the connectivity structure of graphs as aninductive bias. Latent graph inference focuses on learning an adequate graphstructure to diffuse information on and improve the downstream performance ofthe model. In this work we employ stereographic projections of the hyperbolicand spherical model spaces, as well as products of Riemannian manifolds, forthe purpose of latent graph inference. Stereographically projected model spacesachieve comparable performance to their non-projected counterparts, whileproviding theoretical guarantees that avoid divergence of the spaces when thecurvature tends to zero. We perform experiments on both homophilic andheterophilic graphs.", "output": "Projections of Model Spaces for Latent Graph Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern computational methods, involving highly sophisticated mathematicalformulations, enable several tasks like modeling complex physical phenomenon,predicting key properties and design optimization. The higher fidelity in thesecomputer models makes it computationally intensive to query them hundreds oftimes for optimization and one usually relies on a simplified model albeit atthe cost of losing predictive accuracy and precision. Towards this, data-drivensurrogate modeling methods have shown a lot of promise in emulating thebehavior of the expensive computer models. However, a major bottleneck in suchmethods is the inability to deal with high input dimensionality and the needfor relatively large datasets. With such problems, the input and outputquantity of interest are tensors of high dimensionality. Commonly usedsurrogate modeling methods for such problems, suffer from requirements likehigh number of computational evaluations that precludes one from performingother numerical tasks like uncertainty quantification and statistical analysis.In this work, we propose an end-to-end approach that maps a high-dimensionalimage like input to an output of high dimensionality or its key statistics. Ourapproach uses two main framework that perform three steps: a) reduce the inputand output from a high-dimensional space to a reduced or low-dimensional space,b) model the input-output relationship in the low-dimensional space, and c)enable the incorporation of domain-specific physical constraints as masks. Inorder to accomplish the task of reducing input dimensionality we leverageprincipal component analysis, that is coupled with two surrogate modelingmethods namely: a) Bayesian hybrid modeling, and b) DeepHyper's deep neuralnetworks. We demonstrate the applicability of the approach on a problem of alinear elastic stress field data.", "output": "Application of probabilistic modeling and automated machine learning framework for high-dimensional stress field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modeling sounds emitted from physical object interactions is critical forimmersive perceptual experiences in real and virtual worlds. Traditionalmethods of impact sound synthesis use physics simulation to obtain a set ofphysics parameters that could represent and synthesize the sound. However, theyrequire fine details of both the object geometries and impact locations, whichare rarely available in the real world and can not be applied to synthesizeimpact sounds from common videos. On the other hand, existing video-driven deeplearning-based approaches could only capture the weak correspondence betweenvisual content and impact sounds since they lack of physics knowledge. In thiswork, we propose a physics-driven diffusion model that can synthesizehigh-fidelity impact sound for a silent video clip. In addition to the videocontent, we propose to use additional physics priors to guide the impact soundsynthesis procedure. The physics priors include both physics parameters thatare directly estimated from noisy real-world impact sound examples withoutsophisticated setup and learned residual parameters that interpret the soundenvironment via neural networks. We further implement a novel diffusion modelwith specific training and inference strategies to combine physics priors andvisual information for impact sound synthesis. Experimental results show thatour model outperforms several existing systems in generating realistic impactsounds. More importantly, the physics-based representations are fullyinterpretable and transparent, thus enabling us to perform sound editingflexibly.", "output": "Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale diffusion models like Stable Diffusion are powerful and findvarious real-world applications while customizing such models by fine-tuning isboth memory and time inefficient. Motivated by the recent progress in naturallanguage processing, we investigate parameter-efficient tuning in largediffusion models by inserting small learnable modules (termed adapters). Inparticular, we decompose the design space of adapters into orthogonal factors-- the input position, the output position as well as the function form, andperform Analysis of Variance (ANOVA), a classical statistical approach foranalyzing the correlation between discrete (design options) and continuousvariables (evaluation metrics). Our analysis suggests that the input positionof adapters is the critical factor influencing the performance of downstreamtasks. Then, we carefully study the choice of the input position, and we findthat putting the input position after the cross-attention block can lead to thebest performance, validated by additional visualization analyses. Finally, weprovide a recipe for parameter-efficient tuning in diffusion models, which iscomparable if not superior to the fully fine-tuned baseline (e.g., DreamBooth)with only 0.75 % extra parameters, across various customized tasks.", "output": "A Closer Look at Parameter-Efficient Tuning in Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, deep learning has been widely used in SAR ATR and achievedexcellent performance on the MSTAR dataset. However, due to constrained imagingconditions, MSTAR has data biases such as background correlation, i.e.,background clutter properties have a spurious correlation with target classes.Deep learning can overfit clutter to reduce training errors. Therefore, thedegree of overfitting for clutter reflects the non-causality of deep learningin SAR ATR. Existing methods only qualitatively analyze this phenomenon. Inthis paper, we quantify the contributions of different regions to targetrecognition based on the Shapley value. The Shapley value of clutter measuresthe degree of overfitting. Moreover, we explain how data bias and model biascontribute to non-causality. Concisely, data bias leads to comparablesignal-to-clutter ratios and clutter textures in training and test sets. Andvarious model structures have different degrees of overfitting for thesebiases. The experimental results of various models under standard operatingconditions on the MSTAR dataset support our conclusions. Our code is availableat ", "output": "Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) facilitates edge devices to cooperatively train aglobal shared model while maintaining the training data locally and privately.However, a common but impractical assumption in FL is that the participatingedge devices possess the same required resources and share identical globalmodel architecture. In this study, we propose a novel FL method calledFederated Intermediate Layers Learning (FedIN), supporting heterogeneous modelswithout utilizing any public dataset. The training models in FedIN are dividedinto three parts, including an extractor, the intermediate layers, and aclassifier. The model architectures of the extractor and classifier are thesame in all devices to maintain the consistency of the intermediate layerfeatures, while the architectures of the intermediate layers can vary forheterogeneous devices according to their resource capacities. To exploit theknowledge from features, we propose IN training, training the intermediatelayers in line with the features from other clients. Additionally, we formulateand solve a convex optimization problem to mitigate the gradient divergenceproblem induced by the conflicts between the IN training and the localtraining. The experiment results show that FedIN achieves the best performancein the heterogeneous model environment compared with the state-of-the-artalgorithms. Furthermore, our ablation study demonstrates the effectiveness ofIN training and the solution to the convex optimization problem.", "output": "FedIN: Federated Intermediate Layers Learning for Model Heterogeneity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce HATELEXICON, a lexicon of slurs and targets of hate speech forthe countries of Brazil, Germany, India and Kenya, to aid training andinterpretability of models. We demonstrate how our lexicon can be used tointerpret model predictions, showing that models developed to classify extremespeech rely heavily on target words when making predictions. Further, wepropose a method to aid shot selection for training in low-resource settingsvia HATELEXICON. In few-shot learning, the selection of shots is of paramountimportance to model performance. In our work, we simulate a few-shot settingfor German and Hindi, using HASOC data for training and the MultilingualHateCheck (MHC) as a benchmark. We show that selecting shots based on ourlexicon leads to models performing better on MHC than models trained on shotssampled randomly. Thus, when given only a few training examples, using ourlexicon to select shots containing more sociocultural information leads tobetter few-shot performance.", "output": "Sociocultural knowledge is needed for selection of shots in hate speech detection tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While neural networks are capable of achieving human-like performance in manytasks such as image classification, the impressive performance of each model islimited to its own dataset. Source-free domain adaptation (SFDA) was introducedto address knowledge transfer between different domains in the absence ofsource data, thus, increasing data privacy. Diversity in representation spacecan be vital to a model`s adaptability in varied and difficult domains. Inunsupervised SFDA, the diversity is limited to learning a single hypothesis onthe source or learning multiple hypotheses with a shared feature extractor.Motivated by the improved predictive performance of ensembles, we propose anovel unsupervised SFDA algorithm that promotes representational diversitythrough the use of separate feature extractors with Distinct BackboneArchitectures (DBA). Although diversity in feature space is increased, theunconstrained mutual information (MI) maximization may potentially introduceamplification of weak hypotheses. Thus we introduce the Weak HypothesisPenalization (WHP) regularizer as a mitigation strategy. Our work proposesPenalized Diversity (PD) where the synergy of DBA and WHP is applied tounsupervised source-free domain adaptation for covariate shift. In addition, PDis augmented with a weighted MI maximization objective for label distributionshift. Empirical results on natural, synthetic, and medical domains demonstratethe effectiveness of PD under different distributional shifts.", "output": "Source-free Domain Adaptation Requires Penalized Diversity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We analyze the data-dependent capacity of neural networks and assessanomalies in inputs from the perspective of networks during inference. Thenotion of data-dependent capacity allows for analyzing the knowledge base of amodel populated by learned features from training data. We define purview asthe additional capacity necessary to characterize inference samples that differfrom the training data. To probe the purview of a network, we utilize gradientsto measure the amount of change required for the model to characterize thegiven inputs more accurately. To eliminate the dependency on ground-truthlabels in generating gradients, we introduce confounding labels that areformulated by combining multiple categorical labels. We demonstrate that ourgradient-based approach can effectively differentiate inputs that cannot beaccurately represented with learned features. We utilize our approach inapplications of detecting anomalous inputs, including out-of-distribution,adversarial, and corrupted samples. Our approach requires no hyperparametertuning or additional data processing and outperforms state-of-the-art methodsby up to 2.7%, 19.8%, and 35.6% of AUROC scores, respectively.", "output": "Probing the Purview of Neural Networks via Gradient Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The real life time series are usually nonstationary, bringing a difficultquestion of model adaptation. Classical approaches like ARMA-ARCH assumearbitrary type of dependence. To avoid such bias, we will focus on recentlyproposed agnostic philosophy of moving estimator: in time $t$ findingparameters optimizing e.g. $F_t=sum_{tau&lt;t} (1-eta)^{t-tau} ln(rho_theta(x_tau))$ moving log-likelihood, evolving in time. It allows for example toestimate parameters using inexpensive exponential moving averages (EMA), likeabsolute central moments $E[|x-mu|^p]$ evolving for one or multiple powers$pinmathbb{R}^+$ using $m_{p,t+1} = m_{p,t} + eta (|x_t-mu_t|^p-m_{p,t})$.Application of such general adaptive methods of moments will be presented onStudent's t-distribution, popular especially in economical applications, hereapplied to log-returns of DJIA companies. While standard ARMA-ARCH approachesprovide evolution of $mu$ and $sigma$, here we also get evolution of $nu$describing $rho(x)sim |x|^{-nu-1}$ tail shape, probability of extreme events- which might turn out catastrophic, destabilizing the market.", "output": "Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Replicability is essential in science as it allows us to validate and verifyresearch findings. Impagliazzo, Lei, Pitassi and Sorrell (`22) recentlyinitiated the study of replicability in machine learning. A learning algorithmis replicable if it typically produces the same output when applied on twoi.i.d. inputs using the same internal randomness. We study a variant ofreplicability that does not involve fixing the randomness. An algorithmsatisfies this form of replicability if it typically produces the same outputwhen applied on two i.i.d. inputs (without fixing the internal randomness).This variant is called global stability and was introduced by Bun, Livni andMoran ('20) in the context of differential privacy.Impagliazzo et al. showed how to boost any replicable algorithm so that itproduces the same output with probability arbitrarily close to 1. In contrast,we demonstrate that for numerous learning tasks, global stability can only beaccomplished weakly, where the same output is produced only with probabilitybounded away from 1. To overcome this limitation, we introduce the concept oflist replicability, which is equivalent to global stability. Moreover, we provethat list replicability can be boosted so that it is achieved with probabilityarbitrarily close to 1. We also describe basic relations between standardlearning-theoretic complexity measures and list replicable numbers. Ourresults, in addition, imply that besides trivial cases, replicable algorithms(in the sense of Impagliazzo et al.) must be randomized.The proof of the impossibility result is based on a topological fixed-pointtheorem. For every algorithm, we are able to locate a \"hard input distribution\"by applying the Poincar'{e}-Miranda theorem in a related topological setting.The equivalence between global stability and list replicability is algorithmic.", "output": "Replicability and stability in learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Sat-NeRF, a modified implementation of the recently introducedShadow Neural Radiance Field (S-NeRF) model. This method is able to synthesizenovel views from a sparse set of satellite images of a scene, while accountingfor the variation in lighting present in the pictures. The trained model canalso be used to accurately estimate the surface elevation of the scene, whichis often a desirable quantity for satellite observation applications. S-NeRFimproves on the standard Neural Radiance Field (NeRF) method by considering theradiance as a function of the albedo and the irradiance. Both these quantitiesare output by fully connected neural network branches of the model, and thelatter is considered as a function of the direct light from the sun and thediffuse color from the sky. The implementations were run on a dataset ofsatellite images, augmented using a zoom-and-crop technique. A hyperparameterstudy for NeRF was carried out, leading to intriguing observations on themodel's convergence. Finally, both NeRF and S-NeRF were run until 100k epochsin order to fully fit the data and produce their best possible predictions. Thecode related to this article can be found aturl{", "output": "NeRF applied to satellite imagery for surface reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electronic Health Records (EHR) are generated from clinical routine carerecording valuable information of broad patient populations, which provideplentiful opportunities for improving patient management and interventionstrategies in clinical practice. To exploit the enormous potential of EHR data,a popular EHR data analysis paradigm in machine learning is EHR representationlearning, which first leverages the individual patient's EHR data to learninformative representations by a backbone, and supports diverse health-caredownstream tasks grounded on the representations. Unfortunately, such aparadigm fails to access the in-depth analysis of patients' relevance, which isgenerally known as cohort studies in clinical practice. Specifically, patientsin the same cohort tend to share similar characteristics, implying theirresemblance in medical conditions such as symptoms or diseases. In this paper,we propose a universal COhort Representation lEarning (CORE) framework toaugment EHR utilization by leveraging the fine-grained cohort information amongpatients. In particular, CORE first develops an explicit patient modeling taskbased on the prior knowledge of patients' diagnosis codes, which measures thelatent relevance among patients to adaptively divide the cohorts for eachpatient. Based on the constructed cohorts, CORE recodes the pre-extracted EHRdata representation from intra- and inter-cohort perspectives, yieldingaugmented EHR data representation learning. CORE is readily applicable todiverse backbone models, serving as a universal plug-in framework to infusecohort information into healthcare methods for boosted performance. We conductan extensive experimental evaluation on two real-world datasets, and theexperimental results demonstrate the effectiveness and generalizability ofCORE.", "output": "Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional neural networks are now seeing widespread use in a variety offields, including image classification, facial and object recognition, medicalimaging analysis, and many more. In addition, there are applications such asphysics-informed simulators in which accurate forecasts in real time with aminimal lag are required. The present neural network designs include millionsof parameters, which makes it difficult to install such complex models ondevices that have limited memory. Compression techniques might be able toresolve these issues by decreasing the size of CNN models that are created byreducing the number of parameters that contribute to the complexity of themodels. We propose a compressed tensor format of convolutional layer, a priori,before the training of the neural network. 3-way kernels or 2-way kernels inconvolutional layers are replaced by one-way fiters. The overfitting phenomenawill be reduced also. The time needed to make predictions or time required fortraining using the original Convolutional Neural Networks model would be cutsignificantly if there were fewer parameters to deal with. In this paper wepresent a method of a priori compressing convolutional neural networks forfinite element (FE) predictions of physical data. Afterwards we validate our apriori compressed models on physical data from a FE model solving a 2D waveequation. We show that the proposed convolutinal compression technique achievesequivalent performance as classical convolutional layers with fewer trainableparameters and lower memory footprint.", "output": "A priori compression of convolutional neural networks for wave simulators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays artificial neural network models achieve remarkable results in manydisciplines. Functions mapping the representation provided by the model to theprobability distribution are the inseparable aspect of deep learning solutions.Although softmax is a commonly accepted probability mapping function in themachine learning community, it cannot return sparse outputs and always spreadsthe positive probability to all positions. In this paper, we propose r-softmax,a modification of the softmax, outputting sparse probability distribution withcontrollable sparsity rate. In contrast to the existing sparse probabilitymapping functions, we provide an intuitive mechanism for controlling the outputsparsity level. We show on several multi-label datasets that r-softmaxoutperforms other sparse alternatives to softmax and is highly competitive withthe original softmax. We also apply r-softmax to the self-attention module of apre-trained transformer language model and demonstrate that it leads toimproved performance when fine-tuning the model on different natural languageprocessing tasks.", "output": "r-softmax: Generalized Softmax with Controllable Sparsity Rate."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent large-scale generative modeling has attained unprecedentedperformance especially in producing high-fidelity images driven by textprompts. Text inversion (TI), alongside the text-to-image model backbones, isproposed as an effective technique in personalizing the generation when theprompts contain user-defined, unseen or long-tail concept tokens. Despite that,we find and show that the deployment of TI remains full of \"dark-magics\" -- toname a few, the harsh requirement of additional datasets, arduous human effortsin the loop and lack of robustness. In this work, we propose a much-enhancedversion of TI, dubbed Controllable Textual Inversion (COTI), in resolving allthe aforementioned problems and in turn delivering a robust, data-efficient andeasy-to-use framework. The core to COTI is a theoretically-guided lossobjective instantiated with a comprehensive and novel weighted scoringmechanism, encapsulated by an active-learning paradigm. The extensive resultsshow that COTI significantly outperforms the prior TI-related approaches with a26.05 decrease in the FID score and a 23.00% boost in the R-precision.", "output": "Controllable Textual Inversion for Personalized Text-to-Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robust feature selection is vital for creating reliable and interpretableMachine Learning (ML) models. When designing statistical prediction models incases where domain knowledge is limited and underlying interactions areunknown, choosing the optimal set of features is often difficult. To mitigatethis issue, we introduce a Multidata (M) causal feature selection approach thatsimultaneously processes an ensemble of time series datasets and produces asingle set of causal drivers. This approach uses the causal discoveryalgorithms PC1 or PCMCI that are implemented in the Tigramite Python package.These algorithms utilize conditional independence tests to infer parts of thecausal graph. Our causal feature selection approach filters outcausally-spurious links before passing the remaining causal features as inputsto ML models (Multiple linear regression, Random Forest) that predict thetargets. We apply our framework to the statistical intensity prediction ofWestern Pacific Tropical Cyclones (TC), for which it is often difficult toaccurately choose drivers and their dimensionality reduction (time lags,vertical levels, and area-averaging). Using more stringent significancethresholds in the conditional independence tests helps eliminate spuriouscausal relationships, thus helping the ML model generalize better to unseen TCcases. M-PC1 with a reduced number of features outperforms M-PCMCI, non-causalML, and other feature selection methods (lagged correlation, random), evenslightly outperforming feature selection based on eXplainable ArtificialIntelligence. The optimal causal drivers obtained from our causal featureselection help improve our understanding of underlying relationships andsuggest new potential drivers of TC intensification.", "output": "Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A fundamental task in robotics is to navigate between two locations. Inparticular, real-world navigation can require long-horizon planning usinghigh-dimensional RGB images, which poses a substantial challenge for end-to-endlearning-based approaches. Current semi-parametric methods instead achievelong-horizon navigation by combining learned modules with a topological memoryof the environment, often represented as a graph over previously collectedimages. However, using these graphs in practice typically involves tuning anumber of pruning heuristics to avoid spurious edges, limit runtime memoryusage and allow reasonably fast graph queries. In this work, we presentOne-4-All (O4A), a method leveraging self-supervised and manifold learning toobtain a graph-free, end-to-end navigation pipeline in which the goal isspecified as an image. Navigation is achieved by greedily minimizing apotential function defined continuously over the O4A latent space. Our systemis trained offline on non-expert exploration sequences of RGB data andcontrols, and does not require any depth or pose measurements. We show that O4Acan reach long-range goals in 8 simulated Gibson indoor environments, andfurther demonstrate successful real-world navigation using a Jackal UGVplatform.", "output": "One-4-All: Neural Potential Fields for Embodied Navigation."}]